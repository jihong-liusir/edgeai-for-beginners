{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# рж╕рзЗрж╢ржи рзк тАУ SLM ржмржирж╛ржо LLM рждрзБрж▓ржирж╛\n",
    "\n",
    "ржПржХржЯрж┐ ржЫрзЛржЯ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ ржПржмржВ Foundry Local-ржП ржЪрж▓ржорж╛ржи ржПржХржЯрж┐ ржмржбрж╝ ржоржбрзЗрж▓рзЗрж░ ржоржзрзНржпрзЗ рж▓рзЗржЯрзЗржирзНрж╕рж┐ ржПржмржВ ржиржорзБржирж╛ ржкрзНрж░рждрж┐ржХрзНрж░рж┐ржпрж╝рж╛ ржЧрзБржгржорж╛ржи рждрзБрж▓ржирж╛ ржХрж░рзБржиред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## тЪб ржжрзНрж░рзБржд рж╢рзБрж░рзБ\n",
    "\n",
    "**ржорзЗржорзЛрж░рж┐-ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб рж╕рзЗржЯржЖржк (ржЖржкржбрзЗржЯ):**\n",
    "1. ржоржбрзЗрж▓ржЧрзБрж▓рзЛ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ CPU ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзЗ (ржпрзЗржХрзЛржирзЛ рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░рзЗ ржХрж╛ржЬ ржХрж░рзЗ)\n",
    "2. `qwen2.5-3b` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ 7B ржПрж░ ржкрж░рж┐ржмрж░рзНрждрзЗ (ржкрзНрж░рж╛ржпрж╝ ~4GB RAM рж╕рж╛рж╢рзНрж░ржпрж╝ ржХрж░рзЗ)\n",
    "3. ржкрзЛрж░рзНржЯ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╕ржирж╛ржХрзНржд рж╣ржпрж╝ (ржХрзЛржирзЛ ржорзНржпрж╛ржирзБржпрж╝рж╛рж▓ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржкрзНрж░ржпрж╝рзЛржЬржи ржирзЗржЗ)\n",
    "4. ржорзЛржЯ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ RAM: ~8GB рж╕рзБржкрж╛рж░рж┐рж╢ржХрзГржд (ржоржбрзЗрж▓ + OS)\n",
    "\n",
    "**ржЯрж╛рж░рзНржорж┐ржирж╛рж▓ рж╕рзЗржЯржЖржк (рзйрзж рж╕рзЗржХрзЗржирзНржб):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "рждрж╛рж░ржкрж░ ржПржЗ ржирзЛржЯржмрзБржХржЯрж┐ ржЪрж╛рж▓рж╛ржи! ЁЯЪА\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: ржирж┐рж░рзНржнрж░рждрж╛ ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи\n",
    "ржЯрж╛ржЗржорж┐ржВ ржПржмржВ ржЪрзНржпрж╛ржЯ ржЕржирзБрж░рзЛржзрзЗрж░ ржЬржирзНржп ржкрзНрж░рзЯрзЛржЬржирзАрзЯ ржирзНржпрзВржирждржо ржкрзНржпрж╛ржХрзЗржЬ (`foundry-local-sdk`, `openai`, `numpy`) ржЗржирж╕рзНржЯрж▓ ржХрж░рзЗред ржПржЯрж┐ ржирж┐рж░рж╛ржкржжржнрж╛ржмрзЗ ржкрзБржирж░рж╛рзЯ ржЪрж╛рж▓рж╛ржирзЛ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# ржкрж░рж┐рж╕рзНржерж┐рждрж┐\n",
    "ржПржХржЯрж┐ ржЫрзЛржЯ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ (SLM) ржПржмржВ ржПржХржЯрж┐ ржмржбрж╝ ржоржбрзЗрж▓рзЗрж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ ржкрзНрж░ржорзНржкржЯрзЗрж░ ржнрж┐рждрзНрждрж┐рждрзЗ рждрзБрж▓ржирж╛ ржХрж░рзБржи, ржпрж╛рждрзЗ рж╕рзБржмрж┐ржзрж╛ ржПржмржВ рж╕рзАржорж╛ржмржжрзНржзрждрж╛ржЧрзБрж▓рж┐ ржмрзЛржЭрж╛ ржпрж╛ржпрж╝:\n",
    "- **рж▓рзЗржЯрзЗржирзНрж╕рж┐ ржкрж╛рж░рзНржержХрзНржп** (ржУржпрж╝рж╛рж▓ ржХрзНрж▓ржХ рж╕рзЗржХрзЗржирзНржб)\n",
    "- **ржЯрзЛржХрзЗржи ржмрзНржпржмрж╣рж╛рж░** (ржпржжрж┐ ржЙржкрж▓ржмрзНржз ржерж╛ржХрзЗ) ржерзНрж░рзБржкрзБржЯрзЗрж░ рж╕рзВржЪржХ рж╣рж┐рж╕рзЗржмрзЗ\n",
    "- **ржЧрзБржгржЧржд ржЖржЙржЯржкрзБржЯрзЗрж░ ржиржорзБржирж╛** ржжрзНрж░рзБржд ржкрж░рзНржпржмрзЗржХрзНрж╖ржгрзЗрж░ ржЬржирзНржп\n",
    "- **ржЧрждрж┐ ржмрзГржжрзНржзрж┐рж░ рж╣рж┐рж╕рж╛ржм** ржХрж░рзНржоржХрзНрж╖ржорждрж╛ ржЙржирзНржирждрж┐рж░ ржкрж░рж┐ржорж╛ржг ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рждрзЗ\n",
    "\n",
    "**ржкрж░рж┐ржмрзЗрж╢ ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓:**\n",
    "- `SLM_ALIAS` - ржЫрзЛржЯ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ (ржбрж┐ржлрж▓рзНржЯ: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - ржмржбрж╝ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ (ржбрж┐ржлрж▓рзНржЯ: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - рждрзБрж▓ржирж╛рж░ ржЬржирзНржп ржкрж░рзАржХрзНрж╖рж╛рж░ ржкрзНрж░ржорзНржкржЯ\n",
    "- `COMPARE_RETRIES` - рж╕рзНржерж┐рждрж┐рж╢рзАрж▓рждрж╛рж░ ржЬржирзНржп ржкрзБржирж░рж╛ржпрж╝ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж╛рж░ рж╕ржВржЦрзНржпрж╛ (ржбрж┐ржлрж▓рзНржЯ: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - рж╕рж╛рж░рзНржнрж┐рж╕ ржПржирзНржбржкржпрж╝рзЗржирзНржЯ ржУржнрж╛рж░рж░рж╛ржЗржб ржХрж░рзБржи (ржпржжрж┐ рж╕рзЗржЯ ржирж╛ ржХрж░рж╛ ржерж╛ржХрзЗ, рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╕ржирж╛ржХрзНржд ржХрж░рж╛ рж╣ржмрзЗ)\n",
    "\n",
    "**ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ (ржЕржлрж┐рж╢рж┐ржпрж╝рж╛рж▓ SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи):**\n",
    "1. **FoundryLocalManager** Foundry Local рж╕рж╛рж░рзНржнрж┐рж╕ржЯрж┐ ржЖрж░ржорзНржн ржПржмржВ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рзЗ\n",
    "2. рж╕рж╛рж░рзНржнрж┐рж╕ржЯрж┐ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╢рзБрж░рзБ рж╣ржпрж╝ ржпржжрж┐ ржПржЯрж┐ ржЪрж╛рж▓рзБ ржирж╛ ржерж╛ржХрзЗ (ржХрзЛржирзЛ ржорзНржпрж╛ржирзБржпрж╝рж╛рж▓ рж╕рзЗржЯржЖржкрзЗрж░ ржкрзНрж░ржпрж╝рзЛржЬржи ржирзЗржЗ)\n",
    "3. ржоржбрзЗрж▓ржЧрзБрж▓рж┐ ржЕрзНржпрж╛рж▓рж┐ржпрж╝рж╛рж╕ ржерзЗржХрзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЖржЗржбрж┐рждрзЗ рж░рзВржкрж╛ржирзНрждрж░рж┐ржд рж╣ржпрж╝\n",
    "4. рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░-ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рж╛ рж╣ржпрж╝ (CUDA, NPU, ржмрж╛ CPU)\n",
    "5. OpenAI-рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг ржХрзНрж▓рж╛ржпрж╝рзЗржирзНржЯ ржЪрзНржпрж╛ржЯ ржХржоржкрзНрж▓рж┐рж╢ржи рж╕ржорзНржкржирзНржи ржХрж░рзЗ\n",
    "6. ржорзЗржЯрзНрж░рж┐ржХрзНрж╕ рж╕ржВржЧрзНрж░рж╣ ржХрж░рж╛ рж╣ржпрж╝: рж▓рзЗржЯрзЗржирзНрж╕рж┐, ржЯрзЛржХрзЗржи, ржЖржЙржЯржкрзБржЯ ржЧрзБржгржорж╛ржи\n",
    "7. ржлрж▓рж╛ржлрж▓ рждрзБрж▓ржирж╛ ржХрж░рзЗ ржЧрждрж┐ ржмрзГржжрзНржзрж┐рж░ ржЕржирзБржкрж╛ржд ржЧржгржирж╛ ржХрж░рж╛ рж╣ржпрж╝\n",
    "\n",
    "ржПржЗ ржХрзНрж╖рзБржжрзНрж░-рждрзБрж▓ржирж╛ ржЖржкржирж╛ржХрзЗ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирж┐рждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗ ржпрзЗ ржЖржкржирж╛рж░ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржмржбрж╝ ржоржбрзЗрж▓рзЗрж░ ржжрж┐ржХрзЗ рж░рж╛ржЙржЯрж┐ржВ ржХржЦржи ржпрзБржХрзНрждрж┐ржпрзБржХрзНрждред\n",
    "\n",
    "**SDK рж░рзЗржлрж╛рж░рзЗржирзНрж╕:** \n",
    "- ржкрж╛ржЗржержи SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- ржУржпрж╝рж╛рж░рзНржХрж╢ржк ржЗржЙржЯрж┐рж▓рж╕: ../samples/workshop_utils.py ржерзЗржХрзЗ ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ\n",
    "\n",
    "**ржорзВрж▓ рж╕рзБржмрж┐ржзрж╛рж╕ржорзВрж╣:**\n",
    "- тЬЕ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ рж╕рж╛рж░рзНржнрж┐рж╕ рж╕ржирж╛ржХрзНрждржХрж░ржг ржПржмржВ ржЖрж░ржорзНржн\n",
    "- тЬЕ рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржирж╛ ржерж╛ржХрж▓рзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╢рзБрж░рзБ\n",
    "- тЬЕ ржмрж┐рж▓рзНржЯ-ржЗржи ржоржбрзЗрж▓ рж░рзЗржЬрзЛрж▓рж┐ржЙрж╢ржи ржПржмржВ ржХрзНржпрж╛рж╢рж┐ржВ\n",
    "- тЬЕ рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи (CUDA/NPU/CPU)\n",
    "- тЬЕ OpenAI SDK рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг\n",
    "- тЬЕ ржкрзБржирж░рж╛ржпрж╝ ржЪрзЗрж╖рзНржЯрж╛ рж╕рж╣ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛\n",
    "- тЬЕ рж╕рзНржерж╛ржирзАржпрж╝ ржЗржиржлрж╛рж░рзЗржирзНрж╕ (ржХрзНрж▓рж╛ржЙржб API ржкрзНрж░ржпрж╝рзЛржЬржи ржирзЗржЗ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ЁЯЪи ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛: Foundry Local ржЪрж╛рж▓рзБ ржерж╛ржХрждрзЗ рж╣ржмрзЗ!\n",
    "\n",
    "**ржПржЗ ржирзЛржЯржмрзБржХ ржЪрж╛рж▓рж╛ржирзЛрж░ ржЖржЧрзЗ**, ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи ржпрзЗ Foundry Local рж╕рж╛рж░рзНржнрж┐рж╕ рж╕рзЗржЯржЖржк ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ:\n",
    "\n",
    "### ржжрзНрж░рзБржд рж╢рзБрж░рзБ ржХржорж╛ржирзНржб (ржЯрж╛рж░рзНржорж┐ржирж╛рж▓рзЗ ржЪрж╛рж▓рж╛ржи):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### ржмрж┐ржХрж▓рзНржк ржоржбрзЗрж▓ (ржпржжрж┐ ржбрж┐ржлрж▓рзНржЯ ржЙржкрж▓ржмрзНржз ржирж╛ ржерж╛ржХрзЗ):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "тЪая╕П **ржпржжрж┐ ржЖржкржирж┐ ржПржЗ ржзрж╛ржкржЧрзБрж▓рж┐ ржПржбрж╝рж┐ржпрж╝рзЗ ржпрж╛ржи**, рждрж╛рж╣рж▓рзЗ ржирзАржЪрзЗрж░ ржирзЛржЯржмрзБржХ рж╕рзЗрж▓ ржЪрж╛рж▓рж╛ржирзЛрж░ рж╕ржоржпрж╝ `APIConnectionError` ржжрзЗржЦрждрзЗ ржкрж╛ржмрзЗржиред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: ржорзВрж▓ ржЖржоржжрж╛ржирж┐\n",
    "ржЯрж╛ржЗржорж┐ржВ ржЗржЙржЯрж┐рж▓рж┐ржЯрж┐ ржПржмржВ Foundry Local / OpenAI ржХрзНрж▓рж╛рзЯрзЗржирзНржЯ ржирж┐рзЯрзЗ ржЖрж╕рзЗ ржпрж╛ ржоржбрзЗрж▓рзЗрж░ рждржерзНржп ржЖржирждрзЗ ржПржмржВ ржЪрзНржпрж╛ржЯ рж╕ржорзНржкржирзНржи ржХрж░рждрзЗ ржмрзНржпржмрж╣рзГржд рж╣рзЯред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: ржПрж▓рж┐ржпрж╝рж╛рж╕ ржПржмржВ ржкрзНрж░ржорзНржкржЯ рж╕рзЗржЯржЖржк\n",
    "ржЫрзЛржЯ ржПржмржВ ржмржбрж╝ ржоржбрзЗрж▓рзЗрж░ ржЬржирзНржп ржкрж░рж┐ржмрзЗрж╢-ржирж┐ржпрж╝ржирзНрждрзНрж░рж┐ржд ржПрж▓рж┐ржпрж╝рж╛рж╕ ржПржмржВ ржПржХржЯрж┐ рждрзБрж▓ржирж╛ржорзВрж▓ржХ ржкрзНрж░ржорзНржкржЯ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред ржмрж┐ржнрж┐ржирзНржи ржоржбрзЗрж▓ ржкрж░рж┐ржмрж╛рж░ ржмрж╛ ржХрж╛ржЬрзЗрж░ рж╕рж╛ржерзЗ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рж╛рж░ ржЬржирзНржп ржкрж░рж┐ржмрзЗрж╢ ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓ржЧрзБрж▓рж┐ рж╕рж╛ржоржЮрзНржЬрж╕рзНржп ржХрж░рзБржиред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ЁЯТб ржорзЗржорзЛрж░рж┐-ржЕржкрзНржЯрж┐ржорж╛ржЗржЬржб ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи\n",
    "\n",
    "**ржПржЗ ржирзЛржЯржмрзБржХржЯрж┐ ржбрж┐ржлрж▓рзНржЯржнрж╛ржмрзЗ ржорзЗржорзЛрж░рж┐-рж╕рж╛рж╢рзНрж░рзЯрзА ржоржбрзЗрж▓ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ:**\n",
    "- `phi-4-mini` тЖТ ~4GB RAM (Foundry Local рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ CPU ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзЗ)\n",
    "- `qwen2.5-3b` тЖТ ~3GB RAM (7B ржПрж░ ржкрж░рж┐ржмрж░рзНрждрзЗ, ржпрж╛ ~7GB+ ржкрзНрж░ржпрж╝рзЛржЬржи)\n",
    "\n",
    "**ржкрзЛрж░рзНржЯ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ рж╕ржирж╛ржХрзНрждржХрж░ржг:**\n",
    "- Foundry Local ржмрж┐ржнрж┐ржирзНржи ржкрзЛрж░рзНржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░рзЗ (рж╕рж╛ржзрж╛рж░ржгржд 55769 ржмрж╛ 59959)\n",
    "- ржирж┐ржЪрзЗрж░ ржбрж╛ржпрж╝рж╛ржЧржирж╕рзНржЯрж┐ржХ рж╕рзЗрж▓ржЯрж┐ рж╕ржарж┐ржХ ржкрзЛрж░рзНржЯ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╕ржирж╛ржХрзНржд ржХрж░рзЗ\n",
    "- ржХрзЛржирзЛ ржорзНржпрж╛ржирзБржпрж╝рж╛рж▓ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржкрзНрж░ржпрж╝рзЛржЬржи ржирзЗржЗ!\n",
    "\n",
    "**ржпржжрж┐ ржЖржкржирж╛рж░ RAM рж╕рзАржорж┐ржд рж╣ржпрж╝ (<8GB), ржЖрж░ржУ ржЫрзЛржЯ ржоржбрзЗрж▓ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ЁЯТб Using official Foundry SDK pattern from workshop_utils\n",
      "   тЖТ FoundryLocalManager handles service lifecycle\n",
      "   тЖТ Automatic model resolution and hardware optimization\n",
      "   тЖТ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nЁЯТб Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   тЖТ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   тЖТ Automatic model resolution and hardware optimization\")\n",
    "print(\"   тЖТ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: ржПржХрзНрж╕рж┐ржХрж┐ржЙрж╢ржи рж╣рзЗрж▓рзНржкрж╛рж░рж╕ (Foundry SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи)\n",
    "ржУржпрж╝рж╛рж░рзНржХрж╢ржкрзЗрж░ ржиржорзБржирж╛ржЧрзБрж▓рж┐рждрзЗ ржиржерж┐ржнрзБржХрзНржд ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ Foundry Local SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ:\n",
    "\n",
    "**ржкржжрзНржзрждрж┐:**\n",
    "- **FoundryLocalManager** - Foundry Local рж╕рж╛рж░рзНржнрж┐рж╕ржЯрж┐ ржЖрж░ржорзНржн ржПржмржВ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рзЗ\n",
    "- **ржЕржЯрзЛ-ржбрж┐ржЯрзЗржХрж╢ржи** - рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ ржПржирзНржбржкржпрж╝рзЗржирзНржЯ ржЖржмрж┐рж╖рзНржХрж╛рж░ ржХрж░рзЗ ржПржмржВ рж╕рж╛рж░рзНржнрж┐рж╕ рж▓рж╛ржЗржлрж╕рж╛ржЗржХрзЗрж▓ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рзЗ\n",
    "- **ржоржбрзЗрж▓ рж░рзЗржЬрзЛрж▓рж┐ржЙрж╢ржи** - ржЙржкржирж╛ржоржЧрзБрж▓рж┐ржХрзЗ ржкрзВрж░рзНржг ржоржбрзЗрж▓ ржЖржЗржбрж┐рждрзЗ рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рзЗ (ржпрзЗржоржи, phi-4-mini тЖТ phi-4-mini-instruct-cpu)\n",
    "- **рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи** - ржЙржкрж▓ржмрзНржз рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░рзЗрж░ ржЬржирзНржп рж╕рзЗрж░рж╛ ржнрзНржпрж╛рж░рж┐ржпрж╝рзЗржирзНржЯ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзЗ (CUDA, NPU, ржмрж╛ CPU)\n",
    "- **OpenAI ржХрзНрж▓рж╛ржпрж╝рзЗржирзНржЯ** - OpenAI-рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг API ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕рзЗрж░ ржЬржирзНржп ржорзНржпрж╛ржирзЗржЬрж╛рж░рзЗрж░ ржПржирзНржбржкржпрж╝рзЗржирзНржЯ ржжрж┐ржпрж╝рзЗ ржХржиржлрж┐ржЧрж╛рж░ ржХрж░рж╛\n",
    "\n",
    "**рж╕рж╣ржирж╢рзАрж▓рждрж╛рж░ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп:**\n",
    "- ржПржХрзНрж╕ржкрзЛржирзЗржирж╢рж┐ржпрж╝рж╛рж▓ ржмрзНржпрж╛ржХржЕржл рж░рж┐ржЯрзНрж░рж╛ржЗ рж▓ржЬрж┐ржХ (ржкрж░рж┐ржмрзЗрж╢рзЗрж░ ржорж╛ржзрзНржпржорзЗ ржХржиржлрж┐ржЧрж╛рж░ржпрзЛржЧрзНржп)\n",
    "- рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржирж╛ ржерж╛ржХрж▓рзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ ржЖрж░ржорзНржн ржХрж░рж╛\n",
    "- ржЖрж░ржорзНржнрзЗрж░ ржкрж░рзЗ рж╕ржВржпрзЛржЧ ржпрж╛ржЪрж╛ржЗ\n",
    "- ржмрж┐рж╢ржж рждрзНрж░рзБржЯрж┐ рж░рж┐ржкрзЛрж░рзНржЯрж┐ржВ рж╕рж╣ ржиржоржирзАржпрж╝ рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛\n",
    "- ржкрзБржирж░рж╛ржмрзГрждрзНрждрж┐ ржЖрж░ржорзНржн ржПржбрж╝рж╛рждрзЗ ржоржбрзЗрж▓ ржХрзНржпрж╛рж╢рж┐ржВ\n",
    "\n",
    "**ржлрж▓рж╛ржлрж▓рзЗрж░ ржХрж╛ржарж╛ржорзЛ:**\n",
    "- рж▓рзЗржЯрзЗржирзНрж╕рж┐ ржкрж░рж┐ржорж╛ржк (ржУржпрж╝рж╛рж▓ ржХрзНрж▓ржХ ржЯрж╛ржЗржо)\n",
    "- ржЯрзЛржХрзЗржи ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржЯрзНрж░рзНржпрж╛ржХрж┐ржВ (ржпржжрж┐ ржЙржкрж▓ржмрзНржз ржерж╛ржХрзЗ)\n",
    "- ржиржорзБржирж╛ ржЖржЙржЯржкрзБржЯ (ржкрж╛ржаржпрзЛржЧрзНржпрждрж╛рж░ ржЬржирзНржп рж╕ржВржХрзНрж╖рж┐ржкрзНржд)\n",
    "- ржмрзНржпрж░рзНрже ржЕржирзБрж░рзЛржзрзЗрж░ ржЬржирзНржп рждрзНрж░рзБржЯрж┐рж░ ржмрж┐рж╢ржж\n",
    "\n",
    "ржПржЗ ржкрзНржпрж╛ржЯрж╛рж░рзНржиржЯрж┐ workshop_utils ржоржбрж┐ржЙрж▓ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ, ржпрж╛ ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржЕржирзБрж╕рж░ржг ржХрж░рзЗред\n",
    "\n",
    "**SDK рж░рзЗржлрж╛рж░рзЗржирзНрж╕:**\n",
    "- ржкрзНрж░ржзрж╛ржи рж░рж┐ржкрзЛржЬрж┐ржЯрж░рж┐: https://github.com/microsoft/Foundry-Local\n",
    "- ржкрж╛ржЗржержи SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- ржУржпрж╝рж╛рж░рзНржХрж╢ржк ржЗржЙржЯрж┐рж▓рж╕: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Execution helpers defined: setup(), run()\n",
      "   тЖТ Uses workshop_utils for proper SDK integration\n",
      "   тЖТ setup() initializes with FoundryLocalManager\n",
      "   тЖТ run() executes inference via OpenAI-compatible API\n",
      "   тЖТ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        тЖТ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        тЖТ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        тЖТ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        тЖТ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nтЭМ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nЁЯТб Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      тЖТ foundry service status\")\n",
    "    print(f\"      тЖТ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      тЖТ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      тЖТ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"тЬЕ Execution helpers defined: setup(), run()\")\n",
    "print(\"   тЖТ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   тЖТ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   тЖТ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   тЖТ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: ржкрзНрж░рж┐-ржлрзНрж▓рж╛ржЗржЯ рж╕рзЗрж▓ржл-ржЯрзЗрж╕рзНржЯ\n",
    "FoundryLocalManager ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЙржнржпрж╝ ржоржбрзЗрж▓рзЗрж░ ржЬржирзНржп ржПржХржЯрж┐ рж╣рж╛рж▓ржХрж╛ ржХрж╛ржирзЗржХрзНржЯрж┐ржнрж┐ржЯрж┐ ржЪрзЗржХ ржЪрж╛рж▓рж╛ржирзЛ рж╣ржпрж╝ред ржПржЯрж┐ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзЗ:\n",
    "- рж╕рж╛рж░рзНржнрж┐рж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ржпрзЛржЧрзНржп\n",
    "- ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржЗржирж┐рж╢рж┐ржпрж╝рж╛рж▓рж╛ржЗржЬ ржХрж░рж╛ ржпрж╛ржпрж╝\n",
    "- ржЕрзНржпрж╛рж▓рж┐ржпрж╝рж╛рж╕ржЧрзБрж▓рзЛ ржкрзНрж░ржХрзГржд ржоржбрзЗрж▓ ржЖржЗржбрж┐рждрзЗ рж░рзЗржЬрж▓рзНржн рж╣ржпрж╝\n",
    "- рждрзБрж▓ржирж╛ ржЪрж╛рж▓рж╛ржирзЛрж░ ржЖржЧрзЗ рж╕ржВржпрзЛржЧ рж╕рзНржерж┐рждрж┐рж╢рзАрж▓\n",
    "\n",
    "setup() ржлрж╛ржВрж╢ржи workshop_utils ржерзЗржХрзЗ ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "тЭМ Foundry Local service not found!\n",
      "\n",
      "ЁЯТб To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "тЪая╕П  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"тЬЕ Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"тЬЕ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"тЪая╕П  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"тЪая╕П  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nтЭМ Foundry Local service not found!\")\n",
    "    print(\"\\nЁЯТб To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nтЬЕ Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nтЪая╕П  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪая╕П  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ЁЯТб Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"тЪая╕П  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ЁЯТб Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ЁЯЫая╕П ржжрзНрж░рзБржд рж╕ржорж╛ржзрж╛ржи: ржирзЛржЯржмрзБржХ ржерзЗржХрзЗ Foundry Local рж╢рзБрж░рзБ ржХрж░рзБржи (ржРржЪрзНржЫрж┐ржХ)\n",
    "\n",
    "ржпржжрж┐ ржЙржкрж░рзЗрж░ ржбрж╛ржпрж╝рж╛ржЧржирж╕рзНржЯрж┐ржХ ржжрзЗржЦрж╛ржпрж╝ ржпрзЗ рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржирзЗржЗ, ржЖржкржирж┐ ржПржЯрж┐ ржПржЦрж╛ржи ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗржи:\n",
    "\n",
    "**ржирзЛржЯ:** ржПржЯрж┐ ржЙржЗржирзНржбрзЛржЬрзЗ рж╕ржмржЪрзЗржпрж╝рзЗ ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗред ржЕржирзНржпрж╛ржирзНржп ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржорзЗ, ржЯрж╛рж░рзНржорж┐ржирж╛рж▓ ржХржорж╛ржирзНржб ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржиред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### тЪая╕П рж╕ржВржпрзЛржЧ рждрзНрж░рзБржЯрж┐ рж╕ржорж╛ржзрж╛ржи\n",
    "\n",
    "ржпржжрж┐ ржЖржкржирж┐ `APIConnectionError` ржжрзЗржЦрждрзЗ ржкрж╛ржи, рждрж╛рж╣рж▓рзЗ рж╕ржорзНржнржмржд Foundry Local рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржирзЗржЗ ржмрж╛ ржоржбрзЗрж▓ржЧрзБрж▓рзЛ рж▓рзЛржб ржХрж░рж╛ рж╣рзЯржирж┐ред ржПржЗ ржзрж╛ржкржЧрзБрж▓рзЛ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржи:\n",
    "\n",
    "**рзз. рж╕рж╛рж░рзНржнрж┐рж╕рзЗрж░ рж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**рзи. рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржХрж░рзБржи (ржпржжрж┐ ржЪрж╛рж▓рзБ ржирж╛ ржерж╛ржХрзЗ):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**рзй. ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ ржоржбрзЗрж▓ржЧрзБрж▓рзЛ рж▓рзЛржб ржХрж░рзБржи:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**рзк. ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржЙржкрж▓ржмрзНржз ржЖржЫрзЗ:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**рж╕рж╛ржзрж╛рж░ржг рж╕ржорж╕рзНржпрж╛:**\n",
    "- тЭМ рж╕рж╛рж░рзНржнрж┐рж╕ ржЪрж╛рж▓рзБ ржирзЗржЗ тЖТ `foundry service start` ржЪрж╛рж▓рж╛ржи\n",
    "- тЭМ ржоржбрзЗрж▓ржЧрзБрж▓рзЛ рж▓рзЛржб ржХрж░рж╛ рж╣рзЯржирж┐ тЖТ `foundry model run <model-name>` ржЪрж╛рж▓рж╛ржи\n",
    "- тЭМ ржкрзЛрж░рзНржЯрзЗрж░ рж╕ржВржШрж░рзНрж╖ тЖТ ржжрзЗржЦрзБржи ржЕржирзНржп ржХрзЛржирзЛ рж╕рж╛рж░рзНржнрж┐рж╕ ржкрзЛрж░рзНржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржЫрзЗ ржХрж┐ржирж╛\n",
    "- тЭМ ржлрж╛ржпрж╝рж╛рж░ржУржпрж╝рж╛рж▓ ржмрзНрж▓ржХ ржХрж░ржЫрзЗ тЖТ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи рж▓рзЛржХрж╛рж▓ рж╕ржВржпрзЛржЧржЧрзБрж▓рзЛ ржЕржирзБржорзЛржжрж┐ржд\n",
    "\n",
    "**ржжрзНрж░рзБржд рж╕ржорж╛ржзрж╛ржи:** ржкрзНрж░рж┐-ржлрзНрж▓рж╛ржЗржЯ ржЪрзЗржХрзЗрж░ ржЖржЧрзЗ ржирж┐ржЪрзЗрж░ ржбрж╛ржпрж╝рж╛ржЧржирж╕рзНржЯрж┐ржХ рж╕рзЗрж▓ржЯрж┐ ржЪрж╛рж▓рж╛ржиред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  тЬЕ phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  тЬЕ qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'тЬЕ' if details['status'] == 'success' else 'тЭМ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### тЬЕ ржкрзНрж░рж┐-ржлрзНрж▓рж╛ржЗржЯ ржЪрзЗржХ: ржоржбрзЗрж▓рзЗрж░ ржкрзНрж░рж╛ржкрзНржпрждрж╛\n",
    "\n",
    "ржПржЗ рж╕рзЗрж▓ржЯрж┐ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзЗ ржпрзЗ ржЙржнржпрж╝ ржоржбрзЗрж▓ ржирж┐рж░рзНржзрж╛рж░рж┐ржд ржПржирзНржбржкржпрж╝рзЗржирзНржЯрзЗ ржкрзМржБржЫрж╛ржирзЛ ржпрж╛ржпрж╝ ржХрж┐ржирж╛, рждрзБрж▓ржирж╛ ржЪрж╛рж▓рж╛ржирзЛрж░ ржЖржЧрзЗред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### ржмрзНржпрж╛ржЦрзНржпрж╛: рж░рж╛ржи рждрзБрж▓ржирж╛ ржПржмржВ ржлрж▓рж╛ржлрж▓ рж╕ржВржЧрзНрж░рж╣\n",
    "ржЕржлрж┐рж╢рж┐ржпрж╝рж╛рж▓ Foundry SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЙржнржпрж╝ ржПрж▓рж┐ржпрж╝рж╛рж╕рзЗрж░ ржЙржкрж░ ржкрзБржирж░рж╛ржмрзГрждрзНрждрж┐ ржХрж░рзЗ:\n",
    "1. ржкрзНрж░рждрж┐ржЯрж┐ ржоржбрзЗрж▓ржХрзЗ setup() ржжрж┐ржпрж╝рзЗ ржЗржирж┐рж╢рж┐ржпрж╝рж╛рж▓рж╛ржЗржЬ ржХрж░рзБржи (FoundryLocalManager ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ)\n",
    "2. OpenAI-рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг API ржжрж┐ржпрж╝рзЗ ржЗржиржлрж╛рж░рзЗржирзНрж╕ ржЪрж╛рж▓рж╛ржи\n",
    "3. рж▓рзЗржЯрзЗржирзНрж╕рж┐, ржЯрзЛржХрзЗржи ржПржмржВ ржиржорзБржирж╛ ржЖржЙржЯржкрзБржЯ рж╕ржВржЧрзНрж░рж╣ ржХрж░рзБржи\n",
    "4. рждрзБрж▓ржирж╛ржорзВрж▓ржХ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг рж╕рж╣ JSON рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐ ржХрж░рзБржи\n",
    "\n",
    "ржПржЯрж┐ session04/model_compare.py-рждрзЗ Workshop ржиржорзБржирж╛ржЧрзБрж▓рж┐рж░ ржПржХржЗ ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржЕржирзБрж╕рж░ржг ржХрж░рзЗред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "тЬЕ phi-4-mini         success         51.975          ~247 (est.)    \n",
      "тЬЕ qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ЁЯТб SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'тЬЕ' if status == 'success' else 'тЭМ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nЁЯТб SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nтЪая╕П  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### ржлрж▓рж╛ржлрж▓ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг\n",
    "\n",
    "**ржорзВрж▓ рж╕рзВржЪржХрж╕ржорзВрж╣:**\n",
    "- **рж▓рзЗржЯрзЗржирзНрж╕рж┐**: ржХржо рж╣рж▓рзЗ ржнрж╛рж▓рзЛ - ржжрзНрж░рзБржд ржкрзНрж░рждрж┐ржХрзНрж░рж┐ржпрж╝рж╛ рж╕ржоржпрж╝ ржирж┐рж░рзНржжрзЗрж╢ ржХрж░рзЗ\n",
    "- **ржЯрзЛржХрзЗржи**: ржмрзЗрж╢рж┐ ржерзНрж░рзБржкрзБржЯ = ржмрзЗрж╢рж┐ ржЯрзЛржХрзЗржи ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ржХрзГржд\n",
    "- **рж░рзБржЯ**: ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзЗ ржХрзЛржи API ржПржирзНржбржкржпрж╝рзЗржирзНржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ\n",
    "\n",
    "**SLM ржмржирж╛ржо LLM ржХржЦржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЗржи:**\n",
    "- **SLM (ржЫрзЛржЯ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓)**: ржжрзНрж░рзБржд ржкрзНрж░рждрж┐ржХрзНрж░рж┐ржпрж╝рж╛, ржХржо рж░рж┐рж╕рзЛрж░рзНрж╕ ржмрзНржпржмрж╣рж╛рж░, рж╕рж╣ржЬ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп ржЙржкржпрзБржХрзНржд\n",
    "- **LLM (ржмржбрж╝ ржнрж╛рж╖рж╛ ржоржбрзЗрж▓)**: ржЙржЪрзНржЪ ржорж╛ржирзЗрж░, ржнрж╛рж▓рзЛ ржпрзБржХрзНрждрж┐ ржкрзНрж░ржжрж╛ржи, ржпржЦржи ржорж╛ржи рж╕ржмржЪрзЗржпрж╝рзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рждржЦржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи\n",
    "\n",
    "**ржкрж░ржмрж░рзНрждрзА ржкржжржХрзНрж╖рзЗржк:**\n",
    "1. ржмрж┐ржнрж┐ржирзНржи ржкрзНрж░ржорзНржкржЯ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржи ржПржмржВ ржжрзЗржЦрзБржи ржХрж┐ржнрж╛ржмрзЗ ржЬржЯрж┐рж▓рждрж╛ рждрзБрж▓ржирж╛рж░ ржЙржкрж░ ржкрзНрж░ржнрж╛ржм ржлрзЗрж▓рзЗ\n",
    "2. ржЕржирзНржпрж╛ржирзНржп ржоржбрзЗрж▓ ржЬрзЛржбрж╝рж╛ ржирж┐ржпрж╝рзЗ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи\n",
    "3. Workshop рж░рж╛ржЙржЯрж╛рж░ ржиржорзБржирж╛ (Session 06) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи ржХрж╛ржЬрзЗрж░ ржЬржЯрж┐рж▓рждрж╛рж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ ржмрзБржжрзНржзрж┐ржорждрзНрждрж╛рж░ рж╕рж╛ржерзЗ рж░рж╛ржЙржЯ ржХрж░рж╛рж░ ржЬржирзНржп\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "тЬЕ SLM Model: phi-4-mini\n",
      "тЬЕ LLM Model: qwen2.5-7b\n",
      "тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "тЬЕ Pre-flight passed: True\n",
      "тЬЕ Comparison completed: True\n",
      "тЬЕ Both models responded: True\n",
      "======================================================================\n",
      "ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"тЬЕ SLM Model: {SLM}\")\n",
    "print(f\"тЬЕ LLM Model: {LLM}\")\n",
    "print(f\"тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"тЬЕ Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"тЪая╕П  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"тЪая╕П  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"тЪая╕П  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nтЪая╕П  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nЁЯТб Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nЁЯТб Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ржЕрж╕рзНржмрзАржХрзГрждрж┐**:  \nржПржЗ ржиржерж┐ржЯрж┐ AI ржЕржирзБржмрж╛ржж ржкрж░рж┐рж╖рзЗржмрж╛ [Co-op Translator](https://github.com/Azure/co-op-translator) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЕржирзБржмрж╛ржж ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЖржорж░рж╛ ржпржерж╛рж╕рж╛ржзрзНржп рж╕ржарж┐ржХ ржЕржирзБржмрж╛ржжрзЗрж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж┐, рждржмрзЗ ржжржпрж╝рж╛ ржХрж░рзЗ ржоржирзЗ рж░рж╛ржЦржмрзЗржи ржпрзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ ржЕржирзБржмрж╛ржжрзЗ рждрзНрж░рзБржЯрж┐ ржмрж╛ ржЕрж╕ржЩрзНржЧрждрж┐ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗред ржиржерж┐ржЯрж┐рж░ ржорзВрж▓ ржнрж╛рж╖рж╛ржпрж╝ ржерж╛ржХрж╛ рж╕ржВрж╕рзНржХрж░ржгржЯрж┐ржХрзЗ ржкрзНрж░рж╛ржорж╛ржгрж┐ржХ ржЙрзОрж╕ рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ ржЙржЪрж┐рждред ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рждржерзНржпрзЗрж░ ржЬржирзНржп, ржкрзЗрж╢рж╛ржжрж╛рж░ ржорж╛ржиржм ржЕржирзБржмрж╛ржж рж╕рзБржкрж╛рж░рж┐рж╢ ржХрж░рж╛ рж╣ржпрж╝ред ржПржЗ ржЕржирзБржмрж╛ржж ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржлрж▓рзЗ рж╕рзГрж╖рзНржЯ ржХрзЛржирзЛ ржнрзБрж▓ ржмрзЛржЭрж╛ржмрзБржЭрж┐ ржмрж╛ ржнрзБрж▓ ржмрзНржпрж╛ржЦрзНржпрж╛рж░ ржЬржирзНржп ржЖржорж░рж╛ ржжрж╛ржпрж╝рзА ржиржЗред\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-09T10:03:19+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}