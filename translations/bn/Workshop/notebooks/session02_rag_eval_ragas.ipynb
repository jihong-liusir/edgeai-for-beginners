{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392a8a8",
   "metadata": {},
   "source": [
    "# সেশন ২ – RAG মূল্যায়ন রাগাস ব্যবহার করে\n",
    "\n",
    "রাগাস মেট্রিকস ব্যবহার করে ন্যূনতম RAG পাইপলাইন মূল্যায়ন করুন: উত্তর প্রাসঙ্গিকতা, বিশ্বাসযোগ্যতা, প্রসঙ্গ নির্ভুলতা।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34473b",
   "metadata": {},
   "source": [
    "# পরিস্থিতি\n",
    "এই পরিস্থিতি একটি ক্ষুদ্র Retrieval Augmented Generation (RAG) পাইপলাইন স্থানীয়ভাবে মূল্যায়ন করে। আমরা:\n",
    "- একটি ছোট সিন্থেটিক ডকুমেন্ট কর্পাস সংজ্ঞায়িত করি।\n",
    "- ডকুমেন্টগুলিকে এম্বেড করি এবং একটি সাধারণ সাদামাটা সিমিলারিটি রিট্রিভার বাস্তবায়ন করি।\n",
    "- একটি স্থানীয় মডেল (Foundry Local / OpenAI-সামঞ্জস্যপূর্ণ) ব্যবহার করে ভিত্তিপ্রস্তরিত উত্তর তৈরি করি।\n",
    "- ragas মেট্রিক্স (`answer_relevancy`, `faithfulness`, `context_precision`) গণনা করি।\n",
    "- একটি দ্রুত মোড (env `RAG_FAST=1`) সমর্থন করি যা দ্রুত পুনরাবৃত্তির জন্য শুধুমাত্র উত্তর প্রাসঙ্গিকতা গণনা করে।\n",
    "\n",
    "এই নোটবুকটি ব্যবহার করুন আপনার স্থানীয় মডেল + এম্বেডিং স্ট্যাক বড় কর্পাসে স্কেল করার আগে তথ্যভিত্তিক ভিত্তিপ্রস্তরিত উত্তর তৈরি করে কিনা তা যাচাই করতে।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb1aa2",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: নির্ভরতা ইনস্টলেশন\n",
    "প্রয়োজনীয় লাইব্রেরি ইনস্টল করা:\n",
    "- `foundry-local-sdk` স্থানীয় মডেল ব্যবস্থাপনার জন্য।\n",
    "- `openai` ক্লায়েন্ট ইন্টারফেসের জন্য।\n",
    "- `sentence-transformers` ঘন এম্বেডিংসের জন্য।\n",
    "- `ragas` + `datasets` মূল্যায়ন ও মেট্রিক গণনার জন্য।\n",
    "- `langchain-openai` রাগাস LLM ইন্টারফেসের অ্যাডাপ্টারের জন্য।\n",
    "\n",
    "পুনরায় চালানো নিরাপদ; যদি পরিবেশ ইতিমধ্যেই প্রস্তুত থাকে তবে এড়িয়ে যান।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff641221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (ragas pulls datasets, evaluate, etc.)\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers ragas datasets numpy langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e82678",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: মূল ইমপোর্ট এবং মেট্রিক্স\n",
    "মূল লাইব্রেরি এবং রাগাস মেট্রিক্স লোড করা হয়। প্রধান উপাদানগুলো:\n",
    "- এম্বেডিংয়ের জন্য SentenceTransformer।\n",
    "- `evaluate` এবং নির্বাচিত রাগাস মেট্রিক্স।\n",
    "- মূল্যায়ন কর্পাস তৈরি করার জন্য `Dataset`।\n",
    "\n",
    "এই ইমপোর্টগুলো কোনো রিমোট কল ট্রিগার করে না (এম্বেডিংয়ের জন্য সম্ভাব্য মডেল ক্যাশ লোড ছাড়া)।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01938",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: টয় কর্পাস এবং প্রশ্নোত্তর গ্রাউন্ড ট্রুথ\n",
    "একটি ছোট ইন-মেমরি কর্পাস (`DOCS`), ব্যবহারকারীর প্রশ্নের একটি সেট এবং প্রত্যাশিত গ্রাউন্ড ট্রুথ উত্তর সংজ্ঞায়িত করে। এগুলো দ্রুত এবং নির্ধারিত মেট্রিক গণনার সুযোগ দেয়, যেখানে বাইরের ডেটা আনার প্রয়োজন হয় না। বাস্তব পরিস্থিতিতে, আপনি প্রোডাকশন কোয়েরি এবং কিউরেটেড উত্তরের নমুনা সংগ্রহ করবেন।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27307d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    " 'Foundry Local exposes a local OpenAI-compatible endpoint.',\n",
    " 'RAG retrieves relevant context snippets before generation.',\n",
    " 'Local inference improves privacy and reduces latency.',\n",
    "]\n",
    "QUESTIONS = [\n",
    " 'What advantage does local inference offer?',\n",
    " 'How does RAG improve grounding?',\n",
    "]\n",
    "GROUND_TRUTH = [\n",
    " 'It reduces latency and preserves privacy.',\n",
    " 'It adds retrieved context snippets for factual grounding.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b2ec",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: সার্ভিস ইনিট, এম্বেডিংস এবং সেফটি প্যাচ\n",
    "Foundry Local ম্যানেজার ইনিশিয়ালাইজ করে, `promptTemplate` এর জন্য একটি স্কিমা-ড্রিফট সেফটি প্যাচ প্রয়োগ করে, মডেল আইডি রিজলভ করে, OpenAI-সামঞ্জস্যপূর্ণ ক্লায়েন্ট তৈরি করে এবং ডকুমেন্ট কর্পাসের জন্য ডেন্স এম্বেডিংস প্রি-কম্পিউট করে। এটি রিট্রিভাল + জেনারেশনের জন্য পুনরায় ব্যবহারযোগ্য স্টেট প্রস্তুত করে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service running: True | Endpoint: http://127.0.0.1:57127/v1\n",
      "Cached models: [FoundryModelInfo(alias=gpt-oss-20b, id=gpt-oss-20b-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=9882 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu:4, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0)]\n",
      "Using model id: Phi-4-mini-instruct-cuda-gpu:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leestott\\AppData\\Local\\miniforge\\envs\\demo\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from foundry_local.models import FoundryModelInfo\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Safe monkeypatch for potential null promptTemplate field (schema drift guard) ---\n",
    "_original_from_list_response = FoundryModelInfo.from_list_response\n",
    "\n",
    "def _safe_from_list_response(response):  # type: ignore\n",
    "    try:\n",
    "        if isinstance(response, dict) and response.get(\"promptTemplate\") is None:\n",
    "            response[\"promptTemplate\"] = {}\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        print(f\"Warning normalizing promptTemplate: {e}\")\n",
    "    return _original_from_list_response(response)\n",
    "\n",
    "if getattr(FoundryModelInfo.from_list_response, \"__name__\", \"\") != \"_safe_from_list_response\":\n",
    "    FoundryModelInfo.from_list_response = staticmethod(_safe_from_list_response)  # type: ignore\n",
    "# --- End monkeypatch ---\n",
    "\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS','phi-3.5-mini')\n",
    "manager = FoundryLocalManager(alias)\n",
    "print(f\"Service running: {manager.is_service_running()} | Endpoint: {manager.endpoint}\")\n",
    "print('Cached models:', manager.list_cached_models())\n",
    "model_info = manager.get_model_info(alias)\n",
    "model_id = model_info.id\n",
    "print(f\"Using model id: {model_id}\")\n",
    "\n",
    "# OpenAI-compatible client\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import numpy as np\n",
    "doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d24a8",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: Retriever Function\n",
    "একটি সাধারণ ভেক্টর সাদৃশ্য রিট্রিভার সংজ্ঞায়িত করে যা নরমালাইজড এমবেডিংসের উপর ডট প্রোডাক্ট ব্যবহার করে। শীর্ষ-k ডকুমেন্ট (ডিফল্ট k=2) ফেরত দেয়। প্রোডাকশনে স্কেল এবং লেটেন্সির জন্য ANN ইনডেক্স (FAISS, Chroma, Milvus) দিয়ে এটি পরিবর্তন করুন।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af32d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return [DOCS[i] for i in sims.argsort()[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f284e",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: Generation Function\n",
    "`generate` একটি সীমাবদ্ধ প্রম্পট তৈরি করে (সিস্টেম নির্দেশ দেয় শুধুমাত্র প্রসঙ্গ ব্যবহার করতে) এবং স্থানীয় মডেলকে কল করে। কম তাপমাত্রা (0.1) সৃজনশীলতার চেয়ে সঠিক তথ্য উত্তোলনকে প্রাধান্য দেয়। এটি ছাঁটাই করা উত্তর পাঠ্য ফেরত দেয়।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7798ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, contexts):\n",
    "    ctx = \"\\n\".join(contexts)\n",
    "    messages = [\n",
    "        {'role':'system','content':'Answer using ONLY the provided context.'},\n",
    "        {'role':'user','content':f\"Context:\\n{ctx}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=model_id, messages=messages, max_tokens=120, temperature=0.1)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde788",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: ফলব্যাক ক্লায়েন্ট ইনিশিয়ালাইজেশন\n",
    "নিশ্চিত করে যে `client` বিদ্যমান থাকে, এমনকি যদি পূর্বের ইনিশিয়ালাইজেশন সেলটি এড়িয়ে যাওয়া হয় বা ব্যর্থ হয়—পরে মূল্যায়ন ধাপে NameError প্রতিরোধ করে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback client initialization (added after patch failure)\n",
    "try:\n",
    "    client  # type: ignore\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "    print('Initialized OpenAI-compatible client (late init).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17386ee",
   "metadata": {},
   "source": [
    "### ব্যাখ্যা: মূল্যায়ন লুপ এবং মেট্রিকস\n",
    "মূল্যায়ন ডেটাসেট তৈরি করে (প্রয়োজনীয় কলাম: প্রশ্ন, উত্তর, প্রসঙ্গ, গ্রাউন্ড ট্রুথ, রেফারেন্স) এবং তারপর নির্বাচিত রাগাস মেট্রিকসগুলোর উপর পুনরাবৃত্তি করে।\n",
    "\n",
    "অপ্টিমাইজেশন:\n",
    "- FAST_MODE শুধুমাত্র উত্তর প্রাসঙ্গিকতায় সীমাবদ্ধ থাকে দ্রুত ধোঁয়া পরীক্ষার জন্য।\n",
    "- প্রতি-মেট্রিক লুপ সম্পূর্ণ পুনরায় গণনা এড়ায় যখন একটি মেট্রিক ব্যর্থ হয়।\n",
    "\n",
    "একটি ডিকশনারি আউটপুট করে যেখানে মেট্রিক -> স্কোর (ব্যর্থ হলে NaN)।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset columns: ['question', 'answer', 'contexts', 'ground_truths', 'reference']\n",
      "Metrics to compute: ['answer_relevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy finished in 78.1s -> 0.6975427764759168\n",
      "RAG evaluation results: {'answer_relevancy': 0.6975427764759168}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6975427764759168}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build evaluation dataset with required columns (including 'reference' for context_precision)\n",
    "records = []\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTH):\n",
    "    ctxs = retrieve(q)\n",
    "    ans = generate(q, ctxs)\n",
    "    records.append({\n",
    "        'question': q,\n",
    "        'answer': ans,\n",
    "        'contexts': ctxs,\n",
    "        'ground_truths': [gt],\n",
    "        'reference': gt\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.run_config import RunConfig\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=model_id, base_url=manager.endpoint, api_key=manager.api_key or 'not-needed', temperature=0.0, timeout=60)\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "# Fast mode: only answer_relevancy unless RAG_FAST=0\n",
    "FAST_MODE = os.getenv('RAG_FAST','1') == '1'\n",
    "metrics = [answer_relevancy] if FAST_MODE else [answer_relevancy, faithfulness, context_precision]\n",
    "\n",
    "base_timeout = 45 if FAST_MODE else 120\n",
    "\n",
    "ds = Dataset.from_list(records)\n",
    "print('Evaluation dataset columns:', ds.column_names)\n",
    "print('Metrics to compute:', [m.name for m in metrics])\n",
    "\n",
    "results_dict = {}\n",
    "for metric in metrics:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        cfg = RunConfig(timeout=base_timeout, max_workers=1)\n",
    "        partial = evaluate(ds, metrics=[metric], llm=ragas_llm, embeddings=LocalEmbeddings(), run_config=cfg, show_progress=False)\n",
    "        raw_val = partial[metric.name]\n",
    "        if isinstance(raw_val, list):\n",
    "            numeric = [v for v in raw_val if isinstance(v, (int, float))]\n",
    "            score = float(np.nanmean(numeric)) if numeric else math.nan\n",
    "        else:\n",
    "            score = float(raw_val)\n",
    "        results_dict[metric.name] = score\n",
    "    except Exception as e:\n",
    "        results_dict[metric.name] = math.nan\n",
    "        print(f\"Metric {metric.name} failed: {e}\")\n",
    "    finally:\n",
    "        print(f\"{metric.name} finished in {time.time()-t0:.1f}s -> {results_dict[metric.name]}\")\n",
    "\n",
    "print('RAG evaluation results:', results_dict)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**অস্বীকৃতি**:  \nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়বদ্ধ থাকব না।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "4895a2e01d85b98643a177c89ff7757f",
   "translation_date": "2025-10-09T09:58:44+00:00",
   "source_file": "Workshop/notebooks/session02_rag_eval_ragas.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}