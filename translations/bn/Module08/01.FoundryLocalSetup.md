<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T15:03:51+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "bn"
}
-->
# рж╕рзЗрж╢ржи рзз: Foundry Local ржПрж░ рж╕рж╛ржерзЗ рж╢рзБрж░рзБ ржХрж░рж╛

## рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

Microsoft Foundry Local рж╕рж░рж╛рж╕рж░рж┐ ржЖржкржирж╛рж░ Windows 11 ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржкрж░рж┐ржмрзЗрж╢рзЗ Azure AI Foundry ржПрж░ ржХрзНрж╖ржорждрж╛ ржирж┐ржпрж╝рзЗ ржЖрж╕рзЗ, ржпрж╛ ржЧрзЛржкржирзАржпрж╝рждрж╛-рж╕ржВрж░ржХрзНрж╖ржгржХрж╛рж░рзА, ржХржо-рж▓рзЗржЯрзЗржирзНрж╕рж┐ AI ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯржХрзЗ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ-ржЧрзНрж░рзЗржб ржЯрзБрж▓рж╕рзЗрж░ рж╕рж╛ржерзЗ рж╕ржХрзНрж╖ржо ржХрж░рзЗред ржПржЗ рж╕рзЗрж╢ржирзЗ ржЬржиржкрзНрж░рж┐ржпрж╝ ржоржбрзЗрж▓ ржпрзЗржоржи phi, qwen, deepseek, ржПржмржВ GPT-OSS-20B ржПрж░ рж╕ржорзНржкрзВрж░рзНржг ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи, ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржПржмржВ рж╣рж╛рждрзЗ-ржХрж▓ржорзЗ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржХржнрж╛рж░ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред

## рж╢рзЗржЦрж╛рж░ рж▓ржХрзНрж╖рзНржпрж╕ржорзВрж╣

ржПржЗ рж╕рзЗрж╢ржирзЗрж░ рж╢рзЗрж╖рзЗ, ржЖржкржирж┐:
- Windows 11-ржП Foundry Local ржЗржирж╕рзНржЯрж▓ ржПржмржВ ржХржиржлрж┐ржЧрж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗржи
- CLI ржХржорж╛ржирзНржб ржПржмржВ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржЕржкрж╢ржиржЧрзБрж▓рзЛ ржЖржпрж╝рждрзНржд ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗржи
- ржЕржкрзНржЯрж┐ржорж╛рж▓ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕рзЗрж░ ржЬржирзНржп ржоржбрзЗрж▓ ржХрзНржпрж╛рж╢рж┐ржВ ржХрзМрж╢рж▓ ржмрзБржЭрждрзЗ ржкрж╛рж░ржмрзЗржи
- phi, qwen, deepseek, ржПржмржВ GPT-OSS-20B ржоржбрзЗрж▓ рж╕ржлрж▓ржнрж╛ржмрзЗ ржЪрж╛рж▓рж╛рждрзЗ ржкрж╛рж░ржмрзЗржи
- Foundry Local ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЖржкржирж╛рж░ ржкрзНрж░ржержо AI ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗржи

## ржкрзВрж░рзНржмрж╢рж░рзНржд

### рж╕рж┐рж╕рзНржЯрзЗржорзЗрж░ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛
- **Windows 11**: рж╕ржВрж╕рзНржХрж░ржг 22H2 ржмрж╛ рждрж╛рж░ ржкрж░ржмрж░рзНрждрзА
- **RAM**: ржирзНржпрзВржирждржо 16GB, рж╕рзБржкрж╛рж░рж┐рж╢ржХрзГржд 32GB
- **рж╕рзНржЯрзЛрж░рзЗржЬ**: ржоржбрзЗрж▓ ржПржмржВ ржХрзНржпрж╛рж╢рзЗрж░ ржЬржирзНржп 50GB ржлрзНрж░рж┐ рж╕рзНржкрзЗрж╕
- **рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░**: NPU- ржмрж╛ GPU-рж╕ржХрзНрж╖ржо ржбрж┐ржнрж╛ржЗрж╕ (Copilot+ PC ржмрж╛ NVIDIA GPU) ржкржЫржирзНржжржирзАржпрж╝
- **ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ**: ржоржбрзЗрж▓ ржбрж╛ржЙржирж▓рзЛржбрзЗрж░ ржЬржирзНржп ржЙржЪрзНржЪ-ржЧрждрж┐рж░ ржЗржирзНржЯрж╛рж░ржирзЗржЯ

### ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржкрж░рж┐ржмрзЗрж╢
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## ржЕржВрж╢ рзз: ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржПржмржВ рж╕рзЗржЯржЖржк

### ржзрж╛ржк рзз: Foundry Local ржЗржирж╕рзНржЯрж▓ ржХрж░рзБржи

Winget ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржмрж╛ GitHub ржерзЗржХрзЗ ржЗржирж╕рзНржЯрж▓рж╛рж░ ржбрж╛ржЙржирж▓рзЛржб ржХрж░рзЗ Foundry Local ржЗржирж╕рзНржЯрж▓ ржХрж░рзБржи:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### ржзрж╛ржк рзи: ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## ржЕржВрж╢ рзи: CLI ржмрзБржЭрждрзЗ ржкрж╛рж░рж╛

### ржорзВрж▓ ржХржорж╛ржирзНржбрзЗрж░ ржХрж╛ржарж╛ржорзЛ

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## ржЕржВрж╢ рзй: ржоржбрзЗрж▓ ржХрзНржпрж╛рж╢рж┐ржВ ржПржмржВ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛

Foundry Local ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржПржмржВ рж╕рзНржЯрзЛрж░рзЗржЬ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬ ржХрж░рж╛рж░ ржЬржирзНржп ржмрзБржжрзНржзрж┐ржорж╛ржи ржоржбрзЗрж▓ ржХрзНржпрж╛рж╢рж┐ржВ ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рзЗ:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## ржЕржВрж╢ рзк: рж╣рж╛рждрзЗ-ржХрж▓ржорзЗ ржоржбрзЗрж▓ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ

### Microsoft Phi ржоржбрзЗрж▓ ржЪрж╛рж▓рж╛ржирзЛ

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen ржоржбрзЗрж▓рзЗрж░ рж╕рж╛ржерзЗ ржХрж╛ржЬ ржХрж░рж╛

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek ржоржбрзЗрж▓ ржЪрж╛рж▓рж╛ржирзЛ

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B ржЪрж╛рж▓рж╛ржирзЛ

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## ржЕржВрж╢ рзл: ржЖржкржирж╛рж░ ржкрзНрж░ржержо ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рж╛

### ржЖржзрзБржирж┐ржХ ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи (OpenAI SDK + Foundry Local)

Foundry Local ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи рж╕рж╣ OpenAI SDK ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржПржХржЯрж┐ ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рзБржи, ржЖржорж╛ржжрзЗрж░ Sample 01 ржПрж░ ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржЕржирзБрж╕рж░ржг ржХрж░рзЗред

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("тЪая╕П Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ЁЯМР Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ЁЯПа Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"тЪая╕П Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ЁЯФз Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржЪрж╛рж▓рж╛ржи

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## ржЕржВрж╢ рзм: рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржПржмржВ рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи

### рж╕рж╛ржзрж╛рж░ржг рж╕ржорж╕рзНржпрж╛ ржПржмржВ рж╕ржорж╛ржзрж╛ржи

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### рж╕рж┐рж╕рзНржЯрзЗржо рж░рж┐рж╕рзЛрж░рзНрж╕ ржоржирж┐ржЯрж░рж┐ржВ (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### ржкрж░рж┐ржмрзЗрж╢ ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓

| ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓ | ржмрж┐ржмрж░ржг | ржбрж┐ржлрж▓рзНржЯ | ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ |
|-----------|--------|--------|-------------|
| `MODEL` | ржоржбрзЗрж▓рзЗрж░ ржЙржкржирж╛ржо ржмрж╛ ржирж╛ржо | `phi-4-mini` | ржирж╛ |
| `BASE_URL` | Foundry Local ржмрзЗрж╕ URL | `http://localhost:8000` | ржирж╛ |
| `API_KEY` | API ржХрзА (рж╕рж╛ржзрж╛рж░ржгржд рж▓рзЛржХрж╛рж▓ ржЬржирзНржп ржкрзНрж░ржпрж╝рзЛржЬржи рж╣ржпрж╝ ржирж╛) | `""` | ржирж╛ |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI ржПржирзНржбржкржпрж╝рзЗржирзНржЯ | - | Azure ржПрж░ ржЬржирзНржп |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API ржХрзА | - | Azure ржПрж░ ржЬржирзНржп |
| `AZURE_OPENAI_API_VERSION` | Azure API рж╕ржВрж╕рзНржХрж░ржг | `2024-08-01-preview` | ржирж╛ |

### рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи

- **OpenAI SDK ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи**: ржнрж╛рж▓рзЛ рж░ржХрзНрж╖ржгрж╛ржмрзЗржХрзНрж╖ржгрзЗрж░ ржЬржирзНржп OpenAI SDK ржХрзЗ ржХрж╛ржБржЪрж╛ HTTP ржЕржирзБрж░рзЛржзрзЗрж░ ржЙржкрж░ ржкрзНрж░рж╛ржзрж╛ржирзНржп ржжрж┐ржи
- **FoundryLocalManager**: ржЙржкрж▓ржмрзНржз рж╣рж▓рзЗ рж╕рж╛рж░рзНржнрж┐рж╕ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛рж░ ржЬржирзНржп ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ SDK ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- **рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛**: ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп рж╕ржарж┐ржХ ржлрж▓рзЛржмрзНржпрж╛ржХ ржХрзМрж╢рж▓ ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рзБржи
- **ржирж┐ржпрж╝ржорж┐ржд ржЖржкржЧрзНрж░рзЗржб ржХрж░рзБржи**: ржирждрзБржи ржоржбрзЗрж▓ ржПржмржВ ржлрж┐ржХрзНрж╕ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ ржХрж░рждрзЗ Foundry Local ржЖржкржбрзЗржЯ рж░рж╛ржЦрзБржи
- **ржЫрзЛржЯ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзБржи**: ржЫрзЛржЯ ржоржбрзЗрж▓ (Phi mini, Qwen 7B) ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржи ржПржмржВ ржзрзАрж░рзЗ ржзрзАрж░рзЗ рж╕рзНржХрзЗрж▓ ржХрж░рзБржи
- **рж░рж┐рж╕рзЛрж░рзНрж╕ ржоржирж┐ржЯрж░ ржХрж░рзБржи**: ржкрзНрж░ржорзНржкржЯ ржПржмржВ рж╕рзЗржЯрж┐ржВрж╕ ржЯрж┐ржЙржи ржХрж░рж╛рж░ рж╕ржоржпрж╝ CPU/GPU/ржорзЗржорзЛрж░рж┐ ржЯрзНрж░рзНржпрж╛ржХ ржХрж░рзБржи

## ржЕржВрж╢ рзн: рж╣рж╛рждрзЗ-ржХрж▓ржорзЗ ржЕржирзБрж╢рзАрж▓ржи

### ржЕржирзБрж╢рзАрж▓ржи рзз: ржжрзНрж░рзБржд ржорж╛рж▓рзНржЯрж┐-ржоржбрзЗрж▓ ржЯрзЗрж╕рзНржЯ

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### ржЕржирзБрж╢рзАрж▓ржи рзи: OpenAI SDK ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи ржЯрзЗрж╕рзНржЯ

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"тЬЕ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"тЭМ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### ржЕржирзБрж╢рзАрж▓ржи рзй: рж╕рж╛рж░рзНржнрж┐рж╕рзЗрж░ рж╕ржорзНржкрзВрж░рзНржг рж╕рзНржмрж╛рж╕рзНржерзНржп ржкрж░рзАржХрзНрж╖рж╛

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"тЬЕ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"тЬЕ {model_id}: Working")
            except Exception as e:
                print(f"тЭМ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"тЭМ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## рж░рзЗржлрж╛рж░рзЗржирзНрж╕

- **Foundry Local ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржи**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI рж░рзЗржлрж╛рж░рзЗржирзНрж╕ ржПржмржВ ржХржорж╛ржирзНржбрзЗрж░ ржУржнрж╛рж░ржнрж┐ржЙ**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face ржоржбрзЗрж▓ ржХржорзНржкрж╛ржЗрж▓ ржХрж░рзБржи**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: OpenAI SDK ржПрж░ ржорж╛ржзрзНржпржорзЗ ржжрзНрж░рзБржд ржЪрзНржпрж╛ржЯ**: samples/01/README.md
- **Sample 02: ржЙржирзНржиржд SDK ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи**: samples/02/README.md

---

