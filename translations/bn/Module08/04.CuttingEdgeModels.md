<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T17:45:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "bn"
}
-->
# সেশন ৪: আধুনিক মডেল – LLMs, SLMs, এবং অন-ডিভাইস ইনফারেন্স

## সংক্ষিপ্ত বিবরণ

LLMs এবং SLMs তুলনা করুন, স্থানীয় বনাম ক্লাউড ইনফারেন্সের সুবিধা-অসুবিধা মূল্যায়ন করুন, এবং EdgeAI পরিস্থিতি প্রদর্শনের জন্য Phi এবং ONNX Runtime ব্যবহার করে ডেমো তৈরি করুন। আমরা Chainlit RAG, WebGPU ইনফারেন্স অপশন, এবং Open WebUI ইন্টিগ্রেশনও তুলে ধরব।

রেফারেন্স:
- Foundry Local ডকস: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI হাউ-টু (Open WebUI সহ চ্যাট অ্যাপ): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## শেখার লক্ষ্য
- LLM বনাম SLM-এর খরচ, লেটেন্সি, এবং সঠিকতার সুবিধা-অসুবিধা বুঝুন
- নির্দিষ্ট ব্যবসায়িক প্রয়োজনের জন্য স্থানীয় এবং ক্লাউড ইনফারেন্সের মধ্যে নির্বাচন করুন
- Chainlit ব্যবহার করে একটি ছোট RAG ডেমো তৈরি করুন
- ব্রাউজার-সাইড অ্যাক্সিলারেশনের জন্য WebGPU অন্বেষণ করুন
- Open WebUI-কে Foundry Local-এর সাথে সংযুক্ত করুন

## অংশ ১: LLM বনাম SLM – সিদ্ধান্ত গ্রহণের ম্যাট্রিক্স

বিবেচনা করুন:
- লেটেন্সি: SLMs অন-ডিভাইসে প্রায়ই সেকেন্ডের নিচে প্রতিক্রিয়া প্রদান করে
- খরচ: স্থানীয় ইনফারেন্স ক্লাউড খরচ কমায়
- গোপনীয়তা: সংবেদনশীল ডেটা ডিভাইসেই থাকে
- সক্ষমতা: LLMs জটিল কাজগুলোতে SLMs-এর চেয়ে ভালো পারফর্ম করতে পারে
- নির্ভরযোগ্যতা: হাইব্রিড কৌশল ডাউনটাইমের ঝুঁকি কমায়

## অংশ ২: স্থানীয় বনাম ক্লাউড – হাইব্রিড প্যাটার্ন

- বড়/জটিল প্রম্পটের জন্য ক্লাউড ফ্যালব্যাক সহ স্থানীয়-প্রথম
- গোপনীয়তা-সংবেদনশীল বা অফলাইন পরিস্থিতির জন্য স্থানীয় সহ ক্লাউড-প্রথম
- কাজের ধরন অনুযায়ী রুট করুন (কোড-জেন DeepSeek-এ, সাধারণ চ্যাট Phi/Qwen-এ)

## অংশ ৩: Chainlit ব্যবহার করে RAG চ্যাট অ্যাপ (মিনিমাল)

ডিপেনডেন্সি ইনস্টল করুন:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

চালান:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

বর্ধিত করুন: একটি সাধারণ রিট্রিভার (স্থানীয় ফাইল) যোগ করুন এবং রিট্রিভ করা প্রসঙ্গ ব্যবহারকারীর প্রম্পটের আগে যুক্ত করুন।

## অংশ ৪: WebGPU ইনফারেন্স (প্রাথমিক ধারণা)

WebGPU ব্যবহার করে ব্রাউজারে ছোট মডেল সরাসরি চালান। এটি গোপনীয়তা-প্রথম ডেমো এবং জিরো-ইনস্টল অভিজ্ঞতার জন্য আদর্শ। নিচে ONNX Runtime Web এবং WebGPU এক্সিকিউশন প্রোভাইডার ব্যবহার করে একটি ধাপে ধাপে উদাহরণ দেওয়া হয়েছে।

১) WebGPU সাপোর্ট পরীক্ষা করুন
- Chromium ব্রাউজার: chrome://gpu → নিশ্চিত করুন “WebGPU” সক্রিয়
- প্রোগ্রাম্যাটিক চেক (আমরা কোডেও পরীক্ষা করব): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

২) একটি মিনিমাল প্রজেক্ট তৈরি করুন
একটি ফোল্ডার এবং দুটি ফাইল তৈরি করুন: `index.html` এবং `main.js`।

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

৩) স্থানীয়ভাবে সার্ভ করুন (Windows cmd.exe)
একটি সাধারণ স্ট্যাটিক সার্ভার ব্যবহার করুন যাতে ব্রাউজার মডেলটি ফেচ করতে পারে।

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

আপনার ব্রাউজারে http://localhost:5173 খুলুন। আপনি ইনিশিয়ালাইজেশন লগ, WebGPU সহ সেশন তৈরি, এবং একটি argmax প্রেডিকশন দেখতে পাবেন।

৪) সমস্যা সমাধান
- যদি WebGPU অনুপলব্ধ থাকে: Chrome/Edge আপডেট করুন এবং নিশ্চিত করুন GPU ড্রাইভার বর্তমান, তারপর chrome://flags-এ “Enable WebGPU” পরীক্ষা করুন।
- যদি CORS বা fetch ত্রুটি ঘটে: নিশ্চিত করুন আপনি http:// (file:// নয়) এর মাধ্যমে ফাইল সার্ভ করছেন এবং মডেল URL ক্রস-অরিজিন রিকোয়েস্ট অনুমতি দেয়।
- CPU-তে ফ্যালব্যাক: `executionProviders: ['wasm']` পরিবর্তন করুন এবং বেসলাইন আচরণ যাচাই করুন।

৫) পরবর্তী ধাপ
- একটি ডোমেইন-স্পেসিফিক ONNX মডেল (যেমন, ইমেজ ক্লাসিফিকেশন বা একটি ছোট টেক্সট মডেল) ব্যবহার করুন।
- বাস্তব ইনপুটের জন্য প্রিপ্রসেসিং/পোস্টপ্রসেসিং লজিক যোগ করুন।
- বড় মডেল বা প্রোডাকশন লেটেন্সির জন্য, Foundry Local বা ONNX Runtime Server পছন্দ করুন।

## অংশ ৫: Open WebUI + Foundry Local (ধাপে ধাপে)

এটি Open WebUI-কে Foundry Local-এর OpenAI-সামঞ্জস্যপূর্ণ এন্ডপয়েন্টের সাথে সংযুক্ত করে একটি স্থানীয় চ্যাট UI তৈরি করে।

১) প্রয়োজনীয়তা
- Foundry Local ইনস্টল এবং কাজ করছে (`foundry --version`)
- একটি মডেল স্থানীয়ভাবে চালানোর জন্য প্রস্তুত (যেমন, `phi-4-mini`)
- Docker Desktop ইনস্টল (Open WebUI-এর জন্য সুপারিশকৃত)

২) Foundry Local দিয়ে একটি মডেল চালু করুন
```powershell
foundry model run phi-4-mini
```
এটি `http://localhost:8000`-এ একটি OpenAI-সামঞ্জস্যপূর্ণ API প্রকাশ করে।

৩) Open WebUI চালু করুন (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
নোট:
- Windows-এ, `host.docker.internal` কন্টেইনারকে আপনার হোস্টে `localhost`-এ পৌঁছাতে দেয়।
- আমরা `OPENAI_API_BASE_URL` Foundry Local-এর এন্ডপয়েন্টে এবং একটি ডামি `OPENAI_API_KEY` সেট করেছি।

৪) Open WebUI UI থেকে কনফিগার করুন (বিকল্প)
- http://localhost:3000-এ ব্রাউজ করুন
- প্রাথমিক সেটআপ সম্পন্ন করুন (অ্যাডমিন ব্যবহারকারী)
- Settings → Models/Providers-এ যান
- Base URL সেট করুন: `http://host.docker.internal:8000/v1`
- API Key সেট করুন: `local-key` (প্লেসহোল্ডার)
- সংরক্ষণ করুন

৫) একটি টেস্ট প্রম্পট চালান
- Open WebUI চ্যাটে, মডেল নাম `phi-4-mini` নির্বাচন করুন বা প্রবেশ করুন
- প্রম্পট: “অন-ডিভাইস AI ইনফারেন্সের পাঁচটি সুবিধা তালিকাভুক্ত করুন।”
- আপনি আপনার স্থানীয় মডেল থেকে একটি প্রতিক্রিয়া স্ট্রিম দেখতে পাবেন

৬) সমস্যা সমাধান
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

৭) ঐচ্ছিক: Open WebUI ডেটা সংরক্ষণ করুন
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## হাতে-কলমে চেকলিস্ট
- [ ] স্থানীয়ভাবে SLM এবং LLM-এর প্রতিক্রিয়া/লেটেন্সি তুলনা করুন
- [ ] অন্তত দুটি মডেলের বিরুদ্ধে Chainlit ডেমো চালান
- [ ] Open WebUI-কে আপনার স্থানীয় এন্ডপয়েন্টে সংযুক্ত করুন এবং পরীক্ষা করুন

## পরবর্তী ধাপ
- সেশন ৫-এ এজেন্ট ওয়ার্কফ্লো প্রস্তুত করুন
- যেখানে স্থানীয়/ক্লাউড হাইব্রিড ROI উন্নত করে সেই পরিস্থিতি চিহ্নিত করুন

---

