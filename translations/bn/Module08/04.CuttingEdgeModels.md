<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:45:57+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "bn"
}
-->
# рж╕рзЗрж╢ржи рзк: Chainlit ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐

## рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

ржПржЗ рж╕рзЗрж╢ржирзЗ Chainlit ржПржмржВ Microsoft Foundry Local ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рж╛рж░ ржЙржкрж░ ржЬрзЛрж░ ржжрзЗржУржпрж╝рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЖржкржирж┐ AI ржХржерзЛржкржХржержирзЗрж░ ржЬржирзНржп ржЖржзрзБржирж┐ржХ ржУржпрж╝рзЗржм ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ рждрзИрж░рж┐ ржХрж░рж╛, рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ рж░рзЗрж╕ржкржирзНрж╕ ржмрж╛рж╕рзНрждржмрж╛ржпрж╝ржи ржПржмржВ рж╕ржарж┐ржХ рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржУ ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржбрж┐ржЬрж╛ржЗржирзЗрж░ ржорж╛ржзрзНржпржорзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржорзЛрждрж╛ржпрж╝рзЗржи ржХрж░рждрзЗ рж╢рж┐ржЦржмрзЗржиред

**ржЖржкржирж┐ ржпрж╛ рждрзИрж░рж┐ ржХрж░ржмрзЗржи:**
- **Chainlit ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржк**: рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ рж░рзЗрж╕ржкржирзНрж╕ рж╕рж╣ ржЖржзрзБржирж┐ржХ ржУржпрж╝рзЗржм UI
- **WebGPU ржбрзЗржорзЛ**: ржЧрзЛржкржирзАржпрж╝рждрж╛-ржкрзНрж░ржержо ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржмрзНрж░рж╛ржЙржЬрж╛рж░-ржнрж┐рждрзНрждрж┐ржХ ржЗржиржлрж╛рж░рзЗржирзНрж╕  
- **Open WebUI ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи**: Foundry Local рж╕рж╣ ржкрзЗрж╢рж╛ржжрж╛рж░ ржЪрзНржпрж╛ржЯ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕
- **ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржкрзНржпрж╛ржЯрж╛рж░рзНржи**: рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛, ржкрж░рзНржпржмрзЗржХрзНрж╖ржг ржПржмржВ ржорзЛрждрж╛ржпрж╝рзЗржи ржХрзМрж╢рж▓

## рж╢рзЗржЦрж╛рж░ рж▓ржХрзНрж╖рзНржп

- Chainlit ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рж╛
- ржЙржирзНржиржд ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржЕржнрж┐ржЬрзНржЮрждрж╛рж░ ржЬржирзНржп рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ рж░рзЗрж╕ржкржирзНрж╕ ржмрж╛рж╕рзНрждржмрж╛ржпрж╝ржи ржХрж░рж╛
- Foundry Local SDK ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржЖржпрж╝рждрзНржд ржХрж░рж╛
- рж╕ржарж┐ржХ рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржПржмржВ ржЧрзНрж░рзЗрж╕ржлрзБрж▓ ржбрж┐ржЧрзНрж░рзЗржбрзЗрж╢ржи ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рж╛
- ржмрж┐ржнрж┐ржирзНржи ржкрж░рж┐ржмрзЗрж╢рзЗрж░ ржЬржирзНржп ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржорзЛрждрж╛ржпрж╝рзЗржи ржПржмржВ ржХржиржлрж┐ржЧрж╛рж░ ржХрж░рж╛
- ржХржерзЛржкржХржержи AI-ржПрж░ ржЬржирзНржп ржЖржзрзБржирж┐ржХ ржУржпрж╝рзЗржм UI ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржмрзЛржЭрж╛

## ржкрзВрж░рзНржмрж╢рж░рзНржд

- **Foundry Local**: ржЗржирж╕рзНржЯрж▓ ржПржмржВ ржЪрж╛рж▓рзБ ([ржЗржирж╕рзНржЯрж▓рзЗрж╢ржи ржЧрж╛ржЗржб](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: рзй.рззрзж ржмрж╛ рждрж╛рж░ ржмрзЗрж╢рж┐ ржнрж╛рж░рзНрж╕ржи рж╕рж╣ ржнрж╛рж░рзНржЪрзБржпрж╝рж╛рж▓ ржПржиржнрж╛ржпрж╝рж░ржиржорзЗржирзНржЯ рж╕ржХрзНрж╖ржорждрж╛
- **ржоржбрзЗрж▓**: ржЕржирзНрждржд ржПржХржЯрж┐ ржоржбрзЗрж▓ рж▓рзЛржб ржХрж░рж╛ (`foundry model run phi-4-mini`)
- **ржмрзНрж░рж╛ржЙржЬрж╛рж░**: WebGPU рж╕ржорж░рзНржержи рж╕рж╣ ржЖржзрзБржирж┐ржХ ржУржпрж╝рзЗржм ржмрзНрж░рж╛ржЙржЬрж╛рж░ (Chrome/Edge)
- **Docker**: Open WebUI ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржирзЗрж░ ржЬржирзНржп (ржРржЪрзНржЫрж┐ржХ)

## ржЕржВрж╢ рзз: ржЖржзрзБржирж┐ржХ ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржмрзЛржЭрж╛

### ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

```
User Browser тЖРтЖТ Chainlit UI тЖРтЖТ Python Backend тЖРтЖТ Foundry Local тЖРтЖТ AI Model
      тЖУ              тЖУ              тЖУ              тЖУ            тЖУ
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### ржкрзНрж░ржзрж╛ржи ржкрзНрж░ржпрзБржХрзНрждрж┐

**Foundry Local SDK ржкрзНржпрж╛ржЯрж╛рж░рзНржи:**
- `FoundryLocalManager(alias)`: рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ рж╕рж╛рж░рзНржнрж┐рж╕ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯ
- `manager.endpoint` ржПржмржВ `manager.api_key`: рж╕ржВржпрзЛржЧрзЗрж░ ржмрж┐ржмрж░ржг
- `manager.get_model_info(alias).id`: ржоржбрзЗрж▓ рж╢ржирж╛ржХрзНрждржХрж░ржг

**Chainlit Framework:**
- `@cl.on_chat_start`: ржЪрзНржпрж╛ржЯ рж╕рзЗрж╢ржи рж╢рзБрж░рзБ ржХрж░рж╛
- `@cl.on_message`: ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржЗржиржХрж╛ржорж┐ржВ ржорзЗрж╕рзЗржЬ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рж╛  
- `cl.Message().stream_token()`: рж░рж┐ржпрж╝рзЗрж▓-ржЯрж╛ржЗржо рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ
- рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ UI ржЬрзЗржирж╛рж░рзЗрж╢ржи ржПржмржВ WebSocket ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯ

## ржЕржВрж╢ рзи: рж▓рзЛржХрж╛рж▓ ржмржирж╛ржо ржХрзНрж▓рж╛ржЙржб рж╕рж┐ржжрзНржзрж╛ржирзНржд ржорзНржпрж╛ржЯрзНрж░рж┐ржХрзНрж╕

### ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп

| ржжрж┐ржХ | рж▓рзЛржХрж╛рж▓ (Foundry) | ржХрзНрж▓рж╛ржЙржб (Azure OpenAI) |
|--------|-----------------|---------------------|
| **рж▓рзЗржЯрзЗржирзНрж╕рж┐** | ЁЯЪА рзлрзж-рзирзжрзжms (ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ ржЫрж╛ржбрж╝рж╛) | тП▒я╕П рзирзжрзж-рзирзжрзжрзжms (ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ ржирж┐рж░рзНржнрж░) |
| **ржЧрзЛржкржирзАржпрж╝рждрж╛** | ЁЯФТ ржбрзЗржЯрж╛ ржбрж┐ржнрж╛ржЗрж╕ ржЫрзЗржбрж╝рзЗ ржпрж╛ржпрж╝ ржирж╛ | тЪая╕П ржбрзЗржЯрж╛ ржХрзНрж▓рж╛ржЙржбрзЗ ржкрж╛ржарж╛ржирзЛ рж╣ржпрж╝ |
| **ржЦрж░ржЪ** | ЁЯТ░ рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржкрж░ ржЦрж░ржЪ ржирзЗржЗ | ЁЯТ╕ ржкрзНрж░рждрж┐ ржЯрзЛржХрзЗржирзЗрж░ ржЬржирзНржп ржЦрж░ржЪ |
| **ржЕржлрж▓рж╛ржЗржи** | тЬЕ ржЗржирзНржЯрж╛рж░ржирзЗржЯ ржЫрж╛ржбрж╝рж╛ржЗ ржХрж╛ржЬ ржХрж░рзЗ | тЭМ ржЗржирзНржЯрж╛рж░ржирзЗржЯ ржкрзНрж░ржпрж╝рзЛржЬржи |
| **ржоржбрзЗрж▓ рж╕рж╛ржЗржЬ** | тЪая╕П рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржжрзНржмрж╛рж░рж╛ рж╕рзАржорж╛ржмржжрзНржз | тЬЕ ржмрзГрж╣рждрзНрждржо ржоржбрзЗрж▓ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ |
| **рж╕рзНржХрзЗрж▓рж┐ржВ** | тЪая╕П рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ ржирж┐рж░рзНржнрж░ | тЬЕ рж╕рзАржорж╛рж╣рзАржи рж╕рзНржХрзЗрж▓рж┐ржВ |

### рж╣рж╛ржЗржмрзНрж░рж┐ржб ржХрзМрж╢рж▓ ржкрзНржпрж╛ржЯрж╛рж░рзНржи

**рж▓рзЛржХрж╛рж▓-ржкрзНрж░ржержо ржлрж▓рзЛржмрзНржпрж╛ржХ рж╕рж╣:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ржЯрж╛рж╕рзНржХ-ржнрж┐рждрзНрждрж┐ржХ рж░рж╛ржЙржЯрж┐ржВ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ржЕржВрж╢ рзй: ржиржорзБржирж╛ рзжрзк - Chainlit ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи

### ржжрзНрж░рзБржд рж╢рзБрж░рзБ

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржиржЯрж┐ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ `http://localhost:8080` ржП ржЖржзрзБржирж┐ржХ ржЪрзНржпрж╛ржЯ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ рж╕рж╣ ржЦрзБрж▓ржмрзЗред

### ржорзВрж▓ ржмрж╛рж╕рзНрждржмрж╛ржпрж╝ржи

ржиржорзБржирж╛ рзжрзк ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ ржкрзНржпрж╛ржЯрж╛рж░рзНржи ржкрзНрж░ржжрж░рзНрж╢ржи ржХрж░рзЗ:

**рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ рж╕рж╛рж░рзНржнрж┐рж╕ ржЖржмрж┐рж╖рзНржХрж╛рж░:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ ржЪрзНржпрж╛ржЯ рж╣рзНржпрж╛ржирзНржбрж▓рж╛рж░:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржЕржкрж╢ржи

**ржкрж░рж┐ржмрзЗрж╢ ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓:**

| ржнрзЗрж░рж┐ржпрж╝рзЗржмрж▓ | ржмрж┐ржмрж░ржг | ржбрж┐ржлрж▓рзНржЯ | ржЙржжрж╛рж╣рж░ржг |
|----------|-------------|---------|----------|
| `MODEL` | ржмрзНржпржмрж╣рзГржд ржоржбрзЗрж▓рзЗрж░ ржЙржкржирж╛ржо | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local ржПржирзНржбржкржпрж╝рзЗржирзНржЯ | рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ рж╕ржирж╛ржХрзНржд | `http://localhost:51211` |
| `API_KEY` | API ржХрзА (рж▓рзЛржХрж╛рж▓рзЗрж░ ржЬржирзНржп ржРржЪрзНржЫрж┐ржХ) | `""` | `your-api-key` |

**ржЙржирзНржиржд ржмрзНржпржмрж╣рж╛рж░:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ржЕржВрж╢ рзк: Jupyter ржирзЛржЯржмрзБржХ рждрзИрж░рж┐ ржПржмржВ ржмрзНржпржмрж╣рж╛рж░

### ржирзЛржЯржмрзБржХ рж╕ржорж░рзНржержирзЗрж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

ржиржорзБржирж╛ рзжрзк-ржП ржПржХржЯрж┐ ржмрж┐рж╕рзНрждрзГржд Jupyter ржирзЛржЯржмрзБржХ (`chainlit_app.ipynb`) ржЕржирзНрждрж░рзНржнрзБржХрзНржд рж░ржпрж╝рзЗржЫрзЗ ржпрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ:

- **ЁЯУЪ рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржмрж┐рж╖ржпрж╝ржмрж╕рзНрждрзБ**: ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ рж╢рзЗржЦрж╛рж░ ржЙржкржХрж░ржг
- **ЁЯФм ржЗржирзНржЯрж╛рж░рзЗржХрзНржЯрж┐ржн ржЕржирзБрж╕ржирзНржзрж╛ржи**: ржХрзЛржб рж╕рзЗрж▓ ржЪрж╛рж▓рж╛ржирзЛ ржПржмржВ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рж╛
- **ЁЯУК ржнрж┐ржЬрзНржпрзБржпрж╝рж╛рж▓ ржбрзЗржорзЛржирж╕рзНржЯрзНрж░рзЗрж╢ржи**: ржЪрж╛рж░рзНржЯ, ржбрж╛ржпрж╝рж╛ржЧрзНрж░рж╛ржо ржПржмржВ ржЖржЙржЯржкрзБржЯ ржнрж┐ржЬрзНржпрзБржпрж╝рж╛рж▓рж╛ржЗржЬрзЗрж╢ржи
- **ЁЯЫая╕П ржЙржирзНржиржпрж╝ржи рж╕рж░ржЮрзНржЬрж╛ржо**: ржЯрзЗрж╕рзНржЯрж┐ржВ ржПржмржВ ржбрж┐ржмрж╛ржЧрж┐ржВ рж╕ржХрзНрж╖ржорждрж╛

### ржЖржкржирж╛рж░ ржирж┐ржЬрж╕рзНржм ржирзЛржЯржмрзБржХ рждрзИрж░рж┐ ржХрж░рж╛

#### ржзрж╛ржк рзз: Jupyter ржкрж░рж┐ржмрзЗрж╢ рж╕рзЗржЯ ржЖржк ржХрж░рзБржи

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### ржзрж╛ржк рзи: ржПржХржЯрж┐ ржирждрзБржи ржирзЛржЯржмрзБржХ рждрзИрж░рж┐ ржХрж░рзБржи

**VS Code ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ:**
1. Module08 ржбрж┐рж░рзЗржХрзНржЯрж░рж┐рждрзЗ VS Code ржЦрзБрж▓рзБржи
2. `.ipynb` ржПржХрзНрж╕ржЯрзЗржирж╢ржи рж╕рж╣ ржПржХржЯрж┐ ржирждрзБржи ржлрж╛ржЗрж▓ рждрзИрж░рж┐ ржХрж░рзБржи
3. "Foundry Local" ржХрж╛рж░рзНржирзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рзБржи ржпржЦржи ржкрзНрж░ржорзНржкржЯ ржХрж░рж╛ рж╣ржмрзЗ
4. ржЖржкржирж╛рж░ ржмрж┐рж╖ржпрж╝ржмрж╕рзНрждрзБ рж╕рж╣ рж╕рзЗрж▓ ржпрзЛржЧ ржХрж░рж╛ рж╢рзБрж░рзБ ржХрж░рзБржи

**Jupyter Lab ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### ржирзЛржЯржмрзБржХ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░ рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи

#### рж╕рзЗрж▓ рж╕ржВржЧржаржи

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("тЬЕ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### ржЗржирзНржЯрж╛рж░рзЗржХрзНржЯрж┐ржн ржЙржжрж╛рж╣рж░ржг ржПржмржВ ржЕржирзБрж╢рзАрж▓ржи

#### ржЕржирзБрж╢рзАрж▓ржи рзз: ржХрзНрж▓рж╛ржпрж╝рзЗржирзНржЯ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржЯрзЗрж╕рзНржЯрж┐ржВ

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nЁЯзк Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'тЬЕ Success' if result['status'] == 'ok' else 'тЭМ Failed'}")
```

#### ржЕржирзБрж╢рзАрж▓ржи рзи: рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ рж░рзЗрж╕ржкржирзНрж╕ рж╕рж┐ржорзБрж▓рзЗрж╢ржи

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ЁЯМК Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nтЬЕ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ржЕржВрж╢ рзл: WebGPU ржмрзНрж░рж╛ржЙржЬрж╛рж░ ржЗржиржлрж╛рж░рзЗржирзНрж╕ ржбрзЗржорзЛ

### рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

WebGPU AI ржоржбрзЗрж▓ржЧрзБрж▓рзЛржХрзЗ рж╕рж░рж╛рж╕рж░рж┐ ржмрзНрж░рж╛ржЙржЬрж╛рж░рзЗ ржЪрж╛рж▓рж╛ржирзЛрж░ рж╕рзБржпрзЛржЧ ржжрзЗржпрж╝, ржпрж╛ рж╕рж░рзНржмрж╛ржзрж┐ржХ ржЧрзЛржкржирзАржпрж╝рждрж╛ ржПржмржВ ржЬрж┐рж░рзЛ-ржЗржирж╕рзНржЯрж▓ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред ржПржЗ ржиржорзБржирж╛ ONNX Runtime Web рж╕рж╣ WebGPU ржПржХрзНрж╕рж┐ржХрж┐ржЙрж╢ржи ржкрзНрж░ржжрж░рзНрж╢ржи ржХрж░рзЗред

### ржзрж╛ржк рзз: WebGPU рж╕ржорж░рзНржержи ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи

**ржмрзНрж░рж╛ржЙржЬрж╛рж░ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝рждрж╛:**
- Chrome/Edge рззрззрзй+ WebGPU рж╕ржХрзНрж░рж┐ржпрж╝ рж╕рж╣
- ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи: `chrome://gpu` тЖТ "WebGPU" рж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи
- ржкрзНрж░рзЛржЧрзНрж░рж╛ржорзНржпрж╛ржЯрж┐ржХ ржкрж░рзАржХрзНрж╖рж╛: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### ржзрж╛ржк рзи: WebGPU ржбрзЗржорзЛ рждрзИрж░рж┐ ржХрж░рзБржи

ржбрж┐рж░рзЗржХрзНржЯрж░рж┐ рждрзИрж░рж┐ ржХрж░рзБржи: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ЁЯЪА WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'тЭМ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ЁЯФН WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('тЬЕ ONNX Runtime session created with WebGPU');
        log(`ЁЯУК Input names: ${session.inputNames.join(', ')}`);
        log(`ЁЯУК Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'тЬЕ WebGPU inference complete!';
        log(`ЁЯОп Predicted class: ${maxIdx}`);
        log(`ЁЯУИ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `тЭМ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### ржзрж╛ржк рзй: ржбрзЗржорзЛ ржЪрж╛рж▓рж╛ржи

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ржЕржВрж╢ рзм: Open WebUI ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи

### рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

Open WebUI ржПржХржЯрж┐ ржкрзЗрж╢рж╛ржжрж╛рж░ ChatGPT-ржПрж░ ржорждрзЛ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ ржпрж╛ Foundry Local-ржПрж░ OpenAI-рж╕рж╛ржоржЮрзНржЬрж╕рзНржпржкрзВрж░рзНржг API-рждрзЗ рж╕ржВржпрзБржХрзНржд рж╣ржпрж╝ред

### ржзрж╛ржк рзз: ржкрзВрж░рзНржмрж╢рж░рзНржд

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### ржзрж╛ржк рзи: Docker рж╕рзЗржЯржЖржк (ржкрзНрж░рж╕рзНрждрж╛ржмрж┐ржд)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**ржирзЛржЯ:** `host.docker.internal` Windows-ржП Docker ржХржирзНржЯрзЗржЗржирж╛рж░ржЧрзБрж▓рзЛржХрзЗ рж╣рзЛрж╕рзНржЯ ржорзЗрж╢рж┐ржирзЗ ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕ ржХрж░рждрзЗ ржжрзЗржпрж╝ред

### ржзрж╛ржк рзй: ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи

1. **ржмрзНрж░рж╛ржЙржЬрж╛рж░ ржЦрзБрж▓рзБржи:** `http://localhost:3000` ржП ржпрж╛ржи
2. **ржкрзНрж░рж╛ржержорж┐ржХ рж╕рзЗржЯржЖржк:** ржЕрзНржпрж╛ржбржорж┐ржи ржЕрзНржпрж╛ржХрж╛ржЙржирзНржЯ рждрзИрж░рж┐ ржХрж░рзБржи
3. **ржоржбрзЗрж▓ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи:**
   - Settings тЖТ Models тЖТ OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (ржпрзЗржХрзЛржирзЛ ржорж╛ржи ржХрж╛ржЬ ржХрж░ржмрзЗ)
4. **рж╕ржВржпрзЛржЧ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи:** ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржбрзНрж░ржкржбрж╛ржЙржи-ржП ржкрзНрж░ржжрж░рзНрж╢рж┐ржд рж╣ржУржпрж╝рж╛ ржЙржЪрж┐ржд

### рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи

**рж╕рж╛ржзрж╛рж░ржг рж╕ржорж╕рзНржпрж╛:**

1. **рж╕ржВржпрзЛржЧ ржкрзНрж░рждрзНржпрж╛ржЦрзНржпрж╛ржи:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржкрзНрж░ржжрж░рзНрж╢рж┐ржд рж╣ржЪрзНржЫрзЗ ржирж╛:**
   - ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи ржоржбрзЗрж▓ рж▓рзЛржб рж╣ржпрж╝рзЗржЫрзЗ: `foundry model list`
   - API рж░рзЗрж╕ржкржирзНрж╕ ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи: `curl http://localhost:51211/v1/models`
   - Open WebUI ржХржирзНржЯрзЗржЗржирж╛рж░ ржкрзБржирж░рж╛ржпрж╝ ржЪрж╛рж▓рзБ ржХрж░рзБржи

## ржЕржВрж╢ рзн: ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржорзЛрждрж╛ржпрж╝рзЗржи ржмрж┐ржмрзЗржЪржирж╛

### ржкрж░рж┐ржмрзЗрж╢ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи

**ржЙржирзНржиржпрж╝ржи рж╕рзЗржЯржЖржк:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржорзЛрждрж╛ржпрж╝рзЗржи:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### рж╕рж╛ржзрж╛рж░ржг ржкрзЛрж░рзНржЯ рж╕ржорж╕рзНржпрж╛ ржПржмржВ рж╕ржорж╛ржзрж╛ржи

**ржкрзЛрж░рзНржЯ рзлрззрзирззрзз рж╕ржВржШрж░рзНрж╖ ржкрзНрж░рждрж┐рж░рзЛржз:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржкрж░рзНржпржмрзЗржХрзНрж╖ржг

**рж╣рзЗрж▓рже ржЪрзЗржХ ржмрж╛рж╕рзНрждржмрж╛ржпрж╝ржи:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## рж╕рж╛рж░рж╕ржВржХрзНрж╖рзЗржк

рж╕рзЗрж╢ржи рзк Chainlit ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржХржерзЛржкржХржержи AI-ржПрж░ ржЬржирзНржп ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рждрзИрж░рж┐ ржХрж░рж╛рж░ ржЙржкрж░ ржЖрж▓рзЛржХржкрж╛ржд ржХрж░рзЗржЫрзЗред ржЖржкржирж┐ рж╢рж┐ржЦрзЗржЫрзЗржи:

- тЬЕ **Chainlit Framework**: ржЪрзНржпрж╛ржЯ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржЖржзрзБржирж┐ржХ UI ржПржмржВ рж╕рзНржЯрзНрж░рж┐ржорж┐ржВ рж╕ржорж░рзНржержи
- тЬЕ **Foundry Local ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи**: SDK ржмрзНржпржмрж╣рж╛рж░ ржПржмржВ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржкрзНржпрж╛ржЯрж╛рж░рзНржи  
- тЬЕ **WebGPU ржЗржиржлрж╛рж░рзЗржирзНрж╕**: рж╕рж░рзНржмрж╛ржзрж┐ржХ ржЧрзЛржкржирзАржпрж╝рждрж╛рж░ ржЬржирзНржп ржмрзНрж░рж╛ржЙржЬрж╛рж░-ржнрж┐рждрзНрждрж┐ржХ AI
- тЬЕ **Open WebUI рж╕рзЗржЯржЖржк**: ржкрзЗрж╢рж╛ржжрж╛рж░ ржЪрзНржпрж╛ржЯ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ ржорзЛрждрж╛ржпрж╝рзЗржи
- тЬЕ **ржкрзНрж░рзЛржбрж╛ржХрж╢ржи ржкрзНржпрж╛ржЯрж╛рж░рзНржи**: рждрзНрж░рзБржЯрж┐ ржкрж░рж┐ржЪрж╛рж▓ржирж╛, ржкрж░рзНржпржмрзЗржХрзНрж╖ржг ржПржмржВ рж╕рзНржХрзЗрж▓рж┐ржВ

ржиржорзБржирж╛ рзжрзк ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи Microsoft Foundry Local-ржПрж░ ржорж╛ржзрзНржпржорзЗ рж╕рзНржерж╛ржирзАржпрж╝ AI ржоржбрзЗрж▓ржЧрзБрж▓рзЛржХрзЗ ржХрж╛ржЬрзЗ рж▓рж╛ржЧрж┐ржпрж╝рзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржЪрзНржпрж╛ржЯ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ рждрзИрж░рж┐рж░ рж╕рзЗрж░рж╛ ржЕржирзБрж╢рзАрж▓ржи ржкрзНрж░ржжрж░рзНрж╢ржи ржХрж░рзЗ, ржпрж╛ ржЪржорзОржХрж╛рж░ ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред

## рж░рзЗржлрж╛рж░рзЗржирзНрж╕

- **[ржиржорзБржирж╛ рзжрзк: Chainlit ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи](samples/04/README.md)**: рж╕ржорзНржкрзВрж░рзНржг ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рж╕рж╣ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи
- **[Chainlit рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржирзЛржЯржмрзБржХ](samples/04/chainlit_app.ipynb)**: ржЗржирзНржЯрж╛рж░рзЗржХрзНржЯрж┐ржн рж╢рзЗржЦрж╛рж░ ржЙржкржХрж░ржг
- **[Foundry Local ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: рж╕ржорзНржкрзВрж░рзНржг ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи
- **[Chainlit ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи](https://docs.chainlit.io/)**: ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ ржлрзНрж░рзЗржоржУржпрж╝рж╛рж░рзНржХ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи
- **[Open WebUI ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи ржЧрж╛ржЗржб](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: ржЕржлрж┐рж╕рж┐ржпрж╝рж╛рж▓ ржЯрж┐ржЙржЯрзЛрж░рж┐ржпрж╝рж╛рж▓

---

**ржЕрж╕рзНржмрзАржХрзГрждрж┐**:  
ржПржЗ ржиржерж┐ржЯрж┐ AI ржЕржирзБржмрж╛ржж ржкрж░рж┐рж╖рзЗржмрж╛ [Co-op Translator](https://github.com/Azure/co-op-translator) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЕржирзБржмрж╛ржж ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЖржорж░рж╛ ржпржерж╛рж╕рж╛ржзрзНржп рж╕ржарж┐ржХрждрж╛рж░ ржЬржирзНржп ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж┐, рждржмрзЗ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржоржирзЗ рж░рж╛ржЦржмрзЗржи ржпрзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ ржЕржирзБржмрж╛ржжрзЗ рждрзНрж░рзБржЯрж┐ ржмрж╛ ржЕрж╕ржЩрзНржЧрждрж┐ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗред ржорзВрж▓ ржнрж╛рж╖рж╛ржпрж╝ ржерж╛ржХрж╛ ржиржерж┐ржЯрж┐ржХрзЗ ржкрзНрж░рж╛ржорж╛ржгрж┐ржХ ржЙрзОрж╕ рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ ржЙржЪрж┐рждред ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рждржерзНржпрзЗрж░ ржЬржирзНржп, ржкрзЗрж╢рж╛ржжрж╛рж░ ржорж╛ржиржм ржЕржирзБржмрж╛ржж рж╕рзБржкрж╛рж░рж┐рж╢ ржХрж░рж╛ рж╣ржпрж╝ред ржПржЗ ржЕржирзБржмрж╛ржж ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржлрж▓рзЗ ржХрзЛржирзЛ ржнрзБрж▓ ржмрзЛржЭрж╛ржмрзБржЭрж┐ ржмрж╛ ржнрзБрж▓ ржмрзНржпрж╛ржЦрзНржпрж╛ рж╣рж▓рзЗ ржЖржорж░рж╛ ржжрж╛ржпрж╝ржмржжрзНржз ржерж╛ржХржм ржирж╛ред