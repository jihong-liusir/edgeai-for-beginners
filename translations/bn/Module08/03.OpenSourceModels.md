<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T17:42:41+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "bn"
}
-->
# সেশন ৩: ফাউন্ড্রি লোকাল দিয়ে ওপেন-সোর্স মডেল

## সংক্ষিপ্ত বিবরণ

এই সেশনে ফাউন্ড্রি লোকালে ওপেন-সোর্স মডেল আনার পদ্ধতি আলোচনা করা হয়েছে: কমিউনিটি মডেল নির্বাচন, Hugging Face কন্টেন্ট ইন্টিগ্রেশন, এবং "নিজস্ব মডেল আনুন" (BYOM) কৌশল গ্রহণ। এছাড়াও, আপনি Model Mondays সিরিজ সম্পর্কে জানবেন যা ধারাবাহিক শিক্ষার এবং মডেল আবিষ্কারের জন্য সহায়ক।

রেফারেন্স:
- ফাউন্ড্রি লোকাল ডকস: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face মডেল কম্পাইল করুন: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- ফাউন্ড্রি লোকাল GitHub: https://github.com/microsoft/Foundry-Local

## শেখার লক্ষ্য
- লোকাল ইনফারেন্সের জন্য ওপেন-সোর্স মডেল আবিষ্কার এবং মূল্যায়ন করা
- ফাউন্ড্রি লোকালে নির্দিষ্ট Hugging Face মডেল কম্পাইল এবং চালানো
- সঠিকতা, লেটেন্সি এবং রিসোর্স প্রয়োজনীয়তার জন্য মডেল নির্বাচন কৌশল প্রয়োগ করা
- ক্যাশ এবং ভার্সনিং দিয়ে লোকাল মডেল পরিচালনা করা

## অংশ ১: মডেল আবিষ্কার এবং নির্বাচন (ধাপে ধাপে)

ধাপ ১) লোকাল ক্যাটালগে উপলব্ধ মডেলগুলোর তালিকা তৈরি করুন  
```cmd
foundry model list
```
  
ধাপ ২) দুটি প্রার্থী মডেল দ্রুত চেষ্টা করুন (প্রথমবার চালানোর সময় স্বয়ংক্রিয়ভাবে ডাউনলোড হবে)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
ধাপ ৩) মৌলিক মেট্রিক্স নোট করুন  
- একটি নির্দিষ্ট প্রম্পটের জন্য লেটেন্সি (সাবজেক্টিভ) এবং গুণমান পর্যবেক্ষণ করুন  
- প্রতিটি মডেল চালানোর সময় Task Manager দিয়ে মেমরি ব্যবহার দেখুন  

## অংশ ২: CLI দিয়ে ক্যাটালগ মডেল চালানো (ধাপে ধাপে)

ধাপ ১) একটি মডেল শুরু করুন  
```cmd
foundry model run llama-3.2
```
  
ধাপ ২) OpenAI-সামঞ্জস্যপূর্ণ এন্ডপয়েন্টে একটি টেস্ট প্রম্পট পাঠান  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## অংশ ৩: BYOM – Hugging Face মডেল কম্পাইল করা (ধাপে ধাপে)

মডেল কম্পাইল করার জন্য অফিসিয়াল নির্দেশিকা অনুসরণ করুন। নিচে উচ্চ-স্তরের ধারা দেওয়া হয়েছে—Microsoft Learn আর্টিকেলে সঠিক কমান্ড এবং সমর্থিত কনফিগারেশন দেখুন।

ধাপ ১) একটি কাজের ডিরেক্টরি প্রস্তুত করুন  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
ধাপ ২) একটি সমর্থিত HF মডেল কম্পাইল করুন  
- Learn ডক থেকে ধাপগুলো ব্যবহার করে ONNX মডেল কনভার্ট করুন এবং আপনার `models` ডিরেক্টরিতে রাখুন  
- নিশ্চিত করুন:  
```cmd
foundry cache ls
```
  
আপনার কম্পাইল করা মডেলের নাম দেখতে পাবেন (যেমন, `llama-3.2`)।  

ধাপ ৩) কম্পাইল করা মডেল চালান  
```cmd
foundry model run llama-3.2 --verbose
```
  
নোট:  
- কম্পাইল এবং চালানোর জন্য পর্যাপ্ত ডিস্ক এবং RAM নিশ্চিত করুন  
- ফ্লো যাচাই করার জন্য ছোট মডেল দিয়ে শুরু করুন, তারপর বড় মডেলে স্কেল করুন  

## অংশ ৪: ব্যবহারিক মডেল কিউরেশন (ধাপে ধাপে)

ধাপ ১) একটি `models.json` রেজিস্ট্রি তৈরি করুন  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
ধাপ ২) ছোট সিলেক্টর স্ক্রিপ্ট  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## অংশ ৫: হাতে-কলমে বেঞ্চমার্ক (ধাপে ধাপে)

ধাপ ১) সাধারণ লেটেন্সি বেঞ্চমার্ক  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
ধাপ ২) গুণমান স্পট-চেক  
- একটি নির্দিষ্ট প্রম্পট সেট ব্যবহার করুন, আউটপুটগুলো CSV/JSON-এ ক্যাপচার করুন  
- ফ্লুয়েন্সি, প্রাসঙ্গিকতা, এবং সঠিকতা (১–৫) ম্যানুয়ালি রেট করুন  

## অংশ ৬: পরবর্তী পদক্ষেপ
- নতুন মডেল এবং টিপসের জন্য Model Mondays সাবস্ক্রাইব করুন: https://aka.ms/model-mondays  
- আপনার টিমের `models.json`-এ আবিষ্কারগুলো যোগ করুন  
- সেশন ৪-এর জন্য প্রস্তুতি নিন: LLMs বনাম SLMs, লোকাল বনাম ক্লাউড ইনফারেন্স, এবং হাতে-কলমে ডেমো তুলনা

---

