<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T20:32:13+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "bn"
}
-->
# অধ্যায় ০৩ - মডেল কনটেক্সট প্রোটোকল (MCP) ইন্টিগ্রেশন

## MCP (মডেল কনটেক্সট প্রোটোকল) পরিচিতি

মডেল কনটেক্সট প্রোটোকল (MCP) একটি বিপ্লবী কাঠামো যা ভাষার মডেলগুলিকে বাইরের টুল এবং সিস্টেমের সাথে একটি মানসম্মত উপায়ে যোগাযোগ করতে সক্ষম করে। প্রচলিত পদ্ধতিতে যেখানে মডেলগুলো বিচ্ছিন্ন থাকে, MCP একটি সুসংজ্ঞায়িত প্রোটোকলের মাধ্যমে AI মডেল এবং বাস্তব জগতের মধ্যে সেতুবন্ধন তৈরি করে।

### MCP কী?

MCP একটি যোগাযোগ প্রোটোকল হিসেবে কাজ করে যা ভাষার মডেলগুলোকে সক্ষম করে:
- বাইরের ডেটা সোর্সের সাথে সংযোগ স্থাপন
- টুল এবং ফাংশন কার্যকর করা
- API এবং সার্ভিসের সাথে যোগাযোগ
- রিয়েল-টাইম তথ্য অ্যাক্সেস
- জটিল বহু-ধাপের কার্যক্রম সম্পাদন

এই প্রোটোকল স্থির ভাষার মডেলগুলোকে গতিশীল এজেন্টে রূপান্তরিত করে, যা শুধুমাত্র টেক্সট জেনারেশনের বাইরে ব্যবহারিক কাজ সম্পাদন করতে সক্ষম।

## ছোট ভাষার মডেল (SLMs) MCP-তে

ছোট ভাষার মডেল (SLMs) AI মোতায়েনের একটি দক্ষ পদ্ধতি উপস্থাপন করে, যা বিভিন্ন সুবিধা প্রদান করে:

### SLMs-এর সুবিধা
- **সম্পদ দক্ষতা**: কম কম্পিউটেশনাল প্রয়োজনীয়তা
- **দ্রুত প্রতিক্রিয়া সময়**: রিয়েল-টাইম অ্যাপ্লিকেশনের জন্য কম লেটেন্সি  
- **খরচ সাশ্রয়ী**: ন্যূনতম অবকাঠামো প্রয়োজন
- **গোপনীয়তা**: স্থানীয়ভাবে চালানো যায়, ডেটা স্থানান্তর ছাড়াই
- **কাস্টমাইজেশন**: নির্দিষ্ট ডোমেইনের জন্য সহজে ফাইন-টিউন করা যায়

### MCP-এর সাথে SLMs কেন ভালো কাজ করে

SLMs এবং MCP একত্রে একটি শক্তিশালী সংমিশ্রণ তৈরি করে যেখানে মডেলের যুক্তি করার ক্ষমতা বাইরের টুল দ্বারা বৃদ্ধি পায়, তাদের ছোট প্যারামিটার সংখ্যা সত্ত্বেও উন্নত কার্যকারিতা প্রদান করে।

## পাইথন MCP SDK-এর সংক্ষিপ্ত বিবরণ

পাইথন MCP SDK MCP-সক্ষম অ্যাপ্লিকেশন তৈরি করার জন্য ভিত্তি প্রদান করে। SDK অন্তর্ভুক্ত করে:

- **ক্লায়েন্ট লাইব্রেরি**: MCP সার্ভারের সাথে সংযোগ স্থাপনের জন্য
- **সার্ভার ফ্রেমওয়ার্ক**: কাস্টম MCP সার্ভার তৈরি করার জন্য
- **প্রোটোকল হ্যান্ডলার**: যোগাযোগ পরিচালনার জন্য
- **টুল ইন্টিগ্রেশন**: বাইরের ফাংশন কার্যকর করার জন্য

## বাস্তবায়ন: Phi-4 MCP ক্লায়েন্ট

Microsoft-এর Phi-4 মিনি মডেল MCP ক্ষমতার সাথে সংযুক্ত করে একটি বাস্তব উদাহরণ অন্বেষণ করা যাক।

### সিস্টেম আর্কিটেকচার

বাস্তবায়ন একটি স্তরযুক্ত আর্কিটেকচার অনুসরণ করে:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### মূল উপাদান

#### ১. MCP ক্লায়েন্ট ক্লাস

**BaseMCPClient**: সাধারণ কার্যকারিতা প্রদানকারী বিমূর্ত ভিত্তি
- অ্যাসিঙ্ক কনটেক্সট ম্যানেজার প্রোটোকল
- মানক ইন্টারফেস সংজ্ঞা
- সম্পদ ব্যবস্থাপনা

**Phi4MiniMCPClient**: STDIO-ভিত্তিক বাস্তবায়ন
- স্থানীয় প্রক্রিয়া যোগাযোগ
- স্ট্যান্ডার্ড ইনপুট/আউটপুট পরিচালনা
- সাবপ্রসেস ব্যবস্থাপনা

**Phi4MiniSSEMCPClient**: সার্ভার-সেন্ট ইভেন্ট বাস্তবায়ন
- HTTP স্ট্রিমিং যোগাযোগ
- রিয়েল-টাইম ইভেন্ট পরিচালনা
- ওয়েব-ভিত্তিক সার্ভার সংযোগ

#### ২. LLM ইন্টিগ্রেশন

**OllamaClient**: স্থানীয় মডেল হোস্টিং
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: উচ্চ-প্রদর্শন পরিবেশন
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### ৩. টুল প্রসেসিং পাইপলাইন

টুল প্রসেসিং পাইপলাইন MCP টুলগুলোকে ভাষার মডেলের সাথে সামঞ্জস্যপূর্ণ ফরম্যাটে রূপান্তরিত করে:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## শুরু করা: ধাপে ধাপে নির্দেশিকা

### ধাপ ১: পরিবেশ সেটআপ

প্রয়োজনীয় ডিপেনডেন্সি ইনস্টল করুন:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### ধাপ ২: মৌলিক কনফিগারেশন

আপনার পরিবেশ ভেরিয়েবল সেট করুন:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### ধাপ ৩: আপনার প্রথম MCP ক্লায়েন্ট চালানো

**মৌলিক Ollama সেটআপ:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM ব্যাকএন্ড ব্যবহার:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**সার্ভার-সেন্ট ইভেন্ট সংযোগ:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**কাস্টম MCP সার্ভার:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### ধাপ ৪: প্রোগ্রাম্যাটিক ব্যবহার

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## উন্নত বৈশিষ্ট্য

### মাল্টি-ব্যাকএন্ড সমর্থন

বাস্তবায়ন Ollama এবং vLLM উভয় ব্যাকএন্ড সমর্থন করে, যা আপনার প্রয়োজন অনুযায়ী নির্বাচন করার সুযোগ দেয়:

- **Ollama**: স্থানীয় উন্নয়ন এবং পরীক্ষার জন্য ভালো
- **vLLM**: প্রোডাকশন এবং উচ্চ-থ্রুপুট পরিস্থিতির জন্য অপ্টিমাইজড

### নমনীয় সংযোগ প্রোটোকল

দুটি সংযোগ মোড সমর্থিত:

**STDIO মোড**: সরাসরি প্রক্রিয়া যোগাযোগ
- কম লেটেন্সি
- স্থানীয় টুলের জন্য উপযুক্ত
- সহজ সেটআপ

**SSE মোড**: HTTP-ভিত্তিক স্ট্রিমিং
- নেটওয়ার্ক-সক্ষম
- বিতরণকৃত সিস্টেমের জন্য ভালো
- রিয়েল-টাইম আপডেট

### টুল ইন্টিগ্রেশন ক্ষমতা

সিস্টেম বিভিন্ন টুলের সাথে ইন্টিগ্রেট করতে পারে:
- ওয়েব অটোমেশন (Playwright)
- ফাইল অপারেশন
- API ইন্টারঅ্যাকশন
- সিস্টেম কমান্ড
- কাস্টম ফাংশন

## ত্রুটি পরিচালনা এবং সেরা অনুশীলন

### বিস্তৃত ত্রুটি ব্যবস্থাপনা

বাস্তবায়ন শক্তিশালী ত্রুটি পরিচালনা অন্তর্ভুক্ত করে:

**সংযোগ ত্রুটি:**
- MCP সার্ভার ব্যর্থতা
- নেটওয়ার্ক টাইমআউট
- সংযোগ সমস্যা

**টুল কার্যকর ত্রুটি:**
- অনুপস্থিত টুল
- প্যারামিটার যাচাই
- কার্যকর ব্যর্থতা

**প্রতিক্রিয়া প্রসেসিং ত্রুটি:**
- JSON পার্সিং সমস্যা
- ফরম্যাট অসঙ্গতি
- LLM প্রতিক্রিয়া অস্বাভাবিকতা

### সেরা অনুশীলন

1. **সম্পদ ব্যবস্থাপনা**: অ্যাসিঙ্ক কনটেক্সট ম্যানেজার ব্যবহার করুন
2. **ত্রুটি পরিচালনা**: বিস্তৃত try-catch ব্লক বাস্তবায়ন করুন
3. **লগিং**: উপযুক্ত লগিং লেভেল সক্রিয় করুন
4. **নিরাপত্তা**: ইনপুট যাচাই এবং আউটপুট স্যানিটাইজ করুন
5. **পারফরম্যান্স**: সংযোগ পুলিং এবং ক্যাশিং ব্যবহার করুন

## বাস্তব জীবনের প্রয়োগ

### ওয়েব অটোমেশন
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### ডেটা প্রসেসিং
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API ইন্টিগ্রেশন
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## পারফরম্যান্স অপ্টিমাইজেশন

### মেমরি ব্যবস্থাপনা
- দক্ষ বার্তা ইতিহাস পরিচালনা
- যথাযথ সম্পদ পরিষ্কার
- সংযোগ পুলিং

### নেটওয়ার্ক অপ্টিমাইজেশন
- অ্যাসিঙ্ক HTTP অপারেশন
- কনফিগারযোগ্য টাইমআউট
- গ্রেসফুল ত্রুটি পুনরুদ্ধার

### একযোগে প্রসেসিং
- নন-ব্লকিং I/O
- সমান্তরাল টুল কার্যকর
- দক্ষ অ্যাসিঙ্ক প্যাটার্ন

## নিরাপত্তা বিবেচনা

### ডেটা সুরক্ষা
- নিরাপদ API কী ব্যবস্থাপনা
- ইনপুট যাচাই
- আউটপুট স্যানিটাইজেশন

### নেটওয়ার্ক নিরাপত্তা
- HTTPS সমর্থন
- স্থানীয় এন্ডপয়েন্ট ডিফল্ট
- নিরাপদ টোকেন পরিচালনা

### কার্যকর নিরাপত্তা
- টুল ফিল্টারিং
- স্যান্ডবক্সড পরিবেশ
- অডিট লগিং

## উপসংহার

MCP-এর সাথে SLMs সংযুক্তি AI অ্যাপ্লিকেশন উন্নয়নে একটি নতুন দৃষ্টিভঙ্গি উপস্থাপন করে। ছোট মডেলের দক্ষতা এবং বাইরের টুলের শক্তি একত্রিত করে, ডেভেলপাররা এমন বুদ্ধিমান সিস্টেম তৈরি করতে পারেন যা সম্পদ-দক্ষ এবং অত্যন্ত সক্ষম।

Phi-4 MCP ক্লায়েন্ট বাস্তবায়ন দেখায় কীভাবে এই সংযুক্তি বাস্তবে অর্জন করা যায়, উন্নত AI-চালিত অ্যাপ্লিকেশন তৈরির জন্য একটি শক্ত ভিত্তি প্রদান করে।

মূল বিষয়গুলো:
- MCP ভাষার মডেল এবং বাইরের সিস্টেমের মধ্যে সেতুবন্ধন তৈরি করে
- SLMs টুল দ্বারা বৃদ্ধি পেলে দক্ষতা ছাড়াই সক্ষমতা প্রদান করে
- মডুলার আর্কিটেকচার সহজ সম্প্রসারণ এবং কাস্টমাইজেশন সক্ষম করে
- প্রোডাকশন ব্যবহারের জন্য যথাযথ ত্রুটি পরিচালনা এবং নিরাপত্তা ব্যবস্থা অপরিহার্য

এই টিউটোরিয়ালটি আপনার নিজস্ব SLM-চালিত MCP অ্যাপ্লিকেশন তৈরি করার জন্য ভিত্তি প্রদান করে, অটোমেশন, ডেটা প্রসেসিং এবং বুদ্ধিমান সিস্টেম ইন্টিগ্রেশনের সম্ভাবনা উন্মোচন করে।

---

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় লেখা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ ব্যবহার করার পরামর্শ দেওয়া হচ্ছে। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।  