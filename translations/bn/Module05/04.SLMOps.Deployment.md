<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T21:21:36+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "bn"
}
-->
# Section 4: Deployment - Production-Ready Model Implementation

## Overview

ржПржЗ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржЯрж┐ржЙржЯрзЛрж░рж┐ржпрж╝рж╛рж▓ржЯрж┐ ржЖржкржирж╛ржХрзЗ Foundry Local ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬржб ржоржбрзЗрж▓ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯрзЗрж░ рж╕ржорзНржкрзВрж░рзНржг ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╢рзЗржЦрж╛ржмрзЗред ржЖржорж░рж╛ ржоржбрзЗрж▓ ржХржиржнрж╛рж░рзНрж╕ржи, ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи ржПржмржВ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи рж╢рзБрж░рзБ ржерзЗржХрзЗ рж╢рзЗрж╖ ржкрж░рзНржпржирзНржд ржХржнрж╛рж░ ржХрж░ржмред

## Prerequisites

рж╢рзБрж░рзБ ржХрж░рж╛рж░ ржЖржЧрзЗ ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзБржи ржпрзЗ ржЖржкржирж╛рж░ ржХрж╛ржЫрзЗ ржирж┐ржорзНржирж▓рж┐ржЦрж┐рждржЧрзБрж▓рж┐ рж░ржпрж╝рзЗржЫрзЗ:

- тЬЕ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп ржкрзНрж░рж╕рзНрждрзБржд ржПржХржЯрж┐ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ onnx ржоржбрзЗрж▓
- тЬЕ Windows ржмрж╛ Mac ржХржорзНржкрж┐ржЙржЯрж╛рж░
- тЬЕ Python 3.10 ржмрж╛ рждрж╛рж░ ржмрзЗрж╢рж┐
- тЬЕ ржХржоржкржХрзНрж╖рзЗ 8GB ржлрзНрж░рж┐ RAM
- тЬЕ ржЖржкржирж╛рж░ рж╕рж┐рж╕рзНржЯрзЗржорзЗ Foundry Local ржЗржирж╕рзНржЯрж▓ ржХрж░рж╛ ржЖржЫрзЗ

## Part 1: Environment Setup

### Installing Required Tools

ржЖржкржирж╛рж░ ржЯрж╛рж░рзНржорж┐ржирж╛рж▓ (Windows-ржП Command Prompt, Mac-ржП Terminal) ржЦрзБрж▓рзБржи ржПржмржВ ржирж┐ржЪрзЗрж░ ржХржорж╛ржирзНржбржЧрзБрж▓рзЛ ржХрзНрж░ржорж╛ржирзБрж╕рж╛рж░рзЗ ржЪрж╛рж▓рж╛ржи:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

тЪая╕П **ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржирзЛржЯ**: ржЖржкржирж╛ржХрзЗ CMake рж╕ржВрж╕рзНржХрж░ржг 3.31 ржмрж╛ рждрж╛рж░ ржирждрзБржирждрж░ рж╕ржВрж╕рзНржХрж░ржгржУ ржкрзНрж░ржпрж╝рзЛржЬржи рж╣ржмрзЗ, ржпрж╛ [cmake.org](https://cmake.org/download/) ржерзЗржХрзЗ ржбрж╛ржЙржирж▓рзЛржб ржХрж░рж╛ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред

## Part 2: Model Conversion and Quantization

### Choosing the Right Format

ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржЫрзЛржЯ ржнрж╛рж╖рж╛рж░ ржоржбрзЗрж▓рзЗрж░ ржЬржирзНржп, ржЖржорж░рж╛ **ONNX ржлрж░ржорзНржпрж╛ржЯ** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛рж░ рж╕рзБржкрж╛рж░рж┐рж╢ ржХрж░рж┐ ржХрж╛рж░ржг ржПржЯрж┐ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ:

- ЁЯЪА ржЙржирзНржиржд ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи
- ЁЯФз рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░-ржЕрзНржпрж╛ржЧржирж╕рзНржЯрж┐ржХ ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ
- ЁЯПн ржкрзНрж░рзЛржбрж╛ржХрж╢ржи-рж░рзЗржбрж┐ рж╕ржХрзНрж╖ржорждрж╛
- ЁЯУ▒ ржХрзНрж░рж╕-ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо рж╕рж╛ржоржЮрзНржЬрж╕рзНржпрждрж╛

### Method 1: One-Command Conversion (Recommended)

ржЖржкржирж╛рж░ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржоржбрзЗрж▓ рж╕рж░рж╛рж╕рж░рж┐ ржХржиржнрж╛рж░рзНржЯ ржХрж░рждрзЗ ржирж┐ржЪрзЗрж░ ржХржорж╛ржирзНржбржЯрж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ ржмрзНржпрж╛ржЦрзНржпрж╛:**
- `--model_name_or_path`: ржЖржкржирж╛рж░ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржоржбрзЗрж▓рзЗрж░ ржкрж╛рже
- `--device cpu`: ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржирзЗрж░ ржЬржирзНржп CPU ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- `--precision int4`: INT4 ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи (ржкрзНрж░рж╛ржпрж╝ 75% рж╕рж╛ржЗржЬ ржХржорж╛ржирзЛ)
- `--output_path`: ржХржиржнрж╛рж░рзНржЯ ржХрж░рж╛ ржоржбрзЗрж▓рзЗрж░ ржЖржЙржЯржкрзБржЯ ржкрж╛рже

### Method 2: Configuration File Approach (Advanced Users)

`finetuned_conversion_config.json` ржирж╛ржорзЗ ржПржХржЯрж┐ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржлрж╛ржЗрж▓ рждрзИрж░рж┐ ржХрж░рзБржи:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

рждрж╛рж░ржкрж░ ржЪрж╛рж▓рж╛ржи:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Quantization Options Comparison

| Precision | File Size | Inference Speed | Model Quality | Recommended Use |
|-----------|-----------|-----------------|---------------|-----------------|
| FP16      | Baseline ├Ч 0.5 | ржжрзНрж░рзБржд | рж╕рзЗрж░рж╛ | ржЙржЪрзНржЪ-ржорж╛ржирзЗрж░ рж╣рж╛рж░рзНржбржУржпрж╝рзНржпрж╛рж░ |
| INT8      | Baseline ├Ч 0.25 | ржЦрзБржм ржжрзНрж░рзБржд | ржнрж╛рж▓рзЛ | ржмрзНржпрж╛рж▓рзЗржирзНрж╕ржб ржкржЫржирзНржж |
| INT4      | Baseline ├Ч 0.125 | ржжрзНрж░рзБрждрждржо | ржЧрзНрж░рж╣ржгржпрзЛржЧрзНржп | рж╕рзАржорж┐ржд рж░рж┐рж╕рзЛрж░рзНрж╕ |

ЁЯТб **рж╕рзБржкрж╛рж░рж┐рж╢**: ржЖржкржирж╛рж░ ржкрзНрж░ржержо ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯрзЗрж░ ржЬржирзНржп INT4 ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ ржХрж░рзБржиред ржпржжрж┐ ржорж╛ржи рж╕ржирзНрждрзЛрж╖ржЬржиржХ ржирж╛ рж╣ржпрж╝, рждрж╛рж╣рж▓рзЗ INT8 ржмрж╛ FP16 ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред

## Part 3: Foundry Local Deployment Configuration

### Creating Model Configuration

Foundry Local ржоржбрзЗрж▓ ржбрж┐рж░рзЗржХрзНржЯрж░рж┐рждрзЗ ржпрж╛ржи:

```bash
foundry cache cd ./models/
```

ржЖржкржирж╛рж░ ржоржбрзЗрж▓ ржбрж┐рж░рзЗржХрзНржЯрж░рж┐ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░ рждрзИрж░рж┐ ржХрж░рзБржи:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

ржЖржкржирж╛рж░ ржоржбрзЗрж▓ ржбрж┐рж░рзЗржХрзНржЯрж░рж┐рждрзЗ `inference_model.json` ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржлрж╛ржЗрж▓ рждрзИрж░рж┐ ржХрж░рзБржи:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Model-Specific Template Configurations

#### For Qwen Series Models:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Part 4: Model Testing and Optimization

### Verifying Model Installation

ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи Foundry Local ржЖржкржирж╛рж░ ржоржбрзЗрж▓ржЯрж┐ ржЪрж┐ржирждрзЗ ржкрж╛рж░рзЗ ржХрж┐ржирж╛:

```bash
foundry cache ls
```

ржЖржкржирж╛рж░ рждрж╛рж▓рж┐ржХрж╛ржпрж╝ `your-finetuned-model-int4` ржжрзЗржЦрждрзЗ ржкрж╛ржУржпрж╝рж╛ ржЙржЪрж┐рждред

### Starting Model Testing

```bash
foundry model run your-finetuned-model-int4
```

### Performance Benchmarking

ржкрж░рзАржХрзНрж╖рж╛рж░ рж╕ржоржпрж╝ ржкрзНрж░ржзрж╛ржи ржорзЗржЯрзНрж░рж┐ржХржЧрзБрж▓рзЛ ржкрж░рзНржпржмрзЗржХрзНрж╖ржг ржХрж░рзБржи:

1. **Response Time**: ржкрзНрж░рждрж┐ржЯрж┐ рж░рзЗрж╕ржкржирзНрж╕рзЗрж░ ржЧржбрж╝ рж╕ржоржпрж╝ ржкрж░рж┐ржорж╛ржк ржХрж░рзБржи
2. **Memory Usage**: RAM ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржкрж░рзНржпржмрзЗржХрзНрж╖ржг ржХрж░рзБржи
3. **CPU Utilization**: ржкрзНрж░рж╕рзЗрж╕рж░рзЗрж░ рж▓рзЛржб ржкрж░рзАржХрзНрж╖рж╛ ржХрж░рзБржи
4. **Output Quality**: рж░рзЗрж╕ржкржирзНрж╕рзЗрж░ ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХрждрж╛ ржПржмржВ рж╕рж╛ржоржЮрзНржЬрж╕рзНржп ржорзВрж▓рзНржпрж╛ржпрж╝ржи ржХрж░рзБржи

### Quality Validation Checklist

- тЬЕ ржоржбрзЗрж▓ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржбрзЛржорзЗржЗржи ржкрзНрж░рж╢рзНржирзЗрж░ рж╕ржарж┐ржХржнрж╛ржмрзЗ ржЙрждрзНрждрж░ ржжрзЗржпрж╝
- тЬЕ рж░рзЗрж╕ржкржирзНрж╕ ржлрж░ржорзНржпрж╛ржЯ ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржЖржЙржЯржкрзБржЯ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░рзЗрж░ рж╕рж╛ржерзЗ ржорж┐рж▓рзЗ ржпрж╛ржпрж╝
- тЬЕ ржжрзАрж░рзНржШ рж╕ржоржпрж╝ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ рж╕ржоржпрж╝ ржХрзЛржирзЛ ржорзЗржорзЛрж░рж┐ рж▓рж┐ржХ рж╣ржпрж╝ ржирж╛
- тЬЕ ржмрж┐ржнрж┐ржирзНржи ржЗржиржкрзБржЯ ржжрзИрж░рзНржШрзНржпрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕
- тЬЕ ржПржЬ ржХрзЗрж╕ ржПржмржВ ржЕржХрж╛рж░рзНржпржХрж░ ржЗржиржкрзБржЯ рж╕ржарж┐ржХржнрж╛ржмрзЗ ржкрж░рж┐ржЪрж╛рж▓ржирж╛ ржХрж░рзЗ

## Summary

ржЕржнрж┐ржиржирзНржжржи! ржЖржкржирж┐ рж╕ржлрж▓ржнрж╛ржмрзЗ рж╕ржорзНржкржирзНржи ржХрж░рзЗржЫрзЗржи:

- тЬЕ ржлрж╛ржЗржи-ржЯрж┐ржЙржи ржХрж░рж╛ ржоржбрзЗрж▓рзЗрж░ ржлрж░ржорзНржпрж╛ржЯ ржХржиржнрж╛рж░рзНрж╕ржи
- тЬЕ ржоржбрзЗрж▓ ржХрзЛржпрж╝рж╛ржирзНржЯрж╛ржЗржЬрзЗрж╢ржи ржЕржкрзНржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи
- тЬЕ Foundry Local ржбрж┐ржкрзНрж▓ржпрж╝ржорзЗржирзНржЯ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи
- тЬЕ ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЯрж┐ржЙржирж┐ржВ ржПржмржВ рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи

---

**ржЕрж╕рзНржмрзАржХрзГрждрж┐**:  
ржПржЗ ржиржерж┐ржЯрж┐ AI ржЕржирзБржмрж╛ржж ржкрж░рж┐рж╖рзЗржмрж╛ [Co-op Translator](https://github.com/Azure/co-op-translator) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЕржирзБржмрж╛ржж ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЖржорж░рж╛ ржпржерж╛рж╕ржорзНржнржм рж╕ржарж┐ржХ ржЕржирзБржмрж╛ржжрзЗрж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рж┐, рждржмрзЗ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржоржирзЗ рж░рж╛ржЦржмрзЗржи ржпрзЗ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ ржЕржирзБржмрж╛ржжрзЗ рждрзНрж░рзБржЯрж┐ ржмрж╛ ржЕрж╕ржЩрзНржЧрждрж┐ ржерж╛ржХрждрзЗ ржкрж╛рж░рзЗред ржиржерж┐ржЯрж┐рж░ ржорзВрж▓ ржнрж╛рж╖рж╛ржпрж╝ рж▓рзЗржЦрж╛ рж╕ржВрж╕рзНржХрж░ржгржЯрж┐ржХрзЗржЗ ржкрзНрж░рж╛ржорж╛ржгрж┐ржХ ржЙрзОрж╕ рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рж╛ ржЙржЪрж┐рждред ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рждржерзНржпрзЗрж░ ржЬржирзНржп, ржкрзЗрж╢рж╛ржжрж╛рж░ ржорж╛ржиржм ржЕржирзБржмрж╛ржж ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛рж░ ржкрж░рж╛ржорж░рзНрж╢ ржжрзЗржУржпрж╝рж╛ рж╣ржпрж╝ред ржПржЗ ржЕржирзБржмрж╛ржж ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржлрж▓рзЗ рж╕рзГрж╖рзНржЯ ржХрзЛржирзЛ ржнрзБрж▓ ржмрзЛржЭрж╛ржмрзБржЭрж┐ ржмрж╛ ржнрзБрж▓ ржмрзНржпрж╛ржЦрзНржпрж╛рж░ ржЬржирзНржп ржЖржорж░рж╛ ржжрж╛ржпрж╝рзА ржиржЗред