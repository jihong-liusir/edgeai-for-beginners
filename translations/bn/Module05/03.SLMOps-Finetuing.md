<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-17T21:19:24+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "bn"
}
-->
# অধ্যায় ৩: ফাইন-টিউনিং - নির্দিষ্ট কাজের জন্য মডেল কাস্টমাইজেশন

## সূচিপত্র
1. [ফাইন-টিউনিং এর পরিচিতি](../../../Module05)
2. [কেন ফাইন-টিউনিং গুরুত্বপূর্ণ](../../../Module05)
3. [ফাইন-টিউনিং এর প্রকারভেদ](../../../Module05)
4. [মাইক্রোসফট অলিভ দিয়ে ফাইন-টিউনিং](../../../Module05)
5. [হ্যান্ডস-অন উদাহরণ](../../../Module05)
6. [সেরা পদ্ধতি এবং নির্দেশিকা](../../../Module05)
7. [উন্নত কৌশল](../../../Module05)
8. [মূল্যায়ন এবং পর্যবেক্ষণ](../../../Module05)
9. [সাধারণ চ্যালেঞ্জ এবং সমাধান](../../../Module05)
10. [উপসংহার](../../../Module05)

## ফাইন-টিউনিং এর পরিচিতি

**ফাইন-টিউনিং** একটি শক্তিশালী মেশিন লার্নিং কৌশল যা পূর্বে প্রশিক্ষিত মডেলকে নির্দিষ্ট কাজ সম্পাদন বা বিশেষায়িত ডেটাসেটের সাথে কাজ করার জন্য মানিয়ে নিতে ব্যবহৃত হয়। মডেলকে শূন্য থেকে প্রশিক্ষণ দেওয়ার পরিবর্তে, ফাইন-টিউনিং পূর্বে প্রশিক্ষিত মডেলের অর্জিত জ্ঞানকে কাজে লাগিয়ে আপনার নির্দিষ্ট ব্যবহার ক্ষেত্রে সামঞ্জস্য করে।

### ফাইন-টিউনিং কী?

ফাইন-টিউনিং একটি **ট্রান্সফার লার্নিং** এর রূপ যেখানে আপনি:
- একটি পূর্বে প্রশিক্ষিত মডেল দিয়ে শুরু করেন যা বড় ডেটাসেট থেকে সাধারণ প্যাটার্ন শিখেছে
- আপনার নির্দিষ্ট ডেটাসেট ব্যবহার করে মডেলের অভ্যন্তরীণ প্যারামিটার সামঞ্জস্য করেন
- মূল্যবান জ্ঞান ধরে রেখে মডেলকে আপনার কাজের জন্য বিশেষায়িত করেন

এটি অনেকটা একজন দক্ষ শেফকে নতুন রান্নার ধরন শেখানোর মতো - তারা ইতিমধ্যেই রান্নার মৌলিক বিষয়গুলো জানে, কিন্তু নতুন স্টাইলের জন্য নির্দিষ্ট কৌশল এবং স্বাদ শিখতে হবে।

### প্রধান সুবিধা

- **সময় সাশ্রয়ী**: শূন্য থেকে প্রশিক্ষণের তুলনায় অনেক দ্রুত
- **ডেটা দক্ষতা**: ভালো পারফরম্যান্স অর্জনের জন্য ছোট ডেটাসেটের প্রয়োজন
- **খরচ সাশ্রয়ী**: কম কম্পিউটেশনাল প্রয়োজনীয়তা
- **উন্নত পারফরম্যান্স**: শূন্য থেকে প্রশিক্ষণের তুলনায় প্রায়শই ভালো ফলাফল অর্জন করে
- **সম্পদ অপ্টিমাইজেশন**: ছোট দল এবং সংস্থার জন্য শক্তিশালী AI সহজলভ্য করে

## কেন ফাইন-টিউনিং গুরুত্বপূর্ণ

### বাস্তব জীবনের প্রয়োগ

ফাইন-টিউনিং বিভিন্ন পরিস্থিতিতে অপরিহার্য:

**1. ডোমেইন অভিযোজন**
- মেডিকেল AI: সাধারণ ভাষার মডেলকে চিকিৎসা পরিভাষা এবং ক্লিনিকাল নোটের জন্য মানিয়ে নেওয়া
- লিগ্যাল টেক: আইনি নথি বিশ্লেষণ এবং চুক্তি পর্যালোচনার জন্য মডেল বিশেষায়িত করা
- ফাইন্যান্সিয়াল সার্ভিস: আর্থিক রিপোর্ট বিশ্লেষণ এবং ঝুঁকি মূল্যায়নের জন্য মডেল কাস্টমাইজ করা

**2. কাজের বিশেষায়ন**
- বিষয়বস্তু তৈরি: নির্দিষ্ট লেখার স্টাইল বা টোনের জন্য ফাইন-টিউনিং
- কোড তৈরি: নির্দিষ্ট প্রোগ্রামিং ভাষা বা ফ্রেমওয়ার্কের জন্য মডেল মানিয়ে নেওয়া
- অনুবাদ: নির্দিষ্ট ভাষার জোড়া বা প্রযুক্তিগত ক্ষেত্রের জন্য পারফরম্যান্স উন্নত করা

**3. কর্পোরেট প্রয়োগ**
- গ্রাহক সেবা: এমন চ্যাটবট তৈরি করা যা কোম্পানি-নির্দিষ্ট পরিভাষা বুঝতে পারে
- অভ্যন্তরীণ ডকুমেন্টেশন: AI সহকারী তৈরি করা যা সংগঠনের প্রক্রিয়াগুলির সাথে পরিচিত
- শিল্প-নির্দিষ্ট সমাধান: এমন মডেল তৈরি করা যা ক্ষেত্র-নির্দিষ্ট পরিভাষা এবং কর্মপ্রবাহ বুঝতে পারে

## ফাইন-টিউনিং এর প্রকারভেদ

### ১. সম্পূর্ণ ফাইন-টিউনিং (ইন্সট্রাকশন ফাইন-টিউনিং)

সম্পূর্ণ ফাইন-টিউনিংয়ে, প্রশিক্ষণের সময় সমস্ত মডেল প্যারামিটার আপডেট করা হয়। এই পদ্ধতি:
- সর্বাধিক নমনীয়তা এবং পারফরম্যান্স সম্ভাবনা প্রদান করে
- উল্লেখযোগ্য কম্পিউটেশনাল সম্পদের প্রয়োজন
- মডেলের সম্পূর্ণ নতুন সংস্করণ তৈরি করে
- এমন পরিস্থিতির জন্য সেরা যেখানে আপনার পর্যাপ্ত প্রশিক্ষণ ডেটা এবং কম্পিউটেশনাল সম্পদ রয়েছে

### ২. প্যারামিটার-দক্ষ ফাইন-টিউনিং (PEFT)

PEFT পদ্ধতিগুলি শুধুমাত্র একটি ছোট অংশের প্যারামিটার আপডেট করে, যা প্রক্রিয়াটিকে আরও দক্ষ করে তোলে:

#### লো-র‍্যাঙ্ক অ্যাডাপটেশন (LoRA)
- বিদ্যমান ওজনগুলিতে ছোট প্রশিক্ষণযোগ্য র‍্যাঙ্ক ডিকম্পোজিশন ম্যাট্রিক্স যোগ করে
- প্রশিক্ষণযোগ্য প্যারামিটারের সংখ্যা নাটকীয়ভাবে কমিয়ে দেয়
- সম্পূর্ণ ফাইন-টিউনিংয়ের কাছাকাছি পারফরম্যান্স বজায় রাখে
- বিভিন্ন অভিযোজনের মধ্যে সহজে স্যুইচিং সক্ষম করে

#### QLoRA (কোয়ান্টাইজড LoRA)
- LoRA কে কোয়ান্টাইজেশন কৌশলের সাথে একত্রিত করে
- মেমরি প্রয়োজনীয়তা আরও কমিয়ে দেয়
- ভোক্তা হার্ডওয়্যারে বড় মডেলের ফাইন-টিউনিং সক্ষম করে
- দক্ষতা এবং পারফরম্যান্সের মধ্যে ভারসাম্য বজায় রাখে

#### অ্যাডাপ্টার
- বিদ্যমান স্তরের মধ্যে ছোট নিউরাল নেটওয়ার্ক যোগ করে
- বেস মডেল স্থির রেখে লক্ষ্যযুক্ত ফাইন-টিউনিং সক্ষম করে
- মডেল কাস্টমাইজেশনের জন্য মডুলার পদ্ধতি সক্ষম করে

### ৩. কাজ-নির্দিষ্ট ফাইন-টিউনিং

নির্দিষ্ট ডাউনস্ট্রিম কাজের জন্য মডেল মানিয়ে নেওয়ার উপর ফোকাস করে:
- **শ্রেণীবিভাজন**: শ্রেণীকরণ কাজের জন্য মডেল সামঞ্জস্য করা
- **তৈরি করা**: বিষয়বস্তু তৈরি এবং টেক্সট জেনারেশনের জন্য অপ্টিমাইজ করা
- **তথ্য বের করা**: তথ্য বের করা এবং নামযুক্ত সত্তা সনাক্তকরণের জন্য ফাইন-টিউনিং
- **সারসংক্ষেপ**: নথি সারসংক্ষেপের জন্য মডেল বিশেষায়িত করা

## মাইক্রোসফট অলিভ দিয়ে ফাইন-টিউনিং

মাইক্রোসফট অলিভ একটি ব্যাপক মডেল অপ্টিমাইজেশন টুলকিট যা ফাইন-টিউনিং প্রক্রিয়াকে সহজ করে এবং এন্টারপ্রাইজ-গ্রেড বৈশিষ্ট্য প্রদান করে।

### মাইক্রোসফট অলিভ কী?

মাইক্রোসফট অলিভ একটি ওপেন-সোর্স মডেল অপ্টিমাইজেশন টুল যা:
- বিভিন্ন হার্ডওয়্যার লক্ষ্যগুলির জন্য ফাইন-টিউনিং ওয়ার্কফ্লো সহজ করে
- জনপ্রিয় মডেল আর্কিটেকচারের জন্য বিল্ট-ইন সাপোর্ট প্রদান করে (Llama, Phi, Qwen, Gemma)
- ক্লাউড এবং লোকাল উভয় ডিপ্লয়মেন্ট অপশন প্রদান করে
- Azure ML এবং অন্যান্য মাইক্রোসফট AI পরিষেবার সাথে নির্বিঘ্নে ইন্টিগ্রেট করে
- স্বয়ংক্রিয় অপ্টিমাইজেশন এবং কোয়ান্টাইজেশন সমর্থন করে

### প্রধান বৈশিষ্ট্য

- **হার্ডওয়্যার-সচেতন অপ্টিমাইজেশন**: নির্দিষ্ট হার্ডওয়্যারের জন্য মডেল স্বয়ংক্রিয়ভাবে অপ্টিমাইজ করে (CPU, GPU, NPU)
- **মাল্টি-ফরম্যাট সাপোর্ট**: PyTorch, Hugging Face, এবং ONNX মডেলের সাথে কাজ করে
- **স্বয়ংক্রিয় ওয়ার্কফ্লো**: ম্যানুয়াল কনফিগারেশন এবং ট্রায়াল-এন্ড-এরর কমিয়ে দেয়
- **এন্টারপ্রাইজ ইন্টিগ্রেশন**: Azure ML এবং ক্লাউড ডিপ্লয়মেন্টের জন্য বিল্ট-ইন সাপোর্ট
- **এক্সটেনসিবল আর্কিটেকচার**: কাস্টম অপ্টিমাইজেশন কৌশলগুলিকে অনুমতি দেয়

### ইনস্টলেশন এবং সেটআপ

#### বেসিক ইনস্টলেশন

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### ঐচ্ছিক নির্ভরতা

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### ইনস্টলেশন যাচাই করুন

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## হ্যান্ডস-অন উদাহরণ

### উদাহরণ ১: অলিভ CLI দিয়ে বেসিক ফাইন-টিউনিং

এই উদাহরণটি একটি ছোট ভাষার মডেলকে বাক্য শ্রেণীবিভাজনের জন্য ফাইন-টিউনিং দেখায়:

#### ধাপ ১: আপনার পরিবেশ প্রস্তুত করুন

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### ধাপ ২: মডেল ফাইন-টিউন করুন

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### ধাপ ৩: ডিপ্লয়মেন্টের জন্য অপ্টিমাইজ করুন

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### উদাহরণ ২: কাস্টম ডেটাসেট দিয়ে উন্নত কনফিগারেশন

#### ধাপ ১: কাস্টম ডেটাসেট প্রস্তুত করুন

আপনার প্রশিক্ষণ ডেটা সহ একটি JSON ফাইল তৈরি করুন:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### ধাপ ২: কনফিগারেশন ফাইল তৈরি করুন

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### ধাপ ৩: ফাইন-টিউনিং সম্পাদন করুন

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### উদাহরণ ৩: মেমরি দক্ষতার জন্য QLoRA ফাইন-টিউনিং

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## সেরা পদ্ধতি এবং নির্দেশিকা

### ডেটা প্রস্তুতি

**১. ডেটার গুণমান পরিমাণের চেয়ে গুরুত্বপূর্ণ**
- বড় পরিমাণের নিম্নমানের ডেটার পরিবর্তে উচ্চ-মানের, বৈচিত্র্যময় উদাহরণকে অগ্রাধিকার দিন
- ডেটা আপনার লক্ষ্য ব্যবহার ক্ষেত্রে প্রতিনিধিত্বমূলক কিনা তা নিশ্চিত করুন
- ডেটা পরিষ্কার এবং ধারাবাহিকভাবে প্রি-প্রসেস করুন

**২. ডেটার ফরম্যাট এবং টেমপ্লেট**
- সমস্ত প্রশিক্ষণ উদাহরণের মধ্যে ধারাবাহিক ফরম্যাটিং ব্যবহার করুন
- আপনার ব্যবহার ক্ষেত্রে মিলে যায় এমন স্পষ্ট ইনপুট-আউটপুট টেমপ্লেট তৈরি করুন
- ইন্সট্রাকশন-টিউনড মডেলের জন্য উপযুক্ত ইন্সট্রাকশন ফরম্যাটিং অন্তর্ভুক্ত করুন

**৩. ডেটাসেট বিভাজন**
- ডেটার ১০-২০% যাচাইয়ের জন্য সংরক্ষণ করুন
- প্রশিক্ষণ/যাচাই বিভাজনের মধ্যে অনুরূপ বিতরণ বজায় রাখুন
- শ্রেণীবিভাজন কাজের জন্য স্তরিত নমুনা বিবেচনা করুন

### প্রশিক্ষণ কনফিগারেশন

**১. লার্নিং রেট নির্বাচন**
- ফাইন-টিউনিংয়ের জন্য ছোট লার্নিং রেট (1e-5 থেকে 1e-4) দিয়ে শুরু করুন
- ভালো কনভার্জেন্সের জন্য লার্নিং রেট শিডিউলিং ব্যবহার করুন
- লস কার্ভ পর্যবেক্ষণ করে রেট সামঞ্জস্য করুন

**২. ব্যাচ সাইজ অপ্টিমাইজেশন**
- উপলব্ধ মেমরির সাথে ব্যাচ সাইজের ভারসাম্য বজায় রাখুন
- বড় কার্যকর ব্যাচ সাইজের জন্য গ্রেডিয়েন্ট অ্যাকুমুলেশন ব্যবহার করুন
- ব্যাচ সাইজ এবং লার্নিং রেটের সম্পর্ক বিবেচনা করুন

**৩. প্রশিক্ষণের সময়কাল**
- ওভারফিটিং এড়াতে যাচাই মেট্রিক পর্যবেক্ষণ করুন
- যাচাই পারফরম্যান্স স্থিতিশীল হলে দ্রুত থামানোর পদ্ধতি ব্যবহার করুন
- পুনরুদ্ধার এবং বিশ্লেষণের জন্য নিয়মিত চেকপয়েন্ট সংরক্ষণ করুন

### মডেল নির্বাচন

**১. বেস মডেল পছন্দ**
- সম্ভব হলে অনুরূপ ডোমেইনে পূর্বে প্রশিক্ষিত মডেল নির্বাচন করুন
- আপনার কম্পিউটেশনাল সীমাবদ্ধতার সাথে মডেলের আকার বিবেচনা করুন
- বাণিজ্যিক ব্যবহারের জন্য লাইসেন্সিং প্রয়োজনীয়তা মূল্যায়ন করুন

**২. ফাইন-টিউনিং পদ্ধতি নির্বাচন**
- সম্পদ-সীমাবদ্ধ পরিবেশের জন্য LoRA/QLoRA ব্যবহার করুন
- সর্বাধিক পারফরম্যান্স গুরুত্বপূর্ণ হলে সম্পূর্ণ ফাইন-টিউনিং নির্বাচন করুন
- একাধিক কাজের পরিস্থিতির জন্য অ্যাডাপ্টার-ভিত্তিক পদ্ধতি বিবেচনা করুন

### সম্পদ ব্যবস্থাপনা

**১. হার্ডওয়্যার অপ্টিমাইজেশন**
- আপনার মডেলের আকার এবং পদ্ধতির জন্য উপযুক্ত হার্ডওয়্যার নির্বাচন করুন
- গ্রেডিয়েন্ট চেকপয়েন্টিং দিয়ে GPU মেমরি দক্ষতার সাথে ব্যবহার করুন
- বড় মডেলের জন্য ক্লাউড-ভিত্তিক সমাধান বিবেচনা করুন

**২. মেমরি ব্যবস্থাপনা**
- উপলব্ধ হলে মিশ্র প্রিসিশন প্রশিক্ষণ ব্যবহার করুন
- মেমরি সীমাবদ্ধতার জন্য গ্রেডিয়েন্ট অ্যাকুমুলেশন বাস্তবায়ন করুন
- প্রশিক্ষণের সময় GPU মেমরি ব্যবহার পর্যবেক্ষণ করুন

## উন্নত কৌশল

### মাল্টি-অ্যাডাপ্টার প্রশিক্ষণ

বেস মডেল ভাগ করে বিভিন্ন কাজের জন্য একাধিক অ্যাডাপ্টার প্রশিক্ষণ দিন:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### হাইপারপ্যারামিটার অপ্টিমাইজেশন

পদ্ধতিগতভাবে হাইপারপ্যারামিটার টিউনিং বাস্তবায়ন করুন:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### কাস্টম লস ফাংশন

ডোমেইন-নির্দিষ্ট লস ফাংশন বাস্তবায়ন করুন:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## মূল্যায়ন এবং পর্যবেক্ষণ

### মেট্রিক্স এবং মূল্যায়ন

**১. স্ট্যান্ডার্ড মেট্রিক্স**
- **সঠিকতা**: শ্রেণীবিভাজন কাজের জন্য সামগ্রিক সঠিকতা
- **পারপ্লেক্সিটি**: ভাষা মডেলিংয়ের গুণমানের পরিমাপ
- **BLEU/ROUGE**: টেক্সট জেনারেশন এবং সারসংক্ষেপের গুণমান
- **F1 স্কোর**: শ্রেণীবিভাজনের জন্য ভারসাম্যপূর্ণ প্রিসিশন এবং রিকল

**২. ডোমেইন-নির্দিষ্ট মেট্রিক্স**
- **কাজ-নির্দিষ্ট বেঞ্চমার্ক**: আপনার ডোমেইনের জন্য প্রতিষ্ঠিত বেঞ্চমার্ক ব্যবহার করুন
- **মানব মূল্যায়ন**: বিষয়গত কাজের জন্য মানব মূল্যায়ন অন্তর্ভুক্ত করুন
- **ব্যবসায়িক মেট্রিক্স**: প্রকৃত ব্যবসায়িক উদ্দেশ্যের সাথে সামঞ্জস্য করুন

**৩. মূল্যায়ন সেটআপ**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### প্রশিক্ষণ অগ্রগতি পর্যবেক্ষণ

**১. লস ট্র্যাকিং**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**২. যাচাই পর্যবেক্ষণ**
- প্রশিক্ষণ লসের পাশাপাশি যাচাই লস ট্র্যাক করুন
- ওভারফিটিংয়ের লক্ষণ পর্যবেক্ষণ করুন (যাচাই লস বৃদ্ধি পাচ্ছে যখন প্রশিক্ষণ লস কমছে)
- যাচাই মেট্রিকের উপর ভিত্তি করে দ্রুত থামানোর পদ্ধতি ব্যবহার করুন

**৩. সম্পদ পর্যবেক্ষণ**
- GPU/CPU ব্যবহার পর্যবেক্ষণ করুন
- মেমরি ব্যবহারের প্যাটার্ন ট্র্যাক করুন
- প্রশিক্ষণের গতি এবং থ্রুপুট পর্যবেক্ষণ করুন

## সাধারণ চ্যালেঞ্জ এবং সমাধান

### চ্যালেঞ্জ ১: ওভারফিটিং

**লক্ষণ:**
- প্রশিক্ষণ লস কমতে থাকে যখন যাচাই লস বৃদ্ধি পায়
- প্রশিক্ষণ এবং যাচাই পারফরম্যান্সের মধ্যে বড় ফাঁক
- নতুন ডেটায় দুর্বল সাধারণীকরণ

**সমাধান:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### চ্যালেঞ্জ ২: মেমরি সীমাবদ্ধতা

**সমাধান:**
- গ্রেডিয়েন্ট চেকপয়েন্টিং ব্যবহার করুন
- গ্রেডিয়েন্ট অ্যাকুমুলেশন বাস্তবায়ন করুন
- প্যারামিটার-দক্ষ পদ্ধতি (LoRA, QLoRA) নির্বাচন করুন
- বড় মডেলের জন্য মডেল প্যারালেলিজম ব্যবহার করুন

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### চ্যালেঞ্জ ৩: ধীর প্রশিক্ষণ

**সমাধান:**
- ডেটা লোডিং পাইপলাইন অপ্টিমাইজ করুন
- মিশ্র প্রিসিশন প্রশিক্ষণ ব্যবহার করুন
- দক্ষ ব্যাচিং কৌশল বাস্তবায়ন করুন
- বড়

---

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়বদ্ধ থাকব না।