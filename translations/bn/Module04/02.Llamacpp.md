<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T21:06:10+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "bn"
}
-->
# অধ্যায় ২ : Llama.cpp ইমপ্লিমেন্টেশন গাইড

## বিষয়সূচি
1. [ভূমিকা](../../../Module04)
2. [Llama.cpp কী?](../../../Module04)
3. [ইনস্টলেশন](../../../Module04)
4. [সোর্স থেকে বিল্ড করা](../../../Module04)
5. [মডেল কোয়ান্টাইজেশন](../../../Module04)
6. [প্রাথমিক ব্যবহার](../../../Module04)
7. [উন্নত ফিচারসমূহ](../../../Module04)
8. [পাইথন ইন্টিগ্রেশন](../../../Module04)
9. [সমস্যা সমাধান](../../../Module04)
10. [সেরা পদ্ধতিগুলো](../../../Module04)

## ভূমিকা

এই বিস্তৃত টিউটোরিয়ালটি আপনাকে Llama.cpp সম্পর্কে সবকিছু শেখাবে, প্রাথমিক ইনস্টলেশন থেকে শুরু করে উন্নত ব্যবহারের কৌশল পর্যন্ত। Llama.cpp একটি শক্তিশালী C++ ইমপ্লিমেন্টেশন যা বড় ভাষার মডেল (LLMs) এর কার্যকর ইনফারেন্স নিশ্চিত করে, ন্যূনতম সেটআপ এবং বিভিন্ন হার্ডওয়্যার কনফিগারেশনে চমৎকার পারফরম্যান্স প্রদান করে।

## Llama.cpp কী?

Llama.cpp একটি LLM ইনফারেন্স ফ্রেমওয়ার্ক যা C/C++ এ লেখা হয়েছে। এটি বড় ভাষার মডেলগুলোকে স্থানীয়ভাবে চালানোর সুযোগ দেয়, ন্যূনতম সেটআপ এবং সর্বাধুনিক পারফরম্যান্স সহ। এর মূল বৈশিষ্ট্যগুলো হলো:

### মূল বৈশিষ্ট্য
- **সাধারণ C/C++ ইমপ্লিমেন্টেশন** যা কোনো ডিপেনডেন্সি ছাড়াই কাজ করে
- **ক্রস-প্ল্যাটফর্ম সামঞ্জস্যতা** (Windows, macOS, Linux)
- **হার্ডওয়্যার অপ্টিমাইজেশন** বিভিন্ন আর্কিটেকচারের জন্য
- **কোয়ান্টাইজেশন সাপোর্ট** (1.5-বিট থেকে 8-বিট ইন্টিজার কোয়ান্টাইজেশন)
- **CPU এবং GPU অ্যাক্সিলারেশন** সাপোর্ট
- **মেমোরি দক্ষতা** সীমিত পরিবেশের জন্য

### সুবিধাসমূহ
- বিশেষায়িত হার্ডওয়্যার ছাড়াই CPU-তে কার্যকরভাবে চলে
- একাধিক GPU ব্যাকএন্ড সাপোর্ট করে (CUDA, Metal, OpenCL, Vulkan)
- হালকা এবং পোর্টেবল
- অ্যাপল সিলিকনকে প্রথম শ্রেণির নাগরিক হিসেবে বিবেচনা করা হয়েছে - ARM NEON, Accelerate এবং Metal ফ্রেমওয়ার্কের মাধ্যমে অপ্টিমাইজড
- মেমোরি ব্যবহারের জন্য বিভিন্ন কোয়ান্টাইজেশন লেভেল সাপোর্ট করে

## ইনস্টলেশন

### পদ্ধতি ১: প্রি-বিল্ট বাইনারি (শুরু করার জন্য সুপারিশকৃত)

#### GitHub Releases থেকে ডাউনলোড করুন
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) এ যান
2. আপনার সিস্টেমের জন্য উপযুক্ত বাইনারি ডাউনলোড করুন:
   - Windows এর জন্য `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS এর জন্য `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux এর জন্য `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. আর্কাইভটি এক্সট্র্যাক্ট করুন এবং ডিরেক্টরিটিকে আপনার সিস্টেমের PATH-এ যোগ করুন

#### প্যাকেজ ম্যানেজার ব্যবহার করে

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (বিভিন্ন ডিস্ট্রিবিউশন):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### পদ্ধতি ২: পাইথন প্যাকেজ (llama-cpp-python)

#### বেসিক ইনস্টলেশন
```bash
pip install llama-cpp-python
```

#### হার্ডওয়্যার অ্যাক্সিলারেশন সহ
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## সোর্স থেকে বিল্ড করা

### প্রয়োজনীয়তা

**সিস্টেমের চাহিদা:**
- C++ কম্পাইলার (GCC, Clang, বা MSVC)
- CMake (সংস্করণ ৩.১৪ বা তার বেশি)
- Git
- আপনার প্ল্যাটফর্মের জন্য বিল্ড টুলস

**প্রয়োজনীয়তা ইনস্টল করা:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022 ইনস্টল করুন C++ ডেভেলপমেন্ট টুলস সহ
- অফিসিয়াল ওয়েবসাইট থেকে CMake ইনস্টল করুন
- Git ইনস্টল করুন

### বেসিক বিল্ড প্রক্রিয়া

1. **রিপোজিটরি ক্লোন করুন:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **বিল্ড কনফিগার করুন:**
```bash
cmake -B build
```

3. **প্রজেক্ট বিল্ড করুন:**
```bash
cmake --build build --config Release
```

দ্রুত কম্পাইলেশনের জন্য, প্যারালাল জবস ব্যবহার করুন:
```bash
cmake --build build --config Release -j 8
```

### হার্ডওয়্যার-নির্দিষ্ট বিল্ড

#### CUDA সাপোর্ট (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal সাপোর্ট (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS সাপোর্ট (CPU অপ্টিমাইজেশন)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan সাপোর্ট
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### উন্নত বিল্ড অপশন

#### ডিবাগ বিল্ড
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### অতিরিক্ত ফিচার সহ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## মডেল কোয়ান্টাইজেশন

### GGUF ফরম্যাট বোঝা

GGUF (Generalized GGML Unified Format) একটি অপ্টিমাইজড ফাইল ফরম্যাট যা Llama.cpp এবং অন্যান্য ফ্রেমওয়ার্ক ব্যবহার করে বড় ভাষার মডেলগুলো কার্যকরভাবে চালানোর জন্য ডিজাইন করা হয়েছে। এটি প্রদান করে:

- স্ট্যান্ডার্ডাইজড মডেল ওজন সংরক্ষণ
- প্ল্যাটফর্ম জুড়ে উন্নত সামঞ্জস্যতা
- উন্নত পারফরম্যান্স
- দক্ষ মেটাডেটা হ্যান্ডলিং

### কোয়ান্টাইজেশন টাইপস

Llama.cpp বিভিন্ন কোয়ান্টাইজেশন লেভেল সাপোর্ট করে:

| টাইপ | বিট | বর্ণনা | ব্যবহার ক্ষেত্র |
|------|------|-------------|----------|
| F16 | 16 | হাফ প্রিসিশন | উচ্চ মানের, বড় মেমোরি |
| Q8_0 | 8 | ৮-বিট কোয়ান্টাইজেশন | ভালো ভারসাম্য |
| Q4_0 | 4 | ৪-বিট কোয়ান্টাইজেশন | মাঝারি মান, ছোট সাইজ |
| Q2_K | 2 | ২-বিট কোয়ান্টাইজেশন | সবচেয়ে ছোট সাইজ, নিম্ন মান |

### মডেল রূপান্তর

#### PyTorch থেকে GGUF-এ রূপান্তর
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face থেকে সরাসরি ডাউনলোড
অনেক মডেল GGUF ফরম্যাটে Hugging Face-এ উপলব্ধ:
- "GGUF" নাম সহ মডেলগুলো অনুসন্ধান করুন
- উপযুক্ত কোয়ান্টাইজেশন লেভেল ডাউনলোড করুন
- সরাসরি Llama.cpp দিয়ে ব্যবহার করুন

## প্রাথমিক ব্যবহার

### কমান্ড লাইন ইন্টারফেস

#### সহজ টেক্সট জেনারেশন
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face থেকে মডেল ব্যবহার
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### সার্ভার মোড
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### সাধারণ প্যারামিটার

| প্যারামিটার | বর্ণনা | উদাহরণ |
|-----------|-------------|---------|
| `-m` | মডেল ফাইল পাথ | `-m model.gguf` |
| `-p` | প্রম্পট টেক্সট | `-p "Hello world"` |
| `-n` | জেনারেট করার টোকেন সংখ্যা | `-n 100` |
| `-c` | কন্টেক্সট সাইজ | `-c 4096` |
| `-t` | থ্রেড সংখ্যা | `-t 8` |
| `-ngl` | GPU লেয়ার | `-ngl 32` |
| `-temp` | টেম্পারেচার | `-temp 0.7` |

### ইন্টারঅ্যাকটিভ মোড

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## উন্নত ফিচারসমূহ

### সার্ভার API

#### সার্ভার শুরু করা
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API ব্যবহার
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### পারফরম্যান্স অপ্টিমাইজেশন

#### মেমোরি ম্যানেজমেন্ট
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### মাল্টি-থ্রেডিং
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU অ্যাক্সিলারেশন
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## পাইথন ইন্টিগ্রেশন

### llama-cpp-python দিয়ে বেসিক ব্যবহার

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### চ্যাট ইন্টারফেস

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### স্ট্রিমিং রেসপন্স

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain এর সাথে ইন্টিগ্রেশন

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## সমস্যার সমাধান

### সাধারণ সমস্যা এবং সমাধান

#### বিল্ড ত্রুটি

**সমস্যা: CMake পাওয়া যাচ্ছে না**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**সমস্যা: কম্পাইলার পাওয়া যাচ্ছে না**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### রানটাইম সমস্যা

**সমস্যা: মডেল লোডিং ব্যর্থ হচ্ছে**
- মডেল ফাইল পাথ যাচাই করুন
- ফাইল পারমিশন চেক করুন
- পর্যাপ্ত RAM নিশ্চিত করুন
- ভিন্ন কোয়ান্টাইজেশন লেভেল চেষ্টা করুন

**সমস্যা: দুর্বল পারফরম্যান্স**
- হার্ডওয়্যার অ্যাক্সিলারেশন চালু করুন
- থ্রেড সংখ্যা বাড়ান
- উপযুক্ত কোয়ান্টাইজেশন ব্যবহার করুন
- GPU মেমোরি ব্যবহার চেক করুন

#### মেমোরি সমস্যা

**সমস্যা: মেমোরি শেষ হয়ে যাচ্ছে**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### প্ল্যাটফর্ম-নির্দিষ্ট সমস্যা

#### Windows
- MinGW বা Visual Studio কম্পাইলার ব্যবহার করুন
- সঠিক PATH কনফিগারেশন নিশ্চিত করুন
- অ্যান্টিভাইরাস হস্তক্ষেপ চেক করুন

#### macOS
- Apple Silicon এর জন্য Metal চালু করুন
- প্রয়োজন হলে সামঞ্জস্যতার জন্য Rosetta 2 ব্যবহার করুন
- Xcode কমান্ড লাইন টুলস চেক করুন

#### Linux
- ডেভেলপমেন্ট প্যাকেজ ইনস্টল করুন
- GPU ড্রাইভার সংস্করণ চেক করুন
- CUDA টুলকিট ইনস্টলেশন যাচাই করুন

## সেরা পদ্ধতিগুলো

### মডেল নির্বাচন
1. **আপনার হার্ডওয়্যারের উপর ভিত্তি করে উপযুক্ত কোয়ান্টাইজেশন নির্বাচন করুন**
2. **মডেলের আকার বনাম মানের ভারসাম্য বিবেচনা করুন**
3. **আপনার নির্দিষ্ট ব্যবহারের জন্য বিভিন্ন মডেল পরীক্ষা করুন**

### পারফরম্যান্স অপ্টিমাইজেশন
1. **GPU অ্যাক্সিলারেশন ব্যবহার করুন** যদি উপলব্ধ থাকে
2. **আপনার CPU এর জন্য থ্রেড সংখ্যা অপ্টিমাইজ করুন**
3. **আপনার ব্যবহারের জন্য উপযুক্ত কন্টেক্সট সাইজ সেট করুন**
4. **বড় মডেলের জন্য মেমোরি ম্যাপিং চালু করুন**

### প্রোডাকশন ডিপ্লয়মেন্ট
1. **API অ্যাক্সেসের জন্য সার্ভার মোড ব্যবহার করুন**
2. **উপযুক্ত ত্রুটি হ্যান্ডলিং বাস্তবায়ন করুন**
3. **রিসোর্স ব্যবহার পর্যবেক্ষণ করুন**
4. **লগিং এবং মনিটরিং সেট আপ করুন**

### ডেভেলপমেন্ট ওয়ার্কফ্লো
1. **পরীক্ষার জন্য ছোট মডেল দিয়ে শুরু করুন**
2. **মডেল কনফিগারেশনের জন্য ভার্সন কন্ট্রোল ব্যবহার করুন**
3. **আপনার কনফিগারেশন ডকুমেন্ট করুন**
4. **বিভিন্ন প্ল্যাটফর্মে পরীক্ষা করুন**

### নিরাপত্তা বিবেচনা
1. **ইনপুট প্রম্পট যাচাই করুন**
2. **রেট লিমিটিং বাস্তবায়ন করুন**
3. **API এন্ডপয়েন্ট সুরক্ষিত করুন**
4. **অপব্যবহারের প্যাটার্ন পর্যবেক্ষণ করুন**

## উপসংহার

Llama.cpp একটি শক্তিশালী এবং কার্যকর উপায় প্রদান করে বড় ভাষার মডেলগুলো স্থানীয়ভাবে চালানোর জন্য, বিভিন্ন হার্ডওয়্যার কনফিগারেশনে। আপনি AI অ্যাপ্লিকেশন ডেভেলপ করুন, গবেষণা করুন, বা LLM নিয়ে পরীক্ষা করুন, এই ফ্রেমওয়ার্কটি নমনীয়তা এবং পারফরম্যান্স প্রদান করে যা বিভিন্ন ব্যবহারের ক্ষেত্রে প্রয়োজনীয়।

মূল বিষয়গুলো:
- আপনার প্রয়োজন অনুযায়ী ইনস্টলেশন পদ্ধতি নির্বাচন করুন
- আপনার নির্দিষ্ট হার্ডওয়্যার কনফিগারেশনের জন্য অপ্টিমাইজ করুন
- প্রাথমিক ব্যবহার দিয়ে শুরু করুন এবং ধীরে ধীরে উন্নত ফিচারগুলো অন্বেষণ করুন
- সহজ ইন্টিগ্রেশনের জন্য পাইথন বাইন্ডিং ব্যবহার বিবেচনা করুন
- প্রোডাকশন ডিপ্লয়মেন্টের জন্য সেরা পদ্ধতিগুলো অনুসরণ করুন

আরও তথ্য এবং আপডেটের জন্য, [Llama.cpp এর অফিসিয়াল রিপোজিটরি](https://github.com/ggml-org/llama.cpp) দেখুন এবং উপলব্ধ বিস্তৃত ডকুমেন্টেশন এবং কমিউনিটি রিসোর্সগুলো অনুসরণ করুন।

## ➡️ পরবর্তী পদক্ষেপ

- [০৩: মাইক্রোসফট অলিভ অপ্টিমাইজেশন স্যুট](./03.MicrosoftOlive.md)

---

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় লেখা সংস্করণটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ ব্যবহার করার পরামর্শ দেওয়া হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।