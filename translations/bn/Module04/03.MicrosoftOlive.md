<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T17:16:30+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "bn"
}
-->
# Section 3 : Microsoft Olive Optimization Suite

## সূচিপত্র
1. [ভূমিকা](../../../Module04)
2. [Microsoft Olive কী?](../../../Module04)
3. [ইনস্টলেশন](../../../Module04)
4. [দ্রুত শুরু করার নির্দেশিকা](../../../Module04)
5. [উদাহরণ: Qwen3-কে ONNX INT4-এ রূপান্তর করা](../../../Module04)
6. [উন্নত ব্যবহার](../../../Module04)
7. [সেরা পদ্ধতি](../../../Module04)
8. [সমস্যা সমাধান](../../../Module04)
9. [অতিরিক্ত সম্পদ](../../../Module04)

## ভূমিকা

Microsoft Olive একটি শক্তিশালী, সহজে ব্যবহারযোগ্য হার্ডওয়্যার-সচেতন মডেল অপ্টিমাইজেশন টুলকিট যা বিভিন্ন হার্ডওয়্যার প্ল্যাটফর্মে মেশিন লার্নিং মডেল মোতায়েনের জন্য অপ্টিমাইজেশন প্রক্রিয়াকে সহজ করে। আপনি CPU, GPU, বা বিশেষায়িত AI অ্যাক্সিলারেটর লক্ষ্য করলেও, Olive আপনাকে মডেলের সঠিকতা বজায় রেখে সর্বোত্তম কর্মক্ষমতা অর্জনে সহায়তা করে।

## Microsoft Olive কী?

Olive একটি সহজে ব্যবহারযোগ্য হার্ডওয়্যার-সচেতন মডেল অপ্টিমাইজেশন টুল যা মডেল কম্প্রেশন, অপ্টিমাইজেশন এবং কম্পাইলেশনের জন্য শিল্প-নেতৃস্থানীয় কৌশলগুলিকে একত্রিত করে। এটি ONNX Runtime-এর সাথে একটি E2E ইনফারেন্স অপ্টিমাইজেশন সমাধান হিসেবে কাজ করে।

### প্রধান বৈশিষ্ট্য

- **হার্ডওয়্যার-সচেতন অপ্টিমাইজেশন**: আপনার লক্ষ্য হার্ডওয়্যারের জন্য সেরা অপ্টিমাইজেশন কৌশলগুলি স্বয়ংক্রিয়ভাবে নির্বাচন করে
- **৪০+ বিল্ট-ইন অপ্টিমাইজেশন উপাদান**: মডেল কম্প্রেশন, কোয়ান্টাইজেশন, গ্রাফ অপ্টিমাইজেশন এবং আরও অনেক কিছু অন্তর্ভুক্ত
- **সহজ CLI ইন্টারফেস**: সাধারণ অপ্টিমাইজেশন কাজের জন্য সহজ কমান্ড
- **মাল্টি-ফ্রেমওয়ার্ক সাপোর্ট**: PyTorch, Hugging Face মডেল এবং ONNX-এর সাথে কাজ করে
- **জনপ্রিয় মডেল সাপোর্ট**: Olive স্বয়ংক্রিয়ভাবে Llama, Phi, Qwen, Gemma ইত্যাদি জনপ্রিয় মডেল আর্কিটেকচার অপ্টিমাইজ করতে পারে

### সুবিধা

- **উন্নয়ন সময় হ্রাস**: বিভিন্ন অপ্টিমাইজেশন কৌশল নিয়ে ম্যানুয়ালি পরীক্ষা করার প্রয়োজন নেই
- **কর্মক্ষমতা বৃদ্ধি**: উল্লেখযোগ্য গতি উন্নতি (কিছু ক্ষেত্রে ৬x পর্যন্ত)
- **ক্রস-প্ল্যাটফর্ম মোতায়েন**: অপ্টিমাইজড মডেল বিভিন্ন হার্ডওয়্যার এবং অপারেটিং সিস্টেমে কাজ করে
- **সঠিকতা বজায় রাখা**: অপ্টিমাইজেশন মডেলের গুণমান বজায় রেখে কর্মক্ষমতা উন্নত করে

## ইনস্টলেশন

### প্রয়োজনীয়তা

- Python 3.8 বা তার বেশি
- pip প্যাকেজ ম্যানেজার
- ভার্চুয়াল এনভায়রনমেন্ট (প্রস্তাবিত)

### মৌলিক ইনস্টলেশন

ভার্চুয়াল এনভায়রনমেন্ট তৈরি এবং সক্রিয় করুন:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

অটো-অপ্টিমাইজেশন বৈশিষ্ট্য সহ Olive ইনস্টল করুন:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ঐচ্ছিক নির্ভরতা

Olive অতিরিক্ত বৈশিষ্ট্যের জন্য বিভিন্ন ঐচ্ছিক নির্ভরতা প্রদান করে:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ইনস্টলেশন যাচাই করুন

```bash
olive --help
```

যদি সফল হয়, আপনি Olive CLI সাহায্য বার্তা দেখতে পাবেন।

## দ্রুত শুরু করার নির্দেশিকা

### আপনার প্রথম অপ্টিমাইজেশন

Olive-এর অটো-অপ্টিমাইজেশন বৈশিষ্ট্য ব্যবহার করে একটি ছোট ভাষা মডেল অপ্টিমাইজ করুন:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### এই কমান্ডটি কী করে

অপ্টিমাইজেশন প্রক্রিয়ায় অন্তর্ভুক্ত: স্থানীয় ক্যাশ থেকে মডেল সংগ্রহ করা, ONNX গ্রাফ ক্যাপচার করা এবং ONNX ডেটা ফাইলে ওজন সংরক্ষণ করা, ONNX গ্রাফ অপ্টিমাইজ করা এবং RTN পদ্ধতি ব্যবহার করে মডেলকে int4-এ কোয়ান্টাইজ করা।

### কমান্ড প্যারামিটার ব্যাখ্যা

- `--model_name_or_path`: Hugging Face মডেল আইডেন্টিফায়ার বা স্থানীয় পথ
- `--output_path`: অপ্টিমাইজড মডেল সংরক্ষণের জন্য ডিরেক্টরি
- `--device`: লক্ষ্য ডিভাইস (cpu, gpu)
- `--provider`: এক্সিকিউশন প্রদানকারী (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ইনফারেন্সের জন্য ONNX Runtime Generate AI ব্যবহার করুন
- `--precision`: কোয়ান্টাইজেশন নির্ভুলতা (int4, int8, fp16)
- `--log_level`: লগিং ভার্বোসিটি (0=minimal, 1=verbose)

## উদাহরণ: Qwen3-কে ONNX INT4-এ রূপান্তর করা

Hugging Face-এর প্রদত্ত উদাহরণ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) অনুসারে, এখানে Qwen3 মডেল অপ্টিমাইজ করার পদ্ধতি:

### ধাপ ১: মডেল ডাউনলোড করুন (ঐচ্ছিক)

ডাউনলোড সময় কমানোর জন্য শুধুমাত্র প্রয়োজনীয় ফাইলগুলি ক্যাশ করুন:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ধাপ ২: Qwen3 মডেল অপ্টিমাইজ করুন

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ধাপ ৩: অপ্টিমাইজড মডেল পরীক্ষা করুন

আপনার অপ্টিমাইজড মডেল পরীক্ষা করার জন্য একটি সাধারণ Python স্ক্রিপ্ট তৈরি করুন:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### আউটপুট কাঠামো

অপ্টিমাইজেশনের পরে, আপনার আউটপুট ডিরেক্টরিতে থাকবে:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## উন্নত ব্যবহার

### কনফিগারেশন ফাইল

আরও জটিল অপ্টিমাইজেশন ওয়ার্কফ্লোয়ের জন্য, আপনি JSON কনফিগারেশন ফাইল ব্যবহার করতে পারেন:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

কনফিগারেশন দিয়ে চালান:

```bash
olive run --config config.json
```

### GPU অপ্টিমাইজেশন

CUDA GPU অপ্টিমাইজেশনের জন্য:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows)-এর জন্য:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive দিয়ে ফাইন-টিউনিং

Olive মডেল ফাইন-টিউনিংও সমর্থন করে:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## সেরা পদ্ধতি

### ১. মডেল নির্বাচন
- পরীক্ষার জন্য ছোট মডেল দিয়ে শুরু করুন (যেমন, ০.৫B-৭B প্যারামিটার)
- নিশ্চিত করুন যে আপনার লক্ষ্য মডেল আর্কিটেকচার Olive দ্বারা সমর্থিত

### ২. হার্ডওয়্যার বিবেচনা
- আপনার অপ্টিমাইজেশন লক্ষ্য আপনার মোতায়েন হার্ডওয়্যারের সাথে মেলান
- CUDA-সামঞ্জস্যপূর্ণ হার্ডওয়্যার থাকলে GPU অপ্টিমাইজেশন ব্যবহার করুন
- Windows মেশিনে ইন্টিগ্রেটেড গ্রাফিক্সের জন্য DirectML বিবেচনা করুন

### ৩. নির্ভুলতা নির্বাচন
- **INT4**: সর্বাধিক কম্প্রেশন, সামান্য সঠিকতা ক্ষতি
- **INT8**: আকার এবং সঠিকতার মধ্যে ভালো ভারসাম্য
- **FP16**: সামান্য সঠিকতা ক্ষতি, মাঝারি আকার হ্রাস

### ৪. পরীক্ষা এবং যাচাই
- আপনার নির্দিষ্ট ব্যবহারের ক্ষেত্রে অপ্টিমাইজড মডেলগুলি সর্বদা পরীক্ষা করুন
- কর্মক্ষমতা মেট্রিক তুলনা করুন (লেটেন্সি, থ্রুপুট, সঠিকতা)
- মূল্যায়নের জন্য প্রতিনিধিত্বমূলক ইনপুট ডেটা ব্যবহার করুন

### ৫. পুনরাবৃত্তিমূলক অপ্টিমাইজেশন
- দ্রুত ফলাফলের জন্য অটো-অপ্টিমাইজেশন দিয়ে শুরু করুন
- সূক্ষ্ম নিয়ন্ত্রণের জন্য কনফিগারেশন ফাইল ব্যবহার করুন
- বিভিন্ন অপ্টিমাইজেশন পাস নিয়ে পরীক্ষা করুন

## সমস্যা সমাধান

### সাধারণ সমস্যা

#### ১. ইনস্টলেশন সমস্যা
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ২. CUDA/GPU সমস্যা
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ৩. মেমরি সমস্যা
- অপ্টিমাইজেশনের সময় ছোট ব্যাচ সাইজ ব্যবহার করুন
- প্রথমে উচ্চতর নির্ভুলতা দিয়ে কোয়ান্টাইজেশন চেষ্টা করুন (int8-এর পরিবর্তে int4)
- মডেল ক্যাশিংয়ের জন্য পর্যাপ্ত ডিস্ক স্পেস নিশ্চিত করুন

#### ৪. মডেল লোডিং ত্রুটি
- মডেল পথ এবং অ্যাক্সেস অনুমতি যাচাই করুন
- যাচাই করুন যে মডেলটি `trust_remote_code=True` প্রয়োজন কিনা
- নিশ্চিত করুন যে সমস্ত প্রয়োজনীয় মডেল ফাইল ডাউনলোড করা হয়েছে

### সাহায্য পাওয়া

- **ডকুমেন্টেশন**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **উদাহরণ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## অতিরিক্ত সম্পদ

### অফিসিয়াল লিঙ্ক
- **GitHub রিপোজিটরি**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime ডকুমেন্টেশন**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face উদাহরণ**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### কমিউনিটি উদাহরণ
- **Jupyter Notebooks**: Olive GitHub রিপোজিটরিতে উপলব্ধ — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: VS Code-এর জন্য AI Toolkit ওভারভিউ — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ব্লগ পোস্ট**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### সম্পর্কিত টুল
- **ONNX Runtime**: উচ্চ-প্রদর্শন ইনফারেন্স ইঞ্জিন — https://onnxruntime.ai/
- **Hugging Face Transformers**: অনেক সামঞ্জস্যপূর্ণ মডেলের উৎস — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: ক্লাউড-ভিত্তিক অপ্টিমাইজেশন ওয়ার্কফ্লো — https://learn.microsoft.com/azure/machine-learning/

## ➡️ পরবর্তী কী

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

