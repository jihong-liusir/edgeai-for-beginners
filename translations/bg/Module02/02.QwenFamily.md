<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:47:45+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "bg"
}
-->
# Раздел 2: Основи на семейството Qwen

Моделното семейство Qwen представлява цялостния подход на Alibaba Cloud към големите езикови модели и мултимодалния AI, демонстрирайки, че отворените модели могат да постигнат забележителна производителност, като същевременно са достъпни за различни сценарии на внедряване. Важно е да разберем как семейството Qwen предоставя мощни AI възможности с гъвкави опции за внедряване, като същевременно поддържа конкурентна производителност при разнообразни задачи.

## Ресурси за разработчици

### Репозитори на модели в Hugging Face
Избрани модели от семейството Qwen са достъпни чрез [Hugging Face](https://huggingface.co/models?search=qwen), предоставяйки достъп до някои варианти на тези модели. Можете да разгледате наличните варианти, да ги адаптирате за вашите специфични нужди и да ги внедрите чрез различни рамки.

### Инструменти за локална разработка
За локална разработка и тестване можете да използвате [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), за да стартирате наличните модели Qwen на вашата разработваща машина с оптимизирана производителност.

### Ресурси за документация
- [Документация за моделите Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Оптимизиране на моделите Qwen за внедряване на периферни устройства](https://github.com/microsoft/olive)

## Въведение

В този урок ще разгледаме семейството модели Qwen на Alibaba и неговите основни концепции. Ще обхванем еволюцията на семейството Qwen, иновативните методологии за обучение, които правят моделите Qwen ефективни, ключовите варианти в семейството и практическите приложения в различни сценарии.

## Цели на обучението

До края на този урок ще можете:

- Да разберете философията на дизайна и еволюцията на семейството модели Qwen на Alibaba
- Да идентифицирате ключовите иновации, които позволяват на моделите Qwen да постигат висока производителност при различни размери параметри
- Да разпознаете предимствата и ограниченията на различните варианти на моделите Qwen
- Да приложите знанията за моделите Qwen, за да изберете подходящи варианти за реални сценарии

## Разбиране на съвременния пейзаж на AI модели

Пейзажът на AI се е развил значително, като различни организации преследват различни подходи към разработката на езикови модели. Докато някои се фокусират върху затворени модели, други акцентират върху достъпността и прозрачността на отворените модели. Традиционният подход включва или масивни затворени модели, достъпни само чрез API, или отворени модели, които може да изостават в способностите си.

Тази парадигма създава предизвикателства за организациите, които търсят мощни AI възможности, като същевременно запазват контрол върху своите данни, разходи и гъвкавост на внедряване. Традиционният подход често изисква избор между върхова производителност и практически съображения за внедряване.

## Предизвикателството за достъпно AI съвършенство

Необходимостта от висококачествен, достъпен AI става все по-важна в различни сценарии. Помислете за приложения, които изискват гъвкави опции за внедряване за различни организационни нужди, икономически ефективни реализации, където разходите за API могат да станат значителни, многоезични способности за глобални приложения или специализирана експертиза в области като програмиране и математика.

### Основни изисквания за внедряване

Съвременните AI внедрения се сблъскват с няколко основни изисквания, които ограничават практическата приложимост:

- **Достъпност**: Наличност на отворен код за прозрачност и персонализация
- **Икономическа ефективност**: Разумни изисквания за изчислителни ресурси за различни бюджети
- **Гъвкавост**: Различни размери на модели за различни сценарии на внедряване
- **Глобален обхват**: Силни многоезични и междукултурни способности
- **Специализация**: Варианти, специфични за домейна, за конкретни случаи на употреба

## Философията на моделите Qwen

Семейството модели Qwen представлява цялостен подход към разработката на AI модели, като приоритизира достъпността на отворения код, многоезичните способности и практическото внедряване, като същевременно поддържа конкурентни характеристики на производителност. Моделите Qwen постигат това чрез разнообразни размери на модели, висококачествени методологии за обучение и специализирани варианти за различни домейни.

Семейството Qwen обхваща различни подходи, предназначени да предоставят опции в спектъра на производителност-ефективност, позволявайки внедряване от мобилни устройства до корпоративни сървъри, като същевременно предоставя значими AI възможности. Целта е да се демократизира достъпът до висококачествен AI, като се предоставя гъвкавост при избора на внедряване.

### Основни принципи на дизайна на Qwen

Моделите Qwen са изградени върху няколко основни принципа, които ги отличават от други семейства езикови модели:

- **Отворен код на първо място**: Пълна прозрачност и достъпност за изследвания и търговска употреба
- **Цялостно обучение**: Обучение върху масивни, разнообразни набори от данни, обхващащи множество езици и домейни
- **Мащабируема архитектура**: Различни размери на модели, съответстващи на различни изчислителни изисквания
- **Специализирано съвършенство**: Варианти, оптимизирани за конкретни задачи

## Ключови технологии, които позволяват семейството Qwen

### Масивно мащабно обучение

Един от определящите аспекти на семейството Qwen е мащабът на обучителните данни и изчислителните ресурси, инвестирани в разработката на модели. Моделите Qwen използват внимателно подбрани, многоезични набори от данни, обхващащи трилиони токени, предназначени да предоставят цялостни знания за света и способности за разсъждение.

Този подход работи чрез комбиниране на висококачествено уеб съдържание, академична литература, хранилища с код и многоезични ресурси. Методологията на обучение акцентира както върху широчината на знанията, така и върху дълбочината на разбирането в различни домейни и езици.

### Напреднало разсъждение и мислене

Последните модели Qwen включват сложни способности за разсъждение, които позволяват комплексно решаване на проблеми на няколко стъпки:

**Режим на мислене (Qwen3)**: Моделите могат да се ангажират с детайлно разсъждение стъпка по стъпка, преди да предоставят окончателни отговори, подобно на човешки подходи за решаване на проблеми.

**Дву-режимен режим**: Способност за превключване между режим за бърз отговор за прости запитвания и режим за дълбоко мислене за сложни проблеми.

**Интеграция на вериги от мисли**: Естествено включване на стъпки за разсъждение, които подобряват прозрачността и точността при сложни задачи.

### Архитектурни иновации

Семейството Qwen включва няколко архитектурни оптимизации, предназначени както за производителност, така и за ефективност:

**Мащабируем дизайн**: Консистентна архитектура в различни размери на модели, позволяваща лесно мащабиране и сравнение.

**Мултимодална интеграция**: Безпроблемна интеграция на текст, визия и аудио обработка в унифицирани архитектури.

**Оптимизация за внедряване**: Различни опции за квантизация и формати за внедряване за различни хардуерни конфигурации.

## Размери на модели и опции за внедряване

Съвременните среди за внедряване се възползват от гъвкавостта на моделите Qwen при различни изчислителни изисквания:

### Малки модели (0.5B-3B)

Qwen предоставя ефективни малки модели, подходящи за внедряване на периферни устройства, мобилни приложения и среди с ограничени ресурси, като същевременно поддържа впечатляващи способности.

### Средни модели (7B-32B)

Средните модели предлагат подобрени способности за професионални приложения, осигурявайки отличен баланс между производителност и изчислителни изисквания.

### Големи модели (72B+)

Модели в пълен мащаб предоставят върхова производителност за взискателни приложения, изследвания и корпоративни внедрения, изискващи максимални способности.

## Предимства на семейството модели Qwen

### Достъпност на отворения код

Моделите Qwen предоставят пълна прозрачност и възможности за персонализация, позволявайки на организациите да разбират, модифицират и адаптират моделите към своите специфични нужди без зависимост от доставчици.

### Гъвкавост на внедряване

Разнообразието от размери на модели позволява внедряване в различни хардуерни конфигурации, от мобилни устройства до високопроизводителни сървъри, предоставяйки на организациите гъвкавост в избора на AI инфраструктура.

### Многоезично съвършенство

Моделите Qwen се отличават в многоезичното разбиране и генериране, поддържайки десетки езици със специална сила в английски и китайски, което ги прави подходящи за глобални приложения.

### Конкурентна производителност

Моделите Qwen последователно постигат конкурентни резултати в бенчмаркове, като същевременно предоставят достъпност на отворения код, демонстрирайки, че отворените модели могат да се конкурират с затворените алтернативи.

### Специализирани способности

Варианти, специфични за домейна, като Qwen-Coder и Qwen-Math, предоставят специализирана експертиза, като същевременно поддържат общи способности за езиково разбиране.

## Практически примери и случаи на употреба

Преди да се потопим в техническите детайли, нека разгледаме някои конкретни примери за това, което моделите Qwen могат да постигнат:

### Пример за математическо разсъждение

Qwen-Math се отличава в решаването на математически проблеми стъпка по стъпка. Например, когато бъде попитан да реши сложен проблем с интеграли:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Пример за многоезична поддръжка

Моделите Qwen демонстрират силни многоезични способности в различни езици:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Пример за мултимодални способности

Qwen-VL може да обработва текст и изображения едновременно:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Пример за генериране на код

Qwen-Coder се отличава в генерирането и обяснението на код в множество програмни езици:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

Тази имплементация следва най-добрите практики с ясни имена на променливи, изчерпателна документация и ефективна логика.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Пример за внедряване на мобилно устройство с квантизация
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Зареждане на квантизиран модел за мобилно внедряване

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Еволюцията на семейството Qwen

### Qwen 1.0 и 1.5: Основни модели

Ранните модели Qwen установиха основните принципи на цялостно обучение и достъпност на отворения код:

- **Qwen-7B (7B параметри)**: Първоначално издание, фокусирано върху разбиране на китайски и английски език
- **Qwen-14B (14B параметри)**: Подобрени способности с подобрено разсъждение и знания
- **Qwen-72B (72B параметри)**: Голям мащабен модел, предоставящ върхова производителност
- **Серия Qwen1.5**: Разширена до множество размери (0.5B до 110B) с подобрено управление на дълги контексти

### Семейство Qwen2: Мултимодално разширение

Серията Qwen2 отбеляза значителен напредък както в езиковите, така и в мултимодалните способности:

- **Qwen2-0.5B до 72B**: Цялостен диапазон от езикови модели за различни нужди от внедряване
- **Qwen2-57B-A14B (MoE)**: Архитектура с микс от експерти за ефективно използване на параметри
- **Qwen2-VL**: Напреднали способности за визия и език за разбиране на изображения
- **Qwen2-Audio**: Способности за обработка и разбиране на аудио
- **Qwen2-Math**: Специализирано математическо разсъждение и решаване на проблеми

### Семейство Qwen2.5: Подобрена производителност

Серията Qwen2.5 донесе значителни подобрения във всички измерения:

- **Разширено обучение**: 18 трилиона токени обучителни данни за подобрени способности
- **Разширен контекст**: До 128K токени дължина на контекста, с Turbo вариант, поддържащ 1M токени
- **Подобрена специализация**: Подобрени варианти Qwen2.5-Coder и Qwen2.5-Math
- **По-добра многоезична поддръжка**: Подобрена производителност в 27+ езика

### Семейство Qwen3: Напреднало разсъждение

Последното поколение разширява границите на способностите за разсъждение и мислене:

- **Qwen3-235B-A22B**: Водещ модел с микс от експерти с общо 235B параметри
- **Qwen3-30B-A3B**: Ефективен MoE модел със силна производителност на активен параметър
- **Плътни модели**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B за различни сценарии на внедряване
- **Режим на мислене**: Хибриден подход за разсъждение, поддържащ както бързи отговори, така и дълбоко мислене
- **Многоезично съвършенство**: Поддръжка на 119 езика и диалекти
- **Подобрено обучение**: 36 трилиона токени разнообразни, висококачествени обучителни данни

## Приложения на моделите Qwen

### Корпоративни приложения

Организациите използват моделите Qwen за анализ на документи, автоматизация на обслужването на клиенти, помощ при генериране на код и приложения за бизнес интелигентност. Отвореният характер позволява персонализация за специфични бизнес нужди, като същевременно запазва поверителността и контрола върху данните.

### Мобилни и периферни изчисления

Мобилните приложения използват моделите Qwen за превод в реално време, интелигентни асистенти, генериране на съдържание и персонализирани препоръки. Диапазонът от размери на модели позволява внедряване от мобилни устройства до периферни сървъри.

### Образователни технологии

Образователните платформи използват моделите Qwen за персонализирано обучение, автоматизирано генериране на съдържание, помощ при изучаване на езици и интерактивни образователни преживявания. Специализирани модели като Qwen-M
- Qwen3-235B-A22B постига конкурентни резултати в оценките на бенчмаркове за кодиране, математика и общи способности в сравнение с други водещи модели като DeepSeek-R1, o1, o3-mini, Grok-3 и Gemini-2.5-Pro.  
- Qwen3-30B-A3B превъзхожда QwQ-32B с 10 пъти повече активирани параметри.  
- Qwen3-4B може да се конкурира с производителността на Qwen2.5-72B-Instruct.  

**Постижения в ефективността:**  
- Базовите модели Qwen3-MoE постигат сходна производителност с плътните базови модели Qwen2.5, използвайки само 10% от активните параметри.  
- Значителни икономии на разходи както при обучение, така и при изводи в сравнение с плътните модели.  

**Многоезични възможности:**  
- Моделите Qwen3 поддържат 119 езика и диалекта.  
- Силна производителност в разнообразни езикови и културни контексти.  

**Мащаб на обучение:**  
- Qwen3 използва почти два пъти повече данни, с приблизително 36 трилиона токена, обхващащи 119 езика и диалекта, в сравнение с 18 трилиона токена на Qwen2.5.  

### Матрица за сравнение на модели  

| Серия модели | Диапазон на параметрите | Дължина на контекста | Основни предимства | Най-добри приложения |  
|--------------|--------------------------|----------------------|--------------------|-----------------------|  
| **Qwen2.5** | 0.5B-72B | 32K-128K | Балансирана производителност, многоезичност | Общи приложения, внедряване в производство |  
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Генериране на код, програмиране | Софтуерна разработка, помощ при кодиране |  
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Математическо разсъждение | Образователни платформи, STEM приложения |  
| **Qwen2.5-VL** | Различни | Променлива | Разбиране на визия и език | Мултимодални приложения, анализ на изображения |  
| **Qwen3** | 0.6B-235B | Променлива | Напреднало разсъждение, режим на мислене | Сложно разсъждение, изследователски приложения |  
| **Qwen3 MoE** | 30B-235B общо | Променлива | Ефективна производителност в голям мащаб | Корпоративни приложения, нужди от висока производителност |  

## Ръководство за избор на модел  

### За основни приложения  
- **Qwen2.5-0.5B/1.5B**: Мобилни приложения, крайни устройства, приложения в реално време.  
- **Qwen2.5-3B/7B**: Общи чатботове, генериране на съдържание, системи за въпроси и отговори.  

### За математически и логически задачи  
- **Qwen2.5-Math**: Решаване на математически проблеми и STEM образование.  
- **Qwen3 с режим на мислене**: Сложно разсъждение, изискващо анализ стъпка по стъпка.  

### За програмиране и разработка  
- **Qwen2.5-Coder**: Генериране на код, дебъгинг, помощ при програмиране.  
- **Qwen3**: Напреднали задачи за програмиране с логически способности.  

### За мултимодални приложения  
- **Qwen2.5-VL**: Разбиране на изображения, визуални въпроси и отговори.  
- **Qwen-Audio**: Обработка на аудио и разбиране на реч.  

### За корпоративно внедряване  
- **Qwen2.5-32B/72B**: Високопроизводително езиково разбиране.  
- **Qwen3-235B-A22B**: Максимални възможности за взискателни приложения.  

## Платформи за внедряване и достъпност  

### Облачни платформи  
- **Hugging Face Hub**: Обширно хранилище за модели с подкрепа от общността.  
- **ModelScope**: Платформа за модели на Alibaba с инструменти за оптимизация.  
- **Различни облачни доставчици**: Поддръжка чрез стандартни ML платформи.  

### Локални рамки за разработка  
- **Transformers**: Стандартна интеграция на Hugging Face за лесно внедряване.  
- **vLLM**: Високопроизводително обслужване за производствени среди.  
- **Ollama**: Опростено локално внедряване и управление.  
- **ONNX Runtime**: Кросплатформена оптимизация за различен хардуер.  
- **llama.cpp**: Ефективна C++ имплементация за разнообразни платформи.  

### Образователни ресурси  
- **Документация на Qwen**: Официална документация и карти на модели.  
- **Hugging Face Model Hub**: Интерактивни демонстрации и примери от общността.  
- **Научни статии**: Технически публикации в arxiv за задълбочено разбиране.  
- **Форуми на общността**: Активна подкрепа и дискусии в общността.  

### Започване с моделите Qwen  

#### Платформи за разработка  
1. **Hugging Face Transformers**: Започнете със стандартна Python интеграция.  
2. **ModelScope**: Изследвайте оптимизираните инструменти за внедряване на Alibaba.  
3. **Локално внедряване**: Използвайте Ollama или директни transformers за локално тестване.  

#### Път за обучение  
1. **Разберете основните концепции**: Изучете архитектурата и възможностите на семейството Qwen.  
2. **Експериментирайте с варианти**: Опитайте различни размери на модели, за да разберете компромисите в производителността.  
3. **Практикувайте внедряване**: Внедрете модели в среди за разработка.  
4. **Оптимизирайте внедряването**: Фина настройка за производствени случаи.  

#### Най-добри практики  
- **Започнете с малки модели**: Започнете с по-малки модели (1.5B-7B) за първоначална разработка.  
- **Използвайте шаблони за чат**: Прилагайте правилно форматиране за оптимални резултати.  
- **Следете ресурсите**: Проследявайте използването на паметта и скоростта на извода.  
- **Обмислете специализация**: Изберете варианти, специфични за домейна, когато е подходящо.  

## Модели за напреднала употреба  

### Примери за фина настройка  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```
  
### Специализирано инженерство на подсказки  

**За сложни задачи за разсъждение:**  
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```
  
**За генериране на код с контекст:**  
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```
  
### Многоезични приложения  

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```
  
### 🔧 Модели за производствено внедряване  

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```
  

## Стратегии за оптимизация на производителността  

### Оптимизация на паметта  

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```
  
### Оптимизация на извода  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```
  

## Най-добри практики и насоки  

### Сигурност и поверителност  

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```
  
### Мониторинг и оценка  

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```
  

## Заключение  

Семейството модели Qwen представлява цялостен подход към демократизацията на AI технологиите, като същевременно поддържа конкурентна производителност в разнообразни приложения. Чрез ангажимента си към достъпност с отворен код, многоезични възможности и гъвкави опции за внедряване, Qwen позволява на организациите и разработчиците да използват мощни AI възможности, независимо от техните ресурси или специфични изисквания.  

### Основни изводи  

**Отличие с отворен код**: Qwen демонстрира, че моделите с отворен код могат да постигнат производителност, конкурентна на собствените алтернативи, като същевременно предоставят прозрачност, персонализация и контрол.  

**Мащабируема архитектура**: Диапазонът от 0.5B до 235B параметри позволява внедряване в целия спектър от изчислителни среди, от мобилни устройства до корпоративни клъстери.  

**Специализирани възможности**: Варианти, специфични за домейна, като Qwen-Coder, Qwen-Math и Qwen-VL, предоставят специализирана експертиза, като същевременно поддържат общо езиково разбиране.  

**Глобална достъпност**: Силната многоезична поддръжка на над 119 езика прави Qwen подходящ за международни приложения и разнообразни потребителски бази.  

**Непрекъснати иновации**: Еволюцията от Qwen 1.0 до Qwen3 показва последователно подобрение на възможностите, ефективността и опциите за внедряване.  

### Бъдещи перспективи  

С развитието на семейството Qwen можем да очакваме:  
- **Подобрена ефективност**: Продължаваща оптимизация за по-добри съотношения производителност-параметри.  
- **Разширени мултимодални възможности**: Интеграция на по-сложна обработка на визия, аудио и текст.  
- **Подобрено разсъждение**: Напреднали механизми за мислене и способности за решаване на многоетапни проблеми.  
- **По-добри инструменти за внедряване**: Подобрени рамки и инструменти за оптимизация за разнообразни сценарии на внедряване.  
- **Растеж на общността**: Разширена екосистема от инструменти, приложения и приноси от общността.  

### Следващи стъпки  

Независимо дали създавате чатбот, разработвате образователни инструменти, създавате помощници за кодиране или работите върху многоезични приложения, семейството Qwen предоставя мащабируеми решения със силна подкрепа от общността и изчерпателна документация.  

За най-новите актуализации, издания на модели и подробна техническа документация, посетете официалните хранилища на Qwen в Hugging Face и изследвайте активните дискусии и примери от общността.  

Бъдещето на AI разработката се крие в достъпни, прозрачни и мощни инструменти, които позволяват иновации във всички сектори и мащаби. Семейството Qwen въплъщава тази визия, предоставяйки на организациите и разработчиците основата за изграждане на следващото поколение AI приложения.  

## Допълнителни ресурси  

- **Официална документация**: [Документация на Qwen](https://qwen.readthedocs.io/)  
- **Хранилище на модели**: [Колекции на Qwen в Hugging Face](https://huggingface.co/collections/Qwen/)  
- **Научни публикации**: [Научни публикации за Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)  
- **Общност**: [Дискусии и проблеми в GitHub](https://github.com/QwenLM/)  
- **Платформа ModelScope**: [ModelScope на Alibaba](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)  

## Резултати от обучението  

След завършване на този модул ще можете:  
1. Да обясните архитектурните предимства на семейството модели Qwen и подхода му с отворен код.  
2. Да изберете подходящия вариант на Qwen въз основа на специфични изисквания за приложение и ограничения на ресурсите.  
3. Да внедрите модели Qwen в различни сценарии на внедряване с оптимизирани конфигурации.  
4. Да приложите техники за квантизация и оптимизация за подобряване на производителността на моделите Qwen.  
5. Да оцените компромисите между размер на модела, производителност и възможности в семейството Qwen.  

## Какво следва  

- [03: Основи на семейството Gemma](03.GemmaFamily.md)  

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.