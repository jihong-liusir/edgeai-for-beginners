{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# Сесия 5 – Мултиагентен оркестратор\n",
    "\n",
    "Демонстрира проста двуагентна последователност (Изследовател -> Редактор) с използване на Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### Обяснение: Инсталиране на зависимости\n",
    "Инсталира `foundry-local-sdk` и `openai`, необходими за достъп до локални модели и завършване на чатове. Идемпотентно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# Сценарий\n",
    "Реализира минимален модел на оркестратор с два агента:\n",
    "- **Агент изследовател** събира кратки фактически точки\n",
    "- **Агент редактор** пренаписва за яснота на изпълнителско ниво\n",
    "\n",
    "Демонстрира споделена памет за всеки агент, последователно предаване на междинни резултати и проста функция за обработка. Може да се разшири с повече роли (напр. Критик, Проверител) или паралелни клонове.\n",
    "\n",
    "**Променливи на средата:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - Модел по подразбиране за използване (по подразбиране: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - Основен модел за агента (замества ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - Модел за агента редактор (по подразбиране основния)\n",
    "\n",
    "**Референция за SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**Как работи:**\n",
    "1. **FoundryLocalManager** автоматично стартира услугата Foundry Local\n",
    "2. Изтегля и зарежда посочения модел (или използва кеширана версия)\n",
    "3. Осигурява съвместим крайна точка с OpenAI за взаимодействие\n",
    "4. Всеки агент може да използва различен модел за специализирани задачи\n",
    "5. Вградена логика за повторение обработва временни грешки безпроблемно\n",
    "\n",
    "**Основни характеристики:**\n",
    "- ✅ Автоматично откриване и инициализация на услугата\n",
    "- ✅ Управление на жизнения цикъл на модела (изтегляне, кеширане, зареждане)\n",
    "- ✅ Съвместимост с OpenAI SDK за познат API\n",
    "- ✅ Поддръжка на множество модели за специализация на агенти\n",
    "- ✅ Надеждно обработване на грешки с логика за повторение\n",
    "- ✅ Локално извеждане (без необходимост от облачен API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### Обяснение: Основни импорти и типизация\n",
    "Въвежда dataclasses за съхранение на съобщенията на агентите и подсказки за типизация за по-голяма яснота. Импортира Foundry Local manager + OpenAI клиент за последващи действия на агентите.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### Обяснение: Инициализация на модела (SDK модел)\n",
    "Използва Foundry Local Python SDK за надеждно управление на модели:\n",
    "- **FoundryLocalManager(alias)** - Автоматично стартира услугата и зарежда модела чрез псевдоним\n",
    "- **get_model_info(alias)** - Превръща псевдонима в конкретен ID на модела\n",
    "- **manager.endpoint** - Осигурява крайна точка за клиента на OpenAI\n",
    "- **manager.api_key** - Осигурява API ключ (опционално за локално използване)\n",
    "- Поддържа отделни модели за различни агенти (основен срещу редактор)\n",
    "- Вградена логика за повторение с експоненциално увеличаване на интервала за устойчивост\n",
    "- Проверка на връзката, за да се гарантира, че услугата е готова\n",
    "\n",
    "**Основен SDK модел:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**Управление на жизнения цикъл:**\n",
    "- Мениджърите се съхраняват глобално за правилно почистване\n",
    "- Всеки агент може да използва различен модел за специализация\n",
    "- Автоматично откриване на услуги и управление на връзката\n",
    "- Плавно повторение с експоненциално увеличаване на интервала при неуспехи\n",
    "\n",
    "Това гарантира правилна инициализация преди започване на оркестрацията на агентите.\n",
    "\n",
    "**Референция:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### Обяснение: Класове Agent и Memory\n",
    "Определя лек клас `AgentMsg` за записи в паметта и `Agent`, който обхваща:\n",
    "- **Роля на системата** - Персона и инструкции на агента\n",
    "- **История на съобщенията** - Поддържа контекста на разговора\n",
    "- **Метод act()** - Изпълнява действия с подходящо управление на грешки\n",
    "\n",
    "Агентът може да използва различни модели (основна срещу редакторска) и поддържа изолиран контекст за всеки агент. Този модел позволява:\n",
    "- Запазване на паметта между действията\n",
    "- Гъвкаво задаване на модели за всеки агент\n",
    "- Изолация на грешки и възстановяване\n",
    "- Лесно свързване и оркестрация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### Обяснение: Оркестриран конвейер\n",
    "Създава два специализирани агента:\n",
    "- **Изследовател**: Използва основния модел, събира фактическа информация\n",
    "- **Редактор**: Може да използва отделен модел (ако е конфигуриран), усъвършенства и пренаписва\n",
    "\n",
    "Функцията `pipeline`:\n",
    "1. Изследователят събира сурова информация\n",
    "2. Редакторът я усъвършенства в готов за изпълнение резултат\n",
    "3. Връща както междинните, така и окончателните резултати\n",
    "\n",
    "Този модел позволява:\n",
    "- Специализация на моделите (различни модели за различни роли)\n",
    "- Подобряване на качеството чрез многоетапна обработка\n",
    "- Проследимост на трансформацията на информацията\n",
    "- Лесно разширяване с повече агенти или паралелна обработка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### Обяснение: Изпълнение на тръбопровода и резултати\n",
    "Изпълнява многослоен тръбопровод с агенти върху въпрос, свързан с съответствие и латентност, за да демонстрира:\n",
    "- Многоетапна трансформация на информацията\n",
    "- Специализация и сътрудничество между агентите\n",
    "- Подобряване на качеството на изхода чрез прецизиране\n",
    "- Проследимост (запазване както на междинните, така и на крайните резултати)\n",
    "\n",
    "**Структура на резултата:**\n",
    "- `question` - Оригиналният въпрос на потребителя\n",
    "- `research` - Суров изход от изследването (фактически точки)\n",
    "- `final` - Прецизирано резюме за ръководители\n",
    "- `models` - Кои модели са използвани за всеки етап\n",
    "\n",
    "**Идеи за разширение:**\n",
    "1. Добавяне на агент Критик за преглед на качеството\n",
    "2. Реализиране на паралелни агенти за изследване на различни аспекти\n",
    "3. Добавяне на агент Проверител за проверка на фактите\n",
    "4. Използване на различни модели за различни нива на сложност\n",
    "5. Реализиране на обратни връзки за итеративно подобрение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### Напреднали: Персонализирана конфигурация на агент\n",
    "\n",
    "Опитайте да персонализирате поведението на агента, като промените променливите на средата преди да изпълните клетката за инициализация:\n",
    "\n",
    "**Налични модели:**\n",
    "- Използвайте `foundry model ls` в терминала, за да видите всички налични модели\n",
    "- Примери: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b и др.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-08T14:39:19+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}