<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T01:30:54+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "bg"
}
-->
# Сесия 3: Откриване и управление на модели с отворен код

## Преглед

Тази сесия се фокусира върху практическо откриване и управление на модели с Foundry Local. Ще научите как да изброявате наличните модели, да тествате различни опции и да разбирате основните характеристики на производителността. Подходът акцентира върху практическо изследване с помощта на foundry CLI, за да ви помогне да изберете правилните модели за вашите случаи на употреба.

## Цели на обучението

- Овладяване на командите на foundry CLI за откриване и управление на модели
- Разбиране на кеша на модели и моделите за локално съхранение
- Научаване как бързо да тествате и сравнявате различни модели
- Установяване на практични работни процеси за избор и оценка на модели
- Изследване на разрастващата се екосистема от модели, достъпни чрез Foundry Local

## Предварителни изисквания

- Завършена Сесия 1: Запознаване с Foundry Local
- Инсталиран и достъпен Foundry Local CLI
- Достатъчно място за съхранение за изтегляне на модели (моделите могат да варират от 1GB до 20GB+)
- Основно разбиране на типовете модели и случаи на употреба

## Преглед

Тази сесия разглежда как да интегрирате модели с отворен код в Foundry Local.

## Част 6: Практическо упражнение

### Упражнение: Откриване и сравнение на модели

Създайте свой собствен скрипт за оценка на модели, базиран на Пример 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### Вашата задача

1. **Стартирайте скрипта Пример 03**: `samples\03\list_and_bench.cmd`
2. **Опитайте различни модели**: Тествайте поне 3 различни модела
3. **Сравнете производителността**: Отбележете разликите в скоростта и качеството на отговорите
4. **Документирайте откритията**: Създайте проста таблица за сравнение

### Примерен формат за сравнение

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## Част 7: Отстраняване на проблеми и добри практики

### Чести проблеми и решения

**Моделът не стартира:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**Недостатъчна памет:**
- Започнете с по-малки модели (`phi-4-mini`)
- Затворете други приложения
- Надградете RAM, ако често достигате лимити

**Бавна производителност:**
- Уверете се, че моделът е напълно зареден (проверете подробния изход)
- Затворете ненужни приложения във фонов режим
- Помислете за по-бързо съхранение (SSD)

### Добри практики

1. **Започнете с малки модели**: Започнете с `phi-4-mini`, за да валидирате настройката
2. **Един модел наведнъж**: Спрете предишните модели, преди да стартирате нови
3. **Следете ресурсите**: Наблюдавайте използването на паметта
4. **Тествайте последователно**: Използвайте едни и същи подсказки за справедливи сравнения
5. **Документирайте резултатите**: Водете бележки за производителността на моделите за вашите случаи на употреба

## Част 8: Следващи стъпки и ресурси

### Подготовка за Сесия 4

- **Фокус на Сесия 4**: Инструменти и техники за оптимизация
- **Предварителни изисквания**: Умения за превключване на модели и основно тестване на производителността
- **Препоръчително**: Идентифицирайте 2-3 любими модела от тази сесия

### Допълнителни ресурси

- **[Документация за Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Официална документация
- **[CLI Референция](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Пълна справка за команди
- **[Model Mondays](https://aka.ms/model-mondays)**: Седмични акценти на модели
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Общност и проблеми
- **[Пример 03: Откриване на модели](samples/03/README.md)**: Практически примерен скрипт

### Основни изводи

✅ **Откриване на модели**: Използвайте `foundry model list`, за да изследвате наличните модели  
✅ **Бързо тестване**: Шаблонът `list_and_bench.cmd` за бърза оценка  
✅ **Мониторинг на производителността**: Основно наблюдение на използването на ресурси и времето за отговор  
✅ **Избор на модели**: Практически насоки за избор на модели според случая на употреба  
✅ **Управление на кеша**: Разбиране на процедурите за съхранение и почистване  

Сега имате практически умения за откриване, тестване и избор на подходящи модели за вашите AI приложения, използвайки лесния CLI подход на Foundry Local.

## Цели на обучението

- Откриване и оценка на модели с отворен код за локално използване
- Компилиране и стартиране на избрани модели от Hugging Face в Foundry Local
- Прилагане на стратегии за избор на модели според точност, латентност и нужди от ресурси
- Управление на модели локално с кеш и версиониране

## Част 1: Откриване на модели с Foundry CLI

### Основни команди за управление на модели

Foundry CLI предоставя лесни команди за откриване и управление на модели:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### Стартиране на първите ви модели

Започнете с популярни, добре тествани модели, за да разберете характеристиките на производителността:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**Забележка:** Флагът `--verbose` предоставя подробна информация за стартиране, включително:
- Напредък при изтегляне на модела (при първо стартиране)
- Детайли за разпределение на паметта
- Информация за свързване на услугата
- Метрики за инициализация на производителността

### Разбиране на категориите модели

**Малки езикови модели (SLMs):**
- `phi-4-mini`: Бърз, ефективен, подходящ за общи чатове
- `phi-4`: По-способна версия с по-добро разсъждение

**Средни модели:**
- `qwen2.5-7b`: Отлично разсъждение и по-дълъг контекст
- `deepseek-r1-7b`: Оптимизиран за генериране на код

**Големи модели:**
- `llama-3.2`: Най-новият модел с отворен код от Meta
- `qwen2.5-14b`: Разсъждение на корпоративно ниво

## Част 2: Бързо тестване и сравнение на модели

### Подход на Пример 03: Прост списък и оценка

Базирано на шаблона Пример 03, ето минималния работен процес:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### Тестване на производителността на моделите

След като моделът е стартиран, тествайте го с последователни подсказки:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### Алтернатива за тестване с PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## Част 3: Управление на кеша и съхранението на модели

### Разбиране на кеша на модели

Foundry Local автоматично управлява изтеглянията и кеширането на модели:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### Съображения за съхранение на модели

**Типични размери на модели:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Най-добри практики за съхранение:**
- Дръжте 2-3 модела кеширани за бързо превключване
- Премахнете неизползваните модели, за да освободите място: `foundry cache clean`
- Наблюдавайте използването на диска, особено на по-малки SSD дискове
- Обмислете компромисите между размер на модела и способности

### Мониторинг на производителността на модели

Докато моделите работят, наблюдавайте системните ресурси:

**Windows Task Manager:**
- Следете използването на паметта (моделите остават заредени в RAM)
- Наблюдавайте използването на CPU по време на изводи
- Проверете дисковия I/O при първоначално зареждане на модела

**Мониторинг от командния ред:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## Част 4: Практически насоки за избор на модели

### Избор на модели според случая на употреба

**За общи чатове и въпроси и отговори:**
- Започнете с: `phi-4-mini` (бърз, ефективен)
- Надградете до: `phi-4` (по-добро разсъждение)
- Напреднали: `qwen2.5-7b` (по-дълъг контекст)

**За генериране на код:**
- Препоръчителен: `deepseek-r1-7b`
- Алтернатива: `qwen2.5-7b` (също добър за код)

**За сложни разсъждения:**
- Най-добър: `qwen2.5-7b` или `qwen2.5-14b`
- Бюджетен вариант: `phi-4`

### Ръководство за хардуерни изисквания

**Минимални системни изисквания:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**Препоръчителни за най-добра производителност:**
- 32GB+ RAM за удобно превключване между модели
- SSD съхранение за по-бързо зареждане на модели
- Модерен CPU с добра производителност на единична нишка
- Поддръжка на NPU (Windows 11 Copilot+ PC) за ускорение

### Работен процес за превключване на модели

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## Част 5: Простото оценяване на модели

### Основно тестване на производителността

Ето един лесен подход за сравнение на производителността на модели:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### Ръчна оценка на качеството

За всеки модел, тествайте с последователни подсказки и оценявайте ръчно:

**Тестови подсказки:**
1. "Обяснете квантовото изчисление с прости думи."
2. "Напишете Python функция за сортиране на списък."
3. "Какви са плюсовете и минусите на дистанционната работа?"
4. "Обобщете ползите от edge AI."

**Критерии за оценка:**
- **Точност**: Дали информацията е правилна?
- **Яснота**: Дали обяснението е лесно за разбиране?
- **Пълнота**: Дали отговаря на целия въпрос?
- **Скорост**: Колко бързо отговаря?

### Мониторинг на използването на ресурси

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## Част 6: Следващи стъпки

- Абонирайте се за Model Mondays за нови модели и съвети: https://aka.ms/model-mondays
- Споделете откритията си с екипа в `models.json`
- Подгответе се за Сесия 4: сравнение на LLMs срещу SLMs, локално срещу облачно използване и практически демонстрации

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.