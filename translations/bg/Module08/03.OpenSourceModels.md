<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:01:31+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "bg"
}
-->
# Сесия 3: Модели с отворен код във Foundry Local

## Преглед

Тази сесия разглежда как да интегрирате модели с отворен код във Foundry Local: избор на модели от общността, интегриране на съдържание от Hugging Face и прилагане на стратегии „донеси свой собствен модел“ (BYOM). Ще откриете и серията Model Mondays за непрекъснато обучение и откриване на модели.

Референции:
- Документация за Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Компилиране на модели от Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Цели на обучението
- Откриване и оценка на модели с отворен код за локално изпълнение
- Компилиране и стартиране на избрани модели от Hugging Face във Foundry Local
- Прилагане на стратегии за избор на модели според точност, латентност и ресурсни нужди
- Управление на модели локално с кеширане и версиониране

## Част 1: Откриване и избор на модели (Стъпка по стъпка)

Стъпка 1) Списък на наличните модели в локалния каталог
```cmd
foundry model list
```

Стъпка 2) Бърз тест на два кандидата (автоматично изтегляне при първо стартиране)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Стъпка 3) Записване на основни метрики
- Наблюдавайте латентността (субективно) и качеството за фиксиран подканващ текст
- Следете използването на паметта чрез Task Manager, докато всеки модел работи

## Част 2: Стартиране на модели от каталога чрез CLI (Стъпка по стъпка)

Стъпка 1) Стартиране на модел
```cmd
foundry model run llama-3.2
```

Стъпка 2) Изпращане на тестов подканващ текст чрез OpenAI-съвместимия край
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```

## Част 3: BYOM – Компилиране на модели от Hugging Face (Стъпка по стъпка)

Следвайте официалното ръководство за компилиране на модели. Основен процес по-долу — вижте статията в Microsoft Learn за точните команди и поддържаните конфигурации.

Стъпка 1) Подготовка на работна директория
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Стъпка 2) Компилиране на поддържан HF модел
- Използвайте стъпките от документацията, за да конвертирате и поставите компилирания ONNX модел в директорията `models`
- Потвърдете с:
```cmd
foundry cache ls
```
Трябва да видите името на компилирания модел (например, `llama-3.2`).

Стъпка 3) Стартиране на компилирания модел
```cmd
foundry model run llama-3.2 --verbose
```

Бележки:
- Уверете се, че имате достатъчно дисково пространство и RAM за компилиране и стартиране
- Започнете с по-малки модели, за да валидирате процеса, след това преминете към по-големи

## Част 4: Практическа селекция на модели (Стъпка по стъпка)

Стъпка 1) Създаване на регистър `models.json`
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Стъпка 2) Малък скрипт за селекция
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```

## Част 5: Практически бенчмаркове (Стъпка по стъпка)

Стъпка 1) Прост бенчмарк за латентност
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Стъпка 2) Проверка на качеството
- Използвайте фиксиран набор от подканващи текстове, запишете изходите в CSV/JSON
- Ръчно оценете плавност, релевантност и коректност (1–5)

## Част 6: Следващи стъпки
- Абонирайте се за Model Mondays за нови модели и съвети: https://aka.ms/model-mondays
- Споделете откритията си в екипния `models.json`
- Подгответе се за Сесия 4: сравнение на LLMs срещу SLMs, локално срещу облачно изпълнение и практически демонстрации

---

