<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-19T00:44:55+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "bg"
}
-->
# Раздел 4: Задълбочен преглед на Apple MLX Framework

## Съдържание
1. [Въведение в Apple MLX](../../../Module04)
2. [Основни функции за разработка на LLM](../../../Module04)
3. [Ръководство за инсталация](../../../Module04)
4. [Първи стъпки с MLX](../../../Module04)
5. [MLX-LM: Езикови модели](../../../Module04)
6. [Работа с големи езикови модели](../../../Module04)
7. [Интеграция с Hugging Face](../../../Module04)
8. [Конвертиране и квантизация на модели](../../../Module04)
9. [Фина настройка на езикови модели](../../../Module04)
10. [Разширени функции за LLM](../../../Module04)
11. [Най-добри практики за LLM](../../../Module04)
12. [Отстраняване на проблеми](../../../Module04)
13. [Допълнителни ресурси](../../../Module04)

## Въведение в Apple MLX

Apple MLX е рамка, създадена специално за ефективно и гъвкаво машинно обучение на Apple Silicon, разработена от Apple Machine Learning Research. Пусната през декември 2023 г., MLX представлява отговорът на Apple на рамки като PyTorch и TensorFlow, с особен акцент върху мощните възможности за големи езикови модели на Mac компютри.

### Какво прави MLX специален за LLM?

MLX е проектиран да използва напълно обединената архитектура на паметта на Apple Silicon, което го прави особено подходящ за локално изпълнение и фина настройка на големи езикови модели на Mac компютри. Рамката елиминира много от проблемите със съвместимостта, с които традиционно се сблъскват Mac потребителите при работа с LLM.

### Кой трябва да използва MLX за LLM?

- **Mac потребители**, които искат да изпълняват LLM локално без зависимост от облака
- **Изследователи**, експериментиращи с фина настройка и персонализация на езикови модели
- **Разработчици**, изграждащи AI приложения с езикови модели
- **Всеки**, който иска да използва Apple Silicon за задачи като генериране на текст, чат и езикови анализи

## Основни функции за разработка на LLM

### 1. Обединена архитектура на паметта
Обединената памет на Apple Silicon позволява MLX да обработва ефективно големи езикови модели без излишно копиране на памет, което е типично за други рамки. Това означава, че можете да работите с по-големи модели на същия хардуер.

### 2. Оптимизация за Apple Silicon
MLX е изграден от основи за чиповете от серията M на Apple, осигурявайки оптимална производителност за трансформаторни архитектури, които често се използват в езиковите модели.

### 3. Поддръжка на квантизация
Вградената поддръжка за 4-битова и 8-битова квантизация намалява изискванията за памет, като същевременно запазва качеството на модела, позволявайки изпълнение на по-големи модели на потребителски хардуер.

### 4. Интеграция с Hugging Face
Безпроблемната интеграция с екосистемата на Hugging Face предоставя достъп до хиляди предварително обучени езикови модели с лесни инструменти за конвертиране.

### 5. Фина настройка с LoRA
Поддръжката на Low-Rank Adaptation (LoRA) позволява ефективна фина настройка на големи модели с минимални изчислителни ресурси.

## Ръководство за инсталация

### Системни изисквания
- **macOS 13.0+** (за оптимизация на Apple Silicon)
- **Python 3.8+**
- **Apple Silicon** (серии M1, M2, M3, M4)
- **Нативна ARM среда** (не под Rosetta)
- **8GB+ RAM** (16GB+ препоръчително за по-големи модели)

### Бърза инсталация за LLM

Най-лесният начин да започнете с езикови модели е да инсталирате MLX-LM:

```bash
pip install mlx-lm
```

Тази единична команда инсталира както основната рамка MLX, така и инструментите за езикови модели.

### Настройка на виртуална среда (Препоръчително)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Допълнителни зависимости за аудио модели

Ако планирате да работите с модели за реч като Whisper:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## Първи стъпки с MLX

### Вашият първи езиков модел

Нека започнем с прост пример за генериране на текст:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Пример с Python API

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Разбиране на зареждането на модели

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Езикови модели

### Поддържани архитектури на модели

MLX-LM поддържа широк набор от популярни архитектури на езикови модели:

- **LLaMA и LLaMA 2** - Основни модели на Meta
- **Mistral и Mixtral** - Ефективни и мощни модели
- **Phi-3** - Компактни езикови модели на Microsoft
- **Qwen** - Многоезични модели на Alibaba
- **Code Llama** - Специализирани за генериране на код
- **Gemma** - Отворени езикови модели на Google

### Интерфейс на командния ред

Интерфейсът на командния ред на MLX-LM предоставя мощни инструменти за работа с езикови модели:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Python API за напреднали случаи

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Работа с големи езикови модели

### Модели за генериране на текст

#### Генериране за единичен отговор
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Следване на инструкции
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Креативно писане
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Многократни разговори

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Интеграция с Hugging Face

### Намиране на модели, съвместими с MLX

MLX работи безпроблемно с екосистемата на Hugging Face:

- **Разгледайте MLX модели**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX общност**: https://huggingface.co/mlx-community (предварително конвертирани модели)
- **Оригинални модели**: Повечето LLaMA, Mistral, Phi и Qwen модели работят с конвертиране

### Зареждане на модели от Hugging Face

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Изтегляне на модели за офлайн употреба

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Конвертиране и квантизация на модели

### Конвертиране на Hugging Face модели към MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Разбиране на квантизацията

Квантизацията намалява размера на модела и използването на памет с минимална загуба на качество:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Персонализирана квантизация

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Фина настройка на езикови модели

### Фина настройка с LoRA (Low-Rank Adaptation)

MLX поддържа ефективна фина настройка с LoRA, което позволява адаптиране на големи модели с минимални изчислителни ресурси:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Подготовка на тренировъчни данни

Създайте JSON файл с вашите тренировъчни примери:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Команда за фина настройка

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Използване на фино настроени модели

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Разширени функции за LLM

### Кеширане на подсказки за ефективност

За повторно използване на същия контекст, MLX поддържа кеширане на подсказки за подобряване на производителността:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Поточно генериране на текст

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Работа с модели за генериране на код

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Работа с чат модели

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Най-добри практики за LLM

### Управление на паметта

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Насоки за избор на модели

**За експерименти и обучение:**
- Използвайте 4-битови квантизирани модели (напр. `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Започнете с по-малки модели като Phi-3-mini

**За производствени приложения:**
- Вземете предвид компромиса между размер на модела и качество
- Тествайте както квантизирани, така и модели с пълна прецизност
- Провеждайте тестове за вашите специфични случаи

**За специфични задачи:**
- **Генериране на код**: CodeLlama, Code Llama Instruct
- **Общ чат**: Mistral-7B-Instruct, Phi-3
- **Многоезични задачи**: Qwen модели
- **Креативно писане**: По-високи настройки на температурата с Mistral или LLaMA

### Най-добри практики за инженеринг на подсказки

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Оптимизация на производителността

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Отстраняване на проблеми

### Често срещани проблеми и решения

#### Проблеми с инсталацията

**Проблем**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Решение**: Използвайте нативен ARM Python или Miniconda:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Проблеми с паметта

**Проблем**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Проблеми с зареждането на модели

**Проблем**: Моделът не се зарежда или генерира лоши резултати
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Проблеми с производителността

**Проблем**: Бавна скорост на генериране
- Затворете други приложения, които използват много памет
- Използвайте квантизирани модели, когато е възможно
- Уверете се, че не работите под Rosetta
- Проверете наличната памет преди зареждане на модели

### Съвети за дебъгване

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Допълнителни ресурси

### Официална документация и хранилища

- **MLX GitHub хранилище**: https://github.com/ml-explore/mlx
- **MLX-LM примери**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX документация**: https://ml-explore.github.io/mlx/
- **Интеграция с Hugging Face MLX**: https://huggingface.co/docs/hub/en/mlx

### Колекции от модели

- **MLX общностни модели**: https://huggingface.co/mlx-community
- **Тенденции в MLX модели**: https://huggingface.co/models?library=mlx&sort=trending

### Примерни приложения

1. **Личен AI асистент**: Създайте локален чатбот с памет за разговори
2. **Помощник за кодиране**: Създайте асистент за кодиране за вашия работен процес
3. **Генератор на съдържание**: Разработете инструменти за писане, обобщение и създаване на съдържание
4. **Персонализирани фино настроени модели**: Адаптирайте модели за задачи в специфични области
5. **Мултимодални приложения**: Комбинирайте генериране на текст с други възможности на MLX

### Общност и обучение

- **MLX дискусии в общността**: GitHub Issues и Discussions
- **Форуми на Hugging Face**: Поддръжка от общността и споделяне на модели
- **Документация за разработчици на Apple**: Официални ресурси за ML на Apple

### Цитиране

Ако използвате MLX в своето изследване, моля, цитирайте:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Заключение

Apple MLX революционизира начина, по който се изпълняват големи езикови модели на Mac компютри. С предоставянето на нативна оптимизация за Apple Silicon, безпроблемна интеграция с Hugging Face и мощни функции като квантизация и фина настройка с LoRA, MLX прави възможно локалното изпълнение на сложни езикови модели с отлична производителност.

Независимо дали изграждате чатботове, асистенти за кодиране, генератори на съдържание или персонализирани фино настроени модели, MLX предоставя инструментите и производителността, необходими за използване на пълния потенциал на вашия Apple Silicon Mac за приложения с езикови модели. Фокусът на рамката върху ефективност и лесна употреба я прави отличен избор както за изследвания, така и за производствени приложения.

Започнете с основните примери в този урок, разгледайте богатата екосистема от предварително конвертирани модели на Hugging Face и постепенно преминете към по-сложни функции като фина настройка и разработка на персонализирани модели. С разрастването на екосистемата MLX, тя се превръща в все по-мощна платформа за разработка на езикови модели на Apple хардуер.

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.