<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:26:54+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "bg"
}
-->
# Секция 4: OpenVINO Toolkit Optimization Suite

## Съдържание
1. [Въведение](../../../Module04)
2. [Какво е OpenVINO?](../../../Module04)
3. [Инсталация](../../../Module04)
4. [Ръководство за бърз старт](../../../Module04)
5. [Пример: Конвертиране и оптимизация на модели с OpenVINO](../../../Module04)
6. [Разширено използване](../../../Module04)
7. [Най-добри практики](../../../Module04)
8. [Отстраняване на проблеми](../../../Module04)
9. [Допълнителни ресурси](../../../Module04)

## Въведение

OpenVINO (Open Visual Inference and Neural Network Optimization) е отворен инструментариум на Intel за внедряване на високопроизводителни AI решения в облака, локални среди и периферни устройства. Независимо дали целите CPU, GPU, VPU или специализирани AI ускорители, OpenVINO предоставя цялостни възможности за оптимизация, като същевременно запазва точността на модела и позволява внедряване на различни платформи.

## Какво е OpenVINO?

OpenVINO е отворен инструментариум, който позволява на разработчиците ефективно да оптимизират, конвертират и внедряват AI модели на различни хардуерни платформи. Той се състои от три основни компонента: OpenVINO Runtime за инференция, Neural Network Compression Framework (NNCF) за оптимизация на модели и OpenVINO Model Server за мащабируемо внедряване.

### Основни характеристики

- **Внедряване на различни платформи**: Поддържа Linux, Windows и macOS с Python, C++ и C API
- **Хардуерно ускорение**: Автоматично откриване на устройства и оптимизация за CPU, GPU, VPU и AI ускорители
- **Рамка за компресия на модели**: Разширени техники за квантизация, изрязване и оптимизация чрез NNCF
- **Съвместимост с рамки**: Директна поддръжка за TensorFlow, ONNX, PaddlePaddle и PyTorch модели
- **Поддръжка на генеративен AI**: Специализиран OpenVINO GenAI за внедряване на големи езикови модели и приложения за генеративен AI

### Ползи

- **Оптимизация на производителността**: Значителни подобрения в скоростта с минимална загуба на точност
- **Намален отпечатък при внедряване**: Минимални външни зависимости улесняват инсталацията и внедряването
- **Подобрено време за стартиране**: Оптимизирано зареждане и кеширане на модели за по-бърза инициализация на приложенията
- **Мащабируемо внедряване**: От периферни устройства до облачна инфраструктура с последователни API
- **Готовност за производство**: Надеждност на ниво предприятие с изчерпателна документация и подкрепа от общността

## Инсталация

### Предварителни изисквания

- Python 3.8 или по-нова версия
- pip пакетен мениджър
- Виртуална среда (препоръчително)
- Съвместим хардуер (препоръчват се Intel CPU, но се поддържат различни архитектури)

### Основна инсталация

Създайте и активирайте виртуална среда:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Инсталирайте OpenVINO Runtime:

```bash
pip install openvino
```

Инсталирайте NNCF за оптимизация на модели:

```bash
pip install nncf
```

### Инсталация на OpenVINO GenAI

За приложения с генеративен AI:

```bash
pip install openvino-genai
```

### Допълнителни зависимости

Допълнителни пакети за специфични случаи:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Проверка на инсталацията

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Ако е успешно, трябва да видите информация за версията на OpenVINO.

## Ръководство за бърз старт

### Вашата първа оптимизация на модел

Нека конвертираме и оптимизираме модел от Hugging Face с OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Какво прави този процес

Работният процес на оптимизация включва: зареждане на оригиналния модел от Hugging Face, конвертиране в OpenVINO Intermediate Representation (IR) формат, прилагане на стандартни оптимизации и компилиране за целевия хардуер.

### Обяснение на ключовите параметри

- `export=True`: Конвертира модела в OpenVINO IR формат
- `compile=False`: Отлага компилацията до време на изпълнение за гъвкавост
- `device`: Целеви хардуер ("CPU", "GPU", "AUTO" за автоматичен избор)
- `save_pretrained()`: Запазва оптимизирания модел за повторно използване

## Пример: Конвертиране и оптимизация на модели с OpenVINO

### Стъпка 1: Конвертиране на модел с NNCF квантизация

Ето как да приложите квантизация след обучение с NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Стъпка 2: Разширена оптимизация с компресия на тегла

За модели, базирани на трансформери, приложете компресия на тегла:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Стъпка 3: Инференция с оптимизиран модел

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Структура на изхода

След оптимизация, вашата директория с модела ще съдържа:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Разширено използване

### Конфигурация с NNCF YAML

За сложни работни процеси на оптимизация използвайте конфигурационни файлове на NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Приложете конфигурацията:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU оптимизация

За ускорение с GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Оптимизация на обработка на партиди

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Внедряване на Model Server

Внедрете оптимизирани модели с OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Клиентски код за Model Server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Най-добри практики

### 1. Избор и подготовка на модел
- Използвайте модели от поддържани рамки (PyTorch, TensorFlow, ONNX)
- Уверете се, че входовете на модела имат фиксирани или известни динамични форми
- Тествайте с представителни набори от данни за калибриране

### 2. Избор на стратегия за оптимизация
- **Квантизация след обучение**: Започнете тук за бърза оптимизация
- **Компресия на тегла**: Идеална за големи езикови модели и трансформери
- **Обучение с осведоменост за квантизация**: Използвайте, когато точността е критична

### 3. Хардуерно-специфична оптимизация
- **CPU**: Използвайте INT8 квантизация за балансирана производителност
- **GPU**: Възползвайте се от FP16 прецизност и обработка на партиди
- **VPU**: Фокусирайте се върху опростяване на модела и сливане на слоеве

### 4. Настройка на производителността
- **Режим на пропускателна способност**: За обработка на големи обеми партиди
- **Режим на латентност**: За приложения в реално време
- **AUTO устройство**: Позволете на OpenVINO да избере оптималния хардуер

### 5. Управление на паметта
- Използвайте динамични форми разумно, за да избегнете натоварване на паметта
- Прилагайте кеширане на модели за по-бързо последващо зареждане
- Наблюдавайте използването на паметта по време на оптимизация

### 6. Валидиране на точността
- Винаги валидирайте оптимизираните модели спрямо оригиналната производителност
- Използвайте представителни тестови набори от данни за оценка
- Обмислете постепенна оптимизация (започнете с консервативни настройки)

## Отстраняване на проблеми

### Често срещани проблеми

#### 1. Проблеми с инсталацията
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Грешки при конвертиране на модели
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Проблеми с производителността
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Проблеми с паметта
- Намалете размера на партидите на модела по време на оптимизация
- Използвайте стрийминг за големи набори от данни
- Активирайте кеширане на модели: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Деградация на точността
- Използвайте по-висока прецизност (INT8 вместо INT4)
- Увеличете размера на набора от данни за калибриране
- Приложете оптимизация с смесена прецизност

### Мониторинг на производителността

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Получаване на помощ

- **Документация**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Форум на общността**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Допълнителни ресурси

### Официални връзки
- **Начална страница на OpenVINO**: [openvino.ai](https://openvino.ai/)
- **GitHub хранилище**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF хранилище**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Ресурси за обучение
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Ръководство за бърз старт**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Ръководство за оптимизация**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Инструменти за интеграция
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Бенчмаркове за производителност
- **Официални бенчмаркове**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Примери от общността
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Изчерпателни уроци, налични в хранилището на OpenVINO notebooks
- **Примерни приложения**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Примери от реалния свят за различни области (компютърно зрение, NLP, аудио)
- **Блог публикации**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Блог публикации на Intel AI и общността с подробни случаи на употреба

### Свързани инструменти
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Допълнителни техники за оптимизация за хардуер на Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - За сравнения при мобилно и периферно внедряване
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Алтернативи за многоплатформени инференции

## ➡️ Какво следва

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.