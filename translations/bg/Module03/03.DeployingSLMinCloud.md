<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-19T01:43:27+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "bg"
}
-->
# Контейнеризирано облачно внедряване - решения за мащабно производство

Този подробен урок обхваща три основни подхода за внедряване на модела Phi-4-mini-instruct на Microsoft в контейнеризирани среди: vLLM, Ollama и SLM Engine с ONNX Runtime. Този модел с 3.8 милиарда параметри представлява оптимален избор за задачи, изискващи логическо разсъждение, като същевременно запазва ефективността за внедряване на крайни устройства.

## Съдържание

1. [Въведение в контейнерното внедряване на Phi-4-mini](../../../Module03)
2. [Цели на обучението](../../../Module03)
3. [Разбиране на класификацията на Phi-4-mini](../../../Module03)
4. [Контейнерно внедряване с vLLM](../../../Module03)
5. [Контейнерно внедряване с Ollama](../../../Module03)
6. [SLM Engine с ONNX Runtime](../../../Module03)
7. [Сравнителна рамка](../../../Module03)
8. [Най-добри практики](../../../Module03)

## Въведение в контейнерното внедряване на Phi-4-mini

Малките езикови модели (SLMs) представляват ключов напредък в EdgeAI, позволявайки сложни възможности за обработка на естествен език на устройства с ограничени ресурси. Този урок се фокусира върху стратегии за контейнеризирано внедряване на Phi-4-mini-instruct на Microsoft, модерен модел за логическо разсъждение, който съчетава способности и ефективност.

### Представен модел: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 милиарда параметри)**: Най-новият лек модел на Microsoft, настроен за инструкции, предназначен за среди с ограничена памет/изчислителна мощност, с изключителни способности в:
- **Математическо разсъждение и сложни изчисления**
- **Генериране, отстраняване на грешки и анализ на код**
- **Логическо решаване на проблеми и разсъждение стъпка по стъпка**
- **Образователни приложения, изискващи подробни обяснения**
- **Извикване на функции и интеграция с инструменти**

Като част от категорията "Малки SLMs" (1.5B - 13.9B параметри), Phi-4-mini постига оптимален баланс между способност за разсъждение и ефективност на ресурсите.

### Ползи от контейнеризираното внедряване на Phi-4-mini

- **Оперативна ефективност**: Бързо извеждане за задачи с разсъждение при по-ниски изчислителни изисквания
- **Гъвкавост на внедряване**: AI възможности на устройството с подобрена поверителност чрез локална обработка
- **Икономичност**: Намалени оперативни разходи в сравнение с по-големите модели, като същевременно се запазва качеството
- **Изолация**: Чисто разделение между инстанциите на модела и сигурни среди за изпълнение
- **Мащабируемост**: Лесно хоризонтално мащабиране за увеличен капацитет за разсъждение

## Цели на обучението

До края на този урок ще можете:

- Да внедрите и оптимизирате Phi-4-mini-instruct в различни контейнеризирани среди
- Да приложите усъвършенствани стратегии за квантизация и компресия за различни сценарии на внедряване
- Да конфигурирате готова за производство оркестрация на контейнери за задачи с разсъждение
- Да оцените и изберете подходящи рамки за внедряване въз основа на специфични изисквания за употреба
- Да приложите най-добрите практики за сигурност, мониторинг и мащабиране за контейнеризирани SLM внедрения

## Разбиране на класификацията на Phi-4-mini

### Спецификации на модела

**Технически детайли:**
- **Параметри**: 3.8 милиарда (категория Малки SLM)
- **Архитектура**: Плътен декодер-ориентиран Transformer с групирано внимание към заявки
- **Дължина на контекста**: 128K токена (32K препоръчителни за оптимална производителност)
- **Речник**: 200K токена с многоезична поддръжка
- **Тренировъчни данни**: 5T токена с висококачествено съдържание, наситено с разсъждения

### Изисквания за ресурси

| Тип внедряване | Мин. RAM | Препоръчителна RAM | VRAM (GPU) | Съхранение | Типични случаи на употреба |
|----------------|----------|--------------------|------------|------------|----------------------------|
| **Разработка** | 6GB | 8GB | - | 8GB | Локално тестване, прототипиране |
| **Производство CPU** | 8GB | 12GB | - | 10GB | Edge сървъри, икономично внедряване |
| **Производство GPU** | 6GB | 8GB | 4-6GB | 8GB | Услуги за разсъждение с висок капацитет |
| **Оптимизирано за крайни устройства** | 4GB | 6GB | - | 6GB | Квантизирано внедряване, IoT шлюзове |

### Способности на Phi-4-mini

- **Математическо съвършенство**: Разширено решаване на задачи по аритметика, алгебра и калкулус
- **Интелигентност в кода**: Генериране на код на Python, JavaScript и много други езици с отстраняване на грешки
- **Логическо разсъждение**: Разграждане на проблеми стъпка по стъпка и конструиране на решения
- **Образователна подкрепа**: Подробни обяснения, подходящи за обучение и преподаване
- **Извикване на функции**: Нативна поддръжка за интеграция с инструменти и API взаимодействия

## Контейнерно внедряване с vLLM

vLLM предоставя отлична поддръжка за Phi-4-mini-instruct с оптимизирана производителност на извеждане и OpenAI-съвместими API, което го прави идеален за производствени услуги за разсъждение.

### Бързи примери за стартиране

#### Основно внедряване на CPU (Разработка)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Производствено внедряване с GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Конфигурация за производство

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестване на способностите за разсъждение на Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Контейнерно внедряване с Ollama

Ollama предоставя отлична поддръжка за Phi-4-mini-instruct със опростено внедряване и управление, което го прави идеален за разработка и балансирани производствени внедрения.

### Бърза настройка

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Конфигурация за производство

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Оптимизация на модела и варианти

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Примери за използване на API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine с ONNX Runtime

ONNX Runtime предоставя оптимална производителност за внедряване на Phi-4-mini-instruct на крайни устройства с усъвършенствана оптимизация и съвместимост между платформи.

### Основна настройка

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Опростена имплементация на сървър

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Скрипт за конвертиране на модела

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Конфигурация за производство

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестване на внедряването с ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Сравнителна рамка

### Сравнение на рамките за Phi-4-mini

| Функция | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Сложност на настройката** | Средна | Лесна | Сложна |
| **Производителност (GPU)** | Отлична (~25 ток/сек) | Много добра (~20 ток/сек) | Добра (~15 ток/сек) |
| **Производителност (CPU)** | Добра (~8 ток/сек) | Много добра (~12 ток/сек) | Отлична (~15 ток/сек) |
| **Използване на памет** | 8-12GB | 6-10GB | 4-8GB |
| **Съвместимост на API** | OpenAI съвместим | Персонализиран REST | Персонализиран FastAPI |
| **Извикване на функции** | ✅ Нативно | ✅ Поддържано | ⚠️ Персонализирана имплементация |
| **Поддръжка на квантизация** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Quantization |
| **Готовност за производство** | ✅ Отлична | ✅ Много добра | ✅ Добра |
| **Внедряване на крайни устройства** | Добро | Отлично | Изключително |

## Допълнителни ресурси

### Официална документация
- **Microsoft Phi-4 Model Card**: Подробни спецификации и указания за употреба
- **vLLM Документация**: Усъвършенствани опции за конфигурация и оптимизация
- **Ollama Моделна библиотека**: Общностни модели и примери за персонализация
- **ONNX Runtime Ръководства**: Стратегии за оптимизация на производителността и внедряване

### Инструменти за разработка
- **Hugging Face Transformers**: За взаимодействие и персонализация на модела
- **OpenAI API Спецификация**: За тестване на съвместимостта с vLLM
- **Най-добри практики за Docker**: Насоки за сигурност и оптимизация на контейнери
- **Kubernetes Внедряване**: Модели за оркестрация за мащабиране в производство

### Ресурси за обучение
- **Бенчмаркинг на производителността на SLM**: Методи за сравнителен анализ
- **Внедряване на Edge AI**: Най-добри практики за среди с ограничени ресурси
- **Оптимизация на задачи за разсъждение**: Стратегии за създаване на подсказки за математически и логически проблеми
- **Сигурност на контейнери**: Практики за укрепване на внедрявания на AI модели

## Резултати от обучението

След завършване на този модул ще можете:

1. Да внедрите модела Phi-4-mini-instruct в контейнеризирани среди, използвайки различни рамки
2. Да конфигурирате и оптимизирате SLM внедрения за различни хардуерни среди
3. Да приложите най-добрите практики за сигурност за контейнеризирани AI внедрения
4. Да сравните и изберете подходящи рамки за внедряване въз основа на специфични изисквания за употреба
5. Да приложите стратегии за мониторинг и мащабиране за услуги с производствено качество на SLM

## Какво следва

- Върнете се към [Модул 1](../Module01/README.md)
- Върнете се към [Модул 2](../Module02/README.md)

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.