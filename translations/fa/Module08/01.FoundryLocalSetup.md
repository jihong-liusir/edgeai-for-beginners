<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T13:36:52+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "fa"
}
-->
# جلسه ۱: شروع کار با Foundry Local

## مرور کلی

Microsoft Foundry Local قابلیت‌های Azure AI Foundry را مستقیماً به محیط توسعه ویندوز ۱۱ شما می‌آورد و امکان توسعه هوش مصنوعی با حفظ حریم خصوصی، تأخیر کم و ابزارهای سطح سازمانی را فراهم می‌کند. این جلسه شامل نصب کامل، پیکربندی و اجرای عملی مدل‌های محبوب از جمله phi، qwen، deepseek و GPT-OSS-20B است.

## اهداف یادگیری

در پایان این جلسه، شما قادر خواهید بود:
- Foundry Local را روی ویندوز ۱۱ نصب و پیکربندی کنید
- دستورات CLI و گزینه‌های پیکربندی را به خوبی یاد بگیرید
- استراتژی‌های کش مدل برای عملکرد بهینه را درک کنید
- مدل‌های phi، qwen، deepseek و GPT-OSS-20B را با موفقیت اجرا کنید
- اولین برنامه هوش مصنوعی خود را با استفاده از Foundry Local ایجاد کنید

## پیش‌نیازها

### الزامات سیستم
- **ویندوز ۱۱**: نسخه 22H2 یا بالاتر
- **رم**: حداقل ۱۶ گیگابایت، توصیه شده ۳۲ گیگابایت
- **فضای ذخیره‌سازی**: ۵۰ گیگابایت فضای آزاد برای مدل‌ها و کش
- **سخت‌افزار**: دستگاه مجهز به NPU یا GPU ترجیحاً (Copilot+ PC یا GPU انویدیا)
- **شبکه**: اینترنت پرسرعت برای دانلود مدل‌ها

### محیط توسعه
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## بخش ۱: نصب و راه‌اندازی

### مرحله ۱: نصب Foundry Local

Foundry Local را با استفاده از Winget نصب کنید یا نصب‌کننده را از GitHub دانلود کنید:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### مرحله ۲: تأیید نصب

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## بخش ۲: درک CLI

### ساختار دستورات اصلی

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## بخش ۳: مدیریت و کش مدل‌ها

Foundry Local از کش هوشمند مدل‌ها برای بهینه‌سازی عملکرد و ذخیره‌سازی استفاده می‌کند:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## بخش ۴: اجرای عملی مدل‌ها

### اجرای مدل‌های Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### کار با مدل‌های Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### اجرای مدل‌های DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### اجرای GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## بخش ۵: ایجاد اولین برنامه شما

### رابط چت ساده (API سازگار با OpenAI)

یک برنامه چت ساده با استفاده از API REST سازگار با OpenAI در Foundry Local ایجاد کنید. مطمئن شوید که یک مدل در ترمینال دیگر در حال اجرا است.

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### اجرای برنامه چت

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## بخش ۶: رفع اشکال و بهترین روش‌ها

### مشکلات رایج و راه‌حل‌ها

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### نظارت بر منابع سیستم (ویندوز)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### بهترین روش‌ها

- از دستورات `foundry model ...`، `foundry cache ...` و `foundry service ...` استفاده کنید (به مرجع CLI مراجعه کنید)
- به‌طور منظم ارتقا دهید تا به مدل‌ها و اصلاحات جدید دسترسی پیدا کنید
- با مدل‌های کوچک‌تر (Phi mini، Qwen 7B) شروع کنید و به تدریج مقیاس را افزایش دهید
- هنگام تنظیم پرامپت‌ها و تنظیمات، CPU/GPU/رم را نظارت کنید

## بخش ۷: تمرین‌های عملی

### تمرین ۱: اجرای سریع چند مدل

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### تمرین ۲: بنچمارک تأخیر پایه

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## منابع

- شروع کار با Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- مرجع CLI و مرور دستورات: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- کامپایل مدل‌های Hugging Face برای Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- GitHub Microsoft Foundry Local: https://github.com/microsoft/Foundry-Local

---

