<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "382a763fcea7087e68a94c26216e5e70",
  "translation_date": "2025-09-22T13:34:00+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "fa"
}
-->
# جلسه ۵: ساخت سریع عوامل هوش مصنوعی با Foundry Local

[!NOTE] قابلیت‌های عوامل در Foundry Local در حال تکامل هستند—قبل از اجرای الگوهای پیشرفته، پشتیبانی را در آخرین یادداشت‌های انتشار تأیید کنید.

## مرور کلی

از Foundry Local برای نمونه‌سازی سریع برنامه‌های عامل‌محور استفاده کنید: درخواست‌های سیستمی، پایه‌گذاری و الگوهای هماهنگی. هنگامی که پشتیبانی از عوامل موجود باشد، می‌توانید از فراخوانی تابع سازگار با OpenAI استانداردسازی کنید یا در طراحی‌های ترکیبی از عوامل Azure AI در سمت ابری استفاده کنید.

مراجع:
- مستندات Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- عوامل Azure AI Foundry: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- نمونه فراخوانی تابع (نمونه‌های Foundry Local): https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## اهداف یادگیری
- طراحی درخواست‌های سیستمی و استراتژی‌های پایه‌گذاری برای رفتار قابل اعتماد
- اجرای الگوهای فراخوانی تابع (استفاده از ابزار)
- هماهنگی جریان‌های کاری چندعاملی (محلی و ترکیبی)
- برنامه‌ریزی برای مشاهده‌پذیری و ایمنی

## بخش ۱: درخواست‌های سیستمی و پایه‌گذاری

- تعریف نقش‌ها، محدودیت‌ها و طرح‌های خروجی دقیق
- پایه‌گذاری پاسخ‌ها با داده‌های محلی یا سازمانی
- اجرای خروجی‌های JSON برای اتوماسیون پایین‌دستی

## بخش ۲: فراخوانی تابع (سازگار با OpenAI)

```python
# tools.py
import json

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

FUNCTIONS = [
    {
        "name": "get_weather",
        "description": "Get current weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "City name"}
            },
            "required": ["city"]
        }
    }
]
```

```python
# agent.py
import requests
import json
from tools import FUNCTIONS, get_weather

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def call_model(messages, functions=None):
    payload = {
        "model": MODEL,
        "messages": messages,
        "functions": functions,
        "function_call": "auto"
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    return r.json()

messages = [{"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": "What's the weather in Paris?"}]

resp = call_model(messages, functions=FUNCTIONS)
choice = resp["choices"][0]["message"]

if "function_call" in choice:
    fc = choice["function_call"]
    if fc["name"] == "get_weather":
        args = json.loads(fc["arguments"])
        result = get_weather(args["city"])
        messages.append(choice)
        messages.append({"role": "function", "name": "get_weather", "content": result})
        final = call_model(messages)
        print(final["choices"][0]["message"]["content"]) 
else:
    print(choice.get("content"))
```

اجرا:
```powershell
# Ensure a model is running
foundry model run phi-4-mini
python agent.py
```


## بخش ۳: هماهنگی چندعاملی (الگو)

طراحی یک هماهنگ‌کننده که وظایف را به عوامل متخصص (بازیابی، استدلال، اجرا) با استفاده از نقطه پایانی سازگار با OpenAI در Foundry Local هدایت کند.

مرحله ۱) تعریف عوامل متخصص  
```python
# agents/specialists.py
import requests
BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

headers = {"Content-Type": "application/json", "Authorization": "Bearer local-key"}

def chat(messages, max_tokens=300, temperature=0.4):
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json={
        "model": MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }, headers=headers, timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

class RetrievalAgent:
    SYSTEM = "You retrieve relevant snippets from knowledge sources based on a query."
    def run(self, query: str) -> str:
        # Placeholder: in real use, fetch from local files or vector DB
        messages = [{"role": "system", "content": self.SYSTEM},
                    {"role": "user", "content": f"Retrieve key facts for: {query}"}]
        return chat(messages)

class ReasoningAgent:
    SYSTEM = "You analyze inputs step by step and produce structured conclusions."
    def run(self, context: str, question: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\nThink step-by-step and produce a concise answer."}
        ]
        return chat(messages)

class ExecutionAgent:
    SYSTEM = "You transform decisions into actionable steps (JSON with actions)."
    def run(self, decision: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Turn this decision into 3 executable steps as JSON:\n{decision}"}
        ]
        return chat(messages)
```
  
مرحله ۲) ساخت هماهنگ‌کننده  
```python
# agents/coordinator.py
from agents.specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent

class Coordinator:
    def __init__(self):
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()

    def handle(self, user_goal: str) -> dict:
        # 1. Retrieve context
        context = self.retrieval.run(user_goal)
        # 2. Reason on context
        decision = self.reasoning.run(context, user_goal)
        # 3. Produce actionable steps
        actions = self.execution.run(decision)
        return {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions
        }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    coord = Coordinator()
    result = coord.handle("Create a plan to onboard 5 new customers this month")
    print(result)
```
  
مرحله ۳) اعتبارسنجی در برابر Foundry Local  
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator
python -m samples.05.agents.coordinator
```
  

راهنمایی‌ها:
- اجرای تلاش مجدد و زمان‌بندی بین عوامل
- افزودن یک حافظه کوچک درون‌برنامه‌ای (dict) برای وضعیت مکالمه/رشته
- معرفی محدودیت نرخ هنگام زنجیره‌سازی چندین تماس

## بخش ۴: مشاهده‌پذیری و ایمنی

پیگیری درخواست‌ها، پاسخ‌ها و خطاها به صورت محلی، در حالی که بهداشت داده‌ها را در پشته عامل خود اجرا می‌کنید.

مرحله ۱) ثبت درخواست سبک (اختیاری)

[!NOTE] کمک‌کننده زیر به صورت پیش‌فرض شامل نمی‌شود. اگر می‌خواهید ثبت JSON محلی برای آزمایش‌ها ایجاد کنید، `infra/obs.py` بسازید.  
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```
  
ادغام ثبت در عوامل (اختیاری):  
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```
  

مرحله ۲) اعتبارسنجی دسترسی و سلامت پایه از طریق CLI  
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```
  

مرحله ۳) حذف و بهداشت PII  
- قبل از ارسال پیام‌ها به مدل، فیلدهای حساس (ایمیل‌ها، شماره تلفن‌ها، شناسه‌ها) را حذف یا هش کنید  
- داده‌های خام منبع را روی دستگاه نگه دارید، فقط رشته‌های زمینه ضروری را ارسال کنید  

مثال کمک‌کننده حذف:  
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
استفاده در عوامل:  
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```
  

مرحله ۴) قطع‌کننده‌های مدار و مدیریت خطا  
- هر تماس عامل را با try/except و بازگشت نمایی بپیچید  
- در صورت شکست‌های مکرر، خط لوله را کوتاه کنید  

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```
  

مرحله ۵) مسیر حسابرسی محلی و صادرات  
- ثبت JSON را تحت `./agent_logs` ذخیره کنید  
- به صورت دوره‌ای ثبت‌ها را فشرده و چرخش دهید  
- خلاصه‌ها را برای بررسی‌ها صادر کنید (تعداد، میانگین تأخیر، نرخ خطا)  

مرحله ۶) بررسی متقابل با مستندات Microsoft Learn  
- Foundry Local یک API سازگار با OpenAI ارائه می‌دهد (اعتبارسنجی با `curl /v1/models`)  
- از `foundry model run <name>` برای تأیید دسترسی مدل استفاده کنید  
- راهنمای رسمی برای ادغام مشتری و برنامه‌های نمونه را دنبال کنید (Open WebUI/چگونه‌ها)  

مراجع:
- Foundry Local (Learn): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Open WebUI چگونه: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
- نمونه فراخوانی تابع: https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## مراحل بعدی
- عوامل Azure AI را برای هماهنگی میزبانی‌شده در ابر بررسی کنید  
- اتصال‌دهنده‌های سازمانی (Microsoft Graph، جستجو، پایگاه‌های داده) اضافه کنید  

---

