<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T13:35:45+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fa"
}
-->
# جلسه ۳: مدل‌های متن‌باز با Foundry Local

## مرور کلی

در این جلسه، نحوه استفاده از مدل‌های متن‌باز در Foundry Local بررسی می‌شود: انتخاب مدل‌های جامعه، ادغام محتوای Hugging Face و اتخاذ استراتژی‌های «مدل خود را بیاورید» (BYOM). همچنین با سری Model Mondays برای یادگیری مداوم و کشف مدل‌ها آشنا خواهید شد.

مراجع:
- مستندات Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- کامپایل مدل‌های Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## اهداف یادگیری
- کشف و ارزیابی مدل‌های متن‌باز برای استنتاج محلی
- کامپایل و اجرای مدل‌های منتخب Hugging Face در Foundry Local
- اعمال استراتژی‌های انتخاب مدل برای دقت، تأخیر و نیازهای منابع
- مدیریت مدل‌ها به صورت محلی با استفاده از کش و نسخه‌بندی

## بخش ۱: کشف و انتخاب مدل (گام به گام)

گام ۱) لیست مدل‌های موجود در کاتالوگ محلی
```cmd
foundry model list
```

گام ۲) آزمایش سریع دو مدل منتخب (دانلود خودکار در اولین اجرا)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

گام ۳) یادداشت معیارهای پایه
- مشاهده تأخیر (به صورت ذهنی) و کیفیت برای یک پرسش ثابت
- بررسی استفاده از حافظه از طریق Task Manager هنگام اجرای هر مدل

## بخش ۲: اجرای مدل‌های کاتالوگ از طریق CLI (گام به گام)

گام ۱) شروع یک مدل
```cmd
foundry model run llama-3.2
```

گام ۲) ارسال یک پرسش آزمایشی از طریق نقطه پایانی سازگار با OpenAI
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## بخش ۳: BYOM – کامپایل مدل‌های Hugging Face (گام به گام)

دستورالعمل رسمی برای کامپایل مدل‌ها را دنبال کنید. جریان کلی در زیر آمده است—برای دستورات دقیق و پیکربندی‌های پشتیبانی‌شده به مقاله Microsoft Learn مراجعه کنید.

گام ۱) آماده‌سازی یک دایرکتوری کاری
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

گام ۲) کامپایل یک مدل HF پشتیبانی‌شده
- از مراحل موجود در مستندات Learn برای تبدیل و قرار دادن مدل کامپایل‌شده ONNX در دایرکتوری `models` استفاده کنید
- تأیید کنید با:
```cmd
foundry cache ls
```

باید نام مدل کامپایل‌شده خود را ببینید (برای مثال، `llama-3.2`).

گام ۳) اجرای مدل کامپایل‌شده
```cmd
foundry model run llama-3.2 --verbose
```

نکات:
- اطمینان حاصل کنید که فضای دیسک و RAM کافی برای کامپایل و اجرا دارید
- با مدل‌های کوچک‌تر شروع کنید تا جریان را تأیید کنید، سپس مقیاس را افزایش دهید

## بخش ۴: مدیریت عملی مدل‌ها (گام به گام)

گام ۱) ایجاد یک رجیستری `models.json`
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

گام ۲) اسکریپت انتخاب‌کننده کوچک
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## بخش ۵: معیارهای عملی (گام به گام)

گام ۱) معیار ساده تأخیر
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

گام ۲) بررسی کیفیت
- از یک مجموعه پرسش ثابت استفاده کنید، خروجی‌ها را به یک فایل CSV/JSON ذخیره کنید
- به صورت دستی روانی، ارتباط و درستی را امتیازدهی کنید (۱–۵)

## بخش ۶: مراحل بعدی
- برای مدل‌ها و نکات جدید به Model Mondays مشترک شوید: https://aka.ms/model-mondays
- یافته‌های خود را به `models.json` تیم خود اضافه کنید
- برای جلسه ۴ آماده شوید: مقایسه LLMها با SLMها، استنتاج محلی در مقابل ابری، و دموهای عملی

---

