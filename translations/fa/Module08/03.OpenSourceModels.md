<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:10:09+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fa"
}
-->
# جلسه ۳: کشف و مدیریت مدل‌های متن‌باز

## مرور کلی

این جلسه بر کشف و مدیریت عملی مدل‌ها با Foundry Local تمرکز دارد. شما یاد خواهید گرفت که چگونه مدل‌های موجود را لیست کنید، گزینه‌های مختلف را آزمایش کنید و ویژگی‌های عملکردی پایه را درک کنید. این رویکرد بر کاوش عملی با CLI Foundry تأکید دارد تا به شما کمک کند مدل‌های مناسب برای موارد استفاده خود را انتخاب کنید.

## اهداف یادگیری

- تسلط بر دستورات CLI Foundry برای کشف و مدیریت مدل‌ها
- درک الگوهای ذخیره‌سازی محلی و کش مدل‌ها
- یادگیری آزمایش سریع و مقایسه مدل‌های مختلف
- ایجاد جریان‌های کاری عملی برای انتخاب و ارزیابی مدل‌ها
- بررسی اکوسیستم در حال رشد مدل‌های موجود از طریق Foundry Local

## پیش‌نیازها

- تکمیل جلسه ۱: شروع کار با Foundry Local
- نصب و دسترسی به CLI Foundry Local
- فضای ذخیره‌سازی کافی برای دانلود مدل‌ها (اندازه مدل‌ها می‌تواند از ۱ گیگابایت تا بیش از ۲۰ گیگابایت باشد)
- درک پایه‌ای از انواع مدل‌ها و موارد استفاده

## بخش ۶: تمرین عملی

### تمرین: کشف و مقایسه مدل‌ها

اسکریپت ارزیابی مدل خود را بر اساس نمونه ۰۳ ایجاد کنید:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### وظیفه شما

1. **اجرای اسکریپت نمونه ۰۳**: `samples\03\list_and_bench.cmd`
2. **آزمایش مدل‌های مختلف**: حداقل ۳ مدل مختلف را آزمایش کنید
3. **مقایسه عملکرد**: تفاوت‌های سرعت و کیفیت پاسخ را یادداشت کنید
4. **مستندسازی یافته‌ها**: یک نمودار مقایسه ساده ایجاد کنید

### قالب مثال مقایسه

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## بخش ۷: رفع اشکال و بهترین روش‌ها

### مشکلات رایج و راه‌حل‌ها

**مدل اجرا نمی‌شود:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**حافظه ناکافی:**
- با مدل‌های کوچک‌تر شروع کنید (`phi-4-mini`)
- برنامه‌های دیگر را ببندید
- در صورت مواجهه مکرر با محدودیت‌ها، RAM را ارتقا دهید

**عملکرد کند:**
- مطمئن شوید مدل به طور کامل بارگذاری شده است (خروجی verbose را بررسی کنید)
- برنامه‌های پس‌زمینه غیرضروری را ببندید
- ذخیره‌سازی سریع‌تر (SSD) را در نظر بگیرید

### بهترین روش‌ها

1. **کوچک شروع کنید**: با `phi-4-mini` برای اعتبارسنجی تنظیمات شروع کنید
2. **یک مدل در هر زمان**: مدل‌های قبلی را قبل از شروع مدل‌های جدید متوقف کنید
3. **منابع را نظارت کنید**: استفاده از حافظه را زیر نظر داشته باشید
4. **آزمایش مداوم**: از همان درخواست‌ها برای مقایسه منصفانه استفاده کنید
5. **نتایج را مستند کنید**: یادداشت‌هایی درباره عملکرد مدل برای موارد استفاده خود نگه دارید

## بخش ۸: مراحل بعدی و منابع

### آماده‌سازی برای جلسه ۴

- **تمرکز جلسه ۴**: ابزارها و تکنیک‌های بهینه‌سازی
- **پیش‌نیازها**: راحتی با تغییر مدل‌ها و آزمایش عملکرد پایه
- **توصیه‌شده**: شناسایی ۲-۳ مدل مورد علاقه از این جلسه

### منابع اضافی

- **[مستندات Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: مستندات رسمی
- **[مرجع CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: مرجع کامل دستورات
- **[Model Mondays](https://aka.ms/model-mondays)**: معرفی مدل‌های هفتگی
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: جامعه و مشکلات
- **[نمونه ۰۳: کشف مدل](samples/03/README.md)**: اسکریپت مثال عملی

### نکات کلیدی

✅ **کشف مدل**: از `foundry model list` برای بررسی مدل‌های موجود استفاده کنید  
✅ **آزمایش سریع**: الگوی `list_and_bench.cmd` برای ارزیابی سریع  
✅ **نظارت بر عملکرد**: اندازه‌گیری استفاده از منابع و زمان پاسخ پایه  
✅ **انتخاب مدل**: دستورالعمل‌های عملی برای انتخاب مدل‌ها بر اساس موارد استفاده  
✅ **مدیریت کش**: درک ذخیره‌سازی و روش‌های پاکسازی  

اکنون مهارت‌های عملی برای کشف، آزمایش و انتخاب مدل‌های مناسب برای برنامه‌های هوش مصنوعی خود با استفاده از رویکرد ساده CLI Foundry Local را دارید.

## اهداف یادگیری

- کشف و ارزیابی مدل‌های متن‌باز برای استنتاج محلی
- کامپایل و اجرای مدل‌های انتخابی Hugging Face در Foundry Local
- اعمال استراتژی‌های انتخاب مدل برای دقت، تأخیر و نیازهای منابع
- مدیریت مدل‌ها به صورت محلی با کش و نسخه‌بندی

## بخش ۱: کشف مدل با CLI Foundry

### دستورات پایه مدیریت مدل

CLI Foundry دستورات ساده‌ای برای کشف و مدیریت مدل‌ها ارائه می‌دهد:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### اجرای اولین مدل‌های شما

با مدل‌های محبوب و آزمایش‌شده شروع کنید تا ویژگی‌های عملکردی را درک کنید:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**توجه:** فلگ `--verbose` اطلاعات راه‌اندازی دقیق ارائه می‌دهد، از جمله:
- پیشرفت دانلود مدل (در اولین اجرا)
- جزئیات تخصیص حافظه
- اطلاعات اتصال سرویس
- معیارهای اولیه‌سازی عملکرد

### درک دسته‌بندی مدل‌ها

**مدل‌های زبان کوچک (SLMs):**
- `phi-4-mini`: سریع، کارآمد، عالی برای چت عمومی
- `phi-4`: نسخه توانمندتر با استدلال بهتر

**مدل‌های متوسط:**
- `qwen2.5-7b`: استدلال عالی و زمینه طولانی‌تر
- `deepseek-r1-7b`: بهینه‌شده برای تولید کد

**مدل‌های بزرگ‌تر:**
- `llama-3.2`: جدیدترین مدل متن‌باز Meta
- `qwen2.5-14b`: استدلال در سطح سازمانی

## بخش ۲: آزمایش سریع مدل و مقایسه

### رویکرد نمونه ۰۳: لیست و ارزیابی ساده

بر اساس الگوی نمونه ۰۳، اینجا جریان کاری حداقلی آورده شده است:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### آزمایش عملکرد مدل

پس از اجرای یک مدل، آن را با درخواست‌های ثابت آزمایش کنید:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### جایگزین آزمایش PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## بخش ۳: مدیریت کش و ذخیره‌سازی مدل

### درک کش مدل

Foundry Local به طور خودکار دانلود و کش مدل‌ها را مدیریت می‌کند:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### ملاحظات ذخیره‌سازی مدل

**اندازه‌های معمول مدل:**
- `phi-4-mini`: ~۲.۵ گیگابایت
- `qwen2.5-7b`: ~۴.۱ گیگابایت  
- `deepseek-r1-7b`: ~۴.۳ گیگابایت
- `llama-3.2`: ~۴.۹ گیگابایت
- `qwen2.5-14b`: ~۸.۲ گیگابایت

**بهترین روش‌های ذخیره‌سازی:**
- ۲-۳ مدل را برای تغییر سریع کش نگه دارید
- مدل‌های استفاده‌نشده را برای آزادسازی فضا حذف کنید: `foundry cache clean`
- استفاده از دیسک را به‌ویژه در SSD‌های کوچک نظارت کنید
- ملاحظات اندازه مدل در مقابل قابلیت‌ها را در نظر بگیرید

### نظارت بر عملکرد مدل

در حالی که مدل‌ها اجرا می‌شوند، منابع سیستم را نظارت کنید:

**Task Manager ویندوز:**
- استفاده از حافظه را مشاهده کنید (مدل‌ها در RAM بارگذاری می‌مانند)
- استفاده از CPU را در طول استنتاج نظارت کنید
- I/O دیسک را در طول بارگذاری اولیه مدل بررسی کنید

**نظارت خط فرمان:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## بخش ۴: دستورالعمل‌های عملی انتخاب مدل

### انتخاب مدل‌ها بر اساس موارد استفاده

**برای چت عمومی و پرسش و پاسخ:**
- شروع با: `phi-4-mini` (سریع، کارآمد)
- ارتقا به: `phi-4` (استدلال بهتر)
- پیشرفته: `qwen2.5-7b` (زمینه طولانی‌تر)

**برای تولید کد:**
- توصیه‌شده: `deepseek-r1-7b`
- جایگزین: `qwen2.5-7b` (همچنین برای کد مناسب)

**برای استدلال پیچیده:**
- بهترین: `qwen2.5-7b` یا `qwen2.5-14b`
- گزینه اقتصادی: `phi-4`

### راهنمای نیازهای سخت‌افزاری

**حداقل نیازهای سیستم:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**توصیه‌شده برای بهترین عملکرد:**
- RAM 32 گیگابایت یا بیشتر برای تغییر راحت بین مدل‌ها
- ذخیره‌سازی SSD برای بارگذاری سریع‌تر مدل‌ها
- CPU مدرن با عملکرد خوب تک‌نخی
- پشتیبانی NPU (کامپیوترهای Windows 11 Copilot+) برای شتاب‌دهی

### جریان کاری تغییر مدل

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## بخش ۵: ارزیابی ساده مدل

### آزمایش عملکرد پایه

اینجا یک رویکرد ساده برای مقایسه عملکرد مدل آورده شده است:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### ارزیابی کیفیت دستی

برای هر مدل، با درخواست‌های ثابت آزمایش کنید و به صورت دستی ارزیابی کنید:

**درخواست‌های آزمایش:**
1. "محاسبات کوانتومی را به زبان ساده توضیح دهید."
2. "یک تابع پایتون برای مرتب‌سازی یک لیست بنویسید."
3. "مزایا و معایب کار از راه دور چیست؟"
4. "مزایای هوش مصنوعی لبه را خلاصه کنید."

**معیارهای ارزیابی:**
- **دقت**: آیا اطلاعات صحیح است؟
- **وضوح**: آیا توضیح آسان برای فهم است؟
- **کامل بودن**: آیا به طور کامل به سوال پاسخ داده شده است؟
- **سرعت**: پاسخ چقدر سریع ارائه می‌شود؟

### نظارت بر استفاده از منابع

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## بخش ۶: مراحل بعدی

- برای مدل‌های جدید و نکات به Model Mondays مشترک شوید: https://aka.ms/model-mondays
- یافته‌های خود را به `models.json` تیم خود اضافه کنید
- برای جلسه ۴ آماده شوید: مقایسه LLM‌ها در مقابل SLM‌ها، استنتاج محلی در مقابل ابری، و دموهای عملی

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه انسانی حرفه‌ای استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.