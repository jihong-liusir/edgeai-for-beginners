<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T12:18:07+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fa"
}
-->
# جلسه ۳: کشف و مدیریت مدل‌های متن‌باز

## مرور کلی

این جلسه بر کشف و مدیریت عملی مدل‌ها با Foundry Local تمرکز دارد. شما یاد خواهید گرفت که چگونه مدل‌های موجود را فهرست کنید، گزینه‌های مختلف را آزمایش کنید و ویژگی‌های عملکردی پایه را درک کنید. این رویکرد بر کاوش عملی با CLI Foundry تأکید دارد تا به شما کمک کند مدل‌های مناسب برای موارد استفاده خود را انتخاب کنید.

## اهداف یادگیری

- تسلط بر دستورات CLI Foundry برای کشف و مدیریت مدل‌ها  
- درک الگوهای ذخیره‌سازی محلی و کش مدل  
- یادگیری آزمایش سریع و مقایسه مدل‌های مختلف  
- ایجاد جریان‌های کاری عملی برای انتخاب و ارزیابی مدل‌ها  
- بررسی اکوسیستم در حال رشد مدل‌ها از طریق Foundry Local  

## پیش‌نیازها

- تکمیل جلسه ۱: شروع کار با Foundry Local  
- نصب و دسترسی به CLI Foundry Local  
- فضای ذخیره‌سازی کافی برای دانلود مدل‌ها (اندازه مدل‌ها می‌تواند از ۱ گیگابایت تا بیش از ۲۰ گیگابایت باشد)  
- درک پایه‌ای از انواع مدل‌ها و موارد استفاده  

## بخش ۶: تمرین عملی

### تمرین: کشف و مقایسه مدل‌ها

اسکریپت ارزیابی مدل خود را بر اساس نمونه ۰۳ ایجاد کنید:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### وظیفه شما

1. **اجرای اسکریپت نمونه ۰۳**: `samples\03\list_and_bench.cmd`  
2. **آزمایش مدل‌های مختلف**: حداقل ۳ مدل مختلف را آزمایش کنید  
3. **مقایسه عملکرد**: تفاوت‌های سرعت و کیفیت پاسخ را یادداشت کنید  
4. **مستندسازی یافته‌ها**: یک نمودار مقایسه ساده ایجاد کنید  

### قالب مثال مقایسه

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## بخش ۷: رفع اشکال و بهترین روش‌ها

### مشکلات رایج و راه‌حل‌ها

**مدل اجرا نمی‌شود:**  
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```
  

**حافظه ناکافی:**  
- با مدل‌های کوچک‌تر شروع کنید (`phi-4-mini`)  
- برنامه‌های دیگر را ببندید  
- در صورت مواجهه مکرر با محدودیت‌ها، RAM را ارتقا دهید  

**عملکرد کند:**  
- مطمئن شوید مدل به طور کامل بارگذاری شده است (خروجی verbose را بررسی کنید)  
- برنامه‌های پس‌زمینه غیرضروری را ببندید  
- ذخیره‌سازی سریع‌تر (SSD) را در نظر بگیرید  

### بهترین روش‌ها

1. **کوچک شروع کنید**: با `phi-4-mini` برای اعتبارسنجی تنظیمات شروع کنید  
2. **یک مدل در هر زمان**: مدل‌های قبلی را قبل از شروع مدل‌های جدید متوقف کنید  
3. **منابع را نظارت کنید**: استفاده از حافظه را زیر نظر داشته باشید  
4. **آزمایش مداوم**: از همان درخواست‌ها برای مقایسه منصفانه استفاده کنید  
5. **نتایج را مستند کنید**: یادداشت‌هایی درباره عملکرد مدل برای موارد استفاده خود نگه دارید  

## بخش ۸: مراحل بعدی و منابع

### آماده‌سازی برای جلسه ۴

- **تمرکز جلسه ۴**: ابزارها و تکنیک‌های بهینه‌سازی  
- **پیش‌نیازها**: راحتی با تغییر مدل و آزمایش عملکرد پایه  
- **توصیه‌شده**: شناسایی ۲-۳ مدل مورد علاقه از این جلسه  

### منابع اضافی

- **[مستندات Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: مستندات رسمی  
- **[مرجع CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: مرجع کامل دستورات  
- **[Model Mondays](https://aka.ms/model-mondays)**: معرفی مدل‌های هفتگی  
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: جامعه و مسائل  
- **[نمونه ۰۳: کشف مدل](samples/03/README.md)**: اسکریپت نمونه عملی  

### نکات کلیدی

✅ **کشف مدل**: از `foundry model list` برای بررسی مدل‌های موجود استفاده کنید  
✅ **آزمایش سریع**: الگوی `list_and_bench.cmd` برای ارزیابی سریع  
✅ **نظارت بر عملکرد**: استفاده پایه از منابع و اندازه‌گیری زمان پاسخ  
✅ **انتخاب مدل**: دستورالعمل‌های عملی برای انتخاب مدل‌ها بر اساس موارد استفاده  
✅ **مدیریت کش**: درک ذخیره‌سازی و روش‌های پاکسازی  

اکنون مهارت‌های عملی برای کشف، آزمایش و انتخاب مدل‌های مناسب برای برنامه‌های هوش مصنوعی خود با استفاده از رویکرد ساده CLI Foundry Local را دارید.

---

## اهداف یادگیری

- کشف و ارزیابی مدل‌های متن‌باز برای استنتاج محلی  
- کامپایل و اجرای مدل‌های انتخابی Hugging Face در Foundry Local  
- اعمال استراتژی‌های انتخاب مدل برای دقت، تأخیر و نیازهای منابع  
- مدیریت مدل‌ها به صورت محلی با کش و نسخه‌بندی  

## بخش ۱: کشف مدل با CLI Foundry

### دستورات پایه مدیریت مدل

CLI Foundry دستورات ساده‌ای برای کشف و مدیریت مدل‌ها ارائه می‌دهد:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### اجرای اولین مدل‌ها

با مدل‌های محبوب و آزمایش‌شده شروع کنید تا ویژگی‌های عملکردی را درک کنید:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```


**توجه:** فلگ `--verbose` اطلاعات راه‌اندازی دقیق ارائه می‌دهد، از جمله:  
- پیشرفت دانلود مدل (در اولین اجرا)  
- جزئیات تخصیص حافظه  
- اطلاعات اتصال سرویس  
- معیارهای اولیه عملکرد  

### درک دسته‌بندی مدل‌ها

**مدل‌های زبان کوچک (SLMs):**  
- `phi-4-mini`: سریع، کارآمد، مناسب برای چت عمومی  
- `phi-4`: نسخه توانمندتر با استدلال بهتر  

**مدل‌های متوسط:**  
- `qwen2.5-7b-instruct`: استدلال عالی و زمینه طولانی‌تر  
- `deepseek-r1-distill-qwen-7b`: بهینه‌شده برای تولید کد  

**مدل‌های بزرگ‌تر:**  
- `llama-3.2`: جدیدترین مدل متن‌باز Meta  
- `qwen2.5-14b-instruct`: استدلال در سطح سازمانی  

## بخش ۲: آزمایش سریع مدل و مقایسه

### رویکرد نمونه ۰۳: لیست و ارزیابی ساده

بر اساس الگوی نمونه ۰۳، این جریان کاری حداقلی است:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### آزمایش عملکرد مدل

پس از اجرای مدل، آن را با درخواست‌های ثابت آزمایش کنید:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### جایگزین آزمایش PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## بخش ۳: مدیریت کش و ذخیره‌سازی مدل

### درک کش مدل

Foundry Local به طور خودکار دانلود و کش مدل‌ها را مدیریت می‌کند:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### ملاحظات ذخیره‌سازی مدل

**اندازه‌های معمول مدل:**  
- `phi-4-mini`: ~۲.۵ گیگابایت  
- `qwen2.5-7b-instruct`: ~۴.۱ گیگابایت  
- `deepseek-r1-distill-qwen-7b`: ~۴.۳ گیگابایت  
- `llama-3.2`: ~۴.۹ گیگابایت  
- `qwen2.5-14b-instruct`: ~۸.۲ گیگابایت  

**بهترین روش‌های ذخیره‌سازی:**  
- ۲-۳ مدل را برای تغییر سریع کش نگه دارید  
- مدل‌های استفاده‌نشده را برای آزاد کردن فضا حذف کنید: `foundry cache clean`  
- استفاده از دیسک را نظارت کنید، به ویژه در SSD‌های کوچک‌تر  
- توازن بین اندازه مدل و قابلیت‌ها را در نظر بگیرید  

### نظارت بر عملکرد مدل

در حالی که مدل‌ها اجرا می‌شوند، منابع سیستم را نظارت کنید:

**Task Manager ویندوز:**  
- استفاده از حافظه را مشاهده کنید (مدل‌ها در RAM بارگذاری می‌شوند)  
- استفاده از CPU را در طول استنتاج نظارت کنید  
- I/O دیسک را در طول بارگذاری اولیه مدل بررسی کنید  

**نظارت خط فرمان:**  
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## بخش ۴: دستورالعمل‌های عملی انتخاب مدل

### انتخاب مدل‌ها بر اساس موارد استفاده

**برای چت عمومی و پرسش و پاسخ:**  
- شروع با: `phi-4-mini` (سریع، کارآمد)  
- ارتقا به: `phi-4` (استدلال بهتر)  
- پیشرفته: `qwen2.5-7b-instruct` (زمینه طولانی‌تر)  

**برای تولید کد:**  
- توصیه‌شده: `deepseek-r1-distill-qwen-7b`  
- جایگزین: `qwen2.5-7b-instruct` (همچنین مناسب برای کد)  

**برای استدلال پیچیده:**  
- بهترین: `qwen2.5-7b-instruct` یا `qwen2.5-14b-instruct`  
- گزینه اقتصادی: `phi-4`  

### راهنمای نیازهای سخت‌افزاری

**حداقل نیازهای سیستم:**  
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**توصیه‌شده برای بهترین عملکرد:**  
- RAM 32GB+ برای تغییر راحت بین مدل‌ها  
- ذخیره‌سازی SSD برای بارگذاری سریع‌تر مدل‌ها  
- CPU مدرن با عملکرد خوب تک‌ترد  
- پشتیبانی NPU (کامپیوترهای Windows 11 Copilot+) برای شتاب‌دهی  

### جریان کاری تغییر مدل

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```


## بخش ۵: ارزیابی ساده مدل

### آزمایش عملکرد پایه

در اینجا یک رویکرد ساده برای مقایسه عملکرد مدل‌ها آورده شده است:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### ارزیابی کیفیت دستی

برای هر مدل، با درخواست‌های ثابت آزمایش کنید و به صورت دستی ارزیابی کنید:

**درخواست‌های آزمایشی:**  
1. "محاسبات کوانتومی را به زبان ساده توضیح دهید."  
2. "یک تابع پایتون برای مرتب‌سازی یک لیست بنویسید."  
3. "مزایا و معایب کار از راه دور چیست؟"  
4. "مزایای هوش مصنوعی لبه را خلاصه کنید."  

**معیارهای ارزیابی:**  
- **دقت**: آیا اطلاعات صحیح است؟  
- **وضوح**: آیا توضیح قابل فهم است؟  
- **کامل بودن**: آیا به کل سؤال پاسخ داده شده است؟  
- **سرعت**: پاسخ چقدر سریع است؟  

### نظارت بر استفاده از منابع

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## بخش ۶: مراحل بعدی

- برای مدل‌های جدید و نکات به Model Mondays مشترک شوید: https://aka.ms/model-mondays  
- یافته‌های خود را به `models.json` تیم خود اضافه کنید  
- برای جلسه ۴ آماده شوید: مقایسه LLM‌ها و SLM‌ها، استنتاج محلی در مقابل ابری، و دموهای عملی  

---

