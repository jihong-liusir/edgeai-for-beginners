{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# جلسه ۵ – هماهنگ‌کننده چند‌عاملی\n",
    "\n",
    "نمایش یک خط لوله ساده دو‌عاملی (پژوهشگر -> ویراستار) با استفاده از Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### توضیح: نصب وابستگی‌ها\n",
    "`foundry-local-sdk` و `openai` را نصب می‌کند که برای دسترسی به مدل محلی و تکمیل چت مورد نیاز هستند. این عملیات بی‌اثر است.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# سناریو\n",
    "الگوی هماهنگ‌کننده دو عامل حداقلی را پیاده‌سازی می‌کند:\n",
    "- **عامل پژوهشگر** نکات واقعی مختصر را جمع‌آوری می‌کند\n",
    "- **عامل ویراستار** برای وضوح اجرایی بازنویسی می‌کند\n",
    "\n",
    "نشان‌دهنده حافظه مشترک برای هر عامل، انتقال متوالی خروجی میانی و یک تابع خط لوله ساده است. قابل گسترش به نقش‌های بیشتر (مانند منتقد، تأییدکننده) یا شاخه‌های موازی.\n",
    "\n",
    "**متغیرهای محیطی:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - مدل پیش‌فرض برای استفاده (پیش‌فرض: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - مدل عامل اصلی (جایگزین ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - مدل عامل ویراستار (پیش‌فرض: اصلی)\n",
    "\n",
    "**مرجع SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**نحوه کار:**\n",
    "1. **FoundryLocalManager** به طور خودکار سرویس Foundry Local را راه‌اندازی می‌کند\n",
    "2. مدل مشخص‌شده را دانلود و بارگذاری می‌کند (یا از نسخه ذخیره‌شده استفاده می‌کند)\n",
    "3. یک نقطه پایانی سازگار با OpenAI برای تعامل فراهم می‌کند\n",
    "4. هر عامل می‌تواند از مدل متفاوتی برای وظایف تخصصی استفاده کند\n",
    "5. منطق بازگشت داخلی به طور مؤثر خرابی‌های گذرا را مدیریت می‌کند\n",
    "\n",
    "**ویژگی‌های کلیدی:**\n",
    "- ✅ کشف و راه‌اندازی خودکار سرویس\n",
    "- ✅ مدیریت چرخه عمر مدل (دانلود، ذخیره، بارگذاری)\n",
    "- ✅ سازگاری با SDK OpenAI برای API آشنا\n",
    "- ✅ پشتیبانی از چند مدل برای تخصص عامل\n",
    "- ✅ مدیریت خطاهای قوی با منطق بازگشت\n",
    "- ✅ استنتاج محلی (بدون نیاز به API ابری)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### توضیح: وارد کردن‌های اصلی و تایپینگ  \n",
    "معرفی دیتاکلاس‌ها برای ذخیره‌سازی پیام‌های عامل و استفاده از تایپینگ برای وضوح بیشتر. وارد کردن مدیر محلی Foundry و کلاینت OpenAI برای اقدامات بعدی عامل.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### توضیح: مقداردهی اولیه مدل (الگوی SDK)\n",
    "از SDK محلی پایتون Foundry برای مدیریت قدرتمند مدل استفاده می‌شود:\n",
    "- **FoundryLocalManager(alias)** - سرویس را به‌صورت خودکار راه‌اندازی کرده و مدل را با استفاده از alias بارگذاری می‌کند\n",
    "- **get_model_info(alias)** - alias را به شناسه مدل مشخص تبدیل می‌کند\n",
    "- **manager.endpoint** - نقطه پایانی سرویس را برای کلاینت OpenAI فراهم می‌کند\n",
    "- **manager.api_key** - کلید API را فراهم می‌کند (اختیاری برای استفاده محلی)\n",
    "- پشتیبانی از مدل‌های جداگانه برای عوامل مختلف (اصلی در مقابل ویرایشگر)\n",
    "- منطق داخلی تلاش مجدد با بازگشت نمایی برای افزایش مقاومت\n",
    "- تأیید اتصال برای اطمینان از آماده بودن سرویس\n",
    "\n",
    "**الگوی کلیدی SDK:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**مدیریت چرخه عمر:**\n",
    "- مدیران به‌صورت جهانی ذخیره می‌شوند تا پاک‌سازی مناسب انجام شود\n",
    "- هر عامل می‌تواند از مدل متفاوتی برای تخصصی‌سازی استفاده کند\n",
    "- کشف خودکار سرویس و مدیریت اتصال\n",
    "- تلاش مجدد با بازگشت نمایی به‌صورت روان در صورت بروز خطا\n",
    "\n",
    "این موارد اطمینان حاصل می‌کنند که مقداردهی اولیه به‌درستی قبل از شروع هماهنگی عوامل انجام شود.\n",
    "\n",
    "**مرجع:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### توضیح: کلاس‌های Agent و Memory\n",
    "تعریف `AgentMsg` سبک برای ورودی‌های حافظه و `Agent` که شامل موارد زیر است:\n",
    "- **نقش سیستم** - شخصیت و دستورالعمل‌های عامل\n",
    "- **تاریخچه پیام‌ها** - حفظ زمینه مکالمه\n",
    "- **متد act()** - اجرای اقدامات با مدیریت صحیح خطاها\n",
    "\n",
    "عامل می‌تواند از مدل‌های مختلف (اصلی در مقابل ویرایشگر) استفاده کند و زمینه‌ای جداگانه برای هر عامل حفظ کند. این الگو امکان موارد زیر را فراهم می‌کند:\n",
    "- حفظ حافظه در طول اقدامات\n",
    "- تخصیص انعطاف‌پذیر مدل برای هر عامل\n",
    "- جداسازی خطاها و بازیابی\n",
    "- زنجیره‌سازی و هماهنگی آسان\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### توضیح: خط لوله هماهنگ‌شده\n",
    "دو عامل تخصصی ایجاد می‌کند:\n",
    "- **پژوهشگر**: از مدل اصلی استفاده می‌کند و اطلاعات واقعی جمع‌آوری می‌کند\n",
    "- **ویرایشگر**: می‌تواند از مدل جداگانه (در صورت پیکربندی) استفاده کند، اطلاعات را اصلاح و بازنویسی می‌کند\n",
    "\n",
    "تابع `pipeline`:\n",
    "1. پژوهشگر اطلاعات خام را جمع‌آوری می‌کند\n",
    "2. ویرایشگر آن را به خروجی آماده برای اجرا تبدیل می‌کند\n",
    "3. هر دو نتیجه میانی و نهایی را بازمی‌گرداند\n",
    "\n",
    "این الگو امکان‌پذیر می‌کند:\n",
    "- تخصص مدل (مدل‌های مختلف برای نقش‌های مختلف)\n",
    "- بهبود کیفیت از طریق پردازش چندمرحله‌ای\n",
    "- قابلیت ردیابی تحول اطلاعات\n",
    "- گسترش آسان به عوامل بیشتر یا پردازش موازی\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### توضیح: اجرای خط لوله و نتایج\n",
    "اجرای خط لوله چندعاملی بر روی یک سؤال با موضوع انطباق و تأخیر برای نشان دادن:\n",
    "- تبدیل اطلاعات چندمرحله‌ای\n",
    "- تخصص و همکاری عوامل\n",
    "- بهبود کیفیت خروجی از طریق اصلاح\n",
    "- قابلیت ردیابی (حفظ خروجی‌های میانی و نهایی)\n",
    "\n",
    "**ساختار نتیجه:**\n",
    "- `question` - پرسش اصلی کاربر\n",
    "- `research` - خروجی تحقیق خام (نکات واقعی)\n",
    "- `final` - خلاصه اجرایی اصلاح‌شده\n",
    "- `models` - مدل‌هایی که در هر مرحله استفاده شده‌اند\n",
    "\n",
    "**ایده‌های توسعه:**\n",
    "1. افزودن عامل منتقد برای بررسی کیفیت\n",
    "2. اجرای عوامل تحقیق موازی برای جنبه‌های مختلف\n",
    "3. افزودن عامل تأییدکننده برای بررسی صحت اطلاعات\n",
    "4. استفاده از مدل‌های مختلف برای سطوح پیچیدگی متفاوت\n",
    "5. پیاده‌سازی حلقه‌های بازخورد برای بهبود تدریجی\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### پیشرفته: پیکربندی سفارشی عامل\n",
    "\n",
    "سعی کنید رفتار عامل را با تغییر متغیرهای محیطی قبل از اجرای سلول اولیه‌سازی سفارشی کنید:\n",
    "\n",
    "**مدل‌های موجود:**\n",
    "- از دستور `foundry model ls` در ترمینال استفاده کنید تا تمام مدل‌های موجود را ببینید\n",
    "- مثال‌ها: phi-4-mini، phi-3.5-mini، qwen2.5-7b، llama-3.2-3b و غیره.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم ترجمه‌ها دقیق باشند، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما هیچ مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-08T22:11:21+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}