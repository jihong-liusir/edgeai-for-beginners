<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T15:20:09+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "fa"
}
-->
# بخش ۰۳ - یکپارچه‌سازی پروتکل زمینه مدل (MCP)

## مقدمه‌ای بر MCP (پروتکل زمینه مدل)

پروتکل زمینه مدل (MCP) یک چارچوب انقلابی است که به مدل‌های زبانی امکان تعامل با ابزارها و سیستم‌های خارجی را به صورت استاندارد می‌دهد. برخلاف روش‌های سنتی که مدل‌ها به صورت ایزوله عمل می‌کنند، MCP پلی بین مدل‌های هوش مصنوعی و دنیای واقعی از طریق یک پروتکل تعریف‌شده ایجاد می‌کند.

### MCP چیست؟

MCP به عنوان یک پروتکل ارتباطی عمل می‌کند که به مدل‌های زبانی اجازه می‌دهد:
- به منابع داده خارجی متصل شوند
- ابزارها و توابع را اجرا کنند
- با APIها و خدمات تعامل داشته باشند
- به اطلاعات لحظه‌ای دسترسی پیدا کنند
- عملیات پیچیده چندمرحله‌ای را انجام دهند

این پروتکل مدل‌های زبانی ایستا را به عوامل پویا تبدیل می‌کند که قادر به انجام وظایف عملی فراتر از تولید متن هستند.

## مدل‌های زبانی کوچک (SLMs) در MCP

مدل‌های زبانی کوچک رویکردی کارآمد برای استقرار هوش مصنوعی ارائه می‌دهند که مزایای متعددی دارند:

### مزایای SLMها
- **کارایی منابع**: نیاز کمتر به محاسبات
- **زمان پاسخ سریع‌تر**: کاهش تأخیر برای برنامه‌های لحظه‌ای  
- **صرفه‌جویی اقتصادی**: نیاز به زیرساخت حداقلی
- **حریم خصوصی**: امکان اجرا به صورت محلی بدون انتقال داده
- **سفارشی‌سازی**: آسان‌تر برای تنظیم دقیق در حوزه‌های خاص

### چرا SLMها با MCP خوب کار می‌کنند؟

ترکیب SLMها با MCP یک ترکیب قدرتمند ایجاد می‌کند که قابلیت‌های استدلال مدل را با ابزارهای خارجی تقویت می‌کند و کمبود تعداد پارامترهای مدل را با عملکرد بهبود‌یافته جبران می‌کند.

## نمای کلی Python MCP SDK

Python MCP SDK پایه‌ای برای ساخت برنامه‌های مجهز به MCP فراهم می‌کند. این SDK شامل موارد زیر است:

- **کتابخانه‌های کلاینت**: برای اتصال به سرورهای MCP
- **چارچوب سرور**: برای ایجاد سرورهای سفارشی MCP
- **مدیریت پروتکل**: برای مدیریت ارتباطات
- **یکپارچه‌سازی ابزارها**: برای اجرای توابع خارجی

## پیاده‌سازی عملی: کلاینت MCP مدل Phi-4

بیایید یک پیاده‌سازی واقعی را با استفاده از مدل کوچک Phi-4 مایکروسافت که قابلیت‌های MCP را یکپارچه کرده است بررسی کنیم.

### معماری سیستم

پیاده‌سازی از معماری لایه‌ای پیروی می‌کند:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### اجزای اصلی

#### ۱. کلاس‌های کلاینت MCP

**BaseMCPClient**: پایه انتزاعی که عملکردهای مشترک را فراهم می‌کند
- پروتکل مدیریت زمینه غیرهمزمان
- تعریف رابط استاندارد
- مدیریت منابع

**Phi4MiniMCPClient**: پیاده‌سازی مبتنی بر STDIO
- ارتباط فرآیند محلی
- مدیریت ورودی/خروجی استاندارد
- مدیریت زیرفرآیندها

**Phi4MiniSSEMCPClient**: پیاده‌سازی رویدادهای ارسال‌شده توسط سرور
- ارتباط جریان HTTP
- مدیریت رویدادهای لحظه‌ای
- اتصال به سرورهای مبتنی بر وب

#### ۲. یکپارچه‌سازی LLM

**OllamaClient**: میزبانی مدل محلی
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: سرویس‌دهی با عملکرد بالا
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### ۳. خط پردازش ابزار

خط پردازش ابزار MCP را به قالب‌هایی تبدیل می‌کند که با مدل‌های زبانی سازگار هستند:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## شروع به کار: راهنمای گام‌به‌گام

### گام ۱: تنظیم محیط

نصب وابستگی‌های مورد نیاز:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### گام ۲: پیکربندی اولیه

تنظیم متغیرهای محیطی:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### گام ۳: اجرای اولین کلاینت MCP

**تنظیم اولیه Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**استفاده از Backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**اتصال رویدادهای ارسال‌شده توسط سرور:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**سرور MCP سفارشی:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### گام ۴: استفاده برنامه‌نویسی

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## ویژگی‌های پیشرفته

### پشتیبانی از چند Backend

پیاده‌سازی از هر دو Backend Ollama و vLLM پشتیبانی می‌کند و به شما امکان انتخاب بر اساس نیازهایتان را می‌دهد:

- **Ollama**: مناسب برای توسعه و آزمایش محلی
- **vLLM**: بهینه‌شده برای تولید و سناریوهای با توان بالا

### پروتکل‌های اتصال انعطاف‌پذیر

دو حالت اتصال پشتیبانی می‌شوند:

**حالت STDIO**: ارتباط مستقیم فرآیند
- تأخیر کمتر
- مناسب برای ابزارهای محلی
- تنظیم ساده

**حالت SSE**: جریان مبتنی بر HTTP
- قابلیت شبکه
- مناسب برای سیستم‌های توزیع‌شده
- به‌روزرسانی‌های لحظه‌ای

### قابلیت‌های یکپارچه‌سازی ابزار

سیستم می‌تواند با ابزارهای مختلف یکپارچه شود:
- اتوماسیون وب (Playwright)
- عملیات فایل
- تعاملات API
- دستورات سیستم
- توابع سفارشی

## مدیریت خطا و بهترین روش‌ها

### مدیریت جامع خطا

پیاده‌سازی شامل مدیریت خطای قوی برای موارد زیر است:

**خطاهای اتصال:**
- خرابی سرور MCP
- زمان‌بندی شبکه
- مشکلات اتصال

**خطاهای اجرای ابزار:**
- ابزارهای گم‌شده
- اعتبارسنجی پارامترها
- خرابی‌های اجرا

**خطاهای پردازش پاسخ:**
- مشکلات تجزیه JSON
- ناسازگاری قالب
- ناهنجاری‌های پاسخ LLM

### بهترین روش‌ها

۱. **مدیریت منابع**: از مدیریت زمینه غیرهمزمان استفاده کنید  
۲. **مدیریت خطا**: بلوک‌های try-catch جامع پیاده‌سازی کنید  
۳. **ثبت وقایع**: سطوح مناسب ثبت وقایع را فعال کنید  
۴. **امنیت**: ورودی‌ها را اعتبارسنجی و خروجی‌ها را پاک‌سازی کنید  
۵. **عملکرد**: از اتصال‌های مشترک و کش استفاده کنید  

## کاربردهای واقعی

### اتوماسیون وب
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### پردازش داده
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### یکپارچه‌سازی API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## بهینه‌سازی عملکرد

### مدیریت حافظه
- مدیریت کارآمد تاریخچه پیام‌ها
- پاک‌سازی مناسب منابع
- اتصال‌های مشترک

### بهینه‌سازی شبکه
- عملیات HTTP غیرهمزمان
- زمان‌بندی‌های قابل تنظیم
- بازیابی خطای مناسب

### پردازش همزمان
- I/O غیرمسدودکننده
- اجرای موازی ابزارها
- الگوهای غیرهمزمان کارآمد

## ملاحظات امنیتی

### حفاظت از داده‌ها
- مدیریت امن کلیدهای API
- اعتبارسنجی ورودی‌ها
- پاک‌سازی خروجی‌ها

### امنیت شبکه
- پشتیبانی از HTTPS
- پیش‌فرض‌های نقطه پایانی محلی
- مدیریت امن توکن‌ها

### ایمنی اجرا
- فیلتر کردن ابزارها
- محیط‌های ایزوله
- ثبت وقایع حسابرسی

## نتیجه‌گیری

مدل‌های زبانی کوچک یکپارچه با MCP نمایانگر یک تغییر پارادایم در توسعه برنامه‌های هوش مصنوعی هستند. با ترکیب کارایی مدل‌های کوچک با قدرت ابزارهای خارجی، توسعه‌دهندگان می‌توانند سیستم‌های هوشمندی ایجاد کنند که هم از نظر منابع کارآمد و هم بسیار توانمند باشند.

پیاده‌سازی کلاینت MCP مدل Phi-4 نشان می‌دهد که چگونه این یکپارچه‌سازی می‌تواند در عمل انجام شود و پایه‌ای محکم برای ساخت برنامه‌های پیشرفته مجهز به هوش مصنوعی فراهم کند.

نکات کلیدی:
- MCP شکاف بین مدل‌های زبانی و سیستم‌های خارجی را پر می‌کند
- SLMها با ابزارهای تقویت‌شده کارایی را بدون قربانی کردن قابلیت‌ها ارائه می‌دهند
- معماری ماژولار امکان گسترش و سفارشی‌سازی آسان را فراهم می‌کند
- مدیریت خطا و اقدامات امنیتی مناسب برای استفاده در تولید ضروری هستند

این آموزش پایه‌ای برای ساخت برنامه‌های MCP مجهز به SLM شما فراهم می‌کند و امکاناتی برای اتوماسیون، پردازش داده‌ها و یکپارچه‌سازی سیستم‌های هوشمند باز می‌کند.

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.