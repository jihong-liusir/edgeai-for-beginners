<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T15:54:31+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "fa"
}
-->
# بخش ۴: پیاده‌سازی مدل آماده تولید

## مرور کلی

این آموزش جامع شما را از طریق فرآیند کامل استقرار مدل‌های کمینه‌سازی شده و تنظیم‌شده با استفاده از Foundry Local راهنمایی می‌کند. ما تبدیل مدل، بهینه‌سازی کمینه‌سازی و پیکربندی استقرار را از ابتدا تا انتها پوشش خواهیم داد.

## پیش‌نیازها

قبل از شروع، مطمئن شوید که موارد زیر را دارید:

- ✅ یک مدل تنظیم‌شده onnx آماده برای استقرار
- ✅ کامپیوتر ویندوز یا مک
- ✅ پایتون نسخه 3.10 یا بالاتر
- ✅ حداقل ۸ گیگابایت رم موجود
- ✅ Foundry Local نصب شده بر روی سیستم شما

## بخش ۱: تنظیم محیط

### نصب ابزارهای مورد نیاز

ترمینال خود را باز کنید (Command Prompt در ویندوز، Terminal در مک) و دستورات زیر را به ترتیب اجرا کنید:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **نکته مهم**: شما همچنین به نسخه 3.31 یا جدیدتر CMake نیاز دارید که می‌توانید آن را از [cmake.org](https://cmake.org/download/) دانلود کنید.

## بخش ۲: تبدیل مدل و کمینه‌سازی

### انتخاب فرمت مناسب

برای مدل‌های کوچک زبان تنظیم‌شده، ما استفاده از فرمت **ONNX** را توصیه می‌کنیم زیرا:

- 🚀 بهینه‌سازی عملکرد بهتر
- 🔧 استقرار مستقل از سخت‌افزار
- 🏭 قابلیت‌های آماده تولید
- 📱 سازگاری بین پلتفرم‌ها

### روش ۱: تبدیل با یک دستور (توصیه‌شده)

از دستور زیر برای تبدیل مستقیم مدل تنظیم‌شده خود استفاده کنید:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**توضیح پارامترها:**
- `--model_name_or_path`: مسیر مدل تنظیم‌شده شما
- `--device cpu`: استفاده از CPU برای بهینه‌سازی
- `--precision int4`: استفاده از کمینه‌سازی INT4 (کاهش اندازه تقریباً ۷۵٪)
- `--output_path`: مسیر خروجی برای مدل تبدیل‌شده

### روش ۲: استفاده از فایل پیکربندی (برای کاربران پیشرفته)

یک فایل پیکربندی به نام `finetuned_conversion_config.json` ایجاد کنید:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

سپس اجرا کنید:

```bash
olive run --config ./finetuned_conversion_config.json
```

### مقایسه گزینه‌های کمینه‌سازی

| دقت       | اندازه فایل | سرعت استنتاج | کیفیت مدل | استفاده توصیه‌شده |
|-----------|------------|---------------|-----------|--------------------|
| FP16      | پایه × 0.5 | سریع          | بهترین    | سخت‌افزار پیشرفته |
| INT8      | پایه × 0.25 | بسیار سریع   | خوب       | انتخاب متعادل     |
| INT4      | پایه × 0.125 | سریع‌ترین    | قابل قبول | منابع محدود       |

💡 **توصیه**: برای اولین استقرار خود با کمینه‌سازی INT4 شروع کنید. اگر کیفیت رضایت‌بخش نبود، INT8 یا FP16 را امتحان کنید.

## بخش ۳: پیکربندی استقرار Foundry Local

### ایجاد پیکربندی مدل

به دایرکتوری مدل‌های Foundry Local بروید:

```bash
foundry cache cd ./models/
```

ساختار دایرکتوری مدل خود را ایجاد کنید:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

فایل پیکربندی `inference_model.json` را در دایرکتوری مدل خود ایجاد کنید:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### پیکربندی‌های قالب مخصوص مدل

#### برای مدل‌های سری Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## بخش ۴: آزمایش و بهینه‌سازی مدل

### بررسی نصب مدل

بررسی کنید که آیا Foundry Local می‌تواند مدل شما را شناسایی کند:

```bash
foundry cache ls
```

شما باید `your-finetuned-model-int4` را در لیست مشاهده کنید.

### شروع آزمایش مدل

```bash
foundry model run your-finetuned-model-int4
```

### ارزیابی عملکرد

در طول آزمایش، معیارهای کلیدی زیر را نظارت کنید:

1. **زمان پاسخ**: اندازه‌گیری میانگین زمان برای هر پاسخ
2. **مصرف حافظه**: نظارت بر مصرف رم
3. **استفاده از CPU**: بررسی بار پردازنده
4. **کیفیت خروجی**: ارزیابی ارتباط و انسجام پاسخ‌ها

### چک‌لیست اعتبارسنجی کیفیت

- ✅ مدل به طور مناسب به پرسش‌های حوزه تنظیم‌شده پاسخ می‌دهد
- ✅ قالب پاسخ با ساختار خروجی مورد انتظار مطابقت دارد
- ✅ هیچ نشتی حافظه در طول استفاده طولانی‌مدت وجود ندارد
- ✅ عملکرد ثابت در طول ورودی‌های مختلف
- ✅ مدیریت صحیح موارد خاص و ورودی‌های نامعتبر

## خلاصه

تبریک! شما با موفقیت موارد زیر را تکمیل کردید:

- ✅ تبدیل فرمت مدل تنظیم‌شده
- ✅ بهینه‌سازی کمینه‌سازی مدل
- ✅ پیکربندی استقرار Foundry Local
- ✅ تنظیم عملکرد و رفع مشکلات

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.