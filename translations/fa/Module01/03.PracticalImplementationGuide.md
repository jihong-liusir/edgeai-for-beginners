<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-17T15:22:23+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "fa"
}
-->
# بخش ۳: راهنمای عملی پیاده‌سازی

## مرور کلی

این راهنمای جامع به شما کمک می‌کند تا برای دوره EdgeAI آماده شوید، دوره‌ای که بر ساخت راه‌حل‌های عملی هوش مصنوعی تمرکز دارد که به‌طور کارآمد بر روی دستگاه‌های لبه اجرا می‌شوند. این دوره بر توسعه عملی با استفاده از چارچوب‌های مدرن و مدل‌های پیشرفته بهینه‌شده برای استقرار در لبه تأکید دارد.

## ۱. تنظیم محیط توسعه

### زبان‌های برنامه‌نویسی و چارچوب‌ها

**محیط پایتون**
- **نسخه**: پایتون ۳.۱۰ یا بالاتر (توصیه‌شده: پایتون ۳.۱۱)
- **مدیر بسته**: pip یا conda
- **محیط مجازی**: از محیط‌های venv یا conda برای جداسازی استفاده کنید
- **کتابخانه‌های کلیدی**: کتابخانه‌های خاص EdgeAI را در طول دوره نصب خواهیم کرد

**محیط Microsoft .NET**
- **نسخه**: .NET 8 یا بالاتر
- **IDE**: Visual Studio 2022، Visual Studio Code یا JetBrains Rider
- **SDK**: اطمینان حاصل کنید که .NET SDK برای توسعه چندسکویی نصب شده باشد

### ابزارهای توسعه

**ویرایشگرهای کد و IDE‌ها**
- Visual Studio Code (توصیه‌شده برای توسعه چندسکویی)
- PyCharm یا Visual Studio (برای توسعه زبان‌محور)
- Jupyter Notebooks برای توسعه تعاملی و نمونه‌سازی اولیه

**کنترل نسخه**
- Git (آخرین نسخه)
- حساب GitHub برای دسترسی به مخازن و همکاری

## ۲. نیازمندی‌های سخت‌افزاری و توصیه‌ها

### حداقل نیازمندی‌های سیستم
- **CPU**: پردازنده چند هسته‌ای (Intel i5/AMD Ryzen 5 یا معادل آن)
- **RAM**: حداقل ۸ گیگابایت، توصیه‌شده ۱۶ گیگابایت
- **فضای ذخیره‌سازی**: ۵۰ گیگابایت فضای موجود برای مدل‌ها و ابزارهای توسعه
- **سیستم‌عامل**: Windows 10/11، macOS 10.15+ یا Linux (Ubuntu 20.04+)

### استراتژی منابع محاسباتی
این دوره به گونه‌ای طراحی شده که در سخت‌افزارهای مختلف قابل دسترسی باشد:

**توسعه محلی (تمرکز بر CPU/NPU)**
- توسعه اصلی از شتاب‌دهی CPU و NPU استفاده خواهد کرد
- مناسب برای اکثر لپ‌تاپ‌ها و دسکتاپ‌های مدرن
- تمرکز بر کارایی و سناریوهای عملی استقرار

**منابع GPU ابری (اختیاری)**
- **Azure Machine Learning**: برای آموزش و آزمایش‌های سنگین
- **Google Colab**: نسخه رایگان برای اهداف آموزشی
- **Kaggle Notebooks**: پلتفرم محاسبات ابری جایگزین

### ملاحظات دستگاه‌های لبه
- آشنایی با پردازنده‌های مبتنی بر ARM
- دانش سخت‌افزار موبایل و IoT و محدودیت‌های آن‌ها
- آشنایی با بهینه‌سازی مصرف انرژی

## ۳. خانواده‌های مدل اصلی و منابع

### خانواده‌های مدل اصلی

**خانواده Microsoft Phi-4**
- **توضیحات**: مدل‌های جمع‌وجور و کارآمد طراحی‌شده برای استقرار در لبه
- **نقاط قوت**: نسبت عملکرد به اندازه عالی، بهینه‌شده برای وظایف استدلال
- **منبع**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **موارد استفاده**: تولید کد، استدلال ریاضی، مکالمه عمومی

**خانواده Qwen-3**
- **توضیحات**: نسل جدید مدل‌های چندزبانه Alibaba
- **نقاط قوت**: قابلیت‌های چندزبانه قوی، معماری کارآمد
- **منبع**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **موارد استفاده**: برنامه‌های چندزبانه، راه‌حل‌های هوش مصنوعی بین‌فرهنگی

**خانواده Google Gemma-3n**
- **توضیحات**: مدل‌های سبک گوگل بهینه‌شده برای استقرار در لبه
- **نقاط قوت**: استنتاج سریع، معماری مناسب موبایل
- **منبع**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **موارد استفاده**: برنامه‌های موبایل، پردازش بلادرنگ

### معیارهای انتخاب مدل
- **توازن عملکرد و اندازه**: درک زمان انتخاب مدل‌های کوچک‌تر در مقابل مدل‌های بزرگ‌تر
- **بهینه‌سازی وظیفه‌محور**: تطبیق مدل‌ها با موارد استفاده خاص
- **محدودیت‌های استقرار**: حافظه، تأخیر و مصرف انرژی

## ۴. ابزارهای کمینه‌سازی و بهینه‌سازی

### چارچوب Llama.cpp
- **مخزن**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **هدف**: موتور استنتاج با عملکرد بالا برای LLM‌ها
- **ویژگی‌های کلیدی**:
  - استنتاج بهینه‌شده برای CPU
  - فرمت‌های کمینه‌سازی متعدد (Q4، Q5، Q8)
  - سازگاری چندسکویی
  - اجرای حافظه‌کارآمد
- **نصب و استفاده اولیه**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **مخزن**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **هدف**: ابزار بهینه‌سازی مدل برای استقرار در لبه
- **ویژگی‌های کلیدی**:
  - گردش‌کارهای خودکار بهینه‌سازی مدل
  - بهینه‌سازی آگاه از سخت‌افزار
  - یکپارچگی با ONNX Runtime
  - ابزارهای ارزیابی عملکرد
- **نصب و استفاده اولیه**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # تعریف مدل و تنظیمات بهینه‌سازی
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # اجرای گردش‌کار بهینه‌سازی
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # ذخیره مدل بهینه‌شده
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # نصب MLX
  pip install mlx
  
  # مثال اسکریپت پایتون برای بارگذاری و بهینه‌سازی مدل
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **مخزن**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **هدف**: شتاب‌دهی استنتاج چندسکویی برای مدل‌های ONNX
- **ویژگی‌های کلیدی**:
  - بهینه‌سازی‌های خاص سخت‌افزار (CPU، GPU، NPU)
  - بهینه‌سازی گراف برای استنتاج
  - پشتیبانی از کمینه‌سازی
  - پشتیبانی چندزبانه (پایتون، C++، C#، جاوااسکریپت)
- **نصب و استفاده اولیه**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## ۵. منابع و مطالعات توصیه‌شده

### مستندات ضروری
- **مستندات ONNX Runtime**: درک استنتاج چندسکویی
- **راهنمای Transformers در Hugging Face**: بارگذاری مدل و استنتاج
- **الگوهای طراحی Edge AI**: بهترین روش‌ها برای استقرار در لبه

### مقالات فنی
- "هوش مصنوعی کارآمد در لبه: بررسی تکنیک‌های کمینه‌سازی"
- "فشرده‌سازی مدل برای دستگاه‌های موبایل و لبه"
- "بهینه‌سازی مدل‌های Transformer برای محاسبات لبه"

### منابع جامعه
- **جامعه‌های Slack/Discord EdgeAI**: پشتیبانی همتا و بحث
- **مخازن GitHub**: پیاده‌سازی‌های نمونه و آموزش‌ها
- **کانال‌های YouTube**: بررسی‌های فنی و آموزش‌ها

## ۶. ارزیابی و تأیید

### چک‌لیست پیش از دوره
- [ ] نصب و تأیید پایتون ۳.۱۰+
- [ ] نصب و تأیید .NET 8+
- [ ] تنظیم محیط توسعه
- [ ] ایجاد حساب Hugging Face
- [ ] آشنایی اولیه با خانواده‌های مدل هدف
- [ ] نصب و آزمایش ابزارهای کمینه‌سازی
- [ ] برآورده کردن نیازمندی‌های سخت‌افزاری
- [ ] تنظیم حساب‌های محاسبات ابری (در صورت نیاز)

## اهداف کلیدی یادگیری

تا پایان این راهنما، شما قادر خواهید بود:

۱. یک محیط توسعه کامل برای توسعه برنامه‌های EdgeAI تنظیم کنید  
۲. ابزارها و چارچوب‌های لازم برای بهینه‌سازی مدل را نصب و پیکربندی کنید  
۳. پیکربندی‌های سخت‌افزاری و نرم‌افزاری مناسب برای پروژه‌های EdgeAI خود انتخاب کنید  
۴. ملاحظات کلیدی برای استقرار مدل‌های هوش مصنوعی بر روی دستگاه‌های لبه را درک کنید  
۵. سیستم خود را برای تمرین‌های عملی در دوره آماده کنید  

## منابع اضافی

### مستندات رسمی
- **مستندات پایتون**: مستندات رسمی زبان پایتون  
- **مستندات Microsoft .NET**: منابع رسمی توسعه .NET  
- **مستندات ONNX Runtime**: راهنمای جامع ONNX Runtime  
- **مستندات TensorFlow Lite**: مستندات رسمی TensorFlow Lite  

### ابزارهای توسعه
- **Visual Studio Code**: ویرایشگر کد سبک با افزونه‌های توسعه هوش مصنوعی  
- **Jupyter Notebooks**: محیط محاسبات تعاملی برای آزمایش‌های ML  
- **Docker**: پلتفرم کانتینرسازی برای محیط‌های توسعه سازگار  
- **Git**: سیستم کنترل نسخه برای مدیریت کد  

### منابع یادگیری
- **مقالات تحقیقاتی EdgeAI**: جدیدترین تحقیقات آکادمیک درباره مدل‌های کارآمد  
- **دوره‌های آنلاین**: مواد آموزشی مکمل درباره بهینه‌سازی هوش مصنوعی  
- **فروم‌های جامعه**: پلتفرم‌های پرسش و پاسخ برای چالش‌های توسعه EdgeAI  
- **مجموعه داده‌های معیار**: مجموعه داده‌های استاندارد برای ارزیابی عملکرد مدل  

## نتایج یادگیری

پس از تکمیل این راهنمای آماده‌سازی، شما:

۱. یک محیط توسعه کاملاً تنظیم‌شده برای توسعه EdgeAI خواهید داشت  
۲. نیازمندی‌های سخت‌افزاری و نرم‌افزاری برای سناریوهای مختلف استقرار را درک خواهید کرد  
۳. با چارچوب‌ها و ابزارهای کلیدی مورد استفاده در طول دوره آشنا خواهید شد  
۴. قادر خواهید بود مدل‌های مناسب را بر اساس محدودیت‌ها و نیازمندی‌های دستگاه انتخاب کنید  
۵. دانش ضروری درباره تکنیک‌های بهینه‌سازی برای استقرار در لبه را خواهید داشت  

## ➡️ مرحله بعدی

- [04: سخت‌افزار و استقرار EdgeAI](04.EdgeDeployment.md)  

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.