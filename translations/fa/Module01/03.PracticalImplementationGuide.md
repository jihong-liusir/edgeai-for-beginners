<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:13:48+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "fa"
}
-->
# بخش ۳: راهنمای عملی پیاده‌سازی

## مرور کلی

این راهنمای جامع به شما کمک می‌کند تا برای دوره EdgeAI آماده شوید. این دوره بر ساخت راه‌حل‌های عملی هوش مصنوعی که به‌طور کارآمد بر روی دستگاه‌های لبه اجرا می‌شوند، تمرکز دارد. دوره بر توسعه عملی با استفاده از چارچوب‌های مدرن و مدل‌های پیشرفته بهینه‌شده برای استقرار در لبه تأکید دارد.

## ۱. تنظیم محیط توسعه

### زبان‌های برنامه‌نویسی و چارچوب‌ها

**محیط پایتون**
- **نسخه**: پایتون ۳.۱۰ یا بالاتر (توصیه‌شده: پایتون ۳.۱۱)
- **مدیر بسته**: pip یا conda
- **محیط مجازی**: از venv یا محیط‌های conda برای جداسازی استفاده کنید
- **کتابخانه‌های کلیدی**: کتابخانه‌های خاص EdgeAI را در طول دوره نصب خواهیم کرد

**محیط Microsoft .NET**
- **نسخه**: .NET 8 یا بالاتر
- **IDE**: Visual Studio 2022، Visual Studio Code یا JetBrains Rider
- **SDK**: اطمینان حاصل کنید که .NET SDK برای توسعه چندسکویی نصب شده باشد

### ابزارهای توسعه

**ویرایشگرهای کد و IDE‌ها**
- Visual Studio Code (توصیه‌شده برای توسعه چندسکویی)
- PyCharm یا Visual Studio (برای توسعه زبان خاص)
- Jupyter Notebooks برای توسعه تعاملی و نمونه‌سازی

**کنترل نسخه**
- Git (آخرین نسخه)
- حساب GitHub برای دسترسی به مخازن و همکاری

## ۲. نیازمندی‌های سخت‌افزاری و توصیه‌ها

### حداقل نیازمندی‌های سیستم
- **CPU**: پردازنده چند هسته‌ای (Intel i5/AMD Ryzen 5 یا معادل آن)
- **RAM**: حداقل ۸ گیگابایت، توصیه‌شده ۱۶ گیگابایت
- **فضای ذخیره‌سازی**: ۵۰ گیگابایت فضای موجود برای مدل‌ها و ابزارهای توسعه
- **سیستم‌عامل**: Windows 10/11، macOS 10.15+ یا Linux (Ubuntu 20.04+)

### استراتژی منابع محاسباتی
این دوره به گونه‌ای طراحی شده است که در سخت‌افزارهای مختلف قابل دسترس باشد:

**توسعه محلی (تمرکز بر CPU/NPU)**
- توسعه اصلی از شتاب‌دهی CPU و NPU استفاده خواهد کرد
- مناسب برای اکثر لپ‌تاپ‌ها و دسکتاپ‌های مدرن
- تمرکز بر کارایی و سناریوهای عملی استقرار

**منابع GPU ابری (اختیاری)**
- **Azure Machine Learning**: برای آموزش و آزمایش‌های سنگین
- **Google Colab**: نسخه رایگان برای اهداف آموزشی
- **Kaggle Notebooks**: پلتفرم محاسبات ابری جایگزین

### ملاحظات دستگاه‌های لبه
- آشنایی با پردازنده‌های مبتنی بر ARM
- دانش محدودیت‌های سخت‌افزار موبایل و IoT
- آشنایی با بهینه‌سازی مصرف انرژی

## ۳. خانواده‌های مدل اصلی و منابع

### خانواده‌های مدل اصلی

**خانواده Microsoft Phi-4**
- **توضیحات**: مدل‌های جمع‌وجور و کارآمد طراحی‌شده برای استقرار در لبه
- **نقاط قوت**: نسبت عملکرد به اندازه عالی، بهینه‌شده برای وظایف استدلال
- **منبع**: [Phi-4 Collection در Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **موارد استفاده**: تولید کد، استدلال ریاضی، مکالمه عمومی

**خانواده Qwen-3**
- **توضیحات**: نسل جدید مدل‌های چندزبانه Alibaba
- **نقاط قوت**: قابلیت‌های چندزبانه قوی، معماری کارآمد
- **منبع**: [Qwen-3 Collection در Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **موارد استفاده**: برنامه‌های چندزبانه، راه‌حل‌های هوش مصنوعی بین‌فرهنگی

**خانواده Google Gemma-3n**
- **توضیحات**: مدل‌های سبک گوگل بهینه‌شده برای استقرار در لبه
- **نقاط قوت**: استنتاج سریع، معماری مناسب موبایل
- **منبع**: [Gemma-3n Collection در Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **موارد استفاده**: برنامه‌های موبایل، پردازش بلادرنگ

### معیارهای انتخاب مدل
- **توازن عملکرد و اندازه**: درک زمان انتخاب مدل‌های کوچک‌تر یا بزرگ‌تر
- **بهینه‌سازی وظیفه‌محور**: تطبیق مدل‌ها با موارد استفاده خاص
- **محدودیت‌های استقرار**: حافظه، تأخیر و ملاحظات مصرف انرژی

## ۴. ابزارهای کمینه‌سازی و بهینه‌سازی

### چارچوب Llama.cpp
- **مخزن**: [Llama.cpp در GitHub](https://github.com/ggml-org/llama.cpp)
- **هدف**: موتور استنتاج با عملکرد بالا برای LLM‌ها
- **ویژگی‌های کلیدی**:
  - استنتاج بهینه‌شده برای CPU
  - فرمت‌های کمینه‌سازی متعدد (Q4، Q5، Q8)
  - سازگاری چندسکویی
  - اجرای حافظه‌کارآمد
- **نصب و استفاده اولیه**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```


### Microsoft Olive
- **مخزن**: [Microsoft Olive در GitHub](https://github.com/microsoft/olive)
- **هدف**: ابزار بهینه‌سازی مدل برای استقرار در لبه
- **ویژگی‌های کلیدی**:
  - جریان‌های کاری بهینه‌سازی مدل خودکار
  - بهینه‌سازی مبتنی بر سخت‌افزار
  - ادغام با ONNX Runtime
  - ابزارهای ارزیابی عملکرد
- **نصب و استفاده اولیه**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # مثال اسکریپت پایتون برای بهینه‌سازی مدل
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```


### Apple MLX (کاربران macOS)
- **مخزن**: [Apple MLX در GitHub](https://github.com/ml-explore/mlx)
- **هدف**: چارچوب یادگیری ماشین برای Apple Silicon
- **ویژگی‌های کلیدی**:
  - بهینه‌سازی بومی Apple Silicon
  - عملیات حافظه‌کارآمد
  - API مشابه PyTorch
  - پشتیبانی از معماری حافظه یکپارچه
- **نصب و استفاده اولیه**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```


### ONNX Runtime
- **مخزن**: [ONNX Runtime در GitHub](https://github.com/microsoft/onnxruntime)
- **هدف**: شتاب‌دهی استنتاج چندسکویی برای مدل‌های ONNX
- **ویژگی‌های کلیدی**:
  - بهینه‌سازی‌های خاص سخت‌افزار (CPU، GPU، NPU)
  - بهینه‌سازی گراف برای استنتاج
  - پشتیبانی از کمینه‌سازی
  - پشتیبانی چندزبانه (پایتون، C++، C#، جاوااسکریپت)
- **نصب و استفاده اولیه**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## ۵. منابع و مطالعات توصیه‌شده

### مستندات ضروری
- **مستندات ONNX Runtime**: درک استنتاج چندسکویی
- **راهنمای Hugging Face Transformers**: بارگذاری مدل و استنتاج
- **الگوهای طراحی Edge AI**: بهترین روش‌ها برای استقرار در لبه

### مقالات فنی
- "هوش مصنوعی کارآمد در لبه: بررسی تکنیک‌های کمینه‌سازی"
- "فشرده‌سازی مدل برای دستگاه‌های موبایل و لبه"
- "بهینه‌سازی مدل‌های ترانسفورمر برای محاسبات لبه"

### منابع جامعه
- **جامعه‌های Slack/Discord EdgeAI**: پشتیبانی و بحث‌های همتا
- **مخازن GitHub**: پیاده‌سازی‌ها و آموزش‌های نمونه
- **کانال‌های YouTube**: بررسی‌های فنی و آموزش‌ها

## ۶. ارزیابی و تأیید

### چک‌لیست پیش از دوره
- [ ] پایتون ۳.۱۰+ نصب و تأیید شده است
- [ ] .NET 8+ نصب و تأیید شده است
- [ ] محیط توسعه پیکربندی شده است
- [ ] حساب Hugging Face ایجاد شده است
- [ ] آشنایی اولیه با خانواده‌های مدل هدف
- [ ] ابزارهای کمینه‌سازی نصب و آزمایش شده‌اند
- [ ] نیازمندی‌های سخت‌افزاری برآورده شده است
- [ ] حساب‌های محاسبات ابری تنظیم شده‌اند (در صورت نیاز)

## اهداف کلیدی یادگیری

تا پایان این راهنما، شما قادر خواهید بود:

1. یک محیط توسعه کامل برای توسعه برنامه‌های EdgeAI تنظیم کنید
2. ابزارها و چارچوب‌های لازم برای بهینه‌سازی مدل را نصب و پیکربندی کنید
3. پیکربندی‌های سخت‌افزاری و نرم‌افزاری مناسب برای پروژه‌های EdgeAI خود انتخاب کنید
4. ملاحظات کلیدی برای استقرار مدل‌های هوش مصنوعی بر روی دستگاه‌های لبه را درک کنید
5. سیستم خود را برای تمرین‌های عملی در دوره آماده کنید

## منابع اضافی

### مستندات رسمی
- **مستندات پایتون**: مستندات رسمی زبان پایتون
- **مستندات Microsoft .NET**: منابع رسمی توسعه .NET
- **مستندات ONNX Runtime**: راهنمای جامع ONNX Runtime
- **مستندات TensorFlow Lite**: مستندات رسمی TensorFlow Lite

### ابزارهای توسعه
- **Visual Studio Code**: ویرایشگر کد سبک با افزونه‌های توسعه هوش مصنوعی
- **Jupyter Notebooks**: محیط محاسبات تعاملی برای آزمایش‌های یادگیری ماشین
- **Docker**: پلتفرم کانتینرسازی برای محیط‌های توسعه سازگار
- **Git**: سیستم کنترل نسخه برای مدیریت کد

### منابع یادگیری
- **مقالات تحقیقاتی EdgeAI**: جدیدترین تحقیقات آکادمیک درباره مدل‌های کارآمد
- **دوره‌های آنلاین**: مواد آموزشی مکمل درباره بهینه‌سازی هوش مصنوعی
- **فروم‌های جامعه**: پلتفرم‌های پرسش و پاسخ برای چالش‌های توسعه EdgeAI
- **مجموعه داده‌های معیار**: مجموعه داده‌های استاندارد برای ارزیابی عملکرد مدل

## نتایج یادگیری

پس از تکمیل این راهنمای آماده‌سازی، شما:

1. یک محیط توسعه کاملاً پیکربندی‌شده برای توسعه EdgeAI خواهید داشت
2. نیازمندی‌های سخت‌افزاری و نرم‌افزاری برای سناریوهای مختلف استقرار را درک خواهید کرد
3. با چارچوب‌ها و ابزارهای کلیدی مورد استفاده در طول دوره آشنا خواهید شد
4. قادر خواهید بود مدل‌های مناسب را بر اساس محدودیت‌ها و نیازهای دستگاه انتخاب کنید
5. دانش ضروری درباره تکنیک‌های بهینه‌سازی برای استقرار در لبه را خواهید داشت

## ➡️ مرحله بعدی

- [04: سخت‌افزار و استقرار EdgeAI](04.EdgeDeployment.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.