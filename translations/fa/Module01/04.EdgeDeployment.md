<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1f5ed7b55962c3dccafd3da6f9ec252",
  "translation_date": "2025-09-17T15:30:00+00:00",
  "source_file": "Module01/04.EdgeDeployment.md",
  "language_code": "fa"
}
-->
# بخش ۴: پلتفرم‌های سخت‌افزاری برای استقرار هوش مصنوعی در لبه

استقرار هوش مصنوعی در لبه، نتیجه بهینه‌سازی مدل و انتخاب سخت‌افزار است که قابلیت‌های هوشمند را مستقیماً به دستگاه‌هایی که داده تولید می‌کنند، می‌آورد. این بخش به بررسی ملاحظات عملی، نیازمندی‌های سخت‌افزاری و مزایای استراتژیک استقرار هوش مصنوعی در لبه در پلتفرم‌های مختلف می‌پردازد، با تمرکز بر راه‌حل‌های سخت‌افزاری پیشرو از Intel، Qualcomm، NVIDIA و کامپیوترهای Windows AI.

## منابع برای توسعه‌دهندگان

### مستندات و منابع آموزشی
- [Microsoft Learn: توسعه هوش مصنوعی در لبه](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)
- [منابع هوش مصنوعی لبه Intel](https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/edge-ai.html)
- [منابع توسعه‌دهنده هوش مصنوعی Qualcomm](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- [مستندات NVIDIA Jetson](https://developer.nvidia.com/embedded/learn/getting-started-jetson)
- [مستندات Windows AI](https://learn.microsoft.com/windows/ai/)

### ابزارها و SDKها
- [ONNX Runtime](https://onnxruntime.ai/) - چارچوب استنتاج چندپلتفرمی
- [OpenVINO Toolkit](https://docs.openvino.ai/) - ابزار بهینه‌سازی Intel
- [TensorRT](https://developer.nvidia.com/tensorrt) - SDK استنتاج با عملکرد بالا از NVIDIA
- [DirectML](https://learn.microsoft.com/windows/ai/directml/dml-intro) - API یادگیری ماشین با شتاب سخت‌افزاری از Microsoft

## مقدمه

در این بخش، جنبه‌های عملی استقرار مدل‌های هوش مصنوعی در دستگاه‌های لبه را بررسی خواهیم کرد. ملاحظات ضروری برای استقرار موفق، انتخاب پلتفرم سخت‌افزاری و استراتژی‌های بهینه‌سازی خاص برای سناریوهای مختلف محاسبات لبه را پوشش خواهیم داد.

## اهداف آموزشی

در پایان این بخش، شما قادر خواهید بود:

- ملاحظات کلیدی برای استقرار موفق هوش مصنوعی در لبه را درک کنید
- پلتفرم‌های سخت‌افزاری مناسب برای بارهای کاری مختلف هوش مصنوعی در لبه را شناسایی کنید
- مزایا و معایب راه‌حل‌های سخت‌افزاری مختلف هوش مصنوعی در لبه را بشناسید
- تکنیک‌های بهینه‌سازی خاص برای پلتفرم‌های سخت‌افزاری مختلف هوش مصنوعی در لبه را اعمال کنید

## ملاحظات استقرار هوش مصنوعی در لبه

استقرار هوش مصنوعی در دستگاه‌های لبه چالش‌ها و نیازمندی‌های منحصر به فردی نسبت به استقرار در فضای ابری دارد. اجرای موفق هوش مصنوعی در لبه نیازمند توجه دقیق به چندین عامل است:

### محدودیت‌های منابع سخت‌افزاری

دستگاه‌های لبه معمولاً منابع محاسباتی محدودی نسبت به زیرساخت‌های ابری دارند:

- **محدودیت‌های حافظه**: بسیاری از دستگاه‌های لبه دارای RAM محدود (چند مگابایت تا چند گیگابایت) هستند
- **محدودیت‌های ذخیره‌سازی**: فضای ذخیره‌سازی محدود بر اندازه مدل و مدیریت داده تأثیر می‌گذارد
- **قدرت پردازش**: محدودیت‌های CPU/GPU/NPU بر سرعت استنتاج تأثیر می‌گذارد
- **مصرف انرژی**: بسیاری از دستگاه‌های لبه با باتری کار می‌کنند یا محدودیت‌های حرارتی دارند

### ملاحظات اتصال

هوش مصنوعی در لبه باید با اتصال متغیر به خوبی عمل کند:

- **اتصال متناوب**: عملیات باید در زمان قطعی شبکه ادامه یابد
- **محدودیت‌های پهنای باند**: قابلیت‌های انتقال داده کمتر نسبت به مراکز داده
- **نیازهای تأخیر**: بسیاری از برنامه‌ها نیاز به پردازش بلادرنگ یا نزدیک به بلادرنگ دارند
- **همگام‌سازی داده‌ها**: مدیریت پردازش محلی با همگام‌سازی دوره‌ای با فضای ابری

### نیازهای امنیتی و حریم خصوصی

هوش مصنوعی در لبه چالش‌های امنیتی خاصی را معرفی می‌کند:

- **امنیت فیزیکی**: دستگاه‌ها ممکن است در مکان‌های قابل دسترسی فیزیکی مستقر شوند
- **حفاظت از داده‌ها**: پردازش داده‌های حساس در دستگاه‌های بالقوه آسیب‌پذیر
- **احراز هویت**: کنترل دسترسی امن برای عملکرد دستگاه‌های لبه
- **مدیریت به‌روزرسانی**: مکانیزم‌های امن برای به‌روزرسانی مدل و نرم‌افزار

### استقرار و مدیریت

ملاحظات عملی استقرار شامل موارد زیر است:

- **مدیریت ناوگان**: بسیاری از استقرارهای لبه شامل تعداد زیادی دستگاه توزیع‌شده هستند
- **کنترل نسخه**: مدیریت نسخه‌های مدل در دستگاه‌های توزیع‌شده
- **نظارت**: ردیابی عملکرد و تشخیص ناهنجاری در لبه
- **مدیریت چرخه عمر**: از استقرار اولیه تا به‌روزرسانی‌ها و بازنشستگی

## گزینه‌های پلتفرم سخت‌افزاری برای هوش مصنوعی در لبه

### راه‌حل‌های هوش مصنوعی لبه Intel

Intel چندین پلتفرم سخت‌افزاری بهینه‌شده برای استقرار هوش مصنوعی در لبه ارائه می‌دهد:

#### Intel NUC

Intel NUC (واحد محاسباتی بعدی) عملکرد کلاس دسکتاپ را در یک فرم فاکتور جمع‌وجور ارائه می‌دهد:

- **پردازنده‌های Intel Core** با گرافیک یکپارچه Iris Xe
- **RAM**: پشتیبانی تا ۶۴ گیگابایت DDR4
- **سازگاری با Neural Compute Stick 2** برای شتاب‌دهی اضافی هوش مصنوعی
- **مناسب برای**: بارهای کاری متوسط تا پیچیده هوش مصنوعی در مکان‌های ثابت با دسترسی به برق

[Intel NUC برای هوش مصنوعی در لبه](https://www.intel.com/content/www/us/en/products/details/nuc.html)

#### واحدهای پردازش دید Intel Movidius (VPUs)

سخت‌افزار تخصصی برای بینایی کامپیوتری و شتاب‌دهی شبکه‌های عصبی:

- **مصرف انرژی فوق‌العاده کم** (۱-۳ وات معمولی)
- **شتاب‌دهی اختصاصی شبکه‌های عصبی**
- **فرم فاکتور جمع‌وجور** برای ادغام در دوربین‌ها و حسگرها
- **مناسب برای**: برنامه‌های بینایی کامپیوتری با محدودیت‌های سخت‌گیرانه انرژی

[Intel Movidius VPU](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html)

#### Intel Neural Compute Stick 2

شتاب‌دهنده شبکه عصبی با قابلیت اتصال USB:

- **Intel Movidius Myriad X VPU**
- **تا ۴ TOPS** عملکرد
- **رابط USB 3.0** برای ادغام آسان
- **مناسب برای**: نمونه‌سازی سریع و افزودن قابلیت‌های هوش مصنوعی به سیستم‌های موجود

[Intel Neural Compute Stick 2](https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html)

#### رویکرد توسعه

Intel ابزار OpenVINO را برای بهینه‌سازی و استقرار مدل‌ها ارائه می‌دهد:

```python
# Example: Using OpenVINO for edge deployment
from openvino.inference_engine import IECore

# Initialize the Inference Engine
ie = IECore()

# Read the network from IR files
net = ie.read_network(model="optimized_model.xml", weights="optimized_model.bin")

# Prepare input and output blobs
input_blob = next(iter(net.input_info))
output_blob = next(iter(net.outputs))

# Load the network to the device (CPU, GPU, MYRIAD, etc.)
exec_net = ie.load_network(network=net, device_name="CPU")

# Prepare input
input_data = preprocess_image("sample.jpg")

# Run inference
result = exec_net.infer(inputs={input_blob: input_data})

# Process output
output = result[output_blob]
```

### راه‌حل‌های هوش مصنوعی Qualcomm

پلتفرم‌های Qualcomm بر برنامه‌های موبایل و تعبیه‌شده تمرکز دارند:

#### Qualcomm Snapdragon

سیستم‌های روی تراشه (SoCs) Snapdragon شامل موارد زیر هستند:

- **Qualcomm AI Engine** با Hexagon DSP
- **Adreno GPU** برای گرافیک و محاسبات موازی
- **هسته‌های Kryo CPU** برای پردازش عمومی
- **مناسب برای**: گوشی‌های هوشمند، تبلت‌ها، هدست‌های XR و دوربین‌های هوشمند

[Qualcomm Snapdragon برای هوش مصنوعی در لبه](https://www.qualcomm.com/products/mobile/snapdragon/pcs/product-list)

#### Qualcomm Cloud AI 100

شتاب‌دهنده استنتاج هوش مصنوعی اختصاصی:

- **تا ۴۰۰ TOPS** عملکرد هوش مصنوعی
- **بهینه‌سازی مصرف انرژی** برای مراکز داده و استقرار در لبه
- **معماری مقیاس‌پذیر** برای سناریوهای استقرار مختلف
- **مناسب برای**: برنامه‌های هوش مصنوعی با توان بالا در محیط‌های کنترل‌شده

[Qualcomm Cloud AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence)

#### پلتفرم رباتیک Qualcomm RB5/RB6

طراحی‌شده برای رباتیک و محاسبات پیشرفته در لبه:

- **اتصال ۵G یکپارچه**
- **قابلیت‌های پیشرفته هوش مصنوعی و بینایی کامپیوتری**
- **پشتیبانی جامع از حسگرها**
- **مناسب برای**: ربات‌های خودمختار، پهپادها و سیستم‌های صنعتی هوشمند

[پلتفرم رباتیک Qualcomm](https://www.qualcomm.com/products/application/robotics/qualcomm-robotics-rb6-platform)

#### رویکرد توسعه

Qualcomm SDK پردازش عصبی و ابزار کارایی مدل هوش مصنوعی را ارائه می‌دهد:

```python
# Example: Using Qualcomm Neural Processing SDK
from qti.aisw.dlc_utils import modeltools

# Convert your model to DLC format
converter = modeltools.DlcConverter()
converter.convert(
    input_network="model.tflite",
    input_dim=[1, 224, 224, 3],
    output_path="optimized_model.dlc"
)

# Use SNPE runtime for inference
from qti.aisw.snpe_runtime import SnpeRuntime

# Initialize runtime
runtime = SnpeRuntime()
runtime.load("optimized_model.dlc")

# Prepare input
input_tensor = preprocess_image("sample.jpg")

# Run inference
outputs = runtime.execute(input_tensor)

# Process results
predictions = postprocess_output(outputs)
```

### 🎮 راه‌حل‌های هوش مصنوعی لبه NVIDIA

NVIDIA پلتفرم‌های قدرتمند شتاب‌دهنده GPU برای استقرار در لبه ارائه می‌دهد:

#### خانواده NVIDIA Jetson

پلتفرم‌های محاسباتی هوش مصنوعی طراحی‌شده برای لبه:

##### سری Jetson Orin
- **تا ۲۷۵ TOPS** عملکرد هوش مصنوعی
- **معماری NVIDIA Ampere** GPU
- **پیکربندی‌های توان** از ۵ وات تا ۶۰ وات
- **مناسب برای**: رباتیک پیشرفته، تحلیل ویدئوی هوشمند و دستگاه‌های پزشکی

##### Jetson Nano
- **محاسبات هوش مصنوعی سطح ابتدایی** (۴۷۲ GFLOPS)
- **GPU ۱۲۸ هسته‌ای Maxwell**
- **مصرف انرژی بهینه** (۵-۱۰ وات)
- **مناسب برای**: پروژه‌های سرگرمی، برنامه‌های آموزشی و استقرارهای ساده هوش مصنوعی

[پلتفرم NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)

#### NVIDIA Clara Guardian

پلتفرم برای برنامه‌های هوش مصنوعی در حوزه سلامت:

- **حس‌گر بلادرنگ** برای نظارت بر بیماران
- **ساخته‌شده بر پایه Jetson** یا سرورهای شتاب‌دهنده GPU
- **بهینه‌سازی‌های خاص حوزه سلامت**
- **مناسب برای**: بیمارستان‌های هوشمند، نظارت بر بیماران و تصویربرداری پزشکی

[NVIDIA Clara Guardian](https://developer.nvidia.com/clara)

#### پلتفرم NVIDIA EGX

راه‌حل‌های محاسباتی لبه در سطح سازمانی:

- **مقیاس‌پذیر از GPUهای NVIDIA A100 تا T4**
- **راه‌حل‌های سرور تأییدشده** از شرکای OEM
- **شامل مجموعه نرم‌افزاری NVIDIA AI Enterprise**
- **مناسب برای**: استقرارهای هوش مصنوعی در لبه در مقیاس بزرگ در محیط‌های صنعتی و سازمانی

[پلتفرم NVIDIA EGX](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)

#### رویکرد توسعه

NVIDIA ابزار TensorRT را برای استقرار بهینه مدل‌ها ارائه می‌دهد:

```python
# Example: Using TensorRT for optimized inference
import tensorrt as trt
import numpy as np

# Create builder and network
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

# Parse ONNX model
parser = trt.OnnxParser(network, logger)
with open("model.onnx", "rb") as f:
    parser.parse(f.read())

# Create optimization config
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB
config.set_flag(trt.BuilderFlag.FP16)

# Build and serialize engine
engine = builder.build_engine(network, config)
with open("model.trt", "wb") as f:
    f.write(engine.serialize())

# Create runtime and execute
runtime = trt.Runtime(logger)
engine = runtime.deserialize_cuda_engine(open("model.trt", "rb").read())
context = engine.create_execution_context()

# Run inference
input_data = preprocess_image("sample.jpg")
output = run_inference(context, input_data, engine)
```

### کامپیوترهای Windows AI

کامپیوترهای Windows AI دسته جدیدی از سخت‌افزارهای هوش مصنوعی در لبه هستند که واحدهای پردازش عصبی (NPUs) تخصصی دارند:

#### Qualcomm Snapdragon X Elite/Plus

نسل اول کامپیوترهای Windows Copilot+ شامل موارد زیر هستند:

- **Hexagon NPU** با عملکرد ۴۵+ TOPS هوش مصنوعی
- **Qualcomm Oryon CPU** با حداکثر ۱۲ هسته
- **Adreno GPU** برای گرافیک و شتاب‌دهی اضافی هوش مصنوعی
- **مناسب برای**: بهره‌وری هوش مصنوعی، تولید محتوا و توسعه نرم‌افزار

[Qualcomm Snapdragon X Elite](https://www.qualcomm.com/snapdragon/laptops)

#### Intel Core Ultra (Meteor Lake و فراتر)

پردازنده‌های کامپیوترهای هوش مصنوعی Intel شامل موارد زیر هستند:

- **Intel AI Boost (NPU)** با عملکرد تا ۱۰ TOPS
- **Intel Arc GPU** ارائه‌دهنده شتاب‌دهی اضافی هوش مصنوعی
- **هسته‌های CPU عملکرد و کارایی**
- **مناسب برای**: لپ‌تاپ‌های تجاری، ایستگاه‌های کاری خلاقانه و محاسبات روزمره با هوش مصنوعی

[پردازنده‌های Intel Core Ultra](https://www.intel.com/content/www/us/en/products/details/processors/core/ultra.html)

#### سری AMD Ryzen AI

پردازنده‌های متمرکز بر هوش مصنوعی AMD شامل موارد زیر هستند:

- **NPU مبتنی بر XDNA** با عملکرد تا ۱۶ TOPS
- **هسته‌های CPU Zen 4** برای پردازش عمومی
- **گرافیک RDNA 3** برای قابلیت‌های محاسباتی اضافی
- **مناسب برای**: حرفه‌ای‌های خلاق، توسعه‌دهندگان و کاربران قدرتمند

[پردازنده‌های AMD Ryzen AI](https://www.amd.com/en/processors/ryzen-ai.html)

#### رویکرد توسعه

کامپیوترهای Windows AI از پلتفرم توسعه‌دهنده Windows و DirectML استفاده می‌کنند:

```csharp
// Example: Using Windows App SDK and DirectML
using Microsoft.AI.DirectML;
using Microsoft.Windows.AI;

// Load model
var modelPath = "optimized_model.onnx";
var modelOptions = new OnnxModelOptions
{
    InterOpNumThreads = 4,
    IntraOpNumThreads = 4
};

// Create model
var model = await OnnxModel.CreateFromFileAsync(modelPath, modelOptions);

// Prepare input
var inputFeatures = new List<InputFeature>
{
    new InputFeature
    {
        Name = "input",
        Value = imageData
    }
};

// Run inference
var results = await model.EvaluateAsync(inputFeatures);

// Process output
var output = results.First().Value;
```

## ⚡ تکنیک‌های بهینه‌سازی خاص سخت‌افزار

### 🔍 رویکردهای کمینه‌سازی

پلتفرم‌های سخت‌افزاری مختلف از تکنیک‌های کمینه‌سازی خاص بهره می‌برند:

#### بهینه‌سازی‌های Intel OpenVINO
- **کمینه‌سازی INT8** برای CPU و GPU یکپارچه
- **دقت FP16** برای بهبود عملکرد با حداقل کاهش دقت
- **کمینه‌سازی نامتقارن** برای مدیریت توزیع‌های فعال‌سازی

#### بهینه‌سازی‌های Qualcomm AI Engine
- **کمینه‌سازی UINT8** برای Hexagon DSP
- **دقت ترکیبی** با استفاده از تمام واحدهای محاسباتی موجود
- **کمینه‌سازی کانال‌محور** برای بهبود دقت

#### بهینه‌سازی‌های NVIDIA TensorRT
- **دقت INT8 و FP16** برای شتاب‌دهی GPU
- **ادغام لایه‌ها** برای کاهش انتقال حافظه
- **تنظیم خودکار هسته‌ها** برای معماری‌های خاص GPU

#### بهینه‌سازی‌های NPU Windows
- **کمینه‌سازی INT8/INT4** برای اجرای NPU
- **بهینه‌سازی گراف DirectML**
- **شتاب‌دهی زمان اجرای Windows ML**

### تطبیق‌های خاص معماری

سخت‌افزارهای مختلف نیازمند ملاحظات معماری خاص هستند:

- **Intel**: بهینه‌سازی برای دستورالعمل‌های برداری AVX-512 و Intel Deep Learning Boost
- **Qualcomm**: استفاده از محاسبات ناهمگن در Hexagon DSP، Adreno GPU و Kryo CPU
- **NVIDIA**: حداکثر استفاده از موازی‌سازی GPU و هسته‌های CUDA
- **Windows NPU**: طراحی برای پردازش همکاری NPU-CPU-GPU

### استراتژی‌های مدیریت حافظه

مدیریت مؤثر حافظه بسته به پلتفرم متفاوت است:

- **Intel**: بهینه‌سازی برای استفاده از کش و الگوهای دسترسی به حافظه
- **Qualcomm**: مدیریت حافظه مشترک در پردازنده‌های ناهمگن
- **NVIDIA**: استفاده از حافظه یکپارچه CUDA و بهینه‌سازی استفاده از VRAM
- **Windows NPU**: تعادل بارهای کاری بین حافظه اختصاصی NPU و RAM سیستم

## معیارهای ارزیابی عملکرد

هنگام ارزیابی استقرار هوش مصنوعی در لبه، این معیارهای کلیدی را در نظر بگیرید:

### معیارهای عملکرد

- **زمان استنتاج**: میلی‌ثانیه‌ها برای هر استنتاج (کمتر بهتر است)
- **توان عملیاتی**: استنتاج‌ها در هر ثانیه (بیشتر بهتر است)
- **تأخیر**: زمان پاسخ‌دهی انتها به انتها (کمتر بهتر است)
- **FPS**: فریم‌ها در هر ثانیه برای برنامه‌های بینایی (بیشتر بهتر است)

### معیارهای کارایی

- **عملکرد به ازای هر وات**: TOPS/W یا استنتاج‌ها/ثانیه/وات
- **انرژی به ازای هر استنتاج**: ژول مصرف‌شده برای هر استنتاج
- **تأثیر باتری**: کاهش زمان اجرا هنگام اجرای بارهای کاری هوش مصنوعی
- **کارایی حرارتی**: افزایش دما در طول عملیات پایدار

### معیارهای دقت

- **دقت Top-1/Top-5**: درصد صحت طبقه‌بندی
- **mAP**: میانگین دقت برای تشخیص اشیاء
- **امتیاز F1**: تعادل دقت و فراخوانی
- **تأثیر کمینه‌سازی**: تفاوت دقت بین مدل‌های با دقت کامل و کمینه‌شده

## الگوهای استقرار و بهترین روش‌ها

### استراتژی‌های استقرار سازمانی

- **کانتینری‌سازی**: استفاده از Docker یا مشابه برای استقرار سازگار
- **مدیریت
- **مدیریت به‌روزرسانی**: مکانیزم‌های به‌روزرسانی OTA برای مدل‌ها و نرم‌افزارها

### الگوهای ترکیبی ابر-لبه

- **آموزش در ابر، استنتاج در لبه**: آموزش در ابر، استقرار در لبه
- **پیش‌پردازش در لبه، تحلیل در ابر**: پردازش اولیه در لبه، تحلیل پیچیده در ابر
- **یادگیری فدرال**: بهبود مدل به صورت توزیع‌شده بدون متمرکز کردن داده‌ها
- **یادگیری افزایشی**: بهبود مداوم مدل از داده‌های لبه

### الگوهای یکپارچه‌سازی

- **یکپارچه‌سازی حسگرها**: اتصال مستقیم به دوربین‌ها، میکروفون‌ها و سایر حسگرها
- **کنترل عملگرها**: کنترل بلادرنگ موتورها، نمایشگرها و خروجی‌های دیگر
- **یکپارچه‌سازی سیستم‌ها**: ارتباط با سیستم‌های موجود در سازمان
- **یکپارچه‌سازی اینترنت اشیا**: اتصال به اکوسیستم‌های گسترده اینترنت اشیا

## ملاحظات استقرار خاص صنعت

### مراقبت‌های بهداشتی

- **حریم خصوصی بیماران**: رعایت قوانین HIPAA برای داده‌های پزشکی
- **مقررات دستگاه‌های پزشکی**: الزامات FDA و سایر مقررات
- **نیازهای قابلیت اطمینان**: تحمل خطا برای برنامه‌های حیاتی
- **استانداردهای یکپارچه‌سازی**: FHIR، HL7 و سایر استانداردهای تعامل‌پذیری در مراقبت‌های بهداشتی

### تولید

- **محیط صنعتی**: مقاوم‌سازی برای شرایط سخت
- **نیازهای بلادرنگ**: عملکرد قطعی برای سیستم‌های کنترلی
- **سیستم‌های ایمنی**: یکپارچه‌سازی با پروتکل‌های ایمنی صنعتی
- **یکپارچه‌سازی سیستم‌های قدیمی**: اتصال به زیرساخت‌های OT موجود

### خودروسازی

- **ایمنی عملکردی**: رعایت استاندارد ISO 26262
- **مقاوم‌سازی محیطی**: عملکرد در شرایط دمایی شدید
- **مدیریت توان**: عملکرد بهینه برای مصرف باتری
- **مدیریت چرخه عمر**: پشتیبانی بلندمدت برای طول عمر خودروها

### شهرهای هوشمند

- **استقرار در فضای باز**: مقاومت در برابر شرایط جوی و امنیت فیزیکی
- **مدیریت مقیاس**: از هزاران تا میلیون‌ها دستگاه توزیع‌شده
- **تغییرات شبکه**: عملکرد با اتصال ناپایدار
- **ملاحظات حریم خصوصی**: مدیریت مسئولانه داده‌های فضای عمومی

## روندهای آینده در سخت‌افزار هوش مصنوعی لبه

### توسعه‌های نوظهور سخت‌افزار

- **سیلیکون مخصوص هوش مصنوعی**: واحدهای پردازش عصبی (NPU) و شتاب‌دهنده‌های هوش مصنوعی تخصصی‌تر
- **محاسبات نورومورفیک**: معماری‌های الهام‌گرفته از مغز برای بهبود بهره‌وری
- **محاسبات در حافظه**: کاهش جابجایی داده‌ها برای عملیات هوش مصنوعی
- **بسته‌بندی چند‌تراشه‌ای**: یکپارچه‌سازی ناهمگن پردازنده‌های تخصصی هوش مصنوعی

### هم‌تکاملی نرم‌افزار-سخت‌افزار

- **جستجوی معماری عصبی آگاه به سخت‌افزار**: مدل‌های بهینه‌شده برای سخت‌افزار خاص
- **پیشرفت‌های کامپایلر**: ترجمه بهتر مدل‌ها به دستورالعمل‌های سخت‌افزاری
- **بهینه‌سازی‌های گراف تخصصی**: تبدیل شبکه‌ها برای سخت‌افزار خاص
- **انطباق پویا**: بهینه‌سازی در زمان اجرا بر اساس منابع موجود

### تلاش‌های استانداردسازی

- **ONNX و ONNX Runtime**: تعامل‌پذیری مدل در پلتفرم‌های مختلف
- **MLIR**: نمایش میانی چندسطحی برای یادگیری ماشین
- **OpenXLA**: کامپایل شتاب‌یافته جبر خطی
- **TMUL**: لایه‌های انتزاعی پردازنده‌های تنسور

## شروع به کار با استقرار هوش مصنوعی لبه

### تنظیم محیط توسعه

1. **انتخاب سخت‌افزار هدف**: انتخاب پلتفرم مناسب برای مورد استفاده شما
2. **نصب SDKها و ابزارها**: راه‌اندازی کیت توسعه سازنده
3. **پیکربندی ابزارهای بهینه‌سازی**: نصب نرم‌افزارهای کوانت‌سازی و کامپایل
4. **راه‌اندازی خط لوله CI/CD**: ایجاد جریان کاری تست و استقرار خودکار

### چک‌لیست استقرار

- **بهینه‌سازی مدل**: کوانت‌سازی، هرس و بهینه‌سازی معماری
- **تست عملکرد**: بنچمارک روی سخت‌افزار هدف تحت شرایط واقعی
- **تحلیل توان**: اندازه‌گیری الگوهای مصرف انرژی
- **ممیزی امنیت**: بررسی حفاظت داده و کنترل‌های دسترسی
- **مکانیزم به‌روزرسانی**: پیاده‌سازی قابلیت‌های به‌روزرسانی امن
- **تنظیم نظارت**: استقرار جمع‌آوری داده‌های تله‌متری و هشداردهی

## ➡️ گام بعدی

- مرور [نمای کلی ماژول 1](./README.md)
- بررسی [ماژول 2: مبانی مدل‌های زبان کوچک](../Module02/README.md)
- ادامه به [ماژول 3: استراتژی‌های استقرار SLM](../Module03/README.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.