<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "142e0d1a5b794b8333cfd4895804ced5",
  "translation_date": "2025-09-17T16:05:59+00:00",
  "source_file": "Module07/README.md",
  "language_code": "fa"
}
-->
# فصل ۰۷: نمونه‌های EdgeAI

هوش مصنوعی در لبه (Edge AI) ترکیبی از هوش مصنوعی و محاسبات لبه است که امکان پردازش هوشمند را مستقیماً روی دستگاه‌ها فراهم می‌کند، بدون نیاز به اتصال به ابر. این فصل پنج پیاده‌سازی متفاوت EdgeAI را در پلتفرم‌ها و چارچوب‌های مختلف بررسی می‌کند و انعطاف‌پذیری و قدرت اجرای مدل‌های هوش مصنوعی در لبه را به نمایش می‌گذارد.

## ۱. EdgeAI در NVIDIA Jetson Orin Nano

NVIDIA Jetson Orin Nano یک پیشرفت بزرگ در محاسبات هوش مصنوعی لبه‌ای قابل دسترس است که تا ۶۷ TOPS عملکرد هوش مصنوعی را در یک فرم کوچک به اندازه کارت اعتباری ارائه می‌دهد. این پلتفرم قدرتمند هوش مصنوعی لبه‌ای توسعه هوش مصنوعی مولد را برای علاقه‌مندان، دانش‌آموزان و توسعه‌دهندگان حرفه‌ای دموکراتیک می‌کند.

### ویژگی‌های کلیدی
- ارائه تا ۶۷ TOPS عملکرد هوش مصنوعی—بهبود ۱.۷ برابر نسبت به نسل قبلی
- ۱۰۲۴ هسته CUDA و تا ۳۲ هسته Tensor برای پردازش هوش مصنوعی
- پردازنده ۶ هسته‌ای Arm Cortex-A78AE v8.2 ۶۴ بیتی با فرکانس حداکثر ۱.۵ گیگاهرتز
- قیمت فقط ۲۴۹ دلار، ارائه پلتفرمی مقرون‌به‌صرفه و قابل دسترس برای توسعه‌دهندگان، دانش‌آموزان و سازندگان

### کاربردها
Jetson Orin Nano در اجرای مدل‌های هوش مصنوعی مولد مدرن از جمله Vision Transformers، مدل‌های زبان بزرگ و مدل‌های ترکیبی تصویر-زبان بسیار موفق است. این دستگاه به طور خاص برای موارد استفاده GenAI طراحی شده و اکنون می‌توانید چندین مدل LLM را روی یک دستگاه کوچک اجرا کنید. موارد استفاده محبوب شامل رباتیک هوش مصنوعی، پهپادهای هوشمند، دوربین‌های هوشمند و دستگاه‌های خودمختار لبه‌ای است.

**اطلاعات بیشتر**: [سوپرکامپیوتر Jetson Orin Nano NVIDIA: گام بزرگ بعدی در EdgeAI](https://medium.com/data-science-in-your-pocket/nvidias-jetson-orin-nano-supercomputer-the-next-big-thing-in-edgeai-e9eff687ae62)

## ۲. EdgeAI در اپلیکیشن‌های موبایل با .NET MAUI و ONNX Runtime GenAI

این راه‌حل نشان می‌دهد که چگونه می‌توان هوش مصنوعی مولد و مدل‌های زبان بزرگ (LLMs) را در اپلیکیشن‌های موبایل چندپلتفرمی با استفاده از .NET MAUI (رابط کاربری اپلیکیشن چندپلتفرمی) و ONNX Runtime GenAI ادغام کرد. این رویکرد به توسعه‌دهندگان .NET امکان می‌دهد اپلیکیشن‌های موبایل پیشرفته‌ای بسازند که به صورت بومی روی دستگاه‌های Android و iOS اجرا شوند.

### ویژگی‌های کلیدی
- ساخته شده بر اساس چارچوب .NET MAUI، ارائه یک کدبیس واحد برای اپلیکیشن‌های Android و iOS
- ادغام ONNX Runtime GenAI امکان اجرای مدل‌های هوش مصنوعی مولد را مستقیماً روی دستگاه‌های موبایل فراهم می‌کند
- پشتیبانی از شتاب‌دهنده‌های سخت‌افزاری مختلف برای دستگاه‌های موبایل، از جمله CPU، GPU و پردازنده‌های هوش مصنوعی تخصصی موبایل
- بهینه‌سازی‌های پلتفرم خاص مانند CoreML برای iOS و NNAPI برای Android از طریق ONNX Runtime
- اجرای کامل چرخه هوش مصنوعی مولد شامل پیش‌پردازش و پس‌پردازش، استنتاج، پردازش logits، جستجو و نمونه‌گیری، و مدیریت حافظه کش KV

### مزایای توسعه
رویکرد .NET MAUI به توسعه‌دهندگان امکان می‌دهد از مهارت‌های موجود در C# و .NET خود استفاده کنند و در عین حال اپلیکیشن‌های هوش مصنوعی چندپلتفرمی بسازند. چارچوب ONNX Runtime GenAI از معماری‌های مدل مختلف از جمله Llama، Mistral، Phi، Gemma و بسیاری دیگر پشتیبانی می‌کند. هسته‌های ARM64 بهینه‌سازی شده، ضرب ماتریس‌های INT4 کم‌کمیت را تسریع می‌کنند و عملکرد کارآمدی را روی سخت‌افزار موبایل ارائه می‌دهند، در حالی که تجربه توسعه .NET آشنا حفظ می‌شود.

### موارد استفاده
این راه‌حل برای توسعه‌دهندگانی که می‌خواهند اپلیکیشن‌های موبایل هوش مصنوعی بسازند، از جمله چت‌بات‌های هوشمند، اپلیکیشن‌های تشخیص تصویر، ابزارهای ترجمه زبان و سیستم‌های توصیه شخصی‌سازی شده که به طور کامل روی دستگاه اجرا می‌شوند و قابلیت حفظ حریم خصوصی و عملکرد آفلاین را ارائه می‌دهند، ایده‌آل است.

**اطلاعات بیشتر**: [.NET MAUI ONNX Runtime GenAI Example](https://github.com/microsoft/onnxruntime-genai/tree/jialli/genny-maui/examples/csharp/GennyMaui)

## ۳. EdgeAI در Azure با موتور مدل‌های زبان کوچک

راه‌حل EdgeAI مبتنی بر Azure مایکروسافت بر استقرار کارآمد مدل‌های زبان کوچک (SLMs) در محیط‌های ترکیبی ابر-لبه تمرکز دارد. این رویکرد شکاف بین خدمات هوش مصنوعی در مقیاس ابر و نیازهای استقرار لبه را پر می‌کند.

### مزایای معماری
- ادغام بی‌نقص با خدمات هوش مصنوعی Azure
- اجرای SLMs/LLMs و مدل‌های چندوجهی روی دستگاه و در ابر با ONNX Runtime
- بهینه‌سازی شده برای استقرار در مقیاس سازمانی
- پشتیبانی از به‌روزرسانی و مدیریت مداوم مدل‌ها

### موارد استفاده
پیاده‌سازی EdgeAI Azure در سناریوهایی که نیاز به استقرار هوش مصنوعی در سطح سازمانی با قابلیت‌های مدیریت ابر دارند، بسیار موفق است. این شامل پردازش هوشمند اسناد، تحلیل‌های بلادرنگ و جریان‌های کاری هوش مصنوعی ترکیبی است که از منابع محاسباتی ابر و لبه استفاده می‌کنند.

**اطلاعات بیشتر**: [Azure EdgeAI SLM Engine](https://github.com/microsoft/onnxruntime-genai/tree/main/examples/slm_engine)

## ۴. EdgeAI با Windows ML

Windows ML نمایانگر پیشرفته‌ترین زمان اجرای مایکروسافت برای استنتاج مدل روی دستگاه و استقرار ساده است که به عنوان پایه‌ای برای Windows AI Foundry عمل می‌کند. این پلتفرم به توسعه‌دهندگان امکان می‌دهد اپلیکیشن‌های هوش مصنوعی ویندوزی بسازند که از تمام سخت‌افزارهای PC بهره‌مند شوند.

### قابلیت‌های پلتفرم
- کارکرد روی تمام PCهای ویندوز ۱۱ با نسخه ۲۴H2 (بیلد ۲۶۱۰۰) یا بالاتر
- کارکرد روی تمام سخت‌افزارهای PC x64 و ARM64، حتی PCهایی که NPU یا GPU ندارند
- امکان استفاده از مدل‌های خود توسعه‌دهندگان و استقرار کارآمد آن‌ها در اکوسیستم شرکای سیلیکونی شامل AMD، Intel، NVIDIA و Qualcomm در CPU، GPU، NPU
- با استفاده از APIهای زیرساختی، توسعه‌دهندگان دیگر نیازی به ایجاد چندین نسخه از اپلیکیشن خود برای هدف‌گذاری سیلیکون‌های مختلف ندارند

### مزایای توسعه‌دهندگان
Windows ML سخت‌افزار و ارائه‌دهندگان اجرا را انتزاع می‌کند، بنابراین شما می‌توانید روی نوشتن کد خود تمرکز کنید. علاوه بر این، Windows ML به طور خودکار به‌روزرسانی می‌شود تا از جدیدترین NPUها، GPUها و CPUها پشتیبانی کند. این پلتفرم یک چارچوب یکپارچه برای توسعه هوش مصنوعی در اکوسیستم سخت‌افزاری متنوع ویندوز ارائه می‌دهد.

**اطلاعات بیشتر**: 
- [Windows ML Overview](https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview)
- [راهنمای توسعه EdgeAI ویندوز](../windowdeveloper.md) - راهنمای جامع برای توسعه هوش مصنوعی لبه‌ای ویندوز

## ۵. EdgeAI با اپلیکیشن‌های محلی Foundry

Foundry Local به توسعه‌دهندگان امکان می‌دهد اپلیکیشن‌های تولید بازیابی‌شده (RAG) را با استفاده از منابع محلی در .NET بسازند، ترکیب مدل‌های زبان محلی با قابلیت‌های جستجوی معنایی. این رویکرد راه‌حل‌های هوش مصنوعی متمرکز بر حفظ حریم خصوصی را ارائه می‌دهد که به طور کامل روی زیرساخت‌های محلی عمل می‌کنند.

### معماری فنی
- ترکیب مدل زبان Phi-3، تعبیه‌های محلی و هسته معنایی برای ایجاد سناریوی RAG
- استفاده از تعبیه‌ها به عنوان بردارهایی (آرایه‌ها) از مقادیر شناور که محتوا و معنای معنایی آن را نشان می‌دهند
- هسته معنایی به عنوان هماهنگ‌کننده اصلی عمل می‌کند، Phi-3 و اجزای هوشمند را ادغام می‌کند تا یک خط لوله RAG یکپارچه ایجاد کند
- پشتیبانی از پایگاه‌های داده برداری محلی از جمله SQLite و Qdrant

### مزایای پیاده‌سازی
RAG، یا تولید بازیابی‌شده، فقط یک روش پیچیده برای گفتن "جستجو کردن برخی اطلاعات و قرار دادن آن‌ها در درخواست" است. این پیاده‌سازی محلی حفظ حریم خصوصی داده‌ها را تضمین می‌کند و در عین حال پاسخ‌های هوشمند مبتنی بر پایگاه‌های دانش سفارشی ارائه می‌دهد. این رویکرد به ویژه برای سناریوهای سازمانی که نیاز به حاکمیت داده‌ها و قابلیت‌های عملیاتی آفلاین دارند، ارزشمند است.

**اطلاعات بیشتر**: [نمونه‌های RAG محلی Foundry](https://github.com/microsoft/Foundry-Local/tree/main/samples/dotNET/rag)

## منابع توسعه EdgeAI ویندوز

برای توسعه‌دهندگانی که به طور خاص پلتفرم ویندوز را هدف قرار می‌دهند، ما یک راهنمای جامع ایجاد کرده‌ایم که اکوسیستم کامل EdgeAI ویندوز را پوشش می‌دهد. این منبع اطلاعات دقیقی درباره Windows AI Foundry، از جمله APIها، ابزارها و بهترین روش‌ها برای توسعه EdgeAI در ویندوز ارائه می‌دهد.

### پلتفرم Windows AI Foundry
پلتفرم Windows AI Foundry مجموعه کاملی از ابزارها و APIها را ارائه می‌دهد که به طور خاص برای توسعه هوش مصنوعی لبه‌ای روی دستگاه‌های ویندوز طراحی شده‌اند. این شامل پشتیبانی تخصصی از سخت‌افزار شتاب‌دهنده NPU، ادغام Windows ML و تکنیک‌های بهینه‌سازی پلتفرم است.

**راهنمای جامع**: [راهنمای توسعه EdgeAI ویندوز](../windowdeveloper.md)

این راهنما شامل موارد زیر است:
- نمای کلی پلتفرم Windows AI Foundry و اجزای آن
- API Phi Silica برای استنتاج کارآمد روی سخت‌افزار NPU
- APIهای بینایی کامپیوتری برای پردازش تصویر و OCR
- ادغام و بهینه‌سازی زمان اجرای Windows ML
- CLI Foundry Local برای توسعه و آزمایش محلی
- استراتژی‌های بهینه‌سازی سخت‌افزار برای دستگاه‌های ویندوز
- مثال‌های عملی پیاده‌سازی و بهترین روش‌ها

### ابزار هوش مصنوعی برای توسعه EdgeAI
برای توسعه‌دهندگانی که از Visual Studio Code استفاده می‌کنند، افزونه ابزار هوش مصنوعی یک محیط توسعه جامع ارائه می‌دهد که به طور خاص برای ساخت، آزمایش و استقرار اپلیکیشن‌های EdgeAI طراحی شده است. این ابزار کل جریان کاری توسعه EdgeAI را در VS Code ساده می‌کند.

**راهنمای توسعه**: [ابزار هوش مصنوعی برای توسعه EdgeAI](../aitoolkit.md)

راهنمای ابزار هوش مصنوعی شامل موارد زیر است:
- کشف و انتخاب مدل برای استقرار لبه
- جریان‌های کاری آزمایش و بهینه‌سازی محلی
- ادغام ONNX و Ollama برای مدل‌های لبه
- تکنیک‌های تبدیل و کم‌کمیت مدل
- توسعه عامل برای سناریوهای لبه
- ارزیابی عملکرد و نظارت
- آماده‌سازی استقرار و بهترین روش‌ها

## نتیجه‌گیری

این پنج پیاده‌سازی EdgeAI بلوغ و تنوع راه‌حل‌های هوش مصنوعی لبه‌ای موجود امروز را نشان می‌دهند. از دستگاه‌های لبه‌ای شتاب‌دهنده سخت‌افزاری مانند Jetson Orin Nano گرفته تا چارچوب‌های نرم‌افزاری مانند ONNX Runtime GenAI و Windows ML، توسعه‌دهندگان گزینه‌های بی‌سابقه‌ای برای استقرار اپلیکیشن‌های هوشمند در لبه دارند.

نخ مشترک در میان همه این پلتفرم‌ها دموکراتیک کردن قابلیت‌های هوش مصنوعی است، که یادگیری ماشین پیشرفته را برای توسعه‌دهندگان در سطوح مهارتی و موارد استفاده مختلف قابل دسترس می‌کند. چه در حال ساخت اپلیکیشن‌های موبایل، نرم‌افزار دسکتاپ یا سیستم‌های جاسازی شده باشید، این راه‌حل‌های EdgeAI پایه‌ای برای نسل بعدی اپلیکیشن‌های هوشمند فراهم می‌کنند که به طور کارآمد و خصوصی در لبه عمل می‌کنند.

هر پلتفرم مزایای منحصر به فردی ارائه می‌دهد: Jetson Orin Nano برای محاسبات لبه‌ای شتاب‌دهنده سخت‌افزاری، ONNX Runtime GenAI برای توسعه موبایل چندپلتفرمی، Azure EdgeAI برای یکپارچگی ابر-لبه سازمانی، Windows ML برای اپلیکیشن‌های بومی ویندوز و Foundry Local برای پیاده‌سازی‌های RAG متمرکز بر حفظ حریم خصوصی. این پلتفرم‌ها با هم یک اکوسیستم جامع برای توسعه EdgeAI را نمایان می‌کنند.

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.