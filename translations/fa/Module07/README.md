<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e9e31a2b5ff0f6a682a258fa859a8ff5",
  "translation_date": "2025-09-26T19:23:27+00:00",
  "source_file": "Module07/README.md",
  "language_code": "fa"
}
-->
# فصل ۰۷: نمونه‌های EdgeAI

هوش مصنوعی در لبه (Edge AI) ترکیبی از هوش مصنوعی و محاسبات لبه است که امکان پردازش هوشمند را مستقیماً بر روی دستگاه‌ها بدون نیاز به اتصال به ابر فراهم می‌کند. این فصل به بررسی پنج پیاده‌سازی متفاوت EdgeAI در پلتفرم‌ها و چارچوب‌های مختلف می‌پردازد و انعطاف‌پذیری و قدرت اجرای مدل‌های هوش مصنوعی در لبه را نشان می‌دهد.

## ۱. EdgeAI در NVIDIA Jetson Orin Nano

NVIDIA Jetson Orin Nano یک پیشرفت بزرگ در محاسبات هوش مصنوعی لبه‌ای قابل دسترس است که تا ۶۷ TOPS عملکرد هوش مصنوعی را در یک فرم کوچک به اندازه کارت اعتباری ارائه می‌دهد. این پلتفرم قدرتمند هوش مصنوعی لبه‌ای توسعه هوش مصنوعی مولد را برای علاقه‌مندان، دانش‌آموزان و توسعه‌دهندگان حرفه‌ای دموکراتیک می‌کند.

### ویژگی‌های کلیدی
- ارائه تا ۶۷ TOPS عملکرد هوش مصنوعی—بهبود ۱.۷ برابر نسبت به نسل قبلی
- ۱۰۲۴ هسته CUDA و تا ۳۲ هسته Tensor برای پردازش هوش مصنوعی
- پردازنده ۶ هسته‌ای Arm Cortex-A78AE v8.2 ۶۴ بیتی با فرکانس حداکثر ۱.۵ گیگاهرتز
- قیمت فقط ۲۴۹ دلار، ارائه پلتفرمی مقرون‌به‌صرفه و قابل دسترس برای توسعه‌دهندگان، دانش‌آموزان و سازندگان

### کاربردها
Jetson Orin Nano در اجرای مدل‌های مدرن هوش مصنوعی مولد از جمله Vision Transformers، مدل‌های زبان بزرگ و مدل‌های Vision-Language بسیار موفق است. این دستگاه به‌طور خاص برای موارد استفاده GenAI طراحی شده و اکنون می‌توانید چندین مدل LLM را روی یک دستگاه کوچک اجرا کنید. موارد استفاده محبوب شامل رباتیک هوش مصنوعی، پهپادهای هوشمند، دوربین‌های هوشمند و دستگاه‌های خودمختار لبه‌ای است.

**اطلاعات بیشتر**: [سوپرکامپیوتر Jetson Orin Nano NVIDIA: گام بزرگ بعدی در EdgeAI](https://medium.com/data-science-in-your-pocket/nvidias-jetson-orin-nano-supercomputer-the-next-big-thing-in-edgeai-e9eff687ae62)

## ۲. EdgeAI در برنامه‌های موبایل با .NET MAUI و ONNX Runtime GenAI

این راه‌حل نشان می‌دهد که چگونه می‌توان هوش مصنوعی مولد و مدل‌های زبان بزرگ (LLMs) را در برنامه‌های موبایل چندپلتفرمی با استفاده از .NET MAUI (رابط کاربری چندپلتفرمی برنامه) و ONNX Runtime GenAI ادغام کرد. این رویکرد به توسعه‌دهندگان .NET امکان می‌دهد برنامه‌های موبایل پیشرفته‌ای بسازند که به‌طور بومی روی دستگاه‌های Android و iOS اجرا شوند.

### ویژگی‌های کلیدی
- ساخته شده بر اساس چارچوب .NET MAUI، ارائه یک کدبیس واحد برای برنامه‌های Android و iOS
- ادغام ONNX Runtime GenAI امکان اجرای مدل‌های هوش مصنوعی مولد را مستقیماً روی دستگاه‌های موبایل فراهم می‌کند
- پشتیبانی از شتاب‌دهنده‌های سخت‌افزاری مختلف برای دستگاه‌های موبایل، از جمله CPU، GPU و پردازنده‌های هوش مصنوعی موبایل
- بهینه‌سازی‌های خاص پلتفرم مانند CoreML برای iOS و NNAPI برای Android از طریق ONNX Runtime
- اجرای کامل چرخه هوش مصنوعی مولد شامل پیش‌پردازش و پس‌پردازش، استنتاج، پردازش logits، جستجو و نمونه‌گیری، و مدیریت حافظه کش KV

### مزایای توسعه
رویکرد .NET MAUI به توسعه‌دهندگان امکان می‌دهد از مهارت‌های موجود در C# و .NET خود استفاده کنند و در عین حال برنامه‌های هوش مصنوعی چندپلتفرمی بسازند. چارچوب ONNX Runtime GenAI از معماری‌های مختلف مدل از جمله Llama، Mistral، Phi، Gemma و بسیاری دیگر پشتیبانی می‌کند. هسته‌های ARM64 بهینه‌سازی شده ضرب ماتریس INT4 را تسریع می‌کنند و عملکرد کارآمدی را روی سخت‌افزار موبایل ارائه می‌دهند، در حالی که تجربه توسعه آشنا .NET حفظ می‌شود.

### موارد استفاده
این راه‌حل برای توسعه‌دهندگانی که می‌خواهند برنامه‌های موبایل هوش مصنوعی با استفاده از فناوری‌های .NET بسازند، ایده‌آل است. از جمله چت‌بات‌های هوشمند، برنامه‌های تشخیص تصویر، ابزارهای ترجمه زبان و سیستم‌های توصیه شخصی‌سازی شده که به‌طور کامل روی دستگاه اجرا می‌شوند و قابلیت حفظ حریم خصوصی و عملکرد آفلاین را ارائه می‌دهند.

**اطلاعات بیشتر**: [.NET MAUI ONNX Runtime GenAI Example](https://github.com/microsoft/onnxruntime-genai/tree/jialli/genny-maui/examples/csharp/GennyMaui)

## ۳. EdgeAI در Azure با موتور مدل‌های زبان کوچک

راه‌حل EdgeAI مبتنی بر Azure مایکروسافت بر استقرار کارآمد مدل‌های زبان کوچک (SLMs) در محیط‌های ترکیبی ابر-لبه تمرکز دارد. این رویکرد شکاف بین خدمات هوش مصنوعی در مقیاس ابر و نیازهای استقرار لبه را پر می‌کند.

### مزایای معماری
- ادغام بی‌نقص با خدمات هوش مصنوعی Azure
- اجرای SLMs/LLMs و مدل‌های چندوجهی روی دستگاه و در ابر با ONNX Runtime
- بهینه‌سازی شده برای استقرار در مقیاس سازمانی
- پشتیبانی از به‌روزرسانی و مدیریت مداوم مدل‌ها

### موارد استفاده
پیاده‌سازی EdgeAI Azure در سناریوهایی که نیاز به استقرار هوش مصنوعی در سطح سازمانی با قابلیت‌های مدیریت ابر دارند، بسیار موفق است. این شامل پردازش هوشمند اسناد، تحلیل‌های بلادرنگ و جریان‌های کاری هوش مصنوعی ترکیبی است که از منابع محاسباتی ابر و لبه استفاده می‌کنند.

**اطلاعات بیشتر**: [Azure EdgeAI SLM Engine](https://github.com/microsoft/onnxruntime-genai/tree/main/examples/slm_engine)

## [۴. EdgeAI با Windows ML](./windowdeveloper.md)

Windows ML نمایانگر محیط اجرایی پیشرفته مایکروسافت برای استنتاج مدل‌های روی دستگاه و استقرار ساده است که به‌عنوان پایه‌ای برای Windows AI Foundry عمل می‌کند. این پلتفرم به توسعه‌دهندگان امکان می‌دهد برنامه‌های هوش مصنوعی ویندوزی بسازند که از تمام سخت‌افزارهای PC بهره‌مند شوند.

### قابلیت‌های پلتفرم
- کارکرد روی تمام PCهای ویندوز ۱۱ با نسخه ۲۴H2 (بیلد ۲۶۱۰۰) یا بالاتر
- کارکرد روی تمام سخت‌افزارهای x64 و ARM64 PC، حتی PCهایی که NPU یا GPU ندارند
- امکان استفاده از مدل‌های خود توسعه‌دهندگان و استقرار کارآمد آن‌ها در اکوسیستم شرکای سیلیکونی شامل AMD، Intel، NVIDIA و Qualcomm
- با استفاده از APIهای زیرساختی، توسعه‌دهندگان دیگر نیازی به ایجاد نسخه‌های متعدد از برنامه خود برای هدف‌گذاری سیلیکون‌های مختلف ندارند

### مزایای توسعه‌دهندگان
Windows ML سخت‌افزار و ارائه‌دهندگان اجرا را انتزاع می‌کند، بنابراین شما می‌توانید بر نوشتن کد خود تمرکز کنید. علاوه بر این، Windows ML به‌طور خودکار به‌روزرسانی می‌شود تا از جدیدترین NPUها، GPUها و CPUها پشتیبانی کند. این پلتفرم یک چارچوب یکپارچه برای توسعه هوش مصنوعی در اکوسیستم سخت‌افزاری متنوع ویندوز ارائه می‌دهد.

**اطلاعات بیشتر**: 
- [مروری بر Windows ML](https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview)
- [راهنمای توسعه EdgeAI ویندوز](./windowdeveloper.md) - راهنمای جامع برای توسعه هوش مصنوعی لبه‌ای ویندوز

## [۵. EdgeAI با برنامه‌های محلی Foundry](./foundrylocal.md)

Foundry Local به توسعه‌دهندگان ویندوز و مک امکان می‌دهد برنامه‌های تولید بازیابی‌شده (RAG) را با استفاده از منابع محلی در .NET بسازند و مدل‌های زبان محلی را با قابلیت‌های جستجوی معنایی ترکیب کنند. این رویکرد راه‌حل‌های هوش مصنوعی متمرکز بر حفظ حریم خصوصی را ارائه می‌دهد که به‌طور کامل بر روی زیرساخت‌های محلی عمل می‌کنند.

### معماری فنی
- ترکیب مدل زبان Phi، تعبیه‌های محلی و هسته معنایی برای ایجاد سناریوی RAG
- استفاده از تعبیه‌ها به‌عنوان بردارهایی (آرایه‌ها) از مقادیر نقطه شناور که محتوا و معنای معنایی آن را نشان می‌دهند
- هسته معنایی به‌عنوان هماهنگ‌کننده اصلی عمل می‌کند و Phi و اجزای هوشمند را برای ایجاد یک خط لوله RAG یکپارچه ادغام می‌کند
- پشتیبانی از پایگاه‌های داده برداری محلی از جمله SQLite و Qdrant

### مزایای پیاده‌سازی
RAG، یا تولید بازیابی‌شده، فقط یک روش پیچیده برای گفتن "جستجو کردن برخی اطلاعات و قرار دادن آن‌ها در درخواست" است. این پیاده‌سازی محلی حفظ حریم خصوصی داده‌ها را تضمین می‌کند و در عین حال پاسخ‌های هوشمند مبتنی بر پایگاه‌های دانش سفارشی ارائه می‌دهد. این رویکرد به‌ویژه برای سناریوهای سازمانی که نیاز به حاکمیت داده‌ها و قابلیت‌های عملیاتی آفلاین دارند، ارزشمند است.

**اطلاعات بیشتر**: 
- [Foundry Local](./foundrylocal.md)
- [نمونه‌های RAG Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/dotNET/rag)

### Foundry Local ویندوز

Foundry Local مایکروسافت یک سرور REST سازگار با OpenAI ارائه می‌دهد که توسط ONNX Runtime برای اجرای مدل‌ها به‌صورت محلی روی ویندوز قدرت گرفته است. در زیر خلاصه‌ای سریع و معتبر آمده است؛ برای جزئیات کامل به مستندات رسمی مراجعه کنید.

- شروع به کار: https://learn.microsoft.com/azure/ai-foundry/foundry-local/get-started
- معماری: https://learn.microsoft.com/azure/ai-foundry/foundry-local/concepts/foundry-local-architecture
- مرجع CLI: https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli
- راهنمای کامل ویندوز در این مخزن: [foundrylocal.md](./foundrylocal.md)

نصب یا ارتقا روی ویندوز (cmd.exe):
```cmd
winget install Microsoft.FoundryLocal
winget upgrade --id Microsoft.FoundryLocal
foundry --version
```

کاوش دسته‌های CLI:
```cmd
foundry model --help
foundry service --help
foundry cache --help
```

اجرای یک مدل و کشف نقطه پایانی پویا:
```cmd
foundry model run gpt-oss-20b
foundry service status
```

بررسی سریع REST برای لیست مدل‌ها (PORT را از وضعیت جایگزین کنید):
```cmd
curl -s http://localhost:PORT/v1/models
```

نکات:
- ادغام SDK: https://learn.microsoft.com/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- آوردن مدل خودتان (کامپایل): https://learn.microsoft.com/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## منابع توسعه EdgeAI ویندوز

برای توسعه‌دهندگانی که به‌طور خاص پلتفرم ویندوز را هدف قرار می‌دهند، ما یک راهنمای جامع ایجاد کرده‌ایم که اکوسیستم کامل EdgeAI ویندوز را پوشش می‌دهد. این منبع اطلاعات دقیقی درباره Windows AI Foundry، از جمله APIها، ابزارها و بهترین شیوه‌ها برای توسعه EdgeAI در ویندوز ارائه می‌دهد.

### پلتفرم Windows AI Foundry
پلتفرم Windows AI Foundry مجموعه‌ای جامع از ابزارها و APIها را به‌طور خاص برای توسعه هوش مصنوعی لبه‌ای روی دستگاه‌های ویندوز ارائه می‌دهد. این شامل پشتیبانی تخصصی از سخت‌افزار شتاب‌دهنده NPU، ادغام Windows ML و تکنیک‌های بهینه‌سازی خاص پلتفرم است.

**راهنمای جامع**: [راهنمای توسعه EdgeAI ویندوز](../windowdeveloper.md)

این راهنما شامل موارد زیر است:
- مروری بر پلتفرم Windows AI Foundry و اجزای آن
- API Phi Silica برای استنتاج کارآمد روی سخت‌افزار NPU
- APIهای بینایی کامپیوتری برای پردازش تصویر و OCR
- ادغام و بهینه‌سازی محیط اجرایی Windows ML
- CLI Foundry Local برای توسعه و آزمایش محلی
- استراتژی‌های بهینه‌سازی سخت‌افزار برای دستگاه‌های ویندوز
- نمونه‌های پیاده‌سازی عملی و بهترین شیوه‌ها

### [ابزار هوش مصنوعی برای توسعه EdgeAI](./aitoolkit.md)
برای توسعه‌دهندگانی که از Visual Studio Code استفاده می‌کنند، افزونه ابزار هوش مصنوعی یک محیط توسعه جامع ارائه می‌دهد که به‌طور خاص برای ساخت، آزمایش و استقرار برنامه‌های EdgeAI طراحی شده است. این ابزار کل جریان کاری توسعه EdgeAI را در VS Code ساده می‌کند.

**راهنمای توسعه**: [ابزار هوش مصنوعی برای توسعه EdgeAI](./aitoolkit.md)

راهنمای ابزار هوش مصنوعی شامل موارد زیر است:
- کشف و انتخاب مدل برای استقرار لبه
- جریان‌های کاری آزمایش و بهینه‌سازی محلی
- ادغام ONNX و Ollama برای مدل‌های لبه
- تکنیک‌های تبدیل و کمینه‌سازی مدل
- توسعه عامل برای سناریوهای لبه
- ارزیابی عملکرد و نظارت
- آماده‌سازی برای استقرار و بهترین شیوه‌ها

## نتیجه‌گیری

این پنج پیاده‌سازی EdgeAI بلوغ و تنوع راه‌حل‌های هوش مصنوعی لبه‌ای موجود امروز را نشان می‌دهند. از دستگاه‌های لبه‌ای شتاب‌دهنده سخت‌افزاری مانند Jetson Orin Nano گرفته تا چارچوب‌های نرم‌افزاری مانند ONNX Runtime GenAI و Windows ML، توسعه‌دهندگان گزینه‌های بی‌سابقه‌ای برای استقرار برنامه‌های هوشمند در لبه دارند.

نخ مشترک در میان همه این پلتفرم‌ها دموکراتیک کردن قابلیت‌های هوش مصنوعی است که دسترسی به یادگیری ماشین پیشرفته را برای توسعه‌دهندگان در سطوح مهارتی و موارد استفاده مختلف فراهم می‌کند. چه در حال ساخت برنامه‌های موبایل، نرم‌افزار دسکتاپ یا سیستم‌های تعبیه‌شده باشید، این راه‌حل‌های EdgeAI پایه‌ای برای نسل بعدی برنامه‌های هوشمند فراهم می‌کنند که به‌طور کارآمد و خصوصی در لبه عمل می‌کنند.

هر پلتفرم مزایای منحصربه‌فردی ارائه می‌دهد: Jetson Orin Nano برای محاسبات لبه‌ای شتاب‌دهنده سخت‌افزاری، ONNX Runtime GenAI برای توسعه موبایل چندپلتفرمی، Azure EdgeAI برای ادغام ابر-لبه سازمانی، Windows ML برای برنامه‌های بومی ویندوز و Foundry Local برای پیاده‌سازی‌های RAG متمرکز بر حفظ حریم خصوصی. این پلتفرم‌ها با هم یک اکوسیستم جامع برای توسعه EdgeAI را نمایان می‌کنند.

---

