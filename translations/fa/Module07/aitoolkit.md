<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-17T16:13:09+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "fa"
}
-->
# راهنمای توسعه هوش مصنوعی لبه با ابزار AI Toolkit در Visual Studio Code

## مقدمه

به راهنمای جامع استفاده از ابزار AI Toolkit در Visual Studio Code برای توسعه هوش مصنوعی لبه خوش آمدید. با حرکت هوش مصنوعی از محاسبات متمرکز ابری به دستگاه‌های لبه توزیع‌شده، توسعه‌دهندگان به ابزارهای قدرتمند و یکپارچه‌ای نیاز دارند که بتوانند چالش‌های خاص استقرار در لبه - از محدودیت‌های منابع گرفته تا نیازهای عملیاتی آفلاین - را مدیریت کنند.

ابزار AI Toolkit در Visual Studio Code این فاصله را پر می‌کند و محیطی کامل برای توسعه، آزمایش و بهینه‌سازی برنامه‌های هوش مصنوعی که به‌طور کارآمد روی دستگاه‌های لبه اجرا می‌شوند، فراهم می‌کند. چه در حال توسعه برای حسگرهای IoT، دستگاه‌های موبایل، سیستم‌های تعبیه‌شده یا سرورهای لبه باشید، این ابزار کل جریان کاری توسعه شما را در محیط آشنای VS Code ساده می‌کند.

این راهنما شما را با مفاهیم اساسی، ابزارها و بهترین روش‌ها برای استفاده از AI Toolkit در پروژه‌های هوش مصنوعی لبه، از انتخاب اولیه مدل تا استقرار در تولید، آشنا می‌کند.

## مرور کلی

ابزار AI Toolkit یک محیط توسعه یکپارچه برای چرخه کامل برنامه‌های هوش مصنوعی لبه در VS Code ارائه می‌دهد. این ابزار با مدل‌های محبوب هوش مصنوعی از ارائه‌دهندگانی مانند OpenAI، Anthropic، Google و GitHub یکپارچه شده و از استقرار مدل‌های محلی از طریق ONNX و Ollama پشتیبانی می‌کند - قابلیت‌های حیاتی برای برنامه‌های هوش مصنوعی لبه که نیاز به استنتاج روی دستگاه دارند.

آنچه ابزار AI Toolkit را برای توسعه هوش مصنوعی لبه متمایز می‌کند، تمرکز آن بر کل خط لوله استقرار لبه است. برخلاف ابزارهای سنتی توسعه هوش مصنوعی که عمدتاً بر استقرار ابری تمرکز دارند، AI Toolkit ویژگی‌های تخصصی برای بهینه‌سازی مدل، آزمایش در شرایط محدود منابع و ارزیابی عملکرد خاص لبه ارائه می‌دهد. این ابزار درک می‌کند که توسعه هوش مصنوعی لبه نیازمند ملاحظات متفاوتی است - اندازه‌های کوچک‌تر مدل، زمان‌های استنتاج سریع‌تر، قابلیت آفلاین و بهینه‌سازی‌های خاص سخت‌افزاری.

این پلتفرم از سناریوهای مختلف استقرار، از استنتاج ساده روی دستگاه تا معماری‌های پیچیده چندمدلی لبه، پشتیبانی می‌کند. ابزارهایی برای تبدیل مدل، کمینه‌سازی و بهینه‌سازی ارائه می‌دهد که برای استقرار موفقیت‌آمیز در لبه ضروری هستند، در حالی که بهره‌وری توسعه‌دهنده که VS Code به آن معروف است را حفظ می‌کند.

## اهداف یادگیری

در پایان این راهنما، شما قادر خواهید بود:

### مهارت‌های اصلی
- **نصب و پیکربندی** ابزار AI Toolkit برای جریان‌های کاری توسعه هوش مصنوعی لبه
- **پیمایش و استفاده** از رابط کاربری AI Toolkit، شامل Model Catalog، Playground و Agent Builder
- **انتخاب و ارزیابی** مدل‌های هوش مصنوعی مناسب برای استقرار لبه بر اساس عملکرد و محدودیت‌های منابع
- **تبدیل و بهینه‌سازی** مدل‌ها با استفاده از فرمت ONNX و تکنیک‌های کمینه‌سازی برای دستگاه‌های لبه

### مهارت‌های توسعه هوش مصنوعی لبه
- **طراحی و پیاده‌سازی** برنامه‌های هوش مصنوعی لبه با استفاده از محیط توسعه یکپارچه
- **آزمایش مدل‌ها** در شرایط مشابه لبه با استفاده از استنتاج محلی و نظارت بر منابع
- **ایجاد و سفارشی‌سازی** عوامل هوش مصنوعی بهینه‌شده برای سناریوهای استقرار لبه
- **ارزیابی عملکرد مدل** با استفاده از معیارهای مرتبط با محاسبات لبه (زمان تأخیر، مصرف حافظه، دقت)

### بهینه‌سازی و استقرار
- **اعمال تکنیک‌های کمینه‌سازی و هرس** برای کاهش اندازه مدل در حالی که عملکرد قابل قبول حفظ می‌شود
- **بهینه‌سازی مدل‌ها** برای پلتفرم‌های سخت‌افزاری خاص لبه شامل شتاب‌دهنده‌های CPU، GPU و NPU
- **اجرای بهترین روش‌ها** برای توسعه هوش مصنوعی لبه شامل مدیریت منابع و استراتژی‌های جایگزین
- **آماده‌سازی مدل‌ها و برنامه‌ها** برای استقرار در تولید روی دستگاه‌های لبه

### مفاهیم پیشرفته هوش مصنوعی لبه
- **یکپارچه‌سازی با چارچوب‌های هوش مصنوعی لبه** شامل ONNX Runtime، Windows ML و TensorFlow Lite
- **پیاده‌سازی معماری‌های چندمدلی** و سناریوهای یادگیری فدرال برای محیط‌های لبه
- **رفع مشکلات رایج هوش مصنوعی لبه** شامل محدودیت‌های حافظه، سرعت استنتاج و سازگاری سخت‌افزاری
- **طراحی استراتژی‌های نظارت و ثبت** برای برنامه‌های هوش مصنوعی لبه در تولید

### کاربرد عملی
- **ایجاد راه‌حل‌های کامل هوش مصنوعی لبه** از انتخاب مدل تا استقرار
- **نمایش مهارت** در جریان‌های کاری توسعه خاص لبه و تکنیک‌های بهینه‌سازی
- **اعمال مفاهیم آموخته‌شده** به موارد استفاده واقعی هوش مصنوعی لبه شامل IoT، موبایل و برنامه‌های تعبیه‌شده
- **ارزیابی و مقایسه** استراتژی‌های مختلف استقرار هوش مصنوعی لبه و مزایا و معایب آن‌ها

## ویژگی‌های کلیدی برای توسعه هوش مصنوعی لبه

### 1. کاتالوگ مدل و کشف
- **پشتیبانی از مدل‌های محلی**: کشف و دسترسی به مدل‌های هوش مصنوعی بهینه‌شده برای استقرار لبه
- **یکپارچه‌سازی ONNX**: دسترسی به مدل‌ها در فرمت ONNX برای استنتاج کارآمد لبه
- **پشتیبانی از Ollama**: استفاده از مدل‌های محلی از طریق Ollama برای حفظ حریم خصوصی و عملیات آفلاین
- **مقایسه مدل‌ها**: مقایسه مدل‌ها به‌صورت کنار هم برای یافتن تعادل بهینه بین عملکرد و مصرف منابع برای دستگاه‌های لبه

### 2. محیط آزمایش تعاملی
- **محیط آزمایش محلی**: آزمایش مدل‌ها به‌صورت محلی قبل از استقرار لبه
- **آزمایش چندمدلی**: آزمایش با تصاویر، متن و ورودی‌های دیگر معمول در سناریوهای لبه
- **تنظیم پارامترها**: آزمایش با پارامترهای مختلف مدل برای بهینه‌سازی محدودیت‌های لبه
- **نظارت بر عملکرد در زمان واقعی**: مشاهده سرعت استنتاج و مصرف منابع در طول توسعه

### 3. سازنده عامل برای برنامه‌های لبه
- **مهندسی درخواست‌ها**: ایجاد درخواست‌های بهینه که به‌طور کارآمد با مدل‌های کوچک‌تر لبه کار کنند
- **یکپارچه‌سازی ابزار MCP**: یکپارچه‌سازی ابزارهای Model Context Protocol برای قابلیت‌های پیشرفته عوامل لبه
- **تولید کد**: تولید کد آماده تولید بهینه‌شده برای سناریوهای استقرار لبه
- **خروجی‌های ساختاریافته**: طراحی عوامل که پاسخ‌های منظم و ساختاریافته مناسب برای برنامه‌های لبه ارائه دهند

### 4. ارزیابی و آزمایش مدل
- **معیارهای عملکرد**: ارزیابی مدل‌ها با استفاده از معیارهای مرتبط با استقرار لبه (زمان تأخیر، مصرف حافظه، دقت)
- **آزمایش دسته‌ای**: آزمایش چندین پیکربندی مدل به‌صورت همزمان برای یافتن تنظیمات بهینه لبه
- **ارزیابی سفارشی**: ایجاد معیارهای ارزیابی سفارشی خاص موارد استفاده هوش مصنوعی لبه
- **پروفایل منابع**: تحلیل نیازهای حافظه و محاسبات برای برنامه‌ریزی استقرار لبه

### 5. تبدیل و بهینه‌سازی مدل
- **تبدیل به ONNX**: تبدیل مدل‌ها از فرمت‌های مختلف به ONNX برای سازگاری لبه
- **کمینه‌سازی**: کاهش اندازه مدل و بهبود سرعت استنتاج از طریق تکنیک‌های کمینه‌سازی
- **بهینه‌سازی سخت‌افزاری**: بهینه‌سازی مدل‌ها برای سخت‌افزار خاص لبه (CPU، GPU، NPU)
- **تبدیل فرمت**: تبدیل مدل‌ها از منابعی مانند Hugging Face برای استقرار لبه

### 6. تنظیم دقیق برای سناریوهای لبه
- **انطباق دامنه**: سفارشی‌سازی مدل‌ها برای موارد استفاده و محیط‌های خاص لبه
- **آموزش محلی**: آموزش مدل‌ها به‌صورت محلی با پشتیبانی GPU برای نیازهای خاص لبه
- **یکپارچه‌سازی Azure**: استفاده از Azure Container Apps برای تنظیم دقیق مبتنی بر ابر قبل از استقرار لبه
- **یادگیری انتقالی**: انطباق مدل‌های پیش‌آموزش‌دیده برای وظایف و محدودیت‌های خاص لبه

### 7. نظارت بر عملکرد و ردیابی
- **تحلیل عملکرد لبه**: نظارت بر عملکرد مدل در شرایط مشابه لبه
- **جمع‌آوری ردیابی**: جمع‌آوری داده‌های عملکرد دقیق برای بهینه‌سازی
- **شناسایی گلوگاه‌ها**: شناسایی مشکلات عملکرد قبل از استقرار روی دستگاه‌های لبه
- **ردیابی مصرف منابع**: نظارت بر حافظه، CPU و زمان استنتاج برای بهینه‌سازی لبه

## جریان کاری توسعه هوش مصنوعی لبه

### مرحله 1: کشف و انتخاب مدل
1. **کاتالوگ مدل را بررسی کنید**: از کاتالوگ مدل برای یافتن مدل‌های مناسب استقرار لبه استفاده کنید
2. **مقایسه عملکرد**: مدل‌ها را بر اساس اندازه، دقت و سرعت استنتاج ارزیابی کنید
3. **آزمایش محلی**: از مدل‌های Ollama یا ONNX برای آزمایش محلی قبل از استقرار لبه استفاده کنید
4. **ارزیابی نیازهای منابع**: نیازهای حافظه و محاسبات برای دستگاه‌های لبه هدف را تعیین کنید

### مرحله 2: بهینه‌سازی مدل
1. **تبدیل به ONNX**: مدل‌های انتخاب‌شده را به فرمت ONNX برای سازگاری لبه تبدیل کنید
2. **اعمال کمینه‌سازی**: اندازه مدل را از طریق کمینه‌سازی INT8 یا INT4 کاهش دهید
3. **بهینه‌سازی سخت‌افزاری**: برای سخت‌افزار لبه هدف (ARM، x86، شتاب‌دهنده‌های تخصصی) بهینه‌سازی کنید
4. **اعتبارسنجی عملکرد**: اطمینان حاصل کنید که مدل‌های بهینه‌شده دقت قابل قبول را حفظ می‌کنند

### مرحله 3: توسعه برنامه
1. **طراحی عامل**: از سازنده عامل برای ایجاد عوامل هوش مصنوعی بهینه‌شده برای لبه استفاده کنید
2. **مهندسی درخواست‌ها**: درخواست‌هایی ایجاد کنید که به‌طور مؤثر با مدل‌های کوچک‌تر لبه کار کنند
3. **آزمایش یکپارچه‌سازی**: عوامل را در شرایط شبیه‌سازی‌شده لبه آزمایش کنید
4. **تولید کد**: کد تولیدی بهینه‌شده برای استقرار لبه ایجاد کنید

### مرحله 4: ارزیابی و آزمایش
1. **ارزیابی دسته‌ای**: چندین پیکربندی را آزمایش کنید تا تنظیمات بهینه لبه را پیدا کنید
2. **پروفایل عملکرد**: سرعت استنتاج، مصرف حافظه و دقت را تحلیل کنید
3. **شبیه‌سازی لبه**: در شرایط مشابه محیط استقرار لبه آزمایش کنید
4. **آزمایش فشار**: عملکرد را تحت شرایط بار مختلف ارزیابی کنید

### مرحله 5: آماده‌سازی استقرار
1. **بهینه‌سازی نهایی**: بر اساس نتایج آزمایش، بهینه‌سازی‌های نهایی را اعمال کنید
2. **بسته‌بندی استقرار**: مدل‌ها و کد را برای استقرار لبه بسته‌بندی کنید
3. **مستندسازی**: نیازها و پیکربندی استقرار را مستند کنید
4. **راه‌اندازی نظارت**: نظارت و ثبت برای استقرار تولید آماده کنید

## مخاطبان هدف برای توسعه هوش مصنوعی لبه

### توسعه‌دهندگان هوش مصنوعی لبه
- توسعه‌دهندگان برنامه که دستگاه‌های لبه و راه‌حل‌های IoT مجهز به هوش مصنوعی ایجاد می‌کنند
- توسعه‌دهندگان سیستم‌های تعبیه‌شده که قابلیت‌های هوش مصنوعی را در دستگاه‌های محدود منابع ادغام می‌کنند
- توسعه‌دهندگان موبایل که برنامه‌های هوش مصنوعی روی دستگاه برای گوشی‌های هوشمند و تبلت‌ها ایجاد می‌کنند

### مهندسان هوش مصنوعی لبه
- مهندسان هوش مصنوعی که مدل‌ها را برای استقرار لبه بهینه‌سازی می‌کنند و خطوط لوله استنتاج را مدیریت می‌کنند
- مهندسان DevOps که مدل‌های هوش مصنوعی را در زیرساخت‌های لبه توزیع‌شده مستقر و مدیریت می‌کنند
- مهندسان عملکرد که بارهای کاری هوش مصنوعی را برای محدودیت‌های سخت‌افزاری لبه بهینه‌سازی می‌کنند

### پژوهشگران و مربیان
- پژوهشگران هوش مصنوعی که مدل‌ها و الگوریتم‌های کارآمد برای محاسبات لبه توسعه می‌دهند
- مربیانی که مفاهیم هوش مصنوعی لبه را آموزش می‌دهند و تکنیک‌های بهینه‌سازی را نشان می‌دهند
- دانشجویانی که درباره چالش‌ها و راه‌حل‌های استقرار هوش مصنوعی لبه یاد می‌گیرند

## موارد استفاده هوش مصنوعی لبه

### دستگاه‌های هوشمند IoT
- **تشخیص تصویر در زمان واقعی**: استقرار مدل‌های بینایی کامپیوتری روی دوربین‌ها و حسگرهای IoT
- **پردازش صوتی**: پیاده‌سازی تشخیص گفتار و پردازش زبان طبیعی روی بلندگوهای هوشمند
- **نگهداری پیش‌بینی‌کننده**: اجرای مدل‌های تشخیص ناهنجاری روی دستگاه‌های صنعتی لبه
- **نظارت بر محیط زیست**: استقرار مدل‌های تحلیل داده‌های حسگر برای کاربردهای محیطی

### برنامه‌های موبایل و تعبیه‌شده
- **ترجمه روی دستگاه**: پیاده‌سازی مدل‌های ترجمه زبان که به‌صورت آفلاین کار می‌کنند
- **واقعیت افزوده**: استقرار تشخیص و ردیابی اشیاء در زمان واقعی برای برنامه‌های AR
- **نظارت بر سلامت**: اجرای مدل‌های تحلیل سلامت روی دستگاه‌های پوشیدنی و تجهیزات پزشکی
- **سیستم‌های خودمختار**: پیاده‌سازی مدل‌های تصمیم‌گیری برای پهپادها، ربات‌ها و وسایل نقلیه

### زیرساخت محاسبات لبه
- **مراکز داده لبه**: استقرار مدل‌های هوش مصنوعی در مراکز داده لبه برای برنامه‌های کم‌تأخیر
- **یکپارچه‌سازی CDN**: ادغام قابلیت‌های پردازش هوش مصنوعی در شبکه‌های تحویل محتوا
- **لبه 5G**: استفاده از محاسبات لبه 5G برای برنامه‌های مجهز به هوش مصنوعی
- **محاسبات مه**: پیاده‌سازی پردازش هوش مصنوعی در محیط‌های محاسبات مه

## نصب و راه‌اندازی

### نصب سریع
افزونه AI Toolkit را مستقیماً از بازار Visual Studio Code نصب کنید:

```
Install: AI Toolkit for Visual Studio Code (ms-windows-ai-studio.windows-ai-studio)
```

### پیش‌نیازهای توسعه هوش مصنوعی لبه
- **ONNX Runtime**: نصب ONNX Runtime برای استنتاج مدل
- **Ollama** (اختیاری): نصب Ollama برای سرویس‌دهی مدل‌های محلی
- **محیط Python**: تنظیم Python با کتابخانه‌های مورد نیاز هوش مصنوعی
- **ابزارهای سخت‌افزاری لبه**: نصب ابزارهای توسعه خاص سخت‌افزار (CUDA، OpenVINO و غیره)

### پیکربندی اولیه
1. VS Code را باز کنید و افزونه AI Toolkit را نصب کنید
2. منابع مدل (ONNX، Ollama، ارائه‌دهندگان ابری) را پیکربندی کنید
3. محیط توسعه محلی برای آزمایش لبه را تنظیم کنید
4. گزینه‌های شتاب‌دهنده سخت‌افزاری را برای ماشین توسعه خود پیکربندی کنید

## شروع به کار با توسعه هوش مصنوعی لبه

### مرحله 1: انتخاب مدل
1. نمای AI Toolkit را در Activity Bar باز کنید
2. کاتالوگ مدل را برای مدل‌های سازگار با لبه مرور کنید
3. بر اساس اندازه مدل، فرمت (ONNX) و ویژگی‌های عملکرد فیلتر کنید
4. مدل‌ها را با استفاده از ابزارهای مقایسه داخلی مقایسه کنید

### مرحله 2: آزمایش محلی
1. از Playground برای آزمایش مدل‌های انتخاب‌شده به‌صورت محلی استفاده کنید
2. با درخواست‌ها و پارامترهای مختلف آزمایش کنید
3. معیارهای عملکرد را در طول آزمایش نظارت کنید
4. پاسخ‌های مدل را برای نیازهای موارد استفاده لبه ارزیابی کنید

### مرحله 3: بهینه‌سازی مدل
1. از ابزارهای تبدیل مدل برای بهینه‌سازی استقرار لبه استفاده کنید
2. کمینه‌سازی را برای کاهش اندازه مدل اعمال کنید
3. مدل‌های بهینه‌شده را آزمایش کنید تا از عملکرد قابل قبول اطمینان حاصل کنید
4. تنظیمات بهینه‌سازی و مبادلات عملکرد را مستند کنید

### مرحله 4: توسعه عامل
1. از سازنده عامل برای ایجاد عوامل هوش مصنوعی بهینه‌شده برای لبه استفاده کنید
2. درخواست‌هایی توسعه دهید که به‌طور مؤثر با مدل‌های کوچک‌تر کار کنند
3. ابزارها و API‌های لازم برای سناریوهای لبه را ادغام کنید
4. عوامل را در شرایط شبیه‌سازی‌شده لبه آزمایش کنید

###
- **امنیت**: اجرای تدابیر امنیتی مناسب برای برنامه‌های هوش مصنوعی در لبه

## یکپارچه‌سازی با چارچوب‌های هوش مصنوعی در لبه

### ONNX Runtime
- **استقرار چندسکویی**: استقرار مدل‌های ONNX در پلتفرم‌های مختلف لبه
- **بهینه‌سازی سخت‌افزار**: استفاده از بهینه‌سازی‌های خاص سخت‌افزار ONNX Runtime
- **پشتیبانی موبایل**: استفاده از ONNX Runtime Mobile برای برنامه‌های گوشی‌های هوشمند و تبلت‌ها
- **یکپارچه‌سازی با IoT**: استقرار در دستگاه‌های IoT با استفاده از توزیع‌های سبک ONNX Runtime

### Windows ML
- **دستگاه‌های ویندوز**: بهینه‌سازی برای دستگاه‌های لبه مبتنی بر ویندوز و رایانه‌های شخصی
- **شتاب‌دهی NPU**: استفاده از واحدهای پردازش عصبی در دستگاه‌های ویندوز
- **DirectML**: استفاده از DirectML برای شتاب‌دهی GPU در پلتفرم‌های ویندوز
- **یکپارچه‌سازی UWP**: یکپارچه‌سازی با برنامه‌های Universal Windows Platform

### TensorFlow Lite
- **بهینه‌سازی موبایل**: استقرار مدل‌های TensorFlow Lite در دستگاه‌های موبایل و تعبیه‌شده
- **نمایندگان سخت‌افزاری**: استفاده از نمایندگان سخت‌افزاری تخصصی برای شتاب‌دهی
- **کنترل‌کننده‌های کوچک**: استقرار در میکروکنترلرها با استفاده از TensorFlow Lite Micro
- **پشتیبانی چندسکویی**: استقرار در سیستم‌های Android، iOS و لینوکس تعبیه‌شده

### Azure IoT Edge
- **ترکیب ابر و لبه**: ترکیب آموزش در ابر با استنتاج در لبه
- **استقرار ماژول‌ها**: استقرار مدل‌های هوش مصنوعی به‌عنوان ماژول‌های IoT Edge
- **مدیریت دستگاه‌ها**: مدیریت دستگاه‌های لبه و به‌روزرسانی مدل‌ها از راه دور
- **تله‌متری**: جمع‌آوری داده‌های عملکرد و معیارهای مدل از استقرارهای لبه

## سناریوهای پیشرفته هوش مصنوعی در لبه

### استقرار چندمدلی
- **مجموعه مدل‌ها**: استقرار چندین مدل برای بهبود دقت یا افزونگی
- **آزمایش A/B**: آزمایش هم‌زمان مدل‌های مختلف در دستگاه‌های لبه
- **انتخاب پویا**: انتخاب مدل‌ها بر اساس شرایط فعلی دستگاه
- **اشتراک منابع**: بهینه‌سازی استفاده از منابع در میان مدل‌های مستقر شده

### یادگیری فدرال
- **آموزش توزیع‌شده**: آموزش مدل‌ها در میان چندین دستگاه لبه
- **حفظ حریم خصوصی**: نگه‌داشتن داده‌های آموزشی به‌صورت محلی درحالی‌که بهبودهای مدل به اشتراک گذاشته می‌شود
- **یادگیری مشارکتی**: امکان یادگیری دستگاه‌ها از تجربیات جمعی
- **هماهنگی لبه و ابر**: هماهنگی یادگیری بین دستگاه‌های لبه و زیرساخت ابر

### پردازش بلادرنگ
- **پردازش جریان**: پردازش جریان‌های داده پیوسته در دستگاه‌های لبه
- **استنتاج با تأخیر کم**: بهینه‌سازی برای حداقل تأخیر در استنتاج
- **پردازش دسته‌ای**: پردازش کارآمد دسته‌های داده در دستگاه‌های لبه
- **پردازش تطبیقی**: تنظیم پردازش بر اساس قابلیت‌های فعلی دستگاه

## رفع مشکلات توسعه هوش مصنوعی در لبه

### مشکلات رایج
- **محدودیت‌های حافظه**: مدل بیش از حد بزرگ برای حافظه دستگاه هدف
- **سرعت استنتاج**: استنتاج مدل بیش از حد کند برای نیازهای بلادرنگ
- **کاهش دقت**: بهینه‌سازی باعث کاهش غیرقابل‌قبول دقت مدل می‌شود
- **سازگاری سخت‌افزار**: مدل با سخت‌افزار هدف سازگار نیست

### استراتژی‌های اشکال‌زدایی
- **پروفایل عملکرد**: استفاده از ویژگی‌های ردیابی ابزار هوش مصنوعی برای شناسایی گلوگاه‌ها
- **نظارت بر منابع**: نظارت بر استفاده از حافظه و CPU در طول توسعه
- **آزمایش تدریجی**: آزمایش بهینه‌سازی‌ها به‌صورت تدریجی برای جداسازی مشکلات
- **شبیه‌سازی سخت‌افزار**: استفاده از ابزارهای توسعه برای شبیه‌سازی سخت‌افزار هدف

### راه‌حل‌های بهینه‌سازی
- **کوانتیزاسیون بیشتر**: اعمال تکنیک‌های کوانتیزاسیون تهاجمی‌تر
- **معماری مدل**: بررسی معماری‌های مدل مختلف بهینه‌شده برای لبه
- **بهینه‌سازی پیش‌پردازش**: بهینه‌سازی پیش‌پردازش داده برای محدودیت‌های لبه
- **بهینه‌سازی استنتاج**: استفاده از بهینه‌سازی‌های استنتاج خاص سخت‌افزار

## منابع و مراحل بعدی

### مستندات
- [راهنمای مدل‌های ابزار هوش مصنوعی](https://code.visualstudio.com/docs/intelligentapps/models)
- [مستندات Playground مدل](https://code.visualstudio.com/docs/intelligentapps/playground)
- [مستندات ONNX Runtime](https://onnxruntime.ai/)
- [مستندات Windows ML](https://docs.microsoft.com/en-us/windows/ai/)

### جامعه و پشتیبانی
- [گیت‌هاب ابزار هوش مصنوعی VS Code](https://github.com/microsoft/vscode-ai-toolkit)
- [جامعه ONNX](https://github.com/onnx/onnx)
- [جامعه توسعه‌دهندگان هوش مصنوعی در لبه](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [بازار افزونه‌های VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### منابع آموزشی
- [دوره اصول هوش مصنوعی در لبه](./Module01/README.md)
- [راهنمای مدل‌های زبان کوچک](./Module02/README.md)
- [استراتژی‌های استقرار در لبه](./Module03/README.md)
- [توسعه هوش مصنوعی در لبه ویندوز](./windowdeveloper.md)

## نتیجه‌گیری

ابزار هوش مصنوعی برای Visual Studio Code یک پلتفرم جامع برای توسعه هوش مصنوعی در لبه ارائه می‌دهد، از کشف و بهینه‌سازی مدل تا استقرار و نظارت. با استفاده از ابزارها و جریان‌های کاری یکپارچه آن، توسعه‌دهندگان می‌توانند به‌طور مؤثر برنامه‌های هوش مصنوعی ایجاد، آزمایش و مستقر کنند که به‌خوبی در دستگاه‌های لبه با منابع محدود اجرا شوند.

پشتیبانی این ابزار از ONNX، Ollama و ارائه‌دهندگان مختلف ابر، همراه با قابلیت‌های بهینه‌سازی و ارزیابی آن، آن را به انتخابی ایده‌آل برای توسعه هوش مصنوعی در لبه تبدیل می‌کند. چه در حال ساخت برنامه‌های IoT، ویژگی‌های هوش مصنوعی موبایل، یا سیستم‌های هوش تعبیه‌شده باشید، ابزار هوش مصنوعی ابزارها و جریان‌های کاری موردنیاز برای استقرار موفقیت‌آمیز هوش مصنوعی در لبه را فراهم می‌کند.

با ادامه تکامل هوش مصنوعی در لبه، ابزار هوش مصنوعی برای VS Code در خط مقدم باقی می‌ماند و ابزارها و قابلیت‌های پیشرفته‌ای را برای توسعه‌دهندگان فراهم می‌کند تا نسل بعدی برنامه‌های هوشمند در لبه را بسازند.

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.