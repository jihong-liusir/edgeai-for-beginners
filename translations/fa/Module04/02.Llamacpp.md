<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T15:43:48+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "fa"
}
-->
# بخش ۲: راهنمای پیاده‌سازی Llama.cpp

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [Llama.cpp چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [ساخت از سورس](../../../Module04)
5. [کوانتیزاسیون مدل](../../../Module04)
6. [استفاده پایه](../../../Module04)
7. [ویژگی‌های پیشرفته](../../../Module04)
8. [یکپارچه‌سازی با پایتون](../../../Module04)
9. [رفع مشکلات](../../../Module04)
10. [بهترین روش‌ها](../../../Module04)

## مقدمه

این آموزش جامع شما را از نصب اولیه تا سناریوهای پیشرفته استفاده از Llama.cpp راهنمایی می‌کند. Llama.cpp یک پیاده‌سازی قدرتمند C++ است که امکان استنتاج مدل‌های زبانی بزرگ (LLM) را با تنظیمات حداقلی و عملکرد عالی در پیکربندی‌های مختلف سخت‌افزاری فراهم می‌کند.

## Llama.cpp چیست؟

Llama.cpp یک چارچوب استنتاج LLM است که به زبان C/C++ نوشته شده و امکان اجرای مدل‌های زبانی بزرگ به صورت محلی با تنظیمات حداقلی و عملکرد پیشرفته در طیف گسترده‌ای از سخت‌افزارها را فراهم می‌کند. ویژگی‌های کلیدی شامل موارد زیر است:

### ویژگی‌های اصلی
- **پیاده‌سازی ساده C/C++** بدون وابستگی‌ها
- **سازگاری چندپلتفرمی** (ویندوز، macOS، لینوکس)
- **بهینه‌سازی سخت‌افزاری** برای معماری‌های مختلف
- **پشتیبانی از کوانتیزاسیون** (از ۱.۵ بیت تا ۸ بیت)
- **شتاب‌دهی CPU و GPU**
- **کارایی حافظه** برای محیط‌های محدود

### مزایا
- اجرای کارآمد روی CPU بدون نیاز به سخت‌افزار تخصصی
- پشتیبانی از چندین بک‌اند GPU (CUDA، Metal، OpenCL، Vulkan)
- سبک و قابل حمل
- بهینه‌سازی برای Apple Silicon از طریق ARM NEON، Accelerate و Metal
- پشتیبانی از سطوح مختلف کوانتیزاسیون برای کاهش مصرف حافظه

## نصب

### روش ۱: فایل‌های باینری آماده (توصیه‌شده برای مبتدیان)

#### دانلود از GitHub Releases
1. به [صفحه GitHub Releases Llama.cpp](https://github.com/ggml-org/llama.cpp/releases) مراجعه کنید.
2. فایل باینری مناسب سیستم خود را دانلود کنید:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` برای ویندوز
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` برای macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` برای لینوکس

3. آرشیو را استخراج کرده و مسیر آن را به PATH سیستم خود اضافه کنید.

#### استفاده از مدیران بسته

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**لینوکس (توزیع‌های مختلف):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### روش ۲: بسته پایتون (llama-cpp-python)

#### نصب پایه
```bash
pip install llama-cpp-python
```

#### با شتاب‌دهی سخت‌افزاری
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## ساخت از سورس

### پیش‌نیازها

**نیازمندی‌های سیستم:**
- کامپایلر C++ (GCC، Clang یا MSVC)
- CMake (نسخه ۳.۱۴ یا بالاتر)
- Git
- ابزارهای ساخت برای پلتفرم شما

**نصب پیش‌نیازها:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**ویندوز:**
- نصب Visual Studio 2022 با ابزارهای توسعه C++
- نصب CMake از وب‌سایت رسمی
- نصب Git

### فرآیند ساخت پایه

1. **کلون کردن مخزن:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **پیکربندی ساخت:**
```bash
cmake -B build
```

3. **ساخت پروژه:**
```bash
cmake --build build --config Release
```

برای کامپایل سریع‌تر، از کارهای موازی استفاده کنید:
```bash
cmake --build build --config Release -j 8
```

### ساخت‌های خاص سخت‌افزار

#### پشتیبانی CUDA (GPUهای NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### پشتیبانی Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### پشتیبانی OpenBLAS (بهینه‌سازی CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### پشتیبانی Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### گزینه‌های پیشرفته ساخت

#### ساخت دیباگ
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### با ویژگی‌های اضافی
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## کوانتیزاسیون مدل

### آشنایی با فرمت GGUF

GGUF (فرمت یکپارچه GGML عمومی) یک فرمت فایل بهینه‌شده است که برای اجرای مدل‌های زبانی بزرگ به صورت کارآمد با استفاده از Llama.cpp و چارچوب‌های دیگر طراحی شده است. این فرمت ارائه می‌دهد:

- ذخیره استاندارد وزن‌های مدل
- سازگاری بهبود‌یافته در پلتفرم‌ها
- عملکرد بهتر
- مدیریت کارآمد متاداده‌ها

### انواع کوانتیزاسیون

Llama.cpp از سطوح مختلف کوانتیزاسیون پشتیبانی می‌کند:

| نوع | بیت | توضیحات | مورد استفاده |
|-----|-----|---------|--------------|
| F16 | 16 | دقت نیمه | کیفیت بالا، حافظه زیاد |
| Q8_0 | 8 | کوانتیزاسیون ۸ بیت | تعادل خوب |
| Q4_0 | 4 | کوانتیزاسیون ۴ بیت | کیفیت متوسط، اندازه کوچک‌تر |
| Q2_K | 2 | کوانتیزاسیون ۲ بیت | کوچک‌ترین اندازه، کیفیت پایین‌تر |

### تبدیل مدل‌ها

#### از PyTorch به GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### دانلود مستقیم از Hugging Face
بسیاری از مدل‌ها در فرمت GGUF در Hugging Face موجود هستند:
- مدل‌هایی را با نام "GGUF" جستجو کنید.
- سطح کوانتیزاسیون مناسب را دانلود کنید.
- مستقیماً با Llama.cpp استفاده کنید.

## استفاده پایه

### رابط خط فرمان

#### تولید متن ساده
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### استفاده از مدل‌ها از Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### حالت سرور
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### پارامترهای رایج

| پارامتر | توضیحات | مثال |
|---------|---------|------|
| `-m` | مسیر فایل مدل | `-m model.gguf` |
| `-p` | متن ورودی | `-p "Hello world"` |
| `-n` | تعداد توکن‌های تولیدی | `-n 100` |
| `-c` | اندازه کانتکست | `-c 4096` |
| `-t` | تعداد رشته‌ها | `-t 8` |
| `-ngl` | لایه‌های GPU | `-ngl 32` |
| `-temp` | دما | `-temp 0.7` |

### حالت تعاملی

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## ویژگی‌های پیشرفته

### API سرور

#### راه‌اندازی سرور
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### استفاده از API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### بهینه‌سازی عملکرد

#### مدیریت حافظه
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### چندرشته‌ای
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### شتاب‌دهی GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## یکپارچه‌سازی با پایتون

### استفاده پایه با llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### رابط چت

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### پاسخ‌های استریم

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### یکپارچه‌سازی با LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## رفع مشکلات

### مشکلات رایج و راه‌حل‌ها

#### خطاهای ساخت

**مشکل: CMake پیدا نشد**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**مشکل: کامپایلر پیدا نشد**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### مشکلات زمان اجرا

**مشکل: بارگذاری مدل شکست خورد**
- مسیر فایل مدل را بررسی کنید.
- مجوزهای فایل را چک کنید.
- از RAM کافی اطمینان حاصل کنید.
- سطوح مختلف کوانتیزاسیون را امتحان کنید.

**مشکل: عملکرد ضعیف**
- شتاب‌دهی سخت‌افزاری را فعال کنید.
- تعداد رشته‌ها را افزایش دهید.
- کوانتیزاسیون مناسب را استفاده کنید.
- مصرف حافظه GPU را بررسی کنید.

#### مشکلات حافظه

**مشکل: کمبود حافظه**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### مشکلات خاص پلتفرم

#### ویندوز
- از کامپایلر MinGW یا Visual Studio استفاده کنید.
- تنظیمات PATH را به درستی انجام دهید.
- تداخل آنتی‌ویروس را بررسی کنید.

#### macOS
- Metal را برای Apple Silicon فعال کنید.
- در صورت نیاز از Rosetta 2 برای سازگاری استفاده کنید.
- ابزارهای خط فرمان Xcode را بررسی کنید.

#### لینوکس
- بسته‌های توسعه را نصب کنید.
- نسخه‌های درایور GPU را بررسی کنید.
- نصب ابزارهای CUDA را تأیید کنید.

## بهترین روش‌ها

### انتخاب مدل
1. **سطح کوانتیزاسیون مناسب** را بر اساس سخت‌افزار خود انتخاب کنید.
2. **اندازه مدل** را در مقابل کیفیت بررسی کنید.
3. **مدل‌های مختلف** را برای مورد استفاده خاص خود آزمایش کنید.

### بهینه‌سازی عملکرد
1. **از شتاب‌دهی GPU** در صورت امکان استفاده کنید.
2. **تعداد رشته‌ها** را برای CPU خود بهینه کنید.
3. **اندازه کانتکست مناسب** را برای مورد استفاده خود تنظیم کنید.
4. **نقشه‌برداری حافظه** را برای مدل‌های بزرگ فعال کنید.

### استقرار در تولید
1. **از حالت سرور** برای دسترسی API استفاده کنید.
2. **مدیریت خطا** را به درستی پیاده‌سازی کنید.
3. **مصرف منابع را نظارت کنید.**
4. **ثبت و نظارت را تنظیم کنید.**

### جریان کاری توسعه
1. **با مدل‌های کوچک‌تر** برای آزمایش شروع کنید.
2. **از کنترل نسخه** برای تنظیمات مدل استفاده کنید.
3. **تنظیمات خود را مستند کنید.**
4. **در پلتفرم‌های مختلف آزمایش کنید.**

### ملاحظات امنیتی
1. **ورودی‌های متن را اعتبارسنجی کنید.**
2. **محدودیت نرخ را پیاده‌سازی کنید.**
3. **نقاط پایانی API را ایمن کنید.**
4. **الگوهای سوءاستفاده را نظارت کنید.**

## نتیجه‌گیری

Llama.cpp راهی قدرتمند و کارآمد برای اجرای مدل‌های زبانی بزرگ به صورت محلی در پیکربندی‌های مختلف سخت‌افزاری ارائه می‌دهد. چه در حال توسعه برنامه‌های هوش مصنوعی باشید، چه تحقیق کنید یا صرفاً با LLM‌ها آزمایش کنید، این چارچوب انعطاف‌پذیری و عملکرد مورد نیاز برای طیف گسترده‌ای از موارد استفاده را فراهم می‌کند.

نکات کلیدی:
- روش نصب مناسب نیازهای خود را انتخاب کنید.
- برای پیکربندی سخت‌افزار خاص خود بهینه‌سازی کنید.
- با استفاده پایه شروع کرده و به تدریج ویژگی‌های پیشرفته را کشف کنید.
- از اتصال‌های پایتون برای یکپارچه‌سازی آسان‌تر استفاده کنید.
- بهترین روش‌ها را برای استقرار در تولید دنبال کنید.

برای اطلاعات بیشتر و به‌روزرسانی‌ها، به [مخزن رسمی Llama.cpp](https://github.com/ggml-org/llama.cpp) مراجعه کنید و از مستندات جامع و منابع جامعه موجود استفاده کنید.

## ➡️ گام بعدی

- [03: مجموعه بهینه‌سازی Microsoft Olive](./03.MicrosoftOlive.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.