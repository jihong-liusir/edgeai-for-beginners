<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T13:08:38+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fa"
}
-->
# بخش ۳: مجموعه بهینه‌سازی Microsoft Olive

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [Microsoft Olive چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [راهنمای شروع سریع](../../../Module04)
5. [مثال: تبدیل Qwen3 به ONNX INT4](../../../Module04)
6. [استفاده پیشرفته](../../../Module04)
7. [بهترین روش‌ها](../../../Module04)
8. [رفع مشکلات](../../../Module04)
9. [منابع اضافی](../../../Module04)

## مقدمه

Microsoft Olive یک ابزار قدرتمند و آسان برای بهینه‌سازی مدل‌های یادگیری ماشین است که فرآیند بهینه‌سازی مدل‌ها برای اجرا روی پلتفرم‌های سخت‌افزاری مختلف را ساده می‌کند. چه هدف شما CPU، GPU یا شتاب‌دهنده‌های تخصصی هوش مصنوعی باشد، Olive به شما کمک می‌کند تا عملکرد بهینه را بدون کاهش دقت مدل به دست آورید.

## Microsoft Olive چیست؟

Olive یک ابزار بهینه‌سازی مدل سخت‌افزار محور است که تکنیک‌های پیشرو در صنعت را در زمینه فشرده‌سازی مدل، بهینه‌سازی و کامپایل ترکیب می‌کند. این ابزار با ONNX Runtime به عنوان یک راه‌حل بهینه‌سازی انتها به انتها برای استنتاج کار می‌کند.

### ویژگی‌های کلیدی

- **بهینه‌سازی سخت‌افزار محور**: به طور خودکار بهترین تکنیک‌های بهینه‌سازی را برای سخت‌افزار هدف شما انتخاب می‌کند
- **بیش از ۴۰ مؤلفه بهینه‌سازی داخلی**: شامل فشرده‌سازی مدل، کوانتیزاسیون، بهینه‌سازی گراف و موارد دیگر
- **رابط خط فرمان آسان**: دستورات ساده برای وظایف رایج بهینه‌سازی
- **پشتیبانی از چندین چارچوب**: سازگار با PyTorch، مدل‌های Hugging Face و ONNX
- **پشتیبانی از مدل‌های محبوب**: Olive می‌تواند مدل‌های معماری محبوب مانند Llama، Phi، Qwen، Gemma و غیره را به صورت خودکار بهینه کند

### مزایا

- **کاهش زمان توسعه**: نیازی به آزمایش دستی تکنیک‌های مختلف بهینه‌سازی نیست
- **افزایش عملکرد**: بهبودهای قابل توجه سرعت (تا ۶ برابر در برخی موارد)
- **اجرای چند پلتفرمی**: مدل‌های بهینه‌سازی شده روی سخت‌افزارها و سیستم‌عامل‌های مختلف کار می‌کنند
- **حفظ دقت**: بهینه‌سازی‌ها کیفیت مدل را حفظ کرده و عملکرد را بهبود می‌بخشند

## نصب

### پیش‌نیازها

- Python نسخه ۳.۸ یا بالاتر
- مدیر بسته pip
- محیط مجازی (توصیه می‌شود)

### نصب پایه

ایجاد و فعال‌سازی محیط مجازی:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

نصب Olive با ویژگی‌های بهینه‌سازی خودکار:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### وابستگی‌های اختیاری

Olive وابستگی‌های اختیاری مختلفی برای ویژگی‌های اضافی ارائه می‌دهد:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### تأیید نصب

```bash
olive --help
```

اگر موفقیت‌آمیز باشد، باید پیام راهنمای CLI Olive را مشاهده کنید.

## راهنمای شروع سریع

### اولین بهینه‌سازی شما

بیایید یک مدل زبان کوچک را با استفاده از ویژگی بهینه‌سازی خودکار Olive بهینه کنیم:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### این دستور چه کاری انجام می‌دهد؟

فرآیند بهینه‌سازی شامل: دریافت مدل از کش محلی، گرفتن گراف ONNX و ذخیره وزن‌ها در یک فایل داده ONNX، بهینه‌سازی گراف ONNX و کوانتیزه کردن مدل به int4 با استفاده از روش RTN است.

### توضیح پارامترهای دستور

- `--model_name_or_path`: شناسه مدل Hugging Face یا مسیر محلی
- `--output_path`: دایرکتوری که مدل بهینه‌سازی شده در آن ذخیره می‌شود
- `--device`: دستگاه هدف (cpu، gpu)
- `--provider`: ارائه‌دهنده اجرا (CPUExecutionProvider، CUDAExecutionProvider، DmlExecutionProvider)
- `--use_ort_genai`: استفاده از ONNX Runtime Generate AI برای استنتاج
- `--precision`: دقت کوانتیزاسیون (int4، int8، fp16)
- `--log_level`: سطح جزئیات لاگ (۰=حداقل، ۱=مفصل)

## مثال: تبدیل Qwen3 به ONNX INT4

بر اساس مثال ارائه شده توسط Hugging Face در [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)، در اینجا نحوه بهینه‌سازی مدل Qwen3 آمده است:

### مرحله ۱: دانلود مدل (اختیاری)

برای کاهش زمان دانلود، فقط فایل‌های ضروری را کش کنید:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### مرحله ۲: بهینه‌سازی مدل Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### مرحله ۳: تست مدل بهینه‌سازی شده

یک اسکریپت ساده پایتون برای تست مدل بهینه‌سازی شده ایجاد کنید:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ساختار خروجی

پس از بهینه‌سازی، دایرکتوری خروجی شما شامل موارد زیر خواهد بود:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## استفاده پیشرفته

### فایل‌های پیکربندی

برای جریان‌های کاری پیچیده‌تر بهینه‌سازی، می‌توانید از فایل‌های پیکربندی JSON استفاده کنید:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

اجرا با پیکربندی:

```bash
olive run --config config.json
```

### بهینه‌سازی GPU

برای بهینه‌سازی CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

برای DirectML (ویندوز):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### تنظیم دقیق با Olive

Olive همچنین از تنظیم دقیق مدل‌ها پشتیبانی می‌کند:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## بهترین روش‌ها

### ۱. انتخاب مدل
- برای آزمایش با مدل‌های کوچک‌تر شروع کنید (مثلاً ۰.۵B-7B پارامتر)
- اطمینان حاصل کنید که معماری مدل هدف شما توسط Olive پشتیبانی می‌شود

### ۲. ملاحظات سخت‌افزاری
- هدف بهینه‌سازی خود را با سخت‌افزار اجرایی خود مطابقت دهید
- اگر سخت‌افزار سازگار با CUDA دارید، از بهینه‌سازی GPU استفاده کنید
- DirectML را برای ماشین‌های ویندوز با گرافیک یکپارچه در نظر بگیرید

### ۳. انتخاب دقت
- **INT4**: حداکثر فشرده‌سازی، کمی کاهش دقت
- **INT8**: تعادل خوب بین اندازه و دقت
- **FP16**: کاهش حداقل دقت، کاهش متوسط اندازه

### ۴. تست و اعتبارسنجی
- همیشه مدل‌های بهینه‌سازی شده را با موارد استفاده خاص خود تست کنید
- معیارهای عملکرد (زمان تأخیر، توان عملیاتی، دقت) را مقایسه کنید
- از داده‌های ورودی نماینده برای ارزیابی استفاده کنید

### ۵. بهینه‌سازی تکراری
- برای نتایج سریع با بهینه‌سازی خودکار شروع کنید
- از فایل‌های پیکربندی برای کنترل دقیق استفاده کنید
- با گذرهای مختلف بهینه‌سازی آزمایش کنید

## رفع مشکلات

### مشکلات رایج

#### ۱. مشکلات نصب
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ۲. مشکلات CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ۳. مشکلات حافظه
- از اندازه‌های دسته کوچک‌تر در طول بهینه‌سازی استفاده کنید
- ابتدا کوانتیزاسیون با دقت بالاتر را امتحان کنید (int8 به جای int4)
- فضای دیسک کافی برای کش مدل فراهم کنید

#### ۴. خطاهای بارگذاری مدل
- مسیر مدل و مجوزهای دسترسی را بررسی کنید
- بررسی کنید که آیا مدل نیاز به `trust_remote_code=True` دارد
- اطمینان حاصل کنید که همه فایل‌های مورد نیاز مدل دانلود شده‌اند

### دریافت کمک

- **مستندات**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **مشکلات GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **مثال‌ها**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## منابع اضافی

### لینک‌های رسمی
- **مخزن GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **مستندات ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **مثال Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### مثال‌های جامعه
- **دفترچه‌های Jupyter**: موجود در مخزن GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **افزونه VS Code**: نمای کلی ابزار AI برای VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **پست‌های وبلاگ**: وبلاگ متن‌باز مایکروسافت — https://opensource.microsoft.com/blog/

### ابزارهای مرتبط
- **ONNX Runtime**: موتور استنتاج با عملکرد بالا — https://onnxruntime.ai/
- **Hugging Face Transformers**: منبع بسیاری از مدل‌های سازگار — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: جریان‌های کاری بهینه‌سازی مبتنی بر ابر — https://learn.microsoft.com/azure/machine-learning/

## ➡️ مرحله بعدی

- [۰۴: مجموعه بهینه‌سازی OpenVINO Toolkit](./04.openvino.md)

---

