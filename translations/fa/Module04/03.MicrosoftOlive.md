<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T15:40:13+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fa"
}
-->
# بخش ۳: مجموعه بهینه‌سازی Microsoft Olive

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [Microsoft Olive چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [راهنمای شروع سریع](../../../Module04)
5. [مثال: تبدیل Qwen3 به ONNX INT4](../../../Module04)
6. [استفاده پیشرفته](../../../Module04)
7. [بهترین روش‌ها](../../../Module04)
8. [رفع مشکلات](../../../Module04)
9. [منابع اضافی](../../../Module04)

## مقدمه

Microsoft Olive یک ابزار قدرتمند و آسان برای بهینه‌سازی مدل‌های یادگیری ماشین است که فرآیند بهینه‌سازی مدل‌ها برای اجرا روی پلتفرم‌های سخت‌افزاری مختلف را ساده می‌کند. چه هدف شما CPU، GPU یا شتاب‌دهنده‌های تخصصی هوش مصنوعی باشد، Olive به شما کمک می‌کند تا عملکرد بهینه را بدون کاهش دقت مدل به دست آورید.

## Microsoft Olive چیست؟

Olive یک ابزار بهینه‌سازی مدل سخت‌افزار محور است که تکنیک‌های پیشرو در صنعت را در زمینه فشرده‌سازی مدل، بهینه‌سازی و کامپایل ترکیب می‌کند. این ابزار با ONNX Runtime به عنوان یک راه‌حل بهینه‌سازی انتها به انتها برای استنتاج کار می‌کند.

### ویژگی‌های کلیدی

- **بهینه‌سازی سخت‌افزار محور**: به طور خودکار بهترین تکنیک‌های بهینه‌سازی را برای سخت‌افزار هدف شما انتخاب می‌کند
- **بیش از ۴۰ مؤلفه بهینه‌سازی داخلی**: شامل فشرده‌سازی مدل، کوانتیزاسیون، بهینه‌سازی گراف و موارد دیگر
- **رابط خط فرمان آسان**: دستورات ساده برای وظایف رایج بهینه‌سازی
- **پشتیبانی از چندین چارچوب**: سازگار با PyTorch، مدل‌های Hugging Face و ONNX
- **پشتیبانی از مدل‌های محبوب**: Olive می‌تواند مدل‌های محبوب مانند Llama، Phi، Qwen، Gemma و غیره را به طور خودکار بهینه کند

### مزایا

- **کاهش زمان توسعه**: نیازی به آزمایش دستی تکنیک‌های مختلف بهینه‌سازی نیست
- **افزایش عملکرد**: بهبودهای قابل توجه در سرعت (تا ۶ برابر در برخی موارد)
- **استقرار چند پلتفرمی**: مدل‌های بهینه‌سازی شده روی سخت‌افزارها و سیستم‌عامل‌های مختلف کار می‌کنند
- **حفظ دقت**: بهینه‌سازی‌ها کیفیت مدل را حفظ کرده و عملکرد را بهبود می‌بخشند

## نصب

### پیش‌نیازها

- Python نسخه ۳.۸ یا بالاتر
- مدیر بسته pip
- محیط مجازی (توصیه می‌شود)

### نصب پایه

ایجاد و فعال‌سازی محیط مجازی:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

نصب Olive با ویژگی‌های بهینه‌سازی خودکار:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### وابستگی‌های اختیاری

Olive وابستگی‌های اختیاری مختلفی برای ویژگی‌های اضافی ارائه می‌دهد:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### تأیید نصب

```bash
olive --help
```

اگر موفقیت‌آمیز باشد، باید پیام راهنمای CLI Olive را مشاهده کنید.

## راهنمای شروع سریع

### اولین بهینه‌سازی شما

بیایید یک مدل زبان کوچک را با استفاده از ویژگی بهینه‌سازی خودکار Olive بهینه کنیم:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### این دستور چه کاری انجام می‌دهد

فرآیند بهینه‌سازی شامل: دریافت مدل از حافظه محلی، گرفتن گراف ONNX و ذخیره وزن‌ها در یک فایل داده ONNX، بهینه‌سازی گراف ONNX و کوانتیزه کردن مدل به int4 با استفاده از روش RTN است.

### توضیح پارامترهای دستور

- `--model_name_or_path`: شناسه مدل Hugging Face یا مسیر محلی
- `--output_path`: دایرکتوری که مدل بهینه‌سازی شده در آن ذخیره می‌شود
- `--device`: دستگاه هدف (cpu، gpu)
- `--provider`: ارائه‌دهنده اجرا (CPUExecutionProvider، CUDAExecutionProvider، DmlExecutionProvider)
- `--use_ort_genai`: استفاده از ONNX Runtime Generate AI برای استنتاج
- `--precision`: دقت کوانتیزاسیون (int4، int8، fp16)
- `--log_level`: سطح جزئیات گزارش (۰=حداقل، ۱=مفصل)

## مثال: تبدیل Qwen3 به ONNX INT4

بر اساس مثال ارائه شده Hugging Face در [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)، در اینجا نحوه بهینه‌سازی مدل Qwen3 آمده است:

### مرحله ۱: دانلود مدل (اختیاری)

برای کاهش زمان دانلود، فقط فایل‌های ضروری را کش کنید:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### مرحله ۲: بهینه‌سازی مدل Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### مرحله ۳: آزمایش مدل بهینه‌سازی شده

یک اسکریپت ساده پایتون برای آزمایش مدل بهینه‌سازی شده ایجاد کنید:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ساختار خروجی

پس از بهینه‌سازی، دایرکتوری خروجی شما شامل موارد زیر خواهد بود:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## استفاده پیشرفته

### فایل‌های پیکربندی

برای جریان‌های کاری پیچیده‌تر بهینه‌سازی، می‌توانید از فایل‌های پیکربندی JSON استفاده کنید:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

اجرا با پیکربندی:

```bash
olive run --config config.json
```

### بهینه‌سازی GPU

برای بهینه‌سازی GPU با CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

برای DirectML (ویندوز):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### تنظیم دقیق با Olive

Olive همچنین از تنظیم دقیق مدل‌ها پشتیبانی می‌کند:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## بهترین روش‌ها

### ۱. انتخاب مدل
- برای آزمایش با مدل‌های کوچک‌تر شروع کنید (مثلاً ۰.۵B-7B پارامتر)
- مطمئن شوید معماری مدل هدف شما توسط Olive پشتیبانی می‌شود

### ۲. ملاحظات سخت‌افزاری
- هدف بهینه‌سازی خود را با سخت‌افزار استقرار خود مطابقت دهید
- اگر سخت‌افزار سازگار با CUDA دارید، از بهینه‌سازی GPU استفاده کنید
- برای ماشین‌های ویندوز با گرافیک یکپارچه، DirectML را در نظر بگیرید

### ۳. انتخاب دقت
- **INT4**: حداکثر فشرده‌سازی، کمی کاهش دقت
- **INT8**: تعادل خوب بین اندازه و دقت
- **FP16**: کاهش حداقلی دقت، کاهش متوسط اندازه

### ۴. آزمایش و اعتبارسنجی
- همیشه مدل‌های بهینه‌سازی شده را با موارد استفاده خاص خود آزمایش کنید
- معیارهای عملکرد (زمان تأخیر، توان عملیاتی، دقت) را مقایسه کنید
- از داده‌های ورودی نماینده برای ارزیابی استفاده کنید

### ۵. بهینه‌سازی تکراری
- برای نتایج سریع با بهینه‌سازی خودکار شروع کنید
- از فایل‌های پیکربندی برای کنترل دقیق استفاده کنید
- با گذرهای مختلف بهینه‌سازی آزمایش کنید

## رفع مشکلات

### مشکلات رایج

#### ۱. مشکلات نصب
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ۲. مشکلات CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ۳. مشکلات حافظه
- از اندازه‌های دسته کوچک‌تر در طول بهینه‌سازی استفاده کنید
- ابتدا کوانتیزاسیون با دقت بالاتر را امتحان کنید (int8 به جای int4)
- فضای دیسک کافی برای کش مدل فراهم کنید

#### ۴. خطاهای بارگذاری مدل
- مسیر مدل و مجوزهای دسترسی را بررسی کنید
- بررسی کنید که آیا مدل نیاز به `trust_remote_code=True` دارد
- مطمئن شوید که همه فایل‌های مورد نیاز مدل دانلود شده‌اند

### دریافت کمک

- **مستندات**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **مشکلات GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **مثال‌ها**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## منابع اضافی

### لینک‌های رسمی
- **مخزن GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **مستندات ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **مثال Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### مثال‌های جامعه
- **دفترچه‌های Jupyter**: در مخزن GitHub Olive موجود است
- **افزونه VS Code**: افزونه AI Toolkit از Olive برای بهینه‌سازی مدل استفاده می‌کند
- **پست‌های وبلاگ**: وبلاگ متن‌باز مایکروسافت آموزش‌های مفصل Olive دارد

### ابزارهای مرتبط
- **ONNX Runtime**: موتور استنتاج با عملکرد بالا
- **Hugging Face Transformers**: منبع بسیاری از مدل‌های سازگار
- **Azure Machine Learning**: جریان‌های کاری بهینه‌سازی مبتنی بر ابر

## ➡️ مرحله بعدی

- [04: مجموعه بهینه‌سازی OpenVINO Toolkit](./04.openvino.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.