<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T15:34:42+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "fa"
}
-->
# بخش ۴: مجموعه ابزار بهینه‌سازی OpenVINO

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [OpenVINO چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [راهنمای شروع سریع](../../../Module04)
5. [مثال: تبدیل و بهینه‌سازی مدل‌ها با OpenVINO](../../../Module04)
6. [استفاده پیشرفته](../../../Module04)
7. [بهترین روش‌ها](../../../Module04)
8. [رفع مشکلات](../../../Module04)
9. [منابع اضافی](../../../Module04)

## مقدمه

OpenVINO (بهینه‌سازی شبکه‌های عصبی و استنتاج بصری باز) مجموعه ابزار متن‌باز اینتل برای پیاده‌سازی راه‌حل‌های هوش مصنوعی با عملکرد بالا در محیط‌های ابری، محلی و لبه است. چه هدف شما پردازنده‌ها، پردازنده‌های گرافیکی، واحدهای پردازش بصری (VPU) یا شتاب‌دهنده‌های تخصصی هوش مصنوعی باشد، OpenVINO قابلیت‌های جامع بهینه‌سازی را ارائه می‌دهد و در عین حفظ دقت مدل، امکان پیاده‌سازی چندپلتفرمی را فراهم می‌کند.

## OpenVINO چیست؟

OpenVINO یک مجموعه ابزار متن‌باز است که به توسعه‌دهندگان امکان می‌دهد مدل‌های هوش مصنوعی را بهینه‌سازی، تبدیل و به‌طور مؤثر در پلتفرم‌های سخت‌افزاری متنوع پیاده‌سازی کنند. این مجموعه شامل سه بخش اصلی است: OpenVINO Runtime برای استنتاج، Neural Network Compression Framework (NNCF) برای بهینه‌سازی مدل، و OpenVINO Model Server برای پیاده‌سازی مقیاس‌پذیر.

### ویژگی‌های کلیدی

- **پیاده‌سازی چندپلتفرمی**: پشتیبانی از لینوکس، ویندوز و macOS با API‌های پایتون، C++ و C
- **شتاب‌دهی سخت‌افزاری**: کشف خودکار دستگاه و بهینه‌سازی برای CPU، GPU، VPU و شتاب‌دهنده‌های هوش مصنوعی
- **چارچوب فشرده‌سازی مدل**: تکنیک‌های پیشرفته کمینه‌سازی، هرس و بهینه‌سازی از طریق NNCF
- **سازگاری با چارچوب‌ها**: پشتیبانی مستقیم از مدل‌های TensorFlow، ONNX، PaddlePaddle و PyTorch
- **پشتیبانی از هوش مصنوعی مولد**: OpenVINO GenAI برای پیاده‌سازی مدل‌های زبان بزرگ و کاربردهای هوش مصنوعی مولد

### مزایا

- **بهینه‌سازی عملکرد**: بهبودهای قابل‌توجه سرعت با حداقل کاهش دقت
- **کاهش حجم پیاده‌سازی**: وابستگی‌های خارجی کم برای ساده‌سازی نصب و پیاده‌سازی
- **زمان راه‌اندازی بهینه**: بارگذاری و کش مدل بهینه برای شروع سریع‌تر برنامه‌ها
- **پیاده‌سازی مقیاس‌پذیر**: از دستگاه‌های لبه تا زیرساخت‌های ابری با API‌های سازگار
- **آماده برای تولید**: قابلیت اطمینان در سطح سازمانی با مستندات جامع و پشتیبانی جامعه

## نصب

### پیش‌نیازها

- پایتون نسخه ۳.۸ یا بالاتر
- مدیر بسته pip
- محیط مجازی (توصیه‌شده)
- سخت‌افزار سازگار (پردازنده‌های اینتل توصیه می‌شوند، اما از معماری‌های مختلف پشتیبانی می‌شود)

### نصب پایه

ایجاد و فعال‌سازی محیط مجازی:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

نصب OpenVINO Runtime:

```bash
pip install openvino
```

نصب NNCF برای بهینه‌سازی مدل:

```bash
pip install nncf
```

### نصب OpenVINO GenAI

برای کاربردهای هوش مصنوعی مولد:

```bash
pip install openvino-genai
```

### وابستگی‌های اختیاری

بسته‌های اضافی برای موارد استفاده خاص:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### تأیید نصب

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

اگر موفقیت‌آمیز باشد، باید اطلاعات نسخه OpenVINO را مشاهده کنید.

## راهنمای شروع سریع

### اولین بهینه‌سازی مدل شما

بیایید یک مدل Hugging Face را با استفاده از OpenVINO تبدیل و بهینه‌سازی کنیم:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### این فرآیند چه کاری انجام می‌دهد؟

جریان کاری بهینه‌سازی شامل: بارگذاری مدل اصلی از Hugging Face، تبدیل به فرمت Intermediate Representation (IR) OpenVINO، اعمال بهینه‌سازی‌های پیش‌فرض و کامپایل برای سخت‌افزار هدف است.

### توضیح پارامترهای کلیدی

- `export=True`: تبدیل مدل به فرمت IR OpenVINO
- `compile=False`: تأخیر در کامپایل تا زمان اجرا برای انعطاف‌پذیری
- `device`: سخت‌افزار هدف ("CPU"، "GPU"، "AUTO" برای انتخاب خودکار)
- `save_pretrained()`: ذخیره مدل بهینه‌سازی‌شده برای استفاده مجدد

## مثال: تبدیل و بهینه‌سازی مدل‌ها با OpenVINO

### مرحله ۱: تبدیل مدل با کمینه‌سازی NNCF

در اینجا نحوه اعمال کمینه‌سازی پس از آموزش با استفاده از NNCF آمده است:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### مرحله ۲: بهینه‌سازی پیشرفته با فشرده‌سازی وزن

برای مدل‌های مبتنی بر ترانسفورمر، فشرده‌سازی وزن را اعمال کنید:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### مرحله ۳: استنتاج با مدل بهینه‌سازی‌شده

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### ساختار خروجی

پس از بهینه‌سازی، دایرکتوری مدل شما شامل موارد زیر خواهد بود:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## استفاده پیشرفته

### پیکربندی با NNCF YAML

برای جریان‌های کاری پیچیده بهینه‌سازی، از فایل‌های پیکربندی NNCF استفاده کنید:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

اعمال پیکربندی:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### بهینه‌سازی GPU

برای شتاب‌دهی GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### بهینه‌سازی پردازش دسته‌ای

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### پیاده‌سازی سرور مدل

مدل‌های بهینه‌سازی‌شده را با OpenVINO Model Server پیاده‌سازی کنید:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

کد کلاینت برای سرور مدل:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## بهترین روش‌ها

### ۱. انتخاب و آماده‌سازی مدل
- از مدل‌های چارچوب‌های پشتیبانی‌شده (PyTorch، TensorFlow، ONNX) استفاده کنید
- اطمینان حاصل کنید که ورودی‌های مدل دارای شکل‌های ثابت یا پویا شناخته‌شده هستند
- با مجموعه داده‌های نماینده برای کالیبراسیون آزمایش کنید

### ۲. انتخاب استراتژی بهینه‌سازی
- **کمینه‌سازی پس از آموزش**: از اینجا برای بهینه‌سازی سریع شروع کنید
- **فشرده‌سازی وزن**: مناسب برای مدل‌های زبان بزرگ و ترانسفورمرها
- **آموزش آگاه از کمینه‌سازی**: زمانی که دقت حیاتی است استفاده کنید

### ۳. بهینه‌سازی سخت‌افزار خاص
- **CPU**: از کمینه‌سازی INT8 برای عملکرد متعادل استفاده کنید
- **GPU**: از دقت FP16 و پردازش دسته‌ای بهره ببرید
- **VPU**: بر ساده‌سازی مدل و ترکیب لایه‌ها تمرکز کنید

### ۴. تنظیم عملکرد
- **حالت توان عملیاتی**: برای پردازش دسته‌ای با حجم بالا
- **حالت تأخیر**: برای برنامه‌های تعاملی بلادرنگ
- **دستگاه AUTO**: اجازه دهید OpenVINO سخت‌افزار بهینه را انتخاب کند

### ۵. مدیریت حافظه
- از شکل‌های پویا به‌طور معقول استفاده کنید تا از سربار حافظه جلوگیری شود
- کش مدل را برای بارگذاری سریع‌تر پیاده‌سازی کنید
- استفاده از حافظه را در طول بهینه‌سازی نظارت کنید

### ۶. اعتبارسنجی دقت
- همیشه مدل‌های بهینه‌سازی‌شده را در برابر عملکرد اصلی اعتبارسنجی کنید
- از مجموعه داده‌های آزمایشی نماینده برای ارزیابی استفاده کنید
- بهینه‌سازی تدریجی را در نظر بگیرید (با تنظیمات محافظه‌کارانه شروع کنید)

## رفع مشکلات

### مشکلات رایج

#### ۱. مشکلات نصب
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### ۲. خطاهای تبدیل مدل
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### ۳. مشکلات عملکرد
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### ۴. مشکلات حافظه
- کاهش اندازه دسته مدل در طول بهینه‌سازی
- استفاده از جریان‌سازی برای مجموعه داده‌های بزرگ
- فعال‌سازی کش مدل: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### ۵. کاهش دقت
- استفاده از دقت بالاتر (INT8 به جای INT4)
- افزایش اندازه مجموعه داده کالیبراسیون
- اعمال بهینه‌سازی دقت مختلط

### نظارت بر عملکرد

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### دریافت کمک

- **مستندات**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **مشکلات GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **انجمن جامعه**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## منابع اضافی

### لینک‌های رسمی
- **صفحه اصلی OpenVINO**: [openvino.ai](https://openvino.ai/)
- **مخزن GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **مخزن NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **مدل‌های آماده**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### منابع آموزشی
- **دفترچه‌های OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **راهنمای شروع سریع**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **راهنمای بهینه‌سازی**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### ابزارهای یکپارچه‌سازی
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### معیارهای عملکرد
- **معیارهای رسمی**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **مدل‌های آماده NNCF**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### مثال‌های جامعه
- **دفترچه‌های Jupyter**: [مخزن دفترچه‌های OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - آموزش‌های جامع موجود در مخزن دفترچه‌های OpenVINO
- **برنامه‌های نمونه**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - مثال‌های واقعی برای حوزه‌های مختلف (بینایی کامپیوتری، NLP، صوت)
- **پست‌های وبلاگ**: [وبلاگ هوش مصنوعی اینتل](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - پست‌های وبلاگ اینتل و جامعه با موارد استفاده دقیق

### ابزارهای مرتبط
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - تکنیک‌های بهینه‌سازی اضافی برای سخت‌افزار اینتل
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - برای مقایسه پیاده‌سازی در موبایل و لبه
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - جایگزین‌های موتور استنتاج چندپلتفرمی

## ➡️ مرحله بعدی

- [05: بررسی عمیق چارچوب Apple MLX](./05.AppleMLX.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.