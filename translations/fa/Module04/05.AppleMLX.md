<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-17T15:41:49+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "fa"
}
-->
# بخش ۴: بررسی عمیق چارچوب Apple MLX

## فهرست مطالب
1. [مقدمه‌ای بر Apple MLX](../../../Module04)
2. [ویژگی‌های کلیدی برای توسعه LLM](../../../Module04)
3. [راهنمای نصب](../../../Module04)
4. [شروع کار با MLX](../../../Module04)
5. [MLX-LM: مدل‌های زبانی](../../../Module04)
6. [کار با مدل‌های زبانی بزرگ](../../../Module04)
7. [یکپارچگی با Hugging Face](../../../Module04)
8. [تبدیل و کمینه‌سازی مدل‌ها](../../../Module04)
9. [تنظیم دقیق مدل‌های زبانی](../../../Module04)
10. [ویژگی‌های پیشرفته LLM](../../../Module04)
11. [بهترین روش‌ها برای LLMها](../../../Module04)
12. [رفع مشکلات](../../../Module04)
13. [منابع اضافی](../../../Module04)

## مقدمه‌ای بر Apple MLX

Apple MLX یک چارچوب آرایه‌ای است که به طور خاص برای یادگیری ماشین کارآمد و انعطاف‌پذیر بر روی Apple Silicon طراحی شده و توسط تیم تحقیقاتی یادگیری ماشین اپل توسعه یافته است. این چارچوب که در دسامبر ۲۰۲۳ منتشر شد، پاسخ اپل به چارچوب‌هایی مانند PyTorch و TensorFlow است و تمرکز ویژه‌ای بر قابلیت‌های قدرتمند مدل‌های زبانی بزرگ بر روی کامپیوترهای مک دارد.

### چه چیزی MLX را برای LLMها خاص می‌کند؟

MLX به طور کامل از معماری حافظه یکپارچه Apple Silicon بهره می‌برد و آن را به گزینه‌ای مناسب برای اجرای و تنظیم دقیق مدل‌های زبانی بزرگ به صورت محلی بر روی کامپیوترهای مک تبدیل می‌کند. این چارچوب بسیاری از مشکلات سازگاری که کاربران مک معمولاً با LLMها داشتند را از بین می‌برد.

### چه کسانی باید از MLX برای LLMها استفاده کنند؟

- **کاربران مک** که می‌خواهند LLMها را به صورت محلی و بدون وابستگی به ابر اجرا کنند
- **محققان** که در حال آزمایش تنظیم دقیق و سفارشی‌سازی مدل‌های زبانی هستند
- **توسعه‌دهندگان** که برنامه‌های هوش مصنوعی با قابلیت‌های مدل زبانی می‌سازند
- **هر کسی** که می‌خواهد از Apple Silicon برای تولید متن، چت و وظایف زبانی استفاده کند

## ویژگی‌های کلیدی برای توسعه LLM

### ۱. معماری حافظه یکپارچه
حافظه یکپارچه Apple Silicon به MLX اجازه می‌دهد مدل‌های زبانی بزرگ را بدون سربار کپی حافظه که در سایر چارچوب‌ها معمول است، به طور کارآمد مدیریت کند. این بدان معناست که می‌توانید با مدل‌های بزرگ‌تر بر روی همان سخت‌افزار کار کنید.

### ۲. بهینه‌سازی بومی Apple Silicon
MLX از ابتدا برای تراشه‌های سری M اپل ساخته شده است و عملکرد بهینه‌ای برای معماری‌های ترانسفورمر که معمولاً در مدل‌های زبانی استفاده می‌شوند، ارائه می‌دهد.

### ۳. پشتیبانی از کمینه‌سازی
پشتیبانی داخلی از کمینه‌سازی ۴ بیت و ۸ بیت نیازهای حافظه را کاهش می‌دهد و در عین حال کیفیت مدل را حفظ می‌کند، و امکان اجرای مدل‌های بزرگ‌تر بر روی سخت‌افزار مصرف‌کننده را فراهم می‌کند.

### ۴. یکپارچگی با Hugging Face
یکپارچگی بی‌دردسر با اکوسیستم Hugging Face دسترسی به هزاران مدل زبانی از پیش آموزش‌دیده را با ابزارهای تبدیل ساده فراهم می‌کند.

### ۵. تنظیم دقیق LoRA
پشتیبانی از تنظیم دقیق Low-Rank Adaptation (LoRA) امکان تنظیم کارآمد مدل‌های بزرگ با منابع محاسباتی حداقلی را فراهم می‌کند.

## راهنمای نصب

### الزامات سیستم
- **macOS 13.0+** (برای بهینه‌سازی Apple Silicon)
- **Python 3.8+**
- **Apple Silicon** (سری M1، M2، M3، M4)
- **محیط بومی ARM** (نه تحت Rosetta)
- **۸ گیگابایت رم یا بیشتر** (۱۶ گیگابایت یا بیشتر برای مدل‌های بزرگ توصیه می‌شود)

### نصب سریع برای LLMها

ساده‌ترین راه برای شروع کار با مدل‌های زبانی نصب MLX-LM است:

```bash
pip install mlx-lm
```

این دستور واحد هم چارچوب اصلی MLX و هم ابزارهای مدل زبانی را نصب می‌کند.

### تنظیم محیط مجازی (توصیه‌شده)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### وابستگی‌های اضافی برای مدل‌های صوتی

اگر قصد دارید با مدل‌های گفتاری مانند Whisper کار کنید:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## شروع کار با MLX

### اولین مدل زبانی شما

بیایید با اجرای یک مثال ساده تولید متن شروع کنیم:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### مثال API پایتون

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### درک بارگذاری مدل

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: مدل‌های زبانی

### معماری‌های مدل پشتیبانی‌شده

MLX-LM از طیف گسترده‌ای از معماری‌های محبوب مدل زبانی پشتیبانی می‌کند:

- **LLaMA و LLaMA 2** - مدل‌های پایه‌ای متا
- **Mistral و Mixtral** - مدل‌های کارآمد و قدرتمند
- **Phi-3** - مدل‌های فشرده مایکروسافت
- **Qwen** - مدل‌های چندزبانه علی‌بابا
- **Code Llama** - تخصصی برای تولید کد
- **Gemma** - مدل‌های زبانی باز گوگل

### رابط خط فرمان

رابط خط فرمان MLX-LM ابزارهای قدرتمندی برای کار با مدل‌های زبانی ارائه می‌دهد:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### API پایتون برای موارد استفاده پیشرفته

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## کار با مدل‌های زبانی بزرگ

### الگوهای تولید متن

#### تولید تک‌مرحله‌ای
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### پیروی از دستورات
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### نوشتن خلاقانه
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### مکالمات چندمرحله‌ای

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## یکپارچگی با Hugging Face

### یافتن مدل‌های سازگار با MLX

MLX به طور بی‌دردسر با اکوسیستم Hugging Face کار می‌کند:

- **مرور مدل‌های MLX**: https://huggingface.co/models?library=mlx&sort=trending
- **جامعه MLX**: https://huggingface.co/mlx-community (مدل‌های پیش‌تبدیل‌شده)
- **مدل‌های اصلی**: اکثر مدل‌های LLaMA، Mistral، Phi و Qwen با تبدیل کار می‌کنند

### بارگذاری مدل‌ها از Hugging Face

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### دانلود مدل‌ها برای استفاده آفلاین

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## تبدیل و کمینه‌سازی مدل‌ها

### تبدیل مدل‌های Hugging Face به MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### درک کمینه‌سازی

کمینه‌سازی اندازه مدل و استفاده از حافظه را با کمترین افت کیفیت کاهش می‌دهد:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### کمینه‌سازی سفارشی

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## تنظیم دقیق مدل‌های زبانی

### تنظیم دقیق LoRA (Low-Rank Adaptation)

MLX از تنظیم دقیق کارآمد با استفاده از LoRA پشتیبانی می‌کند که به شما امکان می‌دهد مدل‌های بزرگ را با منابع محاسباتی حداقلی تطبیق دهید:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### آماده‌سازی داده‌های آموزشی

یک فایل JSON با مثال‌های آموزشی خود ایجاد کنید:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### دستور تنظیم دقیق

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### استفاده از مدل‌های تنظیم‌شده

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## ویژگی‌های پیشرفته LLM

### ذخیره‌سازی پیش‌زمینه برای کارایی

برای استفاده مکرر از همان زمینه، MLX از ذخیره‌سازی پیش‌زمینه برای بهبود عملکرد پشتیبانی می‌کند:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### تولید متن به صورت جریانی

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### کار با مدل‌های تولید کد

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### کار با مدل‌های چت

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## بهترین روش‌ها برای LLMها

### مدیریت حافظه

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### دستورالعمل‌های انتخاب مدل

**برای آزمایش و یادگیری:**
- از مدل‌های کمینه‌شده ۴ بیت استفاده کنید (مانند `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- با مدل‌های کوچک‌تر مانند Phi-3-mini شروع کنید

**برای برنامه‌های تولیدی:**
- توازن بین اندازه مدل و کیفیت را در نظر بگیرید
- هم مدل‌های کمینه‌شده و هم مدل‌های با دقت کامل را آزمایش کنید
- بر اساس موارد استفاده خاص خود معیارگذاری کنید

**برای وظایف خاص:**
- **تولید کد**: CodeLlama، Code Llama Instruct
- **چت عمومی**: Mistral-7B-Instruct، Phi-3
- **چندزبانه**: مدل‌های Qwen
- **نوشتن خلاقانه**: تنظیمات دمای بالاتر با Mistral یا LLaMA

### بهترین روش‌های مهندسی پیش‌زمینه

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### بهینه‌سازی عملکرد

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## رفع مشکلات

### مشکلات رایج و راه‌حل‌ها

#### مشکلات نصب

**مشکل**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**راه‌حل**: از پایتون ARM بومی یا Miniconda استفاده کنید:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### مشکلات حافظه

**مشکل**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### مشکلات بارگذاری مدل

**مشکل**: مدل بارگذاری نمی‌شود یا خروجی ضعیفی تولید می‌کند
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### مشکلات عملکرد

**مشکل**: سرعت تولید کند
- برنامه‌های دیگر که حافظه زیادی مصرف می‌کنند را ببندید
- در صورت امکان از مدل‌های کمینه‌شده استفاده کنید
- مطمئن شوید که تحت Rosetta اجرا نمی‌کنید
- قبل از بارگذاری مدل‌ها حافظه موجود را بررسی کنید

### نکات اشکال‌زدایی

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## منابع اضافی

### مستندات و مخازن رسمی

- **مخزن GitHub MLX**: https://github.com/ml-explore/mlx
- **نمونه‌های MLX-LM**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **مستندات MLX**: https://ml-explore.github.io/mlx/
- **یکپارچگی Hugging Face MLX**: https://huggingface.co/docs/hub/en/mlx

### مجموعه مدل‌ها

- **مدل‌های جامعه MLX**: https://huggingface.co/mlx-community
- **مدل‌های محبوب MLX**: https://huggingface.co/models?library=mlx&sort=trending

### برنامه‌های نمونه

1. **دستیار هوش مصنوعی شخصی**: یک چت‌بات محلی با حافظه مکالمه بسازید
2. **کمک‌کننده کدنویسی**: یک دستیار کدنویسی برای جریان کاری توسعه خود ایجاد کنید
3. **تولیدکننده محتوا**: ابزارهایی برای نوشتن، خلاصه‌سازی و ایجاد محتوا توسعه دهید
4. **مدل‌های تنظیم‌شده سفارشی**: مدل‌ها را برای وظایف خاص حوزه تطبیق دهید
5. **برنامه‌های چندوجهی**: تولید متن را با سایر قابلیت‌های MLX ترکیب کنید

### جامعه و یادگیری

- **بحث‌های جامعه MLX**: مسائل و بحث‌های GitHub
- **فروم‌های Hugging Face**: پشتیبانی جامعه و اشتراک مدل‌ها
- **مستندات توسعه‌دهنده اپل**: منابع رسمی ML اپل

### استناد

اگر از MLX در تحقیقات خود استفاده می‌کنید، لطفاً استناد کنید:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## نتیجه‌گیری

Apple MLX چشم‌انداز اجرای مدل‌های زبانی بزرگ بر روی کامپیوترهای مک را متحول کرده است. با ارائه بهینه‌سازی بومی Apple Silicon، یکپارچگی بی‌دردسر با Hugging Face و ویژگی‌های قدرتمندی مانند کمینه‌سازی و تنظیم دقیق LoRA، MLX امکان اجرای مدل‌های زبانی پیچیده به صورت محلی با عملکرد عالی را فراهم می‌کند.

چه در حال ساخت چت‌بات‌ها، دستیارهای کدنویسی، تولیدکننده‌های محتوا یا مدل‌های تنظیم‌شده سفارشی باشید، MLX ابزارها و عملکرد مورد نیاز برای استفاده کامل از پتانسیل مک Apple Silicon شما برای برنامه‌های مدل زبانی را فراهم می‌کند. تمرکز این چارچوب بر کارایی و سهولت استفاده، آن را به انتخابی عالی برای تحقیقات و برنامه‌های تولیدی تبدیل کرده است.

با مثال‌های پایه‌ای در این آموزش شروع کنید، اکوسیستم غنی مدل‌های پیش‌تبدیل‌شده در Hugging Face را بررسی کنید و به تدریج به ویژگی‌های پیشرفته‌تر مانند تنظیم دقیق و توسعه مدل‌های سفارشی بپردازید. با رشد مداوم اکوسیستم MLX، این چارچوب به یک پلتفرم قدرتمند برای توسعه مدل‌های زبانی بر روی سخت‌افزار اپل تبدیل شده است.

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه انسانی حرفه‌ای استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.