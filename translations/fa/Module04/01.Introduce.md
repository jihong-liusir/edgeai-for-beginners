<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49e18143f97a802a8647f4be28355348",
  "translation_date": "2025-09-17T15:45:32+00:00",
  "source_file": "Module04/01.Introduce.md",
  "language_code": "fa"
}
-->
# بخش ۱: مبانی تبدیل فرمت مدل و کوانتیزاسیون

تبدیل فرمت مدل و کوانتیزاسیون از پیشرفت‌های مهم در EdgeAI هستند که امکان اجرای قابلیت‌های پیشرفته یادگیری ماشین را بر روی دستگاه‌های با منابع محدود فراهم می‌کنند. درک نحوه تبدیل، بهینه‌سازی و استقرار مؤثر مدل‌ها برای ساخت راه‌حل‌های عملی مبتنی بر هوش مصنوعی در لبه ضروری است.

## مقدمه

در این آموزش، تکنیک‌های تبدیل فرمت مدل و کوانتیزاسیون و استراتژی‌های پیشرفته پیاده‌سازی آن‌ها را بررسی خواهیم کرد. مفاهیم اساسی فشرده‌سازی مدل، مرزها و طبقه‌بندی‌های تبدیل فرمت، تکنیک‌های بهینه‌سازی و استراتژی‌های عملی استقرار در محیط‌های محاسباتی لبه را پوشش خواهیم داد.

## اهداف آموزشی

در پایان این آموزش، شما قادر خواهید بود:

- 🔢 مرزها و طبقه‌بندی‌های کوانتیزاسیون در سطوح دقت مختلف را درک کنید.
- 🛠️ تکنیک‌های کلیدی تبدیل فرمت برای استقرار مدل‌ها بر روی دستگاه‌های لبه را شناسایی کنید.
- 🚀 استراتژی‌های پیشرفته کوانتیزاسیون و فشرده‌سازی برای استنتاج بهینه را بیاموزید.

## درک مرزها و طبقه‌بندی‌های کوانتیزاسیون مدل

کوانتیزاسیون مدل تکنیکی است که برای کاهش دقت پارامترهای شبکه عصبی با تعداد بیت‌های کمتر از مدل‌های با دقت کامل طراحی شده است. در حالی که مدل‌های با دقت کامل از نمایش‌های ۳۲ بیتی شناور استفاده می‌کنند، مدل‌های کوانتیزه شده به طور خاص برای کارایی و استقرار در لبه طراحی شده‌اند.

چارچوب طبقه‌بندی دقت به ما کمک می‌کند تا دسته‌های مختلف سطوح کوانتیزاسیون و موارد استفاده مناسب آن‌ها را درک کنیم. این طبقه‌بندی برای انتخاب سطح دقت مناسب برای سناریوهای خاص محاسباتی لبه بسیار مهم است.

### چارچوب طبقه‌بندی دقت

درک مرزهای دقت به انتخاب سطوح کوانتیزاسیون مناسب برای سناریوهای مختلف محاسباتی لبه کمک می‌کند:

- **🔬 دقت فوق‌العاده پایین**: کوانتیزاسیون ۱ بیت تا ۲ بیت (فشرده‌سازی شدید برای سخت‌افزار تخصصی)
- **📱 دقت پایین**: کوانتیزاسیون ۳ بیت تا ۴ بیت (تعادل بین عملکرد و کارایی)
- **⚖️ دقت متوسط**: کوانتیزاسیون ۵ بیت تا ۸ بیت (نزدیک به قابلیت‌های دقت کامل با حفظ کارایی)

مرز دقیق در جامعه تحقیقاتی سیال است، اما اکثر متخصصان ۸ بیت و پایین‌تر را "کوانتیزه شده" در نظر می‌گیرند، با برخی منابع که آستانه‌های تخصصی برای اهداف سخت‌افزاری مختلف تعیین می‌کنند.

### مزایای کلیدی کوانتیزاسیون مدل

کوانتیزاسیون مدل چندین مزیت اساسی ارائه می‌دهد که آن را برای کاربردهای محاسباتی لبه ایده‌آل می‌کند:

**کارایی عملیاتی**: مدل‌های کوانتیزه شده زمان‌های استنتاج سریع‌تری را به دلیل کاهش پیچیدگی محاسباتی ارائه می‌دهند، که آن‌ها را برای کاربردهای بلادرنگ ایده‌آل می‌کند. آن‌ها به منابع محاسباتی کمتری نیاز دارند، امکان استقرار بر روی دستگاه‌های با منابع محدود را فراهم می‌کنند، انرژی کمتری مصرف می‌کنند و ردپای کربنی کاهش‌یافته‌ای دارند.

**انعطاف‌پذیری استقرار**: این مدل‌ها قابلیت‌های هوش مصنوعی روی دستگاه را بدون نیاز به اتصال اینترنت فراهم می‌کنند، از طریق پردازش محلی حریم خصوصی و امنیت را افزایش می‌دهند، می‌توانند برای کاربردهای خاص حوزه سفارشی شوند و برای محیط‌های مختلف محاسباتی لبه مناسب هستند.

**مقرون‌به‌صرفه بودن**: مدل‌های کوانتیزه شده آموزش و استقرار مقرون‌به‌صرفه‌تری نسبت به مدل‌های با دقت کامل ارائه می‌دهند، با کاهش هزینه‌های عملیاتی و نیازهای پهنای باند کمتر برای کاربردهای لبه.

## استراتژی‌های پیشرفته کسب فرمت مدل

### GGUF (فرمت عمومی جهانی GGML)

GGUF به عنوان فرمت اصلی برای استقرار مدل‌های کوانتیزه شده بر روی CPU و دستگاه‌های لبه عمل می‌کند. این فرمت منابع جامعی برای تبدیل و استقرار مدل ارائه می‌دهد:

**ویژگی‌های کشف فرمت**: این فرمت پشتیبانی پیشرفته‌ای برای سطوح مختلف کوانتیزاسیون، سازگاری مجوز و بهینه‌سازی عملکرد ارائه می‌دهد. کاربران می‌توانند به سازگاری بین پلتفرمی، معیارهای عملکرد بلادرنگ و پشتیبانی WebGPU برای استقرار مبتنی بر مرورگر دسترسی داشته باشند.

**مجموعه‌های سطح کوانتیزاسیون**: فرمت‌های کوانتیزاسیون محبوب شامل Q4_K_M برای فشرده‌سازی متعادل، سری Q5_K_S برای کاربردهای با تمرکز بر کیفیت، Q8_0 برای دقت نزدیک به اصلی و فرمت‌های آزمایشی مانند Q2_K برای استقرار با دقت فوق‌العاده پایین هستند. این فرمت همچنین دارای تغییرات مبتنی بر جامعه با پیکربندی‌های تخصصی برای حوزه‌های خاص و انواع عمومی و تنظیم‌شده برای دستورالعمل است که برای موارد استفاده مختلف بهینه شده‌اند.

### ONNX (تبادل شبکه عصبی باز)

فرمت ONNX سازگاری بین چارچوبی برای مدل‌های کوانتیزه شده با قابلیت‌های یکپارچه‌سازی پیشرفته ارائه می‌دهد:

**یکپارچه‌سازی سازمانی**: این فرمت شامل مدل‌هایی با پشتیبانی در سطح سازمانی و قابلیت‌های بهینه‌سازی است، که شامل کوانتیزاسیون پویا برای دقت تطبیقی و کوانتیزاسیون ایستا برای استقرار تولید می‌شود. همچنین از مدل‌های مختلف چارچوب‌ها با رویکردهای کوانتیزاسیون استاندارد پشتیبانی می‌کند.

**مزایای سازمانی**: ابزارهای داخلی برای بهینه‌سازی، استقرار بین پلتفرمی و شتاب سخت‌افزاری در موتورهای استنتاج مختلف یکپارچه شده‌اند. پشتیبانی مستقیم چارچوب با API‌های استاندارد، ویژگی‌های بهینه‌سازی یکپارچه و جریان‌های کاری جامع استقرار تجربه سازمانی را بهبود می‌بخشد.

## تکنیک‌های پیشرفته کوانتیزاسیون و بهینه‌سازی

### چارچوب بهینه‌سازی Llama.cpp

Llama.cpp تکنیک‌های کوانتیزاسیون پیشرفته‌ای برای حداکثر کارایی در استقرار لبه ارائه می‌دهد:

**روش‌های کوانتیزاسیون**: این چارچوب از سطوح مختلف کوانتیزاسیون از جمله Q4_0 (کوانتیزاسیون ۴ بیت با کاهش اندازه عالی - ایده‌آل برای استقرار موبایل)، Q5_1 (کوانتیزاسیون ۵ بیت با تعادل کیفیت و فشرده‌سازی - مناسب برای استنتاج لبه) و Q8_0 (کوانتیزاسیون ۸ بیت برای کیفیت نزدیک به اصلی - توصیه‌شده برای استفاده تولیدی) پشتیبانی می‌کند. فرمت‌های پیشرفته مانند Q2_K نمایانگر فشرده‌سازی پیشرفته برای سناریوهای شدید هستند.

**مزایای پیاده‌سازی**: استنتاج بهینه‌شده برای CPU با شتاب SIMD بارگذاری و اجرای مدل را بهینه می‌کند. سازگاری بین پلتفرمی در معماری‌های x86، ARM و Apple Silicon قابلیت‌های استقرار مستقل از سخت‌افزار را فراهم می‌کند.

**مقایسه ردپای حافظه**: سطوح مختلف کوانتیزاسیون مبادلات متفاوتی بین اندازه مدل و کیفیت ارائه می‌دهند. Q4_0 تقریباً ۷۵٪ کاهش اندازه، Q5_1 ۷۰٪ کاهش با حفظ بهتر کیفیت و Q8_0 ۵۰٪ کاهش با حفظ عملکرد نزدیک به اصلی را ارائه می‌دهد.

### مجموعه بهینه‌سازی Microsoft Olive

Microsoft Olive جریان‌های کاری جامع بهینه‌سازی مدل را برای محیط‌های تولید طراحی کرده است:

**تکنیک‌های بهینه‌سازی**: این مجموعه شامل کوانتیزاسیون پویا برای انتخاب خودکار دقت، بهینه‌سازی گراف و ترکیب اپراتور برای بهبود کارایی، بهینه‌سازی‌های خاص سخت‌افزار برای استقرار CPU، GPU و NPU و خطوط لوله بهینه‌سازی چندمرحله‌ای است. جریان‌های کاری کوانتیزاسیون تخصصی از سطوح مختلف دقت از ۸ بیت تا پیکربندی‌های آزمایشی ۱ بیت پشتیبانی می‌کنند.

**اتوماسیون جریان کاری**: معیارگذاری خودکار در انواع بهینه‌سازی کیفیت معیارها را در طول بهینه‌سازی حفظ می‌کند. یکپارچه‌سازی با چارچوب‌های محبوب ML مانند PyTorch و ONNX قابلیت‌های بهینه‌سازی استقرار در ابر و لبه را فراهم می‌کند.

### چارچوب Apple MLX

Apple MLX بهینه‌سازی بومی را به طور خاص برای دستگاه‌های Apple Silicon طراحی کرده است:

**بهینه‌سازی Apple Silicon**: این چارچوب از معماری حافظه یکپارچه با یکپارچه‌سازی Metal Performance Shaders، استنتاج دقت مختلط خودکار و استفاده بهینه از پهنای باند حافظه استفاده می‌کند. مدل‌ها عملکرد استثنایی بر روی تراشه‌های سری M نشان می‌دهند و تعادل بهینه‌ای برای استقرار در دستگاه‌های مختلف اپل فراهم می‌کنند.

**ویژگی‌های توسعه**: پشتیبانی از API‌های Python و Swift با عملیات آرایه سازگار با NumPy، قابلیت‌های مشتق‌گیری خودکار و یکپارچه‌سازی بی‌دردسر با ابزارهای توسعه اپل محیط توسعه جامعی را فراهم می‌کند.

## استراتژی‌های استقرار تولید و استنتاج

### Ollama: استقرار محلی ساده‌شده

Ollama استقرار مدل را با ویژگی‌های آماده سازمانی برای محیط‌های محلی و لبه ساده می‌کند:

**قابلیت‌های استقرار**: نصب و اجرای مدل با یک فرمان و کشیدن و ذخیره خودکار مدل. پشتیبانی از فرمت‌های مختلف کوانتیزه شده با REST API برای یکپارچه‌سازی برنامه و قابلیت‌های مدیریت و تغییر مدل چندگانه. سطوح کوانتیزاسیون پیشرفته نیاز به پیکربندی خاص برای استقرار بهینه دارند.

**ویژگی‌های پیشرفته**: پشتیبانی از تنظیم دقیق مدل سفارشی، تولید Dockerfile برای استقرار کانتینری، شتاب GPU با تشخیص خودکار و گزینه‌های کوانتیزاسیون و بهینه‌سازی مدل انعطاف‌پذیری جامع استقرار را فراهم می‌کند.

### VLLM: استنتاج با عملکرد بالا

VLLM بهینه‌سازی استنتاج در سطح تولید را برای سناریوهای با توان بالا ارائه می‌دهد:

**بهینه‌سازی‌های عملکرد**: PagedAttention برای محاسبه کارآمد توجه حافظه، دسته‌بندی پویا برای بهینه‌سازی توان، موازی‌سازی تنسور برای مقیاس‌گذاری چند GPU و رمزگشایی حدسی برای کاهش تأخیر. فرمت‌های کوانتیزاسیون پیشرفته نیاز به هسته‌های استنتاج تخصصی برای عملکرد بهینه دارند.

**یکپارچه‌سازی سازمانی**: نقاط پایانی API سازگار با OpenAI، پشتیبانی از استقرار Kubernetes، یکپارچه‌سازی نظارت و مشاهده‌پذیری و قابلیت‌های مقیاس‌گذاری خودکار راه‌حل‌های استقرار در سطح سازمانی را فراهم می‌کنند.

### راه‌حل‌های لبه مایکروسافت

مایکروسافت قابلیت‌های جامع استقرار لبه را برای محیط‌های سازمانی ارائه می‌دهد:

**ویژگی‌های محاسبات لبه**: طراحی معماری آفلاین اول با بهینه‌سازی محدودیت منابع، مدیریت رجیستری مدل محلی و قابلیت‌های همگام‌سازی لبه به ابر استقرار لبه قابل اعتماد را تضمین می‌کند.

**امنیت و انطباق**: پردازش داده‌های محلی برای حفظ حریم خصوصی، کنترل‌های امنیتی سازمانی، گزارش‌گیری حسابرسی و انطباق و مدیریت دسترسی مبتنی بر نقش امنیت جامع برای استقرارهای لبه فراهم می‌کند.

## بهترین شیوه‌ها برای پیاده‌سازی کوانتیزاسیون مدل

### دستورالعمل‌های انتخاب سطح کوانتیزاسیون

هنگام انتخاب سطوح کوانتیزاسیون برای استقرار لبه، عوامل زیر را در نظر بگیرید:

**ملاحظات تعداد دقت**: دقت فوق‌العاده پایین مانند Q2_K را برای کاربردهای موبایل شدید، دقت پایین مانند Q4_K_M را برای سناریوهای عملکرد متعادل و دقت متوسط مانند Q8_0 را هنگام نزدیک شدن به قابلیت‌های دقت کامل با حفظ کارایی انتخاب کنید. فرمت‌های آزمایشی فشرده‌سازی تخصصی برای کاربردهای تحقیقاتی خاص ارائه می‌دهند.

**هم‌راستایی با موارد استفاده**: قابلیت‌های کوانتیزاسیون را با نیازهای خاص برنامه مطابقت دهید، عواملی مانند حفظ دقت، سرعت استنتاج، محدودیت‌های حافظه و نیازهای عملیات آفلاین را در نظر بگیرید.

### انتخاب استراتژی بهینه‌سازی

**رویکرد کوانتیزاسیون**: سطوح کوانتیزاسیون مناسب را بر اساس نیازهای کیفیت و محدودیت‌های سخت‌افزاری انتخاب کنید. Q4_0 را برای حداکثر فشرده‌سازی، Q5_1 را برای تعادل کیفیت و فشرده‌سازی و Q8_0 را برای حفظ کیفیت نزدیک به اصلی در نظر بگیرید. فرمت‌های آزمایشی نمایانگر مرز فشرده‌سازی شدید برای کاربردهای تخصصی هستند.

**انتخاب چارچوب**: چارچوب‌های بهینه‌سازی را بر اساس سخت‌افزار هدف و نیازهای استقرار انتخاب کنید. از Llama.cpp برای استقرار بهینه‌شده برای CPU، Microsoft Olive برای جریان‌های کاری جامع بهینه‌سازی و Apple MLX برای دستگاه‌های Apple Silicon استفاده کنید.

## تبدیل فرمت عملی و موارد استفاده

### سناریوهای استقرار واقعی

**برنامه‌های موبایل**: فرمت‌های Q4_K در برنامه‌های گوشی هوشمند با ردپای حافظه کم عالی عمل می‌کنند، در حالی که Q8_0 عملکرد متعادل برای برنامه‌های مبتنی بر تبلت ارائه می‌دهد. فرمت‌های Q5_K کیفیت برتر برای برنامه‌های بهره‌وری موبایل ارائه می‌دهند.

**محاسبات دسکتاپ و لبه**: Q5_K عملکرد بهینه برای برنامه‌های دسکتاپ ارائه می‌دهد، Q8_0 استنتاج با کیفیت بالا برای محیط‌های ایستگاه کاری فراهم می‌کند و Q4_K پردازش کارآمد بر روی دستگاه‌های لبه را امکان‌پذیر می‌کند.

**تحقیق و آزمایش**: فرمت‌های کوانتیزاسیون پیشرفته امکان بررسی استنتاج با دقت فوق‌العاده پایین برای تحقیقات دانشگاهی و کاربردهای اثبات مفهوم با محدودیت‌های شدید منابع را فراهم می‌کنند.

### معیارهای عملکرد و مقایسه‌ها

**سرعت استنتاج**: Q4_K سریع‌ترین زمان‌های استنتاج را بر روی CPUهای موبایل ارائه می‌دهد، Q5_K نسبت سرعت-کیفیت متعادل برای برنامه‌های عمومی فراهم می‌کند، Q8_0 کیفیت برتر برای وظایف پیچیده ارائه می‌دهد و فرمت‌های آزمایشی حداکثر توان نظری را با سخت‌افزار تخصصی ارائه می‌دهند.

**نیازهای حافظه**: سطوح کوانتیزاسیون از Q2_K (کمتر از ۵۰۰ مگابایت برای مدل‌های کوچک) تا Q8_0 (تقریباً ۵۰٪ اندازه اصلی) متغیر هستند، با پیکربندی‌های آزمایشی که به حداکثر نسبت‌های فشرده‌سازی دست می‌یابند.

## چالش‌ها و ملاحظات

### مبادلات عملکرد

استقرار کوانتیزاسیون شامل بررسی دقیق مبادلات بین اندازه مدل، سرعت استنتاج و کیفیت خروجی است. در حالی که Q4_K سرعت و کارایی استثنایی ارائه می‌دهد، Q8_0 کیفیت برتر را با هزینه افزایش نیازهای منابع فراهم می‌کند. Q5_K تعادل مناسبی برای اکثر کاربردهای عمومی ایجاد می‌کند.

### سازگاری سخت‌افزاری

دستگاه‌های لبه مختلف قابلیت‌ها و محدودیت‌های متفاوتی دارند. Q4_K بر روی پردازنده‌های پایه به طور کارآمد اجرا می‌شود، Q5_K به منابع محاسباتی متوسط نیاز دارد و Q8_0 از سخت‌افزارهای پیشرفته بهره می‌برد. فرمت‌های آزمایشی به سخت‌افزار یا پیاده‌سازی‌های نرم‌افزاری تخصصی برای عملیات بهینه نیاز دارند.

### امنیت و حریم خصوصی

در حالی که مدل‌های کوانتیزه شده پردازش محلی را برای افزایش حریم خصوصی امکان‌پذیر می‌کنند، اقدامات امنیتی مناسب باید برای حفاظت از مدل‌ها و داده‌ها در محیط‌های لبه اجرا شوند. این امر به ویژه هنگام استقرار فرمت‌های با دقت بالا در محیط‌های سازمانی یا فرمت‌های فشرده در برنامه‌هایی که داده‌های حساس را مدیریت می‌کنند، مهم است.

## روندهای آینده در کوانتیزاسیون مدل

چشم‌انداز کوانتیزاسیون با پیشرفت در تکنیک‌های فشرده‌سازی، روش‌های بهینه‌سازی و استراتژی‌های استقرار به تکامل خود ادامه می‌دهد. توسعه‌های آینده شامل الگوریتم‌های کوانتیزاسیون کارآمدتر، روش‌های فشرده‌سازی بهبود‌یافته و یکپارچه‌سازی بهتر با شتاب‌دهنده‌های سخت‌افزاری لبه خواهد بود.

درک این روندها و حفظ آگاهی از فناوری‌های نوظهور برای به‌روز ماندن با بهترین شیوه‌های توسعه و استقرار کوانتیزاسیون ضروری خواهد بود.

## منابع اضافی

- [مستندات Hugging Face GGUF](https://huggingface.co/docs/hub/en/gguf)
- [بهینه‌سازی مدل ONNX](https://onnxruntime.ai/docs/performance/model-optimizations/)
- [مستندات llama.cpp](https://github.com/ggml-org/llama.cpp)
- [چار

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.