<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-17T14:57:25+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "fa"
}
-->
# بخش ۳: اصول خانواده مدل‌های Gemma

خانواده مدل‌های Gemma نمایانگر رویکرد جامع گوگل به مدل‌های زبان بزرگ متن‌باز و هوش مصنوعی چندرسانه‌ای است. این مدل‌ها نشان می‌دهند که مدل‌های قابل دسترس می‌توانند عملکرد چشمگیری داشته باشند و در عین حال در سناریوهای مختلف از دستگاه‌های موبایل تا ایستگاه‌های کاری سازمانی قابل اجرا باشند. درک این موضوع که خانواده Gemma چگونه قابلیت‌های قدرتمند هوش مصنوعی را با گزینه‌های انعطاف‌پذیر برای اجرا فراهم می‌کند، در حالی که عملکرد رقابتی و اصول مسئولانه هوش مصنوعی را حفظ می‌کند، بسیار مهم است.

## مقدمه

در این آموزش، خانواده مدل‌های Gemma گوگل و مفاهیم اساسی آن را بررسی خواهیم کرد. ما به تکامل خانواده Gemma، روش‌های نوآورانه آموزشی که مدل‌های Gemma را مؤثر می‌سازند، انواع کلیدی در این خانواده و کاربردهای عملی در سناریوهای مختلف اجرا خواهیم پرداخت.

## اهداف آموزشی

در پایان این آموزش، شما قادر خواهید بود:

- فلسفه طراحی و تکامل خانواده مدل‌های Gemma گوگل را درک کنید
- نوآوری‌های کلیدی که به مدل‌های Gemma امکان دستیابی به عملکرد بالا در اندازه‌های مختلف پارامتر را می‌دهند، شناسایی کنید
- مزایا و محدودیت‌های انواع مختلف مدل‌های Gemma را بشناسید
- دانش خود از مدل‌های Gemma را برای انتخاب انواع مناسب در سناریوهای واقعی به کار ببرید

## درک چشم‌انداز مدرن مدل‌های هوش مصنوعی

چشم‌انداز هوش مصنوعی به طور قابل توجهی تکامل یافته است، و سازمان‌های مختلف رویکردهای متفاوتی را برای توسعه مدل‌های زبان دنبال می‌کنند. در حالی که برخی بر مدل‌های اختصاصی و بسته تمرکز دارند که فقط از طریق API‌ها قابل دسترسی هستند، دیگران بر دسترسی متن‌باز و شفافیت تأکید دارند. رویکرد سنتی شامل مدل‌های اختصاصی عظیم با هزینه‌های جاری یا مدل‌های متن‌باز است که ممکن است برای اجرا نیاز به تخصص فنی قابل توجهی داشته باشند.

این پارادایم چالش‌هایی را برای سازمان‌هایی ایجاد می‌کند که به دنبال قابلیت‌های قدرتمند هوش مصنوعی هستند و در عین حال کنترل بر داده‌ها، هزینه‌ها و انعطاف‌پذیری اجرا را حفظ می‌کنند. رویکرد معمول اغلب نیازمند انتخاب بین عملکرد پیشرفته و ملاحظات عملی اجرا است.

## چالش برتری هوش مصنوعی قابل دسترس

نیاز به هوش مصنوعی با کیفیت بالا و قابل دسترس در سناریوهای مختلف به طور فزاینده‌ای مهم شده است. به کاربردهایی فکر کنید که نیاز به گزینه‌های انعطاف‌پذیر اجرا برای نیازهای مختلف سازمانی، پیاده‌سازی‌های مقرون‌به‌صرفه که هزینه‌های API می‌تواند قابل توجه باشد، قابلیت‌های چندرسانه‌ای برای درک جامع، یا اجرای تخصصی در دستگاه‌های موبایل و لبه دارند.

### الزامات کلیدی اجرا

اجراهای مدرن هوش مصنوعی با چندین الزام اساسی مواجه هستند که کاربرد عملی را محدود می‌کنند:

- **دسترسی‌پذیری**: دسترسی متن‌باز برای شفافیت و سفارشی‌سازی
- **مقرون‌به‌صرفه بودن**: نیازهای محاسباتی معقول برای بودجه‌های مختلف
- **انعطاف‌پذیری**: اندازه‌های مختلف مدل برای سناریوهای مختلف اجرا
- **درک چندرسانه‌ای**: قابلیت پردازش تصویر، متن و صوت
- **اجرای لبه**: عملکرد بهینه در دستگاه‌های موبایل و منابع محدود

## فلسفه مدل Gemma

خانواده مدل‌های Gemma نمایانگر رویکرد جامع گوگل به توسعه مدل‌های هوش مصنوعی است که دسترسی متن‌باز، قابلیت‌های چندرسانه‌ای و اجرای عملی را در اولویت قرار می‌دهد و در عین حال ویژگی‌های عملکرد رقابتی را حفظ می‌کند. مدل‌های Gemma این اهداف را از طریق اندازه‌های متنوع مدل، روش‌های آموزشی با کیفیت بالا که از تحقیقات Gemini مشتق شده‌اند، و انواع تخصصی برای دامنه‌ها و سناریوهای مختلف اجرا به دست می‌آورند.

خانواده Gemma شامل رویکردهای مختلفی است که برای ارائه گزینه‌ها در طیف عملکرد-کارایی طراحی شده‌اند، و امکان اجرا از دستگاه‌های موبایل تا سرورهای سازمانی را فراهم می‌کنند و در عین حال قابلیت‌های معنادار هوش مصنوعی را ارائه می‌دهند. هدف این است که دسترسی به فناوری هوش مصنوعی با کیفیت بالا را دموکراتیک کرده و انعطاف‌پذیری در انتخاب‌های اجرا را فراهم کند.

### اصول طراحی اصلی Gemma

مدل‌های Gemma بر اساس چندین اصل بنیادی ساخته شده‌اند که آن‌ها را از خانواده‌های دیگر مدل‌های زبان متمایز می‌کند:

- **اولویت متن‌باز**: شفافیت کامل و دسترسی برای استفاده تحقیقاتی و تجاری
- **توسعه مبتنی بر تحقیق**: ساخته شده با استفاده از همان تحقیق و فناوری که مدل‌های Gemini را قدرت می‌بخشد
- **معماری مقیاس‌پذیر**: اندازه‌های مختلف مدل برای تطابق با نیازهای محاسباتی مختلف
- **هوش مصنوعی مسئولانه**: اقدامات ایمنی یکپارچه و شیوه‌های توسعه مسئولانه

## فناوری‌های کلیدی که خانواده Gemma را ممکن می‌سازند

### روش‌های پیشرفته آموزشی

یکی از جنبه‌های تعریف‌کننده خانواده Gemma، رویکرد آموزشی پیچیده‌ای است که از تحقیقات Gemini گوگل مشتق شده است. مدل‌های Gemma از تقطیر از مدل‌های بزرگ‌تر، یادگیری تقویتی از بازخورد انسانی (RLHF)، و تکنیک‌های ادغام مدل برای دستیابی به عملکرد بهبود یافته در ریاضیات، کدنویسی و پیروی از دستورالعمل‌ها استفاده می‌کنند.

### یکپارچه‌سازی و درک چندرسانه‌ای

مدل‌های اخیر Gemma قابلیت‌های چندرسانه‌ای پیچیده‌ای را شامل می‌شوند که امکان درک جامع از انواع مختلف ورودی را فراهم می‌کنند:

**یکپارچه‌سازی متن و تصویر (Gemma 3)**: Gemma 3 می‌تواند متن و تصاویر را به طور همزمان پردازش کند، و امکان تحلیل تصاویر، پاسخ به سوالات درباره محتوای بصری، استخراج متن از تصاویر، و درک داده‌های بصری پیچیده را فراهم می‌کند.

**پردازش صوتی (Gemma 3n)**: Gemma 3n قابلیت‌های صوتی پیشرفته‌ای از جمله تشخیص خودکار گفتار (ASR) و ترجمه خودکار گفتار (AST) دارد، با عملکرد قوی به ویژه برای ترجمه بین انگلیسی و اسپانیایی، فرانسوی، ایتالیایی و پرتغالی.

**پردازش ورودی‌های ترکیبی**: مدل‌های Gemma از ورودی‌های ترکیبی در میان رسانه‌ها پشتیبانی می‌کنند، و امکان درک تعاملات پیچیده چندرسانه‌ای را فراهم می‌کنند که متن، تصاویر و صوت را به طور همزمان پردازش می‌کنند.

### نوآوری‌های معماری

خانواده Gemma چندین بهینه‌سازی معماری را شامل می‌شود که برای عملکرد و کارایی طراحی شده‌اند:

**گسترش پنجره زمینه**: مدل‌های Gemma 3 دارای پنجره زمینه‌ای ۱۲۸K توکن هستند که ۱۶ برابر بزرگ‌تر از مدل‌های قبلی Gemma است، و امکان پردازش حجم زیادی از اطلاعات از جمله چندین سند یا صدها تصویر را فراهم می‌کند.

**معماری موبایل‌محور (Gemma 3n)**: Gemma 3n از فناوری Per-Layer Embeddings (PLE) و معماری MatFormer استفاده می‌کند، که امکان اجرای مدل‌های بزرگ‌تر با ردپای حافظه‌ای مشابه مدل‌های کوچک‌تر سنتی را فراهم می‌کند.

**قابلیت‌های فراخوانی توابع**: Gemma 3 از فراخوانی توابع پشتیبانی می‌کند، و امکان ساخت رابط‌های زبان طبیعی برای رابط‌های برنامه‌نویسی و ایجاد سیستم‌های اتوماسیون هوشمند را فراهم می‌کند.

## اندازه مدل و گزینه‌های اجرا

محیط‌های اجرای مدرن از انعطاف‌پذیری مدل‌های Gemma در میان نیازهای محاسباتی مختلف بهره‌مند می‌شوند:

### مدل‌های کوچک (۰.۶B-۴B)

Gemma مدل‌های کوچک کارآمدی را ارائه می‌دهد که برای اجرای لبه، برنامه‌های موبایل و محیط‌های محدود منابع مناسب هستند و در عین حال قابلیت‌های چشمگیری را حفظ می‌کنند. مدل ۱B برای برنامه‌های کوچک ایده‌آل است، در حالی که مدل ۴B عملکرد و انعطاف‌پذیری متعادلی با پشتیبانی چندرسانه‌ای ارائه می‌دهد.

### مدل‌های متوسط (۸B-۱۴B)

مدل‌های میان‌رده قابلیت‌های پیشرفته‌ای را برای برنامه‌های حرفه‌ای ارائه می‌دهند، و تعادل عالی بین عملکرد و نیازهای محاسباتی برای اجرا در ایستگاه‌های کاری و سرورها فراهم می‌کنند.

### مدل‌های بزرگ (۲۷B+)

مدل‌های تمام‌مقیاس عملکرد پیشرفته‌ای را برای برنامه‌های پرتقاضا، تحقیق و اجراهای سازمانی که نیاز به حداکثر قابلیت دارند، ارائه می‌دهند. مدل ۲۷B نمایانگر گزینه‌ای است که هنوز می‌تواند بر روی یک GPU اجرا شود.

### مدل‌های بهینه‌شده برای موبایل (Gemma 3n)

مدل‌های Gemma 3n E2B و E4B به طور خاص برای اجرا در موبایل و لبه طراحی شده‌اند، با تعداد مؤثر پارامترهای ۲B و ۴B به ترتیب، در حالی که از معماری نوآورانه برای کاهش ردپای حافظه به حداقل ۲GB برای E2B و ۳GB برای E4B استفاده می‌کنند.

## مزایای خانواده مدل‌های Gemma

### دسترسی متن‌باز

مدل‌های Gemma شفافیت کامل و قابلیت‌های سفارشی‌سازی را با وزن‌های متن‌باز که استفاده تجاری مسئولانه را مجاز می‌کند، ارائه می‌دهند، و امکان تنظیم و اجرای آن‌ها در پروژه‌ها و برنامه‌های سازمانی را فراهم می‌کنند.

### انعطاف‌پذیری اجرا

محدوده اندازه‌های مدل امکان اجرا در میان پیکربندی‌های سخت‌افزاری متنوع، از دستگاه‌های موبایل تا سرورهای پیشرفته، را فراهم می‌کند، با بهینه‌سازی برای پلتفرم‌های مختلف از جمله Google Cloud TPUs، NVIDIA GPUs، AMD GPUs از طریق ROCm، و اجرای CPU از طریق Gemma.cpp.

### برتری چندزبانه

مدل‌های Gemma در درک و تولید چندزبانه برتری دارند، و از بیش از ۱۴۰ زبان با قابلیت‌های چندزبانه بی‌نظیر پشتیبانی می‌کنند، که آن‌ها را برای برنامه‌های جهانی مناسب می‌سازد.

### عملکرد رقابتی

مدل‌های Gemma به طور مداوم نتایج رقابتی در معیارها به دست می‌آورند، و Gemma 3 در ارزیابی‌های ترجیح کاربر در میان مدل‌های محبوب اختصاصی و متن‌باز رتبه بالایی دارد.

### قابلیت‌های تخصصی

برنامه‌های خاص دامنه از درک چندرسانه‌ای Gemma، قابلیت‌های فراخوانی توابع، و عملکرد بهینه در میان پلتفرم‌های سخت‌افزاری مختلف بهره‌مند می‌شوند.

## مثال‌ها و کاربردهای عملی

### مثال درک چندرسانه‌ای

Gemma 3 می‌تواند متن و تصاویر را به طور همزمان برای تحلیل جامع پردازش کند:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### مثال پشتیبانی چندزبانه

مدل‌های Gemma قابلیت‌های چندزبانه قوی در میان بیش از ۱۴۰ زبان را نشان می‌دهند:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### مثال فراخوانی توابع

Gemma 3 از فراخوانی توابع برای ساخت رابط‌های زبان طبیعی پشتیبانی می‌کند:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### مثال اجرای موبایل (Gemma 3n)

Gemma 3n برای اجرا در موبایل و لبه با استفاده کارآمد از حافظه بهینه شده است:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### مثال پردازش صوتی (Gemma 3n)

Gemma 3n شامل قابلیت‌های صوتی پیشرفته برای تشخیص گفتار و ترجمه است:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## تکامل خانواده Gemma

### Gemma 1.0 و 2.0: مدل‌های پایه

مدل‌های اولیه Gemma اصول بنیادی دسترسی متن‌باز و اجرای عملی را پایه‌گذاری کردند:

- **Gemma-2B و 7B**: انتشار اولیه با تمرکز بر درک زبان کارآمد
- **سری Gemma 1.5**: گسترش پردازش زمینه و بهبود عملکرد
- **خانواده Gemma 2**: معرفی قابلیت‌های چندرسانه‌ای و اندازه‌های گسترش‌یافته مدل

### Gemma 3: برتری چندرسانه‌ای

سری Gemma 3 پیشرفت قابل توجهی در قابلیت‌های چندرسانه‌ای و عملکرد داشت. این مدل‌ها از همان تحقیق و فناوری که مدل‌های Gemini 2.0 را قدرت می‌بخشند، ساخته شده‌اند و قابلیت‌های درک متن و تصویر، پنجره زمینه‌ای ۱۲۸K توکن، فراخوانی توابع، و پشتیبانی از بیش از ۱۴۰ زبان را معرفی کردند.

### Gemma 3n: نوآوری موبایل‌محور

Gemma 3n نمایانگر یک پیشرفت در معماری هوش مصنوعی موبایل‌محور است، که شامل فناوری پیشگامانه Per-Layer Embeddings (PLE)، معماری MatFormer برای انعطاف‌پذیری محاسباتی، و قابلیت‌های چندرسانه‌ای جامع از جمله پردازش صوتی است.

## کاربردهای مدل‌های Gemma

### کاربردهای سازمانی

سازمان‌ها از مدل‌های Gemma برای تحلیل اسناد با محتوای بصری، اتوماسیون خدمات مشتری با پشتیبانی چندرسانه‌ای، کمک هوشمند به کدنویسی، و برنامه‌های هوش تجاری استفاده می‌کنند.

### محاسبات موبایل و لبه

برنامه‌های موبایل از Gemma 3n برای هوش مصنوعی بلادرنگ که مستقیماً بر روی دستگاه‌ها اجرا می‌شود، بهره می‌برند.

### فناوری آموزشی

پلتفرم‌های آموزشی از مدل‌های Gemma برای تجربه‌های آموزشی چندرسانه‌ای، تولید محتوای خودکار با عناصر بصری، کمک به یادگیری زبان با پردازش صوتی، و تجربه‌های آموزشی تعاملی استفاده می‌کنند.

### کاربردهای جهانی

برنامه‌های بین‌المللی از قابلیت‌های چندزبانه و چندفرهنگی قوی مدل‌های Gemma بهره‌مند می‌شوند.

## چالش‌ها و محدودیت‌ها

### نیازهای محاسباتی

مدل‌های بزرگ‌تر هنوز به منابع محاسباتی قابل توجهی نیاز دارند.

### عملکرد دامنه تخصصی

برنامه‌های بسیار تخصصی ممکن است از تنظیم دقیق دامنه یا بهینه‌سازی وظیفه‌محور بهره‌مند شوند.

### پیچیدگی انتخاب مدل

محدوده گسترده مدل‌ها و گزینه‌های اجرا ممکن است انتخاب را برای کاربران جدید دشوار کند.

### بهینه‌سازی سخت‌افزار

عملکرد ممکن است در میان پیکربندی‌های سخت‌افزاری مختلف متفاوت باشد.

## آینده خانواده مدل‌های Gemma

خانواده مدل‌های Gemma نمایانگر تکامل مداوم به سمت هوش مصنوعی دموکراتیک و با کیفیت بالا است.

## مثال‌های توسعه و یکپارچه‌سازی

### شروع سریع با Transformers

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3-8B model
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation
messages = [
    {"role": "user", "content": "Explain quantum computing and its potential applications."}
]

# Generate response
input_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### استفاده چندرسانه‌ای با Gemma 3

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

# Load Gemma 3 vision model
model_name = "google/gemma-3-4b-it"
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# Process image and text input
image = Image.open("chart.jpg")
messages = [
    {
        "role": "user", 
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Analyze this chart and explain the key trends you observe."}
        ]
    }
]

# Prepare inputs
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

# Generate multimodal response
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

response = processor.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### پیاده‌سازی فراخوانی توابع

```python
import json

# Define available functions
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    },
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    }
]

def gemma_function_calling(user_query, available_functions):
    """Implement function calling with Gemma 3"""
    
    # Create system prompt with function definitions
    system_prompt = f"""You are a helpful assistant with access to the following functions:

{json.dumps(available_functions, indent=2)}

When the user asks for something that requires a function call, respond with a JSON object containing:
- "function_name": the name of the function to call
- "parameters": the parameters to pass to the function

If no function call is needed, respond normally."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    # Process with Gemma
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        temperature=0.3  # Lower temperature for more structured output
    )
    
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    # Parse function call if present
    try:
        function_call = json.loads(response.strip())
        if "function_name" in function_call:
            return function_call
    except json.JSONDecodeError:
        pass
    
    return {"response": response}

# Example usage
user_request = "What's the weather like in Tokyo and calculate 15% of 850"
result = gemma_function_calling(user_request, functions)
print(result)
```

### 📱 اجرای موبایل با Gemma 3n

```python
# Optimized mobile deployment
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class MobileGemmaService:
    """Optimized Gemma 3n service for mobile deployment"""
    
    def __init__(self, model_name="google/gemma-3n-E2B-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._load_optimized_model()
    
    def _load_optimized_model(self):
        """Load model with mobile optimizations"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load with optimizations for mobile
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # Quantization for efficiency
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if torch.cuda.is_available():
            self.model = torch.jit.trace(self.model, example_inputs=...)  # JIT optimization
    
    def mobile_chat(self, user_input, max_tokens=150):
        """Optimized chat for mobile devices"""
        messages = [{"role": "user", "content": user_input}]
        
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            max_length=512,
            truncation=True
        )
        
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(input_text, "").strip()
    
    def process_audio(self, audio_path, task="transcribe"):
        """Process audio input with Gemma 3n"""
        if task == "transcribe":
            prompt = f"<audio>{audio_path}</audio>\nTranscribe:"
        elif task == "translate":
            prompt = f"<audio>{audio_path}</audio>\nTranslate to English:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return result.replace(prompt, "").strip()

# Initialize mobile service
mobile_gemma = MobileGemmaService()

# Example mobile usage
quick_response = mobile_gemma.mobile_chat("Summarize renewable energy benefits in 2 sentences")
print(f"Mobile Response: {quick_response}")

# Audio processing example
# audio_transcript = mobile_gemma.process_audio("voice_note.wav", task="transcribe")
# print(f"Audio Transcript: {audio_transcript}")
```

### اجرای API با vLLM

```python
from vllm import LLM, SamplingParams
import asyncio
from typing import List, Dict

class GemmaAPIService:
    """High-performance Gemma API service using vLLM"""
    
    def __init__(self, model_name="google/gemma-3-8b-it"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            trust_remote_code=True
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop_token_ids=[self.llm.get_tokenizer().eos_token_id]
        )
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for API processing"""
        tokenizer = self.llm.get_tokenizer()
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_batch(self, batch_messages: List[List[Dict[str, str]]]) -> List[str]:
        """Generate responses for batch of conversations"""
        formatted_prompts = [self.format_messages(messages) for messages in batch_messages]
        
        # Generate responses
        outputs = self.llm.generate(formatted_prompts, self.sampling_params)
        
        # Extract responses
        responses = []
        for output in outputs:
            response = output.outputs[0].text.strip()
            responses.append(response)
        
        return responses
    
    async def stream_generate(self, messages: List[Dict[str, str]]):
        """Stream generation for real-time applications"""
        formatted_prompt = self.format_messages(messages)
        
        # Note: vLLM streaming implementation would go here
        # This is a simplified example
        outputs = self.llm.generate([formatted_prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        # Simulate streaming by yielding chunks
        words = response.split()
        for i in range(0, len(words), 3):  # Yield 3 words at a time
            chunk = " ".join(words[i:i+3])
            yield chunk
            await asyncio.sleep(0.1)  # Simulate streaming delay

# Example API usage
async def api_example():
    api_service = GemmaAPIService()
    
    # Batch processing
    batch_conversations = [
        [{"role": "user", "content": "Explain machine learning briefly"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}],
        [{"role": "user", "content": "How does photosynthesis work?"}]
    ]
    
    responses = await api_service.generate_batch(batch_conversations)
    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response}")
    
    # Streaming example
    print("\nStreaming response:")
    stream_messages = [{"role": "user", "content": "Write a short story about AI and humans working together"}]
    async for chunk in api_service.stream_generate(stream_messages):
        print(chunk, end=" ", flush=True)

# Run API example
# asyncio.run(api_example())
```

## معیارهای عملکرد و دستاوردها

### نکات برجسته عملکرد

**برتری چندرسانه‌ای:**
- جمّا ۳ قابلیت‌های قدرتمندی را برای توسعه‌دهندگان ارائه می‌دهد، با توانایی‌های پیشرفته در استدلال متنی و بصری، و پشتیبانی از ورودی تصویر و متن برای درک چندوجهی  
- جمّا ۳n در میان مدل‌های محبوب اختصاصی و باز در امتیازات Elo Chatbot Arena رتبه بالایی دارد، که نشان‌دهنده ترجیح قوی کاربران است  

**دستاوردهای کارایی:**  
- مدل‌های جمّا ۳ می‌توانند ورودی‌های پرامپت را تا ۱۲۸ هزار توکن مدیریت کنند، که ۱۶ برابر بزرگ‌تر از پنجره زمینه مدل‌های قبلی جمّا است  
- جمّا ۳n از Per-Layer Embeddings (PLE) بهره می‌برد که کاهش قابل توجهی در استفاده از RAM ایجاد می‌کند، در حالی که قابلیت‌های مدل بزرگ‌تر را حفظ می‌کند  

**بهینه‌سازی موبایل:**  
- جمّا ۳n E2B با تنها ۲ گیگابایت حافظه کار می‌کند، در حالی که E4B فقط به ۳ گیگابایت نیاز دارد، با وجود تعداد پارامترهای خام ۵ میلیارد و ۸ میلیارد به ترتیب  
- قابلیت‌های هوش مصنوعی در زمان واقعی مستقیماً بر روی دستگاه‌های موبایل با عملیات آماده آفلاین و اولویت حفظ حریم خصوصی  

**مقیاس آموزش:**  
- جمّا ۳ بر روی ۲ تریلیون توکن برای مدل ۱ میلیارد، ۴ تریلیون برای ۴ میلیارد، ۱۲ تریلیون برای ۱۲ میلیارد، و ۱۴ تریلیون توکن برای مدل‌های ۲۷ میلیارد با استفاده از Google TPUs و چارچوب JAX آموزش داده شده است  

### ماتریس مقایسه مدل  

| سری مدل | محدوده پارامترها | طول زمینه | نقاط قوت کلیدی | بهترین موارد استفاده |  
|--------------|------------------|----------------|---------------|----------------|  
| **جمّا ۳** | ۱ میلیارد-۲۷ میلیارد | ۱۲۸ هزار | درک چندوجهی، فراخوانی توابع | کاربردهای عمومی، وظایف زبان-تصویر |  
| **جمّا ۳n** | E2B (۵ میلیارد)، E4B (۸ میلیارد) | متغیر | بهینه‌سازی موبایل، پردازش صوتی | اپلیکیشن‌های موبایل، محاسبات لبه، هوش مصنوعی در زمان واقعی |  
| **جمّا ۲.۵** | ۰.۵ میلیارد-۷۲ میلیارد | ۳۲ هزار-۱۲۸ هزار | عملکرد متعادل، چندزبانه | استقرار تولید، جریان‌های کاری موجود |  
| **جمّا-VL** | متنوع | متغیر | تخصص در زبان-تصویر | تحلیل تصویر، پاسخ به سوالات بصری |  

## راهنمای انتخاب مدل  

### برای کاربردهای پایه  
- **جمّا ۳-۱ میلیارد**: وظایف متنی سبک، اپلیکیشن‌های موبایل ساده  
- **جمّا ۳-۴ میلیارد**: عملکرد متعادل با پشتیبانی چندوجهی برای استفاده عمومی  

### برای کاربردهای چندوجهی  
- **جمّا ۳-۴ میلیارد/۱۲ میلیارد**: درک تصویر، پاسخ به سوالات بصری  
- **جمّا ۳n**: اپلیکیشن‌های چندوجهی موبایل با قابلیت‌های پردازش صوتی  

### برای استقرار موبایل و لبه  
- **جمّا ۳n E2B**: دستگاه‌های محدود به منابع، هوش مصنوعی موبایل در زمان واقعی  
- **جمّا ۳n E4B**: عملکرد موبایل پیشرفته با قابلیت‌های صوتی  

### برای استقرار سازمانی  
- **جمّا ۳-۱۲ میلیارد/۲۷ میلیارد**: درک زبان و تصویر با عملکرد بالا  
- **قابلیت‌های فراخوانی توابع**: ساخت سیستم‌های اتوماسیون هوشمند  

### برای کاربردهای جهانی  
- **هر نوع جمّا ۳**: پشتیبانی از بیش از ۱۴۰ زبان با درک فرهنگی  
- **جمّا ۳n**: اپلیکیشن‌های جهانی موبایل با ترجمه صوتی  

## پلتفرم‌های استقرار و دسترسی  

### پلتفرم‌های ابری  
- **Vertex AI**: قابلیت‌های MLOps انتها به انتها با تجربه بدون سرور  
- **Google Kubernetes Engine (GKE)**: استقرار کانتینر مقیاس‌پذیر برای بارهای کاری پیچیده  
- **Google GenAI API**: دسترسی مستقیم به API برای نمونه‌سازی سریع  
- **NVIDIA API Catalog**: عملکرد بهینه‌شده بر روی GPUهای NVIDIA  

### چارچوب‌های توسعه محلی  
- **Hugging Face Transformers**: یکپارچه‌سازی استاندارد برای توسعه  
- **Ollama**: استقرار و مدیریت محلی ساده  
- **vLLM**: سرویس‌دهی با عملکرد بالا برای تولید  
- **Gemma.cpp**: اجرای بهینه‌شده برای CPU  
- **Google AI Edge**: بهینه‌سازی استقرار موبایل و لبه  

### منابع آموزشی  
- **Google AI Studio**: مدل‌های جمّا را تنها با چند کلیک امتحان کنید  
- **Kaggle و Hugging Face**: دانلود وزن‌های مدل و مثال‌های جامعه  
- **گزارش‌های فنی**: مستندات جامع و مقالات تحقیقاتی  
- **فروم‌های جامعه**: پشتیبانی فعال جامعه و بحث‌ها  

### شروع کار با مدل‌های جمّا  

#### پلتفرم‌های توسعه  
1. **Google AI Studio**: شروع با آزمایش مبتنی بر وب  
2. **Hugging Face Hub**: کاوش مدل‌ها و پیاده‌سازی‌های جامعه  
3. **استقرار محلی**: استفاده از Ollama یا Transformers برای توسعه  

#### مسیر یادگیری  
1. **درک مفاهیم اصلی**: مطالعه قابلیت‌های چندوجهی و گزینه‌های استقرار  
2. **آزمایش با انواع مختلف**: امتحان اندازه‌های مختلف مدل و نسخه‌های تخصصی  
3. **تمرین پیاده‌سازی**: استقرار مدل‌ها در محیط‌های توسعه  
4. **بهینه‌سازی برای تولید**: تنظیم دقیق برای موارد استفاده و پلتفرم‌های خاص  

#### بهترین روش‌ها  
- **کوچک شروع کنید**: با جمّا ۳-۴ میلیارد برای توسعه و آزمایش اولیه شروع کنید  
- **از قالب‌های رسمی استفاده کنید**: قالب‌های چت مناسب را برای نتایج بهینه اعمال کنید  
- **منابع را نظارت کنید**: استفاده از حافظه و عملکرد استنتاج را پیگیری کنید  
- **تخصص را در نظر بگیرید**: انواع مناسب را برای نیازهای چندوجهی یا موبایل انتخاب کنید  

## الگوهای استفاده پیشرفته  

### مثال‌های تنظیم دقیق  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### مهندسی پرامپت تخصصی  

**برای وظایف چندوجهی:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**برای فراخوانی توابع با زمینه:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### کاربردهای چندزبانه با زمینه فرهنگی  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### الگوهای استقرار تولید  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## استراتژی‌های بهینه‌سازی عملکرد  

### بهینه‌سازی حافظه  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### بهینه‌سازی استنتاج  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## بهترین روش‌ها و دستورالعمل‌ها  

### امنیت و حریم خصوصی  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### نظارت و ارزیابی  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## نتیجه‌گیری  

خانواده مدل جمّا نمایانگر رویکرد جامع گوگل برای دموکراتیزه کردن فناوری هوش مصنوعی است، در حالی که عملکرد رقابتی را در کاربردها و سناریوهای استقرار متنوع حفظ می‌کند. با تعهد به دسترسی منبع باز، قابلیت‌های چندوجهی، و طراحی‌های معماری نوآورانه، جمّا به سازمان‌ها و توسعه‌دهندگان این امکان را می‌دهد که از قابلیت‌های قدرتمند هوش مصنوعی بهره‌مند شوند، صرف‌نظر از منابع یا نیازهای خاص آن‌ها.  

### نکات کلیدی  

**برتری منبع باز**: جمّا نشان می‌دهد که مدل‌های منبع باز می‌توانند عملکردی رقابتی با جایگزین‌های اختصاصی داشته باشند، در حالی که شفافیت، سفارشی‌سازی، و کنترل بر استقرار هوش مصنوعی را ارائه می‌دهند.  

**نوآوری چندوجهی**: ادغام قابلیت‌های متن، تصویر، و صوت در جمّا ۳ و جمّا ۳n نمایانگر پیشرفت قابل توجهی در هوش مصنوعی چندوجهی قابل دسترس است، که درک جامع از انواع مختلف ورودی را امکان‌پذیر می‌کند.  

**معماری موبایل‌محور**: فناوری پیشرفته Per-Layer Embeddings (PLE) و بهینه‌سازی موبایل جمّا ۳n نشان می‌دهد که هوش مصنوعی قدرتمند می‌تواند به طور کارآمد بر روی دستگاه‌های محدود به منابع عمل کند، بدون قربانی کردن قابلیت‌ها.  

**استقرار مقیاس‌پذیر**: محدوده از ۱ میلیارد تا ۲۷ میلیارد پارامتر، با انواع تخصصی موبایل، امکان استقرار در طیف کامل محیط‌های محاسباتی را فراهم می‌کند، در حالی که کیفیت و عملکرد سازگار را حفظ می‌کند.  

**یکپارچه‌سازی مسئولانه هوش مصنوعی**: اقدامات ایمنی داخلی از طریق ShieldGemma 2 و شیوه‌های توسعه مسئولانه تضمین می‌کند که قابلیت‌های قدرتمند هوش مصنوعی می‌توانند به طور ایمن و اخلاقی مستقر شوند.  

### چشم‌انداز آینده  

با ادامه تکامل خانواده جمّا، می‌توان انتظار داشت:  

**قابلیت‌های موبایل پیشرفته**: بهینه‌سازی بیشتر برای استقرار موبایل و لبه با ادغام معماری جمّا ۳n در پلتفرم‌های اصلی مانند Android و Chrome.  

**درک چندوجهی گسترش‌یافته**: پیشرفت مداوم در ادغام زبان-تصویر-صوت برای تجربیات جامع‌تر هوش مصنوعی.  

**کارایی بهبود‌یافته**: نوآوری‌های معماری مداوم برای ارائه نسبت‌های عملکرد-پارامتر بهتر و کاهش نیازهای محاسباتی.  

**یکپارچه‌سازی گسترده‌تر اکوسیستم**: پشتیبانی پیشرفته در چارچوب‌های توسعه، پلتفرم‌های ابری، و ابزارهای استقرار برای یکپارچه‌سازی بدون درز در جریان‌های کاری موجود.  

**رشد جامعه**: گسترش مداوم Gemmaverse با مدل‌ها، ابزارها، و اپلیکیشن‌های ایجاد شده توسط جامعه که قابلیت‌های اصلی را گسترش می‌دهند.  

### مراحل بعدی  

چه در حال ساخت اپلیکیشن‌های موبایل با قابلیت‌های هوش مصنوعی در زمان واقعی باشید، چه در حال توسعه ابزارهای آموزشی چندوجهی، ایجاد سیستم‌های اتوماسیون هوشمند، یا کار بر روی اپلیکیشن‌های جهانی که نیاز به پشتیبانی چندزبانه دارند، خانواده جمّا راه‌حل‌های مقیاس‌پذیر با پشتیبانی قوی جامعه و مستندات جامع ارائه می‌دهد.  

**توصیه‌های شروع کار:**  
1. **با Google AI Studio آزمایش کنید** برای تجربه عملی فوری  
2. **مدل‌ها را از Hugging Face دانلود کنید** برای توسعه محلی و سفارشی‌سازی  
3. **انواع تخصصی را کاوش کنید** مانند جمّا ۳n برای اپلیکیشن‌های موبایل  
4. **قابلیت‌های چندوجهی را پیاده‌سازی کنید** برای تجربیات جامع هوش مصنوعی  
5. **بهترین روش‌های امنیتی را دنبال کنید** برای استقرار تولید  

**برای توسعه موبایل**: با جمّا ۳n E2B برای استقرار کارآمد منابع با قابلیت‌های صوتی و تصویری شروع کنید.  

**برای اپلیکیشن‌های سازمانی**: مدل‌های جمّا ۳-۱۲ میلیارد یا ۲۷ میلیارد را برای حداکثر قابلیت با فراخوانی توابع و استدلال پیشرفته در نظر بگیرید.  

**برای اپلیکیشن‌های جهانی**: از پشتیبانی بیش از ۱۴۰ زبان جمّا با مهندسی پرامپت آگاه به فرهنگ بهره‌مند شوید.  

**برای موارد استفاده تخصصی**: رویکردهای تنظیم دقیق و تکنیک‌های بهینه‌سازی دامنه‌محور را کاوش کنید.  

### 🔮 دموکراتیزه کردن هوش مصنوعی  

خانواده جمّا نمونه‌ای از آینده توسعه هوش مصنوعی است، جایی که مدل‌های قدرتمند و توانمند برای همه، از توسعه‌دهندگان فردی تا شرکت‌های بزرگ، قابل دسترس هستند. با ترکیب تحقیقات پیشرفته با دسترسی منبع باز، گوگل پایه‌ای ایجاد کرده است که نوآوری را در تمام بخش‌ها و مقیاس‌ها امکان‌پذیر می‌کند.  

موفقیت جمّا با بیش از ۱۰۰ میلیون دانلود و ۶۰ هزار+ انواع جامعه نشان‌دهنده قدرت همکاری باز در پیشبرد فناوری هوش مصنوعی است. با حرکت به جلو، خانواده جمّا همچنان به عنوان کاتالیزوری برای نوآوری هوش مصنوعی عمل خواهد کرد، و امکان توسعه اپلیکیشن‌هایی را فراهم می‌کند که قبلاً فقط با مدل‌های اختصاصی و گران‌قیمت ممکن بود.  

آینده هوش مصنوعی باز، قابل دسترس، و قدرتمند است – و خانواده جمّا پیشرو در تحقق این چشم‌انداز است.  

## منابع اضافی  

**مستندات رسمی و مدل‌ها:**  
- **Google AI Studio**: [مدل‌های جمّا را مستقیماً امتحان کنید](https://aistudio.google.com)  
- **مجموعه‌های Hugging Face**:  
  - [انتشار جمّا ۳](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [پیش‌نمایش جمّا ۳n](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **مستندات توسعه‌دهنده Google AI**: [راهنماهای جامع جمّا](https://ai.google.dev/gemma)  
- **مستندات Vertex AI**: [راهنماهای استقرار سازمانی](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**منابع فنی:**  
- **مقالات تحقیقاتی و گزارش‌های فنی**: [انتشارات Google DeepMind](https://deepmind.google/models/gemma/)  
- **پست‌های وبلاگ توسعه‌دهنده**: [آخرین اعلامیه‌ها و آموزش‌ها](https://developers.googleblog.com)  
- **کارت‌های مدل**: مشخصات فنی دقیق و معیارهای عملکرد  

**جامعه و پشتیبانی:**  
- **جامعه Hugging Face**: بحث‌های فعال و مثال‌های جامعه  
- **مخازن GitHub**: پیاده‌سازی‌ها و ابزارهای منبع باز  
- **فروم‌های توسعه‌دهنده**: پشتیبانی جامعه توسعه‌دهنده Google AI  
- **Stack Overflow**: سوالات برچسب‌گذاری شده و راه‌حل‌های جامعه  

**ابزارهای توسعه:**  
- **Ollama**: [استقرار محلی ساده](https://ollama.ai)  
- **vLLM**: [سرویس‌دهی با عملکرد بالا](https://github.com/vllm-project/vllm)  
- **کتابخانه Transformers**: [یکپارچه‌سازی Hugging Face](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: بهینه‌سازی استقرار موبایل و لبه  

**مسیرهای یادگیری:**  
- **مبتدی**: شروع با Google AI Studio → مثال‌های Hugging Face → استقرار محلی  
- **توسعه‌دهنده**: یکپارچه‌سازی Transformers → اپلیکیشن‌های سفارشی → استقرار تولید  
- **محقق**: مقالات فنی → تنظیم دقیق → اپلیکیشن‌های جدید  
- **سازمانی**: استقرار Vertex AI → پیاده‌سازی امنیت → بهینه‌سازی مقیاس  

خانواده مدل جمّا نه تنها مجموعه‌ای از مدل‌های هوش مصنوعی است، بلکه یک اکوسیستم کامل برای ساخت آینده اپلیکیشن‌های هوش مصنوعی قابل دسترس، قدرتمند، و مسئولانه است. امروز شروع به کاوش کنید و به جامعه در حال رشد توسعه‌دهندگان و محققانی بپیوندید که مرزهای ممکن را با هوش مصنوعی منبع باز پیش می‌برند.  

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.