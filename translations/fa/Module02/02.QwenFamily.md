<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:24:49+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "fa"
}
-->
# بخش ۲: اصول خانواده مدل‌های Qwen

خانواده مدل‌های Qwen نمایانگر رویکرد جامع Alibaba Cloud به مدل‌های زبان بزرگ و هوش مصنوعی چندرسانه‌ای است. این مدل‌ها نشان می‌دهند که مدل‌های متن‌باز می‌توانند عملکرد چشمگیری داشته باشند و در عین حال در سناریوهای مختلف قابل اجرا باشند. درک نحوه‌ای که خانواده Qwen قابلیت‌های قدرتمند هوش مصنوعی را با گزینه‌های انعطاف‌پذیر اجرا فراهم می‌کند، در حالی که عملکرد رقابتی در وظایف متنوع حفظ می‌شود، بسیار مهم است.

## منابع برای توسعه‌دهندگان

### مخزن مدل در Hugging Face
مدل‌های منتخب خانواده Qwen از طریق [Hugging Face](https://huggingface.co/models?search=qwen) در دسترس هستند و امکان دسترسی به برخی از انواع این مدل‌ها را فراهم می‌کنند. شما می‌توانید انواع موجود را بررسی کنید، آن‌ها را برای موارد استفاده خاص خود تنظیم کنید و از طریق چارچوب‌های مختلف اجرا کنید.

### ابزارهای توسعه محلی
برای توسعه و آزمایش محلی، می‌توانید از [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) استفاده کنید تا مدل‌های Qwen موجود را با عملکرد بهینه روی دستگاه توسعه خود اجرا کنید.

### منابع مستندات
- [مستندات مدل Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [بهینه‌سازی مدل‌های Qwen برای اجرا در Edge](https://github.com/microsoft/olive)

## مقدمه

در این آموزش، خانواده مدل‌های Qwen از Alibaba و مفاهیم اساسی آن را بررسی خواهیم کرد. ما به تکامل خانواده Qwen، روش‌های نوآورانه آموزش که مدل‌های Qwen را مؤثر می‌سازند، انواع کلیدی در این خانواده و کاربردهای عملی در سناریوهای مختلف خواهیم پرداخت.

## اهداف یادگیری

در پایان این آموزش، شما قادر خواهید بود:

- فلسفه طراحی و تکامل خانواده مدل‌های Qwen از Alibaba را درک کنید
- نوآوری‌های کلیدی که به مدل‌های Qwen امکان دستیابی به عملکرد بالا در اندازه‌های مختلف پارامتر را می‌دهند، شناسایی کنید
- مزایا و محدودیت‌های انواع مختلف مدل‌های Qwen را بشناسید
- دانش خود از مدل‌های Qwen را برای انتخاب انواع مناسب در سناریوهای واقعی به کار ببرید

## درک چشم‌انداز مدرن مدل‌های هوش مصنوعی

چشم‌انداز هوش مصنوعی به طور قابل توجهی تکامل یافته است، و سازمان‌های مختلف رویکردهای متفاوتی را برای توسعه مدل‌های زبان دنبال می‌کنند. در حالی که برخی بر مدل‌های اختصاصی و بسته تمرکز دارند، دیگران بر دسترسی متن‌باز و شفافیت تأکید می‌کنند. رویکرد سنتی شامل مدل‌های اختصاصی عظیم است که فقط از طریق API‌ها قابل دسترسی هستند یا مدل‌های متن‌باز که ممکن است در قابلیت‌ها عقب‌تر باشند.

این پارادایم چالش‌هایی را برای سازمان‌هایی ایجاد می‌کند که به دنبال قابلیت‌های قدرتمند هوش مصنوعی هستند و در عین حال کنترل بر داده‌ها، هزینه‌ها و انعطاف‌پذیری اجرا را حفظ می‌کنند. رویکرد معمول اغلب نیازمند انتخاب بین عملکرد پیشرفته و ملاحظات عملی اجرا است.

## چالش برتری هوش مصنوعی قابل دسترس

نیاز به هوش مصنوعی با کیفیت بالا و قابل دسترس در سناریوهای مختلف به طور فزاینده‌ای مهم شده است. به کاربردهایی فکر کنید که گزینه‌های اجرای انعطاف‌پذیر برای نیازهای مختلف سازمانی، پیاده‌سازی‌های مقرون‌به‌صرفه که هزینه‌های API می‌تواند قابل توجه باشد، قابلیت‌های چندزبانه برای کاربردهای جهانی، یا تخصص دامنه‌ای در زمینه‌هایی مانند کدنویسی و ریاضیات را نیاز دارند.

### الزامات کلیدی اجرا

اجرای مدرن هوش مصنوعی با چندین الزام اساسی مواجه است که کاربرد عملی را محدود می‌کند:

- **دسترسی‌پذیری**: دسترسی متن‌باز برای شفافیت و سفارشی‌سازی
- **مقرون‌به‌صرفه بودن**: نیازهای محاسباتی معقول برای بودجه‌های مختلف
- **انعطاف‌پذیری**: اندازه‌های مختلف مدل برای سناریوهای اجرای متفاوت
- **دسترسی جهانی**: قابلیت‌های چندزبانه و بین‌فرهنگی قوی
- **تخصص**: انواع خاص دامنه برای موارد استفاده خاص

## فلسفه مدل Qwen

خانواده مدل‌های Qwen نمایانگر رویکرد جامعی به توسعه مدل‌های هوش مصنوعی است که دسترسی متن‌باز، قابلیت‌های چندزبانه و اجرای عملی را در اولویت قرار می‌دهد و در عین حال ویژگی‌های عملکرد رقابتی را حفظ می‌کند. مدل‌های Qwen این اهداف را از طریق اندازه‌های متنوع مدل، روش‌های آموزش با کیفیت بالا و انواع خاص دامنه برای حوزه‌های مختلف به دست می‌آورند.

خانواده Qwen شامل رویکردهای مختلفی است که گزینه‌هایی را در طیف عملکرد-کارایی فراهم می‌کند، و امکان اجرا از دستگاه‌های موبایل تا سرورهای سازمانی را فراهم می‌کند و در عین حال قابلیت‌های معنادار هوش مصنوعی را ارائه می‌دهد. هدف این است که دسترسی به هوش مصنوعی با کیفیت بالا را دموکراتیک کنیم و در عین حال انعطاف‌پذیری در انتخاب‌های اجرا را فراهم کنیم.

### اصول طراحی اصلی Qwen

مدل‌های Qwen بر اساس چندین اصل بنیادی ساخته شده‌اند که آن‌ها را از خانواده‌های دیگر مدل‌های زبان متمایز می‌کند:

- **اولویت متن‌باز**: شفافیت کامل و دسترسی برای استفاده تحقیقاتی و تجاری
- **آموزش جامع**: آموزش بر روی مجموعه داده‌های عظیم و متنوع که شامل چندین زبان و دامنه است
- **معماری مقیاس‌پذیر**: اندازه‌های مختلف مدل برای تطابق با نیازهای محاسباتی متفاوت
- **تخصص بهینه‌شده**: انواع خاص دامنه که برای وظایف خاص بهینه شده‌اند

## فناوری‌های کلیدی که خانواده Qwen را ممکن می‌سازند

### آموزش در مقیاس عظیم

یکی از جنبه‌های تعریف‌کننده خانواده Qwen مقیاس عظیم داده‌های آموزشی و منابع محاسباتی است که در توسعه مدل سرمایه‌گذاری شده‌اند. مدل‌های Qwen از مجموعه داده‌های چندزبانه با دقت انتخاب‌شده که شامل تریلیون‌ها توکن است، استفاده می‌کنند و طراحی شده‌اند تا دانش جامع جهانی و قابلیت‌های استدلالی را فراهم کنند.

این رویکرد با ترکیب محتوای وب با کیفیت بالا، ادبیات علمی، مخازن کد و منابع چندزبانه کار می‌کند. روش آموزش بر عمق دانش و عمق درک در دامنه‌ها و زبان‌های مختلف تأکید دارد.

### استدلال و تفکر پیشرفته

مدل‌های اخیر Qwen قابلیت‌های استدلال پیچیده‌ای را شامل می‌شوند که امکان حل مسائل چندمرحله‌ای پیچیده را فراهم می‌کند:

**حالت تفکر (Qwen3)**: مدل‌ها می‌توانند قبل از ارائه پاسخ نهایی، درگیر استدلال مرحله‌به‌مرحله شوند، مشابه رویکردهای حل مسئله انسانی.

**عملکرد دو حالته**: توانایی تغییر بین حالت پاسخ سریع برای پرسش‌های ساده و حالت تفکر عمیق برای مسائل پیچیده.

**ادغام زنجیره‌ای از تفکر**: ترکیب طبیعی مراحل استدلال که شفافیت و دقت را در وظایف پیچیده بهبود می‌بخشد.

### نوآوری‌های معماری

خانواده Qwen شامل چندین بهینه‌سازی معماری است که برای عملکرد و کارایی طراحی شده‌اند:

**طراحی مقیاس‌پذیر**: معماری سازگار در اندازه‌های مختلف مدل که امکان مقیاس‌پذیری و مقایسه آسان را فراهم می‌کند.

**ادغام چندرسانه‌ای**: ادغام بی‌نقص قابلیت‌های پردازش متن، تصویر و صوت در معماری‌های یکپارچه.

**بهینه‌سازی اجرا**: گزینه‌های مختلف کوانتیزاسیون و فرمت‌های اجرا برای پیکربندی‌های سخت‌افزاری مختلف.

## اندازه مدل و گزینه‌های اجرا

محیط‌های اجرای مدرن از انعطاف‌پذیری مدل‌های Qwen در نیازهای محاسباتی مختلف بهره‌مند می‌شوند:

### مدل‌های کوچک (۰.۵B-۳B)

Qwen مدل‌های کوچک کارآمدی را ارائه می‌دهد که برای اجرا در Edge، برنامه‌های موبایل و محیط‌های محدود منابع مناسب هستند و در عین حال قابلیت‌های چشمگیری را حفظ می‌کنند.

### مدل‌های متوسط (۷B-۳۲B)

مدل‌های میان‌رده قابلیت‌های پیشرفته‌ای را برای کاربردهای حرفه‌ای ارائه می‌دهند و تعادل عالی بین عملکرد و نیازهای محاسباتی فراهم می‌کنند.

### مدل‌های بزرگ (۷۲B+)

مدل‌های تمام‌مقیاس عملکرد پیشرفته‌ای را برای کاربردهای پرتقاضا، تحقیقات و اجراهای سازمانی که به حداکثر قابلیت نیاز دارند، ارائه می‌دهند.

## مزایای خانواده مدل‌های Qwen

### دسترسی متن‌باز

مدل‌های Qwen شفافیت کامل و قابلیت‌های سفارشی‌سازی را فراهم می‌کنند و به سازمان‌ها امکان می‌دهند مدل‌ها را برای نیازهای خاص خود درک، تغییر و تطبیق دهند بدون قفل شدن به یک فروشنده.

### انعطاف‌پذیری اجرا

محدوده اندازه‌های مدل امکان اجرا در پیکربندی‌های سخت‌افزاری متنوع، از دستگاه‌های موبایل تا سرورهای پیشرفته، را فراهم می‌کند و به سازمان‌ها انعطاف‌پذیری در انتخاب زیرساخت هوش مصنوعی خود می‌دهد.

### برتری چندزبانه

مدل‌های Qwen در درک و تولید چندزبانه عالی هستند و از ده‌ها زبان با قدرت خاص در انگلیسی و چینی پشتیبانی می‌کنند، که آن‌ها را برای کاربردهای جهانی مناسب می‌سازد.

### عملکرد رقابتی

مدل‌های Qwen به طور مداوم نتایج رقابتی در معیارها به دست می‌آورند و در عین حال دسترسی متن‌باز را فراهم می‌کنند، نشان می‌دهند که مدل‌های متن‌باز می‌توانند با جایگزین‌های اختصاصی رقابت کنند.

### قابلیت‌های تخصصی

انواع خاص دامنه مانند Qwen-Coder و Qwen-Math تخصص ویژه‌ای را ارائه می‌دهند و در عین حال قابلیت‌های عمومی درک زبان را حفظ می‌کنند.

## مثال‌ها و موارد استفاده عملی

قبل از ورود به جزئیات فنی، بیایید برخی از مثال‌های ملموس از آنچه مدل‌های Qwen می‌توانند انجام دهند را بررسی کنیم:

### مثال استدلال ریاضی

Qwen-Math در حل مسائل ریاضی مرحله‌به‌مرحله عالی عمل می‌کند. برای مثال، وقتی از آن خواسته می‌شود یک مسئله پیچیده حساب دیفرانسیل و انتگرال را حل کند:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### مثال پشتیبانی چندزبانه

مدل‌های Qwen قابلیت‌های چندزبانه قوی را در زبان‌های مختلف نشان می‌دهند:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### مثال قابلیت‌های چندرسانه‌ای

Qwen-VL می‌تواند متن و تصاویر را به طور همزمان پردازش کند:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### مثال تولید کد

Qwen-Coder در تولید و توضیح کد در زبان‌های برنامه‌نویسی مختلف عالی عمل می‌کند:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

این پیاده‌سازی از بهترین شیوه‌ها با نام‌های متغیر واضح، مستندات جامع و منطق کارآمد پیروی می‌کند.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# مثال اجرا در دستگاه موبایل با کوانتیزاسیون
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# بارگذاری مدل کوانتیزه‌شده برای اجرا در موبایل

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## تکامل خانواده Qwen

### Qwen 1.0 و 1.5: مدل‌های پایه

مدل‌های اولیه Qwen اصول بنیادی آموزش جامع و دسترسی متن‌باز را ایجاد کردند:

- **Qwen-7B (۷B پارامتر)**: انتشار اولیه با تمرکز بر درک زبان چینی و انگلیسی
- **Qwen-14B (۱۴B پارامتر)**: قابلیت‌های بهبود‌یافته با استدلال و دانش بهتر
- **Qwen-72B (۷۲B پارامتر)**: مدل بزرگ‌مقیاس که عملکرد پیشرفته‌ای ارائه می‌دهد
- **سری Qwen1.5**: گسترش به اندازه‌های مختلف (۰.۵B تا ۱۱۰B) با بهبود در مدیریت زمینه طولانی

### خانواده Qwen2: گسترش چندرسانه‌ای

سری Qwen2 پیشرفت قابل توجهی در قابلیت‌های زبان و چندرسانه‌ای ایجاد کرد:

- **Qwen2-0.5B تا ۷۲B**: محدوده جامع مدل‌های زبان برای نیازهای مختلف اجرا
- **Qwen2-57B-A14B (MoE)**: معماری ترکیب متخصصان برای استفاده کارآمد از پارامترها
- **Qwen2-VL**: قابلیت‌های پیشرفته زبان-تصویر برای درک تصویر
- **Qwen2-Audio**: پردازش و درک صوت
- **Qwen2-Math**: استدلال ریاضی تخصصی و حل مسئله

### خانواده Qwen2.5: عملکرد بهبود‌یافته

سری Qwen2.5 بهبود‌های قابل توجهی در تمام ابعاد ایجاد کرد:

- **آموزش گسترش‌یافته**: ۱۸ تریلیون توکن داده آموزشی برای قابلیت‌های بهبود‌یافته
- **زمینه گسترش‌یافته**: طول زمینه تا ۱۲۸K توکن، با نسخه Turbo که از ۱M توکن پشتیبانی می‌کند
- **تخصص بهبود‌یافته**: مدل‌های بهبود‌یافته Qwen2.5-Coder و Qwen2.5-Math
- **پشتیبانی چندزبانه بهتر**: عملکرد بهبود‌یافته در بیش از ۲۷ زبان

### خانواده Qwen3: استدلال پیشرفته

نسل جدید مرزهای استدلال و قابلیت‌های تفکر را گسترش می‌دهد:

- **Qwen3-235B-A22B**: مدل ترکیب متخصصان پرچم‌دار با ۲۳۵B پارامتر کل
- **Qwen3-30B-A3B**: مدل MoE کارآمد با عملکرد قوی در هر پارامتر فعال
- **مدل‌های متراکم**: Qwen3-32B، ۱۴B، ۸B، ۴B، ۱.۷B، ۰.۶B برای سناریوهای مختلف اجرا
- **حالت تفکر**: رویکرد استدلال ترکیبی که از پاسخ‌های سریع و تفکر عمیق پشتیبانی می‌کند
- **برتری چندزبانه**: پشتیبانی از ۱۱۹ زبان و گویش
- **آموزش بهبود‌یافته**: ۳۶ تریلیون توکن داده آموزشی متنوع و با کیفیت بالا

## کاربردهای مدل‌های Qwen

### کاربردهای سازمانی

سازمان‌ها از مدل‌های Qwen برای تحلیل اسناد، خودکارسازی خدمات مشتری، کمک به تولید کد و کاربردهای هوش تجاری استفاده می‌کنند. ماهیت متن‌باز امکان سفارشی‌سازی برای نیازهای خاص کسب‌وکار را فراهم می‌کند و در عین حال حریم خصوصی داده‌ها و کنترل را حفظ می‌کند.

### محاسبات موبایل و Edge

برنامه‌های موبایل از مدل‌های Qwen برای ترجمه بلادرنگ، دستیارهای هوشمند، تولید محتوا و توصیه‌های شخصی‌سازی‌شده استفاده می‌کنند. محدوده اندازه‌های مدل امکان اجرا از دستگاه‌های موبایل تا سرورهای Edge را فراهم می‌کند.

### فناوری آموزشی

پلتفرم‌های آموزشی از مدل‌های Qwen برای تدریس شخصی‌سازی‌شده، تولید خودکار محتوا، کمک به یادگیری زبان و تجربیات آموزشی تعاملی استفاده می‌کنند. مدل‌های تخصصی مانند Qwen-Math تخصص دامنه‌ای ارائه می‌دهند.

### کاربردهای جهانی

کاربردهای بین‌المللی از قابلیت‌های چندزبانه قوی مدل‌های Qwen بهره‌مند می‌شوند و تجربه‌های هوش مصنوعی سازگار در زبان‌ها و زمینه‌های فرهنگی مختلف را امکان‌پذیر می‌کنند.

## چالش‌ها و محدودیت‌ها

### نیازهای محاسباتی

در حالی که Qwen مدل‌هایی در اندازه‌های مختلف ارائه می‌دهد، انواع بزرگ‌تر هنوز به منابع محاسباتی قابل توجهی برای عملکرد بهینه نیاز دارند، که ممکن است گزینه‌های اجرا را برای برخی سازمان‌ها محدود کند.

### عملکرد دامنه تخصصی

در حالی که مدل‌های Qwen در دامنه‌های عمومی عملکرد خوبی دارند، کاربردهای بسیار تخصصی ممکن است از تنظیم دقیق دامنه یا مدل‌های تخصصی بهره‌مند شوند.

### پیچیدگی انتخاب مدل

محدوده گسترده مدل‌ها و انواع موجود می‌تواند انتخاب را برای کاربران جدید در این اکوسیستم چالش‌برانگیز کند.

### عدم تعادل زبان

در حالی که از زبان‌های زیادی پشتیبانی می‌شود، عملکرد ممکن است در زبان‌های مختلف متفاوت باشد، با قابلیت‌های قوی‌تر در انگلیسی و چینی.

## آینده خانواده مدل‌های Qwen

خانواده مدل‌های Qwen نمایانگر تکامل مداوم به سمت هوش مصنوعی دموکراتیک و با کیفیت بالا است. توسعه‌های آینده شامل بهینه‌سازی‌های کارایی پیشرفته، قابلیت‌های چندرسانه‌ای گسترش‌یافته، مکانیزم‌های استدلال بهبود‌یافته و ادغام بهتر در سناریوهای مختلف اجرا خواهد بود.

با ادامه تکامل فناوری، انتظار می‌رود مدل‌های Qwen به طور فزاینده‌ای توانمند شوند و در عین حال دسترسی متن‌باز خود را حفظ کنند، و امکان اجرای هوش مصنوعی در سناریوها و موارد استفاده متنوع را فراهم کنند.

خانواده Qwen نشان می‌دهد که آینده توسعه هوش مصنوعی می‌تواند هم عملکرد پیشرفته و هم دسترسی متن‌باز را در بر بگیرد، و ابزارهای قدرتمندی را برای سازمان‌ها فراهم کند و در عین حال شفافیت و کنترل را حفظ کند.

## مثال‌های توسعه و ادغام

### شروع سریع با Transformers

در اینجا نحوه شروع با مدل‌های Qwen با استفاده از کتابخانه Transformers از Hugging Face آمده است:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

###
- Qwen3-235B-A22B در ارزیابی‌های معیارهای کدنویسی، ریاضی و قابلیت‌های عمومی، نتایج رقابتی در مقایسه با مدل‌های برتر دیگر مانند DeepSeek-R1، o1، o3-mini، Grok-3 و Gemini-2.5-Pro به دست آورده است.
- Qwen3-30B-A3B عملکرد بهتری نسبت به QwQ-32B با 10 برابر پارامترهای فعال دارد.
- Qwen3-4B می‌تواند با عملکرد Qwen2.5-72B-Instruct رقابت کند.

**دستاوردهای بهره‌وری:**
- مدل‌های پایه Qwen3-MoE عملکرد مشابهی با مدل‌های پایه متراکم Qwen2.5 دارند، در حالی که تنها از 10٪ پارامترهای فعال استفاده می‌کنند.
- صرفه‌جویی قابل توجه در هزینه‌های آموزش و استنتاج در مقایسه با مدل‌های متراکم.

**قابلیت‌های چندزبانه:**
- مدل‌های Qwen3 از 119 زبان و گویش پشتیبانی می‌کنند.
- عملکرد قوی در زمینه‌های زبانی و فرهنگی متنوع.

**مقیاس آموزش:**
- Qwen3 تقریباً دو برابر Qwen2.5 داده استفاده می‌کند، با حدود 36 تریلیون توکن که 119 زبان و گویش را پوشش می‌دهد، در مقایسه با 18 تریلیون توکن Qwen2.5.

### ماتریس مقایسه مدل‌ها

| سری مدل‌ها | محدوده پارامترها | طول کانتکست | نقاط قوت کلیدی | بهترین موارد استفاده |
|------------|------------------|--------------|-----------------|-----------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | عملکرد متعادل، چندزبانه | کاربردهای عمومی، استقرار در تولید |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | تولید کد، برنامه‌نویسی | توسعه نرم‌افزار، کمک به کدنویسی |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | استدلال ریاضی | پلتفرم‌های آموزشی، کاربردهای STEM |
| **Qwen2.5-VL** | متنوع | متغیر | درک زبان-تصویر | کاربردهای چندوجهی، تحلیل تصاویر |
| **Qwen3** | 0.6B-235B | متغیر | استدلال پیشرفته، حالت تفکر | استدلال پیچیده، کاربردهای تحقیقاتی |
| **Qwen3 MoE** | مجموعاً 30B-235B | متغیر | عملکرد کارآمد در مقیاس بزرگ | کاربردهای سازمانی، نیازهای با عملکرد بالا |

## راهنمای انتخاب مدل

### برای کاربردهای پایه
- **Qwen2.5-0.5B/1.5B**: اپلیکیشن‌های موبایل، دستگاه‌های لبه‌ای، کاربردهای بلادرنگ
- **Qwen2.5-3B/7B**: چت‌بات‌های عمومی، تولید محتوا، سیستم‌های پرسش و پاسخ

### برای وظایف ریاضی و استدلال
- **Qwen2.5-Math**: حل مسائل ریاضی و آموزش STEM
- **Qwen3 با حالت تفکر**: استدلال پیچیده که نیاز به تحلیل مرحله به مرحله دارد

### برای برنامه‌نویسی و توسعه
- **Qwen2.5-Coder**: تولید کد، اشکال‌زدایی، کمک به برنامه‌نویسی
- **Qwen3**: وظایف پیشرفته برنامه‌نویسی با قابلیت‌های استدلال

### برای کاربردهای چندوجهی
- **Qwen2.5-VL**: درک تصویر، پاسخ به سوالات بصری
- **Qwen-Audio**: پردازش صوت و درک گفتار

### برای استقرار سازمانی
- **Qwen2.5-32B/72B**: درک زبان با عملکرد بالا
- **Qwen3-235B-A22B**: حداکثر قابلیت برای کاربردهای پرتقاضا

## پلتفرم‌های استقرار و دسترسی
### پلتفرم‌های ابری
- **Hugging Face Hub**: مخزن جامع مدل با پشتیبانی جامعه
- **ModelScope**: پلتفرم مدل علی‌بابا با ابزارهای بهینه‌سازی
- **ارائه‌دهندگان ابری مختلف**: پشتیبانی از طریق پلتفرم‌های استاندارد ML

### چارچوب‌های توسعه محلی
- **Transformers**: یکپارچگی استاندارد Hugging Face برای استقرار آسان
- **vLLM**: سرویس‌دهی با عملکرد بالا برای محیط‌های تولید
- **Ollama**: استقرار و مدیریت محلی ساده‌شده
- **ONNX Runtime**: بهینه‌سازی چندپلتفرمی برای سخت‌افزارهای مختلف
- **llama.cpp**: پیاده‌سازی کارآمد C++ برای پلتفرم‌های متنوع

### منابع آموزشی
- **مستندات Qwen**: مستندات رسمی و کارت‌های مدل
- **Hugging Face Model Hub**: دموهای تعاملی و مثال‌های جامعه
- **مقالات تحقیقاتی**: مقالات فنی در arxiv برای درک عمیق
- **فروم‌های جامعه**: پشتیبانی فعال جامعه و بحث‌ها

### شروع کار با مدل‌های Qwen

#### پلتفرم‌های توسعه
1. **Hugging Face Transformers**: شروع با یکپارچگی استاندارد پایتون
2. **ModelScope**: ابزارهای استقرار بهینه علی‌بابا را بررسی کنید
3. **استقرار محلی**: از Ollama یا Transformers مستقیم برای آزمایش محلی استفاده کنید

#### مسیر یادگیری
1. **درک مفاهیم اصلی**: معماری خانواده Qwen و قابلیت‌های آن را مطالعه کنید
2. **آزمایش با انواع مختلف**: مدل‌های مختلف را امتحان کنید تا توازن عملکرد را درک کنید
3. **تمرین پیاده‌سازی**: مدل‌ها را در محیط‌های توسعه مستقر کنید
4. **بهینه‌سازی استقرار**: برای موارد استفاده تولیدی تنظیم کنید

#### بهترین روش‌ها
- **کوچک شروع کنید**: با مدل‌های کوچک‌تر (1.5B-7B) برای توسعه اولیه شروع کنید
- **از قالب‌های چت استفاده کنید**: قالب‌بندی مناسب را برای نتایج بهینه اعمال کنید
- **منابع را نظارت کنید**: استفاده از حافظه و سرعت استنتاج را پیگیری کنید
- **تخصص را در نظر بگیرید**: هنگام نیاز، انواع خاص دامنه را انتخاب کنید

## الگوهای استفاده پیشرفته

### مثال‌های تنظیم دقیق

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### مهندسی پرامپت تخصصی

**برای وظایف استدلال پیچیده:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**برای تولید کد با کانتکست:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### کاربردهای چندزبانه

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 الگوهای استقرار تولیدی

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## استراتژی‌های بهینه‌سازی عملکرد

### بهینه‌سازی حافظه

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### بهینه‌سازی استنتاج

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## بهترین روش‌ها و دستورالعمل‌ها

### امنیت و حریم خصوصی

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### نظارت و ارزیابی

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## نتیجه‌گیری

خانواده مدل‌های Qwen نمایانگر رویکردی جامع برای دموکراتیزه کردن فناوری هوش مصنوعی است، در حالی که عملکرد رقابتی در کاربردهای متنوع را حفظ می‌کند. با تعهد به دسترسی منبع باز، قابلیت‌های چندزبانه و گزینه‌های استقرار انعطاف‌پذیر، Qwen به سازمان‌ها و توسعه‌دهندگان امکان می‌دهد تا از قابلیت‌های قدرتمند هوش مصنوعی بهره‌مند شوند، صرف‌نظر از منابع یا نیازهای خاص آن‌ها.

### نکات کلیدی

**برتری منبع باز**: Qwen نشان می‌دهد که مدل‌های منبع باز می‌توانند عملکردی رقابتی با جایگزین‌های اختصاصی داشته باشند، در حالی که شفافیت، سفارشی‌سازی و کنترل را فراهم می‌کنند.

**معماری مقیاس‌پذیر**: محدوده 0.5B تا 235B پارامتر امکان استقرار در طیف کامل محیط‌های محاسباتی، از دستگاه‌های موبایل تا خوشه‌های سازمانی را فراهم می‌کند.

**قابلیت‌های تخصصی**: انواع خاص دامنه مانند Qwen-Coder، Qwen-Math و Qwen-VL تخصص ویژه‌ای را ارائه می‌دهند، در حالی که درک عمومی زبان را حفظ می‌کنند.

**دسترسی جهانی**: پشتیبانی چندزبانه قوی در بیش از 119 زبان، Qwen را برای کاربردهای بین‌المللی و پایگاه‌های کاربری متنوع مناسب می‌سازد.

**نوآوری مداوم**: تکامل از Qwen 1.0 به Qwen3 بهبود مداوم در قابلیت‌ها، بهره‌وری و گزینه‌های استقرار را نشان می‌دهد.

### چشم‌انداز آینده

با ادامه تکامل خانواده Qwen، می‌توان انتظار داشت:

- **بهره‌وری بهبود یافته**: بهینه‌سازی مداوم برای نسبت‌های بهتر عملکرد به پارامتر
- **قابلیت‌های چندوجهی گسترش‌یافته**: ادغام پردازش پیچیده‌تر تصویر، صوت و متن
- **استدلال بهبود یافته**: مکانیزم‌های تفکر پیشرفته و قابلیت‌های حل مسئله چندمرحله‌ای
- **ابزارهای استقرار بهتر**: چارچوب‌ها و ابزارهای بهینه‌سازی پیشرفته برای سناریوهای استقرار متنوع
- **رشد جامعه**: گسترش اکوسیستم ابزارها، کاربردها و مشارکت‌های جامعه

### مراحل بعدی

چه در حال ساخت چت‌بات باشید، چه ابزارهای آموزشی توسعه دهید، چه دستیارهای کدنویسی ایجاد کنید یا روی کاربردهای چندزبانه کار کنید، خانواده Qwen راه‌حل‌های مقیاس‌پذیر با پشتیبانی قوی جامعه و مستندات جامع ارائه می‌دهد.

برای آخرین به‌روزرسانی‌ها، انتشار مدل‌ها و مستندات فنی دقیق، به مخازن رسمی Qwen در Hugging Face مراجعه کنید و بحث‌ها و مثال‌های فعال جامعه را بررسی کنید.

آینده توسعه هوش مصنوعی در ابزارهای قابل دسترس، شفاف و قدرتمند نهفته است که نوآوری را در تمام بخش‌ها و مقیاس‌ها امکان‌پذیر می‌سازد. خانواده Qwen این چشم‌انداز را به نمایش می‌گذارد و به سازمان‌ها و توسعه‌دهندگان پایه‌ای برای ساخت نسل بعدی کاربردهای مبتنی بر هوش مصنوعی ارائه می‌دهد.

## منابع اضافی

- **مستندات رسمی**: [مستندات Qwen](https://qwen.readthedocs.io/)
- **مرکز مدل**: [مجموعه‌های Qwen در Hugging Face](https://huggingface.co/collections/Qwen/)
- **مقالات فنی**: [انتشارات تحقیقاتی Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **جامعه**: [بحث‌ها و مشکلات GitHub](https://github.com/QwenLM/)
- **پلتفرم ModelScope**: [ModelScope علی‌بابا](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## نتایج یادگیری

پس از تکمیل این ماژول، شما قادر خواهید بود:

1. مزایای معماری خانواده مدل‌های Qwen و رویکرد منبع باز آن را توضیح دهید.
2. نوع مناسب Qwen را بر اساس نیازهای کاربردی خاص و محدودیت‌های منابع انتخاب کنید.
3. مدل‌های Qwen را در سناریوهای مختلف استقرار با پیکربندی‌های بهینه پیاده‌سازی کنید.
4. تکنیک‌های کوانتیزاسیون و بهینه‌سازی را برای بهبود عملکرد مدل‌های Qwen اعمال کنید.
5. توازن بین اندازه مدل، عملکرد و قابلیت‌ها را در سراسر خانواده Qwen ارزیابی کنید.

## چه چیزی در ادامه است

- [03: اصول خانواده Gemma](03.GemmaFamily.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.