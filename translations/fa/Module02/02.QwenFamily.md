<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T14:40:52+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "fa"
}
-->
# بخش ۲: اصول خانواده مدل‌های Qwen

خانواده مدل‌های Qwen نمایانگر رویکرد جامع Alibaba Cloud به مدل‌های زبانی بزرگ و هوش مصنوعی چندرسانه‌ای است. این خانواده نشان می‌دهد که مدل‌های متن‌باز می‌توانند عملکرد قابل توجهی داشته باشند و در عین حال در سناریوهای مختلف قابل اجرا باشند. مهم است که درک کنیم چگونه خانواده Qwen قابلیت‌های قدرتمند هوش مصنوعی را با گزینه‌های انعطاف‌پذیر برای اجرا فراهم می‌کند و در عین حال عملکرد رقابتی در وظایف متنوع را حفظ می‌کند.

## منابع برای توسعه‌دهندگان

### مخزن مدل در Hugging Face
مدل‌های منتخب خانواده Qwen از طریق [Hugging Face](https://huggingface.co/models?search=qwen) در دسترس هستند و امکان دسترسی به برخی از نسخه‌های این مدل‌ها را فراهم می‌کنند. شما می‌توانید نسخه‌های موجود را بررسی کنید، آن‌ها را برای موارد استفاده خاص خود تنظیم کنید و از طریق چارچوب‌های مختلف اجرا کنید.

### ابزارهای توسعه محلی
برای توسعه و آزمایش محلی، می‌توانید از [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) استفاده کنید تا مدل‌های Qwen موجود را با عملکرد بهینه روی دستگاه توسعه خود اجرا کنید.

### منابع مستندات
- [مستندات مدل Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [بهینه‌سازی مدل‌های Qwen برای اجرا در Edge](https://github.com/microsoft/olive)

## مقدمه

در این آموزش، ما خانواده مدل‌های Qwen از Alibaba و مفاهیم اساسی آن را بررسی خواهیم کرد. ما به تکامل خانواده Qwen، روش‌های نوآورانه آموزشی که مدل‌های Qwen را مؤثر می‌سازند، نسخه‌های کلیدی در این خانواده و کاربردهای عملی در سناریوهای مختلف خواهیم پرداخت.

## اهداف آموزشی

در پایان این آموزش، شما قادر خواهید بود:

- فلسفه طراحی و تکامل خانواده مدل‌های Qwen از Alibaba را درک کنید
- نوآوری‌های کلیدی که به مدل‌های Qwen امکان دستیابی به عملکرد بالا در اندازه‌های مختلف پارامتر را می‌دهند، شناسایی کنید
- مزایا و محدودیت‌های نسخه‌های مختلف مدل‌های Qwen را بشناسید
- دانش خود از مدل‌های Qwen را برای انتخاب نسخه‌های مناسب در سناریوهای واقعی به کار ببرید

## درک چشم‌انداز مدرن مدل‌های هوش مصنوعی

چشم‌انداز هوش مصنوعی به طور قابل توجهی تکامل یافته است، و سازمان‌های مختلف رویکردهای متفاوتی را برای توسعه مدل‌های زبانی دنبال می‌کنند. در حالی که برخی بر مدل‌های اختصاصی و بسته تمرکز دارند، دیگران بر دسترسی متن‌باز و شفافیت تأکید می‌کنند. رویکرد سنتی شامل مدل‌های اختصاصی عظیم است که فقط از طریق API‌ها قابل دسترسی هستند یا مدل‌های متن‌باز که ممکن است در قابلیت‌ها عقب‌تر باشند.

این پارادایم چالش‌هایی را برای سازمان‌هایی ایجاد می‌کند که به دنبال قابلیت‌های قدرتمند هوش مصنوعی هستند و در عین حال کنترل بر داده‌ها، هزینه‌ها و انعطاف‌پذیری اجرا را حفظ می‌کنند. رویکرد سنتی اغلب نیازمند انتخاب بین عملکرد پیشرفته و ملاحظات عملی اجرا است.

## چالش دستیابی به هوش مصنوعی با کیفیت و قابل دسترس

نیاز به هوش مصنوعی با کیفیت بالا و قابل دسترس در سناریوهای مختلف به طور فزاینده‌ای اهمیت پیدا کرده است. به کاربردهایی فکر کنید که نیازمند گزینه‌های انعطاف‌پذیر اجرا برای نیازهای مختلف سازمانی، پیاده‌سازی‌های مقرون‌به‌صرفه که هزینه‌های API می‌تواند قابل توجه باشد، قابلیت‌های چندزبانه برای کاربردهای جهانی، یا تخصص خاص در حوزه‌هایی مانند کدنویسی و ریاضیات هستند.

### الزامات کلیدی اجرا

پیاده‌سازی‌های مدرن هوش مصنوعی با چندین الزام اساسی مواجه هستند که کاربرد عملی را محدود می‌کنند:

- **دسترسی‌پذیری**: دسترسی متن‌باز برای شفافیت و سفارشی‌سازی
- **مقرون‌به‌صرفه بودن**: نیازهای محاسباتی معقول برای بودجه‌های مختلف
- **انعطاف‌پذیری**: اندازه‌های مختلف مدل برای سناریوهای مختلف اجرا
- **دسترسی جهانی**: قابلیت‌های چندزبانه و بین‌فرهنگی قوی
- **تخصص**: نسخه‌های خاص حوزه برای موارد استفاده خاص

## فلسفه مدل Qwen

خانواده مدل‌های Qwen نمایانگر رویکرد جامعی به توسعه مدل‌های هوش مصنوعی است که دسترسی متن‌باز، قابلیت‌های چندزبانه و اجراهای عملی را در اولویت قرار می‌دهد و در عین حال ویژگی‌های عملکرد رقابتی را حفظ می‌کند. مدل‌های Qwen این اهداف را از طریق اندازه‌های متنوع مدل، روش‌های آموزشی با کیفیت بالا و نسخه‌های تخصصی برای حوزه‌های مختلف به دست می‌آورند.

خانواده Qwen شامل رویکردهای مختلفی است که گزینه‌هایی را در طیف عملکرد-کارایی ارائه می‌دهند، و امکان اجرا از دستگاه‌های موبایل تا سرورهای سازمانی را فراهم می‌کنند و در عین حال قابلیت‌های معنادار هوش مصنوعی را ارائه می‌دهند. هدف این است که دسترسی به هوش مصنوعی با کیفیت بالا را دموکراتیک کرده و گزینه‌های انعطاف‌پذیر در انتخاب اجرا فراهم کند.

### اصول طراحی اصلی Qwen

مدل‌های Qwen بر اساس چندین اصل بنیادی ساخته شده‌اند که آن‌ها را از خانواده‌های دیگر مدل‌های زبانی متمایز می‌کند:

- **اولویت متن‌باز**: شفافیت کامل و دسترسی برای تحقیقات و استفاده تجاری
- **آموزش جامع**: آموزش بر روی مجموعه داده‌های عظیم و متنوع که شامل زبان‌ها و حوزه‌های مختلف است
- **معماری مقیاس‌پذیر**: اندازه‌های مختلف مدل برای تطابق با نیازهای محاسباتی مختلف
- **تخصص عالی**: نسخه‌های خاص حوزه که برای وظایف خاص بهینه شده‌اند

## فناوری‌های کلیدی که خانواده Qwen را ممکن می‌سازند

### آموزش در مقیاس عظیم

یکی از جنبه‌های تعریف‌کننده خانواده Qwen مقیاس عظیم داده‌های آموزشی و منابع محاسباتی است که در توسعه مدل‌ها سرمایه‌گذاری شده است. مدل‌های Qwen از مجموعه داده‌های چندزبانه با دقت انتخاب‌شده که شامل تریلیون‌ها توکن است، استفاده می‌کنند و طراحی شده‌اند تا دانش جامع جهانی و قابلیت‌های استدلالی را ارائه دهند.

این رویکرد با ترکیب محتوای وب با کیفیت بالا، ادبیات علمی، مخازن کد و منابع چندزبانه کار می‌کند. روش آموزشی بر عمق دانش و درک در حوزه‌ها و زبان‌های مختلف تأکید دارد.

### استدلال و تفکر پیشرفته

مدل‌های اخیر Qwen قابلیت‌های استدلال پیچیده‌ای را شامل می‌شوند که امکان حل مسائل چندمرحله‌ای را فراهم می‌کند:

**حالت تفکر (Qwen3)**: مدل‌ها می‌توانند قبل از ارائه پاسخ نهایی، درگیر استدلال مرحله‌به‌مرحله شوند، مشابه رویکردهای حل مسئله انسانی.

**عملکرد دو حالته**: توانایی تغییر بین حالت پاسخ سریع برای پرسش‌های ساده و حالت تفکر عمیق برای مسائل پیچیده.

**ادغام زنجیره‌ای تفکر**: ترکیب طبیعی مراحل استدلال که شفافیت و دقت را در وظایف پیچیده بهبود می‌بخشد.

### نوآوری‌های معماری

خانواده Qwen شامل چندین بهینه‌سازی معماری است که برای عملکرد و کارایی طراحی شده‌اند:

**طراحی مقیاس‌پذیر**: معماری سازگار در اندازه‌های مختلف مدل که امکان مقیاس‌پذیری و مقایسه آسان را فراهم می‌کند.

**ادغام چندرسانه‌ای**: ادغام بی‌وقفه پردازش متن، تصویر و صوت در معماری‌های یکپارچه.

**بهینه‌سازی اجرا**: گزینه‌های مختلف کوانتیزاسیون و فرمت‌های اجرا برای پیکربندی‌های سخت‌افزاری مختلف.

## اندازه مدل و گزینه‌های اجرا

محیط‌های اجرای مدرن از انعطاف‌پذیری مدل‌های Qwen در نیازهای محاسباتی مختلف بهره‌مند می‌شوند:

### مدل‌های کوچک (0.5B-3B)

Qwen مدل‌های کوچک و کارآمدی ارائه می‌دهد که برای اجرا در Edge، برنامه‌های موبایل و محیط‌های محدود منابع مناسب هستند و در عین حال قابلیت‌های چشمگیری را حفظ می‌کنند.

### مدل‌های متوسط (7B-32B)

مدل‌های میان‌رده قابلیت‌های پیشرفته‌ای برای کاربردهای حرفه‌ای ارائه می‌دهند و تعادل عالی بین عملکرد و نیازهای محاسباتی را فراهم می‌کنند.

### مدل‌های بزرگ (72B+)

مدل‌های تمام‌مقیاس عملکرد پیشرفته‌ای برای کاربردهای پرتقاضا، تحقیقات و اجراهای سازمانی که نیازمند حداکثر قابلیت هستند، ارائه می‌دهند.

## مزایای خانواده مدل‌های Qwen

### دسترسی متن‌باز

مدل‌های Qwen شفافیت کامل و قابلیت‌های سفارشی‌سازی را فراهم می‌کنند و به سازمان‌ها امکان می‌دهند مدل‌ها را برای نیازهای خاص خود درک، تغییر و تطبیق دهند بدون وابستگی به فروشنده.

### انعطاف‌پذیری اجرا

محدوده اندازه‌های مدل امکان اجرا در پیکربندی‌های سخت‌افزاری متنوع، از دستگاه‌های موبایل تا سرورهای پیشرفته، را فراهم می‌کند و به سازمان‌ها انعطاف‌پذیری در انتخاب زیرساخت هوش مصنوعی خود می‌دهد.

### برتری چندزبانه

مدل‌های Qwen در درک و تولید چندزبانه عالی هستند و از ده‌ها زبان با قدرت خاص در انگلیسی و چینی پشتیبانی می‌کنند، که آن‌ها را برای کاربردهای جهانی مناسب می‌سازد.

### عملکرد رقابتی

مدل‌های Qwen به طور مداوم نتایج رقابتی در معیارها به دست می‌آورند و در عین حال دسترسی متن‌باز را فراهم می‌کنند، که نشان می‌دهد مدل‌های متن‌باز می‌توانند با جایگزین‌های اختصاصی رقابت کنند.

### قابلیت‌های تخصصی

نسخه‌های خاص حوزه مانند Qwen-Coder و Qwen-Math تخصص ویژه‌ای را ارائه می‌دهند و در عین حال قابلیت‌های عمومی درک زبان را حفظ می‌کنند.

## مثال‌ها و موارد استفاده عملی

قبل از ورود به جزئیات فنی، بیایید برخی از مثال‌های ملموس از توانایی‌های مدل‌های Qwen را بررسی کنیم:

### مثال استدلال ریاضی

Qwen-Math در حل مسائل ریاضی مرحله‌به‌مرحله عالی عمل می‌کند. برای مثال، هنگام حل یک مسئله پیچیده حساب دیفرانسیل و انتگرال:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### مثال پشتیبانی چندزبانه

مدل‌های Qwen قابلیت‌های چندزبانه قوی در زبان‌های مختلف نشان می‌دهند:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### مثال قابلیت‌های چندرسانه‌ای

Qwen-VL می‌تواند متن و تصاویر را به طور همزمان پردازش کند:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### مثال تولید کد

Qwen-Coder در تولید و توضیح کد در زبان‌های برنامه‌نویسی مختلف عالی عمل می‌کند:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```

### مثال اجرا در Edge

مدل‌های Qwen می‌توانند با پیکربندی‌های بهینه روی دستگاه‌های Edge مختلف اجرا شوند:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## تکامل خانواده Qwen

### Qwen 1.0 و 1.5: مدل‌های پایه

مدل‌های اولیه Qwen اصول بنیادی آموزش جامع و دسترسی متن‌باز را پایه‌گذاری کردند:

- **Qwen-7B (7 میلیارد پارامتر)**: انتشار اولیه با تمرکز بر درک زبان چینی و انگلیسی
- **Qwen-14B (14 میلیارد پارامتر)**: قابلیت‌های پیشرفته با استدلال و دانش بهبود یافته
- **Qwen-72B (72 میلیارد پارامتر)**: مدل بزرگ‌مقیاس با عملکرد پیشرفته
- **سری Qwen1.5**: گسترش به اندازه‌های مختلف (0.5B تا 110B) با بهبود در مدیریت زمینه طولانی

### خانواده Qwen2: گسترش چندرسانه‌ای

سری Qwen2 پیشرفت قابل توجهی در قابلیت‌های زبانی و چندرسانه‌ای ایجاد کرد:

- **Qwen2-0.5B تا 72B**: محدوده جامع مدل‌های زبانی برای نیازهای مختلف اجرا
- **Qwen2-57B-A14B (MoE)**: معماری ترکیب متخصصان برای استفاده کارآمد از پارامترها
- **Qwen2-VL**: قابلیت‌های پیشرفته زبان-تصویر برای درک تصاویر
- **Qwen2-Audio**: قابلیت‌های پردازش و درک صوت
- **Qwen2-Math**: استدلال ریاضی تخصصی و حل مسئله

### خانواده Qwen2.5: عملکرد بهبود یافته

سری Qwen2.5 پیشرفت‌های قابل توجهی در تمام ابعاد ایجاد کرد:

- **آموزش گسترش‌یافته**: ۱۸ تریلیون توکن داده آموزشی برای قابلیت‌های بهبود یافته
- **زمینه طولانی‌تر**: طول زمینه تا ۱۲۸ هزار توکن، با نسخه Turbo که از ۱ میلیون توکن پشتیبانی می‌کند
- **تخصص بهبود یافته**: نسخه‌های Qwen2.5-Coder و Qwen2.5-Math بهبود یافته
- **پشتیبانی چندزبانه بهتر**: عملکرد بهبود یافته در بیش از ۲۷ زبان

### خانواده Qwen3: استدلال پیشرفته

نسل جدید مرزهای قابلیت‌های استدلال و تفکر را گسترش می‌دهد:

- **Qwen3-235B-A22B**: مدل ترکیب متخصصان پرچم‌دار با ۲۳۵ میلیارد پارامتر کل
- **Qwen3-30B-A3B**: مدل MoE کارآمد با عملکرد قوی در هر پارامتر فعال
- **مدل‌های متراکم**: Qwen3-32B، 14B، 8B، 4B، 1.7B، 0.6B برای سناریوهای مختلف اجرا
- **حالت تفکر**: رویکرد استدلال ترکیبی که از پاسخ‌های سریع و تفکر عمیق پشتیبانی می‌کند
- **برتری چندزبانه**: پشتیبانی از ۱۱۹ زبان و گویش
- **آموزش بهبود یافته**: ۳۶ تریلیون توکن داده آموزشی متنوع و با کیفیت بالا

## کاربردهای مدل‌های Qwen

### کاربردهای سازمانی

سازمان‌ها از مدل‌های Qwen برای تحلیل اسناد، خودکارسازی خدمات مشتری، کمک به تولید کد و کاربردهای هوش تجاری استفاده می‌کنند. ماهیت متن‌باز امکان سفارشی‌سازی برای نیازهای خاص کسب‌وکار را فراهم می‌کند و در عین حال حریم خصوصی و کنترل داده‌ها را حفظ می‌کند.

### محاسبات موبایل و Edge

برنامه‌های موبایل از مدل‌های Qwen برای ترجمه بلادرنگ، دستیارهای هوشمند، تولید محتوا و توصیه‌های شخصی‌سازی‌شده استفاده می‌کنند. محدوده اندازه‌های مدل امکان اجرا از دستگاه‌های موبایل تا سرورهای Edge را فراهم می‌کند.

### فناوری آموزشی

پلتفرم‌های آموزشی از مدل‌های Qwen برای آموزش شخصی‌سازی‌شده، تولید محتوای خودکار، کمک به یادگیری زبان و تجربیات آموزشی تعاملی استفاده می‌کنند. مدل‌های تخصصی مانند Qwen-Math تخصص خاص حوزه را ارائه می‌دهند.

### کاربردهای جهانی

کاربردهای بین‌المللی از قابلیت‌های چندزبانه قوی مدل‌های Qwen بهره‌مند می‌شوند و تجربه‌های هوش مصنوعی سازگار در زبان‌ها و زمینه‌های فرهنگی مختلف را امکان‌پذیر می‌کنند.

## چالش‌ها و محدودیت‌ها

### نیازهای محاسباتی

در حالی که Qwen مدل‌هایی در اندازه‌های مختلف ارائه می‌دهد، نسخه‌های بزرگ‌تر همچنان به منابع محاسباتی قابل توجهی برای عملکرد بهینه نیاز دارند، که ممکن است گزینه‌های اجرا را برای برخی سازمان‌ها محدود کند.

### عملکرد تخصصی حوزه

در حالی که مدل‌های Qwen در حوزه‌های عمومی عملکرد خوبی دارند، کاربردهای بسیار تخصصی ممکن است از تنظیم دقیق یا مدل‌های تخصصی بهره‌مند شوند.

### پیچیدگی انتخاب مدل

محدوده گسترده مدل‌ها و نسخه‌های موجود می‌تواند انتخاب را برای کاربران جدید در این اکوسیستم چالش‌برانگیز کند.

### عدم تعادل زبانی

در حالی که از زبان‌های زیادی پشتیبانی می‌شود، عملکرد ممکن است در زبان‌های مختلف متفاوت باشد، با قابلیت‌های قوی‌تر در انگلیسی و چینی.

## آینده خانواده مدل‌های Qwen

خانواده مدل‌های Qwen نمایانگر تکامل مداوم به سمت هوش مصنوعی با کیفیت بالا و دموکراتیک است. توسعه‌های آینده شامل بهینه‌سازی‌های کارایی پیشرفته، قابلیت‌های چندرسانه‌ای گسترش‌یافته، مکانیزم‌های استدلال بهبود یافته و یکپارچگی بهتر در سناریوهای مختلف اجرا خواهد بود.

با ادامه تکامل فناوری، انتظار می‌رود مدل‌های Qwen به طور فزاینده‌ای توانمند شوند و در عین حال دسترسی متن‌باز خود را حفظ کنند، که امکان اجرای هوش مصنوعی در سناریوها و موارد استفاده متنوع را فراهم می‌کند.

خانواده Qwen نشان می‌دهد که آینده توسعه هوش مصنوعی می‌تواند هم عملکرد پیشرفته و هم دسترسی متن‌باز را در بر بگیرد، و ابزارهای قدرتمندی را در اختیار سازمان‌ها قرار دهد و در عین حال شفافیت و کنترل را حفظ کند.

## مثال‌های توسعه و یکپارچگی

### شروع سریع با Transformers
در اینجا نحوه شروع کار با مدل‌های Qwen با استفاده از کتابخانه Hugging Face Transformers آورده شده است:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### استفاده از مدل‌های Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### استفاده تخصصی از مدل‌ها

**تولید کد با Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**حل مسائل ریاضی:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**وظایف بینایی-زبان:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### حالت تفکر (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 استقرار در موبایل و دستگاه‌های لبه‌ای

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### مثال استقرار API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## معیارهای عملکرد و دستاوردها

خانواده مدل‌های Qwen عملکرد چشمگیری در معیارهای مختلف به دست آورده است و در عین حال دسترسی به منبع باز را حفظ کرده است:

### نکات برجسته عملکرد

**برتری در استدلال:**
- Qwen3-235B-A22B نتایج رقابتی در ارزیابی‌های معیار کدنویسی، ریاضی و قابلیت‌های عمومی در مقایسه با مدل‌های برتر دیگر مانند DeepSeek-R1، o1، o3-mini، Grok-3 و Gemini-2.5-Pro به دست آورده است.
- Qwen3-30B-A3B عملکرد بهتری نسبت به QwQ-32B با 10 برابر پارامترهای فعال دارد.
- Qwen3-4B می‌تواند با عملکرد Qwen2.5-72B-Instruct رقابت کند.

**دستاوردهای کارایی:**
- مدل‌های پایه Qwen3-MoE عملکردی مشابه مدل‌های پایه متراکم Qwen2.5 دارند، در حالی که تنها از 10٪ پارامترهای فعال استفاده می‌کنند.
- صرفه‌جویی قابل توجه در هزینه‌های آموزش و استنتاج در مقایسه با مدل‌های متراکم.

**قابلیت‌های چندزبانه:**
- مدل‌های Qwen3 از 119 زبان و گویش پشتیبانی می‌کنند.
- عملکرد قوی در زمینه‌های زبانی و فرهنگی متنوع.

**مقیاس آموزش:**
- Qwen3 تقریباً دو برابر داده‌های Qwen2.5 را استفاده می‌کند، با حدود 36 تریلیون توکن که 119 زبان و گویش را پوشش می‌دهد، در مقایسه با 18 تریلیون توکن Qwen2.5.

### ماتریس مقایسه مدل‌ها

| سری مدل | محدوده پارامترها | طول زمینه | نقاط قوت کلیدی | بهترین موارد استفاده |
|---------|------------------|-----------|----------------|-----------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | عملکرد متعادل، چندزبانه | کاربردهای عمومی، استقرار در تولید |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | تولید کد، برنامه‌نویسی | توسعه نرم‌افزار، کمک به کدنویسی |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | استدلال ریاضی | پلتفرم‌های آموزشی، کاربردهای STEM |
| **Qwen2.5-VL** | متغیر | متغیر | درک بینایی-زبان | کاربردهای چندوجهی، تحلیل تصاویر |
| **Qwen3** | 0.6B-235B | متغیر | استدلال پیشرفته، حالت تفکر | استدلال پیچیده، کاربردهای پژوهشی |
| **Qwen3 MoE** | 30B-235B کل | متغیر | عملکرد کارآمد در مقیاس بزرگ | کاربردهای سازمانی، نیازهای با عملکرد بالا |

## راهنمای انتخاب مدل

### برای کاربردهای پایه
- **Qwen2.5-0.5B/1.5B**: اپلیکیشن‌های موبایل، دستگاه‌های لبه‌ای، کاربردهای بلادرنگ
- **Qwen2.5-3B/7B**: چت‌بات‌های عمومی، تولید محتوا، سیستم‌های پرسش و پاسخ

### برای وظایف ریاضی و استدلال
- **Qwen2.5-Math**: حل مسائل ریاضی و آموزش STEM
- **Qwen3 با حالت تفکر**: استدلال پیچیده که نیاز به تحلیل گام‌به‌گام دارد

### برای برنامه‌نویسی و توسعه
- **Qwen2.5-Coder**: تولید کد، اشکال‌زدایی، کمک به برنامه‌نویسی
- **Qwen3**: وظایف پیشرفته برنامه‌نویسی با قابلیت‌های استدلال

### برای کاربردهای چندوجهی
- **Qwen2.5-VL**: درک تصاویر، پاسخ به سوالات بصری
- **Qwen-Audio**: پردازش صوت و درک گفتار

### برای استقرار سازمانی
- **Qwen2.5-32B/72B**: درک زبان با عملکرد بالا
- **Qwen3-235B-A22B**: حداکثر قابلیت برای کاربردهای پرتقاضا

## پلتفرم‌های استقرار و دسترسی
### پلتفرم‌های ابری
- **Hugging Face Hub**: مخزن جامع مدل با پشتیبانی جامعه
- **ModelScope**: پلتفرم مدل علی‌بابا با ابزارهای بهینه‌سازی
- **ارائه‌دهندگان مختلف ابری**: پشتیبانی از طریق پلتفرم‌های استاندارد ML

### چارچوب‌های توسعه محلی
- **Transformers**: یکپارچگی استاندارد Hugging Face برای استقرار آسان
- **vLLM**: سرویس‌دهی با عملکرد بالا برای محیط‌های تولیدی
- **Ollama**: استقرار و مدیریت محلی ساده‌شده
- **ONNX Runtime**: بهینه‌سازی چندپلتفرمی برای سخت‌افزارهای مختلف
- **llama.cpp**: پیاده‌سازی کارآمد C++ برای پلتفرم‌های متنوع

### منابع آموزشی
- **مستندات Qwen**: مستندات رسمی و کارت‌های مدل
- **Hugging Face Model Hub**: دموهای تعاملی و مثال‌های جامعه
- **مقالات پژوهشی**: مقالات فنی در arxiv برای درک عمیق
- **انجمن‌های جامعه**: پشتیبانی فعال جامعه و بحث‌ها

### شروع کار با مدل‌های Qwen

#### پلتفرم‌های توسعه
1. **Hugging Face Transformers**: با یکپارچگی استاندارد پایتون شروع کنید
2. **ModelScope**: ابزارهای استقرار بهینه علی‌بابا را کاوش کنید
3. **استقرار محلی**: از Ollama یا Transformers مستقیم برای آزمایش محلی استفاده کنید

#### مسیر یادگیری
1. **مفاهیم اصلی را درک کنید**: معماری و قابلیت‌های خانواده Qwen را مطالعه کنید
2. **با انواع مختلف آزمایش کنید**: اندازه‌های مختلف مدل را امتحان کنید تا توازن عملکرد را درک کنید
3. **پیاده‌سازی را تمرین کنید**: مدل‌ها را در محیط‌های توسعه استقرار دهید
4. **استقرار را بهینه کنید**: برای موارد استفاده تولیدی تنظیم دقیق انجام دهید

#### بهترین شیوه‌ها
- **کوچک شروع کنید**: با مدل‌های کوچک‌تر (1.5B-7B) برای توسعه اولیه شروع کنید
- **از قالب‌های چت استفاده کنید**: قالب‌بندی مناسب را برای نتایج بهینه اعمال کنید
- **منابع را نظارت کنید**: استفاده از حافظه و سرعت استنتاج را پیگیری کنید
- **تخصص را در نظر بگیرید**: هنگام نیاز، انواع خاص دامنه را انتخاب کنید

## الگوهای استفاده پیشرفته

### مثال‌های تنظیم دقیق

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### مهندسی درخواست تخصصی

**برای وظایف استدلال پیچیده:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**برای تولید کد با زمینه:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### کاربردهای چندزبانه

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 الگوهای استقرار تولیدی

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## استراتژی‌های بهینه‌سازی عملکرد

### بهینه‌سازی حافظه

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### بهینه‌سازی استنتاج

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## بهترین شیوه‌ها و دستورالعمل‌ها

### امنیت و حریم خصوصی

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### نظارت و ارزیابی

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## نتیجه‌گیری

خانواده مدل‌های Qwen نمایانگر رویکردی جامع برای دموکراتیزه کردن فناوری هوش مصنوعی است، در حالی که عملکرد رقابتی را در کاربردهای متنوع حفظ می‌کند. با تعهد به دسترسی منبع باز، قابلیت‌های چندزبانه و گزینه‌های استقرار انعطاف‌پذیر، Qwen به سازمان‌ها و توسعه‌دهندگان این امکان را می‌دهد تا از قابلیت‌های قدرتمند هوش مصنوعی بهره‌مند شوند، صرف‌نظر از منابع یا نیازهای خاص آن‌ها.

### نکات کلیدی

**برتری منبع باز**: Qwen نشان می‌دهد که مدل‌های منبع باز می‌توانند عملکردی رقابتی با جایگزین‌های اختصاصی داشته باشند و در عین حال شفافیت، سفارشی‌سازی و کنترل را فراهم کنند.

**معماری مقیاس‌پذیر**: محدوده 0.5B تا 235B پارامتر امکان استقرار در طیف کامل محیط‌های محاسباتی، از دستگاه‌های موبایل تا خوشه‌های سازمانی را فراهم می‌کند.

**قابلیت‌های تخصصی**: انواع خاص دامنه مانند Qwen-Coder، Qwen-Math و Qwen-VL تخصص ویژه‌ای را ارائه می‌دهند و در عین حال درک عمومی زبان را حفظ می‌کنند.

**دسترسی جهانی**: پشتیبانی قوی چندزبانه در بیش از 119 زبان، Qwen را برای کاربردهای بین‌المللی و پایگاه‌های کاربری متنوع مناسب می‌کند.

**نوآوری مستمر**: تکامل از Qwen 1.0 به Qwen3 نشان‌دهنده بهبود مداوم در قابلیت‌ها، کارایی و گزینه‌های استقرار است.

### چشم‌انداز آینده

با ادامه تکامل خانواده Qwen، می‌توان انتظار داشت:

- **کارایی بیشتر**: بهینه‌سازی مداوم برای نسبت‌های بهتر عملکرد به پارامتر
- **قابلیت‌های چندوجهی گسترش‌یافته**: ادغام پردازش پیشرفته‌تر بینایی، صوت و متن
- **استدلال بهبود یافته**: مکانیسم‌های تفکر پیشرفته و قابلیت‌های حل مسئله چندمرحله‌ای
- **ابزارهای استقرار بهتر**: چارچوب‌ها و ابزارهای بهینه‌سازی پیشرفته برای سناریوهای استقرار متنوع
- **رشد جامعه**: گسترش اکوسیستم ابزارها، کاربردها و مشارکت‌های جامعه

### گام‌های بعدی

چه در حال ساخت یک چت‌بات باشید، چه در حال توسعه ابزارهای آموزشی، ایجاد دستیارهای کدنویسی یا کار بر روی کاربردهای چندزبانه، خانواده Qwen راه‌حل‌های مقیاس‌پذیری با پشتیبانی قوی جامعه و مستندات جامع ارائه می‌دهد.

برای آخرین به‌روزرسانی‌ها، انتشار مدل‌ها و مستندات فنی دقیق، به مخازن رسمی Qwen در Hugging Face مراجعه کنید و بحث‌ها و مثال‌های فعال جامعه را کاوش کنید.

آینده توسعه هوش مصنوعی در ابزارهای قابل دسترس، شفاف و قدرتمندی نهفته است که نوآوری را در تمام بخش‌ها و مقیاس‌ها امکان‌پذیر می‌کند. خانواده Qwen این چشم‌انداز را تجسم می‌بخشد و به سازمان‌ها و توسعه‌دهندگان پایه‌ای برای ساخت نسل بعدی برنامه‌های هوش مصنوعی ارائه می‌دهد.

## منابع اضافی

- **مستندات رسمی**: [مستندات Qwen](https://qwen.readthedocs.io/)
- **مرکز مدل**: [مجموعه‌های Qwen در Hugging Face](https://huggingface.co/collections/Qwen/)
- **مقالات فنی**: [انتشارات پژوهشی Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **جامعه**: [بحث‌ها و مشکلات GitHub](https://github.com/QwenLM/)
- **پلتفرم ModelScope**: [ModelScope علی‌بابا](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## نتایج یادگیری

پس از تکمیل این ماژول، شما قادر خواهید بود:

1. مزایای معماری خانواده مدل Qwen و رویکرد منبع باز آن را توضیح دهید.
2. بر اساس نیازهای خاص کاربرد و محدودیت‌های منابع، نوع مناسب Qwen را انتخاب کنید.
3. مدل‌های Qwen را در سناریوهای مختلف استقرار با پیکربندی‌های بهینه پیاده‌سازی کنید.
4. تکنیک‌های کوانتیزاسیون و بهینه‌سازی را برای بهبود عملکرد مدل Qwen اعمال کنید.
5. توازن بین اندازه مدل، عملکرد و قابلیت‌ها را در سراسر خانواده Qwen ارزیابی کنید.

## گام بعدی

- [03: اصول خانواده Gemma](03.GemmaFamily.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.