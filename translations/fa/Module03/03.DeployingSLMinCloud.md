<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T16:04:21+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "fa"
}
-->
# استقرار ابری کانتینری - راهکارهای در مقیاس تولید

این آموزش جامع سه رویکرد اصلی برای استقرار مدل Phi-4-mini-instruct مایکروسافت در محیط‌های کانتینری را پوشش می‌دهد: vLLM، Ollama و SLM Engine با ONNX Runtime. این مدل با ۳.۸ میلیارد پارامتر، انتخابی بهینه برای وظایف استدلالی است که در عین حال کارایی لازم برای استقرار در لبه را حفظ می‌کند.

## فهرست مطالب

1. [مقدمه‌ای بر استقرار کانتینری Phi-4-mini](../../../Module03)
2. [اهداف آموزشی](../../../Module03)
3. [درک طبقه‌بندی Phi-4-mini](../../../Module03)
4. [استقرار کانتینری vLLM](../../../Module03)
5. [استقرار کانتینری Ollama](../../../Module03)
6. [SLM Engine با ONNX Runtime](../../../Module03)
7. [چارچوب مقایسه](../../../Module03)
8. [بهترین شیوه‌ها](../../../Module03)

## مقدمه‌ای بر استقرار کانتینری Phi-4-mini

مدل‌های زبانی کوچک (SLM) پیشرفتی کلیدی در EdgeAI محسوب می‌شوند که قابلیت‌های پردازش زبان طبیعی پیشرفته را در دستگاه‌های با منابع محدود فراهم می‌کنند. این آموزش بر استراتژی‌های استقرار کانتینری مدل Phi-4-mini-instruct مایکروسافت تمرکز دارد؛ مدلی پیشرفته برای استدلال که تعادل بین توانایی و کارایی را حفظ می‌کند.

### مدل برجسته: Phi-4-mini-instruct

**Phi-4-mini-instruct (۳.۸ میلیارد پارامتر)**: جدیدترین مدل سبک مایکروسافت که برای محیط‌های با محدودیت حافظه/محاسبات طراحی شده و قابلیت‌های استثنایی در زمینه‌های زیر دارد:
- **استدلال ریاضی و محاسبات پیچیده**
- **تولید، اشکال‌زدایی و تحلیل کد**
- **حل مسائل منطقی و استدلال گام‌به‌گام**
- **کاربردهای آموزشی که نیاز به توضیحات دقیق دارند**
- **فراخوانی توابع و یکپارچه‌سازی ابزارها**

این مدل که بخشی از دسته "مدل‌های کوچک SLM" (۱.۵ تا ۱۳.۹ میلیارد پارامتر) است، تعادلی بهینه بین توانایی استدلال و کارایی منابع برقرار می‌کند.

### مزایای استقرار کانتینری Phi-4-mini

- **کارایی عملیاتی**: استنتاج سریع برای وظایف استدلالی با نیازهای محاسباتی کمتر
- **انعطاف‌پذیری استقرار**: قابلیت‌های هوش مصنوعی روی دستگاه با حفظ حریم خصوصی از طریق پردازش محلی
- **صرفه‌جویی در هزینه**: کاهش هزینه‌های عملیاتی در مقایسه با مدل‌های بزرگ‌تر در عین حفظ کیفیت
- **ایزوله‌سازی**: جداسازی تمیز بین نمونه‌های مدل و محیط‌های اجرایی امن
- **مقیاس‌پذیری**: مقیاس‌پذیری افقی آسان برای افزایش توان استدلال

## اهداف آموزشی

در پایان این آموزش، شما قادر خواهید بود:

- Phi-4-mini-instruct را در محیط‌های کانتینری مختلف مستقر و بهینه کنید
- استراتژی‌های پیشرفته کوانتیزاسیون و فشرده‌سازی را برای سناریوهای مختلف استقرار پیاده‌سازی کنید
- ارکستراسیون کانتینری آماده تولید را برای بارهای کاری استدلالی پیکربندی کنید
- چارچوب‌های استقرار مناسب را بر اساس نیازهای خاص مورد ارزیابی و انتخاب قرار دهید
- بهترین شیوه‌های امنیت، نظارت و مقیاس‌پذیری را برای استقرار کانتینری SLM اعمال کنید

## درک طبقه‌بندی Phi-4-mini

### مشخصات مدل

**جزئیات فنی:**
- **پارامترها**: ۳.۸ میلیارد (دسته SLM کوچک)
- **معماری**: ترانسفورمر دنس فقط دیکودر با توجه گروهی-پرسش
- **طول زمینه**: ۱۲۸ هزار توکن (۳۲ هزار توکن برای عملکرد بهینه توصیه می‌شود)
- **واژگان**: ۲۰۰ هزار توکن با پشتیبانی چندزبانه
- **داده‌های آموزشی**: ۵ تریلیون توکن از محتوای با کیفیت بالا و غنی از استدلال

### نیازمندی‌های منابع

| نوع استقرار | حداقل RAM | RAM توصیه‌شده | VRAM (GPU) | فضای ذخیره‌سازی | موارد استفاده معمول |
|-------------|-----------|---------------|------------|------------------|-----------------------|
| **توسعه** | ۶ گیگابایت | ۸ گیگابایت | - | ۸ گیگابایت | تست محلی، نمونه‌سازی اولیه |
| **تولید CPU** | ۸ گیگابایت | ۱۲ گیگابایت | - | ۱۰ گیگابایت | سرورهای لبه، استقرار بهینه از نظر هزینه |
| **تولید GPU** | ۶ گیگابایت | ۸ گیگابایت | ۴-۶ گیگابایت | ۸ گیگابایت | خدمات استدلال با توان بالا |
| **بهینه‌شده برای لبه** | ۴ گیگابایت | ۶ گیگابایت | - | ۶ گیگابایت | استقرار کوانتیزه‌شده، دروازه‌های IoT |

### قابلیت‌های Phi-4-mini

- **برتری ریاضی**: حل مسائل پیشرفته حساب، جبر و حسابان
- **هوش کدنویسی**: تولید کد در زبان‌های مختلف مانند پایتون و جاوااسکریپت همراه با اشکال‌زدایی
- **استدلال منطقی**: تجزیه مسائل به گام‌های کوچک‌تر و ساخت راه‌حل
- **پشتیبانی آموزشی**: توضیحات دقیق مناسب برای یادگیری و آموزش
- **فراخوانی توابع**: پشتیبانی بومی از یکپارچه‌سازی ابزارها و تعاملات API

## استقرار کانتینری vLLM

vLLM پشتیبانی عالی از Phi-4-mini-instruct را با عملکرد استنتاج بهینه و APIهای سازگار با OpenAI ارائه می‌دهد که آن را برای خدمات استدلال تولیدی ایده‌آل می‌کند.

### مثال‌های شروع سریع

#### استقرار پایه CPU (توسعه)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### استقرار تولیدی با شتاب GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### پیکربندی تولیدی

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### تست قابلیت‌های استدلال Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## استقرار کانتینری Ollama

Ollama پشتیبانی عالی از Phi-4-mini-instruct را با استقرار و مدیریت ساده ارائه می‌دهد که آن را برای توسعه و استقرار تولیدی متعادل ایده‌آل می‌کند.

### راه‌اندازی سریع

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### پیکربندی تولیدی

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### بهینه‌سازی مدل و انواع آن

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### مثال‌های استفاده از API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine با ONNX Runtime

ONNX Runtime عملکرد بهینه‌ای برای استقرار لبه‌ای Phi-4-mini-instruct با بهینه‌سازی پیشرفته و سازگاری چندپلتفرمی ارائه می‌دهد.

### راه‌اندازی پایه

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### پیاده‌سازی ساده سرور

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### اسکریپت تبدیل مدل

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### پیکربندی تولیدی

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### تست استقرار ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## چارچوب مقایسه

### مقایسه چارچوب‌ها برای Phi-4-mini

| ویژگی | vLLM | Ollama | ONNX Runtime |
|-------|------|--------|--------------|
| **پیچیدگی راه‌اندازی** | متوسط | آسان | پیچیده |
| **عملکرد (GPU)** | عالی (~۲۵ توکن/ثانیه) | بسیار خوب (~۲۰ توکن/ثانیه) | خوب (~۱۵ توکن/ثانیه) |
| **عملکرد (CPU)** | خوب (~۸ توکن/ثانیه) | بسیار خوب (~۱۲ توکن/ثانیه) | عالی (~۱۵ توکن/ثانیه) |
| **مصرف حافظه** | ۸-۱۲ گیگابایت | ۶-۱۰ گیگابایت | ۴-۸ گیگابایت |
| **سازگاری API** | سازگار با OpenAI | REST سفارشی | FastAPI سفارشی |
| **فراخوانی توابع** | ✅ بومی | ✅ پشتیبانی‌شده | ⚠️ پیاده‌سازی سفارشی |
| **پشتیبانی از کوانتیزاسیون** | AWQ، GPTQ | Q4_0، Q5_1، Q8_0 | کوانتیزاسیون ONNX |
| **آمادگی تولیدی** | ✅ عالی | ✅ بسیار خوب | ✅ خوب |
| **استقرار لبه‌ای** | خوب | عالی | برجسته |

## منابع اضافی

### مستندات رسمی
- **کارت مدل Phi-4 مایکروسافت**: مشخصات دقیق و راهنمای استفاده
- **مستندات vLLM**: گزینه‌های پیشرفته پیکربندی و بهینه‌سازی
- **کتابخانه مدل Ollama**: مدل‌های جامعه و مثال‌های سفارشی‌سازی
- **راهنماهای ONNX Runtime**: استراتژی‌های بهینه‌سازی عملکرد و استقرار

### ابزارهای توسعه
- **Hugging Face Transformers**: برای تعامل و سفارشی‌سازی مدل
- **مشخصات API OpenAI**: برای تست سازگاری vLLM
- **بهترین شیوه‌های Docker**: امنیت و بهینه‌سازی کانتینر
- **استقرار Kubernetes**: الگوهای ارکستراسیون برای مقیاس‌گذاری تولیدی

### منابع آموزشی
- **بنچمارک عملکرد SLM**: روش‌شناسی‌های تحلیل مقایسه‌ای
- **استقرار هوش مصنوعی لبه‌ای**: بهترین شیوه‌ها برای محیط‌های با منابع محدود
- **بهینه‌سازی وظایف استدلالی**: استراتژی‌های پرامپتینگ برای مسائل ریاضی و منطقی
- **امنیت کانتینر**: شیوه‌های سخت‌افزاری برای استقرار مدل‌های هوش مصنوعی

## نتایج یادگیری

پس از تکمیل این ماژول، شما قادر خواهید بود:

1. مدل Phi-4-mini-instruct را در محیط‌های کانتینری با استفاده از چارچوب‌های مختلف مستقر کنید
2. استقرار SLM را برای محیط‌های سخت‌افزاری مختلف پیکربندی و بهینه کنید
3. بهترین شیوه‌های امنیتی را برای استقرار کانتینری هوش مصنوعی پیاده‌سازی کنید
4. چارچوب‌های استقرار مناسب را بر اساس نیازهای خاص مقایسه و انتخاب کنید
5. استراتژی‌های نظارت و مقیاس‌پذیری را برای خدمات SLM در سطح تولید اعمال کنید

## گام بعدی

- بازگشت به [ماژول ۱](../Module01/README.md)
- بازگشت به [ماژول ۲](../Module02/README.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.