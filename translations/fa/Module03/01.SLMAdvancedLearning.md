<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-17T16:01:24+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "fa"
}
-->
# بخش ۱: یادگیری پیشرفته SLM - مبانی و بهینه‌سازی

مدل‌های زبان کوچک (SLM) یک پیشرفت مهم در EdgeAI هستند که امکان پردازش پیشرفته زبان طبیعی را در دستگاه‌های با منابع محدود فراهم می‌کنند. درک نحوه استقرار، بهینه‌سازی و استفاده مؤثر از SLM‌ها برای ساخت راه‌حل‌های عملی مبتنی بر هوش مصنوعی در لبه ضروری است.

## مقدمه

در این درس، مدل‌های زبان کوچک (SLM) و استراتژی‌های پیشرفته پیاده‌سازی آن‌ها را بررسی خواهیم کرد. مفاهیم بنیادی SLM‌ها، مرزهای پارامتر و طبقه‌بندی آن‌ها، تکنیک‌های بهینه‌سازی و استراتژی‌های عملی استقرار در محیط‌های محاسباتی لبه را پوشش خواهیم داد.

## اهداف یادگیری

در پایان این درس، شما قادر خواهید بود:

- 🔢 درک مرزهای پارامتر و طبقه‌بندی مدل‌های زبان کوچک.
- 🛠️ شناسایی تکنیک‌های کلیدی بهینه‌سازی برای استقرار SLM‌ها در دستگاه‌های لبه.
- 🚀 یادگیری پیاده‌سازی استراتژی‌های پیشرفته کوانتیزاسیون و فشرده‌سازی برای SLM‌ها.

## درک مرزهای پارامتر و طبقه‌بندی SLM‌ها

مدل‌های زبان کوچک (SLM) مدل‌های هوش مصنوعی هستند که برای پردازش، درک و تولید محتوای زبان طبیعی با تعداد پارامترهای به‌مراتب کمتر نسبت به مدل‌های بزرگ طراحی شده‌اند. در حالی که مدل‌های زبان بزرگ (LLM) شامل صدها میلیارد تا تریلیون پارامتر هستند، SLM‌ها به طور خاص برای کارایی و استقرار در لبه طراحی شده‌اند.

چارچوب طبقه‌بندی پارامتر به ما کمک می‌کند تا دسته‌های مختلف SLM‌ها و موارد استفاده مناسب آن‌ها را درک کنیم. این طبقه‌بندی برای انتخاب مدل مناسب برای سناریوهای خاص محاسبات لبه بسیار مهم است.

### چارچوب طبقه‌بندی پارامتر

درک مرزهای پارامتر به انتخاب مدل‌های مناسب برای سناریوهای مختلف محاسبات لبه کمک می‌کند:

- **🔬 میکرو SLM‌ها**: ۱۰۰ میلیون - ۱.۴ میلیارد پارامتر (فوق سبک برای دستگاه‌های موبایل)
- **📱 SLM‌های کوچک**: ۱.۵ میلیارد - ۱۳.۹ میلیارد پارامتر (تعادل بین عملکرد و کارایی)
- **⚖️ SLM‌های متوسط**: ۱۴ میلیارد - ۳۰ میلیارد پارامتر (نزدیک به قابلیت‌های LLM در حالی که کارایی حفظ می‌شود)

مرز دقیق در جامعه تحقیقاتی سیال باقی می‌ماند، اما اکثر متخصصان مدل‌هایی با کمتر از ۳۰ میلیارد پارامتر را "کوچک" در نظر می‌گیرند، با برخی منابع که آستانه را حتی پایین‌تر، در ۱۰ میلیارد پارامتر تعیین می‌کنند.

### مزایای کلیدی SLM‌ها

SLM‌ها چندین مزیت بنیادی ارائه می‌دهند که آن‌ها را برای کاربردهای محاسبات لبه ایده‌آل می‌کند:

**کارایی عملیاتی**: SLM‌ها زمان‌های استنتاج سریع‌تری را به دلیل پردازش پارامترهای کمتر ارائه می‌دهند، که آن‌ها را برای کاربردهای بلادرنگ ایده‌آل می‌کند. آن‌ها به منابع محاسباتی کمتری نیاز دارند، که امکان استقرار در دستگاه‌های با منابع محدود را فراهم می‌کند، در حالی که مصرف انرژی کمتر و ردپای کربنی کاهش‌یافته را حفظ می‌کنند.

**انعطاف‌پذیری استقرار**: این مدل‌ها قابلیت‌های هوش مصنوعی روی دستگاه را بدون نیاز به اتصال اینترنت فراهم می‌کنند، از طریق پردازش محلی حریم خصوصی و امنیت را افزایش می‌دهند، می‌توانند برای کاربردهای خاص حوزه سفارشی شوند و برای محیط‌های مختلف محاسبات لبه مناسب هستند.

**مقرون‌به‌صرفه بودن**: SLM‌ها آموزش و استقرار مقرون‌به‌صرفه‌تری نسبت به LLM‌ها ارائه می‌دهند، با کاهش هزینه‌های عملیاتی و نیازهای پهنای باند کمتر برای کاربردهای لبه.

## استراتژی‌های پیشرفته دستیابی به مدل

### اکوسیستم Hugging Face

Hugging Face به عنوان مرکز اصلی برای کشف و دسترسی به SLM‌های پیشرفته عمل می‌کند. این پلتفرم منابع جامعی برای کشف و استقرار مدل ارائه می‌دهد:

**ویژگی‌های کشف مدل**: پلتفرم فیلترهای پیشرفته بر اساس تعداد پارامتر، نوع مجوز و معیارهای عملکرد ارائه می‌دهد. کاربران می‌توانند به ابزارهای مقایسه مدل کنار هم، معیارهای عملکرد بلادرنگ و نتایج ارزیابی، و دموهای WebGPU برای آزمایش فوری دسترسی داشته باشند.

**مجموعه‌های SLM منتخب**: مدل‌های محبوب شامل Phi-4-mini-3.8B برای وظایف استدلال پیشرفته، سری Qwen3 (0.6B/1.7B/4B) برای کاربردهای چندزبانه، Google Gemma3 برای وظایف عمومی کارآمد، و مدل‌های آزمایشی مانند BitNET برای استقرار فوق‌العاده کم‌دقت هستند. پلتفرم همچنین مجموعه‌های مبتنی بر جامعه با مدل‌های تخصصی برای حوزه‌های خاص و انواع پیش‌آموزش و تنظیم دستورالعمل بهینه‌شده برای موارد استفاده مختلف را ارائه می‌دهد.

### کاتالوگ مدل Azure AI Foundry

کاتالوگ مدل Azure AI Foundry دسترسی در سطح سازمانی به SLM‌ها با قابلیت‌های یکپارچه‌سازی پیشرفته را فراهم می‌کند:

**یکپارچه‌سازی سازمانی**: کاتالوگ شامل مدل‌هایی است که مستقیماً توسط Azure با پشتیبانی در سطح سازمانی و SLA‌ها فروخته می‌شوند، از جمله Phi-4-mini-3.8B برای قابلیت‌های استدلال پیشرفته و Llama 3-8B برای استقرار تولید. همچنین مدل‌هایی مانند Qwen3 8B از مدل‌های منبع باز معتبر شخص ثالث را شامل می‌شود.

**مزایای سازمانی**: ابزارهای داخلی برای تنظیم دقیق، مشاهده‌پذیری و هوش مصنوعی مسئولانه با توان عملیاتی قابل تخصیص در میان خانواده‌های مدل یکپارچه شده‌اند. پشتیبانی مستقیم مایکروسافت با SLA‌های سازمانی، ویژگی‌های امنیتی و انطباق یکپارچه، و گردش کارهای جامع استقرار تجربه سازمانی را بهبود می‌بخشند.

## تکنیک‌های پیشرفته کوانتیزاسیون و بهینه‌سازی

### چارچوب بهینه‌سازی Llama.cpp

Llama.cpp تکنیک‌های کوانتیزاسیون پیشرفته برای حداکثر کارایی در استقرار لبه ارائه می‌دهد:

**روش‌های کوانتیزاسیون**: چارچوب از سطوح مختلف کوانتیزاسیون پشتیبانی می‌کند، از جمله Q4_0 (کوانتیزاسیون ۴ بیتی با کاهش اندازه عالی - ایده‌آل برای استقرار موبایل Qwen3-0.6B)، Q5_1 (کوانتیزاسیون ۵ بیتی با تعادل کیفیت و فشرده‌سازی - مناسب برای استنتاج لبه Phi-4-mini-3.8B)، و Q8_0 (کوانتیزاسیون ۸ بیتی برای کیفیت نزدیک به اصلی - توصیه‌شده برای استفاده تولیدی Google Gemma3). BitNET نمایانگر پیشرفته‌ترین سطح با کوانتیزاسیون ۱ بیتی برای سناریوهای فشرده‌سازی شدید است.

**مزایای پیاده‌سازی**: استنتاج بهینه‌شده برای CPU با شتاب SIMD بارگذاری و اجرای مدل را بهینه می‌کند. سازگاری چندپلتفرمی در معماری‌های x86، ARM و Apple Silicon قابلیت‌های استقرار مستقل از سخت‌افزار را فراهم می‌کند.

**مثال پیاده‌سازی عملی**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**مقایسه ردپای حافظه**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### مجموعه بهینه‌سازی Microsoft Olive

Microsoft Olive گردش کارهای جامع بهینه‌سازی مدل طراحی‌شده برای محیط‌های تولیدی ارائه می‌دهد:

**تکنیک‌های بهینه‌سازی**: این مجموعه شامل کوانتیزاسیون پویا برای انتخاب خودکار دقت (به‌ویژه مؤثر با مدل‌های سری Qwen3)، بهینه‌سازی گراف و ترکیب اپراتور (بهینه‌شده برای معماری Google Gemma3)، بهینه‌سازی‌های خاص سخت‌افزار برای CPU، GPU و NPU (با پشتیبانی ویژه از Phi-4-mini-3.8B در دستگاه‌های ARM)، و خطوط لوله بهینه‌سازی چندمرحله‌ای است. مدل‌های BitNET نیاز به گردش کارهای کوانتیزاسیون ۱ بیتی تخصصی در چارچوب Olive دارند.

**اتوماسیون گردش کار**: معیارگذاری خودکار در میان انواع بهینه‌سازی کیفیت معیارها را در طول بهینه‌سازی حفظ می‌کند. یکپارچه‌سازی با چارچوب‌های محبوب ML مانند PyTorch و ONNX قابلیت‌های بهینه‌سازی استقرار در ابر و لبه را فراهم می‌کند.

**مثال پیاده‌سازی عملی**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### چارچوب Apple MLX

Apple MLX بهینه‌سازی بومی طراحی‌شده به‌طور خاص برای دستگاه‌های Apple Silicon ارائه می‌دهد:

**بهینه‌سازی Apple Silicon**: چارچوب از معماری حافظه یکپارچه با یکپارچه‌سازی Metal Performance Shaders، استنتاج دقت مختلط خودکار (به‌ویژه مؤثر با Google Gemma3)، و استفاده بهینه از پهنای باند حافظه استفاده می‌کند. Phi-4-mini-3.8B عملکرد استثنایی در تراشه‌های سری M نشان می‌دهد، در حالی که Qwen3-1.7B تعادل مطلوبی برای استقرار در MacBook Air فراهم می‌کند.

**ویژگی‌های توسعه**: پشتیبانی از API‌های Python و Swift با عملیات آرایه سازگار با NumPy، قابلیت‌های تفکیک خودکار، و یکپارچه‌سازی بی‌دردسر با ابزارهای توسعه Apple محیط توسعه جامعی را فراهم می‌کند.

**مثال پیاده‌سازی عملی**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## استراتژی‌های استقرار تولید و استنتاج

### Ollama: استقرار محلی ساده‌شده

Ollama استقرار SLM را با ویژگی‌های آماده سازمانی برای محیط‌های محلی و لبه ساده می‌کند:

**قابلیت‌های استقرار**: نصب و اجرای مدل با یک فرمان با کشیدن و ذخیره خودکار مدل. پشتیبانی از Phi-4-mini-3.8B، کل سری Qwen3 (0.6B/1.7B/4B)، و Google Gemma3 با REST API برای یکپارچه‌سازی برنامه و قابلیت‌های مدیریت و تغییر مدل چندگانه. مدل‌های BitNET نیاز به پیکربندی‌های ساخت آزمایشی برای پشتیبانی از کوانتیزاسیون ۱ بیتی دارند.

**ویژگی‌های پیشرفته**: پشتیبانی از تنظیم دقیق مدل سفارشی، تولید Dockerfile برای استقرار کانتینری، شتاب GPU با تشخیص خودکار، و گزینه‌های کوانتیزاسیون و بهینه‌سازی مدل انعطاف‌پذیری استقرار جامعی را فراهم می‌کند.

### VLLM: استنتاج با عملکرد بالا

VLLM بهینه‌سازی استنتاج در سطح تولید برای سناریوهای با توان بالا ارائه می‌دهد:

**بهینه‌سازی‌های عملکرد**: PagedAttention برای محاسبه توجه حافظه‌کارآمد (به‌ویژه مفید برای معماری ترانسفورمر Phi-4-mini-3.8B)، دسته‌بندی پویا برای بهینه‌سازی توان (بهینه‌شده برای پردازش موازی سری Qwen3)، موازی‌سازی تنسور برای مقیاس‌گذاری چند GPU (پشتیبانی از Google Gemma3)، و رمزگشایی حدسی برای کاهش تأخیر. مدل‌های BitNET نیاز به هسته‌های استنتاج تخصصی برای عملیات ۱ بیتی دارند.

**یکپارچه‌سازی سازمانی**: نقاط پایانی API سازگار با OpenAI، پشتیبانی از استقرار Kubernetes، یکپارچه‌سازی نظارت و مشاهده‌پذیری، و قابلیت‌های مقیاس‌گذاری خودکار راه‌حل‌های استقرار در سطح سازمانی را فراهم می‌کنند.

### Foundry Local: راه‌حل لبه مایکروسافت

Foundry Local قابلیت‌های جامع استقرار لبه برای محیط‌های سازمانی ارائه می‌دهد:

**ویژگی‌های محاسبات لبه**: طراحی معماری آفلاین اول با بهینه‌سازی محدودیت منابع، مدیریت رجیستری مدل محلی، و قابلیت‌های همگام‌سازی لبه به ابر استقرار لبه قابل‌اعتماد را تضمین می‌کند.

**امنیت و انطباق**: پردازش داده‌های محلی برای حفظ حریم خصوصی، کنترل‌های امنیتی سازمانی، گزارش‌گیری انطباق و ثبت حسابرسی، و مدیریت دسترسی مبتنی بر نقش امنیت جامع برای استقرارهای لبه فراهم می‌کند.

## بهترین شیوه‌ها برای پیاده‌سازی SLM

### دستورالعمل‌های انتخاب مدل

هنگام انتخاب SLM‌ها برای استقرار لبه، عوامل زیر را در نظر بگیرید:

**ملاحظات تعداد پارامتر**: مدل‌های میکرو SLM مانند Qwen3-0.6B را برای کاربردهای موبایل فوق سبک، SLM‌های کوچک مانند Qwen3-1.7B یا Google Gemma3 را برای سناریوهای عملکرد متعادل، و SLM‌های متوسط مانند Phi-4-mini-3.8B یا Qwen3-4B را هنگام نزدیک شدن به قابلیت‌های LLM در حالی که کارایی حفظ می‌شود انتخاب کنید. مدل‌های BitNET فشرده‌سازی فوق‌العاده آزمایشی را برای کاربردهای تحقیقاتی خاص ارائه می‌دهند.

**هم‌راستایی با موارد استفاده**: قابلیت‌های مدل را با الزامات کاربرد خاص مطابقت دهید، عواملی مانند کیفیت پاسخ، سرعت استنتاج، محدودیت‌های حافظه و الزامات عملیات آفلاین را در نظر بگیرید.

### انتخاب استراتژی بهینه‌سازی

**رویکرد کوانتیزاسیون**: سطوح کوانتیزاسیون مناسب را بر اساس الزامات کیفیت و محدودیت‌های سخت‌افزاری انتخاب کنید. Q4_0 را برای حداکثر فشرده‌سازی (ایده‌آل برای استقرار موبایل Qwen3-0.6B)، Q5_1 را برای تعادل کیفیت-فشرده‌سازی (مناسب برای Phi-4-mini-3.8B و Google Gemma3)، و Q8_0 را برای حفظ کیفیت نزدیک به اصلی (توصیه‌شده برای محیط‌های تولیدی Qwen3-4B) در نظر بگیرید. کوانتیزاسیون ۱ بیتی BitNET نمایانگر مرز فشرده‌سازی شدید برای کاربردهای تخصصی است.

**انتخاب چارچوب**: چارچوب‌های بهینه‌سازی را بر اساس سخت‌افزار هدف و الزامات استقرار انتخاب کنید. از Llama.cpp برای استقرار بهینه‌شده برای CPU، Microsoft Olive برای گردش کارهای جامع بهینه‌سازی، و Apple MLX برای دستگاه‌های Apple Silicon استفاده کنید.

## مثال‌های عملی مدل و موارد استفاده

### سناریوهای استقرار واقعی

**کاربردهای موبایل**: Qwen3-0.6B در کاربردهای چت‌بات گوشی‌های هوشمند با ردپای حافظه حداقلی عالی عمل می‌کند، در حالی که Google Gemma3 عملکرد متعادلی برای ابزارهای آموزشی مبتنی بر تبلت ارائه می‌دهد. Phi-4-mini-3.8B قابلیت‌های استدلال برتر برای کاربردهای بهره‌وری موبایل ارائه می‌دهد.

**محاسبات دسکتاپ و لبه**: Qwen3-1.7B عملکرد بهینه‌ای برای کاربردهای دستیار دسکتاپ ارائه می‌دهد، Phi-4-mini-3.8B قابلیت‌های پیشرفته تولید کد برای ابزارهای توسعه‌دهنده فراهم می‌کند، و Qwen3-4B تحلیل اسناد پیچیده را در محیط‌های ایستگاه کاری ممکن می‌سازد.

**تحقیق و آزمایش**: مدل‌های BitNET امکان بررسی استنتاج فوق‌العاده کم‌دقت برای تحقیقات دانشگاهی و کاربردهای اثبات مفهوم که نیاز به محدودیت‌های شدید منابع دارند را فراهم می‌کنند.

### معیارهای عملکرد و مقایسه‌ها

**سرعت استنتاج**: Qwen3-0.6B سریع‌ترین زمان‌های استنتاج را در CPUهای موبایل به دست می‌آورد، Google Gemma3 نسبت سرعت-کیفیت متعادلی برای کاربردهای عمومی ارائه می‌دهد، Phi-4-mini-3.8B سرعت استدلال برتر برای وظایف پیچیده ارائه می‌دهد، و BitNET حداکثر توان نظری را با سخت‌افزار تخصصی ارائه می‌دهد.

**نیازهای حافظه**: ردپای حافظه مدل‌ها از Qwen3-0.6B (کمتر از ۱ گیگابایت کوانتیزه‌شده) تا Phi-4-mini-3.8B (تقریباً ۳-۴ گیگابایت کوانتیزه‌شده) متغیر است، با BitNET که ردپای زیر ۵۰۰ مگابایت را در پیکربندی‌های آزمایشی به دست می‌آورد.

## چالش‌ها و ملاحظات

### مصالحه‌های عملکرد

استقرار SLM شامل بررسی دقیق مصالحه‌ها بین اندازه مدل، سرعت استنتاج و کیفیت خروجی است. به عنوان مثال، در حالی که Qwen3-0.6B سرعت و کارایی استثنایی ارائه می‌دهد، Phi-4-mini-3.8B قابلیت‌های استدلال برتر را با هزینه افزایش نیازهای منابع فراهم می‌کند. Google Gemma3 تعادل مناسبی برای اکثر کاربردهای عمومی ایجاد می‌کند.

### سازگاری سخت‌افزار

دستگاه‌های لبه مختلف قابلیت‌ها و محدودیت‌های متفاوتی دارند. Qwen3-0.6B به‌طور کارآمد در پردازنده‌های ARM پایه اجرا می‌شود، Google Gemma3 به منابع محاسباتی متوسط نیاز دارد، و Phi-4-mini-3.8B از سخت‌افزار لبه پیشرفته‌تر بهره می‌برد. مدل‌های BitNET نیاز به سخت‌افزار یا پیاده‌سازی‌های نرم‌افزاری تخصصی برای عملیات ۱ بیتی بهینه دارند.

### امنیت و حریم خصوصی

در حالی که S

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.