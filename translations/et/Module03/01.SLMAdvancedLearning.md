<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-11T11:31:28+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "et"
}
-->
# Section 1: SLM T√§iustatud √ïppimine - Alused ja Optimeerimine

V√§ikesed keelemudelid (SLM-id) t√§histavad olulist edasiminekut EdgeAI-s, v√µimaldades keerukaid loomuliku keele t√∂√∂tlemise v√µimeid piiratud ressurssidega seadmetel. SLM-ide t√µhus kasutamine, optimeerimine ja rakendamine on h√§davajalik praktiliste servap√µhiste AI-lahenduste loomiseks.

## Sissejuhatus

Selles √µppet√ºkis uurime v√§ikeseid keelemudeleid (SLM-e) ja nende t√§iustatud rakendusstrateegiaid. K√§sitleme SLM-ide p√µhikontseptsioone, nende parameetrite piire ja klassifikatsioone, optimeerimistehnikaid ning praktilisi rakendusstrateegiaid serva arvutuskeskkondades.

## √ïppe-eesm√§rgid

Selle √µppet√ºki l√µpuks suudate:

- üî¢ M√µista v√§ikeste keelemudelite parameetrite piire ja klassifikatsioone.
- üõ†Ô∏è Tuvastada peamised optimeerimistehnikad SLM-ide rakendamiseks servaseadmetel.
- üöÄ √ïppida rakendama t√§iustatud kvantiseerimis- ja tihendamisstrateegiaid SLM-ide jaoks.

## SLM-i parameetrite piiride ja klassifikatsioonide m√µistmine

V√§ikesed keelemudelid (SLM-id) on AI-mudelid, mis on loodud loomuliku keele sisu t√∂√∂tlemiseks, m√µistmiseks ja genereerimiseks oluliselt v√§iksema parameetrite arvuga kui nende suured vasted. Kui suurtel keelemudelitel (LLM-id) on sadu miljardeid kuni triljoneid parameetreid, siis SLM-id on spetsiaalselt loodud efektiivsuse ja serva rakendamise jaoks.

Parameetrite klassifikatsiooni raamistik aitab meil m√µista SLM-ide erinevaid kategooriaid ja nende sobivaid kasutusjuhtumeid. See klassifikatsioon on oluline √µige mudeli valimiseks konkreetsetes serva arvutuskeskkondades.

### Parameetrite klassifikatsiooni raamistik

Parameetrite piiride m√µistmine aitab valida sobivaid mudeleid erinevate serva arvutuskeskkondade jaoks:

- **üî¨ Mikro SLM-id**: 100M - 1,4B parameetrit (√ºliv√§ikesed mobiilseadmete jaoks)
- **üì± V√§ikesed SLM-id**: 1,5B - 13,9B parameetrit (tasakaalustatud j√µudlus ja efektiivsus)
- **‚öñÔ∏è Keskmised SLM-id**: 14B - 30B parameetrit (l√§henedes LLM-i v√µimekusele, s√§ilitades samas efektiivsuse)

T√§psed piirid j√§√§vad teadusringkondades muutuvaks, kuid enamik praktikutest peab mudeleid, millel on v√§hem kui 30 miljardit parameetrit, "v√§ikesteks", kusjuures m√µned allikad seavad l√§ve isegi madalamale, n√§iteks 10 miljardi parameetri juurde.

### SLM-ide peamised eelised

SLM-id pakuvad mitmeid p√µhilisi eeliseid, mis muudavad need ideaalseks serva arvutusrakenduste jaoks:

**Operatiivne efektiivsus**: SLM-id tagavad kiirema j√§reldamise t√§nu v√§iksemale parameetrite arvule, muutes need ideaalseks reaalajas rakenduste jaoks. Nad vajavad v√§hem arvutusressursse, v√µimaldades rakendamist piiratud ressurssidega seadmetel, tarbides v√§hem energiat ja s√§ilitades v√§iksema s√ºsinikujalaj√§lje.

**Rakendamise paindlikkus**: Need mudelid v√µimaldavad seadmesisesed AI-v√µimekused ilma interneti√ºhenduse n√µudeta, parandavad privaatsust ja turvalisust kohaliku t√∂√∂tlemise kaudu, neid saab kohandada valdkonnap√µhiste rakenduste jaoks ning need sobivad erinevatesse serva arvutuskeskkondadesse.

**Kulut√µhusus**: SLM-id pakuvad kulut√µhusat treenimist ja rakendamist v√µrreldes LLM-idega, v√§hendades operatiivkulusid ja madalamaid ribalaiuse n√µudeid serva rakenduste jaoks.

## T√§iustatud mudelite hankimise strateegiad

### Hugging Face √∂kos√ºsteem

Hugging Face on peamine keskus tipptasemel SLM-ide avastamiseks ja neile juurdep√§√§suks. Platvorm pakub p√µhjalikke ressursse mudelite avastamiseks ja rakendamiseks:

**Mudelite avastamise funktsioonid**: Platvorm pakub t√§iustatud filtreerimist parameetrite arvu, litsentsit√º√ºbi ja j√µudlusm√µ√µdikute j√§rgi. Kasutajad saavad kasutada k√µrvuti mudelite v√µrdlust√∂√∂riistu, reaalajas j√µudlusn√§itajaid ja hindamistulemusi ning WebGPU demosid koheseks testimiseks.

**Kureeritud SLM-i kogud**: Populaarsed mudelid h√µlmavad Phi-4-mini-3.8B-d t√§iustatud arutlus√ºlesannete jaoks, Qwen3 seeriat (0.6B/1.7B/4B) mitmekeelsete rakenduste jaoks, Google Gemma3 √ºldotstarbeliste √ºlesannete jaoks ja eksperimentaalseid mudeleid nagu BitNET √ºliv√§ikese t√§psusega rakenduste jaoks. Platvormil on ka kogukonna juhitud kogud spetsialiseeritud mudelite jaoks, mis on optimeeritud erinevate kasutusjuhtumite jaoks.

### Azure AI Foundry mudelikataloog

Azure AI Foundry mudelikataloog pakub ettev√µtte tasemel juurdep√§√§su SLM-idele koos t√§iustatud integreerimisv√µimalustega:

**Ettev√µtte integreerimine**: Kataloog sisaldab mudeleid, mida Azure m√º√ºb otse koos ettev√µtte tasemel toe ja SLA-dega, sealhulgas Phi-4-mini-3.8B t√§iustatud arutlusv√µimekuse jaoks ja Llama 3-8B tootmisrakenduste jaoks. Samuti sisaldab see mudeleid, sealhulgas Qwen3 8B usaldusv√§√§rsetelt kolmandate osapoolte avatud l√§htekoodiga mudelitelt.

**Ettev√µtte eelised**: Sisseehitatud t√∂√∂riistad peenh√§√§lestamiseks, j√§lgitavuseks ja vastutustundlikuks AI-ks on integreeritud mudeliperekondade vahelise paindliku Provisioned Throughput'iga. Microsofti otsene tugi koos ettev√µtte SLA-dega, integreeritud turvalisuse ja vastavuse funktsioonid ning p√µhjalikud rakendusvood parandavad ettev√µtte kogemust.

## T√§iustatud kvantiseerimis- ja optimeerimistehnikad

### Llama.cpp optimeerimisraamistik

Llama.cpp pakub tipptasemel kvantiseerimistehnikaid maksimaalse efektiivsuse saavutamiseks serva rakendamisel:

**Kvantiseerimismeetodid**: Raamistik toetab erinevaid kvantiseerimistasemeid, sealhulgas Q4_0 (4-bitine kvantiseerimine suurep√§rase suuruse v√§hendamisega - ideaalne Qwen3-0.6B mobiilirakenduste jaoks), Q5_1 (5-bitine kvantiseerimine tasakaalustades kvaliteeti ja tihendamist - sobib Phi-4-mini-3.8B serva j√§reldamiseks) ja Q8_0 (8-bitine kvantiseerimine peaaegu originaalkvaliteedi jaoks - soovitatav Google Gemma3 tootmisrakenduste jaoks). BitNET t√§histab tipptaset 1-bitise kvantiseerimisega √§√§rmuslike tihendamisstsenaariumide jaoks.

**Rakendamise eelised**: CPU-optimeeritud j√§reldamine SIMD kiirendusega tagab m√§lus√§√§stliku mudeli laadimise ja t√§itmise. Platvormidevaheline √ºhilduvus x86, ARM ja Apple Silicon arhitektuuride vahel v√µimaldab riistvarast s√µltumatut rakendamist.

**Praktiline rakendamise n√§ide**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**M√§lukasutuse v√µrdlus**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive optimeerimiskomplekt

Microsoft Olive pakub p√µhjalikke mudeli optimeerimise t√∂√∂vooge, mis on m√µeldud tootmiskeskkondade jaoks:

**Optimeerimistehnikad**: Komplekt sisaldab d√ºnaamilist kvantiseerimist automaatse t√§psuse valikuga (eriti t√µhus Qwen3 seeria mudelite puhul), graafiku optimeerimist ja operaatorite √ºhendamist (optimeeritud Google Gemma3 arhitektuuri jaoks), riistvaraspetsiifilisi optimeerimisi CPU, GPU ja NPU jaoks (eriline tugi Phi-4-mini-3.8B jaoks ARM-seadmetel) ning mitmeastmelisi optimeerimisvooge. BitNET mudelid vajavad spetsialiseeritud 1-bitise kvantiseerimise t√∂√∂vooge Olive raamistikus.

**T√∂√∂voo automatiseerimine**: Optimeerimisvariantide automaatne v√µrdlus tagab kvaliteedim√µ√µdikute s√§ilimise optimeerimise ajal. Integreerimine populaarsete ML-raamistikega nagu PyTorch ja ONNX pakub pilve- ja serva rakenduste optimeerimisv√µimalusi.

**Praktiline rakendamise n√§ide**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX raamistik

Apple MLX pakub natiivset optimeerimist, mis on spetsiaalselt loodud Apple Silicon seadmete jaoks:

**Apple Silicon optimeerimine**: Raamistik kasutab √ºhtset m√§lustruktuuri Metal Performance Shaders integratsiooniga, automaatset segat√§psuse j√§reldamist (eriti t√µhus Google Gemma3 puhul) ja optimeeritud m√§luriba kasutamist. Phi-4-mini-3.8B n√§itab erakordset j√µudlust M-seeria kiipidel, samas kui Qwen3-1.7B pakub optimaalset tasakaalu MacBook Air rakenduste jaoks.

**Arendusfunktsioonid**: Python ja Swift API tugi NumPy-√ºhilduvate massiivioperatsioonidega, automaatse diferentseerimise v√µimalused ja sujuv integreerimine Apple arendust√∂√∂riistadega pakuvad terviklikku arenduskeskkonda.

**Praktiline rakendamise n√§ide**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Tootmisrakendamine ja j√§reldusstrateegiad

### Ollama: Lihtsustatud kohalik rakendamine

Ollama lihtsustab SLM-ide rakendamist ettev√µtte tasemel funktsioonidega kohalikes ja servakeskkondades:

**Rakendamise v√µimalused**: √úhe k√§suga mudeli installimine ja t√§itmine automaatse mudeli t√µmbamise ja vahem√§llu salvestamisega. Tugi Phi-4-mini-3.8B-le, kogu Qwen3 seeriale (0.6B/1.7B/4B) ja Google Gemma3-le koos REST API-ga rakenduste integreerimiseks ning mitme mudeli haldamise ja vahetamise v√µimalused. BitNET mudelid vajavad eksperimentaalseid ehituse konfiguratsioone 1-bitise kvantiseerimise toetamiseks.

**T√§iustatud funktsioonid**: Kohandatud mudeli peenh√§√§lestamise tugi, Dockerfile'i genereerimine konteineriseeritud rakendamiseks, GPU kiirendus automaatse tuvastamisega ning mudeli kvantiseerimise ja optimeerimise valikud pakuvad terviklikku rakendamise paindlikkust.

### VLLM: K√µrge j√µudlusega j√§reldus

VLLM pakub tootmiskvaliteediga j√§relduse optimeerimist k√µrge l√§bilaskev√µimega stsenaariumide jaoks:

**J√µudluse optimeerimine**: PagedAttention m√§lus√§√§stlikuks t√§helepanu arvutamiseks (eriti kasulik Phi-4-mini-3.8B transformeeriva arhitektuuri jaoks), d√ºnaamiline partiide moodustamine l√§bilaskev√µime optimeerimiseks (optimeeritud Qwen3 seeria paralleelse t√∂√∂tlemise jaoks), tensorite paralleelsus mitme GPU skaleerimiseks (Google Gemma3 tugi) ja spekulatiivne dekodeerimine latentsuse v√§hendamiseks. BitNET mudelid vajavad spetsialiseeritud j√§relduskerneli 1-bitiste operatsioonide jaoks.

**Ettev√µtte integreerimine**: OpenAI-√ºhilduvad API l√µpp-punktid, Kubernetes rakendamise tugi, j√§lgimise ja j√§lgitavuse integreerimine ning automaatse skaleerimise v√µimalused pakuvad ettev√µtte tasemel rakenduslahendusi.

### Foundry Local: Microsofti servalahendus

Foundry Local pakub terviklikke serva rakendamise v√µimalusi ettev√µtte keskkondade jaoks:

**Serva arvutamise funktsioonid**: Offline-esimene arhitektuuri disain ressursipiirangute optimeerimisega, kohaliku mudeli registri haldamine ja serva-pilve s√ºnkroniseerimise v√µimalused tagavad usaldusv√§√§rse serva rakendamise.

**Turvalisus ja vastavus**: Kohalik andmet√∂√∂tlus privaatsuse s√§ilitamiseks, ettev√µtte turvakontrollid, auditi logimine ja vastavuse aruandlus ning rollip√µhine juurdep√§√§su haldamine pakuvad terviklikku turvalisust serva rakenduste jaoks.

## Parimad praktikad SLM-i rakendamiseks

### Mudeli valiku juhised

SLM-ide valimisel serva rakendamiseks arvestage j√§rgmisi tegureid:

**Parameetrite arvu kaalutlused**: Valige mikro SLM-id nagu Qwen3-0.6B √ºliv√§ikeste mobiilirakenduste jaoks, v√§ikesed SLM-id nagu Qwen3-1.7B v√µi Google Gemma3 tasakaalustatud j√µudluse stsenaariumide jaoks ning keskmised SLM-id nagu Phi-4-mini-3.8B v√µi Qwen3-4B, kui l√§henete LLM-i v√µimekusele, s√§ilitades samas efektiivsuse. BitNET mudelid pakuvad eksperimentaalset √ºlitihendust konkreetsete uurimisrakenduste jaoks.

**Kasutusjuhtumi vastavus**: Sobitage mudeli v√µimekused konkreetsete rakendusn√µuetega, arvestades selliseid tegureid nagu vastuse kvaliteet, j√§relduse kiirus, m√§lupiirangud ja offline t√∂√∂ n√µuded.

### Optimeerimisstrateegia valik

**Kvantiseerimise l√§henemine**: Valige sobivad kvantiseerimistasemed kvaliteedin√µuete ja riistvarapiirangute alusel. Arvestage Q4_0 maksimaalse tihenduse jaoks (ideaalne Qwen3-0.6B mobiilirakenduste jaoks), Q5_1 tasakaalustatud kvaliteedi-tihenduse kompromisside jaoks (sobib Phi-4-mini-3.8B ja Google Gemma3 jaoks) ning Q8_0 peaaegu originaalkvaliteedi s√§ilitamiseks (soovitatav Qwen3-4B tootmiskeskkondade jaoks). BitNET-i 1-bitine kvantiseerimine t√§histab √§√§rmusliku tihenduse piiri spetsialiseeritud rakenduste jaoks.

**Raamistiku valik**: Valige optimeerimisraamistikud sihtriistvara ja rakendusn√µuete alusel. Kasutage Llama.cpp CPU-optimeeritud rakendamiseks, Microsoft Olive'i terviklikeks optimeerimisvoogudeks ja Apple MLX-i Apple Silicon seadmete jaoks.

## Praktilised mudelin√§ited ja kasutusjuhtumid

### Reaalsed rakendusstsenaariumid

**Mobiilirakendused**: Qwen3-0.6B paistab silma nutitelefoni vestlusrobotirakendustes minimaalse m√§lukasutusega, samas kui Google Gemma3 pakub tasakaalustatud j√µudlust tahvelarvutip√µhiste haridust√∂√∂riistade jaoks. Phi-4-mini-3.8B pakub suurep√§raseid arutlusv√µimekusi mobiilsete tootlikkusrakenduste jaoks.

**Lauaarvuti ja serva arvutus**: Qwen3-1.7B tagab optimaalse j√µudluse lauaarvuti assistendi rakendustes, Phi-4-mini-3.8B pakub t√§iustatud koodigeneratsiooni v√µimekusi arendust√∂√∂riistade jaoks ning Qwen3-4B v√µimaldab keerukat dokumendianal√º√ºsi t√∂√∂jaamakeskkondades.

**Uurimuslik ja eksperimentaalne**: BitNET mudelid v√µimaldavad uurida √ºliv√§ikese t√§psusega j√§reldusi akadeemiliste uuringute ja kontseptsioonit√µestuse rakenduste jaoks, mis n√µuavad √§√§rmuslikke ressursipiiranguid.

### J√µudlusn√§itajad ja v√µrdlused

**J√§relduse kiirus**: Qwen3-0.6B saavutab kiireima j√§relduskiiruse mobiilsetel CPU-del, Google Gemma3 pakub tasakaalustatud kiirus-kvaliteedi suhet √ºldrakenduste jaoks, Phi-4-mini-3.8B pakub suurep√§raseid arutluskiirusi keerukate √ºlesannete jaoks ning BitNET

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.