<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-10-11T11:35:52+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "et"
}
-->
# Jaotis 2: Kohaliku keskkonna juurutamine - privaatsust esikohale seadvad lahendused

Väikeste keelemudelite (SLM) kohalik juurutamine tähistab paradigmanihet privaatsust säilitavate ja kulutõhusate tehisintellekti lahenduste suunas. See põhjalik juhend tutvustab kahte võimsat raamistikku—Ollama ja Microsoft Foundry Local—mis võimaldavad arendajatel kasutada SLM-i täielikku potentsiaali, säilitades samal ajal täieliku kontrolli oma juurutuskeskkonna üle.

## Sissejuhatus

Selles õppetükis uurime väikeste keelemudelite arenenud juurutusstrateegiaid kohalikes keskkondades. Käsitleme kohaliku AI juurutamise põhimõisteid, tutvume kahe juhtiva platvormiga (Ollama ja Microsoft Foundry Local) ning pakume praktilisi juhiseid tootmisvalmis lahenduste rakendamiseks.

## Õpieesmärgid

Selle õppetüki lõpuks suudate:

- Mõista kohalike SLM-i juurutusraamistike arhitektuuri ja eeliseid.
- Rakendada tootmisvalmis juurutusi, kasutades Ollama ja Microsoft Foundry Local platvorme.
- Võrrelda ja valida sobiv platvorm vastavalt konkreetsetele nõuetele ja piirangutele.
- Optimeerida kohalikke juurutusi jõudluse, turvalisuse ja skaleeritavuse jaoks.

## Kohalike SLM-i juurutusarhitektuuride mõistmine

Kohalik SLM-i juurutamine tähistab põhimõttelist nihet pilvepõhistest AI-teenustest kohapealsetele, privaatsust säilitavatele lahendustele. See lähenemine võimaldab organisatsioonidel säilitada täielikku kontrolli oma AI infrastruktuuri üle, tagades samal ajal andmesuveräänsuse ja operatiivse sõltumatuse.

### Juurutusraamistike klassifikatsioonid

Erinevate juurutusviiside mõistmine aitab valida konkreetsete kasutusjuhtude jaoks sobiva strateegia:

- **Arendusele keskendunud**: Lihtsustatud seadistus katsetamiseks ja prototüüpide loomiseks
- **Ettevõtte tasemel**: Tootmisvalmis lahendused koos ettevõtte integreerimisvõimalustega  
- **Platvormideülene**: Universaalne ühilduvus erinevate operatsioonisüsteemide ja riistvaraga

### Kohaliku SLM-i juurutamise peamised eelised

Kohalik SLM-i juurutamine pakub mitmeid põhilisi eeliseid, mis muudavad selle ideaalseks ettevõtte ja privaatsustundlike rakenduste jaoks:

**Privaatsus ja turvalisus**: Kohalik töötlemine tagab, et tundlikud andmed ei lahku organisatsiooni infrastruktuurist, võimaldades vastavust GDPR-i, HIPAA ja teiste regulatiivsete nõuetega. Õhuga eraldatud juurutused on võimalikud salastatud keskkondades, samas kui täielikud auditeerimisjäljed säilitavad turvaülevaate.

**Kulutõhusus**: Märgipõhiste hinnamudelite kaotamine vähendab oluliselt tegevuskulusid. Madalamad ribalaiuse nõuded ja vähendatud pilvesõltuvus pakuvad prognoositavaid kulustruktuure ettevõtte eelarvestamiseks.

**Jõudlus ja töökindlus**: Kiirem järeldusaeg ilma võrgu latentsuseta võimaldab reaalajas rakendusi. Võrguta funktsionaalsus tagab pideva töö olenemata internetiühendusest, samas kui kohalike ressursside optimeerimine pakub järjepidevat jõudlust.

## Ollama: universaalne kohaliku juurutamise platvorm

### Põhiarhitektuur ja filosoofia

Ollama on loodud universaalseks, arendajasõbralikuks platvormiks, mis demokratiseerib kohaliku LLM-i juurutamise mitmekesiste riistvarakonfiguratsioonide ja operatsioonisüsteemide vahel.

**Tehniline alus**: Tuginedes tugevale llama.cpp raamistikule, kasutab Ollama tõhusat GGUF mudeliformaati optimaalse jõudluse saavutamiseks. Platvormideülene ühilduvus tagab järjepideva käitumise Windowsi, macOS-i ja Linuxi keskkondades, samas kui intelligentne ressursihaldus optimeerib CPU, GPU ja mälu kasutamist.

**Disainifilosoofia**: Ollama seab esikohale lihtsuse, ohverdamata funktsionaalsust, pakkudes nullkonfiguratsiooniga juurutamist koheseks produktiivsuseks. Platvorm säilitab laia mudeli ühilduvuse, pakkudes samas järjepidevaid API-sid erinevate mudeliarhitektuuride jaoks.

### Täiustatud funktsioonid ja võimalused

**Mudelite haldamise tipptase**: Ollama pakub terviklikku mudeli elutsükli haldust automaatse allalaadimise, vahemällu salvestamise ja versioonihaldusega. Platvorm toetab ulatuslikku mudelite ökosüsteemi, sealhulgas Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral ja spetsialiseeritud sisseehitatud mudeleid.

**Kohandamine modelfailide kaudu**: Edasijõudnud kasutajad saavad luua kohandatud mudelikonfiguratsioone konkreetsete parameetrite, süsteemiküsimuste ja käitumise muutmisega. See võimaldab valdkonnaspetsiifilisi optimeerimisi ja spetsialiseeritud rakendusnõudeid.

**Jõudluse optimeerimine**: Ollama tuvastab ja kasutab automaatselt olemasolevat riistvarakiirendust, sealhulgas NVIDIA CUDA, Apple Metal ja OpenCL. Intelligentne mäluhaldus tagab optimaalse ressursside kasutamise erinevates riistvarakonfiguratsioonides.

### Tootmise rakendamise strateegiad

**Paigaldamine ja seadistamine**: Ollama pakub lihtsustatud paigaldamist platvormide vahel, kasutades natiivseid paigaldajaid, paketihaldureid (WinGet, Homebrew, APT) ja Docker konteinerid konteineriseeritud juurutuste jaoks.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Olulised käsud ja toimingud**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Täiustatud konfiguratsioon**: Modelfailid võimaldavad keerukat kohandamist ettevõtte nõuete jaoks:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Arendaja integreerimise näited

**Python API integreerimine**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript integreerimine (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API kasutamine cURL-iga**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Jõudluse häälestamine ja optimeerimine

**Mälu ja lõimede konfiguratsioon**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Kvantiseerimise valik erineva riistvara jaoks**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: ettevõtte serva AI platvorm

### Ettevõtte tasemel arhitektuur

Microsoft Foundry Local esindab terviklikku ettevõtte lahendust, mis on spetsiaalselt loodud tootmise serva AI juurutuste jaoks, sügava integreerimisega Microsofti ökosüsteemi.

**ONNX-põhine alus**: Tuginedes tööstusharu standardile ONNX Runtime, pakub Foundry Local optimeeritud jõudlust mitmekesiste riistvaraarhitektuuride vahel. Platvorm kasutab Windows ML integratsiooni Windowsi natiivseks optimeerimiseks, säilitades samal ajal platvormideülese ühilduvuse.

**Riistvarakiirenduse tipptase**: Foundry Local sisaldab intelligentset riistvara tuvastamist ja optimeerimist CPU-de, GPU-de ja NPU-de vahel. Sügav koostöö riistvaratootjatega (AMD, Intel, NVIDIA, Qualcomm) tagab optimaalse jõudluse ettevõtte riistvarakonfiguratsioonides.

### Täiustatud arendajakogemus

**Mitme liidese juurdepääs**: Foundry Local pakub terviklikke arendusliideseid, sealhulgas võimas CLI mudelite haldamiseks ja juurutamiseks, mitmekeelsed SDK-d (Python, NodeJS) natiivseks integreerimiseks ja RESTful API-d OpenAI ühilduvusega sujuvaks migratsiooniks.

**Visual Studio integratsioon**: Platvorm integreerub sujuvalt AI Toolkitiga VS Code jaoks, pakkudes mudeli konverteerimise, kvantiseerimise ja optimeerimise tööriistu arenduskeskkonnas. See integratsioon kiirendab arendusvooge ja vähendab juurutamise keerukust.

**Mudeli optimeerimise torujuhe**: Microsoft Olive integratsioon võimaldab keerukaid mudeli optimeerimise töövooge, sealhulgas dünaamilist kvantiseerimist, graafiku optimeerimist ja riistvaraspetsiifilist häälestamist. Pilvepõhised konverteerimisvõimalused Azure ML kaudu pakuvad skaleeritavat optimeerimist suurte mudelite jaoks.

### Tootmise rakendamise strateegiad

**Paigaldamine ja konfiguratsioon**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Mudelite haldamise toimingud**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Täiustatud juurutamise konfiguratsioon**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Ettevõtte ökosüsteemi integreerimine

**Turvalisus ja vastavus**: Foundry Local pakub ettevõtte tasemel turvafunktsioone, sealhulgas rollipõhist juurdepääsu kontrolli, auditeerimislogisid, vastavusaruandlust ja krüpteeritud mudeli salvestust. Integreerimine Microsofti turvainfrastruktuuriga tagab vastavuse ettevõtte turvapoliitikatele.

**Sisseehitatud AI teenused**: Platvorm pakub valmis AI võimalusi, sealhulgas Phi Silica kohaliku keele töötlemiseks, AI Imaging pilditöötluseks ja analüüsiks ning spetsialiseeritud API-sid tavapäraste ettevõtte AI ülesannete jaoks.

## Võrdlev analüüs: Ollama vs Foundry Local

### Tehnilise arhitektuuri võrdlus

| **Aspekt** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Mudeli formaat** | GGUF (läbi llama.cpp) | ONNX (läbi ONNX Runtime) |
| **Platvormi fookus** | Universaalne platvormideülene | Windows/ettevõtte optimeerimine |
| **Riistvara integreerimine** | Üldine GPU/CPU tugi | Sügav Windows ML, NPU tugi |
| **Optimeerimine** | llama.cpp kvantiseerimine | Microsoft Olive + ONNX Runtime |
| **Ettevõtte funktsioonid** | Kogukonna juhitud | Ettevõtte tasemel SLA-dega |

### Jõudlusomadused

**Ollama jõudluse tugevused**:
- Erakordne CPU jõudlus läbi llama.cpp optimeerimise
- Järjepidev käitumine erinevatel platvormidel ja riistvaral
- Tõhus mälukasutus intelligentse mudeli laadimisega
- Kiired külmkäivituse ajad arenduse ja testimise stsenaariumide jaoks

**Foundry Local jõudluse eelised**:
- Ülim NPU kasutamine kaasaegsel Windowsi riistvaral
- Optimeeritud GPU kiirendus läbi tootjate partnerluste
- Ettevõtte tasemel jõudluse jälgimine ja optimeerimine
- Skaleeritavad juurutusvõimalused tootmiskeskkondade jaoks

### Arendajakogemuse analüüs

**Ollama arendajakogemus**:
- Minimaalsed seadistusnõuded kohese produktiivsuse jaoks
- Intuitiivne käsurealiides kõigi toimingute jaoks
- Ulatuslik kogukonna tugi ja dokumentatsioon
- Paindlik kohandamine modelfailide kaudu

**Foundry Local arendajakogemus**:
- Terviklik IDE integratsioon Visual Studio ökosüsteemiga
- Ettevõtte arendusvood meeskonnatöö funktsioonidega
- Professionaalsed tugikanalid Microsofti toel
- Täiustatud silumise ja optimeerimise tööriistad

### Kasutusjuhtude optimeerimine

**Vali Ollama, kui**:
- Arendad platvormideüleseid rakendusi, mis vajavad järjepidevat käitumist
- Eelistad avatud lähtekoodi läbipaistvust ja kogukonna panust
- Töötad piiratud ressursside või eelarvega
- Loote eksperimentaalseid või teaduslikke rakendusi
- Vajad laia mudeli ühilduvust erinevate arhitektuuride vahel

**Vali Foundry Local, kui**:
- Juurutad ettevõtte rakendusi, millel on ranged jõudlusnõuded
- Kasutad Windowsi-spetsiifilisi riistvara optimeerimisi (NPU, Windows ML)
- Vajad ettevõtte tuge, SLA-sid ja vastavusfunktsioone
- Loote tootmisrakendusi Microsofti ökosüsteemi integreerimisega
- Vajad täiustatud optimeerimistööriistu ja professionaalseid arendusvooge

## Täiustatud juurutusstrateegiad

### Konteineriseeritud juurutusmustrid

**Ollama konteineriseerimine**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local ettevõtte juurutus**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Jõudluse optimeerimise tehnikad

**Ollama optimeerimisstrateegiad**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local optimeerimine**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Turvalisuse ja vastavuse kaalutlused

### Ettevõtte turvalisuse rakendamine

**Ollama turvalisuse parimad tavad**:
- Võrgu isolatsioon tulemüürireeglite ja VPN-i juurdepääsuga
- Autentimine läbi pöördproksi integratsiooni
- Mudeli terviklikkuse kontroll ja turvaline mudeli levitamine
- API juurdepääsu ja mudeli toimingute auditeerimislogid

**Foundry Local ettevõtte turvalisus**:
- Sisseehitatud rollipõhine juurdepääsu kontroll Active Directory integratsiooniga
- Terviklikud auditeerimisjäljed vastavusaruandlusega
- Krüpteeritud mudeli salvestus ja turvaline mudeli juurutamine
- Integreerimine Microsofti turvainfrastruktuuriga

### Vastavus ja regulatiivsed nõuded

Mõlemad platvormid toetavad regulatiivset vastavust läbi:
- Andmete asukoha kontrollid, mis tagavad kohaliku töötlemise
- Auditeerimislogid regulatiivsete aruandlusnõuete jaoks
- Juurdepääsukontrollid tundlike andmete käsitlemiseks
- Krüpteerimine nii salvestamisel kui edastamisel andmete kaitseks

## Parimad tavad tootmise juurutamiseks

### Jälgimine ja nähtavus

**Olulised mõõdikud jälgimiseks**:
- Mudeli järelduse latentsus ja läbilaskevõime
- Ressursside kasutamine (CPU, GPU, mälu)
- API vastuseajad ja veamäärad
- Mudeli täpsus ja jõudluse kõrvalekalded

**Jälgimise rakendamine**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Pidev integreerimine ja juurutamine

**CI/CD torujuhtme integreerimine**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Tulevikutrendid ja kaalutlused

### Tekkivad tehnoloogiad

Kohaliku SLM-i juurutamise maastik areneb jätkuvalt mitmete võtmetrendidega:

**Täiustatud mudeliarhitektuurid**: Järgmise põlvkonna SLM-id, millel on paranenud efektiivsus ja võimekuse suhtarvud, sealhulgas ekspertide segumudelid dünaamiliseks skaleerimiseks ja spetsialiseeritud arhitektuurid serva juurutamiseks.

**Riistvara integreerimine**: Sügavam integreerimine spetsialiseeritud AI riistvaraga, sealhulgas NPU-d, kohandatud kiibid ja serva arvutuskiirendid, pakub täiustatud jõudlusvõimalusi.

**Ökosüsteemi areng**: Standardiseerimispüüdlused juurutusplatvormide vahel ja parem koostalitlusvõime erinevate raamistikute vahel lihtsustavad mitme platvormi juurutusi.

### Tööstuse kasutuselevõtu mustrid

**Ettevõtte kasutuselevõtt**: Suurenev ettevõtte kasutuselevõtt, mida ajendavad privaatsusnõuded, kulude optimeerimine ja regulatiivsete nõuete vajadused. Valitsuse ja kaitse sektorid keskenduvad eriti õhuga eraldatud juurutustele.

**Globaalne kaalutlus**: Rahvusvahelised andmesuveräänsuse nõuded ajendavad kohaliku juurutamise kasut

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.