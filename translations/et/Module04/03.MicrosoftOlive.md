<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-10-11T11:41:39+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "et"
}
-->
# Jaotis 3: Microsoft Olive optimeerimiskomplekt

## Sisukord
1. [Sissejuhatus](../../../Module04)
2. [Mis on Microsoft Olive?](../../../Module04)
3. [Paigaldamine](../../../Module04)
4. [Kiire alustamise juhend](../../../Module04)
5. [Näide: Qwen3 teisendamine ONNX INT4-ks](../../../Module04)
6. [Täpsem kasutamine](../../../Module04)
7. [Parimad praktikad](../../../Module04)
8. [Tõrkeotsing](../../../Module04)
9. [Täiendavad ressursid](../../../Module04)

## Sissejuhatus

Microsoft Olive on võimas ja kasutajasõbralik riistvarateadlik mudelite optimeerimise tööriistakomplekt, mis lihtsustab masinõppemudelite optimeerimist erinevate riistvaraplatvormide jaoks. Olenemata sellest, kas sihite protsessoreid, graafikakaarte või spetsiaalseid tehisintellekti kiirendeid, aitab Olive saavutada optimaalset jõudlust, säilitades samal ajal mudeli täpsuse.

## Mis on Microsoft Olive?

Olive on lihtsasti kasutatav riistvarateadlik mudelite optimeerimise tööriist, mis ühendab juhtivaid tehnikaid mudelite tihendamise, optimeerimise ja kompileerimise valdkonnas. See töötab koos ONNX Runtime'iga kui terviklik järelduste optimeerimise lahendus.

### Põhifunktsioonid

- **Riistvarateadlik optimeerimine**: Valib automaatselt teie sihtriistvarale parimad optimeerimistehnikad
- **40+ sisseehitatud optimeerimiskomponenti**: Hõlmab mudelite tihendamist, kvantiseerimist, graafiku optimeerimist ja palju muud
- **Lihtne CLI-liides**: Lihtsad käsud tavaliste optimeerimisülesannete jaoks
- **Mitme raamistiku tugi**: Ühildub PyTorch, Hugging Face mudelite ja ONNX-iga
- **Populaarsete mudelite tugi**: Olive suudab automaatselt optimeerida populaarseid mudeliarhitektuure nagu Llama, Phi, Qwen, Gemma jne

### Eelised

- **Lühem arendusaeg**: Ei ole vaja käsitsi katsetada erinevaid optimeerimistehnikaid
- **Jõudluse paranemine**: Märkimisväärne kiiruse kasv (mõnel juhul kuni 6x)
- **Platvormideülene kasutus**: Optimeeritud mudelid töötavad erinevatel riistvaradel ja operatsioonisüsteemidel
- **Säilinud täpsus**: Optimeerimised säilitavad mudeli kvaliteedi, parandades samal ajal jõudlust

## Paigaldamine

### Eeltingimused

- Python 3.8 või uuem
- pip pakettide haldur
- Virtuaalne keskkond (soovitatav)

### Põhipaigaldus

Looge ja aktiveerige virtuaalne keskkond:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Paigaldage Olive koos automaatse optimeerimise funktsioonidega:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valikulised sõltuvused

Olive pakub erinevaid valikulisi sõltuvusi lisafunktsioonide jaoks:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Paigalduse kontrollimine

```bash
olive --help
```

Kui paigaldus õnnestus, peaksite nägema Olive CLI abisõnumit.

## Kiire alustamise juhend

### Teie esimene optimeerimine

Optimeerime väikese keelemudeli, kasutades Olive'i automaatse optimeerimise funktsiooni:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mida see käsk teeb

Optimeerimisprotsess hõlmab: mudeli hankimist kohalikust vahemälust, ONNX graafiku jäädvustamist ja kaalude salvestamist ONNX andmefaili, ONNX graafiku optimeerimist ja mudeli kvantiseerimist int4-ks RTN meetodi abil.

### Käsu parameetrite selgitus

- `--model_name_or_path`: Hugging Face mudeli identifikaator või kohalik tee
- `--output_path`: Kataloog, kuhu optimeeritud mudel salvestatakse
- `--device`: Sihtseade (cpu, gpu)
- `--provider`: Täitmise pakkuja (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Kasuta ONNX Runtime Generate AI-d järelduste tegemiseks
- `--precision`: Kvantiseerimise täpsus (int4, int8, fp16)
- `--log_level`: Logimise detailsus (0=minimaalne, 1=üksikasjalik)

## Näide: Qwen3 teisendamine ONNX INT4-ks

Tuginedes Hugging Face näitele [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), on siin juhend Qwen3 mudeli optimeerimiseks:

### Samm 1: Laadige mudel alla (valikuline)

Allalaadimisaja minimeerimiseks salvestage ainult vajalikud failid:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Samm 2: Optimeerige Qwen3 mudel

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Samm 3: Testige optimeeritud mudelit

Looge lihtne Python skript, et testida oma optimeeritud mudelit:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Väljundi struktuur

Pärast optimeerimist sisaldab teie väljundkataloog järgmist:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Täpsem kasutamine

### Konfiguratsioonifailid

Keerukamate optimeerimistöövoogude jaoks saate kasutada JSON-konfiguratsioonifaile:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Käivitage konfiguratsiooniga:

```bash
olive run --config config.json
```

### GPU optimeerimine

CUDA GPU optimeerimiseks:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) jaoks:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mudelite peenhäälestus Olive'iga

Olive toetab ka mudelite peenhäälestust:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Parimad praktikad

### 1. Mudeli valik
- Alustage väiksemate mudelitega testimiseks (nt 0.5B-7B parameetrit)
- Veenduge, et teie sihtmudeli arhitektuur on Olive'iga ühilduv

### 2. Riistvara kaalutlused
- Sobitage oma optimeerimise sihtmärk oma kasutatava riistvaraga
- Kasutage GPU optimeerimist, kui teil on CUDA-ühilduv riistvara
- Kaaluge DirectML-i kasutamist Windowsi masinatel, millel on integreeritud graafika

### 3. Täpsuse valik
- **INT4**: Maksimaalne tihendus, kerge täpsuse kadu
- **INT8**: Hea tasakaal suuruse ja täpsuse vahel
- **FP16**: Minimaalne täpsuse kadu, mõõdukas suuruse vähenemine

### 4. Testimine ja valideerimine
- Testige alati optimeeritud mudeleid oma konkreetsete kasutusjuhtumitega
- Võrrelge jõudlusmõõdikuid (latentsus, läbilaskevõime, täpsus)
- Kasutage hindamiseks esinduslikke sisendandmeid

### 5. Iteratiivne optimeerimine
- Alustage kiirete tulemuste saamiseks automaatse optimeerimisega
- Kasutage konfiguratsioonifaile täpsemaks juhtimiseks
- Katsetage erinevate optimeerimisetappidega

## Tõrkeotsing

### Levinumad probleemid

#### 1. Paigaldusprobleemid
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU probleemid
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Mälu probleemid
- Kasutage optimeerimise ajal väiksemaid partiisuurusi
- Proovige esmalt kvantiseerimist kõrgema täpsusega (int8 int4 asemel)
- Veenduge, et mudeli vahemällu salvestamiseks on piisavalt kettaruumi

#### 4. Mudeli laadimise vead
- Kontrollige mudeli teed ja juurdepääsuõigusi
- Veenduge, et mudel vajab `trust_remote_code=True`
- Kontrollige, kas kõik vajalikud mudelifailid on alla laaditud

### Abi saamine

- **Dokumentatsioon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHubi probleemid**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Näited**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Täiendavad ressursid

### Ametlikud lingid
- **GitHubi hoidla**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime dokumentatsioon**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face näide**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Kogukonna näited
- **Jupyter Notebookid**: Saadaval Olive'i GitHubi hoidlas — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code laiendus**: AI Toolkit for VS Code ülevaade — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogipostitused**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Seotud tööriistad
- **ONNX Runtime**: Kõrge jõudlusega järeldusmootor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Paljude ühilduvate mudelite allikas — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Pilvepõhised optimeerimistöövood — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Mis edasi?

- [04: OpenVINO Toolkit optimeerimiskomplekt](./04.openvino.md)

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.