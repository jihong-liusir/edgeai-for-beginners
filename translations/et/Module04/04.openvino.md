<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-10-11T11:37:02+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "et"
}
-->
# Jaotis 4: OpenVINO tööriistakomplekti optimeerimiskomplekt

## Sisukord
1. [Sissejuhatus](../../../Module04)
2. [Mis on OpenVINO?](../../../Module04)
3. [Paigaldamine](../../../Module04)
4. [Kiire alustamise juhend](../../../Module04)
5. [Näide: mudelite teisendamine ja optimeerimine OpenVINO abil](../../../Module04)
6. [Täpsem kasutamine](../../../Module04)
7. [Parimad tavad](../../../Module04)
8. [Tõrkeotsing](../../../Module04)
9. [Täiendavad ressursid](../../../Module04)

## Sissejuhatus

OpenVINO (Open Visual Inference and Neural Network Optimization) on Inteli avatud lähtekoodiga tööriistakomplekt, mis võimaldab juurutada tõhusaid tehisintellekti lahendusi pilves, kohapeal ja servakeskkondades. Olenemata sellest, kas sihite protsessoreid, graafikaprotsessoreid, VPU-sid või spetsiaalseid tehisintellekti kiirendeid, pakub OpenVINO ulatuslikke optimeerimisvõimalusi, säilitades samal ajal mudelite täpsuse ja võimaldades platvormidevahelist juurutamist.

## Mis on OpenVINO?

OpenVINO on avatud lähtekoodiga tööriistakomplekt, mis võimaldab arendajatel tõhusalt optimeerida, teisendada ja juurutada tehisintellekti mudeleid erinevatel riistvaraplatvormidel. See koosneb kolmest peamisest komponendist: OpenVINO Runtime järelduste tegemiseks, Neural Network Compression Framework (NNCF) mudelite optimeerimiseks ja OpenVINO Model Server skaleeritavaks juurutamiseks.

### Põhifunktsioonid

- **Platvormidevaheline juurutamine**: Toetab Linuxi, Windowsi ja macOS-i koos Python-, C++- ja C-API-dega
- **Riistvarakiirendus**: Automaatne seadmete avastamine ja optimeerimine protsessorite, graafikaprotsessorite, VPU-de ja tehisintellekti kiirendite jaoks
- **Mudelipressimise raamistik**: Täiustatud kvantiseerimis-, kärpimis- ja optimeerimistehnikad NNCF-i kaudu
- **Raamistike ühilduvus**: Otsene tugi TensorFlow, ONNX, PaddlePaddle ja PyTorch mudelitele
- **Generatiivse tehisintellekti tugi**: Spetsiaalne OpenVINO GenAI suurte keelemudelite ja generatiivsete tehisintellekti rakenduste juurutamiseks

### Eelised

- **Jõudluse optimeerimine**: Märkimisväärne kiiruse paranemine minimaalsete täpsuse kadudega
- **Väiksem juurutusmaht**: Minimaalsed välised sõltuvused lihtsustavad paigaldamist ja juurutamist
- **Kiirem käivitusaeg**: Optimeeritud mudeli laadimine ja vahemällu salvestamine kiirema rakenduse käivitamiseks
- **Skaleeritav juurutamine**: Alates servaseadmetest kuni pilveinfrastruktuurini, kasutades ühtseid API-sid
- **Tootmiskõlblik**: Ettevõtte tasemel töökindlus koos põhjaliku dokumentatsiooni ja kogukonna toega

## Paigaldamine

### Eeltingimused

- Python 3.8 või uuem
- pip pakettide haldur
- Virtuaalne keskkond (soovitatav)
- Ühilduv riistvara (soovitatavalt Inteli protsessorid, kuid toetab erinevaid arhitektuure)

### Põhipaigaldus

Loo ja aktiveeri virtuaalne keskkond:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Paigalda OpenVINO Runtime:

```bash
pip install openvino
```

Paigalda NNCF mudelite optimeerimiseks:

```bash
pip install nncf
```

### OpenVINO GenAI paigaldamine

Generatiivse tehisintellekti rakenduste jaoks:

```bash
pip install openvino-genai
```

### Valikulised sõltuvused

Lisapaketid spetsiifilisteks kasutusjuhtudeks:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Paigalduse kontrollimine

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Kui paigaldus õnnestus, kuvatakse OpenVINO versiooni teave.

## Kiire alustamise juhend

### Sinu esimene mudeli optimeerimine

Teisendame ja optimeerime Hugging Face'i mudeli OpenVINO abil:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Mida see protsess teeb

Optimeerimise töövoog hõlmab: algse mudeli laadimist Hugging Face'ist, selle teisendamist OpenVINO vahepealseks esitusvorminguks (IR), vaikimisi optimeerimiste rakendamist ja sihtriistvara jaoks kompileerimist.

### Oluliste parameetrite selgitus

- `export=True`: Teisendab mudeli OpenVINO IR-vormingusse
- `compile=False`: Lükatakse kompileerimine edasi, et tagada paindlikkus
- `device`: Sihtriistvara ("CPU", "GPU", "AUTO" automaatseks valikuks)
- `save_pretrained()`: Salvestab optimeeritud mudeli taaskasutamiseks

## Näide: mudelite teisendamine ja optimeerimine OpenVINO abil

### Samm 1: Mudeli teisendamine NNCF kvantiseerimisega

Siin on, kuidas rakendada kvantiseerimist pärast treenimist NNCF-i abil:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Samm 2: Täpsem optimeerimine kaalu pressimisega

Transformeripõhiste mudelite jaoks rakenda kaalu pressimist:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Samm 3: Järelduste tegemine optimeeritud mudeliga

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Väljundi struktuur

Pärast optimeerimist sisaldab sinu mudelikataloog:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Täpsem kasutamine

### NNCF YAML konfiguratsioon

Keerukamate optimeerimistöövoogude jaoks kasuta NNCF konfiguratsioonifaile:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Rakenda konfiguratsioon:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Graafikaprotsessori optimeerimine

Graafikaprotsessori kiirenduse jaoks:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Partii töötlemise optimeerimine

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Mudeliserveri juurutamine

Juuruta optimeeritud mudelid OpenVINO Model Serveri abil:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Kliendikood mudeliserveri jaoks:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Parimad tavad

### 1. Mudeli valik ja ettevalmistus
- Kasuta mudeleid toetatud raamistikest (PyTorch, TensorFlow, ONNX)
- Veendu, et mudeli sisendid oleksid fikseeritud või teadaoleva dünaamilise kujuga
- Testi kalibreerimiseks esinduslike andmekogumitega

### 2. Optimeerimisstrateegia valik
- **Kvantiseerimine pärast treenimist**: Alusta siit kiireks optimeerimiseks
- **Kaalu pressimine**: Ideaalne suurte keelemudelite ja transformerite jaoks
- **Kvantiseerimisteadlik treenimine**: Kasuta, kui täpsus on kriitiline

### 3. Riistvaraspetsiifiline optimeerimine
- **Protsessor**: Kasuta INT8 kvantiseerimist tasakaalustatud jõudluse jaoks
- **Graafikaprotsessor**: Kasuta FP16 täpsust ja partii töötlemist
- **VPU**: Keskendu mudeli lihtsustamisele ja kihtide ühendamisele

### 4. Jõudluse häälestamine
- **Läbilaske režiim**: Suuremahulise partii töötlemiseks
- **Latentsusrežiim**: Reaalajas interaktiivsete rakenduste jaoks
- **AUTO seade**: Lase OpenVINO-l valida optimaalne riistvara

### 5. Mälu haldamine
- Kasuta dünaamilisi kujusid säästlikult, et vältida mälukulusid
- Rakenda mudeli vahemällu salvestamist kiiremateks laadimisteks
- Jälgi optimeerimise ajal mälukasutust

### 6. Täpsuse valideerimine
- Kontrolli alati optimeeritud mudeleid algse jõudluse suhtes
- Kasuta hindamiseks esinduslikke testandmekogumeid
- Kaalu järkjärgulist optimeerimist (alusta konservatiivsete seadistustega)

## Tõrkeotsing

### Levinud probleemid

#### 1. Paigaldusprobleemid
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Mudeli teisendamise vead
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Jõudlusprobleemid
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Mäluprobleemid
- Vähenda mudeli partii suurust optimeerimise ajal
- Kasuta suurte andmekogumite jaoks voogedastust
- Luba mudeli vahemällu salvestamine: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Täpsuse halvenemine
- Kasuta kõrgemat täpsust (INT8 asemel INT4)
- Suurenda kalibreerimisandmekogumi suurust
- Rakenda segatäpsusega optimeerimist

### Jõudluse jälgimine

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Abi saamine

- **Dokumentatsioon**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHubi probleemid**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Kogukonna foorum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Täiendavad ressursid

### Ametlikud lingid
- **OpenVINO koduleht**: [openvino.ai](https://openvino.ai/)
- **GitHubi hoidla**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF hoidla**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Mudelizoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Õppematerjalid
- **OpenVINO märkmikud**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Kiire alustamise juhend**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimeerimise juhend**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integreerimisvahendid
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Jõudlusnäitajad
- **Ametlikud jõudlusnäitajad**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF mudelizoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Kogukonna näited
- **Jupyteri märkmikud**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Põhjalikud õpetused OpenVINO märkmike hoidlas
- **Näidisrakendused**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Reaalse maailma näited erinevates valdkondades (arvutinägemine, NLP, heli)
- **Blogipostitused**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Inteli tehisintellekti ja kogukonna blogipostitused koos üksikasjalike kasutusjuhtumitega

### Seotud tööriistad
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Täiendavad optimeerimistehnikad Inteli riistvara jaoks
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Võrdluseks mobiili- ja servajuurutuste jaoks
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Platvormidevaheline järeldusmootori alternatiiv

## ➡️ Mis edasi?

- [05: Apple MLX Frameworki süvaanalüüs](./05.AppleMLX.md)

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.