<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-10-11T11:43:23+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "et"
}
-->
# Jaotis 4: Süvavaade Apple MLX raamistikule

## Sisukord
1. [Sissejuhatus Apple MLX-i](../../../Module04)
2. [Peamised funktsioonid LLM-i arendamiseks](../../../Module04)
3. [Paigaldusjuhend](../../../Module04)
4. [MLX-i kasutamise alustamine](../../../Module04)
5. [MLX-LM: Keelemudelid](../../../Module04)
6. [Töötamine suurte keelemudelitega](../../../Module04)
7. [Hugging Face integratsioon](../../../Module04)
8. [Mudelite konverteerimine ja kvantiseerimine](../../../Module04)
9. [Keelemudelite peenhäälestamine](../../../Module04)
10. [Täiustatud LLM-i funktsioonid](../../../Module04)
11. [Parimad praktikad LLM-ide jaoks](../../../Module04)
12. [Tõrkeotsing](../../../Module04)
13. [Täiendavad ressursid](../../../Module04)

## Sissejuhatus Apple MLX-i

Apple MLX on raamistik, mis on loodud spetsiaalselt tõhusaks ja paindlikuks masinõppeks Apple Siliconi riistvaral, arendatud Apple Machine Learning Researchi poolt. Detsembris 2023 välja antud MLX on Apple'i vastus sellistele raamistikutele nagu PyTorch ja TensorFlow, keskendudes eriti suurte keelemudelite võimekuse võimaldamisele Mac-arvutites.

### Mis teeb MLX-i eriliseks LLM-ide jaoks?

MLX on loodud täielikult ära kasutama Apple Siliconi ühtset mälustruktuuri, muutes selle eriti sobivaks suurte keelemudelite käitamiseks ja peenhäälestamiseks kohapeal Mac-arvutites. Raamistik kõrvaldab paljud ühilduvusprobleemid, millega Maci kasutajad traditsiooniliselt LLM-idega töötades kokku puutusid.

### Kes peaks MLX-i kasutama LLM-ide jaoks?

- **Maci kasutajad**, kes soovivad käitada LLM-e kohapeal ilma pilve sõltuvuseta
- **Teadlased**, kes katsetavad keelemudelite peenhäälestamist ja kohandamist
- **Arendajad**, kes loovad AI-rakendusi keelemudelite funktsioonidega
- **Igaüks**, kes soovib kasutada Apple Siliconi tekstigeneratsiooni, vestluste ja keeleülesannete jaoks

## Peamised funktsioonid LLM-i arendamiseks

### 1. Ühtne mälustruktuur
Apple Siliconi ühtne mälu võimaldab MLX-il tõhusalt hallata suuri keelemudeleid ilma mälukoopiate koormuseta, mis on tüüpiline teistele raamistikutele. See tähendab, et saate töötada suuremate mudelitega samal riistvaral.

### 2. Natiivne Apple Siliconi optimeerimine
MLX on algusest peale loodud Apple'i M-seeria kiipide jaoks, pakkudes optimaalset jõudlust keelemudelite jaoks tavaliselt kasutatavate transformer-arhitektuuride jaoks.

### 3. Kvantiseerimise tugi
Sisseehitatud tugi 4-bitisele ja 8-bitisele kvantiseerimisele vähendab mälunõudeid, säilitades samal ajal mudeli kvaliteedi, võimaldades suuremaid mudeleid käitada tarbijariistvaral.

### 4. Hugging Face integratsioon
Sujuv integratsioon Hugging Face ökosüsteemiga pakub juurdepääsu tuhandetele eelõpetatud keelemudelitele lihtsate konverteerimistööriistadega.

### 5. LoRA peenhäälestamine
Madala astme kohandamise (LoRA) tugi võimaldab suurte mudelite tõhusat peenhäälestamist minimaalsete arvutusressurssidega.

## Paigaldusjuhend

### Süsteeminõuded
- **macOS 13.0+** (Apple Siliconi optimeerimiseks)
- **Python 3.8+**
- **Apple Silicon** (M1, M2, M3, M4 seeria)
- **Natiivne ARM-keskkond** (mitte Rosetta all töötav)
- **8GB+ RAM** (16GB+ soovitatav suuremate mudelite jaoks)

### Kiirpaigaldus LLM-ide jaoks

Lihtsaim viis keelemudelitega alustamiseks on MLX-LM-i paigaldamine:

```bash
pip install mlx-lm
```

See üks käsk paigaldab nii MLX-i põhiraamistiku kui ka keelemudelite tööriistad.

### Virtuaalse keskkonna seadistamine (soovitatav)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Täiendavad sõltuvused audiomudelite jaoks

Kui plaanite töötada kõnemudelitega nagu Whisper:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## MLX-i kasutamise alustamine

### Teie esimene keelemudel

Alustame lihtsa tekstigeneratsiooni näitega:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Python API näide

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Mudeli laadimise mõistmine

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Keelemudelid

### Toetatud mudeliarhitektuurid

MLX-LM toetab laia valikut populaarseid keelemudeli arhitektuure:

- **LLaMA ja LLaMA 2** - Meta põhjalikud mudelid
- **Mistral ja Mixtral** - Tõhusad ja võimsad mudelid
- **Phi-3** - Microsofti kompaktsed keelemudelid
- **Qwen** - Alibaba mitmekeelsed mudelid
- **Code Llama** - Spetsialiseerunud koodigeneratsioonile
- **Gemma** - Google'i avatud keelemudelid

### Käsurealiides

MLX-LM-i käsurealiides pakub võimsaid tööriistu keelemudelitega töötamiseks:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Python API keerukamate kasutusjuhtude jaoks

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Töötamine suurte keelemudelitega

### Tekstigeneratsiooni mustrid

#### Ühe pöörde generatsioon
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Juhiste järgimine
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Loov kirjutamine
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Mitme pöörde vestlused

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Hugging Face integratsioon

### MLX-iga ühilduvate mudelite leidmine

MLX töötab sujuvalt Hugging Face ökosüsteemiga:

- **Sirvi MLX mudeleid**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX kogukond**: https://huggingface.co/mlx-community (eelkonverteeritud mudelid)
- **Originaalmudelid**: Enamik LLaMA, Mistral, Phi ja Qwen mudeleid töötavad konverteerimisega

### Mudelite laadimine Hugging Face'ist

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Mudelite allalaadimine võrguühenduseta kasutamiseks

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Mudelite konverteerimine ja kvantiseerimine

### Hugging Face mudelite konverteerimine MLX-iks

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Kvantiseerimise mõistmine

Kvantiseerimine vähendab mudeli suurust ja mälukasutust minimaalsete kvaliteedikadudega:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Kohandatud kvantiseerimine

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Keelemudelite peenhäälestamine

### LoRA (madala astme kohandamine) peenhäälestamine

MLX toetab tõhusat peenhäälestamist LoRA abil, mis võimaldab suurte mudelite kohandamist minimaalsete arvutusressurssidega:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Treeningandmete ettevalmistamine

Looge JSON-fail oma treeningnäidetega:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Peenhäälestamise käsk

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Peenhäälestatud mudelite kasutamine

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Täiustatud LLM-i funktsioonid

### Konteksti vahemällu salvestamine tõhususe jaoks

Korduva sama konteksti kasutamise korral toetab MLX konteksti vahemällu salvestamist, et parandada jõudlust:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Tekstigeneratsiooni voogesitus

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Töötamine koodigeneratsiooni mudelitega

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Töötamine vestlusmudelitega

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Parimad praktikad LLM-ide jaoks

### Mäluhaldus

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Mudeli valiku juhised

**Katsetamiseks ja õppimiseks:**
- Kasutage 4-bitiseid kvantiseeritud mudeleid (nt `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Alustage väiksemate mudelitega nagu Phi-3-mini

**Tootmisrakenduste jaoks:**
- Kaaluge mudeli suuruse ja kvaliteedi vahelist kompromissi
- Testige nii kvantiseeritud kui ka täppismudeleid
- Tehke oma konkreetsete kasutusjuhtude põhjal võrdlusuuringuid

**Spetsiifiliste ülesannete jaoks:**
- **Koodigeneratsioon**: CodeLlama, Code Llama Instruct
- **Üldine vestlus**: Mistral-7B-Instruct, Phi-3
- **Mitmekeelne**: Qwen mudelid
- **Loov kirjutamine**: Kõrgemad temperatuuriseaded Mistrali või LLaMA-ga

### Juhiste koostamise parimad praktikad

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Jõudluse optimeerimine

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Tõrkeotsing

### Levinud probleemid ja lahendused

#### Paigaldusprobleemid

**Probleem**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Lahendus**: Kasutage natiivset ARM Pythonit või Minicondat:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Mäluprobleemid

**Probleem**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Mudeli laadimise probleemid

**Probleem**: Mudel ei laadi või genereerib halva väljundi
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Jõudlusprobleemid

**Probleem**: Aeglane generatsioonikiirus
- Sulgege muud mälumahukad rakendused
- Kasutage võimalusel kvantiseeritud mudeleid
- Veenduge, et te ei tööta Rosetta all
- Kontrollige enne mudelite laadimist saadaolevat mälu

### Silumise näpunäited

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Täiendavad ressursid

### Ametlik dokumentatsioon ja repositooriumid

- **MLX GitHubi repositoorium**: https://github.com/ml-explore/mlx
- **MLX-LM näited**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX dokumentatsioon**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX integratsioon**: https://huggingface.co/docs/hub/en/mlx

### Mudelikogud

- **MLX kogukonna mudelid**: https://huggingface.co/mlx-community
- **Trendikad MLX mudelid**: https://huggingface.co/models?library=mlx&sort=trending

### Näidisrakendused

1. **Isiklik AI assistent**: Looge kohalik vestlusrobot mäluga
2. **Koodi abiline**: Looge koodigeneratsiooni assistent oma arendustöövoo jaoks
3. **Sisu generaator**: Arendage tööriistu kirjutamiseks, kokkuvõtete tegemiseks ja sisuloomeks
4. **Kohandatud peenhäälestatud mudelid**: Kohandage mudeleid valdkonnapõhiste ülesannete jaoks
5. **Multimodaalsed rakendused**: Kombineerige tekstigeneratsioon teiste MLX-i võimalustega

### Kogukond ja õppimine

- **MLX kogukonna arutelud**: GitHubi probleemid ja arutelud
- **Hugging Face foorumid**: Kogukonna tugi ja mudelite jagamine
- **Apple'i arendajadokumentatsioon**: Ametlikud Apple ML ressursid

### Viitamine

Kui kasutate MLX-i oma uurimistöös, viidake:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Kokkuvõte

Apple MLX on revolutsiooniliselt muutnud suurte keelemudelite käitamise Mac-arvutites. Pakkudes natiivset Apple Siliconi optimeerimist, sujuvat Hugging Face integratsiooni ja võimsaid funktsioone nagu kvantiseerimine ja LoRA peenhäälestamine, võimaldab MLX keerukate keelemudelite kohapealset käitamist suurepärase jõudlusega.

Olgu teie eesmärgiks vestlusrobotite, koodiabiliste, sisugeneraatorite või kohandatud peenhäälestatud mudelite loomine, MLX pakub vajalikke tööriistu ja jõudlust, et kasutada Apple Silicon Maci täielikku potentsiaali keelemudelite rakendustes. Raamistiku keskendumine tõhususele ja kasutusmugavusele teeb sellest suurepärase valiku nii uurimistööks kui ka tootmisrakendusteks.

Alustage selle juhendi põhinäidetega, uurige Hugging Face'i eelkonverteeritud mudelite rikkalikku ökosüsteemi ja liikuge järk-järgult keerukamate funktsioonide, nagu peenhäälestamine ja kohandatud mudelite arendamine, juurde. Kuna MLX-i ökosüsteem jätkab kasvu, muutub see üha võimsamaks platvormiks keelemudelite arendamiseks Apple'i riistvaral.

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.