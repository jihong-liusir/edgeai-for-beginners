<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-10-11T11:39:52+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "et"
}
-->
# Section 2 : Llama.cpp Rakendamise Juhend

## Sisukord
1. [Sissejuhatus](../../../Module04)
2. [Mis on Llama.cpp?](../../../Module04)
3. [Paigaldamine](../../../Module04)
4. [Allikast ehitamine](../../../Module04)
5. [Mudelite kvantiseerimine](../../../Module04)
6. [Põhikasutus](../../../Module04)
7. [Täiustatud funktsioonid](../../../Module04)
8. [Pythoniga integreerimine](../../../Module04)
9. [Tõrkeotsing](../../../Module04)
10. [Parimad praktikad](../../../Module04)

## Sissejuhatus

See põhjalik juhend aitab teil õppida kõike Llama.cpp kohta, alates lihtsast paigaldamisest kuni keerukamate kasutusjuhtudeni. Llama.cpp on võimas C++-i rakendus, mis võimaldab tõhusalt kasutada suuri keelemudeleid (LLM) minimaalse seadistusega ja suurepärase jõudlusega erinevates riistvarakonfiguratsioonides.

## Mis on Llama.cpp?

Llama.cpp on C/C++-is kirjutatud LLM-i järeldusraamistik, mis võimaldab suuri keelemudeleid kohapeal käivitada minimaalse seadistusega ja tipptasemel jõudlusega laias riistvaravalikus. Peamised omadused:

### Põhiomadused
- **Lihtne C/C++ rakendus** ilma sõltuvusteta
- **Platvormideülene ühilduvus** (Windows, macOS, Linux)
- **Riistvara optimeerimine** erinevatele arhitektuuridele
- **Kvantiseerimise tugi** (1,5-bitist kuni 8-bitise täisarvuni)
- **CPU ja GPU kiirendus** tugi
- **Mälu efektiivsus** piiratud keskkondades

### Eelised
- Töötab tõhusalt CPU-l ilma spetsiaalset riistvara vajamata
- Toetab mitut GPU tagapõhja (CUDA, Metal, OpenCL, Vulkan)
- Kerge ja kaasaskantav
- Apple'i silikoon on esmaklassiline - optimeeritud ARM NEON-i, Accelerate'i ja Metal-i raamistikuga
- Toetab erinevaid kvantiseerimistasemeid, et vähendada mälukasutust

## Paigaldamine

### Meetod 1: Eelkompileeritud binaarid (soovitatav algajatele)

#### Laadige alla GitHubi väljalasetest
1. Külastage [Llama.cpp GitHubi väljalasete lehte](https://github.com/ggml-org/llama.cpp/releases)
2. Laadige alla oma süsteemile sobiv binaar:
   - `llama-<versioon>-bin-win-<funktsioon>-<arh>.zip` Windowsi jaoks
   - `llama-<versioon>-bin-macos-<funktsioon>-<arh>.zip` macOS-i jaoks
   - `llama-<versioon>-bin-linux-<funktsioon>-<arh>.zip` Linuxi jaoks

3. Pakkige arhiiv lahti ja lisage kataloog oma süsteemi PATH-i

#### Pakihaldurite kasutamine

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Erinevad distributsioonid):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Meetod 2: Python pakett (llama-cpp-python)

#### Põhipaigaldus
```bash
pip install llama-cpp-python
```

#### Riistvara kiirendusega
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Allikast ehitamine

### Eeltingimused

**Süsteeminõuded:**
- C++ kompilaator (GCC, Clang või MSVC)
- CMake (versioon 3.14 või uuem)
- Git
- Platvormi ehitustööriistad

**Eeltingimuste paigaldamine:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Paigaldage Visual Studio 2022 koos C++ arendustööriistadega
- Paigaldage CMake ametlikult veebisaidilt
- Paigaldage Git

### Põhiehituse protsess

1. **Kloonige repositoorium:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigureerige ehitus:**
```bash
cmake -B build
```

3. **Ehitage projekt:**
```bash
cmake --build build --config Release
```

Kiiremaks kompileerimiseks kasutage paralleelseid töid:
```bash
cmake --build build --config Release -j 8
```

### Riistvaraspetsiifilised ehitused

#### CUDA tugi (NVIDIA GPU-d)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal tugi (Apple'i silikoon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS tugi (CPU optimeerimine)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan tugi
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Täiustatud ehitusvalikud

#### Silumise ehitus
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Täiendavate funktsioonidega
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Mudelite kvantiseerimine

### GGUF formaadi mõistmine

GGUF (Generalized GGML Unified Format) on optimeeritud failiformaat, mis on loodud suurte keelemudelite tõhusaks käitamiseks Llama.cpp ja teiste raamistikute abil. See pakub:

- Standardiseeritud mudelikaalude salvestust
- Parendatud ühilduvust platvormide vahel
- Suurendatud jõudlust
- Tõhusat metaandmete haldust

### Kvantiseerimise tüübid

Llama.cpp toetab erinevaid kvantiseerimistasemeid:

| Tüüp | Bitid | Kirjeldus | Kasutusjuht |
|------|------|-------------|----------|
| F16 | 16 | Pooltäpsus | Kõrge kvaliteet, suur mälu |
| Q8_0 | 8 | 8-bitine kvantiseerimine | Hea tasakaal |
| Q4_0 | 4 | 4-bitine kvantiseerimine | Keskmine kvaliteet, väiksem suurus |
| Q2_K | 2 | 2-bitine kvantiseerimine | Väikseim suurus, madalam kvaliteet |

### Mudelite konverteerimine

#### PyTorchist GGUF-i
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Otse allalaadimine Hugging Face'ist
Paljud mudelid on saadaval GGUF formaadis Hugging Face'is:
- Otsige mudeleid, mille nimes on "GGUF"
- Laadige alla sobiv kvantiseerimistaseme mudel
- Kasutage otse Llama.cpp-ga

## Põhikasutus

### Käsurealiides

#### Lihtne tekstigeneratsioon
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face'i mudelite kasutamine
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Serveri režiim
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Üldised parameetrid

| Parameeter | Kirjeldus | Näide |
|-----------|-------------|---------|
| `-m` | Mudelifaili tee | `-m model.gguf` |
| `-p` | Algteksti sisend | `-p "Tere maailm"` |
| `-n` | Genereeritavate tokenite arv | `-n 100` |
| `-c` | Konteksti suurus | `-c 4096` |
| `-t` | Lõimede arv | `-t 8` |
| `-ngl` | GPU kihid | `-ngl 32` |
| `-temp` | Temperatuur | `-temp 0.7` |

### Interaktiivne režiim

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Täiustatud funktsioonid

### Serveri API

#### Serveri käivitamine
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API kasutamine
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Jõudluse optimeerimine

#### Mälu haldamine
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multitöötlus
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU kiirendus
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Pythoniga integreerimine

### Põhikasutus llama-cpp-pythoniga

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Vestlusliides

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Vastuste voogedastus

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integreerimine LangChainiga

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Tõrkeotsing

### Levinud probleemid ja lahendused

#### Ehitustõrked

**Probleem: CMake puudub**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Probleem: Kompilaator puudub**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Käitusprobleemid

**Probleem: Mudeli laadimine ebaõnnestub**
- Kontrollige mudelifaili teed
- Kontrollige faililubasid
- Veenduge, et RAM-i oleks piisavalt
- Proovige erinevaid kvantiseerimistasemeid

**Probleem: Kehv jõudlus**
- Lubage riistvara kiirendus
- Suurendage lõimede arvu
- Kasutage sobivat kvantiseerimist
- Kontrollige GPU mälukasutust

#### Mälu probleemid

**Probleem: Mälu otsas**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platvormispetsiifilised probleemid

#### Windows
- Kasutage MinGW või Visual Studio kompilaatorit
- Veenduge, et PATH oleks õigesti konfigureeritud
- Kontrollige viirusetõrje häireid

#### macOS
- Lubage Metal Apple'i silikooni jaoks
- Kasutage Rosetta 2 ühilduvuse tagamiseks, kui vaja
- Kontrollige Xcode'i käsureatööriistu

#### Linux
- Paigaldage arendustööriistade paketid
- Kontrollige GPU draiverite versioone
- Veenduge, et CUDA tööriistakomplekt oleks paigaldatud

## Parimad praktikad

### Mudeli valik
1. **Valige sobiv kvantiseerimine** vastavalt oma riistvarale
2. **Arvestage mudeli suuruse** ja kvaliteedi kompromissidega
3. **Testige erinevaid mudeleid** oma konkreetse kasutusjuhtumi jaoks

### Jõudluse optimeerimine
1. **Kasutage GPU kiirendust**, kui see on saadaval
2. **Optimeerige lõimede arvu** vastavalt oma CPU-le
3. **Määrake sobiv konteksti suurus** vastavalt oma kasutusjuhtumile
4. **Lubage mälukaardistamine** suurte mudelite jaoks

### Tootmiskeskkonna juurutamine
1. **Kasutage serveri režiimi** API-le juurdepääsuks
2. **Rakendage korralik veakäsitlus**
3. **Jälgige ressursside kasutust**
4. **Seadistage logimine ja jälgimine**

### Arendustöövoog
1. **Alustage väiksemate mudelitega** testimiseks
2. **Kasutage versioonihaldust** mudeli konfiguratsioonide jaoks
3. **Dokumenteerige oma konfiguratsioonid**
4. **Testige erinevatel platvormidel**

### Turvalisuse kaalutlused
1. **Valideerige sisendtekstid**
2. **Rakendage kiiruse piiramine**
3. **Turvake API lõpp-punktid**
4. **Jälgige väärkasutuse mustreid**

## Kokkuvõte

Llama.cpp pakub võimsat ja tõhusat viisi suurte keelemudelite kohapeal käitamiseks erinevates riistvarakonfiguratsioonides. Olgu teie eesmärk AI rakenduste arendamine, teadustöö või lihtsalt LLM-idega katsetamine, see raamistik pakub paindlikkust ja jõudlust, mida vajate mitmesuguste kasutusjuhtumite jaoks.

Peamised punktid:
- Valige paigaldusmeetod, mis sobib teie vajadustega
- Optimeerige oma konkreetse riistvarakonfiguratsiooni jaoks
- Alustage põhikasutusest ja uurige järk-järgult täiustatud funktsioone
- Kaaluge Pythoniga integreerimist lihtsama kasutuse jaoks
- Järgige parimaid praktikaid tootmiskeskkonna juurutamisel

Lisateabe ja värskenduste saamiseks külastage [ametlikku Llama.cpp repositooriumi](https://github.com/ggml-org/llama.cpp) ning tutvuge põhjaliku dokumentatsiooni ja kogukonna ressurssidega.

## ➡️ Mis edasi

- [03: Microsoft Olive Optimeerimiskomplekt](./03.MicrosoftOlive.md)

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.