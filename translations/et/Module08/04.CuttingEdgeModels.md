<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-11T12:50:34+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "et"
}
-->
# Sessioon 4: Tootmiskõlblike vestlusrakenduste loomine Chainlitiga

## Ülevaade

Selles sessioonis keskendutakse tootmiskõlblike vestlusrakenduste loomisele, kasutades Chainlitit ja Microsoft Foundry Locali. Õpid looma kaasaegseid veebiliideseid AI-vestlusteks, rakendama voogesituse vastuseid ning juurutama vastupidavaid vestlusrakendusi koos tõhusa veakäsitluse ja kasutajakogemuse disainiga.

**Mida sa lood:**
- **Chainlit vestlusrakendus**: Kaasaegne veebiliides voogesituse vastustega
- **WebGPU demo**: Brauseripõhine inferents privaatsust eelistavatele rakendustele  
- **Open WebUI integratsioon**: Professionaalne vestlusliides Foundry Localiga
- **Tootmismustrid**: Veakäsitlus, jälgimine ja juurutamisstrateegiad

## Õpieesmärgid

- Loo tootmiskõlblikke vestlusrakendusi Chainlitiga
- Rakenda voogesituse vastuseid kasutajakogemuse parandamiseks
- Valda Foundry Local SDK integratsioonimustreid
- Rakenda tõhusat veakäsitlust ja sujuvat degradeerumist
- Juuruta ja konfigureeri vestlusrakendusi erinevates keskkondades
- Mõista kaasaegseid veebiliidese mustreid vestlus-AI jaoks

## Eeltingimused

- **Foundry Local**: Paigaldatud ja töötav ([Paigaldusjuhend](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Versioon 3.10 või uuem koos virtuaalse keskkonna võimalusega
- **Mudel**: Vähemalt üks mudel laaditud (`foundry model run phi-4-mini`)
- **Brauser**: Kaasaegne veebibrauser WebGPU toetusega (Chrome/Edge)
- **Docker**: Open WebUI integratsiooni jaoks (valikuline)

## Osa 1: Kaasaegsete vestlusrakenduste mõistmine

### Arhitektuuri ülevaade

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Peamised tehnoloogiad

**Foundry Local SDK mustrid:**
- `FoundryLocalManager(alias)`: Automaatne teenuse haldamine
- `manager.endpoint` ja `manager.api_key`: Ühenduse üksikasjad
- `manager.get_model_info(alias).id`: Mudeli identifitseerimine

**Chainlit raamistik:**
- `@cl.on_chat_start`: Vestlusseansside algatamine
- `@cl.on_message`: Sissetulevate kasutajasõnumite käsitlus  
- `cl.Message().stream_token()`: Reaalajas voogesitus
- Automaatne UI genereerimine ja WebSocketi haldamine

## Osa 2: Kohaliku ja pilve lahenduse otsustusmaatriks

### Jõudlusomadused

| Aspekt | Kohalik (Foundry) | Pilv (Azure OpenAI) |
|--------|-------------------|---------------------|
| **Latentsus** | 🚀 50-200ms (ilma võrguta) | ⏱️ 200-2000ms (võrgust sõltuv) |
| **Privaatsus** | 🔒 Andmed ei lahku seadmest | ⚠️ Andmed saadetakse pilve |
| **Kulu** | 💰 Tasuta pärast riistvara | 💸 Maksa iga tokeni eest |
| **Võrguühenduseta** | ✅ Töötab ilma internetita | ❌ Vajab internetti |
| **Mudeli suurus** | ⚠️ Piiratud riistvaraga | ✅ Juurdepääs suurimatele mudelitele |
| **Skaleerimine** | ⚠️ Riistvarast sõltuv | ✅ Piiramatu skaleerimine |

### Hübriidstrateegia mustrid

**Kohalik esmalt, varuvariant:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Ülesandepõhine suunamine:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Osa 3: Näidis 04 - Chainlit vestlusrakendus

### Kiire alustamine

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Rakendus avaneb automaatselt aadressil `http://localhost:8080` kaasaegse vestlusliidesega.

### Põhiteostus

Näidis 04 rakendus demonstreerib tootmiskõlblikke mustreid:

**Automaatne teenuse avastamine:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Voogesituse vestluse käsitlus:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Konfiguratsioonivalikud

**Keskkonnamuutujad:**

| Muutuja | Kirjeldus | Vaikimisi | Näide |
|---------|-----------|-----------|-------|
| `MODEL` | Kasutatava mudeli alias | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Locali lõpp-punkt | Automaatne tuvastamine | `http://localhost:51211` |
| `API_KEY` | API võti (kohaliku jaoks valikuline) | `""` | `your-api-key` |

**Täiustatud kasutus:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Osa 4: Jupyter Notebookide loomine ja kasutamine

### Notebooki toe ülevaade

Näidis 04 sisaldab põhjalikku Jupyter notebooki (`chainlit_app.ipynb`), mis pakub:

- **📚 Õppematerjalid**: Samm-sammult õppematerjalid
- **🔬 Interaktiivne uurimine**: Koodirakkude käivitamine ja katsetamine
- **📊 Visuaalsed demonstratsioonid**: Diagrammid, skeemid ja väljundi visualiseerimine
- **🛠️ Arendustööriistad**: Testimise ja silumise võimalused

### Oma notebookide loomine

#### Samm 1: Jupyteri keskkonna seadistamine

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Samm 2: Uue notebooki loomine

**VS Code'i kasutamine:**
1. Ava VS Code kataloogis Module08
2. Loo uus fail laiendiga `.ipynb`
3. Vali "Foundry Local" kernel, kui küsitakse
4. Alusta rakkude lisamist oma sisuga

**Jupyter Labi kasutamine:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebooki struktuuri parimad tavad

#### Rakkude organiseerimine

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Interaktiivsed näited ja harjutused

#### Harjutus 1: Kliendi konfiguratsiooni testimine

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Harjutus 2: Voogesituse vastuse simulatsioon

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Osa 5: WebGPU brauseri inferentsi demo

### Ülevaade

WebGPU võimaldab AI-mudelite käitamist otse brauseris maksimaalse privaatsuse ja null-paigalduse kogemuse jaoks. See näidis demonstreerib ONNX Runtime Webi WebGPU täitmisega.

### Samm 1: Kontrolli WebGPU tuge

**Brauseri nõuded:**
- Chrome/Edge 113+ WebGPU lubatud
- Kontroll: `chrome://gpu` → kinnita "WebGPU" staatus
- Programmaatiline kontroll: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Samm 2: Loo WebGPU demo

Loo kataloog: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Samm 3: Käivita demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Osa 6: Open WebUI integratsioon

### Ülevaade

Open WebUI pakub professionaalset ChatGPT-laadset liidest, mis ühendub Foundry Locali OpenAI-ühilduva API-ga.

### Samm 1: Eeltingimused

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Samm 2: Dockeri seadistamine (soovitatav)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Märkus:** `host.docker.internal` võimaldab Docker konteineritel pääseda hostmasinale Windowsis.

### Samm 3: Konfiguratsioon

1. **Ava brauser:** Navigeeri aadressile `http://localhost:3000`
2. **Esialgne seadistamine:** Loo administraatori konto
3. **Mudeli konfiguratsioon:**
   - Seaded → Mudelid → OpenAI API  
   - Põhiaadress: `http://host.docker.internal:51211/v1`
   - API võti: `foundry-local-key` (mis tahes väärtus sobib)
4. **Testi ühendust:** Mudelid peaksid ilmuma rippmenüüs

### Tõrkeotsing

**Levinud probleemid:**

1. **Ühendus keelatud:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Mudelid ei ilmu:**
   - Kontrolli, kas mudel on laaditud: `foundry model list`
   - Kontrolli API vastust: `curl http://localhost:51211/v1/models`
   - Taaskäivita Open WebUI konteiner

## Osa 7: Tootmise juurutamise kaalutlused

### Keskkonna konfiguratsioon

**Arenduskeskkond:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Tootmise juurutamine:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Levinud portiprobleemid ja lahendused

**Port 51211 konflikti ennetamine:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Jõudluse jälgimine

**Tervisekontrolli rakendamine:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Kokkuvõte

Sessioon 4 käsitles tootmiskõlblike Chainlit rakenduste loomist vestlus-AI jaoks. Õppisid:

- ✅ **Chainlit raamistik**: Kaasaegne UI ja voogesituse tugi vestlusrakendustele
- ✅ **Foundry Local integratsioon**: SDK kasutus ja konfiguratsioonimustrid  
- ✅ **WebGPU inferents**: Brauseripõhine AI maksimaalse privaatsuse jaoks
- ✅ **Open WebUI seadistamine**: Professionaalse vestlusliidese juurutamine
- ✅ **Tootmismustrid**: Veakäsitlus, jälgimine ja skaleerimine

Näidis 04 rakendus demonstreerib parimaid tavasid vastupidavate vestlusliideste loomiseks, mis kasutavad kohalikke AI-mudeleid Microsoft Foundry Locali kaudu, pakkudes samal ajal suurepärast kasutajakogemust.

## Viited

- **[Näidis 04: Chainlit rakendus](samples/04/README.md)**: Täielik rakendus koos dokumentatsiooniga
- **[Chainlit õppenotebook](samples/04/chainlit_app.ipynb)**: Interaktiivsed õppematerjalid
- **[Foundry Local dokumentatsioon](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Platvormi täielik dokumentatsioon
- **[Chainlit dokumentatsioon](https://docs.chainlit.io/)**: Ametlik raamistikudokumentatsioon
- **[Open WebUI integratsioonijuhend](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Ametlik õpetus

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.