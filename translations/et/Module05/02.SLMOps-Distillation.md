<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-10-11T11:23:11+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "et"
}
-->
# Sektsioon 2: Mudeli destilleerimine - teooriast praktikani

## Sisukord
1. [Sissejuhatus mudeli destilleerimisse](../../../Module05)
2. [Miks destilleerimine on oluline](../../../Module05)
3. [Destilleerimise protsess](../../../Module05)
4. [Praktiline rakendamine](../../../Module05)
5. [Azure ML destilleerimise näide](../../../Module05)
6. [Parimad praktikad ja optimeerimine](../../../Module05)
7. [Reaalsed rakendused](../../../Module05)
8. [Kokkuvõte](../../../Module05)

## Sissejuhatus mudeli destilleerimisse {#introduction}

Mudeli destilleerimine on võimas tehnika, mis võimaldab luua väiksemaid ja tõhusamaid mudeleid, säilitades samal ajal suuremate ja keerukamate mudelite jõudluse. See protsess hõlmab kompaktse "õpilasmudeli" treenimist, et jäljendada suurema "õpetajamudeli" käitumist.

**Peamised eelised:**
- **Vähendatud arvutusnõuded** järelduste tegemiseks
- **Madalam mälukasutus** ja salvestusvajadus
- **Kiirem järelduste tegemise aeg**, säilitades samas mõistliku täpsuse
- **Kulutõhus juurutamine** piiratud ressurssidega keskkondades

## Miks destilleerimine on oluline {#why-distillation-matters}

Suured keelemudelid (LLM-id) muutuvad üha võimsamaks, kuid samas ka ressursimahukamaks. Kuigi miljardite parameetritega mudel võib anda suurepäraseid tulemusi, ei pruugi see paljudele reaalse maailma rakendustele olla praktiline järgmistel põhjustel:

### Ressursipiirangud
- **Arvutuslik koormus**: Suured mudelid vajavad märkimisväärset GPU mälu ja töötlemisvõimsust
- **Järelduste latentsus**: Keerukad mudelid võtavad vastuste genereerimiseks kauem aega
- **Energiatarbimine**: Suured mudelid tarbivad rohkem energiat, suurendades tegevuskulusid
- **Infrastruktuurikulud**: Suurte mudelite majutamine nõuab kallist riistvara

### Praktilised piirangud
- **Mobiilirakendused**: Suured mudelid ei tööta tõhusalt mobiilseadmetes
- **Reaalajas rakendused**: Rakendused, mis vajavad madalat latentsust, ei saa aeglase järeldusega hakkama
- **Serv-arvutamine**: IoT ja servaseadmetel on piiratud arvutusressursid
- **Kulude kaalutlused**: Paljud organisatsioonid ei saa endale lubada suurte mudelite juurutamiseks vajalikku infrastruktuuri

## Destilleerimise protsess {#the-distillation-process}

Mudeli destilleerimine järgib kaheetapilist protsessi, mis edastab teadmisi õpetajamudelilt õpilasmudelile:

### Etapp 1: Sünteetiliste andmete genereerimine

Õpetajamudel genereerib vastuseid teie treeningandmestikule, luues kvaliteetseid sünteetilisi andmeid, mis kajastavad õpetaja teadmisi ja otsustusmustreid.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Selle etapi peamised aspektid:**
- Õpetajamudel töötleb iga treeningnäidet
- Genereeritud vastused muutuvad õpilasmudeli treenimise "tõeväärtuseks"
- See protsess kajastab õpetaja otsustusmustreid
- Sünteetiliste andmete kvaliteet mõjutab otseselt õpilasmudeli jõudlust

### Etapp 2: Õpilasmudeli peenhäälestamine

Õpilasmudelit treenitakse sünteetilise andmestiku põhjal, et õppida jäljendama õpetaja käitumist ja vastuseid.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Treeningu eesmärgid:**
- Minimeerida erinevust õpilase ja õpetaja väljundite vahel
- Säilitada õpetaja teadmised väiksemas parameetrite ruumis
- Säilitada jõudlus, vähendades samal ajal mudeli keerukust

## Praktiline rakendamine {#practical-implementation}

### Õpetaja ja õpilasmudelite valimine

**Õpetajamudeli valik:**
- Valige suuremahulised LLM-id (100B+ parameetrit), millel on tõestatud jõudlus teie konkreetse ülesande jaoks
- Populaarsed õpetajamudelid hõlmavad:
  - **DeepSeek V3** (671B parameetrit) - suurepärane põhjendamise ja koodi genereerimise jaoks
  - **Meta Llama 3.1 405B Instruct** - laiaulatuslikud üldised võimed
  - **GPT-4** - tugev jõudlus mitmesuguste ülesannete puhul
  - **Claude 3.5 Sonnet** - suurepärane keerukate põhjendamisülesannete jaoks
- Veenduge, et õpetajamudel toimiks hästi teie valdkonnapõhiste andmete puhul

**Õpilasmudeli valik:**
- Tasakaal mudeli suuruse ja jõudlusnõuete vahel
- Keskenduge tõhusatele, väiksematele mudelitele, nagu:
  - **Microsoft Phi-4-mini** - uusim tõhus mudel tugeva põhjendusvõimega
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K ja 128K variandid)
  - Microsoft Phi-3.5 Mini Instruct

### Rakendamise sammud

1. **Andmete ettevalmistamine**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Õpetajamudeli seadistamine**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Sünteetiliste andmete genereerimine**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Õpilasmudeli treenimine**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Azure ML destilleerimise näide {#azure-ml-example}

Azure Machine Learning pakub terviklikku platvormi mudeli destilleerimise rakendamiseks. Siin on, kuidas kasutada Azure ML-i oma destilleerimisprotsessis:

### Eeltingimused

1. **Azure ML tööruum**: Seadistage oma tööruum sobivas piirkonnas
   - Tagage juurdepääs suuremahulistele õpetajamudelitele (DeepSeek V3, Llama 405B)
   - Konfigureerige piirkonnad mudelite kättesaadavuse alusel

2. **Arvutusressursid**: Konfigureerige sobivad arvutusinstantsid treenimiseks
   - Suure mälumahuga instantsid õpetajamudeli järelduste tegemiseks
   - GPU-toega arvutus õpilasmudeli peenhäälestamiseks

### Toetatud ülesandetüübid

Azure ML toetab destilleerimist mitmesuguste ülesannete jaoks:

- **Loodusliku keele tõlgendamine (NLI)**
- **Vestluslik AI**
- **Küsimuste ja vastuste (QA) süsteemid**
- **Matemaatiline põhjendamine**
- **Teksti kokkuvõtete tegemine**

### Näidisrakendus

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Jälgimine ja hindamine

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Parimad praktikad ja optimeerimine {#best-practices}

### Andmete kvaliteet

**Kvaliteetsed treeningandmed on üliolulised:**
- Tagage mitmekesised ja esinduslikud treeningnäited
- Kasutage võimalusel valdkonnapõhiseid andmeid
- Kontrollige õpetajamudeli väljundeid enne nende kasutamist õpilasmudeli treenimiseks
- Tasakaalustage andmestik, et vältida õpilasmudeli õppimises kallutatust

### Hüperparameetrite häälestamine

**Peamised optimeeritavad parameetrid:**
- **Õppemäär**: Alustage väiksemate määradega (1e-5 kuni 5e-5) peenhäälestamiseks
- **Partii suurus**: Tasakaal mälupiirangute ja treeningu stabiilsuse vahel
- **Epohhide arv**: Jälgige üleõppimist; tavaliselt piisab 2-5 epohhist
- **Temperatuuri skaleerimine**: Kohandage õpetaja väljundite pehmust paremaks teadmiste edastamiseks

### Mudeli arhitektuuri kaalutlused

**Õpetaja-õpilase ühilduvus:**
- Tagage arhitektuuriline ühilduvus õpetaja ja õpilasmudelite vahel
- Kaaluge vahekihtide sobitamist paremaks teadmiste edastamiseks
- Kasutage tähelepanu ülekande tehnikaid, kui see on asjakohane

### Hindamisstrateegiad

**Terviklik hindamisviis:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Reaalsed rakendused {#real-world-applications}

### Mobiili- ja servaseadmete juurutamine

Destilleeritud mudelid võimaldavad AI võimekust piiratud ressurssidega seadmetes:
- **Nutitelefoni rakendused** reaalajas tekstitöötlusega
- **IoT seadmed**, mis teevad kohalikku järeldust
- **Manussüsteemid** piiratud arvutusressurssidega

### Kulutõhusad tootmissüsteemid

Organisatsioonid kasutavad destilleerimist tegevuskulude vähendamiseks:
- **Klienditeeninduse vestlusrobotid** kiiremate vastusaegadega
- **Sisu modereerimise süsteemid**, mis töötlevad suuri mahtusid tõhusalt
- **Reaalajas tõlketeenused** madalama latentsusega

### Valdkonnapõhised rakendused

Destilleerimine aitab luua spetsialiseeritud mudeleid:
- **Meditsiiniline diagnoosimine** privaatsust säilitava kohaliku järeldusega
- **Õigusdokumentide analüüs**, mis on optimeeritud konkreetsete õigusvaldkondade jaoks
- **Finantsriskide hindamine** kiire otsustamisvõimega

### Juhtumiuuring: klienditugi DeepSeek V3 → Phi-4-mini

Tehnoloogiaettevõte rakendas destilleerimist oma klienditoesüsteemis:

**Rakenduse üksikasjad:**
- **Õpetajamudel**: DeepSeek V3 (671B parameetrit) - suurepärane keerukate kliendipäringute põhjendamiseks
- **Õpilasmudel**: Phi-4-mini - optimeeritud kiireks järelduseks ja juurutamiseks
- **Treeningandmed**: 50 000 klienditoe vestlust
- **Ülesanne**: Mitme pöördega vestluslik tugi tehniliste probleemide lahendamiseks

**Saavutatud tulemused:**
- **85% vähendamine** järelduse ajas (3,2 sekundilt 0,48 sekundini vastuse kohta)
- **95% vähenemine** mälunõuetes (1,2TB-lt 60GB-le)
- **92% säilitamine** algse mudeli täpsusest tugiteenuste ülesannetes
- **60% kulude vähendamine**
- **Paranenud skaleeritavus** - suudab nüüd teenindada 10x rohkem samaaegseid kasutajaid

**Jõudluse jaotus:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Kokkuvõte {#conclusion}

Mudeli destilleerimine on oluline tehnika, mis aitab demokratiseerida juurdepääsu arenenud AI võimekusele. Luues väiksemaid ja tõhusamaid mudeleid, mis säilitavad suuremate mudelite jõudluse, vastab destilleerimine kasvavale vajadusele praktilise AI juurutamise järele.

### Peamised järeldused

1. **Destilleerimine ühendab lõhe** mudeli jõudluse ja praktiliste piirangute vahel
2. **Kaheetapiline protsess** tagab tõhusa teadmiste edastamise õpetajalt õpilasele
3. **Azure ML pakub tugevat infrastruktuuri** destilleerimisprotsesside rakendamiseks
4. **Õige hindamine ja optimeerimine** on eduka destilleerimise jaoks hädavajalikud
5. **Reaalsed rakendused** näitavad märkimisväärseid eeliseid kulude, kiiruse ja juurdepääsetavuse osas

### Tuleviku suunad

Kuna valdkond areneb edasi, võime oodata:
- **Täiustatud destilleerimistehnikaid** paremate teadmiste edastamise meetoditega
- **Mitme õpetaja destilleerimist** õpilasmudeli võimekuse suurendamiseks
- **Automatiseeritud optimeerimist** destilleerimisprotsessis
- **Laiemat mudelite tuge** erinevate arhitektuuride ja valdkondade jaoks

Mudeli destilleerimine annab organisatsioonidele võimaluse kasutada tipptasemel AI võimekust, säilitades samal ajal praktilised juurutamispiirangud, muutes arenenud keelemudelid kättesaadavaks mitmesugustes rakendustes ja keskkondades.

## ➡️ Mis edasi

- [03: Peenhäälestamine - mudelite kohandamine konkreetsete ülesannete jaoks](./03.SLMOps-Finetuing.md)

---

**Vastutusest loobumine**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta arusaamatuste või valesti tõlgenduste eest, mis võivad tekkida selle tõlke kasutamise tõttu.