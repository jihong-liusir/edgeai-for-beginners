<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-10-11T11:21:23+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "et"
}
-->
# Sektsioon 3: Peenhäälestamine - Mudelite kohandamine konkreetsete ülesannete jaoks

## Sisukord
1. [Sissejuhatus peenhäälestamisse](../../../Module05)
2. [Miks peenhäälestamine on oluline](../../../Module05)
3. [Peenhäälestamise tüübid](../../../Module05)
4. [Peenhäälestamine Microsoft Olive'iga](../../../Module05)
5. [Praktilised näited](../../../Module05)
6. [Parimad tavad ja juhised](../../../Module05)
7. [Täiustatud tehnikad](../../../Module05)
8. [Hindamine ja jälgimine](../../../Module05)
9. [Levinud väljakutsed ja lahendused](../../../Module05)
10. [Kokkuvõte](../../../Module05)

## Sissejuhatus peenhäälestamisse

**Peenhäälestamine** on võimas masinõppe tehnika, mis hõlmab eelnevalt treenitud mudeli kohandamist konkreetsete ülesannete täitmiseks või spetsialiseeritud andmekogumitega töötamiseks. Selle asemel, et mudelit nullist treenida, kasutatakse peenhäälestamisel eelnevalt treenitud mudeli teadmisi ja kohandatakse seda vastavalt teie konkreetsele kasutusjuhtumile.

### Mis on peenhäälestamine?

Peenhäälestamine on **ülekandeõppe** vorm, kus:
- Alustatakse eelnevalt treenitud mudeliga, mis on õppinud üldisi mustreid suurtest andmekogumitest
- Kohandatakse mudeli sisemisi parameetreid teie konkreetse andmekogumi abil
- Säilitatakse väärtuslikud teadmised, samal ajal spetsialiseerides mudelit teie ülesande jaoks

Mõelge sellele nagu õpetaksite kogenud kokka valmistama uut kööki – ta juba mõistab toiduvalmistamise põhitõdesid, kuid vajab konkreetseid tehnikaid ja maitseid uue stiili jaoks.

### Peamised eelised

- **Ajasäästlikkus**: Oluliselt kiirem kui nullist treenimine
- **Andmesäästlikkus**: Vajab väiksemaid andmekogumeid hea tulemuse saavutamiseks
- **Kulutõhusus**: Madalamad arvutusnõuded
- **Parem jõudlus**: Sageli saavutab paremaid tulemusi võrreldes nullist treenimisega
- **Ressursside optimeerimine**: Muudab võimsa AI kättesaadavaks väiksematele meeskondadele ja organisatsioonidele

## Miks peenhäälestamine on oluline

### Reaalsed rakendused

Peenhäälestamine on hädavajalik mitmesugustes olukordades:

**1. Valdkonna kohandamine**
- Meditsiiniline AI: Üldiste keelemudelite kohandamine meditsiinilise terminoloogia ja kliiniliste märkmete jaoks
- Õiguslik tehnoloogia: Mudelite spetsialiseerimine juriidiliste dokumentide analüüsi ja lepingute ülevaatuse jaoks
- Finantsteenused: Mudelite kohandamine finantsaruannete analüüsi ja riskihindamise jaoks

**2. Ülesande spetsialiseerimine**
- Sisu loomine: Peenhäälestamine konkreetsete kirjutamisstiilide või toonide jaoks
- Koodi genereerimine: Mudelite kohandamine konkreetsete programmeerimiskeelte või raamistikute jaoks
- Tõlkimine: Tõhususe parandamine konkreetsete keelepaaride või tehniliste valdkondade jaoks

**3. Ettevõtte rakendused**
- Klienditeenindus: Chatbotide loomine, mis mõistavad ettevõtte spetsiifilist terminoloogiat
- Sisemine dokumentatsioon: AI-assistentide loomine, mis tunnevad organisatsioonilisi protsesse
- Valdkonnaspetsiifilised lahendused: Mudelite arendamine, mis mõistavad sektori spetsiifilist žargooni ja töövooge

## Peenhäälestamise tüübid

### 1. Täielik peenhäälestamine (juhendatud peenhäälestamine)

Täieliku peenhäälestamise korral uuendatakse treenimise ajal kõiki mudeli parameetreid. See lähenemine:
- Pakub maksimaalset paindlikkust ja jõudluspotentsiaali
- Nõuab märkimisväärseid arvutusressursse
- Tulemuseks on täiesti uus mudeliversioon
- Sobib olukordadesse, kus teil on ulatuslik treeningandmestik ja arvutusressursid

### 2. Parameetrite tõhus peenhäälestamine (PEFT)

PEFT meetodid uuendavad ainult väikest osa parameetritest, muutes protsessi tõhusamaks:

#### Madala astme kohandamine (LoRA)
- Lisab olemasolevatele kaaludele väikeseid treenitavaid maatrikseid
- Vähendab oluliselt treenitavate parameetrite arvu
- Säilitab jõudluse, mis on lähedane täielikule peenhäälestamisele
- Võimaldab lihtsat üleminekut erinevate kohanduste vahel

#### QLoRA (kvantiseeritud LoRA)
- Kombineerib LoRA kvantiseerimistehnikatega
- Vähendab veelgi mälunõudeid
- Võimaldab suuremate mudelite peenhäälestamist tarbijaseadmetel
- Tasakaalustab tõhususe ja jõudluse

#### Adapterid
- Lisavad väikeseid närvivõrke olemasolevate kihtide vahele
- Võimaldavad sihipärast peenhäälestamist, hoides baasmudeli muutumatuna
- Võimaldavad modulaarset lähenemist mudeli kohandamisele

### 3. Ülesandespetsiifiline peenhäälestamine

Keskendub mudelite kohandamisele konkreetsete ülesannete jaoks:
- **Klassifikatsioon**: Mudelite kohandamine kategooriate määramise ülesannete jaoks
- **Generatsioon**: Optimeerimine sisu loomise ja tekstigeneratsiooni jaoks
- **Ekstraktsioon**: Peenhäälestamine teabe eraldamise ja nimede tuvastamise jaoks
- **Kokkuvõtete loomine**: Mudelite spetsialiseerimine dokumentide kokkuvõtete loomiseks

## Peenhäälestamine Microsoft Olive'iga

Microsoft Olive on terviklik mudeli optimeerimise tööriistakomplekt, mis lihtsustab peenhäälestamise protsessi ja pakub ettevõtte tasemel funktsioone.

### Mis on Microsoft Olive?

Microsoft Olive on avatud lähtekoodiga mudeli optimeerimise tööriist, mis:
- Lihtsustab peenhäälestamise töövooge erinevate riistvarasihtmärkide jaoks
- Pakub sisseehitatud tuge populaarsetele mudeliarhitektuuridele (Llama, Phi, Qwen, Gemma)
- Pakub nii pilve- kui ka lokaalseid juurutamisvõimalusi
- Integreerub sujuvalt Azure ML-i ja teiste Microsofti AI-teenustega
- Toetab automaatset optimeerimist ja kvantiseerimist

### Peamised funktsioonid

- **Riistvarateadlik optimeerimine**: Optimeerib mudelid automaatselt konkreetse riistvara jaoks (CPU, GPU, NPU)
- **Mitmeformaadiline tugi**: Töötab PyTorch, Hugging Face'i ja ONNX mudelitega
- **Automatiseeritud töövood**: Vähendab käsitsi seadistamist ja katse-eksituse meetodit
- **Ettevõtte integratsioon**: Sisseehitatud tugi Azure ML-ile ja pilvepõhistele juurutustele
- **Laiendatav arhitektuur**: Võimaldab kohandatud optimeerimistehnikaid

### Paigaldamine ja seadistamine

#### Põhipaigaldus

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Valikulised sõltuvused

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Paigalduse kontrollimine

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktilised näited

### Näide 1: Põhiline peenhäälestamine Olive CLI-ga

See näide demonstreerib väikese keelemudeli peenhäälestamist fraaside klassifitseerimiseks:

#### Samm 1: Keskkonna ettevalmistamine

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Samm 2: Mudeli peenhäälestamine

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Samm 3: Optimeerimine juurutamiseks

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Näide 2: Täiustatud konfiguratsioon kohandatud andmekogumiga

#### Samm 1: Kohandatud andmekogumi ettevalmistamine

Looge JSON-fail oma treeningandmetega:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Samm 2: Konfiguratsioonifaili loomine

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Samm 3: Peenhäälestamise käivitamine

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Näide 3: QLoRA peenhäälestamine mälutõhususe jaoks

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Parimad tavad ja juhised

### Andmete ettevalmistamine

**1. Andmete kvaliteet üle kvantiteedi**
- Eelistage kvaliteetseid ja mitmekesiseid näiteid suure hulga kehvade andmete asemel
- Veenduge, et andmed esindaksid teie sihtkasutusjuhtumit
- Puhastage ja eeltöötlege andmeid järjekindlalt

**2. Andmevorming ja mallid**
- Kasutage treeningnäidete puhul järjekindlat vormingut
- Looge selged sisend-väljundmallid, mis vastavad teie kasutusjuhtumile
- Lisage sobiv juhendvorming juhendtreenitud mudelite jaoks

**3. Andmekogumi jagamine**
- Jätke 10-20% andmetest valideerimiseks
- Säilitage sarnased jaotused treening/valideerimisjaotustes
- Kaaluge klassifikatsiooniülesannete jaoks kihilist valimit

### Treeningkonfiguratsioon

**1. Õppemäära valik**
- Alustage väiksemate õppemääradega (1e-5 kuni 1e-4) peenhäälestamiseks
- Kasutage õppemäära ajastamist parema konvergentsi saavutamiseks
- Jälgige kaokõveraid, et vajadusel määrasid kohandada

**2. Partii suuruse optimeerimine**
- Tasakaalustage partii suurus olemasoleva mäluga
- Kasutage gradientide akumuleerimist suuremate efektiivsete partii suuruste jaoks
- Kaaluge partii suuruse ja õppemäära vahelist seost

**3. Treeningu kestus**
- Jälgige valideerimismõõdikuid, et vältida ületreenimist
- Kasutage varajast peatamist, kui valideerimise jõudlus stabiliseerub
- Salvestage regulaarselt kontrollpunkte taastamiseks ja analüüsiks

### Mudeli valik

**1. Baasmudeli valik**
- Valige mudelid, mis on eelnevalt treenitud sarnastel valdkondadel, kui võimalik
- Kaaluge mudeli suurust vastavalt oma arvutuspiirangutele
- Hinnake litsentsinõudeid kommertskasutuseks

**2. Peenhäälestamise meetodi valik**
- Kasutage LoRA/QLoRA-d ressursipiiratud keskkondades
- Valige täielik peenhäälestamine, kui maksimaalne jõudlus on kriitiline
- Kaaluge adapteripõhiseid lähenemisi mitme ülesande stsenaariumide jaoks

### Ressursside haldamine

**1. Riistvara optimeerimine**
- Valige sobiv riistvara oma mudeli suuruse ja meetodi jaoks
- Kasutage GPU mälu tõhusalt gradientide kontrollpunktidega
- Kaaluge pilvepõhiseid lahendusi suuremate mudelite jaoks

**2. Mäluhaldus**
- Kasutage segatäpsusega treenimist, kui see on saadaval
- Rakendage gradientide akumuleerimist mälupiirangute jaoks
- Jälgige GPU mälukasutust kogu treeningu vältel

## Täiustatud tehnikad

### Mitme adapteri treenimine

Treeni mitu adapterit erinevate ülesannete jaoks, jagades samal ajal baasmudelit:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hüpperparameetrite optimeerimine

Rakendage süstemaatilist hüpperparameetrite häälestamist:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Kohandatud kaofunktsioonid

Rakendage valdkonnaspetsiifilisi kaofunktsioone:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Hindamine ja jälgimine

### Mõõdikud ja hindamine

**1. Standardmõõdikud**
- **Täpsus**: Üldine korrektsus klassifikatsiooniülesannete jaoks
- **Perpleksus**: Keelemudeli kvaliteedi mõõt
- **BLEU/ROUGE**: Teksti genereerimise ja kokkuvõtete kvaliteet
- **F1-skoor**: Tasakaalustatud täpsus ja tagasikutsumine klassifikatsiooni jaoks

**2. Valdkonnaspetsiifilised mõõdikud**
- **Ülesandespetsiifilised võrdlusalused**: Kasutage oma valdkonna jaoks kehtestatud võrdlusaluseid
- **Inimeste hindamine**: Kaasake inimeste hinnang subjektiivsete ülesannete jaoks
- **Ärimõõdikud**: Joondage tegelike ärieesmärkidega

**3. Hindamise seadistamine**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Treeningu edenemise jälgimine

**1. Kaokõverate jälgimine**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Valideerimise jälgimine**
- Jälgige valideerimiskaotust koos treeningkaotusega
- Jälgige ületreenimise märke (valideerimiskaotus suureneb, samal ajal kui treeningkaotus väheneb)
- Kasutage varajast peatamist valideerimismõõdikute põhjal

**3. Ressursside jälgimine**
- Jälgige GPU/CPU kasutust
- Jälgige mälukasutuse mustreid
- Jälgige treeningu kiirust ja läbilaskevõimet

## Levinud väljakutsed ja lahendused

### Väljakutse 1: Ületreenimine

**Sümptomid:**
- Treeningkaotus jätkab vähenemist, samal ajal kui valideerimiskaotus suureneb
- Suur erinevus treeningu ja valideerimise jõudluse vahel
- Kehv üldistus uutele andmetele

**Lahendused:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Väljakutse 2: Mälupiirangud

**Lahendused:**
- Kasutage gradientide kontrollpunktimist
- Rakendage gradientide akumuleerimist
- Valige parameetrite tõhusad meetodid (LoRA, QLoRA)
- Kasutage mudeli paralleelsust suurte mudelite jaoks

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Väljakutse 3: Aeglane treenimine

**Lahendused:**
- Optimeerige andmete laadimise torustikke
- Kasutage segatäpsusega treenimist
- Rakendage tõhusaid partii strateegiaid
- Kaaluge hajutatud treenimist suurte andmekogumite jaoks

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Väljakutse 4: Kehv jõudlus

**Diagnostika sammud:**
1. Kontrollige andmete kvaliteeti ja vormingut
2. Kontrollige õppemäära ja treeningu kestust
3. Hinnake baasmudeli valikut
4. Vaadake üle eeltöötlus ja tokeniseerimine

**Lahendused:**
- Suurendage treeningandmete mitmekesisust
- Kohandage õppemäära ajakava
- Proovige erinevaid baasmudeleid
- Rakendage andmete suurendamise tehnikaid

## Kokkuvõte

Peenhäälestamine on võimas tehnika, mis demokratiseerib juurdepääsu tipptasemel AI võimekusele. Kasutades tööriistu nagu Microsoft Olive, saavad organisatsioonid tõhusalt kohandada eelnevalt treenitud mudeleid oma konkreetsetele vajadustele, optimeerides samal ajal jõudlust ja ressursipiiranguid.

### Peamised järeldused

1. **Valige õige lähenemine**: Valige peenhäälestamise meetodid vastavalt oma arvutusressurssidele ja jõudlusnõuetele
2. **Andmete kvaliteet on oluline**: Investeerige kvaliteetsetesse ja esinduslikesse treeningandmetesse
3. **Jälgige ja täiustage**: Hindage ja täiustage oma mudeleid pidevalt
4. **Kasut

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.