<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-11T12:13:54+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "et"
}
-->
# Section 2: Qweni mudelipere põhialused

Qweni mudelipere esindab Alibaba Cloudi terviklikku lähenemist suurtele keelemudelitele ja multimodaalsele tehisintellektile, näidates, et avatud lähtekoodiga mudelid võivad saavutada silmapaistvaid tulemusi, olles samal ajal kättesaadavad erinevates juurutusstsenaariumides. Oluline on mõista, kuidas Qweni pere võimaldab võimsaid tehisintellekti võimeid paindlike juurutusvõimalustega, säilitades samal ajal konkurentsivõimelise jõudluse mitmesugustes ülesannetes.

## Ressursid arendajatele

### Hugging Face mudelite hoidla
Valitud Qweni mudelipere mudelid on saadaval [Hugging Face'i](https://huggingface.co/models?search=qwen) kaudu, pakkudes juurdepääsu mõnedele nende mudelite variantidele. Saate uurida olemasolevaid variante, neid oma konkreetsete kasutusjuhtude jaoks peenhäälestada ja juurutada erinevate raamistikute kaudu.

### Kohalikud arendustööriistad
Kohalikuks arenduseks ja testimiseks saate kasutada [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), et käivitada saadavalolevaid Qweni mudeleid oma arendusmasinas optimeeritud jõudlusega.

### Dokumentatsiooni ressursid
- [Qweni mudelite dokumentatsioon](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Qweni mudelite optimeerimine serva juurutamiseks](https://github.com/microsoft/olive)

## Sissejuhatus

Selles juhendis uurime Alibaba Qweni mudelipere ja selle põhikontseptsioone. Käsitleme Qweni pere arengut, uuenduslikke treeningmetoodikaid, mis muudavad Qweni mudelid tõhusaks, pere peamisi variante ja praktilisi rakendusi erinevates stsenaariumides.

## Õpieesmärgid

Selle juhendi lõpuks suudate:

- Mõista Alibaba Qweni mudelipere disainifilosoofiat ja arengut
- Tuvastada peamised uuendused, mis võimaldavad Qweni mudelitel saavutada kõrget jõudlust erinevate parameetrite suuruste juures
- Tunnustada erinevate Qweni mudelite variantide eeliseid ja piiranguid
- Rakendada teadmisi Qweni mudelitest, et valida sobivaid variante reaalses maailmas kasutamiseks

## Kaasaegse tehisintellekti mudelimaastiku mõistmine

Tehisintellekti maastik on oluliselt arenenud, kus erinevad organisatsioonid järgivad erinevaid lähenemisi keelemudelite arendamisel. Kuigi mõned keskenduvad patenteeritud suletud lähtekoodiga mudelitele, rõhutavad teised avatud lähtekoodi kättesaadavust ja läbipaistvust. Traditsiooniline lähenemine hõlmab kas massiivseid patenteeritud mudeleid, mis on kättesaadavad ainult API-de kaudu, või avatud lähtekoodiga mudeleid, mis võivad jääda võimekuselt maha.

See paradigma tekitab väljakutseid organisatsioonidele, kes otsivad võimsaid tehisintellekti võimeid, säilitades samal ajal kontrolli oma andmete, kulude ja juurutuspaindlikkuse üle. Traditsiooniline lähenemine nõuab sageli valikut tipptasemel jõudluse ja praktiliste juurutuskaalutluste vahel.

## Kvaliteetse ja kättesaadava tehisintellekti väljakutse

Kvaliteetse ja kättesaadava tehisintellekti vajadus on muutunud üha olulisemaks erinevates stsenaariumides. Mõelge rakendustele, mis vajavad paindlikke juurutusvõimalusi erinevate organisatsiooniliste vajaduste jaoks, kulutõhusaid lahendusi, kus API kulud võivad muutuda märkimisväärseks, mitmekeelset võimekust globaalseks kasutamiseks või spetsialiseeritud valdkonna teadmisi, näiteks kodeerimises ja matemaatikas.

### Põhilised juurutusnõuded

Kaasaegsed tehisintellekti juurutused seisavad silmitsi mitmete põhinõuetega, mis piiravad praktilist rakendatavust:

- **Kättesaadavus**: Avatud lähtekoodiga kättesaadavus läbipaistvuse ja kohandamise jaoks
- **Kulutõhusus**: Mõistlikud arvutusnõuded erinevate eelarvete jaoks
- **Paindlikkus**: Erineva suurusega mudelid erinevate juurutusstsenaariumide jaoks
- **Globaalne ulatus**: Tugev mitmekeelne ja kultuuridevaheline võimekus
- **Spetsialiseerumine**: Valdkonnaspetsiifilised variandid konkreetsete kasutusjuhtude jaoks

## Qweni mudelite filosoofia

Qweni mudelipere esindab terviklikku lähenemist tehisintellekti mudelite arendamisele, rõhutades avatud lähtekoodi kättesaadavust, mitmekeelset võimekust ja praktilist juurutamist, säilitades samal ajal konkurentsivõimelised jõudlusomadused. Qweni mudelid saavutavad selle mitmekesiste mudelisuuruste, kvaliteetsete treeningmetoodikate ja spetsialiseeritud variantide kaudu erinevate valdkondade jaoks.

Qweni pere hõlmab mitmesuguseid lähenemisi, mis on mõeldud pakkuma valikuid jõudluse ja efektiivsuse spektris, võimaldades juurutamist mobiilseadmetest ettevõtte serveriteni, pakkudes samal ajal tähendusrikkaid tehisintellekti võimeid. Eesmärk on demokratiseerida juurdepääs kvaliteetsele tehisintellektile, pakkudes paindlikkust juurutusvalikutes.

### Qweni põhidisaini põhimõtted

Qweni mudelid põhinevad mitmel aluspõhimõttel, mis eristavad neid teistest keelemudeliperedest:

- **Avatud lähtekood esikohal**: Täielik läbipaistvus ja kättesaadavus uurimis- ja ärikasutuseks
- **Terviklik treening**: Treenimine massiivsetel, mitmekesistel andmekogumitel, mis hõlmavad mitmeid keeli ja valdkondi
- **Mastaabitav arhitektuur**: Erineva suurusega mudelid, mis vastavad erinevatele arvutusnõuetele
- **Spetsialiseeritud tipptase**: Valdkonnaspetsiifilised variandid, mis on optimeeritud konkreetsete ülesannete jaoks

## Qweni pere võimaldavad peamised tehnoloogiad

### Massiivse ulatusega treening

Üks Qweni pere määratlevaid aspekte on treeningandmete ja arvutusressursside massiivne ulatus, mis on investeeritud mudelite arendamisse. Qweni mudelid kasutavad hoolikalt kureeritud, mitmekeelseid andmekogumeid, mis hõlmavad triljoneid token'eid, et pakkuda terviklikku maailmateadmiste ja arutlusvõimekust.

See lähenemine ühendab kvaliteetse veebisisu, akadeemilise kirjanduse, koodirepositooriumid ja mitmekeelsed ressursid. Treeningmetoodika rõhutab nii teadmiste ulatust kui ka sügavust erinevates valdkondades ja keeltes.

### Täiustatud arutlemine ja mõtlemine

Viimased Qweni mudelid sisaldavad keerukaid arutlemisvõimeid, mis võimaldavad keeruliste mitmeastmeliste probleemide lahendamist:

**Mõtlemisrežiim (Qwen3)**: Mudelid suudavad enne lõplike vastuste andmist tegeleda üksikasjaliku samm-sammulise arutlemisega, sarnaselt inimeste probleemilahendusviisidele.

**Kahe režiimi töö**: Võime lülituda kiire vastuse režiimi lihtsate päringute jaoks ja sügavama mõtlemise režiimi keeruliste probleemide jaoks.

**Mõttekäigu integreerimine**: Loomulik arutlemisetappide kaasamine, mis parandab läbipaistvust ja täpsust keerulistes ülesannetes.

### Arhitektuurilised uuendused

Qweni pere sisaldab mitmeid arhitektuurilisi optimeerimisi, mis on mõeldud nii jõudluse kui ka efektiivsuse jaoks:

**Mastaabitav disain**: Järjepidev arhitektuur mudelisuuruste vahel, mis võimaldab lihtsat mastaapimist ja võrdlemist.

**Multimodaalne integreerimine**: Teksti, visiooni ja heli töötlemise võimekuse sujuv integreerimine ühtsetesse arhitektuuridesse.

**Juurutamise optimeerimine**: Mitmed kvantiseerimisvõimalused ja juurutusvormingud erinevate riistvarakonfiguratsioonide jaoks.

## Mudeli suurus ja juurutusvõimalused

Kaasaegsed juurutuskeskkonnad saavad kasu Qweni mudelite paindlikkusest erinevate arvutusnõuete osas:

### Väikesed mudelid (0.5B-3B)

Qweni mudelid pakuvad tõhusaid väikeseid mudeleid, mis sobivad serva juurutamiseks, mobiilirakendusteks ja ressursipiirangutega keskkondadeks, säilitades samal ajal muljetavaldavad võimekused.

### Keskmised mudelid (7B-32B)

Keskmise suurusega mudelid pakuvad täiustatud võimekusi professionaalseteks rakendusteks, pakkudes suurepärast tasakaalu jõudluse ja arvutusnõuete vahel.

### Suured mudelid (72B+)

Täismastaabis mudelid pakuvad tipptasemel jõudlust nõudlike rakenduste, uurimistöö ja ettevõtte juurutuste jaoks, mis vajavad maksimaalset võimekust.

## Qweni mudelipere eelised

### Avatud lähtekoodi kättesaadavus

Qweni mudelid pakuvad täielikku läbipaistvust ja kohandamisvõimalusi, võimaldades organisatsioonidel mõista, muuta ja kohandada mudeleid vastavalt oma vajadustele ilma tarnijate lukustuseta.

### Juurutamise paindlikkus

Mudelite suuruste valik võimaldab juurutamist mitmekesistes riistvarakonfiguratsioonides, alates mobiilseadmetest kuni tipptasemel serveriteni, pakkudes organisatsioonidele paindlikkust nende tehisintellekti infrastruktuuri valikutes.

### Mitmekeelne tipptase

Qweni mudelid paistavad silma mitmekeelse mõistmise ja genereerimise osas, toetades kümneid keeli, eriti tugevalt inglise ja hiina keeles, muutes need sobivaks globaalseks kasutamiseks.

### Konkurentsivõimeline jõudlus

Qweni mudelid saavutavad järjekindlalt konkurentsivõimelisi tulemusi võrdlusalustel, pakkudes samal ajal avatud lähtekoodi kättesaadavust, näidates, et avatud mudelid võivad konkureerida patenteeritud alternatiividega.

### Spetsialiseeritud võimekused

Valdkonnaspetsiifilised variandid, nagu Qwen-Coder ja Qwen-Math, pakuvad spetsialiseeritud teadmisi, säilitades samal ajal üldise keele mõistmise võimekuse.

## Praktilised näited ja kasutusjuhud

Enne tehniliste detailide juurde minemist uurime mõningaid konkreetseid näiteid, mida Qweni mudelid suudavad saavutada:

### Matemaatilise arutlemise näide

Qwen-Math paistab silma samm-sammulise matemaatilise probleemilahenduse osas. Näiteks keerulise diferentsiaalarvutuse probleemi lahendamisel:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Mitmekeelne tugi

Qweni mudelid näitavad tugevat mitmekeelset võimekust erinevates keeltes:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Multimodaalsed võimekused

Qwen-VL suudab samaaegselt töödelda nii teksti kui ka pilte:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Koodi genereerimise näide

Qwen-Coder paistab silma koodi genereerimise ja selgitamise osas mitmes programmeerimiskeeles:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

See rakendus järgib parimaid tavasid, pakkudes selgeid muutujanimesid, põhjalikku dokumentatsiooni ja tõhusat loogikat.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Näide juurutamisest mobiilseadmes kvantiseerimisega
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Laadi kvantiseeritud mudel mobiilse juurutamise jaoks

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qweni pere areng

### Qwen 1.0 ja 1.5: Alusmudelid

Varased Qweni mudelid kehtestasid tervikliku treeningu ja avatud lähtekoodi kättesaadavuse aluspõhimõtted:

- **Qwen-7B (7B parameetrit)**: Esialgne väljaanne, keskendudes hiina ja inglise keele mõistmisele
- **Qwen-14B (14B parameetrit)**: Täiustatud võimekused parema arutlemise ja teadmistega
- **Qwen-72B (72B parameetrit)**: Suuremahuline mudel, mis pakub tipptasemel jõudlust
- **Qwen1.5 seeria**: Laiendatud mitmele suurusele (0.5B kuni 110B) parema pika konteksti käsitlemisega

### Qwen2 pere: Multimodaalne laienemine

Qwen2 seeria tähistas olulist edasiminekut nii keele- kui ka multimodaalsetes võimekustes:

- **Qwen2-0.5B kuni 72B**: Terviklik valik keelemudeleid erinevate juurutusvajaduste jaoks
- **Qwen2-57B-A14B (MoE)**: Ekspertide segu arhitektuur tõhusaks parameetrite kasutamiseks
- **Qwen2-VL**: Täiustatud visiooni ja keele võimekused piltide mõistmiseks
- **Qwen2-Audio**: Heli töötlemise ja mõistmise võimekused
- **Qwen2-Math**: Spetsialiseeritud matemaatiline arutlemine ja probleemilahendus

### Qwen2.5 pere: Täiustatud jõudlus

Qwen2.5 seeria tõi olulisi täiustusi kõigis mõõtmetes:

- **Laiendatud treening**: 18 triljonit token'it treeningandmeid paremate võimekuste jaoks
- **Pikendatud kontekst**: Kuni 128K token'it konteksti pikkus, Turbo variant toetab 1M token'it
- **Täiustatud spetsialiseerumine**: Parendatud Qwen2.5-Coder ja Qwen2.5-Math variandid
- **Parem mitmekeelne tugi**: Täiustatud jõudlus 27+ keeles

### Qwen3 pere: Täiustatud arutlemine

Viimane põlvkond nihutab arutlemise ja mõtlemise võimekuse piire:

- **Qwen3-235B-A22B**: Lipulaev ekspertide segu mudel 235B koguparameetritega
- **Qwen3-30B-A3B**: Tõhus MoE mudel tugeva jõudlusega aktiivse parameetri kohta
- **Tihedad mudelid**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B erinevate juurutusstsenaariumide jaoks
- **Mõtlemisrežiim**: Hübriidarutlemise lähenemine, mis toetab nii kiireid vastuseid kui ka sügavat mõtlemist
- **Mitmekeelne tipptase**: Tugi 119 keelele ja murdele
- **Täiustatud treening**: 36 triljonit token'it mitmekesist ja kvaliteetset treeningandmeid

## Qweni mudelite rakendused

### Ettevõtte rakendused

Organisatsioonid kasutavad Qweni mudeleid dokumentide analüüsiks, klienditeeninduse automatiseerimiseks, koodi genereerimise abiks ja äriluure rakendusteks. Avatud lähtekoodi olemus võimaldab kohandamist konkreetsete ärivajaduste jaoks, säilitades samal ajal andmete privaatsuse ja kontrolli.

### Mobiilne ja serva arvutus

Mobiilirakendused kasutavad Qweni mudeleid reaalajas tõlkimiseks, intelligentsete assistentide jaoks, sisu genereerimiseks ja isikupärastatud soovituste jaoks. Mudelite suuruste valik võimaldab juurutamist mobiilseadmetest servaserveriteni.

### Haridustehnoloogia

Haridusplatvormid kasutavad Qweni mudeleid isikupärastatud juhendamiseks, automatiseeritud sisuloomeks, keeleõppe abiks ja interaktiivsete hariduskogemuste jaoks. Spetsialiseeritud mudelid, nagu Qwen-Math, pakuvad valdk
- Qwen3-235B-A22B saavutab konkurentsivõimelisi tulemusi kodeerimise, matemaatika ja üldiste võimete võrdlusalustes, võrreldes teiste tippmudelitega nagu DeepSeek-R1, o1, o3-mini, Grok-3 ja Gemini-2.5-Pro.
- Qwen3-30B-A3B ületab QwQ-32B mudeli, kasutades 10 korda rohkem aktiveeritud parameetreid.
- Qwen3-4B suudab konkureerida Qwen2.5-72B-Instruct mudeli jõudlusega.

**Tõhususe saavutused:**
- Qwen3-MoE baasmudelid saavutavad sarnase jõudluse Qwen2.5 tihedate baasmudelitega, kasutades vaid 10% aktiivsetest parameetritest.
- Märkimisväärne kulude kokkuhoid nii treenimisel kui ka järeldamisel võrreldes tihedate mudelitega.

**Mitmekeelne võimekus:**
- Qwen3 mudelid toetavad 119 keelt ja murret.
- Tugev jõudlus erinevates keelelistes ja kultuurilistes kontekstides.

**Treeningu ulatus:**
- Qwen3 kasutab peaaegu kaks korda rohkem andmeid, umbes 36 triljonit tokenit, hõlmates 119 keelt ja murret, võrreldes Qwen2.5 18 triljoni tokeniga.

### Mudelite võrdlusmaatriks

| Mudeliseeria | Parameetrite vahemik | Konteksti pikkus | Peamised tugevused | Parimad kasutusjuhtumid |
|--------------|----------------------|------------------|--------------------|------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Tasakaalustatud jõudlus, mitmekeelne | Üldised rakendused, tootmiskasutus |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Koodi genereerimine, programmeerimine | Tarkvaraarendus, koodiabi |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Matemaatiline arutlemine | Haridusplatvormid, STEM-rakendused |
| **Qwen2.5-VL** | Erinevad | Muutuv | Nägemis-keele mõistmine | Multimodaalsed rakendused, pildianalüüs |
| **Qwen3** | 0.6B-235B | Muutuv | Täiustatud arutlemine, mõtlemisrežiim | Keeruline arutlemine, uurimistööd |
| **Qwen3 MoE** | 30B-235B kokku | Muutuv | Tõhus suurte mudelite jõudlus | Ettevõtte rakendused, kõrge jõudluse vajadused |

## Mudeli valiku juhend

### Põhirakenduste jaoks
- **Qwen2.5-0.5B/1.5B**: Mobiilirakendused, servaseadmed, reaalajas rakendused
- **Qwen2.5-3B/7B**: Üldised vestlusrobotid, sisuloome, küsimuste ja vastuste süsteemid

### Matemaatiliste ja arutlemisülesannete jaoks
- **Qwen2.5-Math**: Matemaatiliste probleemide lahendamine ja STEM-haridus
- **Qwen3 mõtlemisrežiimiga**: Keeruline arutlemine, mis nõuab samm-sammulist analüüsi

### Programmeerimise ja arenduse jaoks
- **Qwen2.5-Coder**: Koodi genereerimine, silumine, programmeerimisabi
- **Qwen3**: Täiustatud programmeerimisülesanded koos arutlemisvõimega

### Multimodaalsete rakenduste jaoks
- **Qwen2.5-VL**: Pildi mõistmine, visuaalsete küsimuste vastamine
- **Qwen-Audio**: Helitöötlus ja kõne mõistmine

### Ettevõtte kasutuselevõtuks
- **Qwen2.5-32B/72B**: Kõrge jõudlusega keele mõistmine
- **Qwen3-235B-A22B**: Maksimaalne võimekus nõudlike rakenduste jaoks

## Platvormid ja juurdepääsetavus

### Pilveplatvormid
- **Hugging Face Hub**: Ulatuslik mudelite hoidla koos kogukonna toega
- **ModelScope**: Alibaba mudeliplatvorm koos optimeerimisvahenditega
- **Erinevad pilveteenuse pakkujad**: Tugi standardsete ML-platvormide kaudu

### Kohalikud arendusraamistikud
- **Transformers**: Standardne Hugging Face integratsioon lihtsaks kasutuselevõtuks
- **vLLM**: Kõrge jõudlusega teenindus tootmiskeskkondade jaoks
- **Ollama**: Lihtsustatud kohalik kasutuselevõtt ja haldus
- **ONNX Runtime**: Platvormideülene optimeerimine erinevatele riistvaradele
- **llama.cpp**: Tõhus C++ rakendus mitmekesistele platvormidele

### Õppematerjalid
- **Qwen dokumentatsioon**: Ametlik dokumentatsioon ja mudelikaardid
- **Hugging Face Model Hub**: Interaktiivsed demod ja kogukonna näited
- **Teadusartiklid**: Tehnilised artiklid arxivis süvitsi mõistmiseks
- **Kogukonna foorumid**: Aktiivne kogukonna tugi ja arutelud

### Qwen mudelitega alustamine

#### Arendusplatvormid
1. **Hugging Face Transformers**: Alusta standardse Python integratsiooniga
2. **ModelScope**: Uuri Alibaba optimeeritud kasutuselevõtu tööriistu
3. **Kohalik kasutuselevõtt**: Kasuta Ollama või otseseid transformers-raamistikke kohalikuks testimiseks

#### Õppimistee
1. **Mõista põhikontseptsioone**: Uuri Qwen mudeliperekonna arhitektuuri ja võimekusi
2. **Katseta variante**: Proovi erinevaid mudelisuurusi, et mõista jõudluse kompromisse
3. **Harjuta rakendamist**: Kasuta mudeleid arenduskeskkondades
4. **Optimeeri kasutuselevõtt**: Häälesta tootmiskasutuse juhtumite jaoks

#### Parimad tavad
- **Alusta väikestest mudelitest**: Kasuta alguses väiksemaid mudeleid (1.5B-7B) arenduseks
- **Kasuta vestlusmalli**: Rakenda õiget vormindust parimate tulemuste saavutamiseks
- **Jälgi ressursse**: Jälgi mälu kasutust ja järeldamise kiirust
- **Kaalu spetsialiseerumist**: Vali valdkonnaspetsiifilised variandid, kui see on asjakohane

## Täiustatud kasutusmustrid

### Häälestamise näited

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Spetsialiseeritud promptide kujundamine

**Keeruliste arutlemisülesannete jaoks:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Koodi genereerimiseks kontekstiga:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Mitmekeelsed rakendused

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Tootmiskasutuse mustrid

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Jõudluse optimeerimise strateegiad

### Mälu optimeerimine

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Järeldamise optimeerimine

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Parimad tavad ja juhised

### Turvalisus ja privaatsus

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Jälgimine ja hindamine

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Kokkuvõte

Qwen mudeliperekond esindab terviklikku lähenemist AI-tehnoloogia demokratiseerimisele, säilitades samal ajal konkurentsivõimelise jõudluse mitmekesistes rakendustes. Avatud lähtekoodiga juurdepääsetavuse, mitmekeelse võimekuse ja paindlike kasutuselevõtu võimaluste kaudu võimaldab Qwen organisatsioonidel ja arendajatel kasutada võimsaid AI-võimekusi, olenemata nende ressurssidest või konkreetsetest vajadustest.

### Peamised järeldused

**Avatud lähtekoodi tipptase**: Qwen näitab, et avatud lähtekoodiga mudelid võivad saavutada jõudluse, mis konkureerib patenteeritud alternatiividega, pakkudes samal ajal läbipaistvust, kohandatavust ja kontrolli.

**Mastaapsus arhitektuuris**: Parameetrite vahemik 0.5B kuni 235B võimaldab kasutuselevõttu kogu arvutuskeskkondade spektris, alates mobiilseadmetest kuni ettevõtte klastriteni.

**Spetsialiseeritud võimekus**: Valdkonnaspetsiifilised variandid nagu Qwen-Coder, Qwen-Math ja Qwen-VL pakuvad spetsialiseeritud ekspertiisi, säilitades samal ajal üldise keele mõistmise.

**Globaalne juurdepääsetavus**: Tugev mitmekeelne tugi enam kui 119 keeles muudab Qweni sobivaks rahvusvahelisteks rakendusteks ja mitmekesisteks kasutajateks.

**Pidev innovatsioon**: Qwen 1.0-st Qwen3-ni näitab järjepidevat paranemist võimekustes, tõhususes ja kasutuselevõtu võimalustes.

### Tulevikuväljavaated

Qwen mudeliperekonna arenedes võime oodata:

- **Tõhususe paranemist**: Jätkuv optimeerimine paremate jõudluse ja parameetrite suhete saavutamiseks
- **Laiendatud multimodaalsed võimekused**: Täiustatud nägemise, heli ja teksti töötlemise integreerimine
- **Paranenud arutlemine**: Täiustatud mõtlemismehhanismid ja mitmeastmeline probleemide lahendamine
- **Paremad kasutuselevõtu tööriistad**: Täiustatud raamistikud ja optimeerimisvahendid mitmekesiste kasutuselevõtu stsenaariumide jaoks
- **Kogukonna kasv**: Laiendatud tööriistade, rakenduste ja kogukonna panuste ökosüsteem

### Järgmised sammud

Olgu teie eesmärgiks vestlusrobotite loomine, haridustööriistade arendamine, koodiabi loomine või mitmekeelsete rakenduste arendamine, Qwen mudeliperekond pakub mastaapseid lahendusi tugeva kogukonna toe ja põhjaliku dokumentatsiooniga.

Viimaste uuenduste, mudeliväljaannete ja üksikasjaliku tehnilise dokumentatsiooni saamiseks külastage Qweni ametlikke hoidlaid Hugging Face'is ning uurige aktiivseid kogukonna arutelusid ja näiteid.

AI-arenduse tulevik peitub juurdepääsetavates, läbipaistvates ja võimsates tööriistades, mis võimaldavad innovatsiooni kõigis sektorites ja mastaapides. Qwen mudeliperekond kehastab seda visiooni, pakkudes organisatsioonidele ja arendajatele alust järgmise põlvkonna AI-põhiste rakenduste loomiseks.

## Täiendavad ressursid

- **Ametlik dokumentatsioon**: [Qwen dokumentatsioon](https://qwen.readthedocs.io/)
- **Mudelihub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Teadusartiklid**: [Qwen teaduspublikatsioonid](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Kogukond**: [GitHub arutelud ja probleemid](https://github.com/QwenLM/)
- **ModelScope platvorm**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Õpitulemused

Pärast selle mooduli läbimist suudate:

1. Selgitada Qwen mudeliperekonna arhitektuurilisi eeliseid ja avatud lähtekoodi lähenemist
2. Valida sobiva Qwen variandi vastavalt konkreetsetele rakendusnõuetele ja ressursipiirangutele
3. Rakendada Qwen mudeleid erinevates kasutuselevõtu stsenaariumides optimeeritud konfiguratsioonidega
4. Rakendada kvantiseerimis- ja optimeerimistehnikaid Qwen mudelite jõudluse parandamiseks
5. Hinnata kompromisse mudeli suuruse, jõudluse ja võimekuse vahel Qwen mudeliperekonnas

## Mis edasi

- [03: Gemma perekonna põhialused](03.GemmaFamily.md)

---

**Lahtiütlus**:  
See dokument on tõlgitud, kasutades AI tõlketeenust [Co-op Translator](https://github.com/Azure/co-op-translator). Kuigi püüame tagada täpsust, palun arvestage, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algkeeles tuleks lugeda autoriteetseks allikaks. Olulise teabe puhul on soovitatav kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valede tõlgenduste eest.