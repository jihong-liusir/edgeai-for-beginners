<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T10:33:31+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "tr"
}
-->
# Bölüm 1: SLM İleri Düzey Öğrenme - Temeller ve Optimizasyon

Küçük Dil Modelleri (SLM'ler), EdgeAI'de önemli bir ilerlemeyi temsil eder ve kaynak kısıtlı cihazlarda gelişmiş doğal dil işleme yeteneklerini mümkün kılar. SLM'leri etkili bir şekilde dağıtmayı, optimize etmeyi ve kullanmayı öğrenmek, pratik edge tabanlı yapay zeka çözümleri oluşturmak için gereklidir.

## Giriş

Bu derste Küçük Dil Modelleri (SLM'ler) ve bunların ileri düzey uygulama stratejilerini inceleyeceğiz. SLM'lerin temel kavramlarını, parametre sınırlarını ve sınıflandırmalarını, optimizasyon tekniklerini ve edge computing ortamlarında pratik dağıtım stratejilerini ele alacağız.

## Öğrenme Hedefleri

Bu dersin sonunda şunları yapabileceksiniz:

- 🔢 Küçük Dil Modellerinin parametre sınırlarını ve sınıflandırmalarını anlayın.
- 🛠️ Edge cihazlarda SLM dağıtımı için temel optimizasyon tekniklerini belirleyin.
- 🚀 SLM'ler için ileri düzey kuantizasyon ve sıkıştırma stratejilerini öğrenin.

## SLM Parametre Sınırlarını ve Sınıflandırmalarını Anlama

Küçük Dil Modelleri (SLM'ler), büyük modellerine kıyasla çok daha az parametreyle doğal dil içeriğini işlemek, anlamak ve üretmek için tasarlanmış yapay zeka modelleridir. Büyük Dil Modelleri (LLM'ler) yüz milyarlarca ila trilyonlarca parametre içerirken, SLM'ler verimlilik ve edge dağıtımı için özel olarak tasarlanmıştır.

Parametre sınıflandırma çerçevesi, SLM'lerin farklı kategorilerini ve uygun kullanım alanlarını anlamamıza yardımcı olur. Bu sınıflandırma, belirli edge computing senaryoları için doğru modeli seçmek açısından kritik öneme sahiptir.

### Parametre Sınıflandırma Çerçevesi

Parametre sınırlarını anlamak, farklı edge computing senaryoları için uygun modelleri seçmeye yardımcı olur:

- **🔬 Mikro SLM'ler**: 100M - 1.4B parametre (mobil cihazlar için ultra hafif)
- **📱 Küçük SLM'ler**: 1.5B - 13.9B parametre (performans ve verimlilik dengesi)
- **⚖️ Orta SLM'ler**: 14B - 30B parametre (LLM yeteneklerine yaklaşırken verimliliği koruma)

Araştırma topluluğunda kesin sınır değişkenlik gösterse de, çoğu uygulayıcı 30 milyar parametreden az modele "küçük" olarak bakar; bazı kaynaklar bu eşiği 10 milyar parametreye kadar düşürür.

### SLM'lerin Temel Avantajları

SLM'ler, edge computing uygulamaları için ideal olmalarını sağlayan birkaç temel avantaj sunar:

**Operasyonel Verimlilik**: Daha az parametre işlemeyi gerektirdiğinden SLM'ler daha hızlı çıkarım süreleri sağlar, bu da onları gerçek zamanlı uygulamalar için ideal kılar. Daha düşük hesaplama kaynakları gerektirir, bu da kaynak kısıtlı cihazlarda dağıtımı mümkün kılar ve daha az enerji tüketimi ile karbon ayak izini azaltır.

**Dağıtım Esnekliği**: Bu modeller, internet bağlantısı gerektirmeden cihaz üzerinde yapay zeka yetenekleri sağlar, yerel işlemle gizlilik ve güvenliği artırır, alanlara özel uygulamalar için özelleştirilebilir ve çeşitli edge computing ortamlarına uygundur.

**Maliyet Etkinliği**: SLM'ler, LLM'lere kıyasla daha düşük operasyonel maliyetler ve edge uygulamaları için daha az bant genişliği gereksinimi ile uygun maliyetli eğitim ve dağıtım sunar.

## İleri Düzey Model Edinme Stratejileri

### Hugging Face Ekosistemi

Hugging Face, en son teknoloji SLM'leri keşfetmek ve erişmek için birincil merkezdir. Platform, model keşfi ve dağıtımı için kapsamlı kaynaklar sağlar:

**Model Keşif Özellikleri**: Platform, parametre sayısı, lisans türü ve performans metriklerine göre gelişmiş filtreleme sunar. Kullanıcılar, yan yana model karşılaştırma araçlarına, gerçek zamanlı performans ölçütlerine ve değerlendirme sonuçlarına, ayrıca anında test için WebGPU demolarına erişebilir.

**Özenle Seçilmiş SLM Koleksiyonları**: Popüler modeller arasında gelişmiş akıl yürütme görevleri için Phi-4-mini-3.8B, çok dilli uygulamalar için Qwen3 serisi (0.6B/1.7B/4B), genel amaçlı görevler için Google Gemma3 ve ultra düşük hassasiyetli dağıtım için BitNET gibi deneysel modeller bulunur. Platform ayrıca belirli alanlar için özel modeller içeren topluluk odaklı koleksiyonlar ve farklı kullanım alanları için optimize edilmiş önceden eğitilmiş ve talimatla ayarlanmış varyantlar sunar.

### Azure AI Foundry Model Kataloğu

Azure AI Foundry Model Kataloğu, gelişmiş entegrasyon yetenekleriyle SLM'lere kurumsal düzeyde erişim sağlar:

**Kurumsal Entegrasyon**: Katalog, doğrudan Azure tarafından satılan ve kurumsal düzeyde destek ve SLA'lar içeren modelleri içerir. Gelişmiş akıl yürütme yetenekleri için Phi-4-mini-3.8B ve üretim dağıtımı için Llama 3-8B gibi modelleri içerir. Ayrıca, güvenilir üçüncü taraf açık kaynak modellerinden Qwen3 8B gibi modelleri de içerir.

**Kurumsal Avantajlar**: Model aileleri arasında değiştirilebilir Tahsis Edilmiş Verimlilik ile ince ayar, gözlemlenebilirlik ve sorumlu yapay zeka için yerleşik araçlar. Microsoft'un doğrudan desteği, kurumsal SLA'lar, entegre güvenlik ve uyumluluk özellikleri ve kapsamlı dağıtım iş akışları kurumsal deneyimi geliştirir.

## İleri Düzey Kuantizasyon ve Optimizasyon Teknikleri

### Llama.cpp Optimizasyon Çerçevesi

Llama.cpp, edge dağıtımı için maksimum verimlilik sağlayan en son kuantizasyon tekniklerini sunar:

**Kuantizasyon Yöntemleri**: Çerçeve, Q4_0 (4-bit kuantizasyon ile mükemmel boyut azaltma - Qwen3-0.6B mobil dağıtımı için ideal), Q5_1 (kalite ve sıkıştırmayı dengeleyen 5-bit kuantizasyon - Phi-4-mini-3.8B edge çıkarımı için uygun) ve Q8_0 (orijinal kaliteye yakın 8-bit kuantizasyon - Google Gemma3 üretim kullanımı için önerilir) gibi çeşitli kuantizasyon seviyelerini destekler. BitNET, aşırı sıkıştırma senaryoları için 1-bit kuantizasyon ile en son teknolojiyi temsil eder.

**Uygulama Faydaları**: SIMD hızlandırma ile CPU'ya optimize edilmiş çıkarım, bellek verimli model yükleme ve yürütme sağlar. x86, ARM ve Apple Silicon mimarileri arasında çapraz platform uyumluluğu, donanım bağımsız dağıtım yeteneklerini mümkün kılar.

**Pratik Uygulama Örneği**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Bellek Ayak İzi Karşılaştırması**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Optimizasyon Paketi

Microsoft Olive, üretim ortamları için tasarlanmış kapsamlı model optimizasyon iş akışları sunar:

**Optimizasyon Teknikleri**: Paket, Qwen3 serisi modellerle özellikle etkili olan otomatik hassasiyet seçimi için dinamik kuantizasyon, Google Gemma3 mimarisi için optimize edilmiş grafik optimizasyonu ve operatör birleştirme, CPU, GPU ve NPU için donanım özel optimizasyonları (ARM cihazlarında Phi-4-mini-3.8B için özel destek) ve çok aşamalı optimizasyon iş akışlarını içerir. BitNET modelleri, Olive çerçevesi içinde özel 1-bit kuantizasyon iş akışları gerektirir.

**İş Akışı Otomasyonu**: Optimizasyon varyantları arasında otomatik karşılaştırma, optimizasyon sırasında kalite metriklerinin korunmasını sağlar. PyTorch ve ONNX gibi popüler ML çerçeveleriyle entegrasyon, bulut ve edge dağıtım optimizasyon yetenekleri sunar.

**Pratik Uygulama Örneği**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX Çerçevesi

Apple MLX, Apple Silicon cihazları için özel olarak tasarlanmış yerel optimizasyon sağlar:

**Apple Silicon Optimizasyonu**: Çerçeve, Metal Performance Shaders entegrasyonu ile birleşik bellek mimarisi, otomatik karışık hassasiyet çıkarımı (özellikle Google Gemma3 ile etkili) ve optimize edilmiş bellek bant genişliği kullanımı sağlar. Phi-4-mini-3.8B, M serisi çiplerde olağanüstü performans gösterirken, Qwen3-1.7B MacBook Air dağıtımları için optimal denge sağlar.

**Geliştirme Özellikleri**: NumPy uyumlu dizi işlemleri, otomatik farklılaşma yetenekleri ve Apple geliştirme araçlarıyla sorunsuz entegrasyon sağlayan Python ve Swift API desteği, kapsamlı bir geliştirme ortamı sunar.

**Pratik Uygulama Örneği**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Üretim Dağıtımı ve Çıkarım Stratejileri

### Ollama: Basitleştirilmiş Yerel Dağıtım

Ollama, edge ve yerel ortamlar için kurumsal düzeyde özelliklerle SLM dağıtımını kolaylaştırır:

**Dağıtım Yetenekleri**: Otomatik model çekme ve önbellekleme ile tek komutla model yükleme ve çalıştırma. Phi-4-mini-3.8B, tüm Qwen3 serisi (0.6B/1.7B/4B) ve Google Gemma3 için destek, REST API ile uygulama entegrasyonu ve çoklu model yönetimi ve geçiş yetenekleri. BitNET modelleri, 1-bit kuantizasyon desteği için deneysel yapılandırmalar gerektirir.

**İleri Düzey Özellikler**: Özel model ince ayar desteği, konteynerize dağıtım için Dockerfile oluşturma, otomatik algılama ile GPU hızlandırma ve model kuantizasyon ve optimizasyon seçenekleri, kapsamlı dağıtım esnekliği sağlar.

### VLLM: Yüksek Performanslı Çıkarım

VLLM, yüksek verimlilik senaryoları için üretim düzeyinde çıkarım optimizasyonu sağlar:

**Performans Optimizasyonları**: Bellek verimli dikkat hesaplaması için PagedAttention (özellikle Phi-4-mini-3.8B'nin transformer mimarisi için faydalı), verimlilik optimizasyonu için dinamik toplama (Qwen3 serisi paralel işlem için optimize edilmiş), çoklu GPU ölçeklendirme için tensör paralelliği (Google Gemma3 desteği) ve gecikme azaltımı için spekülatif kod çözme. BitNET modelleri, 1-bit işlemler için özel çıkarım çekirdekleri gerektirir.

**Kurumsal Entegrasyon**: OpenAI uyumlu API uç noktaları, Kubernetes dağıtım desteği, izleme ve gözlemlenebilirlik entegrasyonu ve otomatik ölçeklendirme yetenekleri, kurumsal düzeyde dağıtım çözümleri sağlar.

### Foundry Local: Microsoft'un Edge Çözümü

Foundry Local, kurumsal ortamlar için kapsamlı edge dağıtım yetenekleri sağlar:

**Edge Computing Özellikleri**: Kaynak kısıtlaması optimizasyonu ile çevrimdışı öncelikli mimari tasarımı, yerel model kayıt defteri yönetimi ve edge-to-cloud senkronizasyon yetenekleri, güvenilir edge dağıtımını sağlar.

**Güvenlik ve Uyumluluk**: Gizliliği korumak için yerel veri işleme, kurumsal güvenlik kontrolleri, denetim kaydı ve uyumluluk raporlama ve rol tabanlı erişim yönetimi, edge dağıtımları için kapsamlı güvenlik sağlar.

## SLM Uygulaması için En İyi Uygulamalar

### Model Seçim Kılavuzları

Edge dağıtımı için SLM seçerken şu faktörleri göz önünde bulundurun:

**Parametre Sayısı Düşünceleri**: Ultra hafif mobil uygulamalar için Qwen3-0.6B gibi mikro SLM'leri, dengeli performans senaryoları için Qwen3-1.7B veya Google Gemma3 gibi küçük SLM'leri ve verimliliği korurken LLM yeteneklerine yaklaşan Phi-4-mini-3.8B veya Qwen3-4B gibi orta SLM'leri seçin. BitNET modelleri, belirli araştırma uygulamaları için deneysel ultra sıkıştırma sunar.

**Kullanım Alanı Uyumu**: Model yeteneklerini belirli uygulama gereksinimlerine uygun hale getirin; yanıt kalitesi, çıkarım hızı, bellek kısıtlamaları ve çevrimdışı çalışma gereksinimlerini göz önünde bulundurun.

### Optimizasyon Stratejisi Seçimi

**Kuantizasyon Yaklaşımı**: Kalite gereksinimlerine ve donanım kısıtlamalarına göre uygun kuantizasyon seviyelerini seçin. Maksimum sıkıştırma için Q4_0'ı (Qwen3-0.6B mobil dağıtımı için ideal), kalite-sıkıştırma dengesi için Q5_1'i (Phi-4-mini-3.8B ve Google Gemma3 için uygun) ve orijinal kaliteyi korumak için Q8_0'ı (Qwen3-4B üretim ortamları için önerilir) düşünün. BitNET'in 1-bit kuantizasyonu, özel uygulamalar için aşırı sıkıştırma sınırını temsil eder.

**Çerçeve Seçimi**: Hedef donanım ve dağıtım gereksinimlerine göre optimizasyon çerçevelerini seçin. CPU'ya optimize edilmiş dağıtım için Llama.cpp, kapsamlı optimizasyon iş akışları için Microsoft Olive ve Apple Silicon cihazları için Apple MLX kullanın.

## Pratik Model Örnekleri ve Kullanım Alanları

### Gerçek Dünya Dağıtım Senaryoları

**Mobil Uygulamalar**: Qwen3-0.6B, minimal bellek ayak izi ile akıllı telefon chatbot uygulamalarında mükemmel performans gösterirken, Google Gemma3 tablet tabanlı eğitim araçları için dengeli performans sağlar. Phi-4-mini-3.8B, mobil üretkenlik uygulamaları için üstün akıl yürütme yetenekleri sunar.

**Masaüstü ve Edge Computing**: Qwen3-1.7B, masaüstü asistan uygulamaları için optimal performans sağlar, Phi-4-mini-3.8B geliştirici araçları için gelişmiş kod üretim yetenekleri sunar ve Qwen3-4B, iş istasyonu ortamlarında sofistike belge analizi sağlar.

**Araştırma ve Deneysel**: BitNET modelleri, aşırı düşük hassasiyetli çıkarımın akademik araştırma ve aşırı kaynak kısıtlamaları gerektiren kavram kanıtı uygulamaları için keşfedilmesini sağlar.

### Performans Ölçütleri ve Karşılaştırmalar

**Çıkarım Hızı**: Qwen3-0.6B, mobil CPU'larda en hızlı çıkarım sürelerini elde eder, Google Gemma3 genel uygulamalar için dengeli hız-kalite oranı sağlar, Phi-4-mini-3.8B karmaşık görevler için üstün akıl yürütme hızı sunar ve BitNET, özel donanımla teorik maksimum verimlilik sağlar.

**Bellek Gereksinimleri**: Model bellek ayak izleri, Qwen3-0.6B (1GB altında kuantize edilmiş) ile Phi-4-mini-3.8B (yaklaşık 3-4GB kuantize edilmiş) arasında değişir; BitNET, deneysel yapılandırmalarda 500MB altı ayak izlerine ulaşır.

## Zorluklar ve Dikkat Edilmesi Gerekenler

### Performans Dengeleri

SLM dağıtımı, model boyutu, çıkarım hızı ve çıktı kalitesi arasındaki dengeleri dikkatlice değerlendirmeyi gerektirir. Örneğin, Qwen3-0.6B olağanüstü hız ve verimlilik sunarken, Phi-4-mini-3.8B üstün akıl yürütme yetenekleri sağlar ancak artan kaynak gereksinimleri ile birlikte gelir. Google Gemma3, çoğu genel uygulama için uygun bir orta yol sunar.

### Donanım Uyumluluğu

Farklı edge cihazlar, değişen yeteneklere ve kısıtlamalara sahiptir. Qwen3-0.6B, temel ARM işlemcilerde verimli çalışırken, Google Gemma3 orta düzeyde hesaplama kaynakları gerektirir ve Phi-4-mini-3.8B daha üst düzey edge donanımından faydalanır. BitNET modelleri, optimal 1-bit işlemler için özel donanım veya yazılım uygulamaları gerektirir.

### Güvenlik ve Gizlilik

SLM'ler, yerel işlemle gizliliği artırırken, edge ortamlarında modelleri ve verileri korumak için uygun güvenlik önlemleri uygulanmalıdır. Bu, özellikle Phi-4-mini-3.8B'nin kurumsal ortamlarda veya Qwen3 serisinin hassas verileri işleyen çok dilli uygulamalarda dağıtımı sırasında önemlidir.

## SLM Gelişiminde Gelecek Trendler

SLM alanı, model mimarilerindeki, optimizasyon tekniklerindeki ve

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul edilmez.