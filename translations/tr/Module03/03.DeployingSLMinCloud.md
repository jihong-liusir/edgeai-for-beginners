<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T00:07:48+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "tr"
}
-->
# Konteynerleştirilmiş Bulut Dağıtımı - Üretim Ölçekli Çözümler

Bu kapsamlı eğitim, Microsoft'un Phi-4-mini-instruct modelini konteynerleştirilmiş ortamlarda dağıtmak için üç ana yaklaşımı ele alır: vLLM, Ollama ve ONNX Runtime ile SLM Engine. 3.8 milyar parametreli bu model, mantık yürütme görevleri için ideal bir seçim olup, uç cihazlarda verimliliği korur.

## İçindekiler

1. [Phi-4-mini Konteyner Dağıtımına Giriş](../../../Module03)
2. [Öğrenme Hedefleri](../../../Module03)
3. [Phi-4-mini Sınıflandırmasını Anlama](../../../Module03)
4. [vLLM Konteyner Dağıtımı](../../../Module03)
5. [Ollama Konteyner Dağıtımı](../../../Module03)
6. [ONNX Runtime ile SLM Engine](../../../Module03)
7. [Karşılaştırma Çerçevesi](../../../Module03)
8. [En İyi Uygulamalar](../../../Module03)

## Phi-4-mini Konteyner Dağıtımına Giriş

Küçük Dil Modelleri (SLM'ler), EdgeAI'de önemli bir ilerlemeyi temsil eder ve kaynak kısıtlı cihazlarda gelişmiş doğal dil işleme yetenekleri sağlar. Bu eğitim, Microsoft'un Phi-4-mini-instruct modelinin konteynerleştirilmiş dağıtım stratejilerine odaklanır. Bu model, yetenek ile verimlilik arasında mükemmel bir denge kurar.

### Öne Çıkan Model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 milyar parametre)**: Microsoft'un hafif, talimatlara uyarlanmış en son modeli, bellek/hesaplama açısından kısıtlı ortamlarda olağanüstü yetenekler sunar:
- **Matematiksel mantık yürütme ve karmaşık hesaplamalar**
- **Kod üretimi, hata ayıklama ve analiz**
- **Mantıksal problem çözme ve adım adım mantık yürütme**
- **Detaylı açıklamalar gerektiren eğitim uygulamaları**
- **Fonksiyon çağrısı ve araç entegrasyonu**

"Small SLMs" kategorisinin bir parçası (1.5B - 13.9B parametre), Phi-4-mini mantık yürütme yeteneği ile kaynak verimliliği arasında ideal bir denge sağlar.

### Konteynerleştirilmiş Phi-4-mini Dağıtımının Faydaları

- **Operasyonel Verimlilik**: Daha düşük hesaplama gereksinimleriyle hızlı mantık yürütme
- **Dağıtım Esnekliği**: Yerel işleme ile artırılmış gizlilik sağlayan cihaz üstü AI yetenekleri
- **Maliyet Etkinliği**: Daha büyük modellere kıyasla operasyonel maliyetlerin düşürülmesi ve kaliteyi koruma
- **İzolasyon**: Model örnekleri arasında temiz ayrım ve güvenli çalışma ortamları
- **Ölçeklenebilirlik**: Artan mantık yürütme kapasitesi için kolay yatay ölçeklendirme

## Öğrenme Hedefleri

Bu eğitimin sonunda şunları yapabileceksiniz:

- Phi-4-mini-instruct modelini çeşitli konteynerleştirilmiş ortamlarda dağıtmak ve optimize etmek
- Farklı dağıtım senaryoları için gelişmiş kuantizasyon ve sıkıştırma stratejilerini uygulamak
- Mantık yürütme iş yükleri için üretime hazır konteyner orkestrasyonunu yapılandırmak
- Belirli kullanım senaryosu gereksinimlerine göre uygun dağıtım çerçevelerini değerlendirmek ve seçmek
- Konteynerleştirilmiş SLM dağıtımları için güvenlik, izleme ve ölçeklendirme en iyi uygulamalarını uygulamak

## Phi-4-mini Sınıflandırmasını Anlama

### Model Özellikleri

**Teknik Detaylar:**
- **Parametreler**: 3.8 milyar (Küçük SLM kategorisi)
- **Mimari**: Yoğun yalnızca kod çözücü Transformer, gruplandırılmış sorgu dikkat mekanizması ile
- **Bağlam Uzunluğu**: 128K token (optimal performans için 32K önerilir)
- **Kelime Dağarcığı**: 200K token, çok dilli destek ile
- **Eğitim Verisi**: 5T token yüksek kaliteli mantık yoğun içerik

### Kaynak Gereksinimleri

| Dağıtım Türü | Min RAM | Önerilen RAM | VRAM (GPU) | Depolama | Tipik Kullanım Senaryoları |
|--------------|---------|--------------|------------|----------|----------------------------|
| **Geliştirme** | 6GB | 8GB | - | 8GB | Yerel test, prototipleme |
| **Üretim CPU** | 8GB | 12GB | - | 10GB | Uç sunucular, maliyet odaklı dağıtım |
| **Üretim GPU** | 6GB | 8GB | 4-6GB | 8GB | Yüksek verimli mantık yürütme hizmetleri |
| **Uç Optimizasyonu** | 4GB | 6GB | - | 6GB | Kuantize edilmiş dağıtım, IoT ağ geçitleri |

### Phi-4-mini Yetenekleri

- **Matematiksel Üstünlük**: İleri düzey aritmetik, cebir ve kalkülüs problem çözme
- **Kod Zekası**: Python, JavaScript ve çok dilli kod üretimi ve hata ayıklama
- **Mantıksal Akıl Yürütme**: Adım adım problem ayrıştırma ve çözüm oluşturma
- **Eğitim Desteği**: Öğrenme ve öğretme senaryoları için uygun detaylı açıklamalar
- **Fonksiyon Çağrısı**: Araç entegrasyonu ve API etkileşimleri için yerel destek

## vLLM Konteyner Dağıtımı

vLLM, Phi-4-mini-instruct için optimize edilmiş çıkarım performansı ve OpenAI uyumlu API'ler ile mükemmel destek sağlar, bu da onu üretim mantık yürütme hizmetleri için ideal hale getirir.

### Hızlı Başlangıç Örnekleri

#### Temel CPU Dağıtımı (Geliştirme)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU Hızlandırmalı Üretim Dağıtımı
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Üretim Yapılandırması

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Phi-4-mini Mantık Yürütme Yeteneklerini Test Etme

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama Konteyner Dağıtımı

Ollama, Phi-4-mini-instruct için basitleştirilmiş dağıtım ve yönetim ile mükemmel destek sağlar, bu da onu geliştirme ve dengeli üretim dağıtımları için ideal hale getirir.

### Hızlı Kurulum

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Üretim Yapılandırması

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Model Optimizasyonu ve Varyantlar

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API Kullanım Örnekleri

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## ONNX Runtime ile SLM Engine

ONNX Runtime, Phi-4-mini-instruct modelinin uç dağıtımı için gelişmiş optimizasyon ve platformlar arası uyumluluk ile en iyi performansı sağlar.

### Temel Kurulum

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Basitleştirilmiş Sunucu Uygulaması

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Model Dönüştürme Scripti

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Üretim Yapılandırması

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ONNX Dağıtımını Test Etme

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Karşılaştırma Çerçevesi

### Phi-4-mini için Çerçeve Karşılaştırması

| Özellik | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Kurulum Karmaşıklığı** | Orta | Kolay | Karmaşık |
| **Performans (GPU)** | Mükemmel (~25 tok/s) | Çok İyi (~20 tok/s) | İyi (~15 tok/s) |
| **Performans (CPU)** | İyi (~8 tok/s) | Çok İyi (~12 tok/s) | Mükemmel (~15 tok/s) |
| **Bellek Kullanımı** | 8-12GB | 6-10GB | 4-8GB |
| **API Uyumluluğu** | OpenAI Uyumlu | Özel REST | Özel FastAPI |
| **Fonksiyon Çağrısı** | ✅ Yerel | ✅ Destekleniyor | ⚠️ Özel Uygulama |
| **Kuantizasyon Desteği** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Kuantizasyon |
| **Üretime Hazır** | ✅ Mükemmel | ✅ Çok İyi | ✅ İyi |
| **Uç Dağıtım** | İyi | Mükemmel | Olağanüstü |

## Ek Kaynaklar

### Resmi Belgeler
- **Microsoft Phi-4 Model Kartı**: Ayrıntılı özellikler ve kullanım kılavuzları
- **vLLM Belgeleri**: Gelişmiş yapılandırma ve optimizasyon seçenekleri
- **Ollama Model Kütüphanesi**: Topluluk modelleri ve özelleştirme örnekleri
- **ONNX Runtime Kılavuzları**: Performans optimizasyonu ve dağıtım stratejileri

### Geliştirme Araçları
- **Hugging Face Transformers**: Model etkileşimi ve özelleştirme için
- **OpenAI API Spesifikasyonu**: vLLM uyumluluk testi için
- **Docker En İyi Uygulamalar**: Konteyner güvenliği ve optimizasyon kılavuzları
- **Kubernetes Dağıtımı**: Üretim ölçeklendirme için orkestrasyon desenleri

### Öğrenme Kaynakları
- **SLM Performans Karşılaştırması**: Karşılaştırmalı analiz metodolojileri
- **Uç AI Dağıtımı**: Kaynak kısıtlı ortamlar için en iyi uygulamalar
- **Mantık Yürütme Görev Optimizasyonu**: Matematiksel ve mantıksal problemler için istem stratejileri
- **Konteyner Güvenliği**: AI model dağıtımları için sertleştirme uygulamaları

## Öğrenme Çıktıları

Bu modülü tamamladıktan sonra şunları yapabileceksiniz:

1. Phi-4-mini-instruct modelini birden fazla çerçeve kullanarak konteynerleştirilmiş ortamlarda dağıtmak
2. Farklı donanım ortamları için SLM dağıtımlarını yapılandırmak ve optimize etmek
3. Konteynerleştirilmiş AI dağıtımları için güvenlik en iyi uygulamalarını uygulamak
4. Belirli kullanım senaryosu gereksinimlerine göre uygun dağıtım çerçevelerini karşılaştırmak ve seçmek
5. Üretim düzeyinde SLM hizmetleri için izleme ve ölçeklendirme stratejilerini uygulamak

## Sıradaki Adımlar

- [Modül 1](../Module01/README.md)'e geri dön
- [Modül 2](../Module02/README.md)'ye geri dön

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.