<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-17T22:52:38+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "tr"
}
-->
# Bölüm 3: Gemma Ailesi Temelleri

Gemma model ailesi, Google'ın açık kaynaklı büyük dil modelleri ve çok modlu yapay zeka konusundaki kapsamlı yaklaşımını temsil eder. Bu modeller, erişilebilir olmanın yanı sıra mobil cihazlardan kurumsal iş istasyonlarına kadar çeşitli senaryolarda kullanılabilirken olağanüstü performans sergileyebileceğini göstermektedir. Gemma ailesinin güçlü yapay zeka yeteneklerini esnek dağıtım seçenekleriyle nasıl sağladığını, rekabetçi performansı korurken sorumlu yapay zeka uygulamalarını nasıl benimsediğini anlamak önemlidir.

## Giriş

Bu eğitimde, Google'ın Gemma model ailesini ve temel kavramlarını inceleyeceğiz. Gemma ailesinin evrimini, bu modelleri etkili kılan yenilikçi eğitim yöntemlerini, ailedeki önemli varyantları ve farklı dağıtım senaryolarındaki pratik uygulamaları ele alacağız.

## Öğrenme Hedefleri

Bu eğitimin sonunda şunları yapabileceksiniz:

- Google'ın Gemma model ailesinin tasarım felsefesini ve evrimini anlayın
- Gemma modellerinin farklı parametre boyutlarında yüksek performans elde etmesini sağlayan temel yenilikleri tanımlayın
- Gemma model varyantlarının avantajlarını ve sınırlamalarını tanıyın
- Gerçek dünya senaryoları için uygun varyantları seçmek üzere Gemma modelleri hakkındaki bilgilerinizi uygulayın

## Modern Yapay Zeka Modeli Manzarasını Anlamak

Yapay zeka manzarası önemli ölçüde gelişti ve farklı kuruluşlar dil modeli geliştirme konusunda çeşitli yaklaşımlar benimsiyor. Bazıları yalnızca API'ler aracılığıyla erişilebilen özel kapalı kaynaklı modellere odaklanırken, diğerleri açık kaynaklı erişilebilirlik ve şeffaflığı vurguluyor. Geleneksel yaklaşım, sürekli maliyetlerle büyük özel modeller veya dağıtım için önemli teknik uzmanlık gerektiren açık kaynaklı modeller içerir.

Bu paradigma, güçlü yapay zeka yetenekleri ararken veri, maliyet ve dağıtım esnekliği üzerinde kontrol sağlamaya çalışan kuruluşlar için zorluklar yaratır. Geleneksel yaklaşım genellikle en son performans ile pratik dağıtım hususları arasında seçim yapmayı gerektirir.

## Erişilebilir Yapay Zeka Mükemmelliği Zorluğu

Yüksek kaliteli, erişilebilir yapay zekaya duyulan ihtiyaç, çeşitli senaryolarda giderek daha önemli hale geldi. Farklı kurumsal ihtiyaçlar için esnek dağıtım seçenekleri gerektiren uygulamaları, API maliyetlerinin önemli hale gelebileceği uygun maliyetli uygulamaları, kapsamlı anlayış için çok modlu yetenekleri veya mobil ve uç cihazlarda özel dağıtımı düşünün.

### Temel Dağıtım Gereksinimleri

Modern yapay zeka dağıtımları, pratik uygulanabilirliği sınırlayan birkaç temel gereksinimle karşı karşıyadır:

- **Erişilebilirlik**: Şeffaflık ve özelleştirme için açık kaynaklı erişim
- **Maliyet Etkinliği**: Çeşitli bütçeler için makul hesaplama gereksinimleri
- **Esneklik**: Farklı dağıtım senaryoları için birden fazla model boyutu
- **Çok Modlu Anlama**: Görüntü, metin ve ses işleme yetenekleri
- **Uç Dağıtım**: Mobil ve kaynak kısıtlı cihazlarda optimize edilmiş performans

## Gemma Model Felsefesi

Gemma model ailesi, açık kaynaklı erişilebilirlik, çok modlu yetenekler ve pratik dağıtımı önceliklendirirken rekabetçi performans özelliklerini koruyan yapay zeka modeli geliştirme konusunda Google'ın kapsamlı yaklaşımını temsil eder. Gemma modelleri, Gemini araştırmasından türetilen yüksek kaliteli eğitim yöntemleri ve farklı alanlar ve dağıtım senaryoları için özel varyantlar aracılığıyla çeşitli model boyutlarıyla bunu başarır.

Gemma ailesi, performans-verimlilik spektrumu boyunca seçenekler sunmak için tasarlanmış çeşitli yaklaşımları kapsar ve mobil cihazlardan kurumsal sunuculara kadar dağıtımı mümkün kılarak anlamlı yapay zeka yetenekleri sağlar. Amaç, yüksek kaliteli yapay zeka teknolojisine erişimi demokratikleştirmek ve dağıtım seçimlerinde esneklik sağlamaktır.

### Temel Gemma Tasarım İlkeleri

Gemma modelleri, diğer dil modeli ailelerinden ayıran birkaç temel ilkeye dayalı olarak oluşturulmuştur:

- **Öncelikli Açık Kaynak**: Araştırma ve ticari kullanım için tam şeffaflık ve erişilebilirlik
- **Araştırma Odaklı Geliştirme**: Gemini modellerini güçlendiren aynı araştırma ve teknoloji kullanılarak oluşturulmuştur
- **Ölçeklenebilir Mimari**: Farklı hesaplama gereksinimlerine uygun birden fazla model boyutu
- **Sorumlu Yapay Zeka**: Entegre güvenlik önlemleri ve sorumlu geliştirme uygulamaları

## Gemma Ailesini Etkinleştiren Temel Teknolojiler

### Gelişmiş Eğitim Yöntemleri

Gemma ailesinin tanımlayıcı yönlerinden biri, Google'ın Gemini araştırmasından türetilen sofistike eğitim yaklaşımıdır. Gemma modelleri, matematik, kodlama ve talimat takibinde gelişmiş performans elde etmek için daha büyük modellerden damıtma, insan geri bildiriminden pekiştirmeli öğrenme (RLHF) ve model birleştirme tekniklerinden yararlanır.

Eğitim süreci, daha büyük talimat modellerinden damıtmayı, insan tercihleriyle uyum sağlamak için insan geri bildiriminden pekiştirmeli öğrenmeyi (RLHF), matematiksel akıl yürütme için makine geri bildiriminden pekiştirmeli öğrenmeyi (RLMF) ve kodlama yetenekleri için yürütme geri bildiriminden pekiştirmeli öğrenmeyi (RLEF) içerir.

### Çok Modlu Entegrasyon ve Anlama

Son Gemma modelleri, farklı giriş türleri arasında kapsamlı bir anlayış sağlayan sofistike çok modlu yetenekler içerir:

**Görsel-Dil Entegrasyonu (Gemma 3)**: Gemma 3, hem metni hem de görüntüleri aynı anda işleyebilir, bu da görüntüleri analiz etmesine, görsel içerik hakkında soruları yanıtlamasına, görüntülerden metin çıkarmasına ve karmaşık görsel verileri anlamasına olanak tanır.

**Ses İşleme (Gemma 3n)**: Gemma 3n, İngilizce ile İspanyolca, Fransızca, İtalyanca ve Portekizce arasında çeviri için özellikle güçlü performansla otomatik konuşma tanıma (ASR) ve otomatik konuşma çevirisi (AST) dahil olmak üzere gelişmiş ses yeteneklerine sahiptir.

**Aralıklı Giriş İşleme**: Gemma modelleri, metin, görüntü ve sesin birlikte işlenebileceği karmaşık çok modlu etkileşimlerin anlaşılmasını sağlayan aralıklı girişleri destekler.

### Mimari Yenilikler

Gemma ailesi, hem performans hem de verimlilik için tasarlanmış birkaç mimari optimizasyon içerir:

**Bağlam Penceresi Genişletmesi**: Gemma 3 modelleri, birden fazla belge veya yüzlerce görüntü dahil olmak üzere büyük miktarda bilgiyi işleyebilen, önceki Gemma modellerinden 16 kat daha büyük olan 128K-token bağlam penceresine sahiptir.

**Mobil-Öncelikli Mimari (Gemma 3n)**: Gemma 3n, daha büyük modellerin daha küçük geleneksel modellerle karşılaştırılabilir bellek ayak izleriyle çalışmasına olanak tanıyan Katman Başına Gömme (PLE) teknolojisi ve MatFormer mimarisinden yararlanır.

**Fonksiyon Çağırma Yetenekleri**: Gemma 3, geliştiricilerin programlama arayüzleri için doğal dil arayüzleri oluşturmasına ve akıllı otomasyon sistemleri geliştirmesine olanak tanıyan fonksiyon çağırmayı destekler.

## Model Boyutu ve Dağıtım Seçenekleri

Modern dağıtım ortamları, Gemma modellerinin çeşitli hesaplama gereksinimleri arasında esnekliğinden yararlanır:

### Küçük Modeller (0.6B-4B)

Gemma, etkileyici yetenekleri korurken uç dağıtım, mobil uygulamalar ve kaynak kısıtlı ortamlar için uygun verimli küçük modeller sağlar. 1B modeli küçük uygulamalar için idealdir, 4B modeli ise çok modlu destekle dengeli performans ve esneklik sunar.

### Orta Modeller (8B-14B)

Orta ölçekli modeller, iş istasyonu ve sunucu dağıtımı için hesaplama gereksinimleri ile performans arasında mükemmel bir denge sağlayarak profesyonel uygulamalar için geliştirilmiş yetenekler sunar.

### Büyük Modeller (27B+)

Tam ölçekli modeller, maksimum yetenek gerektiren zorlu uygulamalar, araştırma ve kurumsal dağıtımlar için en son performansı sunar. 27B modeli, tek bir GPU'da çalışabilen en yetenekli seçeneği temsil eder.

### Mobil Optimizasyonlu Modeller (Gemma 3n)

Gemma 3n E2B ve E4B modelleri, sırasıyla 2B ve 4B etkili parametre sayılarıyla mobil ve uç dağıtım için özel olarak tasarlanmıştır ve yenilikçi mimari kullanarak bellek ayak izini E2B için yalnızca 2GB ve E4B için 3GB'a kadar azaltır.

## Gemma Model Ailesinin Faydaları

### Açık Kaynak Erişilebilirliği

Gemma modelleri, sorumlu ticari kullanıma izin veren açık ağırlıklarla tam şeffaflık ve özelleştirme yetenekleri sağlar ve kuruluşların bunları kendi projelerinde ve uygulamalarında ayarlayıp dağıtmalarına olanak tanır.

### Dağıtım Esnekliği

Model boyutlarının çeşitliliği, mobil cihazlardan yüksek performanslı sunuculara kadar çeşitli donanım yapılandırmaları arasında dağıtımı mümkün kılar ve Google Cloud TPU'lar, NVIDIA GPU'lar, ROCm aracılığıyla AMD GPU'lar ve Gemma.cpp aracılığıyla CPU yürütme dahil olmak üzere çeşitli platformlar için optimizasyon sağlar.

### Çok Dilli Mükemmellik

Gemma modelleri, 140'tan fazla dilde olağanüstü çok dilli yetenekler sunarak küresel uygulamalar için uygun hale gelir.

### Rekabetçi Performans

Gemma modelleri, kullanıcı tercih değerlendirmelerinde hem popüler özel hem de açık modeller arasında yüksek sıralamalarla tutarlı bir şekilde rekabetçi sonuçlar elde eder.

### Uzmanlaşmış Yetenekler

Alanlara özgü uygulamalar, Gemma'nın çok modlu anlayışı, fonksiyon çağırma yetenekleri ve çeşitli donanım platformlarında optimize edilmiş performansından yararlanır.

## Pratik Örnekler ve Kullanım Senaryoları

Teknik ayrıntılara geçmeden önce, Gemma modellerinin neler yapabileceğine dair somut örnekler inceleyelim:

### Çok Modlu Anlama Örneği

Gemma 3, hem metni hem de görüntüleri aynı anda işleyerek kapsamlı analiz sağlar:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### Çok Dilli Destek Örneği

Gemma modelleri, 140'tan fazla dilde güçlü çok dilli yetenekler sergiler:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### Fonksiyon Çağırma Örneği

Gemma 3, doğal dil arayüzleri oluşturmak için fonksiyon çağırmayı destekler:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### Mobil Dağıtım Örneği (Gemma 3n)

Gemma 3n, verimli bellek kullanımıyla mobil ve uç dağıtım için optimize edilmiştir:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### Ses İşleme Örneği (Gemma 3n)

Gemma 3n, konuşma tanıma ve çeviri için gelişmiş ses yetenekleri içerir:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Gemma Ailesinin Evrimi

### Gemma 1.0 ve 2.0: Temel Modeller

Erken Gemma modelleri, açık kaynaklı erişilebilirlik ve pratik dağıtımın temel ilkelerini oluşturdu:

- **Gemma-2B ve 7B**: Verimli dil anlayışına odaklanan ilk sürüm
- **Gemma 1.5 Serisi**: Genişletilmiş bağlam işleme ve geliştirilmiş performans
- **Gemma 2 Ailesi**: Çok modlu yeteneklerin ve genişletilmiş model boyutlarının tanıtımı

### Gemma 3: Çok Modlu Mükemmellik

Gemma 3 serisi, çok modlu yetenekler ve performansta önemli ilerlemeyi işaret etti. Gemini 2.0 modellerini güçlendiren aynı araştırma ve teknolojiden oluşturulan Gemma 3, görsel-dil anlayışı, 128K-token bağlam pencereleri, fonksiyon çağırma ve 140'tan fazla dil desteği sundu.

Gemma 3'ün önemli özellikleri şunları içerir:
- **Gemma 3-1B'den 27B'ye**: Çeşitli dağıtım ihtiyaçları için kapsamlı aralık
- **Çok Modlu Anlama**: Gelişmiş metin ve görsel akıl yürütme yetenekleri
- **Genişletilmiş Bağlam**: 128K-token işleme yeteneği
- **Fonksiyon Çağırma**: Doğal dil arayüzü oluşturma
- **Geliştirilmiş Eğitim**: Damıtma ve pekiştirmeli öğrenme kullanılarak optimize edilmiştir

### Gemma 3n: Mobil-Öncelikli Yenilik

Gemma 3n, mobil öncelikli yapay zeka mimarisinde bir atılımı temsil eder ve çığır açan Katman Başına Gömme (PLE) teknolojisi, hesaplama esnekliği için MatFormer mimarisi ve ses işleme dahil kapsamlı çok modlu yetenekler içerir.

Gemma 3n yenilikleri şunları içerir:
- **E2B ve E4B Modelleri**: Azaltılmış bellek ayak iziyle 2B ve 4B parametre performansı
- **Ses Yetenekleri**: Yüksek kaliteli ASR ve konuşma çevirisi
- **Video Anlama**: Önemli ölçüde geliştirilmiş video işleme yetenekleri
- **Mobil Optimizasyon**: Telefonlar ve tabletlerde gerçek zamanlı yapay zeka için tasarlanmıştır

## Gemma Modellerinin Uygulamaları

### Kurumsal Uygulamalar

Kuruluşlar, görsel içerikle belge analizi, çok modlu destekle müşteri hizmetleri otomasyonu, akıllı kodlama yardımı ve iş zekası uygulamaları için Gemma modellerini kullanır. Açık kaynaklı doğası, veri gizliliği ve kontrolü korurken belirli iş ihtiyaçları için özelleştirmeye olanak tanır.

### Mobil ve Uç Hesaplama

Mobil uygulamalar, cihazlarda doğrudan çalışan gerçek zamanlı yapay zekadan yararlanarak kişisel ve özel deneyimler sağlar ve çok modlu yapay zeka yetenekleriyle son derece hızlıdır. Uygulamalar arasında gerçek zamanlı çeviri, akıllı asistanlar, içerik oluşturma ve kişiselleştirilmiş öneriler bulunur.

### Eğitim Teknolojisi

Eğitim platformları, çok modlu öğretim deneyimleri, görsel öğelerle otomatik içerik oluşturma, ses işleme ile dil öğrenme yardımı ve metin, görüntü ve konuşmayı birleştiren etkileşimli eğitim deneyimleri için Gemma modellerini kullanır.

### Küresel Uygulamalar

Uluslararası uygulamalar, Gemma modellerinin güçlü çok dilli ve kültürler arası yeteneklerinden yararlanarak farklı diller ve kültürel bağlamlar arasında tutarlı yapay zeka deneyimleri sağlar.

## Zorluklar ve Sınırlamalar

### Hesaplama Gereksinimleri

Gemma, çeşitli boyutlarda modeller sunsa da, daha büyük varyantlar optimal performans için hala önemli hesaplama kaynakları gerektirir. Bellek gereksinimleri, küçük modeller için yaklaşık 2GB'den en büyük 27B model için 54GB'ye kadar değişir.

### Uzmanlaşmış Alan Performansı

Gemma modelleri genel alanlar ve çok modlu görevlerde iyi performans gösterse de, son derece uzmanlaşmış uygulamalar alanlara özgü ince ayar veya göreve özel optimizasyondan faydalanabilir.

### Model Seçim Karmaşıklığı

Mevcut modellerin, varyantların ve dağıtım seçeneklerinin geniş yelpazesi, ekosisteme yeni olan kullanıcılar için seçim yapmayı zorlaştırabilir ve performans-verimlilik dengelerinin dikkatlice değerlendirilmesini gerektirir.

### Donanım Optimizasyonu

Gemma modelleri, NVIDIA GPU'lar, Google Cloud TPU'lar ve AMD GPU'lar dahil olmak üzere çeşitli platformlar için optimize edilmiştir, ancak performans farklı donanım yapılandırmaları arasında değişebilir.

## Gemma Model Ailesinin Geleceği

Gemma model ailesi, geliştirilmiş verimlilik optimizasyonları, genişletilmiş çok modlu yetenekler ve farklı dağıtım senaryoları arasında daha iyi entegrasyon ile demokratikleşmiş, yüksek kaliteli yapay zekaya doğru devam eden evrimi temsil eder.

Gelecekteki gelişmeler arasında Gemma 3n mimarisinin Android ve Chrome gibi büyük platformlara entegrasyonu yer alır ve geniş bir cihaz ve uygulama yelpazesinde erişilebilir yapay zeka deneyimleri sağlar.

Teknoloji gelişmeye devam ettikçe, Gemma modellerinin açık kaynaklı erişilebilirliklerini korurken giderek daha yetenekli hale gelmesini ve mobil uygulamalardan kurumsal sistemlere kadar çeşitli senaryolar ve kullanım durumları arasında yap
- Gemma 3, geliştiricilere gelişmiş metin ve görsel akıl yürütme yetenekleri sunarak, çok modlu anlayış için görüntü ve metin girdilerini destekleyen güçlü özellikler sağlar.  
- Gemma 3n, Chatbot Arena Elo puanlarında hem popüler özel hem de açık modeller arasında yüksek sıralarda yer alır, bu da kullanıcıların güçlü bir tercih gösterdiğini ifade eder.  

**Verimlilik Başarıları:**  
- Gemma 3 modelleri, önceki Gemma modellerine göre 16 kat daha büyük bir bağlam penceresi olan 128K token'a kadar girişleri işleyebilir.  
- Gemma 3n, daha büyük model yeteneklerini korurken RAM kullanımında önemli bir azalma sağlayan Katman Bazlı Gömüler (PLE) teknolojisinden yararlanır.  

**Mobil Optimizasyon:**  
- Gemma 3n E2B, yalnızca 2GB bellekle çalışabilirken, E4B yalnızca 3GB gerektirir; bu modellerin sırasıyla 5B ve 8B ham parametre sayısına sahip olmasına rağmen.  
- Gizlilik odaklı, çevrimdışı çalışmaya hazır gerçek zamanlı yapay zeka yetenekleri doğrudan mobil cihazlarda kullanılabilir.  

**Eğitim Ölçeği:**  
- Gemma 3, Google TPU'lar ve JAX Framework kullanılarak 1B için 2T token, 4B için 4T, 12B için 12T ve 27B modeller için 14T token ile eğitildi.  

### Model Karşılaştırma Matrisi  

| Model Serisi | Parametre Aralığı | Bağlam Uzunluğu | Temel Güçler | En İyi Kullanım Alanları |  
|--------------|------------------|----------------|---------------|----------------|  
| **Gemma 3** | 1B-27B | 128K | Çok modlu anlayış, fonksiyon çağırma | Genel uygulamalar, görsel-dil görevleri |  
| **Gemma 3n** | E2B (5B), E4B (8B) | Değişken | Mobil optimizasyon, ses işleme | Mobil uygulamalar, uç bilişim, gerçek zamanlı yapay zeka |  
| **Gemma 2.5** | 0.5B-72B | 32K-128K | Dengeli performans, çok dilli | Üretim dağıtımı, mevcut iş akışları |  
| **Gemma-VL** | Çeşitli | Değişken | Görsel-dil uzmanlığı | Görüntü analizi, görsel soru yanıtlama |  

## Model Seçim Rehberi  

### Temel Uygulamalar İçin  
- **Gemma 3-1B**: Hafif metin görevleri, basit mobil uygulamalar  
- **Gemma 3-4B**: Genel kullanım için çok modlu destekle dengeli performans  

### Çok Modlu Uygulamalar İçin  
- **Gemma 3-4B/12B**: Görüntü anlayışı, görsel soru yanıtlama  
- **Gemma 3n**: Ses işleme yetenekleriyle mobil çok modlu uygulamalar  

### Mobil ve Uç Dağıtım İçin  
- **Gemma 3n E2B**: Kaynak kısıtlı cihazlar, gerçek zamanlı mobil yapay zeka  
- **Gemma 3n E4B**: Ses yetenekleriyle geliştirilmiş mobil performans  

### Kurumsal Dağıtım İçin  
- **Gemma 3-12B/27B**: Yüksek performanslı dil ve görsel anlayış  
- **Fonksiyon çağırma yetenekleri**: Akıllı otomasyon sistemleri oluşturma  

### Küresel Uygulamalar İçin  
- **Herhangi bir Gemma 3 varyantı**: 140+ dil desteği ve kültürel anlayış  
- **Gemma 3n**: Ses çevirisiyle mobil odaklı küresel uygulamalar  

## Dağıtım Platformları ve Erişilebilirlik  

### Bulut Platformları  
- **Vertex AI**: Sunucusuz deneyimle uçtan uca MLOps yetenekleri  
- **Google Kubernetes Engine (GKE)**: Karmaşık iş yükleri için ölçeklenebilir konteyner dağıtımı  
- **Google GenAI API**: Hızlı prototipleme için doğrudan API erişimi  
- **NVIDIA API Catalog**: NVIDIA GPU'larda optimize edilmiş performans  

### Yerel Geliştirme Çerçeveleri  
- **Hugging Face Transformers**: Geliştirme için standart entegrasyon  
- **Ollama**: Basitleştirilmiş yerel dağıtım ve yönetim  
- **vLLM**: Üretim için yüksek performanslı sunum  
- **Gemma.cpp**: CPU için optimize edilmiş yürütme  
- **Google AI Edge**: Mobil ve uç dağıtım optimizasyonu  

### Öğrenme Kaynakları  
- **Google AI Studio**: Gemma modellerini birkaç tıklamayla deneyin  
- **Kaggle ve Hugging Face**: Model ağırlıklarını ve topluluk örneklerini indirin  
- **Teknik Raporlar**: Kapsamlı belgeler ve araştırma makaleleri  
- **Topluluk Forumları**: Aktif topluluk desteği ve tartışmalar  

### Gemma Modelleriyle Başlangıç  

#### Geliştirme Platformları  
1. **Google AI Studio**: Web tabanlı deneylerle başlayın  
2. **Hugging Face Hub**: Modelleri ve topluluk uygulamalarını keşfedin  
3. **Yerel Dağıtım**: Ollama veya Transformers kullanarak geliştirme yapın  

#### Öğrenme Yolu  
1. **Temel Kavramları Anlayın**: Çok modlu yetenekleri ve dağıtım seçeneklerini inceleyin  
2. **Varyantları Deneyin**: Farklı model boyutlarını ve özel versiyonları deneyin  
3. **Uygulama Pratiği Yapın**: Modelleri geliştirme ortamlarında dağıtın  
4. **Üretim için Optimize Edin**: Belirli kullanım alanları ve platformlar için ince ayar yapın  

#### En İyi Uygulamalar  
- **Küçük Başlayın**: İlk geliştirme ve test için Gemma 3-4B ile başlayın  
- **Resmi Şablonları Kullanın**: Optimal sonuçlar için uygun sohbet şablonlarını uygulayın  
- **Kaynakları İzleyin**: Bellek kullanımı ve çıkarım performansını takip edin  
- **Uzmanlaşmayı Düşünün**: Çok modlu veya mobil ihtiyaçlar için uygun varyantları seçin  

## İleri Düzey Kullanım Örüntüleri  

### İnce Ayar Örnekleri  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### Özel İfade Mühendisliği  

**Çok Modlu Görevler İçin:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**Bağlamla Fonksiyon Çağırma İçin:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### Kültürel Bağlamla Çok Dilli Uygulamalar  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### Üretim Dağıtım Örüntüleri  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## Performans Optimizasyon Stratejileri  

### Bellek Optimizasyonu  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### Çıkarım Optimizasyonu  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## En İyi Uygulamalar ve Yönergeler  

### Güvenlik ve Gizlilik  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### İzleme ve Değerlendirme  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## Sonuç  

Gemma model ailesi, Google'ın yapay zeka teknolojisini demokratikleştirme konusundaki kapsamlı yaklaşımını temsil ederken, çeşitli uygulamalar ve dağıtım senaryolarında rekabetçi performans sunar. Açık kaynak erişilebilirliği, çok modlu yetenekler ve yenilikçi mimari tasarımlar konusundaki taahhüdü sayesinde Gemma, organizasyonların ve geliştiricilerin güçlü yapay zeka yeteneklerinden yararlanmasını sağlar.  

### Önemli Çıkarımlar  

**Açık Kaynak Mükemmelliği**: Gemma, açık kaynak modellerinin, şeffaflık, özelleştirme ve yapay zeka dağıtımı üzerinde kontrol sağlarken, özel alternatiflerle rekabetçi performansa ulaşabileceğini gösterir.  

**Çok Modlu Yenilik**: Gemma 3 ve Gemma 3n'deki metin, görsel ve ses yeteneklerinin entegrasyonu, farklı giriş türleri arasında kapsamlı anlayış sağlayan erişilebilir çok modlu yapay zekada önemli bir ilerlemeyi temsil eder.  

**Mobil-Öncelikli Mimari**: Gemma 3n'nin Katman Bazlı Gömüler (PLE) teknolojisi ve mobil optimizasyonu, güçlü yapay zekanın kaynak kısıtlı cihazlarda verimli bir şekilde çalışabileceğini gösterir.  

**Ölçeklenebilir Dağıtım**: 1B'den 27B parametreye kadar uzanan yelpaze, özel mobil varyantlarla birlikte, tutarlı kalite ve performansı korurken, tüm hesaplama ortamlarında dağıtımı mümkün kılar.  

**Sorumlu Yapay Zeka Entegrasyonu**: ShieldGemma 2 aracılığıyla yerleşik güvenlik önlemleri ve sorumlu geliştirme uygulamaları, güçlü yapay zeka yeteneklerinin güvenli ve etik bir şekilde dağıtılmasını sağlar.  

### Gelecek Görünümü  

Gemma ailesi geliştikçe, şunları bekleyebiliriz:  

**Gelişmiş Mobil Yetenekler**: Android ve Chrome gibi büyük platformlara Gemma 3n mimarisi entegrasyonu ile mobil ve uç dağıtım için daha fazla optimizasyon.  

**Genişletilmiş Çok Modlu Anlayış**: Daha kapsamlı yapay zeka deneyimleri için görsel-dil-ses entegrasyonunda sürekli ilerleme.  

**Geliştirilmiş Verimlilik**: Daha iyi performans-parametre oranları ve azaltılmış hesaplama gereksinimleri sunmak için devam eden mimari yenilikler.  

**Daha Geniş Ekosistem Entegrasyonu**: Mevcut iş akışlarına sorunsuz entegrasyon için geliştirme çerçeveleri, bulut platformları ve dağıtım araçları arasında artırılmış destek.  

**Topluluk Büyümesi**: Çekirdek yetenekleri genişleten topluluk tarafından oluşturulan modeller, araçlar ve uygulamalarla Gemmaverse'in sürekli genişlemesi.  

### Sonraki Adımlar  

Gerçek zamanlı yapay zeka yetenekleriyle mobil uygulamalar oluşturuyor, çok modlu eğitim araçları geliştiriyor, akıllı otomasyon sistemleri yaratıyor veya çok dilli destek gerektiren küresel uygulamalar üzerinde çalışıyorsanız, Gemma ailesi güçlü topluluk desteği ve kapsamlı belgelerle ölçeklenebilir çözümler sunar.  

**Başlangıç Önerileri:**  
1. **Google AI Studio ile deney yapın**: Hemen pratik yapmaya başlayın  
2. **Hugging Face'den modelleri indirin**: Yerel geliştirme ve özelleştirme için  
3. **Özel varyantları keşfedin**: Mobil uygulamalar için Gemma 3n gibi  
4. **Çok modlu yetenekleri uygulayın**: Kapsamlı yapay zeka deneyimleri için  
5. **Güvenlik en iyi uygulamalarını takip edin**: Üretim dağıtımı için  

**Mobil Geliştirme İçin**: Ses ve görsel yeteneklerle kaynak verimli dağıtım için Gemma 3n E2B ile başlayın.  

**Kurumsal Uygulamalar İçin**: Fonksiyon çağırma ve gelişmiş akıl yürütme yetenekleriyle maksimum kapasite için Gemma 3-12B veya 27B modellerini düşünün.  

**Küresel Uygulamalar İçin**: Kültürel farkındalıkla ifade mühendisliği kullanarak Gemma'nın 140+ dil desteğinden yararlanın.  

**Özel Kullanım Alanları İçin**: İnce ayar yaklaşımlarını ve alan spesifik optimizasyon tekniklerini keşfedin.  

### 🔮 Yapay Zekanın Demokratikleşmesi  

Gemma ailesi, güçlü ve yetenekli modellerin bireysel geliştiricilerden büyük işletmelere kadar herkes için erişilebilir olduğu bir yapay zeka geliştirme geleceğini örneklemektedir. Google, açık kaynak erişilebilirliği ile ileri düzey araştırmayı birleştirerek, tüm sektörlerde ve ölçeklerde yeniliği mümkün kılan bir temel oluşturmuştur.  

Gemma'nın 100 milyondan fazla indirme ve 60.000+ topluluk varyantıyla elde ettiği başarı, yapay zeka teknolojisini ilerletmede açık iş birliğinin gücünü göstermektedir. İleriye doğru ilerlerken, Gemma ailesi, daha önce yalnızca özel, pahalı modellerle mümkün olan uygulamaların geliştirilmesini sağlayarak yapay zeka inovasyonu için bir katalizör olmaya devam edecektir.  

Yapay zekanın geleceği açık, erişilebilir ve güçlüdür – ve Gemma ailesi bu vizyonu gerçeğe dönüştürmede öncülük etmektedir.  

## Ek Kaynaklar  

**Resmi Belgeler ve Modeller:**  
- **Google AI Studio**: [Gemma modellerini doğrudan deneyin](https://aistudio.google.com)  
- **Hugging Face Koleksiyonları**:  
  - [Gemma 3 Yayını](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [Gemma 3n Önizleme](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **Google AI Geliştirici Belgeleri**: [Kapsamlı Gemma rehberleri](https://ai.google.dev/gemma)  
- **Vertex AI Belgeleri**: [Kurumsal dağıtım rehberleri](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**Teknik Kaynaklar:**  
- **Araştırma Makaleleri ve Teknik Raporlar**: [Google DeepMind yayınları](https://deepmind.google/models/gemma/)  
- **Geliştirici Blog Yazıları**: [En son duyurular ve eğitimler](https://developers.googleblog.com)  
- **Model Kartları**: Ayrıntılı teknik özellikler ve performans ölçütleri  

**Topluluk ve Destek:**  
- **Hugging Face Topluluğu**: Aktif tartışmalar ve topluluk örnekleri  
- **GitHub Depoları**: Açık kaynak uygulamalar ve araçlar  
- **Geliştirici Forumları**: Google AI Geliştirici topluluk desteği  
- **Stack Overflow**: Etiketlenmiş sorular ve topluluk çözümleri  

**Geliştirme Araçları:**  
- **Ollama**: [Basit yerel dağıtım](https://ollama.ai)  
- **vLLM**: [Yüksek performanslı sunum](https://github.com/vllm-project/vllm)  
- **Transformers Kütüphanesi**: [Hugging Face entegrasyonu](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: Mobil ve uç dağıtım optimizasyonu  

**Öğrenme Yolları:**  
- **Başlangıç**: Google AI Studio → Hugging Face örnekleri → Yerel dağıtım  
- **Geliştirici**: Transformers entegrasyonu → Özel uygulamalar → Üretim dağıtımı  
- **Araştırmacı**: Teknik makaleler → İnce ayar → Yenilikçi uygulamalar  
- **Kurumsal**: Vertex AI dağıtımı → Güvenlik uygulaması → Ölçek optimizasyonu  

Gemma model ailesi, yalnızca bir yapay zeka modeli koleksiyonu değil, erişilebilir, güçlü ve sorumlu yapay zeka uygulamaları oluşturmak için eksiksiz bir ekosistemdir. Bugün keşfetmeye başlayın ve açık kaynak yapay zekanın sınırlarını zorlayan geliştiriciler ve araştırmacılar topluluğuna katılın.  

## Ek Kaynaklar  

### Resmi Belgeler  
- Google Gemma Teknik Belgeleri  
- Model Kartları ve Kullanım Kılavuzları  
- Sorumlu Yapay Zeka Uygulama Rehberi  
- Google'ın Vertex AI Entegrasyon Rehberi  

### Geliştirme Araçları  
- Bulut dağıtımı için Google AI Studio  
- Model entegrasyonu için Hugging Face Transformers  
- Yüksek performanslı sunum için vLLM  
- CPU için optimize edilmiş çıkarım için Gemma.cpp  

### Öğrenme Kaynakları  
- Gemma 3 ve Gemma 3n Teknik Makaleleri  
- Google AI Blog ve Eğitimler  
- Model Optimizasyonu ve Kuantizasyon Rehberleri  
- Topluluk Forumları ve Tartışma Grupları  

## Öğrenme Çıktıları  

Bu modülü tamamladıktan sonra şunları yapabileceksiniz:  

1. Gemma model ailesinin mimari avantajlarını ve açık kaynak yaklaşımını açıklayın  
2. Belirli uygulama gereksinimlerine ve donanım kısıtlamalarına göre uygun Gemma varyantını seçin  
3. Gemma modellerini mobilden buluta kadar çeşitli dağıtım senaryolarında optimize edilmiş yapılandırmalarla uygulayın  
4. Gemma model performansını artırmak için kuantizasyon ve optimizasyon tekniklerini uygulayın  
5. Gemma ailesi genelinde model boyutu, performans ve yetenekler arasındaki ödünleşimleri değerlendirin  

## Sırada Ne Var  

- [04: BitNET Ailesi Temelleri](04.BitNETFamily.md)  

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.