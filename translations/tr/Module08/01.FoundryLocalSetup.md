<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T21:32:24+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "tr"
}
-->
# Oturum 1: Foundry Local ile Başlangıç

## Genel Bakış

Microsoft Foundry Local, Azure AI Foundry yeteneklerini doğrudan Windows 11 geliştirme ortamınıza getirerek gizliliği koruyan, düşük gecikmeli AI geliştirme imkanı sunar ve kurumsal düzeyde araçlar sağlar. Bu oturum, phi, qwen, deepseek ve GPT-OSS-20B gibi popüler modellerin tam kurulum, yapılandırma ve uygulamalı dağıtımını kapsar.

## Öğrenme Hedefleri

Bu oturumun sonunda:
- Foundry Local'ı Windows 11 üzerinde kurup yapılandırabileceksiniz
- CLI komutlarını ve yapılandırma seçeneklerini ustalıkla kullanabileceksiniz
- Performansı optimize etmek için model önbellekleme stratejilerini anlayabileceksiniz
- Phi, qwen, deepseek ve GPT-OSS-20B modellerini başarıyla çalıştırabileceksiniz
- Foundry Local kullanarak ilk AI uygulamanızı oluşturabileceksiniz

## Ön Koşullar

### Sistem Gereksinimleri
- **Windows 11**: Sürüm 22H2 veya daha yeni
- **RAM**: Minimum 16GB, önerilen 32GB
- **Depolama**: Modeller ve önbellek için 50GB boş alan
- **Donanım**: NPU veya GPU destekli cihaz tercih edilir (Copilot+ PC veya NVIDIA GPU)
- **Ağ**: Modelleri indirmek için yüksek hızlı internet bağlantısı

### Geliştirme Ortamı
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Bölüm 1: Kurulum ve Ayarlar

### Adım 1: Foundry Local'ı Kurun

Foundry Local'ı Winget kullanarak veya GitHub'dan yükleyici indirerek kurun:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Adım 2: Kurulumu Doğrulayın

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Bölüm 2: CLI'yı Anlamak

### Temel Komut Yapısı

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Bölüm 3: Model Önbellekleme ve Yönetimi

Foundry Local, performansı ve depolamayı optimize etmek için akıllı model önbellekleme uygular:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Bölüm 4: Uygulamalı Model Dağıtımı

### Microsoft Phi Modellerini Çalıştırma

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen Modelleri ile Çalışma

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek Modellerini Çalıştırma

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B Çalıştırma

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Bölüm 5: İlk Uygulamanızı Oluşturma

### Modern Sohbet Uygulaması (OpenAI SDK + Foundry Local)

OpenAI SDK ile Foundry Local entegrasyonunu kullanarak üretime hazır bir sohbet uygulaması oluşturun. Örnek 01'deki desenleri takip edin.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("⚠️ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"🌐 Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"🏠 Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"⚠️ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"🔧 Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Sohbet Uygulamasını Çalıştırın

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Bölüm 6: Sorun Giderme ve En İyi Uygulamalar

### Yaygın Sorunlar ve Çözümleri

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Sistem Kaynaklarını İzleme (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Ortam Değişkenleri

| Değişken | Açıklama | Varsayılan | Gerekli |
|----------|-------------|---------|----------|
| `MODEL` | Model takma adı veya adı | `phi-4-mini` | Hayır |
| `BASE_URL` | Foundry Local temel URL'si | `http://localhost:8000` | Hayır |
| `API_KEY` | API anahtarı (genelde yerel için gerekmez) | `""` | Hayır |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI uç noktası | - | Azure için |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API anahtarı | - | Azure için |
| `AZURE_OPENAI_API_VERSION` | Azure API sürümü | `2024-08-01-preview` | Hayır |

### En İyi Uygulamalar

- **OpenAI SDK kullanın**: Daha iyi sürdürülebilirlik için OpenAI SDK'yı ham HTTP isteklerine tercih edin
- **FoundryLocalManager**: Mümkün olduğunda hizmet yönetimi için resmi SDK'yı kullanın
- **Hata Yönetimi**: Üretim uygulamaları için uygun geri dönüş stratejileri uygulayın
- **Düzenli Güncelleme**: Yeni modeller ve düzeltmelere erişmek için Foundry Local'ı güncel tutun
- **Küçük Başlayın**: Daha küçük modellerle başlayın (Phi mini, Qwen 7B) ve büyütün
- **Kaynakları İzleyin**: İstekleri ve ayarları optimize ederken CPU/GPU/belleği takip edin

## Bölüm 7: Uygulamalı Alıştırmalar

### Alıştırma 1: Hızlı Çoklu Model Testi

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Alıştırma 2: OpenAI SDK Entegrasyon Testi

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"✅ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"❌ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### Alıştırma 3: Kapsamlı Hizmet Sağlık Kontrolü

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"✅ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"✅ {model_id}: Working")
            except Exception as e:
                print(f"❌ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"❌ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Kaynaklar

- **Foundry Local ile Başlangıç**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI referansı ve komutlar genel bakışı**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK entegrasyonu**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face modellerini derleme**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Örnek 01: OpenAI SDK ile Hızlı Sohbet**: samples/01/README.md
- **Örnek 02: Gelişmiş SDK Entegrasyonu**: samples/02/README.md

---

