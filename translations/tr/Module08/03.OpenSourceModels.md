<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T18:29:27+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "tr"
}
-->
# Oturum 3: Foundry Local ile Açık Kaynak Modeller

## Genel Bakış

Bu oturum, açık kaynak modelleri Foundry Local'a nasıl entegre edeceğinizi inceliyor: topluluk modellerini seçmek, Hugging Face içeriklerini entegre etmek ve "kendi modelinizi getirin" (BYOM) stratejilerini benimsemek. Ayrıca sürekli öğrenme ve model keşfi için Model Mondays serisini keşfedeceksiniz.

Referanslar:
- Foundry Local belgeleri: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face modellerini derleme: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Öğrenme Hedefleri
- Yerel çıkarım için açık kaynak modelleri keşfetmek ve değerlendirmek
- Hugging Face modellerini Foundry Local içinde derlemek ve çalıştırmak
- Doğruluk, gecikme ve kaynak ihtiyaçları için model seçme stratejilerini uygulamak
- Modelleri önbellek ve sürüm yönetimi ile yerel olarak yönetmek

## Bölüm 1: Model Keşfi ve Seçimi (Adım adım)

Adım 1) Yerel katalogdaki mevcut modelleri listeleyin  
```cmd
foundry model list
```
  
Adım 2) İki aday modeli hızlıca deneyin (ilk çalıştırmada otomatik indirme)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Adım 3) Temel metrikleri not alın  
- Sabit bir istem için gecikmeyi (öznel) ve kaliteyi gözlemleyin  
- Her model çalışırken Görev Yöneticisi üzerinden bellek kullanımını izleyin  

## Bölüm 2: Katalog Modellerini CLI ile Çalıştırma (Adım adım)

Adım 1) Bir modeli başlatın  
```cmd
foundry model run llama-3.2
```
  
Adım 2) OpenAI uyumlu uç noktadan bir test istemi gönderin  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Bölüm 3: BYOM – Hugging Face Modellerini Derleme (Adım adım)

Modelleri derlemek için resmi yönergeleri takip edin. Aşağıda genel akış verilmiştir—tam komutlar ve desteklenen yapılandırmalar için Microsoft Learn makalesine bakın.

Adım 1) Çalışma dizini hazırlayın  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Adım 2) Desteklenen bir HF modelini derleyin  
- Derlenmiş ONNX modelini `models` dizinine dönüştürmek ve yerleştirmek için Learn belgesindeki adımları kullanın  
- Şu komutla doğrulayın:  
```cmd
foundry cache ls
```
  
Derlenmiş model adınızı görmelisiniz (örneğin, `llama-3.2`).

Adım 3) Derlenmiş modeli çalıştırın  
```cmd
foundry model run llama-3.2 --verbose
```
  
Notlar:  
- Derleme ve çalıştırma için yeterli disk ve RAM olduğundan emin olun  
- Akışı doğrulamak için küçük modellerle başlayın, ardından ölçeklendirin  

## Bölüm 4: Pratik Model Kürasyonu (Adım adım)

Adım 1) Bir `models.json` kaydı oluşturun  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Adım 2) Küçük bir seçici betik  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Bölüm 5: Uygulamalı Benchmarklar (Adım adım)

Adım 1) Basit gecikme benchmarkı  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Adım 2) Kalite kontrolü  
- Sabit bir istem seti kullanın, çıktıları bir CSV/JSON dosyasına kaydedin  
- Akıcılık, alaka düzeyi ve doğruluğu manuel olarak derecelendirin (1–5)  

## Bölüm 6: Sonraki Adımlar
- Yeni modeller ve ipuçları için Model Mondays'e abone olun: https://aka.ms/model-mondays  
- Bulgularınızı ekibinizin `models.json` dosyasına katkı sağlayarak paylaşın  
- Oturum 4'e hazırlanın: LLM'ler ve SLM'ler karşılaştırması, yerel ve bulut çıkarımı, ve uygulamalı demolar  

---

