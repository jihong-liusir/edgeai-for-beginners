<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T21:31:56+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "tr"
}
-->
# Oturum 3: Açık Kaynak Model Keşfi ve Yönetimi

## Genel Bakış

Bu oturum, Foundry Local ile pratik model keşfi ve yönetimine odaklanıyor. Mevcut modelleri listelemeyi, farklı seçenekleri test etmeyi ve temel performans özelliklerini anlamayı öğreneceksiniz. Yaklaşım, doğru modelleri kullanım senaryolarınıza uygun şekilde seçmenize yardımcı olmak için foundry CLI ile uygulamalı keşfi vurgular.

## Öğrenme Hedefleri

- Model keşfi ve yönetimi için foundry CLI komutlarını ustalıkla kullanmak
- Model önbelleği ve yerel depolama desenlerini anlamak
- Farklı modelleri hızlıca test etmek ve karşılaştırmak
- Model seçimi ve kıyaslama için pratik iş akışları oluşturmak
- Foundry Local aracılığıyla sunulan büyüyen model ekosistemini keşfetmek

## Ön Koşullar

- Oturum 1: Foundry Local ile Başlangıç tamamlanmış olmalı
- Foundry Local CLI kurulu ve erişilebilir olmalı
- Model indirmeleri için yeterli depolama alanı (modeller 1GB ile 20GB+ arasında değişebilir)
- Model türleri ve kullanım senaryoları hakkında temel bilgi

## Bölüm 6: Uygulamalı Egzersiz

### Egzersiz: Model Keşfi ve Karşılaştırma

Sample 03'e dayalı kendi model değerlendirme scriptinizi oluşturun:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Göreviniz

1. **Sample 03 scriptini çalıştırın**: `samples\03\list_and_bench.cmd`
2. **Farklı modelleri deneyin**: En az 3 farklı modeli test edin
3. **Performansı karşılaştırın**: Hız ve yanıt kalitesindeki farkları not edin
4. **Bulguları belgeleyin**: Basit bir karşılaştırma tablosu oluşturun

### Örnek Karşılaştırma Formatı

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Bölüm 7: Sorun Giderme ve En İyi Uygulamalar

### Yaygın Sorunlar ve Çözümleri

**Model Başlamıyor:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Yetersiz Bellek:**
- Daha küçük modellerle başlayın (`phi-4-mini`)
- Diğer uygulamaları kapatın
- Bellek sınırlarına sıkça ulaşıyorsanız RAM yükseltin

**Yavaş Performans:**
- Modelin tamamen yüklendiğinden emin olun (ayrıntılı çıktıyı kontrol edin)
- Gereksiz arka plan uygulamalarını kapatın
- Daha hızlı depolama kullanmayı düşünün (SSD)

### En İyi Uygulamalar

1. **Küçükten Başlayın**: Kurulumu doğrulamak için `phi-4-mini` ile başlayın
2. **Bir Seferde Bir Model**: Yeni bir model başlatmadan önce önceki modeli durdurun
3. **Kaynakları İzleyin**: Bellek kullanımını takip edin
4. **Tutarlı Testler Yapın**: Adil karşılaştırmalar için aynı istemleri kullanın
5. **Sonuçları Belgeleyin**: Kullanım senaryolarınız için model performansı hakkında notlar tutun

## Bölüm 8: Sonraki Adımlar ve Kaynaklar

### Oturum 4'e Hazırlık

- **Oturum 4 Konusu**: Optimizasyon araçları ve teknikleri
- **Ön Koşullar**: Model değiştirme ve temel performans testlerinde rahat olmak
- **Önerilen**: Bu oturumdan 2-3 favori model belirlemiş olmak

### Ek Kaynaklar

- **[Foundry Local Belgeleri](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Resmi belgeler
- **[CLI Referansı](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Tam komut referansı
- **[Model Mondays](https://aka.ms/model-mondays)**: Haftalık model incelemeleri
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Topluluk ve sorunlar
- **[Sample 03: Model Keşfi](samples/03/README.md)**: Uygulamalı örnek script

### Temel Çıkarımlar

✅ **Model Keşfi**: Mevcut modelleri keşfetmek için `foundry model list` kullanın  
✅ **Hızlı Test**: Hızlı değerlendirme için `list_and_bench.cmd` desenini kullanın  
✅ **Performans İzleme**: Temel kaynak kullanımı ve yanıt süresi ölçümü  
✅ **Model Seçimi**: Kullanım senaryosuna göre model seçimi için pratik yönergeler  
✅ **Önbellek Yönetimi**: Depolama ve temizlik prosedürlerini anlama  

Artık Foundry Local'ın basit CLI yaklaşımını kullanarak AI uygulamalarınız için uygun modelleri keşfetme, test etme ve seçme konusunda pratik becerilere sahipsiniz.

## Öğrenme Hedefleri

- Yerel çıkarım için açık kaynak modelleri keşfetmek ve değerlendirmek
- Foundry Local içinde seçili Hugging Face modellerini derlemek ve çalıştırmak
- Doğruluk, gecikme ve kaynak ihtiyaçları için model seçimi stratejileri uygulamak
- Modelleri önbellek ve sürüm yönetimi ile yerel olarak yönetmek

## Bölüm 1: Foundry CLI ile Model Keşfi

### Temel Model Yönetim Komutları

Foundry CLI, model keşfi ve yönetimi için basit komutlar sunar:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### İlk Modellerinizi Çalıştırma

Performans özelliklerini anlamak için popüler ve iyi test edilmiş modellerle başlayın:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Not:** `--verbose` bayrağı aşağıdaki gibi ayrıntılı başlangıç bilgileri sağlar:
- Model indirme ilerlemesi (ilk çalıştırmada)
- Bellek tahsis detayları
- Hizmet bağlama bilgileri
- Performans başlatma metrikleri

### Model Kategorilerini Anlama

**Küçük Dil Modelleri (SLM):**
- `phi-4-mini`: Hızlı, verimli, genel sohbet için harika
- `phi-4`: Daha iyi akıl yürütme yetenekleriyle daha yetenekli bir versiyon

**Orta Modeller:**
- `qwen2.5-7b-instruct`: Mükemmel akıl yürütme ve daha uzun bağlam
- `deepseek-r1-distill-qwen-7b`: Kod üretimi için optimize edilmiş

**Büyük Modeller:**
- `llama-3.2`: Meta'nın en son açık kaynak modeli
- `qwen2.5-14b-instruct`: Kurumsal düzeyde akıl yürütme

## Bölüm 2: Hızlı Model Testi ve Karşılaştırma

### Sample 03 Yaklaşımı: Basit Liste ve Kıyaslama

Sample 03 desenine dayalı olarak, işte minimal iş akışı:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Model Performansını Test Etme

Bir model çalıştırıldığında, tutarlı istemlerle test edin:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell ile Test Alternatifi

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Bölüm 3: Model Önbelleği ve Depolama Yönetimi

### Model Önbelleğini Anlama

Foundry Local, model indirmelerini ve önbelleği otomatik olarak yönetir:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Model Depolama Düşünceleri

**Tipik Model Boyutları:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Depolama En İyi Uygulamaları:**
- Hızlı geçiş için 2-3 modeli önbellekte tutun
- Alan boşaltmak için kullanılmayan modelleri kaldırın: `foundry cache clean`
- Özellikle daha küçük SSD'lerde disk kullanımını izleyin
- Model boyutu ve yetenekleri arasındaki dengeyi göz önünde bulundurun

### Model Performans İzleme

Modeller çalışırken, sistem kaynaklarını izleyin:

**Windows Görev Yöneticisi:**
- Bellek kullanımını izleyin (modeller RAM'de yüklü kalır)
- Çıkarım sırasında CPU kullanımını izleyin
- İlk model yükleme sırasında disk I/O'yu kontrol edin

**Komut Satırı İzleme:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Bölüm 4: Pratik Model Seçim Yönergeleri

### Kullanım Senaryosuna Göre Model Seçimi

**Genel Sohbet ve Soru-Cevap için:**
- Başlangıç: `phi-4-mini` (hızlı, verimli)
- Yükseltme: `phi-4` (daha iyi akıl yürütme)
- İleri düzey: `qwen2.5-7b-instruct` (daha uzun bağlam)

**Kod Üretimi için:**
- Önerilen: `deepseek-r1-distill-qwen-7b`
- Alternatif: `qwen2.5-7b-instruct` (kod için de iyi)

**Karmaşık Akıl Yürütme için:**
- En İyisi: `qwen2.5-7b-instruct` veya `qwen2.5-14b-instruct`
- Bütçe seçeneği: `phi-4`

### Donanım Gereksinimleri Kılavuzu

**Minimum Sistem Gereksinimleri:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**En İyi Performans için Önerilen:**
- Rahat çoklu model geçişi için 32GB+ RAM
- Daha hızlı model yükleme için SSD depolama
- İyi tek iş parçacıklı performansa sahip modern CPU
- Hızlandırma için NPU desteği (Windows 11 Copilot+ PC'ler)

### Model Geçiş İş Akışı

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Bölüm 5: Basit Model Kıyaslama

### Temel Performans Testi

Model performansını karşılaştırmak için işte basit bir yaklaşım:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuel Kalite Değerlendirmesi

Her model için tutarlı istemlerle test yapın ve manuel olarak değerlendirin:

**Test İstemleri:**
1. "Kuantum bilgisayarları basit terimlerle açıklayın."
2. "Bir listeyi sıralamak için bir Python fonksiyonu yazın."
3. "Uzaktan çalışmanın artıları ve eksileri nelerdir?"
4. "Edge AI'nin faydalarını özetleyin."

**Değerlendirme Kriterleri:**
- **Doğruluk**: Bilgi doğru mu?
- **Açıklık**: Açıklama kolayca anlaşılabilir mi?
- **Tamlık**: Sorunun tamamını ele alıyor mu?
- **Hız**: Ne kadar hızlı yanıt veriyor?

### Kaynak Kullanımı İzleme

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Bölüm 6: Sonraki Adımlar
- Yeni modeller ve ipuçları için Model Mondays'e abone olun: https://aka.ms/model-mondays
- Bulgularınızı ekibinizin `models.json` dosyasına katkıda bulunun
- Oturum 4'e hazırlanın: LLM'ler ve SLM'ler, yerel ve bulut çıkarımı karşılaştırmaları ve uygulamalı demolar

---

