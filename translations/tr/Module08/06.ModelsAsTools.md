<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T18:31:43+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "tr"
}
-->
# Oturum 6: Foundry Local – Modelleri Araç Olarak Kullanma

## Genel Bakış

AI modellerini, Foundry Local ile doğrudan cihaz üzerinde çalışan modüler ve özelleştirilebilir araçlar olarak ele alın. Bu oturum, gizliliği koruyan, düşük gecikmeli çıkarım için pratik iş akışlarını ve bu araçların SDK'lar, API'ler veya CLI aracılığıyla nasıl entegre edileceğini vurgular. Ayrıca, gerektiğinde Azure AI Foundry'ye nasıl ölçeklenebileceğini öğreneceksiniz.

Referanslar:
- Foundry Local belgeleri: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Çıkarım SDK'ları ile entegrasyon: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face modellerini derleme: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Öğrenme Hedefleri
- Cihaz üzerinde model-araç desenleri tasarlayın
- OpenAI uyumlu REST API veya SDK'lar aracılığıyla entegrasyon yapın
- Modelleri alanlara özgü kullanım durumlarına göre özelleştirin
- Azure AI Foundry'ye hibrit ölçeklendirme planı yapın

## Bölüm 1: Araç Soyutlamaları (Adım adım)

Amaç: Modelleri net sözleşmeler ve basit bir yönlendirici ile araçlar olarak temsil edin.

Adım 1) Araç arayüzü ve kaydını tanımlayın  
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```
  
Adım 2) Foundry Local destekli iki araç uygulayın  
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```
  
Adım 3) Göreve göre yönlendirici  
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```
  

## Bölüm 2: SDK ve API Entegrasyonu (Adım adım)

Amaç: Foundry Local uç noktasına karşı OpenAI Python SDK'sını kullanın.

Adım 1) Kurulum  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```
  
Adım 2) Ortam değişkenlerini yapılandırın  
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```
  
Adım 3) Sohbet API'sini çağırın  
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```
  

## Bölüm 3: Alan Özelleştirme (Adım adım)

Amaç: Çıktıları bir alan için istem şablonları ve JSON şeması kullanarak özelleştirin.

Adım 1) Bir alan istem şablonu oluşturun  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Adım 2) JSON çıktısını zorunlu kılın  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Bölüm 4: Çevrimdışı ve Güvenlik Duruşu (Adım adım)

Amaç: Modelleri yerel olarak araçlar olarak çalıştırırken gizliliği ve dayanıklılığı sağlayın.

Adım 1) Yerel uç noktayı önceden ısıtın ve doğrulayın  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Adım 2) Girdileri temizleyin  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Adım 3) Yalnızca yerel bayrak ve günlükleme  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Bölüm 5: Azure AI Foundry'ye Ölçeklendirme (Adım adım)

Amaç: Yerel modelleri taşma kapasitesi için Azure uç noktalarıyla eşleştirin.

Adım 1) Yönlendirme stratejisini belirleyin  
- Gizlilik/gecikme için öncelikle yerel, hatalar veya büyük istemlerde Azure yedekleme  

Adım 2) Basit bir yönlendirici taslağı uygulayın  
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```
  

## Uygulamalı Kontrol Listesi
- [ ] En az iki aracı kaydedin ve istekleri yönlendirin
- [ ] Foundry Local'ı OpenAI SDK ve ham REST aracılığıyla çağırın
- [ ] Bir alan şablonu için JSON çıktısını zorunlu kılın
- [ ] Çağrıları yerel olarak temizleyin ve günlükleyin
- [ ] Azure yedekleme ile basit bir hibrit yönlendirici uygulayın

## Kapanış

Foundry Local, modellerin birer bileşen araç haline geldiği sağlam cihaz üzerinde AI çözümleri sunar. Net arayüzler, yönetişim ve hibrit ölçeklendirme ile ekipler, kullanıcı gizliliğine saygı gösterirken aynı zamanda kurumsal düzeyde hazır gerçek zamanlı, güvenli AI uygulamaları geliştirebilir.

---

