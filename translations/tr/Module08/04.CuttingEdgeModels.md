<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T18:32:48+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "tr"
}
-->
# Oturum 4: En Son Modeller – LLM'ler, SLM'ler ve Cihaz Üzerinde Çıkarım

## Genel Bakış

LLM'ler ve SLM'leri karşılaştırın, yerel ve bulut çıkarım arasındaki dengeyi değerlendirin ve Phi ve ONNX Runtime kullanarak EdgeAI senaryolarını sergileyen demolar uygulayın. Ayrıca Chainlit RAG, WebGPU çıkarım seçenekleri ve Open WebUI entegrasyonunu vurgulayacağız.

Referanslar:
- Foundry Local belgeleri: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI nasıl yapılır (Open WebUI ile sohbet uygulaması): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Öğrenme Hedefleri
- Maliyet, gecikme ve doğruluk açısından LLM ve SLM arasındaki dengeyi anlayın
- Belirli iş ihtiyaçları için yerel ve bulut çıkarım arasında seçim yapın
- Chainlit ile küçük bir RAG demosu uygulayın
- Tarayıcı tarafı hızlandırma için WebGPU'yu keşfedin
- Open WebUI'yi Foundry Local'a bağlayın

## Bölüm 1: LLM vs SLM – Karar Matrisi

Dikkate alınması gerekenler:
- Gecikme: SLM'ler cihaz üzerinde genellikle saniyenin altında yanıtlar verir
- Maliyet: yerel çıkarım bulut maliyetlerini azaltır
- Gizlilik: hassas veriler cihazda kalır
- Yetkinlik: LLM'ler karmaşık görevlerde SLM'lerden daha iyi performans gösterebilir
- Güvenilirlik: hibrit stratejiler kesinti riskini azaltır

## Bölüm 2: Yerel vs Bulut – Hibrit Modeller

- Büyük/karmaşık istemler için bulut yedekli yerel öncelikli
- Gizlilik hassas veya çevrimdışı senaryolar için yerel destekli bulut öncelikli
- Görev türüne göre yönlendirme (kod üretimi için DeepSeek, genel sohbet için Phi/Qwen)

## Bölüm 3: Chainlit ile RAG Sohbet Uygulaması (Minimal)

Bağımlılıkları yükleyin:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Çalıştırın:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Genişletin: basit bir alıcı (yerel dosyalar) ekleyin ve alınan bağlamı kullanıcı istemine önceden ekleyin.

## Bölüm 4: WebGPU Çıkarımı (Ön Bilgi)

WebGPU kullanarak küçük modelleri doğrudan tarayıcıda çalıştırın. Bu, gizlilik öncelikli demolar ve sıfır kurulum deneyimleri için idealdir. Aşağıda ONNX Runtime Web ve WebGPU yürütme sağlayıcısını kullanarak adım adım minimal bir örnek bulunmaktadır.

1) WebGPU desteğini kontrol edin
- Chromium tarayıcıları: chrome://gpu → “WebGPU” etkin olduğunu doğrulayın
- Programatik kontrol (kodda da kontrol edeceğiz): `if (!('gpu' in navigator)) { /* WebGPU yok */ }`

2) Minimal bir proje oluşturun
Bir klasör ve iki dosya oluşturun: `index.html` ve `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Yerel olarak sunun (Windows cmd.exe)
Tarayıcının modeli alabilmesi için basit bir statik sunucu kullanın.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Tarayıcınızda http://localhost:5173 adresini açın. Başlatma günlüklerini, WebGPU ile oturum oluşturmayı ve bir argmax tahminini görmelisiniz.

4) Sorun giderme
- WebGPU kullanılamıyorsa: Chrome/Edge'i güncelleyin ve GPU sürücülerinin güncel olduğundan emin olun, ardından chrome://flags adresinde “WebGPU'yu Etkinleştir” seçeneğini kontrol edin.
- CORS veya fetch hataları oluşursa: dosyaları http:// üzerinden (file:// değil) sunun ve model URL'sinin çapraz kaynak isteklerine izin verdiğinden emin olun.
- CPU'ya geri dönüş: `executionProviders: ['wasm']` değiştirerek temel davranışı doğrulayın.

5) Sonraki adımlar
- Alanına özgü bir ONNX modeli ekleyin (örneğin, görüntü sınıflandırma veya küçük bir metin modeli).
- Gerçek girdiler için ön işleme/son işleme mantığı ekleyin.
- Daha büyük modeller veya üretim gecikmesi için Foundry Local veya ONNX Runtime Server'ı tercih edin.

## Bölüm 5: Open WebUI + Foundry Local (Adım adım)

Bu, Open WebUI'yi Foundry Local'ın OpenAI uyumlu uç noktasına bağlayarak yerel bir sohbet arayüzü sağlar.

1) Ön koşullar
- Foundry Local kurulu ve çalışıyor (`foundry --version`)
- Yerel olarak çalıştırılmaya hazır bir model (örneğin, `phi-4-mini`)
- Docker Desktop kurulu (Open WebUI için önerilir)

2) Foundry Local ile bir model başlatın
```powershell
foundry model run phi-4-mini
```
Bu, `http://localhost:8000` adresinde OpenAI uyumlu bir API sağlar.

3) Open WebUI'yi başlatın (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Notlar:
- Windows'da, `host.docker.internal` konteynerin ana bilgisayarınıza `localhost` üzerinden ulaşmasını sağlar.
- `OPENAI_API_BASE_URL`'i Foundry Local'ın uç noktasına ve sahte bir `OPENAI_API_KEY`'e ayarlıyoruz.

4) Open WebUI arayüzünden yapılandırma (alternatif)
- http://localhost:3000 adresine gidin
- İlk kurulumu tamamlayın (yönetici kullanıcı)
- Ayarlar → Modeller/Sağlayıcılar bölümüne gidin
- Temel URL'yi ayarlayın: `http://host.docker.internal:8000/v1`
- API Anahtarını ayarlayın: `local-key` (yer tutucu)
- Kaydedin

5) Test istemi çalıştırın
- Open WebUI sohbetinde model adı olarak `phi-4-mini` seçin veya girin
- İstem: “Cihaz üzerinde AI çıkarımının beş faydasını listeleyin.”
- Yerel modelinizden gelen bir yanıtın akışını görmelisiniz

6) Sorun giderme
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) İsteğe bağlı: Open WebUI verilerini kalıcı hale getirin
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Uygulamalı Kontrol Listesi
- [ ] Yerel olarak SLM ve LLM yanıtlarını/gecikmelerini karşılaştırın
- [ ] Chainlit demosunu en az iki modele karşı çalıştırın
- [ ] Open WebUI'yi yerel uç noktanıza bağlayın ve test edin

## Sonraki Adımlar
- Oturum 5'teki ajan iş akışlarına hazırlanın
- Hibrit yerel/bulut modellerinin ROI'yi nasıl artırabileceği senaryolarını belirleyin

---

