<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:19:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "tr"
}
-->
# Oturum 4: Chainlit ile Üretim Chat Uygulamaları Oluşturma

## Genel Bakış

Bu oturum, Chainlit ve Microsoft Foundry Local kullanarak üretime hazır chat uygulamaları oluşturmayı ele alıyor. AI sohbetleri için modern web arayüzleri oluşturmayı, akış yanıtlarını uygulamayı ve hata yönetimi ile kullanıcı deneyimi tasarımıyla sağlam chat uygulamaları dağıtmayı öğreneceksiniz.

**Neler Yapacaksınız:**
- **Chainlit Chat Uygulaması**: Akış yanıtlarıyla modern web arayüzü
- **WebGPU Demo**: Gizlilik odaklı uygulamalar için tarayıcı tabanlı çıkarım  
- **Open WebUI Entegrasyonu**: Foundry Local ile profesyonel chat arayüzü
- **Üretim Kalıpları**: Hata yönetimi, izleme ve dağıtım stratejileri

## Öğrenme Hedefleri

- Chainlit ile üretime hazır chat uygulamaları oluşturma
- Gelişmiş kullanıcı deneyimi için akış yanıtlarını uygulama
- Foundry Local SDK entegrasyon kalıplarını öğrenme
- Doğru hata yönetimi ve zarif bozulma uygulama
- Farklı ortamlar için chat uygulamalarını dağıtma ve yapılandırma
- Konuşma AI için modern web arayüzü kalıplarını anlama

## Ön Koşullar

- **Foundry Local**: Kurulu ve çalışır durumda ([Kurulum Kılavuzu](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 veya üstü, sanal ortam desteğiyle
- **Model**: En az bir model yüklenmiş (`foundry model run phi-4-mini`)
- **Tarayıcı**: WebGPU destekli modern web tarayıcı (Chrome/Edge)
- **Docker**: Open WebUI entegrasyonu için (isteğe bağlı)

## Bölüm 1: Modern Chat Uygulamalarını Anlama

### Mimari Genel Bakış

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Temel Teknolojiler

**Foundry Local SDK Kalıpları:**
- `FoundryLocalManager(alias)`: Otomatik servis yönetimi
- `manager.endpoint` ve `manager.api_key`: Bağlantı detayları
- `manager.get_model_info(alias).id`: Model tanımlama

**Chainlit Framework:**
- `@cl.on_chat_start`: Sohbet oturumlarını başlatma
- `@cl.on_message`: Gelen kullanıcı mesajlarını işleme  
- `cl.Message().stream_token()`: Gerçek zamanlı akış
- Otomatik UI oluşturma ve WebSocket yönetimi

## Bölüm 2: Yerel ve Bulut Karar Matrisi

### Performans Özellikleri

| Özellik | Yerel (Foundry) | Bulut (Azure OpenAI) |
|---------|-----------------|---------------------|
| **Gecikme** | 🚀 50-200ms (ağ yok) | ⏱️ 200-2000ms (ağ bağımlı) |
| **Gizlilik** | 🔒 Veri cihazdan çıkmaz | ⚠️ Veri buluta gönderilir |
| **Maliyet** | 💰 Donanım sonrası ücretsiz | 💸 Token başına ödeme |
| **Çevrimdışı** | ✅ İnternet olmadan çalışır | ❌ İnternet gerektirir |
| **Model Boyutu** | ⚠️ Donanımla sınırlı | ✅ En büyük modellere erişim |
| **Ölçeklenebilirlik** | ⚠️ Donanım bağımlı | ✅ Sınırsız ölçeklenebilirlik |

### Hibrit Strateji Kalıpları

**Yerel-Öncelikli ve Yedekleme:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Görev Tabanlı Yönlendirme:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Bölüm 3: Örnek 04 - Chainlit Chat Uygulaması

### Hızlı Başlangıç

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Uygulama otomatik olarak `http://localhost:8080` adresinde modern bir chat arayüzüyle açılır.

### Temel Uygulama

Örnek 04 uygulaması üretime hazır kalıpları gösterir:

**Otomatik Servis Keşfi:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Akış Chat İşleyicisi:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Yapılandırma Seçenekleri

**Ortam Değişkenleri:**

| Değişken | Açıklama | Varsayılan | Örnek |
|----------|----------|------------|-------|
| `MODEL` | Kullanılacak model takma adı | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local endpoint | Otomatik algılanır | `http://localhost:51211` |
| `API_KEY` | API anahtarı (yerel için isteğe bağlı) | `""` | `your-api-key` |

**Gelişmiş Kullanım:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Bölüm 4: Jupyter Notebooks Oluşturma ve Kullanma

### Notebook Desteği Genel Bakış

Örnek 04, aşağıdaki özellikleri sunan kapsamlı bir Jupyter notebook (`chainlit_app.ipynb`) içerir:

- **📚 Eğitim İçeriği**: Adım adım öğrenme materyalleri
- **🔬 Etkileşimli Keşif**: Kod hücrelerini çalıştırma ve deneme
- **📊 Görsel Gösterimler**: Grafikler, diyagramlar ve çıktı görselleştirme
- **🛠️ Geliştirme Araçları**: Test ve hata ayıklama yetenekleri

### Kendi Notebook'larınızı Oluşturma

#### Adım 1: Jupyter Ortamını Ayarlama

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Adım 2: Yeni Bir Notebook Oluşturma

**VS Code Kullanarak:**
1. VS Code'u Module08 dizininde açın
2. `.ipynb` uzantılı yeni bir dosya oluşturun
3. İstendiğinde "Foundry Local" kernelini seçin
4. İçeriğinizi eklemeye başlayın

**Jupyter Lab Kullanarak:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook Yapısı için En İyi Uygulamalar

#### Hücre Organizasyonu

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Etkileşimli Örnekler ve Egzersizler

#### Egzersiz 1: İstemci Yapılandırma Testi

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Egzersiz 2: Akış Yanıtı Simülasyonu

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Bölüm 5: WebGPU Tarayıcı Çıkarım Demo

### Genel Bakış

WebGPU, maksimum gizlilik ve sıfır kurulum deneyimi için AI modellerini doğrudan tarayıcıda çalıştırmayı sağlar. Bu örnek, ONNX Runtime Web ile WebGPU çalıştırmasını gösterir.

### Adım 1: WebGPU Desteğini Kontrol Etme

**Tarayıcı Gereksinimleri:**
- WebGPU etkin Chrome/Edge 113+
- Kontrol: `chrome://gpu` → "WebGPU" durumunu doğrulayın
- Programatik kontrol: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Adım 2: WebGPU Demo Oluşturma

Dizin oluşturun: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Adım 3: Demoyu Çalıştırma

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Bölüm 6: Open WebUI Entegrasyonu

### Genel Bakış

Open WebUI, Foundry Local'ın OpenAI uyumlu API'sine bağlanan profesyonel bir ChatGPT benzeri arayüz sağlar.

### Adım 1: Ön Koşullar

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Adım 2: Docker Kurulumu (Önerilir)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Not:** `host.docker.internal`, Docker konteynerlerinin Windows'ta ana makineye erişmesini sağlar.

### Adım 3: Yapılandırma

1. **Tarayıcıyı Açın:** `http://localhost:3000` adresine gidin
2. **İlk Kurulum:** Yönetici hesabı oluşturun
3. **Model Yapılandırması:**
   - Ayarlar → Modeller → OpenAI API  
   - Temel URL: `http://host.docker.internal:51211/v1`
   - API Anahtarı: `foundry-local-key` (herhangi bir değer kullanılabilir)
4. **Bağlantıyı Test Etme:** Modeller açılır listede görünmelidir

### Sorun Giderme

**Yaygın Sorunlar:**

1. **Bağlantı Reddedildi:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modeller Görünmüyor:**
   - Modelin yüklü olduğunu doğrulayın: `foundry model list`
   - API yanıtını kontrol edin: `curl http://localhost:51211/v1/models`
   - Open WebUI konteynerini yeniden başlatın

## Bölüm 7: Üretim Dağıtım Düşünceleri

### Ortam Yapılandırması

**Geliştirme Kurulumu:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Üretim Dağıtımı:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Yaygın Port Sorunları ve Çözümleri

**Port 51211 Çakışma Önleme:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Performans İzleme

**Sağlık Kontrolü Uygulaması:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Özet

Oturum 4, konuşma AI için üretime hazır Chainlit uygulamaları oluşturmayı ele aldı. Şunları öğrendiniz:

- ✅ **Chainlit Framework**: Chat uygulamaları için modern UI ve akış desteği
- ✅ **Foundry Local Entegrasyonu**: SDK kullanımı ve yapılandırma kalıpları  
- ✅ **WebGPU Çıkarımı**: Maksimum gizlilik için tarayıcı tabanlı AI
- ✅ **Open WebUI Kurulumu**: Profesyonel chat arayüzü dağıtımı
- ✅ **Üretim Kalıpları**: Hata yönetimi, izleme ve ölçeklendirme

Örnek 04 uygulaması, Microsoft Foundry Local aracılığıyla yerel AI modellerini kullanan sağlam chat arayüzleri oluşturmak için en iyi uygulamaları gösterir ve mükemmel kullanıcı deneyimleri sunar.

## Referanslar

- **[Örnek 04: Chainlit Uygulaması](samples/04/README.md)**: Belgelerle tam uygulama
- **[Chainlit Eğitim Not Defteri](samples/04/chainlit_app.ipynb)**: Etkileşimli öğrenme materyalleri
- **[Foundry Local Belgeleri](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Tam platform belgeleri
- **[Chainlit Belgeleri](https://docs.chainlit.io/)**: Resmi framework belgeleri
- **[Open WebUI Entegrasyon Kılavuzu](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Resmi öğretici

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluğu sağlamak için çaba göstersek de, otomatik çeviriler hata veya yanlışlıklar içerebilir. Belgenin orijinal dilindeki hali, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul edilmez.