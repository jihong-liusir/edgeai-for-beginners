<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T23:32:00+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "tr"
}
-->
# Bölüm 4: OpenVINO Toolkit Optimizasyon Paketi

## İçindekiler
1. [Giriş](../../../Module04)
2. [OpenVINO Nedir?](../../../Module04)
3. [Kurulum](../../../Module04)
4. [Hızlı Başlangıç Kılavuzu](../../../Module04)
5. [Örnek: Modelleri OpenVINO ile Dönüştürme ve Optimize Etme](../../../Module04)
6. [Gelişmiş Kullanım](../../../Module04)
7. [En İyi Uygulamalar](../../../Module04)
8. [Sorun Giderme](../../../Module04)
9. [Ek Kaynaklar](../../../Module04)

## Giriş

OpenVINO (Open Visual Inference and Neural Network Optimization), Intel'in bulut, yerel ve uç cihazlarda yüksek performanslı yapay zeka çözümleri sunmak için geliştirdiği açık kaynaklı bir araç setidir. İster CPU, GPU, VPU, ister özel yapay zeka hızlandırıcılarını hedefliyor olun, OpenVINO model doğruluğunu korurken kapsamlı optimizasyon yetenekleri ve platformlar arası dağıtım sağlar.

## OpenVINO Nedir?

OpenVINO, geliştiricilerin yapay zeka modellerini çeşitli donanım platformlarında verimli bir şekilde optimize etmelerini, dönüştürmelerini ve dağıtmalarını sağlayan açık kaynaklı bir araç setidir. Üç ana bileşenden oluşur: OpenVINO Runtime (çıkarsama için), Neural Network Compression Framework (NNCF) (model optimizasyonu için) ve OpenVINO Model Server (ölçeklenebilir dağıtım için).

### Temel Özellikler

- **Platformlar Arası Dağıtım**: Linux, Windows ve macOS'u Python, C++ ve C API'leri ile destekler
- **Donanım Hızlandırma**: CPU, GPU, VPU ve yapay zeka hızlandırıcıları için otomatik cihaz keşfi ve optimizasyon
- **Model Sıkıştırma Çerçevesi**: NNCF aracılığıyla gelişmiş kuantizasyon, budama ve optimizasyon teknikleri
- **Çerçeve Uyumluluğu**: TensorFlow, ONNX, PaddlePaddle ve PyTorch modelleri için doğrudan destek
- **Üretken Yapay Zeka Desteği**: Büyük dil modelleri ve üretken yapay zeka uygulamaları için özel OpenVINO GenAI

### Faydalar

- **Performans Optimizasyonu**: Minimum doğruluk kaybıyla önemli hız artışları
- **Azaltılmış Dağıtım Yükü**: Harici bağımlılıkların minimumda tutulmasıyla kolay kurulum ve dağıtım
- **Geliştirilmiş Başlangıç Süresi**: Daha hızlı uygulama başlatma için optimize edilmiş model yükleme ve önbellekleme
- **Ölçeklenebilir Dağıtım**: Uç cihazlardan bulut altyapısına kadar tutarlı API'lerle
- **Üretime Hazır**: Kapsamlı belgeler ve topluluk desteği ile kurumsal düzeyde güvenilirlik

## Kurulum

### Ön Koşullar

- Python 3.8 veya üzeri
- pip paket yöneticisi
- Sanal ortam (önerilir)
- Uyumlu donanım (Intel CPU'lar önerilir, ancak çeşitli mimarileri destekler)

### Temel Kurulum

Sanal bir ortam oluşturun ve etkinleştirin:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

OpenVINO Runtime'ı yükleyin:

```bash
pip install openvino
```

Model optimizasyonu için NNCF'yi yükleyin:

```bash
pip install nncf
```

### OpenVINO GenAI Kurulumu

Üretken yapay zeka uygulamaları için:

```bash
pip install openvino-genai
```

### İsteğe Bağlı Bağımlılıklar

Belirli kullanım durumları için ek paketler:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Kurulumu Doğrulama

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Başarılı olursa, OpenVINO sürüm bilgilerini görmelisiniz.

## Hızlı Başlangıç Kılavuzu

### İlk Model Optimizasyonunuz

Hugging Face modelini OpenVINO ile dönüştürüp optimize edelim:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Bu Süreç Ne Yapar?

Optimizasyon iş akışı şunları içerir: Hugging Face'den orijinal modeli yükleme, OpenVINO Ara Temsil (IR) formatına dönüştürme, varsayılan optimizasyonları uygulama ve hedef donanım için derleme.

### Temel Parametrelerin Açıklaması

- `export=True`: Modeli OpenVINO IR formatına dönüştürür
- `compile=False`: Esneklik için derlemeyi çalışma zamanına kadar geciktirir
- `device`: Hedef donanım ("CPU", "GPU", "AUTO" otomatik seçim için)
- `save_pretrained()`: Optimize edilmiş modeli yeniden kullanım için kaydeder

## Örnek: Modelleri OpenVINO ile Dönüştürme ve Optimize Etme

### Adım 1: NNCF Kuantizasyonuyla Model Dönüşümü

İşte eğitim sonrası kuantizasyonun nasıl uygulanacağı:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Adım 2: Ağırlık Sıkıştırma ile Gelişmiş Optimizasyon

Transformer tabanlı modeller için ağırlık sıkıştırmayı uygulayın:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Adım 3: Optimize Edilmiş Model ile Çıkarsama

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Çıktı Yapısı

Optimizasyondan sonra model dizininiz şunları içerecektir:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Gelişmiş Kullanım

### NNCF YAML ile Yapılandırma

Karmaşık optimizasyon iş akışları için NNCF yapılandırma dosyalarını kullanın:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Yapılandırmayı uygulayın:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU Optimizasyonu

GPU hızlandırma için:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Toplu İşleme Optimizasyonu

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Model Sunucusu Dağıtımı

Optimize edilmiş modelleri OpenVINO Model Server ile dağıtın:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Model sunucusu için istemci kodu:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## En İyi Uygulamalar

### 1. Model Seçimi ve Hazırlığı
- Desteklenen çerçevelerden (PyTorch, TensorFlow, ONNX) modeller kullanın
- Model girişlerinin sabit veya bilinen dinamik şekillere sahip olduğundan emin olun
- Kalibrasyon için temsilci veri setleriyle test yapın

### 2. Optimizasyon Stratejisi Seçimi
- **Eğitim Sonrası Kuantizasyon**: Hızlı optimizasyon için buradan başlayın
- **Ağırlık Sıkıştırma**: Büyük dil modelleri ve transformerlar için ideal
- **Kuantizasyona Duyarlı Eğitim**: Doğruluğun kritik olduğu durumlarda kullanın

### 3. Donanıma Özgü Optimizasyon
- **CPU**: Dengeli performans için INT8 kuantizasyon kullanın
- **GPU**: FP16 hassasiyetinden ve toplu işlemden yararlanın
- **VPU**: Model basitleştirme ve katman birleştirmeye odaklanın

### 4. Performans Ayarı
- **Verim Modu**: Yüksek hacimli toplu işlem için
- **Gecikme Modu**: Gerçek zamanlı etkileşimli uygulamalar için
- **AUTO Cihazı**: OpenVINO'nun en uygun donanımı seçmesine izin verin

### 5. Bellek Yönetimi
- Bellek yükünü önlemek için dinamik şekilleri dikkatli kullanın
- Daha hızlı yüklemeler için model önbellekleme uygulayın
- Optimizasyon sırasında bellek kullanımını izleyin

### 6. Doğruluk Doğrulama
- Optimize edilmiş modelleri orijinal performansa karşı her zaman doğrulayın
- Değerlendirme için temsilci test veri setleri kullanın
- Kademeli optimizasyonu düşünün (temkinli ayarlarla başlayın)

## Sorun Giderme

### Yaygın Sorunlar

#### 1. Kurulum Problemleri
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Model Dönüşüm Hataları
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Performans Sorunları
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Bellek Sorunları
- Optimizasyon sırasında model toplu boyutunu azaltın
- Büyük veri setleri için akış kullanın
- Model önbellekleme etkinleştirin: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Doğruluk Kaybı
- Daha yüksek hassasiyet kullanın (INT8 yerine INT4)
- Kalibrasyon veri seti boyutunu artırın
- Karışık hassasiyet optimizasyonu uygulayın

### Performans İzleme

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Yardım Alma

- **Belgeler**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Sorunları**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Topluluk Forumu**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Ek Kaynaklar

### Resmi Bağlantılar
- **OpenVINO Ana Sayfa**: [openvino.ai](https://openvino.ai/)
- **GitHub Deposu**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Deposu**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Hayvanat Bahçesi**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Öğrenme Kaynakları
- **OpenVINO Not Defterleri**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Hızlı Başlangıç Kılavuzu**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimizasyon Kılavuzu**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Entegrasyon Araçları
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Performans Karşılaştırmaları
- **Resmi Karşılaştırmalar**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Hayvanat Bahçesi**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Topluluk Örnekleri
- **Jupyter Not Defterleri**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO not defterleri deposunda kapsamlı eğitimler
- **Örnek Uygulamalar**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Çeşitli alanlar için gerçek dünya örnekleri (bilgisayarlı görü, NLP, ses)
- **Blog Yazıları**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI ve topluluk blog yazılarıyla detaylı kullanım örnekleri

### İlgili Araçlar
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intel donanımı için ek optimizasyon teknikleri
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Mobil ve uç cihaz dağıtım karşılaştırmaları için
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Platformlar arası çıkarsama motoru alternatifleri

## ➡️ Sıradaki Adım

- [05: Apple MLX Framework Derinlemesine İnceleme](./05.AppleMLX.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.