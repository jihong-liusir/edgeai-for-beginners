<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T23:42:35+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "tr"
}
-->
# Bölüm 2: Llama.cpp Uygulama Kılavuzu

## İçindekiler
1. [Giriş](../../../Module04)
2. [Llama.cpp Nedir?](../../../Module04)
3. [Kurulum](../../../Module04)
4. [Kaynak Koddan Derleme](../../../Module04)
5. [Model Kuantizasyonu](../../../Module04)
6. [Temel Kullanım](../../../Module04)
7. [Gelişmiş Özellikler](../../../Module04)
8. [Python Entegrasyonu](../../../Module04)
9. [Sorun Giderme](../../../Module04)
10. [En İyi Uygulamalar](../../../Module04)

## Giriş

Bu kapsamlı kılavuz, Llama.cpp hakkında bilmeniz gereken her şeyi, temel kurulumdan gelişmiş kullanım senaryolarına kadar size adım adım anlatacaktır. Llama.cpp, Büyük Dil Modelleri'nin (LLM) verimli bir şekilde çalıştırılmasını sağlayan güçlü bir C++ uygulamasıdır. Minimum kurulum gereksinimi ve çeşitli donanım yapılandırmalarında mükemmel performans sunar.

## Llama.cpp Nedir?

Llama.cpp, büyük dil modellerini yerel olarak çalıştırmayı mümkün kılan, C/C++ ile yazılmış bir LLM çıkarım çerçevesidir. Minimum kurulum gereksinimi ve geniş bir donanım yelpazesinde en son performans özellikleri sunar. Temel özellikler şunlardır:

### Temel Özellikler
- **Bağımlılık içermeyen sade C/C++ uygulaması**
- **Çapraz platform uyumluluğu** (Windows, macOS, Linux)
- **Çeşitli mimariler için donanım optimizasyonu**
- **Kuantizasyon desteği** (1.5-bit'ten 8-bit'e kadar tam sayı kuantizasyonu)
- **CPU ve GPU hızlandırma** desteği
- **Bellek verimliliği** kısıtlı ortamlar için

### Avantajlar
- Özel donanım gerektirmeden CPU üzerinde verimli çalışır
- Birden fazla GPU altyapısını destekler (CUDA, Metal, OpenCL, Vulkan)
- Hafif ve taşınabilir
- Apple Silicon için birinci sınıf destek - ARM NEON, Accelerate ve Metal çerçeveleri ile optimize edilmiştir
- Bellek kullanımını azaltmak için çeşitli kuantizasyon seviyelerini destekler

## Kurulum

### Yöntem 1: Önceden Derlenmiş İkili Dosyalar (Yeni Başlayanlar için Önerilir)

#### GitHub Releases'dan İndirme
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) adresini ziyaret edin.
2. Sisteminiz için uygun ikili dosyayı indirin:
   - Windows için `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS için `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux için `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. Arşivi çıkarın ve dizini sisteminizin PATH'ine ekleyin.

#### Paket Yöneticileri Kullanarak

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Çeşitli Dağıtımlar):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Yöntem 2: Python Paketi (llama-cpp-python)

#### Temel Kurulum
```bash
pip install llama-cpp-python
```

#### Donanım Hızlandırma ile
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Kaynak Koddan Derleme

### Ön Koşullar

**Sistem Gereksinimleri:**
- C++ derleyici (GCC, Clang veya MSVC)
- CMake (sürüm 3.14 veya üzeri)
- Git
- Platformunuz için yapı araçları

**Ön Koşulların Kurulumu:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022'yi C++ geliştirme araçlarıyla yükleyin
- Resmi web sitesinden CMake'i yükleyin
- Git'i yükleyin

### Temel Derleme Süreci

1. **Depoyu klonlayın:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Derlemeyi yapılandırın:**
```bash
cmake -B build
```

3. **Projeyi derleyin:**
```bash
cmake --build build --config Release
```

Daha hızlı derleme için paralel işler kullanın:
```bash
cmake --build build --config Release -j 8
```

### Donanıma Özel Derlemeler

#### CUDA Desteği (NVIDIA GPU'lar)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal Desteği (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS Desteği (CPU Optimizasyonu)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan Desteği
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Gelişmiş Derleme Seçenekleri

#### Hata Ayıklama Derlemesi
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Ek Özelliklerle
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Model Kuantizasyonu

### GGUF Formatını Anlamak

GGUF (Genelleştirilmiş GGML Birleşik Formatı), Llama.cpp ve diğer çerçevelerle büyük dil modellerini verimli bir şekilde çalıştırmak için tasarlanmış optimize edilmiş bir dosya formatıdır. Şunları sağlar:

- Standartlaştırılmış model ağırlık depolama
- Platformlar arası geliştirilmiş uyumluluk
- Artırılmış performans
- Verimli meta veri işleme

### Kuantizasyon Türleri

Llama.cpp çeşitli kuantizasyon seviyelerini destekler:

| Tür | Bit | Açıklama | Kullanım Durumu |
|-----|-----|----------|-----------------|
| F16 | 16 | Yarım hassasiyet | Yüksek kalite, büyük bellek |
| Q8_0 | 8 | 8-bit kuantizasyon | İyi denge |
| Q4_0 | 4 | 4-bit kuantizasyon | Orta kalite, daha küçük boyut |
| Q2_K | 2 | 2-bit kuantizasyon | En küçük boyut, düşük kalite |

### Modelleri Dönüştürme

#### PyTorch'tan GGUF'ye
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face'den Doğrudan İndirme
Birçok model Hugging Face'de GGUF formatında mevcuttur:
- Adında "GGUF" olan modelleri arayın
- Uygun kuantizasyon seviyesini indirin
- Llama.cpp ile doğrudan kullanın

## Temel Kullanım

### Komut Satırı Arayüzü

#### Basit Metin Üretimi
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face Modellerini Kullanma
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Sunucu Modu
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Yaygın Parametreler

| Parametre | Açıklama | Örnek |
|-----------|----------|-------|
| `-m` | Model dosya yolu | `-m model.gguf` |
| `-p` | İstek metni | `-p "Merhaba dünya"` |
| `-n` | Üretilecek token sayısı | `-n 100` |
| `-c` | Bağlam boyutu | `-c 4096` |
| `-t` | İş parçacığı sayısı | `-t 8` |
| `-ngl` | GPU katmanları | `-ngl 32` |
| `-temp` | Sıcaklık | `-temp 0.7` |

### Etkileşimli Mod

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Gelişmiş Özellikler

### Sunucu API'si

#### Sunucuyu Başlatma
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API Kullanımı
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Performans Optimizasyonu

#### Bellek Yönetimi
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Çoklu İş Parçacığı
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU Hızlandırma
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python Entegrasyonu

### llama-cpp-python ile Temel Kullanım

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Sohbet Arayüzü

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Akış Yanıtları

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain ile Entegrasyon

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Sorun Giderme

### Yaygın Sorunlar ve Çözümleri

#### Derleme Hataları

**Sorun: CMake bulunamadı**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Sorun: Derleyici bulunamadı**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Çalışma Zamanı Sorunları

**Sorun: Model yüklenemiyor**
- Model dosya yolunu doğrulayın
- Dosya izinlerini kontrol edin
- Yeterli RAM olduğundan emin olun
- Farklı kuantizasyon seviyelerini deneyin

**Sorun: Düşük performans**
- Donanım hızlandırmayı etkinleştirin
- İş parçacığı sayısını artırın
- Uygun kuantizasyonu kullanın
- GPU bellek kullanımını kontrol edin

#### Bellek Sorunları

**Sorun: Bellek yetersiz**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platforma Özgü Sorunlar

#### Windows
- MinGW veya Visual Studio derleyicisini kullanın
- PATH yapılandırmasını doğru şekilde ayarlayın
- Antivirüs müdahalesini kontrol edin

#### macOS
- Apple Silicon için Metal'i etkinleştirin
- Gerekirse uyumluluk için Rosetta 2'yi kullanın
- Xcode komut satırı araçlarını kontrol edin

#### Linux
- Geliştirme paketlerini yükleyin
- GPU sürücü sürümlerini kontrol edin
- CUDA araç seti kurulumunu doğrulayın

## En İyi Uygulamalar

### Model Seçimi
1. **Donanımınıza uygun kuantizasyonu seçin**
2. **Model boyutu** ve kalite arasındaki dengeyi göz önünde bulundurun
3. **Özel kullanım durumunuz için farklı modelleri test edin**

### Performans Optimizasyonu
1. **GPU hızlandırmayı kullanın** mümkün olduğunda
2. **CPU için iş parçacığı sayısını optimize edin**
3. **Kullanım durumunuza uygun bağlam boyutunu ayarlayın**
4. **Büyük modeller için bellek eşlemesini etkinleştirin**

### Üretim Dağıtımı
1. **API erişimi için sunucu modunu kullanın**
2. **Uygun hata yönetimi uygulayın**
3. **Kaynak kullanımını izleyin**
4. **Günlük kaydı ve izleme ayarlarını yapılandırın**

### Geliştirme İş Akışı
1. **Test için daha küçük modellerle başlayın**
2. **Model yapılandırmaları için sürüm kontrolü kullanın**
3. **Yapılandırmalarınızı belgeleyin**
4. **Farklı platformlarda test yapın**

### Güvenlik Hususları
1. **Giriş isteklerini doğrulayın**
2. **Hız sınırlaması uygulayın**
3. **API uç noktalarını güvence altına alın**
4. **Kötüye kullanım desenlerini izleyin**

## Sonuç

Llama.cpp, çeşitli donanım yapılandırmalarında büyük dil modellerini yerel olarak çalıştırmak için güçlü ve verimli bir yol sunar. İster AI uygulamaları geliştiriyor olun, ister araştırma yapıyor olun, ister sadece LLM'lerle denemeler yapıyor olun, bu çerçeve geniş bir kullanım yelpazesi için gereken esneklik ve performansı sağlar.

Önemli noktalar:
- İhtiyacınıza en uygun kurulum yöntemini seçin
- Donanım yapılandırmanıza göre optimize edin
- Temel kullanımla başlayın ve yavaş yavaş gelişmiş özellikleri keşfedin
- Daha kolay entegrasyon için Python bağlamalarını kullanmayı düşünün
- Üretim dağıtımları için en iyi uygulamaları takip edin

Daha fazla bilgi ve güncelleme için [resmi Llama.cpp deposunu](https://github.com/ggml-org/llama.cpp) ziyaret edin ve mevcut kapsamlı belgeler ile topluluk kaynaklarına göz atın.

## ➡️ Sırada Ne Var?

- [03: Microsoft Olive Optimizasyon Paketi](./03.MicrosoftOlive.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.