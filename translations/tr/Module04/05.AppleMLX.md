<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-17T23:40:26+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "tr"
}
-->
# Bölüm 4: Apple MLX Framework Derinlemesine İnceleme

## İçindekiler
1. [Apple MLX'e Giriş](../../../Module04)
2. [LLM Geliştirme için Temel Özellikler](../../../Module04)
3. [Kurulum Kılavuzu](../../../Module04)
4. [MLX ile Başlangıç](../../../Module04)
5. [MLX-LM: Dil Modelleri](../../../Module04)
6. [Büyük Dil Modelleri ile Çalışma](../../../Module04)
7. [Hugging Face Entegrasyonu](../../../Module04)
8. [Model Dönüşümü ve Kuantizasyon](../../../Module04)
9. [Dil Modellerini İnce Ayar Yapma](../../../Module04)
10. [Gelişmiş LLM Özellikleri](../../../Module04)
11. [LLM'ler için En İyi Uygulamalar](../../../Module04)
12. [Sorun Giderme](../../../Module04)
13. [Ek Kaynaklar](../../../Module04)

## Apple MLX'e Giriş

Apple MLX, Apple Silicon üzerinde verimli ve esnek makine öğrenimi için özel olarak tasarlanmış bir dizi framework'tür ve Apple Machine Learning Research tarafından geliştirilmiştir. Aralık 2023'te piyasaya sürülen MLX, PyTorch ve TensorFlow gibi framework'lere Apple'ın yanıtı olarak öne çıkıyor ve Mac bilgisayarlarda güçlü büyük dil modeli yeteneklerini mümkün kılmaya odaklanıyor.

### MLX'i LLM'ler için Özel Kılan Nedir?

MLX, Apple Silicon'un birleşik bellek mimarisinden tam anlamıyla yararlanacak şekilde tasarlanmıştır ve büyük dil modellerini yerel olarak Mac bilgisayarlarda çalıştırmak ve ince ayar yapmak için özellikle uygundur. Framework, Mac kullanıcılarının LLM'lerle çalışırken geleneksel olarak karşılaştığı birçok uyumluluk sorununu ortadan kaldırır.

### Kimler MLX'i LLM'ler için Kullanmalı?

- **Mac kullanıcıları**, bulut bağımlılığı olmadan LLM'leri yerel olarak çalıştırmak isteyenler
- **Araştırmacılar**, dil modeli ince ayarı ve özelleştirme üzerinde deney yapanlar
- **Geliştiriciler**, dil modeli yetenekleriyle AI uygulamaları geliştirenler
- **Herkes**, metin oluşturma, sohbet ve dil görevleri için Apple Silicon'u kullanmak isteyenler

## LLM Geliştirme için Temel Özellikler

### 1. Birleşik Bellek Mimarisi
Apple Silicon'un birleşik belleği, MLX'in diğer framework'lerde tipik olan bellek kopyalama yükü olmadan büyük dil modellerini verimli bir şekilde işlemesine olanak tanır. Bu, aynı donanımda daha büyük modellerle çalışabileceğiniz anlamına gelir.

### 2. Yerel Apple Silicon Optimizasyonu
MLX, Apple'ın M serisi çipleri için sıfırdan tasarlanmıştır ve dil modellerinde yaygın olarak kullanılan transformer mimarileri için en iyi performansı sağlar.

### 3. Kuantizasyon Desteği
4-bit ve 8-bit kuantizasyon için yerleşik destek, bellek gereksinimlerini azaltırken model kalitesini korur ve daha büyük modellerin tüketici donanımında çalışmasını sağlar.

### 4. Hugging Face Entegrasyonu
Hugging Face ekosistemiyle sorunsuz entegrasyon, basit dönüşüm araçlarıyla binlerce önceden eğitilmiş dil modeline erişim sağlar.

### 5. LoRA İnce Ayarı
Düşük Dereceli Adaptasyon (LoRA) desteği, büyük modellerin minimum hesaplama kaynaklarıyla verimli bir şekilde ince ayar yapılmasını sağlar.

## Kurulum Kılavuzu

### Sistem Gereksinimleri
- **macOS 13.0+** (Apple Silicon optimizasyonu için)
- **Python 3.8+**
- **Apple Silicon** (M1, M2, M3, M4 serisi)
- **Yerel ARM ortamı** (Rosetta altında çalışmıyor)
- **8GB+ RAM** (daha büyük modeller için 16GB+ önerilir)

### LLM'ler için Hızlı Kurulum

Dil modelleriyle başlamak için en kolay yol MLX-LM'yi kurmaktır:

```bash
pip install mlx-lm
```

Bu tek komut, hem temel MLX framework'ünü hem de dil modeli araçlarını yükler.

### Sanal Ortam Kurulumu (Önerilir)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Ses Modelleri için Ek Bağımlılıklar

Whisper gibi konuşma modelleriyle çalışmayı planlıyorsanız:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## MLX ile Başlangıç

### İlk Dil Modeliniz

Basit bir metin oluşturma örneği çalıştırarak başlayalım:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Python API Örneği

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Model Yüklemeyi Anlama

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Dil Modelleri

### Desteklenen Model Mimarileri

MLX-LM, popüler dil modeli mimarilerinin geniş bir yelpazesini destekler:

- **LLaMA ve LLaMA 2** - Meta'nın temel modelleri
- **Mistral ve Mixtral** - Verimli ve güçlü modeller
- **Phi-3** - Microsoft'un kompakt dil modelleri
- **Qwen** - Alibaba'nın çok dilli modelleri
- **Code Llama** - Kod oluşturma için özel modeller
- **Gemma** - Google'ın açık dil modelleri

### Komut Satırı Arayüzü

MLX-LM komut satırı arayüzü, dil modelleriyle çalışmak için güçlü araçlar sunar:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Gelişmiş Kullanım Senaryoları için Python API

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Büyük Dil Modelleri ile Çalışma

### Metin Oluşturma Kalıpları

#### Tek Dönüşlü Oluşturma
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Talimat Takibi
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Yaratıcı Yazım
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Çok Dönüşlü Sohbetler

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Hugging Face Entegrasyonu

### MLX Uyumlu Modelleri Bulma

MLX, Hugging Face ekosistemiyle sorunsuz çalışır:

- **MLX modellerini inceleyin**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX Topluluğu**: https://huggingface.co/mlx-community (önceden dönüştürülmüş modeller)
- **Orijinal modeller**: Çoğu LLaMA, Mistral, Phi ve Qwen modeli dönüşümle çalışır

### Hugging Face'den Modelleri Yükleme

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Modelleri Çevrimdışı Kullanım için İndirme

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Model Dönüşümü ve Kuantizasyon

### Hugging Face Modellerini MLX'e Dönüştürme

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Kuantizasyonu Anlama

Kuantizasyon, model boyutunu ve bellek kullanımını kalite kaybı olmadan azaltır:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Özel Kuantizasyon

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Dil Modellerini İnce Ayar Yapma

### LoRA (Düşük Dereceli Adaptasyon) İnce Ayarı

MLX, LoRA kullanarak büyük modellerin verimli bir şekilde ince ayar yapılmasını destekler ve minimum hesaplama kaynaklarıyla uyum sağlar:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Eğitim Verilerini Hazırlama

Eğitim örneklerinizle bir JSON dosyası oluşturun:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### İnce Ayar Komutu

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### İnce Ayar Yapılmış Modelleri Kullanma

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Gelişmiş LLM Özellikleri

### Verimlilik için İpucu Önbellekleme

Aynı bağlamı tekrar tekrar kullanmak için MLX, performansı artırmak amacıyla ipucu önbellekleme desteği sunar:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Akışlı Metin Oluşturma

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Kod Oluşturma Modelleriyle Çalışma

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Sohbet Modelleriyle Çalışma

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## LLM'ler için En İyi Uygulamalar

### Bellek Yönetimi

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Model Seçim Kılavuzları

**Deney ve Öğrenme için:**
- 4-bit kuantize modelleri kullanın (ör. `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Phi-3-mini gibi daha küçük modellerle başlayın

**Üretim Uygulamaları için:**
- Model boyutu ve kalite arasındaki dengeyi göz önünde bulundurun
- Hem kuantize hem de tam hassasiyetli modelleri test edin
- Özel kullanım senaryolarınızda karşılaştırma yapın

**Belirli Görevler için:**
- **Kod Oluşturma**: CodeLlama, Code Llama Instruct
- **Genel Sohbet**: Mistral-7B-Instruct, Phi-3
- **Çok Dilli**: Qwen modelleri
- **Yaratıcı Yazım**: Mistral veya LLaMA ile daha yüksek sıcaklık ayarları

### İpucu Mühendisliği En İyi Uygulamalar

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Performans Optimizasyonu

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Sorun Giderme

### Yaygın Sorunlar ve Çözümleri

#### Kurulum Sorunları

**Sorun**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Çözüm**: Yerel ARM Python veya Miniconda kullanın:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Bellek Sorunları

**Sorun**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Model Yükleme Sorunları

**Sorun**: Model yüklenemiyor veya kötü çıktı üretiyor
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Performans Sorunları

**Sorun**: Yavaş oluşturma hızı
- Diğer bellek yoğun uygulamaları kapatın
- Mümkün olduğunda kuantize modelleri kullanın
- Rosetta altında çalışmadığınızdan emin olun
- Modelleri yüklemeden önce mevcut belleği kontrol edin

### Hata Ayıklama İpuçları

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Ek Kaynaklar

### Resmi Belgeler ve Depolar

- **MLX GitHub Deposu**: https://github.com/ml-explore/mlx
- **MLX-LM Örnekleri**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX Belgeleri**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX Entegrasyonu**: https://huggingface.co/docs/hub/en/mlx

### Model Koleksiyonları

- **MLX Topluluk Modelleri**: https://huggingface.co/mlx-community
- **Trend MLX Modelleri**: https://huggingface.co/models?library=mlx&sort=trending

### Örnek Uygulamalar

1. **Kişisel AI Asistanı**: Konuşma hafızası olan yerel bir chatbot oluşturun
2. **Kod Yardımcısı**: Geliştirme iş akışınız için bir kodlama asistanı oluşturun
3. **İçerik Üretici**: Yazma, özetleme ve içerik oluşturma araçları geliştirin
4. **Özel İnce Ayar Modelleri**: Alanınıza özel görevler için modelleri uyarlayın
5. **Çok Modlu Uygulamalar**: Metin oluşturmayı diğer MLX yetenekleriyle birleştirin

### Topluluk ve Öğrenme

- **MLX Topluluk Tartışmaları**: GitHub Sorunlar ve Tartışmalar
- **Hugging Face Forumları**: Topluluk desteği ve model paylaşımı
- **Apple Geliştirici Belgeleri**: Resmi Apple ML kaynakları

### Atıf

MLX'i araştırmanızda kullanırsanız, lütfen şu şekilde atıfta bulunun:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Sonuç

Apple MLX, Mac bilgisayarlarda büyük dil modellerini çalıştırma alanında devrim yarattı. Yerel Apple Silicon optimizasyonu, sorunsuz Hugging Face entegrasyonu ve kuantizasyon ve LoRA ince ayarı gibi güçlü özellikler sunarak MLX, sofistike dil modellerini mükemmel performansla yerel olarak çalıştırmayı mümkün kılıyor.

İster chatbotlar, kod asistanları, içerik üreticiler, ister özel ince ayar modelleri geliştiriyor olun, MLX, Apple Silicon Mac'inizin dil modeli uygulamaları için tam potansiyelini kullanmanıza olanak tanıyan araçlar ve performans sağlar. Framework'ün verimlilik ve kullanım kolaylığına odaklanması, hem araştırma hem de üretim uygulamaları için mükemmel bir seçimdir.

Bu eğitimdeki temel örneklerle başlayın, Hugging Face'deki önceden dönüştürülmüş modellerin zengin ekosistemini keşfedin ve ince ayar ve özel model geliştirme gibi daha gelişmiş özelliklere doğru ilerleyin. MLX ekosistemi büyümeye devam ettikçe, Apple donanımında dil modeli geliştirme için giderek daha güçlü bir platform haline geliyor.

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.