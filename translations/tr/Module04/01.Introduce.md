<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49e18143f97a802a8647f4be28355348",
  "translation_date": "2025-09-17T23:45:56+00:00",
  "source_file": "Module04/01.Introduce.md",
  "language_code": "tr"
}
-->
# Bölüm 1: Model Formatı Dönüşümü ve Kuantizasyon Temelleri

Model formatı dönüşümü ve kuantizasyon, EdgeAI'de önemli ilerlemeleri temsil eder ve kaynak kısıtlı cihazlarda gelişmiş makine öğrenimi yeteneklerini mümkün kılar. Modelleri etkili bir şekilde dönüştürmeyi, optimize etmeyi ve dağıtmayı anlamak, pratik edge tabanlı yapay zeka çözümleri oluşturmak için gereklidir.

## Giriş

Bu eğitimde, model formatı dönüşümü ve kuantizasyon tekniklerini ve bunların ileri düzey uygulama stratejilerini inceleyeceğiz. Model sıkıştırmanın temel kavramlarını, format dönüşüm sınırlarını ve sınıflandırmalarını, optimizasyon tekniklerini ve edge bilişim ortamları için pratik dağıtım stratejilerini ele alacağız.

## Öğrenme Hedefleri

Bu eğitimin sonunda şunları yapabileceksiniz:

- 🔢 Farklı hassasiyet seviyelerinin kuantizasyon sınırlarını ve sınıflandırmalarını anlayın.
- 🛠️ Edge cihazlarda model dağıtımı için temel format dönüşüm tekniklerini belirleyin.
- 🚀 Optimize edilmiş çıkarım için ileri düzey kuantizasyon ve sıkıştırma stratejilerini öğrenin.

## Model Kuantizasyon Sınırlarını ve Sınıflandırmalarını Anlamak

Model kuantizasyonu, sinir ağı parametrelerinin tam hassasiyetli karşılıklarına göre önemli ölçüde daha az bit ile hassasiyetini azaltmayı amaçlayan bir tekniktir. Tam hassasiyetli modeller 32-bit kayan nokta temsilleri kullanırken, kuantize edilmiş modeller özellikle verimlilik ve edge dağıtımı için tasarlanmıştır.

Hassasiyet sınıflandırma çerçevesi, farklı kuantizasyon seviyelerinin kategorilerini ve uygun kullanım durumlarını anlamamıza yardımcı olur. Bu sınıflandırma, belirli edge bilişim senaryoları için doğru hassasiyet seviyesini seçmek açısından kritik öneme sahiptir.

### Hassasiyet Sınıflandırma Çerçevesi

Hassasiyet sınırlarını anlamak, farklı edge bilişim senaryoları için uygun kuantizasyon seviyelerini seçmeye yardımcı olur:

- **🔬 Ultra-Düşük Hassasiyet**: 1-bit ila 2-bit kuantizasyon (özel donanım için aşırı sıkıştırma)
- **📱 Düşük Hassasiyet**: 3-bit ila 4-bit kuantizasyon (performans ve verimlilik dengesi)
- **⚖️ Orta Hassasiyet**: 5-bit ila 8-bit kuantizasyon (tam hassasiyet yeteneklerine yaklaşırken verimliliği koruma)

Kesin sınır araştırma topluluğunda değişkenlik gösterse de, çoğu uygulayıcı 8-bit ve altını "kuantize edilmiş" olarak kabul eder ve bazı kaynaklar farklı donanım hedefleri için özel eşikler belirler.

### Model Kuantizasyonunun Temel Avantajları

Model kuantizasyonu, edge bilişim uygulamaları için ideal hale getiren birkaç temel avantaj sunar:

**Operasyonel Verimlilik**: Kuantize edilmiş modeller, azaltılmış hesaplama karmaşıklığı sayesinde daha hızlı çıkarım süreleri sağlar ve gerçek zamanlı uygulamalar için idealdir. Daha düşük hesaplama kaynakları gerektirir, kaynak kısıtlı cihazlarda dağıtımı mümkün kılar, daha az enerji tüketir ve karbon ayak izini azaltır.

**Dağıtım Esnekliği**: Bu modeller, internet bağlantısı gerektirmeden cihaz üzerinde yapay zeka yeteneklerini mümkün kılar, yerel işlemle gizlilik ve güvenliği artırır, alan spesifik uygulamalar için özelleştirilebilir ve çeşitli edge bilişim ortamlarına uygundur.

**Maliyet Etkinliği**: Kuantize edilmiş modeller, tam hassasiyetli modellere kıyasla daha düşük operasyonel maliyetler ve edge uygulamaları için daha düşük bant genişliği gereksinimleri ile uygun maliyetli eğitim ve dağıtım sunar.

## İleri Düzey Model Formatı Edinme Stratejileri

### GGUF (Genel GGML Evrensel Formatı)

GGUF, kuantize edilmiş modellerin CPU ve edge cihazlarda dağıtımı için birincil format olarak hizmet eder. Format, model dönüşümü ve dağıtımı için kapsamlı kaynaklar sağlar:

**Format Keşif Özellikleri**: Format, çeşitli kuantizasyon seviyeleri, lisans uyumluluğu ve performans optimizasyonu için gelişmiş destek sunar. Kullanıcılar çapraz platform uyumluluğu, gerçek zamanlı performans karşılaştırmaları ve tarayıcı tabanlı dağıtım için WebGPU desteğine erişebilir.

**Kuantizasyon Seviyesi Koleksiyonları**: Popüler kuantizasyon formatları arasında dengeli sıkıştırma için Q4_K_M, kalite odaklı uygulamalar için Q5_K_S serisi, orijinal hassasiyete yakın Q8_0 ve ultra-düşük hassasiyet dağıtımı için Q2_K gibi deneysel formatlar bulunur. Format ayrıca belirli alanlar için özel yapılandırmalarla topluluk odaklı varyasyonlar ve farklı kullanım durumları için optimize edilmiş genel amaçlı ve talimat ayarlı varyantlar içerir.

### ONNX (Açık Sinir Ağı Değişimi)

ONNX formatı, kuantize edilmiş modeller için çapraz çerçeve uyumluluğu ve geliştirilmiş entegrasyon yetenekleri sağlar:

**Kurumsal Entegrasyon**: Format, adaptif hassasiyet için dinamik kuantizasyon ve üretim dağıtımı için statik kuantizasyon özellikleriyle kurumsal düzeyde destek ve optimizasyon yeteneklerine sahip modeller içerir. Ayrıca, çeşitli çerçevelerden gelen modelleri standartlaştırılmış kuantizasyon yaklaşımlarıyla destekler.

**Kurumsal Avantajlar**: Farklı çıkarım motorları arasında entegre optimizasyon özellikleri, çapraz platform dağıtımı ve donanım hızlandırma için yerleşik araçlar sunar. Standartlaştırılmış API'ler, entegre optimizasyon özellikleri ve kapsamlı dağıtım iş akışları ile kurumsal deneyimi geliştirir.

## İleri Düzey Kuantizasyon ve Optimizasyon Teknikleri

### Llama.cpp Optimizasyon Çerçevesi

Llama.cpp, edge dağıtımı için maksimum verimlilik sağlayan en son kuantizasyon tekniklerini sunar:

**Kuantizasyon Yöntemleri**: Çerçeve, Q4_0 (mükemmel boyut azaltımı ile 4-bit kuantizasyon - mobil dağıtım için ideal), Q5_1 (kalite ve sıkıştırma arasında denge sağlayan 5-bit kuantizasyon - edge çıkarımı için uygun) ve Q8_0 (orijinal kaliteye yakın 8-bit kuantizasyon - üretim kullanımı için önerilir) gibi çeşitli kuantizasyon seviyelerini destekler. Q2_K gibi ileri düzey formatlar, aşırı senaryolar için en son sıkıştırmayı temsil eder.

**Uygulama Avantajları**: SIMD hızlandırma ile CPU optimize edilmiş çıkarım, bellek verimli model yükleme ve yürütme sağlar. x86, ARM ve Apple Silicon mimarileri arasında çapraz platform uyumluluğu, donanım bağımsız dağıtım yeteneklerini mümkün kılar.

**Bellek Ayak İzi Karşılaştırması**: Farklı kuantizasyon seviyeleri, model boyutu ve kalite arasında değişen ödünleşimler sunar. Q4_0 yaklaşık %75 boyut azaltımı sağlar, Q5_1 %70 azaltım ile daha iyi kalite koruması sunar ve Q8_0 %50 azaltım ile orijinal performansa yakın bir kalite sağlar.

### Microsoft Olive Optimizasyon Paketi

Microsoft Olive, üretim ortamları için tasarlanmış kapsamlı model optimizasyon iş akışları sunar:

**Optimizasyon Teknikleri**: Paket, otomatik hassasiyet seçimi için dinamik kuantizasyon, verimliliği artırmak için grafik optimizasyonu ve operatör birleştirme, CPU, GPU ve NPU dağıtımı için donanım spesifik optimizasyonlar ve çok aşamalı optimizasyon iş akışlarını içerir. 8-bit'ten deneysel 1-bit yapılandırmalara kadar çeşitli hassasiyet seviyelerini destekleyen özel kuantizasyon iş akışları sunar.

**İş Akışı Otomasyonu**: Optimizasyon varyantları arasında otomatik karşılaştırma, optimizasyon sırasında kalite metriklerinin korunmasını sağlar. PyTorch ve ONNX gibi popüler ML çerçeveleriyle entegrasyon, bulut ve edge dağıtım optimizasyon yetenekleri sunar.

### Apple MLX Çerçevesi

Apple MLX, Apple Silicon cihazları için özel olarak tasarlanmış yerel optimizasyon sağlar:

**Apple Silicon Optimizasyonu**: Çerçeve, Metal Performance Shaders entegrasyonu ile birleşik bellek mimarisi, otomatik karma hassasiyet çıkarımı ve optimize edilmiş bellek bant genişliği kullanımı sağlar. Modeller, M serisi çiplerde çeşitli Apple cihaz dağıtımları için optimal denge ile olağanüstü performans gösterir.

**Geliştirme Özellikleri**: NumPy uyumlu dizi işlemleri, otomatik türev yetenekleri ve Apple geliştirme araçlarıyla sorunsuz entegrasyon sağlayan Python ve Swift API desteği, kapsamlı bir geliştirme ortamı sunar.

## Üretim Dağıtımı ve Çıkarım Stratejileri

### Ollama: Basitleştirilmiş Yerel Dağıtım

Ollama, edge ve yerel ortamlar için kurumsal düzeyde özelliklerle model dağıtımını kolaylaştırır:

**Dağıtım Yetenekleri**: Otomatik model çekme ve önbellekleme ile tek komutla model yükleme ve çalıştırma. REST API ile uygulama entegrasyonu ve çoklu model yönetimi ve geçiş yetenekleri. İleri düzey kuantizasyon seviyeleri, optimal dağıtım için özel yapılandırma gerektirir.

**Gelişmiş Özellikler**: Özel model ince ayar desteği, kapsayıcı dağıtımı için Dockerfile oluşturma, otomatik algılama ile GPU hızlandırma ve model kuantizasyonu ve optimizasyon seçenekleri, kapsamlı dağıtım esnekliği sağlar.

### VLLM: Yüksek Performanslı Çıkarım

VLLM, yüksek verim senaryoları için üretim düzeyinde çıkarım optimizasyonu sunar:

**Performans Optimizasyonları**: Bellek verimli dikkat hesaplaması için PagedAttention, verim optimizasyonu için dinamik toplama, çoklu GPU ölçeklendirme için tensör paralelliği ve gecikme azaltımı için spekülatif kod çözme. İleri düzey kuantizasyon formatları, optimal performans için özel çıkarım çekirdekleri gerektirir.

**Kurumsal Entegrasyon**: OpenAI uyumlu API uç noktaları, Kubernetes dağıtım desteği, izleme ve gözlemlenebilirlik entegrasyonu ve otomatik ölçeklendirme yetenekleri, kurumsal düzeyde dağıtım çözümleri sağlar.

### Microsoft'un Edge Çözümleri

Microsoft, kurumsal ortamlar için kapsamlı edge dağıtım yetenekleri sağlar:

**Edge Bilişim Özellikleri**: Kaynak kısıtlaması optimizasyonu ile çevrimdışı öncelikli mimari tasarımı, yerel model kayıt defteri yönetimi ve edge-to-cloud senkronizasyon yetenekleri, güvenilir edge dağıtımını sağlar.

**Güvenlik ve Uyumluluk**: Gizliliği korumak için yerel veri işleme, kurumsal güvenlik kontrolleri, denetim kaydı ve uyumluluk raporlama ve rol tabanlı erişim yönetimi, edge dağıtımları için kapsamlı güvenlik sağlar.

## Model Kuantizasyon Uygulama İçin En İyi Uygulamalar

### Kuantizasyon Seviyesi Seçim Kılavuzları

Edge dağıtımı için kuantizasyon seviyelerini seçerken aşağıdaki faktörleri göz önünde bulundurun:

**Hassasiyet Sayısı Düşünceleri**: Aşırı mobil uygulamalar için Q2_K gibi ultra-düşük hassasiyeti, dengeli performans senaryoları için Q4_K_M gibi düşük hassasiyeti ve verimliliği korurken tam hassasiyet yeteneklerine yaklaşırken Q8_0 gibi orta hassasiyeti seçin. Deneysel formatlar, belirli araştırma uygulamaları için özel sıkıştırma sunar.

**Kullanım Durumu Uyumu**: Hassasiyet koruma, çıkarım hızı, bellek kısıtlamaları ve çevrimdışı çalışma gereksinimleri gibi faktörleri göz önünde bulundurarak kuantizasyon yeteneklerini belirli uygulama gereksinimleriyle eşleştirin.

### Optimizasyon Stratejisi Seçimi

**Kuantizasyon Yaklaşımı**: Kalite gereksinimlerine ve donanım kısıtlamalarına göre uygun kuantizasyon seviyelerini seçin. Maksimum sıkıştırma için Q4_0, kalite-sıkıştırma dengesi için Q5_1 ve orijinal kalite koruması için Q8_0 düşünün. Deneysel formatlar, özel uygulamalar için aşırı sıkıştırma sınırını temsil eder.

**Çerçeve Seçimi**: Hedef donanım ve dağıtım gereksinimlerine göre optimizasyon çerçevelerini seçin. CPU optimize edilmiş dağıtım için Llama.cpp, kapsamlı optimizasyon iş akışları için Microsoft Olive ve Apple Silicon cihazları için Apple MLX kullanın.

## Pratik Format Dönüşümü ve Kullanım Durumları

### Gerçek Dünya Dağıtım Senaryoları

**Mobil Uygulamalar**: Q4_K formatları, minimal bellek ayak izi ile akıllı telefon uygulamalarında mükemmel performans gösterirken, Q8_0 tablet tabanlı uygulamalar için dengeli performans sağlar. Q5_K formatları, mobil üretkenlik uygulamaları için üstün kalite sunar.

**Masaüstü ve Edge Bilişim**: Q5_K masaüstü uygulamalar için optimal performans sağlar, Q8_0 iş istasyonu ortamları için yüksek kaliteli çıkarım sunar ve Q4_K edge cihazlarda verimli işlem sağlar.

**Araştırma ve Deneysel**: İleri düzey kuantizasyon formatları, aşırı kaynak kısıtlamaları gerektiren akademik araştırmalar ve kavram kanıtı uygulamaları için ultra-düşük hassasiyet çıkarımını keşfetmeyi mümkün kılar.

### Performans Karşılaştırmaları ve Kıyaslamalar

**Çıkarım Hızı**: Q4_K mobil CPU'larda en hızlı çıkarım sürelerini sağlar, Q5_K genel uygulamalar için dengeli hız-kalite oranı sunar, Q8_0 karmaşık görevler için üstün kalite sağlar ve deneysel formatlar, özel donanımla teorik maksimum verim sağlar.

**Bellek Gereksinimleri**: Kuantizasyon seviyeleri, küçük modeller için 500MB'nin altında Q2_K'den orijinal boyutun yaklaşık %50'sine kadar Q8_0'a kadar değişir ve deneysel yapılandırmalar maksimum sıkıştırma oranlarına ulaşır.

## Zorluklar ve Dikkat Edilmesi Gerekenler

### Performans Ödünleşimleri

Kuantizasyon dağıtımı, model boyutu, çıkarım hızı ve çıktı kalitesi arasında ödünleşimleri dikkatlice değerlendirmeyi gerektirir. Q4_K olağanüstü hız ve verimlilik sunarken, Q8_0 artan kaynak gereksinimleri pahasına üstün kalite sağlar. Q5_K, çoğu genel uygulama için uygun bir orta yol sunar.

### Donanım Uyumluluğu

Farklı edge cihazlar, değişen yeteneklere ve kısıtlamalara sahiptir. Q4_K temel işlemcilerde verimli çalışırken, Q5_K orta düzeyde hesaplama kaynakları gerektirir ve Q8_0 daha üst düzey donanımdan faydalanır. Deneysel formatlar, optimal işlemler için özel donanım veya yazılım uygulamaları gerektirir.

### Güvenlik ve Gizlilik

Kuantize edilmiş modeller, gizliliği artırmak için yerel işlemeyi mümkün kılarken, edge ortamlarında modelleri ve verileri korumak için uygun güvenlik önlemleri uygulanmalıdır. Bu, özellikle kurumsal ortamlarda yüksek hassasiyetli formatların veya hassas verileri işleyen uygulamalarda sıkıştırılmış formatların dağıtımı sırasında önemlidir.

## Model Kuantizasyonunda Gelecek Trendler

Kuantizasyon alanı, sıkıştırma teknikleri, optimizasyon yöntemleri ve dağıtım stratejilerindeki ilerlemelerle gelişmeye devam ediyor. Gelecekteki gelişmeler arasında daha verimli kuantizasyon algoritmaları, geliştirilmiş sıkıştırma yöntemleri ve edge donanım hızlandırıcılarıyla daha iyi entegrasyon yer alıyor.

Bu trendleri anlamak ve ortaya çıkan teknolojilerden haberdar olmak, kuantizasyon geliştirme ve dağıtım en iyi uygulamalarını takip etmek açısından kritik öneme sahip olacaktır.

## Ek Kaynaklar

- [Hugging Face GGUF Belgeleri](https://huggingface.co/docs/hub/en/gguf)
- [ONNX Model Optimizasyonu](https://onnxruntime.ai/docs/performance/model-optimizations/)
- [llama.cpp Belgeleri](https://github.com/ggml-org/llama.cpp)
- [Microsoft Olive Çerçevesi](https://github.com/microsoft/Olive)
- [Apple MLX Belgeleri](https://github.com/ml-explore/mlx)

## ➡️ Sırada ne var

- [02: Llama.cpp Uygulama Kılavuzu](./02.Llamacpp.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.