<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T18:19:25+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "tr"
}
-->
# Bölüm 3: Microsoft Olive Optimizasyon Paketi

## İçindekiler
1. [Giriş](../../../Module04)
2. [Microsoft Olive Nedir?](../../../Module04)
3. [Kurulum](../../../Module04)
4. [Hızlı Başlangıç Kılavuzu](../../../Module04)
5. [Örnek: Qwen3'ü ONNX INT4'e Dönüştürme](../../../Module04)
6. [Gelişmiş Kullanım](../../../Module04)
7. [En İyi Uygulamalar](../../../Module04)
8. [Sorun Giderme](../../../Module04)
9. [Ek Kaynaklar](../../../Module04)

## Giriş

Microsoft Olive, makine öğrenimi modellerini farklı donanım platformlarında dağıtmak için optimize etme sürecini basitleştiren güçlü ve kullanımı kolay bir donanım odaklı model optimizasyon aracıdır. İster CPU, GPU, ister özel AI hızlandırıcıları hedefliyor olun, Olive, model doğruluğunu korurken en iyi performansı elde etmenize yardımcı olur.

## Microsoft Olive Nedir?

Olive, model sıkıştırma, optimizasyon ve derleme gibi endüstri lideri teknikleri bir araya getiren, donanım odaklı ve kullanımı kolay bir model optimizasyon aracıdır. ONNX Runtime ile birlikte uçtan uca bir çıkarım optimizasyon çözümü olarak çalışır.

### Temel Özellikler

- **Donanım Odaklı Optimizasyon**: Hedef donanımınıza en uygun optimizasyon tekniklerini otomatik olarak seçer
- **40+ Dahili Optimizasyon Bileşeni**: Model sıkıştırma, kuantizasyon, grafik optimizasyonu ve daha fazlasını kapsar
- **Kolay CLI Arayüzü**: Yaygın optimizasyon görevleri için basit komutlar
- **Çoklu Çerçeve Desteği**: PyTorch, Hugging Face modelleri ve ONNX ile çalışır
- **Popüler Model Desteği**: Olive, Llama, Phi, Qwen, Gemma gibi popüler model mimarilerini kutudan çıkar çıkmaz optimize edebilir

### Avantajlar

- **Azaltılmış Geliştirme Süresi**: Farklı optimizasyon tekniklerini manuel olarak denemenize gerek kalmaz
- **Performans Kazançları**: Önemli hız iyileştirmeleri (bazı durumlarda 6 kata kadar)
- **Çapraz Platform Dağıtımı**: Optimize edilmiş modeller farklı donanım ve işletim sistemlerinde çalışır
- **Korunan Doğruluk**: Optimizasyonlar, performansı artırırken model kalitesini korur

## Kurulum

### Ön Koşullar

- Python 3.8 veya üzeri
- pip paket yöneticisi
- Sanal ortam (önerilir)

### Temel Kurulum

Sanal bir ortam oluşturun ve etkinleştirin:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Olive'i otomatik optimizasyon özellikleriyle yükleyin:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opsiyonel Bağımlılıklar

Olive, ek özellikler için çeşitli opsiyonel bağımlılıklar sunar:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Kurulumu Doğrulama

```bash
olive --help
```

Başarılı olursa, Olive CLI yardım mesajını görmelisiniz.

## Hızlı Başlangıç Kılavuzu

### İlk Optimizasyonunuz

Olive'in otomatik optimizasyon özelliğini kullanarak küçük bir dil modelini optimize edelim:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Bu Komut Ne Yapar?

Optimizasyon süreci şunları içerir: modeli yerel önbellekten almak, ONNX Grafiğini yakalamak ve ağırlıkları bir ONNX veri dosyasında depolamak, ONNX Grafiğini optimize etmek ve modeli RTN yöntemiyle int4'e kuantize etmek.

### Komut Parametrelerinin Açıklaması

- `--model_name_or_path`: Hugging Face model tanımlayıcısı veya yerel yol
- `--output_path`: Optimize edilmiş modelin kaydedileceği dizin
- `--device`: Hedef cihaz (cpu, gpu)
- `--provider`: Çalıştırma sağlayıcısı (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Çıkarım için ONNX Runtime Generate AI kullanımı
- `--precision`: Kuantizasyon hassasiyeti (int4, int8, fp16)
- `--log_level`: Günlük ayrıntı düzeyi (0=minimal, 1=detaylı)

## Örnek: Qwen3'ü ONNX INT4'e Dönüştürme

Hugging Face'deki [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) örneğine dayanarak bir Qwen3 modelini nasıl optimize edeceğinizi görelim:

### Adım 1: Modeli İndirme (Opsiyonel)

İndirme süresini en aza indirmek için yalnızca gerekli dosyaları önbelleğe alın:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Adım 2: Qwen3 Modelini Optimize Etme

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Adım 3: Optimize Edilmiş Modeli Test Etme

Optimize edilmiş modelinizi test etmek için basit bir Python betiği oluşturun:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Çıktı Yapısı

Optimizasyondan sonra, çıktı dizininiz şunları içerecektir:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Gelişmiş Kullanım

### Yapılandırma Dosyaları

Daha karmaşık optimizasyon iş akışları için JSON yapılandırma dosyalarını kullanabilirsiniz:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Yapılandırma ile çalıştırma:

```bash
olive run --config config.json
```

### GPU Optimizasyonu

CUDA GPU optimizasyonu için:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) için:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive ile İnce Ayar

Olive ayrıca modelleri ince ayar yapmayı destekler:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## En İyi Uygulamalar

### 1. Model Seçimi
- Test için daha küçük modellerle başlayın (örneğin, 0.5B-7B parametreler)
- Hedef model mimarinizin Olive tarafından desteklendiğinden emin olun

### 2. Donanım Dikkatleri
- Optimizasyon hedefinizi dağıtım donanımınıza uygun hale getirin
- CUDA uyumlu donanımınız varsa GPU optimizasyonunu kullanın
- Windows makinelerde entegre grafikler için DirectML'yi düşünün

### 3. Hassasiyet Seçimi
- **INT4**: Maksimum sıkıştırma, hafif doğruluk kaybı
- **INT8**: Boyut ve doğruluk arasında iyi bir denge
- **FP16**: Minimum doğruluk kaybı, orta düzeyde boyut azaltma

### 4. Test ve Doğrulama
- Optimize edilmiş modelleri özel kullanım durumlarınızla test edin
- Performans metriklerini karşılaştırın (gecikme, verimlilik, doğruluk)
- Değerlendirme için temsilci giriş verileri kullanın

### 5. İteratif Optimizasyon
- Hızlı sonuçlar için otomatik optimizasyonla başlayın
- Daha ayrıntılı kontrol için yapılandırma dosyalarını kullanın
- Farklı optimizasyon geçişlerini deneyin

## Sorun Giderme

### Yaygın Sorunlar

#### 1. Kurulum Problemleri
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU Problemleri
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Bellek Problemleri
- Optimizasyon sırasında daha küçük batch boyutları kullanın
- Önce daha yüksek hassasiyetle kuantizasyonu deneyin (int8 yerine int4)
- Model önbellekleme için yeterli disk alanı olduğundan emin olun

#### 4. Model Yükleme Hataları
- Model yolunu ve erişim izinlerini doğrulayın
- Modelin `trust_remote_code=True` gerektirip gerektirmediğini kontrol edin
- Gerekli tüm model dosyalarının indirildiğinden emin olun

### Yardım Alma

- **Dokümantasyon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Sorunları**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Örnekler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Ek Kaynaklar

### Resmi Bağlantılar
- **GitHub Deposu**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokümantasyonu**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Örneği**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Topluluk Örnekleri
- **Jupyter Notebooks**: Olive GitHub deposunda mevcut — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Uzantısı**: VS Code için AI Toolkit genel bakışı — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Yazıları**: Microsoft Açık Kaynak Blogu — https://opensource.microsoft.com/blog/

### İlgili Araçlar
- **ONNX Runtime**: Yüksek performanslı çıkarım motoru — https://onnxruntime.ai/
- **Hugging Face Transformers**: Birçok uyumlu modelin kaynağı — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Bulut tabanlı optimizasyon iş akışları — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Sıradaki Adım

- [04: OpenVINO Toolkit Optimizasyon Paketi](./04.openvino.md)

---

