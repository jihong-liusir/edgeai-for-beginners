<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1f5ed7b55962c3dccafd3da6f9ec252",
  "translation_date": "2025-09-17T23:27:49+00:00",
  "source_file": "Module01/04.EdgeDeployment.md",
  "language_code": "tr"
}
-->
# Bölüm 4: Edge AI Dağıtım Donanım Platformları

Edge AI dağıtımı, model optimizasyonu ve donanım seçiminin bir araya gelerek, akıllı yetenekleri verinin üretildiği cihazlara doğrudan getirdiği noktayı temsil eder. Bu bölüm, Intel, Qualcomm, NVIDIA ve Windows AI PC'lerinden önde gelen donanım çözümlerine odaklanarak, çeşitli platformlarda Edge AI dağıtımının pratik hususlarını, donanım gereksinimlerini ve stratejik avantajlarını inceler.

## Geliştiriciler için Kaynaklar

### Dokümantasyon ve Öğrenme Kaynakları
- [Microsoft Learn: Edge AI Geliştirme](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)
- [Intel Edge AI Kaynakları](https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/edge-ai.html)
- [Qualcomm AI Geliştirici Kaynakları](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- [NVIDIA Jetson Dokümantasyonu](https://developer.nvidia.com/embedded/learn/getting-started-jetson)
- [Windows AI Dokümantasyonu](https://learn.microsoft.com/windows/ai/)

### Araçlar ve SDK'lar
- [ONNX Runtime](https://onnxruntime.ai/) - Çapraz platform çıkarım çerçevesi
- [OpenVINO Toolkit](https://docs.openvino.ai/) - Intel'in optimizasyon araç seti
- [TensorRT](https://developer.nvidia.com/tensorrt) - NVIDIA'nın yüksek performanslı çıkarım SDK'sı
- [DirectML](https://learn.microsoft.com/windows/ai/directml/dml-intro) - Microsoft'un donanım hızlandırmalı ML API'si

## Giriş

Bu bölümde, AI modellerini edge cihazlara dağıtmanın pratik yönlerini inceleyeceğiz. Başarılı edge dağıtımı için temel hususları, donanım platformu seçimini ve farklı edge bilişim senaryolarına özgü optimizasyon stratejilerini ele alacağız.

## Öğrenme Hedefleri

Bu bölümün sonunda şunları yapabileceksiniz:

- Başarılı edge AI dağıtımı için temel hususları anlayın
- Farklı edge AI iş yükleri için uygun donanım platformlarını belirleyin
- Farklı edge AI donanım çözümleri arasındaki ödünleşimleri tanıyın
- Çeşitli edge AI donanım platformlarına özgü optimizasyon tekniklerini uygulayın

## Edge AI Dağıtım Hususları

AI'yi edge cihazlara dağıtmak, bulut dağıtımına kıyasla benzersiz zorluklar ve gereksinimler getirir. Başarılı edge AI uygulaması, birkaç faktörün dikkatlice değerlendirilmesini gerektirir:

### Donanım Kaynak Kısıtlamaları

Edge cihazlar, bulut altyapısına kıyasla genellikle sınırlı hesaplama kaynaklarına sahiptir:

- **Bellek Sınırlamaları**: Birçok edge cihazda birkaç MB'den birkaç GB'a kadar sınırlı RAM bulunur
- **Depolama Kısıtlamaları**: Sınırlı kalıcı depolama, model boyutunu ve veri yönetimini etkiler
- **İşlem Gücü**: Sınırlı CPU/GPU/NPU yetenekleri çıkarım hızını etkiler
- **Güç Tüketimi**: Birçok edge cihaz pil gücüyle çalışır veya termal sınırlamalara sahiptir

### Bağlantı Hususları

Edge AI, değişken bağlantı koşullarıyla etkili bir şekilde çalışmalıdır:

- **Kesintili Bağlantı**: Ağ kesintileri sırasında işlemler devam etmelidir
- **Bant Genişliği Sınırlamaları**: Veri merkezlerine kıyasla daha düşük veri aktarım yetenekleri
- **Gecikme Gereksinimleri**: Birçok uygulama gerçek zamanlı veya gerçek zamanlıya yakın işlem gerektirir
- **Veri Senkronizasyonu**: Yerel işlemeyi periyodik bulut senkronizasyonuyla yönetme

### Güvenlik ve Gizlilik Gereksinimleri

Edge AI, belirli güvenlik zorluklarını beraberinde getirir:

- **Fiziksel Güvenlik**: Cihazlar fiziksel olarak erişilebilir konumlarda konuşlandırılabilir
- **Veri Koruma**: Potansiyel olarak savunmasız cihazlarda hassas veri işleme
- **Kimlik Doğrulama**: Edge cihaz işlevselliği için güvenli erişim kontrolü
- **Güncelleme Yönetimi**: Model ve yazılım güncellemeleri için güvenli mekanizmalar

### Dağıtım ve Yönetim

Pratik dağıtım hususları şunları içerir:

- **Filo Yönetimi**: Birçok edge dağıtımı, çok sayıda dağıtılmış cihaz içerir
- **Sürüm Kontrolü**: Dağıtılmış cihazlar arasında model sürümlerini yönetme
- **İzleme**: Edge'deki performans takibi ve anomali algılama
- **Yaşam Döngüsü Yönetimi**: İlk dağıtımdan güncellemelere ve emekliliğe kadar

## Edge AI için Donanım Platformu Seçenekleri

### Intel Edge AI Çözümleri

Intel, edge AI dağıtımı için optimize edilmiş birkaç donanım platformu sunar:

#### Intel NUC

Intel NUC (Next Unit of Computing), kompakt bir form faktöründe masaüstü sınıfı performans sunar:

- **Intel Core işlemciler** entegre Iris Xe grafiklerle
- **RAM**: 64GB DDR4'e kadar destekler
- **Neural Compute Stick 2** uyumluluğu ek AI hızlandırma için
- **En uygun kullanım alanı**: Güç erişimi olan sabit konumlarda orta ila karmaşık edge AI iş yükleri

[Intel NUC for Edge AI](https://www.intel.com/content/www/us/en/products/details/nuc.html)

#### Intel Movidius Görü İşleme Birimleri (VPUs)

Bilgisayar görüsü ve sinir ağı hızlandırma için özel donanım:

- **Ultra düşük güç tüketimi** (1-3W tipik)
- **Özel sinir ağı hızlandırma**
- **Kompakt form faktörü** kameralar ve sensörlere entegrasyon için
- **En uygun kullanım alanı**: Güç kısıtlamaları olan bilgisayar görüsü uygulamaları

[Intel Movidius VPU](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html)

#### Intel Neural Compute Stick 2

USB tak-çalıştır sinir ağı hızlandırıcı:

- **Intel Movidius Myriad X VPU**
- **4 TOPS'a kadar** performans
- **USB 3.0 arayüzü** kolay entegrasyon için
- **En uygun kullanım alanı**: Hızlı prototipleme ve mevcut sistemlere AI yetenekleri ekleme

[Intel Neural Compute Stick 2](https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html)

#### Geliştirme Yaklaşımı

Intel, modelleri optimize etmek ve dağıtmak için OpenVINO araç setini sağlar:

```python
# Example: Using OpenVINO for edge deployment
from openvino.inference_engine import IECore

# Initialize the Inference Engine
ie = IECore()

# Read the network from IR files
net = ie.read_network(model="optimized_model.xml", weights="optimized_model.bin")

# Prepare input and output blobs
input_blob = next(iter(net.input_info))
output_blob = next(iter(net.outputs))

# Load the network to the device (CPU, GPU, MYRIAD, etc.)
exec_net = ie.load_network(network=net, device_name="CPU")

# Prepare input
input_data = preprocess_image("sample.jpg")

# Run inference
result = exec_net.infer(inputs={input_blob: input_data})

# Process output
output = result[output_blob]
```

### Qualcomm AI Çözümleri

Qualcomm'un platformları mobil ve gömülü uygulamalara odaklanır:

#### Qualcomm Snapdragon

Snapdragon Sistem-çipleri (SoC'ler) şunları entegre eder:

- **Qualcomm AI Engine** Hexagon DSP ile
- **Adreno GPU** grafik ve paralel hesaplama için
- **Kryo CPU** çekirdekleri genel işlem için
- **En uygun kullanım alanı**: Akıllı telefonlar, tabletler, XR başlıkları ve akıllı kameralar

[Qualcomm Snapdragon for Edge AI](https://www.qualcomm.com/products/mobile/snapdragon/pcs/product-list)

#### Qualcomm Cloud AI 100

Özel edge AI çıkarım hızlandırıcı:

- **400 TOPS'a kadar** AI performansı
- **Güç verimliliği** veri merkezleri ve edge dağıtımı için optimize edilmiş
- **Ölçeklenebilir mimari** çeşitli dağıtım senaryoları için
- **En uygun kullanım alanı**: Kontrollü ortamlarda yüksek verimli edge AI uygulamaları

[Qualcomm Cloud AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence)

#### Qualcomm RB5/RB6 Robotik Platformu

Robotik ve gelişmiş edge bilişim için özel olarak tasarlanmış:

- **Entegre 5G bağlantısı**
- **Gelişmiş AI ve bilgisayar görüsü yetenekleri**
- **Kapsamlı sensör desteği**
- **En uygun kullanım alanı**: Otonom robotlar, dronlar ve akıllı endüstriyel sistemler

[Qualcomm Robotics Platform](https://www.qualcomm.com/products/application/robotics/qualcomm-robotics-rb6-platform)

#### Geliştirme Yaklaşımı

Qualcomm, Neural Processing SDK ve AI Model Efficiency Toolkit sağlar:

```python
# Example: Using Qualcomm Neural Processing SDK
from qti.aisw.dlc_utils import modeltools

# Convert your model to DLC format
converter = modeltools.DlcConverter()
converter.convert(
    input_network="model.tflite",
    input_dim=[1, 224, 224, 3],
    output_path="optimized_model.dlc"
)

# Use SNPE runtime for inference
from qti.aisw.snpe_runtime import SnpeRuntime

# Initialize runtime
runtime = SnpeRuntime()
runtime.load("optimized_model.dlc")

# Prepare input
input_tensor = preprocess_image("sample.jpg")

# Run inference
outputs = runtime.execute(input_tensor)

# Process results
predictions = postprocess_output(outputs)
```

### 🎮 NVIDIA Edge AI Çözümleri

NVIDIA, edge dağıtımı için güçlü GPU hızlandırmalı platformlar sunar:

#### NVIDIA Jetson Ailesi

Edge AI bilişim platformları için özel olarak tasarlanmış:

##### Jetson Orin Serisi
- **275 TOPS'a kadar** AI performansı
- **NVIDIA Ampere mimarisi** GPU
- **Güç yapılandırmaları** 5W'dan 60W'a kadar
- **En uygun kullanım alanı**: Gelişmiş robotik, akıllı video analitiği ve tıbbi cihazlar

##### Jetson Nano
- **Giriş seviyesi AI bilişim** (472 GFLOPS)
- **128 çekirdekli Maxwell GPU**
- **Güç verimli** (5-10W)
- **En uygun kullanım alanı**: Hobi projeleri, eğitim uygulamaları ve basit AI dağıtımları

[NVIDIA Jetson Platform](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)

#### NVIDIA Clara Guardian

Sağlık AI uygulamaları için platform:

- **Gerçek zamanlı algılama** hasta takibi için
- **Jetson veya GPU hızlandırmalı sunucular üzerine inşa edilmiş**
- **Sağlık odaklı optimizasyonlar**
- **En uygun kullanım alanı**: Akıllı hastaneler, hasta takibi ve tıbbi görüntüleme

[NVIDIA Clara Guardian](https://developer.nvidia.com/clara)

#### NVIDIA EGX Platformu

Kurumsal düzeyde edge bilişim çözümleri:

- **NVIDIA A100'dan T4 GPU'lara kadar ölçeklenebilir**
- **OEM ortaklarından sertifikalı sunucu çözümleri**
- **NVIDIA AI Enterprise yazılım paketi dahil**
- **En uygun kullanım alanı**: Endüstriyel ve kurumsal ortamlarda büyük ölçekli edge AI dağıtımları

[NVIDIA EGX Platform](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)

#### Geliştirme Yaklaşımı

NVIDIA, optimize edilmiş model dağıtımı için TensorRT sağlar:

```python
# Example: Using TensorRT for optimized inference
import tensorrt as trt
import numpy as np

# Create builder and network
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

# Parse ONNX model
parser = trt.OnnxParser(network, logger)
with open("model.onnx", "rb") as f:
    parser.parse(f.read())

# Create optimization config
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB
config.set_flag(trt.BuilderFlag.FP16)

# Build and serialize engine
engine = builder.build_engine(network, config)
with open("model.trt", "wb") as f:
    f.write(engine.serialize())

# Create runtime and execute
runtime = trt.Runtime(logger)
engine = runtime.deserialize_cuda_engine(open("model.trt", "rb").read())
context = engine.create_execution_context()

# Run inference
input_data = preprocess_image("sample.jpg")
output = run_inference(context, input_data, engine)
```

### Windows AI PC'leri

Windows AI PC'leri, özel Neural Processing Unit'ler (NPU'lar) içeren en yeni edge AI donanım kategorisini temsil eder:

#### Qualcomm Snapdragon X Elite/Plus

Windows Copilot+ PC'lerin ilk nesli şunları içerir:

- **Hexagon NPU** 45+ TOPS AI performansı ile
- **Qualcomm Oryon CPU** 12 çekirdeğe kadar
- **Adreno GPU** grafik ve ek AI hızlandırma için
- **En uygun kullanım alanı**: AI destekli üretkenlik, içerik oluşturma ve yazılım geliştirme

[Qualcomm Snapdragon X Elite](https://www.qualcomm.com/snapdragon/laptops)

#### Intel Core Ultra (Meteor Lake ve sonrası)

Intel'in AI PC işlemcileri şunları içerir:

- **Intel AI Boost (NPU)** 10 TOPS'a kadar performans sunar
- **Intel Arc GPU** ek AI hızlandırma sağlar
- **Performans ve verimlilik CPU çekirdekleri**
- **En uygun kullanım alanı**: İş dizüstü bilgisayarları, yaratıcı iş istasyonları ve günlük AI destekli bilişim

[Intel Core Ultra Processors](https://www.intel.com/content/www/us/en/products/details/processors/core/ultra.html)

#### AMD Ryzen AI Serisi

AMD'nin AI odaklı işlemcileri şunları içerir:

- **XDNA tabanlı NPU** 16 TOPS'a kadar performans sağlar
- **Zen 4 CPU çekirdekleri** genel işlem için
- **RDNA 3 grafik** ek hesaplama yetenekleri için
- **En uygun kullanım alanı**: Yaratıcı profesyoneller, geliştiriciler ve güç kullanıcıları

[AMD Ryzen AI Processors](https://www.amd.com/en/processors/ryzen-ai.html)

#### Geliştirme Yaklaşımı

Windows AI PC'leri, Windows Developer Platform ve DirectML'yi kullanır:

```csharp
// Example: Using Windows App SDK and DirectML
using Microsoft.AI.DirectML;
using Microsoft.Windows.AI;

// Load model
var modelPath = "optimized_model.onnx";
var modelOptions = new OnnxModelOptions
{
    InterOpNumThreads = 4,
    IntraOpNumThreads = 4
};

// Create model
var model = await OnnxModel.CreateFromFileAsync(modelPath, modelOptions);

// Prepare input
var inputFeatures = new List<InputFeature>
{
    new InputFeature
    {
        Name = "input",
        Value = imageData
    }
};

// Run inference
var results = await model.EvaluateAsync(inputFeatures);

// Process output
var output = results.First().Value;
```

## ⚡ Donanıma Özgü Optimizasyon Teknikleri

### 🔍 Kuantizasyon Yaklaşımları

Farklı donanım platformları, belirli kuantizasyon tekniklerinden faydalanır:

#### Intel OpenVINO Optimizasyonları
- **INT8 kuantizasyonu** CPU ve entegre GPU için
- **FP16 hassasiyeti** minimum doğruluk kaybıyla performansı artırır
- **Asimetrik kuantizasyon** aktivasyon dağılımlarını yönetmek için

#### Qualcomm AI Engine Optimizasyonları
- **UINT8 kuantizasyonu** Hexagon DSP için
- **Karışık hassasiyet** mevcut tüm hesaplama birimlerini kullanır
- **Kanal başına kuantizasyon** doğruluğu artırır

#### NVIDIA TensorRT Optimizasyonları
- **INT8 ve FP16 hassasiyeti** GPU hızlandırma için
- **Katman birleştirme** bellek transferlerini azaltır
- **Çekirdek otomatik ayarlama** belirli GPU mimarileri için

#### Windows NPU Optimizasyonları
- **INT8/INT4 kuantizasyonu** NPU yürütmesi için
- **DirectML grafik optimizasyonları**
- **Windows ML çalışma zamanı hızlandırma**

### Mimariye Özgü Uyarlamalar

Farklı donanımlar, belirli mimari hususlar gerektirir:

- **Intel**: AVX-512 vektör talimatları ve Intel Deep Learning Boost için optimize edin
- **Qualcomm**: Hexagon DSP, Adreno GPU ve Kryo CPU arasında heterojen hesaplamayı kullanın
- **NVIDIA**: GPU paralelliğini ve CUDA çekirdek kullanımını en üst düzeye çıkarın
- **Windows NPU**: NPU-CPU-GPU işbirlikçi işlemeyi tasarlayın

### Bellek Yönetimi Stratejileri

Etkili bellek yönetimi platforma göre değişir:

- **Intel**: Önbellek kullanımı ve bellek erişim desenleri için optimize edin
- **Qualcomm**: Heterojen işlemciler arasında paylaşılan belleği yönetin
- **NVIDIA**: CUDA birleşik belleği kullanın ve VRAM kullanımını optimize edin
- **Windows NPU**: Özel NPU belleği ve sistem RAM arasında iş yüklerini dengeleyin

## Performans Ölçümleme ve Metrikler

Edge AI dağıtımlarını değerlendirirken şu temel metrikleri göz önünde bulundurun:

### Performans Metrikleri

- **Çıkarım Süresi**: Çıkarım başına milisaniye (daha düşük daha iyidir)
- **Verimlilik**: Saniye başına çıkarım (daha yüksek daha iyidir)
- **Gecikme**: Uçtan uca yanıt süresi (daha düşük daha iyidir)
- **FPS**: Görü uygulamaları için saniye başına kare (daha yüksek daha iyidir)

### Verimlilik Metrikleri

- **Watt başına performans**: TOPS/W veya saniye başına çıkarım/watt
- **Çıkarım başına enerji**: Çıkarım başına tüketilen joule
- **Pil Etkisi**: AI iş yükleri çalıştırıldığında çalışma süresi azalması
- **Termal Verimlilik**: Sürekli çalışmada sıcaklık artışı

### Doğruluk Metrikleri

- **Top-1/Top-5 Doğruluk**: Sınıflandırma doğruluk yüzdesi
- **mAP**: Nesne algılama için Ortalama Hassasiyet
- **F1 Skoru**: Hassasiyet ve geri çağırma dengesi
- **Kuantizasyon Etkisi**: Tam hassasiyet ve kuantize modeller arasındaki doğruluk farkı

## Dağıtım Modelleri ve En İyi Uygulamalar

### Kurumsal Dağıtım Stratejileri

- **Konteynerleştirme**: Tutarlı dağıtım için Docker veya benzeri araçlar kullanma
- **Filo Yönetimi**: Azure IoT Edge gibi cihaz yönetim çözümleri
- **İzleme**: Telemetri toplama ve performans takibi
- **Güncelleme Yönetimi**: Modeller ve yazılımlar için OTA güncelleme mekanizmaları

### Hibrit Bulut-Kenar Desenleri

- **Bulut Eğitimi, Kenar Çıkarımı**: Bulutta eğitim, kenarda dağıtım
- **Kenar Ön İşleme, Bulut Analizi**: Kenarda temel işlem, bulutta karmaşık analiz
- **Federatif Öğrenme**: Veriyi merkezileştirmeden dağıtılmış model geliştirme
- **Artımlı Öğrenme**: Kenar verilerinden sürekli model geliştirme

### Entegrasyon Desenleri

- **Sensör Entegrasyonu**: Kameralar, mikrofonlar ve diğer sensörlere doğrudan bağlantı
- **Aktüatör Kontrolü**: Motorlar, ekranlar ve diğer çıktılar için gerçek zamanlı kontrol
- **Sistem Entegrasyonu**: Mevcut kurumsal sistemlerle iletişim
- **IoT Entegrasyonu**: Daha geniş IoT ekosistemleriyle bağlantı

## Sektöre Özgü Dağıtım Dikkat Noktaları

### Sağlık

- **Hasta Gizliliği**: Tıbbi veriler için HIPAA uyumluluğu
- **Tıbbi Cihaz Düzenlemeleri**: FDA ve diğer düzenleyici gereklilikler
- **Güvenilirlik Gereksinimleri**: Kritik uygulamalar için hata toleransı
- **Entegrasyon Standartları**: FHIR, HL7 ve diğer sağlık sektörü birlikte çalışabilirlik standartları

### Üretim

- **Endüstriyel Ortam**: Zorlu koşullar için dayanıklılık
- **Gerçek Zamanlı Gereksinimler**: Kontrol sistemleri için deterministik performans
- **Güvenlik Sistemleri**: Endüstriyel güvenlik protokolleriyle entegrasyon
- **Eski Sistem Entegrasyonu**: Mevcut OT altyapısıyla bağlantı

### Otomotiv

- **Fonksiyonel Güvenlik**: ISO 26262 uyumluluğu
- **Çevresel Dayanıklılık**: Sıcaklık aşırılıklarında çalışma
- **Güç Yönetimi**: Pil verimliliği odaklı çalışma
- **Yaşam Döngüsü Yönetimi**: Araç ömrü boyunca uzun vadeli destek

### Akıllı Şehirler

- **Dış Mekan Dağıtımı**: Hava koşullarına dayanıklılık ve fiziksel güvenlik
- **Ölçek Yönetimi**: Binlerce ila milyonlarca dağıtılmış cihaz
- **Ağ Değişkenliği**: Tutarsız bağlantılarla çalışma
- **Gizlilik Dikkat Noktaları**: Kamusal alan verilerinin sorumlu bir şekilde işlenmesi

## Kenar AI Donanımında Gelecek Trendler

### Yeni Donanım Gelişmeleri

- **AI'ye Özel Silikon**: Daha özel NPU'lar ve AI hızlandırıcılar
- **Nöromorfik Hesaplama**: Verimliliği artırmak için beyin ilhamlı mimariler
- **Bellek İçi Hesaplama**: AI işlemleri için veri hareketini azaltma
- **Çoklu Kalıp Paketleme**: Özel AI işlemcilerinin heterojen entegrasyonu

### Yazılım-Donanım Eş Gelişimi

- **Donanım Farkındalıklı Sinir Mimarisi Arama**: Belirli donanımlar için optimize edilmiş modeller
- **Derleyici İyileştirmeleri**: Modellerin donanım talimatlarına daha iyi çevrilmesi
- **Özel Grafik Optimizasyonları**: Donanıma özgü ağ dönüşümleri
- **Dinamik Adaptasyon**: Mevcut kaynaklara dayalı çalışma zamanı optimizasyonu

### Standartlaştırma Çabaları

- **ONNX ve ONNX Runtime**: Platformlar arası model birlikte çalışabilirliği
- **MLIR**: ML için çok seviyeli ara temsil
- **OpenXLA**: Hızlandırılmış lineer cebir derlemesi
- **TMUL**: Tensor işlemci soyutlama katmanları

## Kenar AI Dağıtımıyla Başlarken

### Geliştirme Ortamı Kurulumu

1. **Hedef Donanımı Seçin**: Kullanım senaryonuza uygun platformu seçin
2. **SDK'ları ve Araçları Kurun**: Üreticinin geliştirme kitini kurun
3. **Optimizasyon Araçlarını Yapılandırın**: Kuantizasyon ve derleme yazılımını kurun
4. **CI/CD Boru Hattını Kurun**: Otomatik test ve dağıtım iş akışını oluşturun

### Dağıtım Kontrol Listesi

- **Model Optimizasyonu**: Kuantizasyon, budama ve mimari optimizasyon
- **Performans Testi**: Gerçekçi koşullarda hedef donanımda kıyaslama
- **Güç Analizi**: Enerji tüketim kalıplarını ölçme
- **Güvenlik Denetimi**: Veri koruma ve erişim kontrollerini doğrulama
- **Güncelleme Mekanizması**: Güvenli güncelleme yeteneklerini uygulama
- **İzleme Kurulumu**: Telemetri toplama ve uyarı sistemlerini dağıtma

## ➡️ Sırada ne var

- [Modül 1 Genel Bakış](./README.md) bölümünü inceleyin
- [Modül 2: Küçük Dil Modeli Temelleri](../Module02/README.md) bölümünü keşfedin
- [Modül 3: SLM Dağıtım Stratejileri](../Module03/README.md) bölümüne geçin

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.