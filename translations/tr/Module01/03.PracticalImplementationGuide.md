<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:28:15+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "tr"
}
-->
# Bölüm 3: Pratik Uygulama Rehberi

## Genel Bakış

Bu kapsamlı rehber, EdgeAI kursuna hazırlanmanıza yardımcı olacak. Kurs, uç cihazlarda verimli bir şekilde çalışan pratik yapay zeka çözümleri geliştirmeye odaklanmaktadır. Kurs, modern çerçeveler ve uç dağıtım için optimize edilmiş en son modeller kullanılarak uygulamalı geliştirmeyi vurgular.

## 1. Geliştirme Ortamının Kurulumu

### Programlama Dilleri ve Çerçeveler

**Python Ortamı**
- **Sürüm**: Python 3.10 veya üzeri (önerilen: Python 3.11)
- **Paket Yöneticisi**: pip veya conda
- **Sanal Ortam**: İzolasyon için venv veya conda ortamlarını kullanın
- **Anahtar Kütüphaneler**: Kurs sırasında belirli EdgeAI kütüphanelerini kuracağız

**Microsoft .NET Ortamı**
- **Sürüm**: .NET 8 veya üzeri
- **IDE**: Visual Studio 2022, Visual Studio Code veya JetBrains Rider
- **SDK**: Çapraz platform geliştirme için .NET SDK'nın kurulu olduğundan emin olun

### Geliştirme Araçları

**Kod Editörleri ve IDE'ler**
- Visual Studio Code (çapraz platform geliştirme için önerilir)
- PyCharm veya Visual Studio (dil odaklı geliştirme için)
- Jupyter Notebooks, etkileşimli geliştirme ve prototipleme için

**Versiyon Kontrolü**
- Git (en son sürüm)
- Depolara erişim ve iş birliği için bir GitHub hesabı

## 2. Donanım Gereksinimleri ve Öneriler

### Minimum Sistem Gereksinimleri
- **CPU**: Çok çekirdekli işlemci (Intel i5/AMD Ryzen 5 veya eşdeğeri)
- **RAM**: Minimum 8GB, önerilen 16GB
- **Depolama**: Modeller ve geliştirme araçları için 50GB boş alan
- **İşletim Sistemi**: Windows 10/11, macOS 10.15+ veya Linux (Ubuntu 20.04+)

### Hesaplama Kaynakları Stratejisi
Kurs, farklı donanım yapılandırmalarında erişilebilir olacak şekilde tasarlanmıştır:

**Yerel Geliştirme (CPU/NPU Odaklı)**
- Birincil geliştirme CPU ve NPU hızlandırmasını kullanacak
- Çoğu modern dizüstü ve masaüstü bilgisayar için uygun
- Verimlilik ve pratik dağıtım senaryolarına odaklanır

**Bulut GPU Kaynakları (İsteğe Bağlı)**
- **Azure Machine Learning**: Yoğun eğitim ve deneyler için
- **Google Colab**: Eğitim amaçlı ücretsiz katman mevcut
- **Kaggle Notebooks**: Alternatif bir bulut bilişim platformu

### Uç Cihaz Dikkat Noktaları
- ARM tabanlı işlemciler hakkında bilgi sahibi olunması
- Mobil ve IoT donanım kısıtlamalarının anlaşılması
- Güç tüketimi optimizasyonu konusunda bilgi sahibi olunması

## 3. Temel Model Aileleri ve Kaynaklar

### Ana Model Aileleri

**Microsoft Phi-4 Ailesi**
- **Açıklama**: Uç dağıtım için tasarlanmış kompakt ve verimli modeller
- **Güçlü Yönler**: Boyut-performans oranı mükemmel, akıl yürütme görevleri için optimize edilmiş
- **Kaynak**: [Phi-4 Koleksiyonu Hugging Face'te](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Kullanım Alanları**: Kod üretimi, matematiksel akıl yürütme, genel konuşma

**Qwen-3 Ailesi**
- **Açıklama**: Alibaba'nın çok dilli modellerinin en son nesli
- **Güçlü Yönler**: Güçlü çok dilli yetenekler, verimli mimari
- **Kaynak**: [Qwen-3 Koleksiyonu Hugging Face'te](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Kullanım Alanları**: Çok dilli uygulamalar, kültürler arası yapay zeka çözümleri

**Google Gemma-3n Ailesi**
- **Açıklama**: Google'ın uç dağıtım için optimize edilmiş hafif modelleri
- **Güçlü Yönler**: Hızlı çıkarım, mobil dostu mimari
- **Kaynak**: [Gemma-3n Koleksiyonu Hugging Face'te](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Kullanım Alanları**: Mobil uygulamalar, gerçek zamanlı işlem

### Model Seçim Kriterleri
- **Performans ve Boyut Dengesi**: Daha küçük veya daha büyük modellerin ne zaman seçileceğini anlamak
- **Görev Odaklı Optimizasyon**: Modelleri belirli kullanım durumlarına göre eşleştirme
- **Dağıtım Kısıtlamaları**: Bellek, gecikme ve güç tüketimi dikkate alınmalı

## 4. Kuantizasyon ve Optimizasyon Araçları

### Llama.cpp Çerçevesi
- **Depo**: [Llama.cpp GitHub'da](https://github.com/ggml-org/llama.cpp)
- **Amacı**: LLM'ler için yüksek performanslı çıkarım motoru
- **Ana Özellikler**:
  - CPU için optimize edilmiş çıkarım
  - Birden fazla kuantizasyon formatı (Q4, Q5, Q8)
  - Çapraz platform uyumluluğu
  - Bellek verimli çalışma
- **Kurulum ve Temel Kullanım**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Depo**: [Microsoft Olive GitHub'da](https://github.com/microsoft/olive)
- **Amacı**: Uç dağıtım için model optimizasyon araç seti
- **Ana Özellikler**:
  - Otomatik model optimizasyon iş akışları
  - Donanım odaklı optimizasyon
  - ONNX Runtime ile entegrasyon
  - Performans kıyaslama araçları
- **Kurulum ve Temel Kullanım**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # Model optimizasyonu için örnek Python betiği
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Kullanıcıları)
- **Depo**: [Apple MLX GitHub'da](https://github.com/ml-explore/mlx)
- **Amacı**: Apple Silicon için makine öğrenimi çerçevesi
- **Ana Özellikler**:
  - Yerel Apple Silicon optimizasyonu
  - Bellek verimli işlemler
  - PyTorch benzeri API
  - Birleşik bellek mimarisi desteği
- **Kurulum ve Temel Kullanım**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Depo**: [ONNX Runtime GitHub'da](https://github.com/microsoft/onnxruntime)
- **Amacı**: ONNX modelleri için çapraz platform çıkarım hızlandırma
- **Ana Özellikler**:
  - Donanıma özel optimizasyonlar (CPU, GPU, NPU)
  - Çıkarım için grafik optimizasyonları
  - Kuantizasyon desteği
  - Çapraz dil desteği (Python, C++, C#, JavaScript)
- **Kurulum ve Temel Kullanım**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. Önerilen Okuma ve Kaynaklar

### Temel Dokümantasyon
- **ONNX Runtime Dokümantasyonu**: Çapraz platform çıkarımı anlama
- **Hugging Face Transformers Rehberi**: Model yükleme ve çıkarım
- **Edge AI Tasarım Desenleri**: Uç dağıtım için en iyi uygulamalar

### Teknik Makaleler
- "Verimli Uç Yapay Zeka: Kuantizasyon Teknikleri Üzerine Bir Araştırma"
- "Mobil ve Uç Cihazlar için Model Sıkıştırma"
- "Uç Bilişim için Transformer Modellerini Optimize Etme"

### Topluluk Kaynakları
- **EdgeAI Slack/Discord Toplulukları**: Eş desteği ve tartışma
- **GitHub Depoları**: Örnek uygulamalar ve eğitimler
- **YouTube Kanalları**: Teknik derinlemesine incelemeler ve eğitimler

## 6. Değerlendirme ve Doğrulama

### Kurs Öncesi Kontrol Listesi
- [ ] Python 3.10+ yüklendi ve doğrulandı
- [ ] .NET 8+ yüklendi ve doğrulandı
- [ ] Geliştirme ortamı yapılandırıldı
- [ ] Hugging Face hesabı oluşturuldu
- [ ] Hedef model aileleri hakkında temel bilgiye sahip olundu
- [ ] Kuantizasyon araçları yüklendi ve test edildi
- [ ] Donanım gereksinimleri karşılandı
- [ ] Bulut bilişim hesapları kuruldu (gerekirse)

## Temel Öğrenme Hedefleri

Bu rehberin sonunda şunları yapabileceksiniz:

1. EdgeAI uygulama geliştirme için eksiksiz bir geliştirme ortamı kurmak
2. Model optimizasyonu için gerekli araçları ve çerçeveleri yüklemek ve yapılandırmak
3. EdgeAI projeleriniz için uygun donanım ve yazılım yapılandırmalarını seçmek
4. Yapay zeka modellerini uç cihazlarda dağıtmak için temel hususları anlamak
5. Kurstaki uygulamalı alıştırmalara sisteminizi hazırlamak

## Ek Kaynaklar

### Resmi Dokümantasyon
- **Python Dokümantasyonu**: Resmi Python dil dokümantasyonu
- **Microsoft .NET Dokümantasyonu**: Resmi .NET geliştirme kaynakları
- **ONNX Runtime Dokümantasyonu**: ONNX Runtime için kapsamlı rehber
- **TensorFlow Lite Dokümantasyonu**: Resmi TensorFlow Lite dokümantasyonu

### Geliştirme Araçları
- **Visual Studio Code**: Yapay zeka geliştirme uzantılarına sahip hafif kod editörü
- **Jupyter Notebooks**: ML denemeleri için etkileşimli bir hesaplama ortamı
- **Docker**: Tutarlı geliştirme ortamları için konteyner platformu
- **Git**: Kod yönetimi için versiyon kontrol sistemi

### Öğrenme Kaynakları
- **EdgeAI Araştırma Makaleleri**: Verimli modeller üzerine en son akademik araştırmalar
- **Çevrimiçi Kurslar**: Yapay zeka optimizasyonu üzerine ek öğrenme materyalleri
- **Topluluk Forumları**: EdgeAI geliştirme zorlukları için SSS platformları
- **Kıyaslama Veri Setleri**: Model performansını değerlendirmek için standart veri setleri

## Öğrenim Çıktıları

Bu hazırlık rehberini tamamladıktan sonra:

1. EdgeAI geliştirme için tamamen yapılandırılmış bir geliştirme ortamına sahip olacaksınız
2. Farklı dağıtım senaryoları için donanım ve yazılım gereksinimlerini anlayacaksınız
3. Kurs boyunca kullanılan temel çerçeveler ve araçlarla tanışmış olacaksınız
4. Cihaz kısıtlamalarına ve gereksinimlerine göre uygun modelleri seçebileceksiniz
5. Uç dağıtım için optimizasyon teknikleri hakkında temel bilgiye sahip olacaksınız

## ➡️ Sıradaki Adım

- [04: EdgeAI Donanım ve Dağıtım](04.EdgeDeployment.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.