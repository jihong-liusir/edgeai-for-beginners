<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-17T23:55:00+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "tr"
}
-->
# Bölüm 3: İnce Ayar - Modelleri Belirli Görevler İçin Özelleştirme

## İçindekiler
1. [İnce Ayara Giriş](../../../Module05)
2. [İnce Ayarın Önemi](../../../Module05)
3. [İnce Ayar Türleri](../../../Module05)
4. [Microsoft Olive ile İnce Ayar](../../../Module05)
5. [Uygulamalı Örnekler](../../../Module05)
6. [En İyi Uygulamalar ve Rehberler](../../../Module05)
7. [İleri Teknikler](../../../Module05)
8. [Değerlendirme ve İzleme](../../../Module05)
9. [Yaygın Zorluklar ve Çözümler](../../../Module05)
10. [Sonuç](../../../Module05)

## İnce Ayara Giriş

**İnce ayar**, önceden eğitilmiş bir modeli belirli görevleri yerine getirmek veya özel veri kümeleriyle çalışmak için uyarlamayı içeren güçlü bir makine öğrenimi tekniğidir. Bir modeli sıfırdan eğitmek yerine, ince ayar, önceden eğitilmiş bir modelin zaten öğrendiği bilgileri kullanır ve bunu sizin özel kullanım durumunuza göre ayarlar.

### İnce Ayar Nedir?

İnce ayar, bir tür **transfer öğrenimi**dir ve şu adımları içerir:
- Büyük veri kümelerinden genel kalıplar öğrenmiş bir önceden eğitilmiş modelle başlamak
- Modelin iç parametrelerini kendi özel veri kümenizi kullanarak ayarlamak
- Değerli bilgiyi korurken modeli görevinize özel hale getirmek

Bunu, yetenekli bir şefe yeni bir mutfağı öğretmek gibi düşünebilirsiniz - temel yemek pişirme bilgisine zaten sahipler, ancak yeni stil için özel teknikleri ve tatları öğrenmeleri gerekir.

### Temel Faydalar

- **Zaman Verimliliği**: Sıfırdan eğitmeye göre çok daha hızlı
- **Veri Verimliliği**: İyi performans elde etmek için daha küçük veri kümeleri gerektirir
- **Maliyet Etkinliği**: Daha düşük hesaplama gereksinimleri
- **Daha İyi Performans**: Sıfırdan eğitime kıyasla genellikle üstün sonuçlar elde eder
- **Kaynak Optimizasyonu**: Güçlü yapay zekayı daha küçük ekipler ve organizasyonlar için erişilebilir hale getirir

## İnce Ayarın Önemi

### Gerçek Dünya Uygulamaları

İnce ayar birçok senaryoda önemlidir:

**1. Alan Uyarlaması**
- Tıbbi Yapay Zeka: Genel dil modellerini tıbbi terminoloji ve klinik notlar için uyarlamak
- Hukuk Teknolojisi: Hukuki belge analizi ve sözleşme incelemesi için modelleri özelleştirmek
- Finansal Hizmetler: Finansal rapor analizi ve risk değerlendirmesi için modelleri uyarlamak

**2. Görev Uzmanlığı**
- İçerik Üretimi: Belirli yazım stilleri veya tonları için ince ayar yapmak
- Kod Üretimi: Belirli programlama dilleri veya çerçeveler için modelleri uyarlamak
- Çeviri: Belirli dil çiftleri veya teknik alanlar için performansı iyileştirmek

**3. Kurumsal Uygulamalar**
- Müşteri Hizmetleri: Şirket spesifik terminolojiyi anlayan sohbet botları oluşturmak
- İç Dokümantasyon: Organizasyonel süreçlere aşina yapay zeka asistanları oluşturmak
- Sektör-Specifik Çözümler: Sektör spesifik jargon ve iş akışlarını anlayan modeller geliştirmek

## İnce Ayar Türleri

### 1. Tam İnce Ayar (Talimat İnce Ayarı)

Tam ince ayarda, eğitim sırasında tüm model parametreleri güncellenir. Bu yaklaşım:
- Maksimum esneklik ve performans potansiyeli sağlar
- Önemli hesaplama kaynakları gerektirir
- Modelin tamamen yeni bir versiyonunu oluşturur
- Büyük eğitim verisi ve hesaplama kaynaklarına sahip senaryolar için en iyisidir

### 2. Parametre-Etkin İnce Ayar (PEFT)

PEFT yöntemleri yalnızca küçük bir parametre alt kümesini güncelleyerek süreci daha verimli hale getirir:

#### Düşük Dereceli Uyarlama (LoRA)
- Mevcut ağırlıklara küçük eğitilebilir dereceli ayrıştırma matrisleri ekler
- Eğitilebilir parametrelerin sayısını önemli ölçüde azaltır
- Tam ince ayara yakın performansı korur
- Farklı uyarlamalar arasında kolay geçiş sağlar

#### QLoRA (Kantitatif LoRA)
- LoRA'yı kantitatif tekniklerle birleştirir
- Bellek gereksinimlerini daha da azaltır
- Daha büyük modellerin tüketici donanımında ince ayarını sağlar
- Verimlilik ile performansı dengeler

#### Adaptörler
- Mevcut katmanlar arasına küçük sinir ağları ekler
- Temel modeli sabit tutarken hedefli ince ayar sağlar
- Model özelleştirme için modüler bir yaklaşım sunar

### 3. Görev-Specifik İnce Ayar

Modelleri belirli alt görevler için uyarlamaya odaklanır:
- **Sınıflandırma**: Kategorilendirme görevleri için modelleri ayarlamak
- **Üretim**: İçerik oluşturma ve metin üretimi için optimize etmek
- **Çıkarım**: Bilgi çıkarımı ve adlandırılmış varlık tanıma için ince ayar yapmak
- **Özetleme**: Belge özetleme için modelleri uzmanlaştırmak

## Microsoft Olive ile İnce Ayar

Microsoft Olive, ince ayar sürecini basitleştirirken kurumsal düzeyde özellikler sunan kapsamlı bir model optimizasyon araç setidir.

### Microsoft Olive Nedir?

Microsoft Olive, açık kaynaklı bir model optimizasyon aracıdır ve:
- Çeşitli donanım hedefleri için ince ayar iş akışlarını kolaylaştırır
- Popüler model mimarileri için yerleşik destek sağlar (Llama, Phi, Qwen, Gemma)
- Hem bulut hem de yerel dağıtım seçenekleri sunar
- Azure ML ve diğer Microsoft AI hizmetleriyle sorunsuz bir şekilde entegre olur
- Otomatik optimizasyon ve kantitatif destek sunar

### Temel Özellikler

- **Donanım-Duyarlı Optimizasyon**: Modelleri belirli donanımlar için otomatik olarak optimize eder (CPU, GPU, NPU)
- **Çoklu Format Desteği**: PyTorch, Hugging Face ve ONNX modelleriyle çalışır
- **Otomatik İş Akışları**: Manuel yapılandırma ve deneme-yanılma sürecini azaltır
- **Kurumsal Entegrasyon**: Azure ML ve bulut dağıtımları için yerleşik destek
- **Genişletilebilir Mimari**: Özel optimizasyon tekniklerine olanak tanır

### Kurulum ve Ayar

#### Temel Kurulum

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Opsiyonel Bağımlılıklar

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Kurulumu Doğrulama

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Uygulamalı Örnekler

### Örnek 1: Olive CLI ile Temel İnce Ayar

Bu örnek, küçük bir dil modelinin ifade sınıflandırması için ince ayarını gösterir:

#### Adım 1: Ortamınızı Hazırlayın

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Adım 2: Modeli İnce Ayar Yapın

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Adım 3: Dağıtım için Optimize Edin

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Örnek 2: Özel Veri Kümesi ile İleri Düzey Yapılandırma

#### Adım 1: Özel Veri Kümesi Hazırlayın

Eğitim verilerinizle bir JSON dosyası oluşturun:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Adım 2: Yapılandırma Dosyası Oluşturun

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Adım 3: İnce Ayarı Gerçekleştirin

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Örnek 3: Bellek Verimliliği için QLoRA İnce Ayarı

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## En İyi Uygulamalar ve Rehberler

### Veri Hazırlığı

**1. Kalite, Miktardan Daha Önemlidir**
- Büyük miktarda düşük kaliteli veri yerine yüksek kaliteli, çeşitli örnekleri önceliklendirin
- Verilerin hedef kullanım durumunuzu temsil ettiğinden emin olun
- Verileri tutarlı bir şekilde temizleyin ve ön işleme tabi tutun

**2. Veri Formatı ve Şablonlar**
- Tüm eğitim örneklerinde tutarlı formatlama kullanın
- Kullanım durumunuza uygun net giriş-çıkış şablonları oluşturun
- Talimat-tuneli modeller için uygun talimat formatlamasını dahil edin

**3. Veri Kümesi Bölme**
- Verilerin %10-20'sini doğrulama için ayırın
- Eğitim/doğrulama bölümleri arasında benzer dağılımlar koruyun
- Sınıflandırma görevleri için tabakalı örnekleme düşünün

### Eğitim Yapılandırması

**1. Öğrenme Hızı Seçimi**
- İnce ayar için daha küçük öğrenme hızlarıyla başlayın (1e-5 ila 1e-4)
- Daha iyi yakınsama için öğrenme hızı planlaması kullanın
- Kayıp eğrilerini izleyerek hızları buna göre ayarlayın

**2. Toplu Boyut Optimizasyonu**
- Toplu boyutu mevcut bellekle dengeleyin
- Daha büyük etkili toplu boyutlar için gradyan birikimi kullanın
- Toplu boyut ile öğrenme hızı arasındaki ilişkiyi göz önünde bulundurun

**3. Eğitim Süresi**
- Aşırı uyumdan kaçınmak için doğrulama metriklerini izleyin
- Doğrulama performansı plato yaptığında erken durdurma kullanın
- Kurtarma ve analiz için düzenli olarak kontrol noktaları kaydedin

### Model Seçimi

**1. Temel Model Seçimi**
- Mümkünse benzer alanlarda önceden eğitilmiş modelleri seçin
- Model boyutunu hesaplama kısıtlamalarınıza göre düşünün
- Ticari kullanım için lisans gerekliliklerini değerlendirin

**2. İnce Ayar Yöntemi Seçimi**
- Kaynak kısıtlı ortamlar için LoRA/QLoRA kullanın
- Maksimum performans kritik olduğunda tam ince ayar seçin
- Birden fazla görev senaryosu için adaptör tabanlı yaklaşımları düşünün

### Kaynak Yönetimi

**1. Donanım Optimizasyonu**
- Model boyutunuza ve yöntemlerinize uygun donanımı seçin
- Gradyan kontrol noktalamayla GPU belleğini verimli bir şekilde kullanın
- Daha büyük modeller için bulut tabanlı çözümleri düşünün

**2. Bellek Yönetimi**
- Mevcut olduğunda karışık hassasiyet eğitimi kullanın
- Bellek kısıtlamaları için gradyan birikimi uygulayın
- Eğitim sırasında GPU bellek kullanımını izleyin

## İleri Teknikler

### Çoklu Adaptör Eğitimi

Temel modeli paylaşırken farklı görevler için birden fazla adaptör eğitin:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hiperparametre Optimizasyonu

Sistematik hiperparametre ayarı uygulayın:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Özel Kayıp Fonksiyonları

Alan spesifik kayıp fonksiyonları uygulayın:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Değerlendirme ve İzleme

### Metrikler ve Değerlendirme

**1. Standart Metrikler**
- **Doğruluk**: Sınıflandırma görevleri için genel doğruluk
- **Perpleksite**: Dil modelleme kalitesi ölçüsü
- **BLEU/ROUGE**: Metin üretimi ve özetleme kalitesi
- **F1 Skoru**: Sınıflandırma için dengeli hassasiyet ve geri çağırma

**2. Alan-Specifik Metrikler**
- **Görev-Specifik Kıyaslamalar**: Alanınız için belirlenmiş kıyaslamaları kullanın
- **İnsan Değerlendirmesi**: Öznel görevler için insan değerlendirmesini dahil edin
- **İş Metrikleri**: Gerçek iş hedefleriyle uyum sağlayın

**3. Değerlendirme Kurulumu**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Eğitim İlerlemesini İzleme

**1. Kayıp Takibi**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Doğrulama İzleme**
- Eğitim kaybıyla birlikte doğrulama kaybını izleyin
- Aşırı uyum belirtilerini izleyin (doğrulama kaybı artarken eğitim kaybı azalır)
- Doğrulama metriklerine dayalı olarak erken durdurma kullanın

**3. Kaynak İzleme**
- GPU/CPU kullanımını izleyin
- Bellek kullanım kalıplarını takip edin
- Eğitim hızı ve verimini izleyin

## Yaygın Zorluklar ve Çözümler

### Zorluk 1: Aşırı Uyum

**Belirtiler:**
- Eğitim kaybı azalmaya devam ederken doğrulama kaybı artar
- Eğitim ve doğrulama performansı arasında büyük fark
- Yeni verilere karşı zayıf genelleme

**Çözümler:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Zorluk 2: Bellek Sınırlamaları

**Çözümler:**
- Gradyan kontrol noktalamayı kullanın
- Gradyan birikimi uygulayın
- Parametre-etkin yöntemleri seçin (LoRA, QLoRA)
- Büyük modeller için model paralelliğini kullanın

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Zorluk 3: Yavaş Eğitim

**Çözümler:**
- Veri yükleme hatlarını optimize edin
- Karışık hassasiyet eğitimi kullanın
- Verimli toplu işleme stratejileri uygulayın
- Büyük veri kümeleri için dağıtılmış eğitimi düşünün

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Zorluk 4: Zayıf Performans

**Tanı Adımları:**
1. Veri kalitesini ve formatını doğrulayın
2. Öğrenme hızını ve eğitim süresini kontrol edin
3. Temel model seçimini değerlendirin
4. Ön işleme ve tokenizasyonu gözden geçirin

**Çözümler:**
- Eğitim veri çeşitliliğini artırın
- Öğrenme hızı planlamasını ayarlayın
- Farklı temel modelleri deneyin
- Veri artırma tekniklerini uygulayın

## Sonuç

İnce ayar, en son yapay zeka yeteneklerine erişimi demokratikleştiren güçlü bir tekniktir. Microsoft Olive gibi araçları kullanarak, organizasyonlar önceden eğitilmiş modelleri özel ihtiyaçlarına verimli bir şekilde uyarlayabilir ve performans ile kaynak kısıtlamalarını optimize edebilir.

### Temel Çıkarımlar

1. **Doğru Yaklaşımı Seçin**: Hesaplama kaynaklarınıza ve performans gereksinimlerinize göre ince ayar yöntemlerini seçin
2. **Veri Kalitesi Önemlidir**: Yüksek kaliteli, temsil edici eğitim verilerine yatırım yapın
3. **İzleyin ve Yineleyin**: Modellerinizi sürekli olarak değerlendirin ve iyileştirin
4. **Araçlardan Yararlanın**: Süreci basitleştirmek ve optimize etmek için Olive gibi çerçeveleri kullanın
5. **Dağıtımı Düşünün**: Model optimizasyonu ve dağıtımı için baştan plan yapın

## ➡️ Sırada Ne Var?

- [04: Dağıtım - Üretime Hazır Model Uygulaması](./04.SLMOps.Deployment.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.