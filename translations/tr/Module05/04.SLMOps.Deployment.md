<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T23:56:33+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "tr"
}
-->
# Bölüm 4: Dağıtım - Üretime Hazır Model Uygulaması

## Genel Bakış

Bu kapsamlı eğitim, Foundry Local kullanarak ince ayar yapılmış kuantize edilmiş modellerin dağıtım sürecini baştan sona size rehberlik edecek. Model dönüşümü, kuantizasyon optimizasyonu ve dağıtım yapılandırmasını ele alacağız.

## Ön Koşullar

Başlamadan önce aşağıdaki gereksinimlere sahip olduğunuzdan emin olun:

- ✅ Dağıtıma hazır ince ayar yapılmış bir onnx modeli
- ✅ Windows veya Mac bilgisayar
- ✅ Python 3.10 veya üstü
- ✅ En az 8GB boş RAM
- ✅ Sisteminizde kurulu Foundry Local

## Bölüm 1: Ortam Kurulumu

### Gerekli Araçların Kurulumu

Terminalinizi açın (Windows'ta Komut İstemi, Mac'te Terminal) ve aşağıdaki komutları sırasıyla çalıştırın:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **Önemli Not**: Ayrıca [cmake.org](https://cmake.org/download/) adresinden indirilebilecek 3.31 veya daha yeni bir CMake sürümüne ihtiyacınız olacak.

## Bölüm 2: Model Dönüşümü ve Kuantizasyon

### Doğru Formatı Seçmek

İnce ayar yapılmış küçük dil modelleri için **ONNX formatını** kullanmanızı öneriyoruz çünkü:

- 🚀 Daha iyi performans optimizasyonu sağlar
- 🔧 Donanım bağımsız dağıtım sunar
- 🏭 Üretime hazır özellikler içerir
- 📱 Platformlar arası uyumluluk sağlar

### Yöntem 1: Tek Komutla Dönüşüm (Önerilen)

İnce ayar yapılmış modelinizi doğrudan dönüştürmek için aşağıdaki komutu kullanın:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Parametre Açıklaması:**
- `--model_name_or_path`: İnce ayar yapılmış modelinizin yolu
- `--device cpu`: Optimizasyon için CPU kullanımı
- `--precision int4`: INT4 kuantizasyonu kullanımı (yaklaşık %75 boyut azaltma)
- `--output_path`: Dönüştürülmüş modelin çıkış yolu

### Yöntem 2: Yapılandırma Dosyası Yaklaşımı (İleri Düzey Kullanıcılar)

`finetuned_conversion_config.json` adlı bir yapılandırma dosyası oluşturun:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Sonra şu komutu çalıştırın:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Kuantizasyon Seçenekleri Karşılaştırması

| Hassasiyet | Dosya Boyutu | Çıkarım Hızı | Model Kalitesi | Önerilen Kullanım |
|------------|--------------|--------------|----------------|-------------------|
| FP16       | Temel × 0.5  | Hızlı        | En İyi         | Üst düzey donanım |
| INT8       | Temel × 0.25 | Çok Hızlı    | İyi            | Dengeli seçim     |
| INT4       | Temel × 0.125| En Hızlı     | Kabul Edilebilir| Kaynak sınırlı    |

💡 **Öneri**: İlk dağıtımınız için INT4 kuantizasyonuyla başlayın. Kalite tatmin edici değilse, INT8 veya FP16 deneyin.

## Bölüm 3: Foundry Local Dağıtım Yapılandırması

### Model Yapılandırması Oluşturma

Foundry Local modeller dizinine gidin:

```bash
foundry cache cd ./models/
```

Model dizin yapınızı oluşturun:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Model dizininizde `inference_model.json` yapılandırma dosyasını oluşturun:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Model-Specifik Şablon Yapılandırmaları

#### Qwen Serisi Modeller İçin:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Bölüm 4: Model Testi ve Optimizasyonu

### Model Kurulumunu Doğrulama

Foundry Local'ın modelinizi tanıyıp tanımadığını kontrol edin:

```bash
foundry cache ls
```

Listede `your-finetuned-model-int4` görmelisiniz.

### Model Testine Başlama

```bash
foundry model run your-finetuned-model-int4
```

### Performans Ölçümleme

Test sırasında önemli metrikleri izleyin:

1. **Yanıt Süresi**: Ortalama yanıt süresini ölçün
2. **Bellek Kullanımı**: RAM tüketimini izleyin
3. **CPU Kullanımı**: İşlemci yükünü kontrol edin
4. **Çıktı Kalitesi**: Yanıtların alaka düzeyini ve tutarlılığını değerlendirin

### Kalite Doğrulama Kontrol Listesi

- ✅ Model, ince ayar yapılmış alan sorgularına uygun şekilde yanıt veriyor
- ✅ Yanıt formatı beklenen çıktı yapısına uyuyor
- ✅ Uzun süreli kullanımda bellek sızıntısı yok
- ✅ Farklı giriş uzunluklarında tutarlı performans
- ✅ Uç durumlar ve geçersiz girişler düzgün şekilde ele alınıyor

## Özet

Tebrikler! Başarıyla tamamladınız:

- ✅ İnce ayar yapılmış model format dönüşümü
- ✅ Model kuantizasyon optimizasyonu
- ✅ Foundry Local dağıtım yapılandırması
- ✅ Performans ayarı ve sorun giderme

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.