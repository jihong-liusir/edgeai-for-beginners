<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-17T23:51:51+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "tr"
}
-->
# Bölüm 2: Model Damıtma - Teoriden Pratiğe

## İçindekiler
1. [Model Damıtmaya Giriş](../../../Module05)
2. [Damıtmanın Neden Önemli Olduğu](../../../Module05)
3. [Damıtma Süreci](../../../Module05)
4. [Pratik Uygulama](../../../Module05)
5. [Azure ML Damıtma Örneği](../../../Module05)
6. [En İyi Uygulamalar ve Optimizasyon](../../../Module05)
7. [Gerçek Dünya Uygulamaları](../../../Module05)
8. [Sonuç](../../../Module05)

## Model Damıtmaya Giriş {#introduction}

Model damıtma, daha küçük ve daha verimli modeller oluşturmayı sağlayan güçlü bir tekniktir. Bu süreç, kompakt bir "öğrenci" modelin daha büyük ve karmaşık bir "öğretmen" modelin davranışını taklit etmek üzere eğitilmesini içerir.

**Temel Faydalar:**
- **Azaltılmış hesaplama gereksinimleri** ile daha hızlı çıkarım
- **Daha düşük bellek kullanımı** ve depolama ihtiyacı
- **Daha hızlı çıkarım süreleri** ile makul doğruluğun korunması
- **Kaynak kısıtlı ortamlarda maliyet etkin dağıtım**

## Damıtmanın Neden Önemli Olduğu {#why-distillation-matters}

Büyük Dil Modelleri (LLM'ler) giderek daha güçlü hale geliyor, ancak aynı zamanda daha fazla kaynak tüketiyor. Milyarlarca parametreye sahip bir model mükemmel sonuçlar verebilir, ancak aşağıdaki nedenlerden dolayı birçok gerçek dünya uygulaması için pratik olmayabilir:

### Kaynak Kısıtlamaları
- **Hesaplama yükü**: Büyük modeller önemli GPU belleği ve işlem gücü gerektirir
- **Çıkarım gecikmesi**: Karmaşık modeller yanıt üretmek için daha fazla zaman alır
- **Enerji tüketimi**: Daha büyük modeller daha fazla enerji harcar, operasyonel maliyetleri artırır
- **Altyapı maliyetleri**: Büyük modelleri barındırmak pahalı donanım gerektirir

### Pratik Sınırlamalar
- **Mobil dağıtım**: Büyük modeller mobil cihazlarda verimli çalışamaz
- **Gerçek zamanlı uygulamalar**: Düşük gecikme gerektiren uygulamalar yavaş çıkarımı kaldıramaz
- **Edge computing**: IoT ve uç cihazlar sınırlı hesaplama kaynaklarına sahiptir
- **Maliyet faktörleri**: Birçok kuruluş büyük model dağıtımı için gereken altyapıyı karşılayamaz

## Damıtma Süreci {#the-distillation-process}

Model damıtma, öğretmen modelden öğrenci modele bilgi aktarımını içeren iki aşamalı bir süreçtir:

### Aşama 1: Sentetik Veri Üretimi

Öğretmen modeli, eğitim veri kümeniz için yanıtlar üreterek öğretmenin bilgi ve akıl yürütme kalıplarını yakalayan yüksek kaliteli sentetik veri oluşturur.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Bu aşamanın temel unsurları:**
- Öğretmen modeli her bir eğitim örneğini işler
- Üretilen yanıtlar öğrenci eğitimi için "doğru veri" haline gelir
- Bu süreç öğretmenin karar verme kalıplarını yakalar
- Sentetik verinin kalitesi, öğrenci modelinin performansını doğrudan etkiler

### Aşama 2: Öğrenci Modeli İnce Ayarı

Öğrenci modeli, öğretmenin davranışını ve yanıtlarını taklit etmeyi öğrenmek için sentetik veri kümesi üzerinde eğitilir.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Eğitim hedefleri:**
- Öğrenci ve öğretmen çıktıları arasındaki farkı en aza indirmek
- Öğretmenin bilgisini daha küçük bir parametre alanında korumak
- Model karmaşıklığını azaltırken performansı sürdürmek

## Pratik Uygulama {#practical-implementation}

### Öğretmen ve Öğrenci Modellerini Seçmek

**Öğretmen Modeli Seçimi:**
- Belirli görevinizde kanıtlanmış performansa sahip büyük ölçekli LLM'ler (100B+ parametre) seçin
- Popüler öğretmen modelleri şunları içerir:
  - **DeepSeek V3** (671B parametre) - akıl yürütme ve kod üretimi için mükemmel
  - **Meta Llama 3.1 405B Instruct** - kapsamlı genel amaçlı yetenekler
  - **GPT-4** - çeşitli görevlerde güçlü performans
  - **Claude 3.5 Sonnet** - karmaşık akıl yürütme görevleri için mükemmel
- Öğretmen modelinin alanınıza özgü verilerde iyi performans gösterdiğinden emin olun

**Öğrenci Modeli Seçimi:**
- Model boyutu ve performans gereksinimleri arasında denge kurun
- Verimli, daha küçük modellere odaklanın:
  - **Microsoft Phi-4-mini** - güçlü akıl yürütme yetenekleriyle en son verimli model
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K ve 128K varyantları)
  - Microsoft Phi-3.5 Mini Instruct

### Uygulama Adımları

1. **Veri Hazırlığı**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Öğretmen Modeli Kurulumu**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Sentetik Veri Üretimi**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Öğrenci Modeli Eğitimi**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Azure ML Damıtma Örneği {#azure-ml-example}

Azure Machine Learning, model damıtma uygulamak için kapsamlı bir platform sunar. İşte Azure ML'den nasıl yararlanabileceğiniz:

### Ön Koşullar

1. **Azure ML Çalışma Alanı**: Uygun bölgede çalışma alanınızı kurun
   - Büyük ölçekli öğretmen modellerine erişim sağlayın (DeepSeek V3, Llama 405B)
   - Model erişilebilirliğine göre bölgeleri yapılandırın

2. **Hesaplama Kaynakları**: Eğitim için uygun hesaplama örneklerini yapılandırın
   - Öğretmen modeli çıkarımı için yüksek bellekli örnekler
   - Öğrenci modeli ince ayarı için GPU destekli hesaplama

### Desteklenen Görev Türleri

Azure ML, çeşitli görevler için damıtmayı destekler:

- **Doğal Dil Yorumlama (NLI)**
- **Konuşma Tabanlı Yapay Zeka**
- **Soru-Cevap (QA)**
- **Matematiksel akıl yürütme**
- **Metin özetleme**

### Örnek Uygulama

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### İzleme ve Değerlendirme

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## En İyi Uygulamalar ve Optimizasyon {#best-practices}

### Veri Kalitesi

**Yüksek kaliteli eğitim verisi çok önemlidir:**
- Çeşitli ve temsil edici eğitim örnekleri sağlayın
- Mümkün olduğunda alanınıza özgü veriler kullanın
- Öğrenci eğitimi için öğretmen modeli çıktısını kullanmadan önce doğrulayın
- Veri kümesini dengeleyerek öğrenci modelinin öğreniminde önyargıyı önleyin

### Hiperparametre Ayarı

**Optimize edilmesi gereken temel parametreler:**
- **Öğrenme oranı**: İnce ayar için daha küçük oranlarla başlayın (1e-5 ila 5e-5)
- **Toplu işlem boyutu**: Bellek kısıtlamaları ve eğitim kararlılığı arasında denge kurun
- **Epoch sayısı**: Aşırı öğrenmeyi izleyin; genellikle 2-5 epoch yeterlidir
- **Sıcaklık ölçekleme**: Daha iyi bilgi aktarımı için öğretmen çıktısının yumuşaklığını ayarlayın

### Model Mimari Düşünceleri

**Öğretmen-Öğrenci Uyumluluğu:**
- Öğretmen ve öğrenci modelleri arasında mimari uyumluluğu sağlayın
- Daha iyi bilgi aktarımı için ara katman eşleştirmesini düşünün
- Uygun olduğunda dikkat aktarım tekniklerini kullanın

### Değerlendirme Stratejileri

**Kapsamlı değerlendirme yaklaşımı:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Gerçek Dünya Uygulamaları {#real-world-applications}

### Mobil ve Uç Dağıtım

Damıtılmış modeller, kaynak kısıtlı cihazlarda yapay zeka yeteneklerini mümkün kılar:
- **Akıllı telefon uygulamaları** ile gerçek zamanlı metin işleme
- **IoT cihazları** yerel çıkarım yapar
- **Gömülü sistemler** sınırlı hesaplama kaynaklarıyla çalışır

### Maliyet Etkin Üretim Sistemleri

Kuruluşlar, operasyonel maliyetleri azaltmak için damıtmayı kullanır:
- **Müşteri hizmetleri chatbotları** ile daha hızlı yanıt süreleri
- **İçerik denetleme sistemleri** yüksek hacimleri verimli bir şekilde işler
- **Gerçek zamanlı çeviri hizmetleri** ile daha düşük gecikme süreleri

### Alan-Specific Uygulamalar

Damıtma, özel modeller oluşturmayı sağlar:
- **Tıbbi teşhis yardımı** ile gizliliği koruyan yerel çıkarım
- **Hukuki belge analizi** belirli hukuki alanlar için optimize edilmiş
- **Finansal risk değerlendirmesi** ile hızlı karar verme yetenekleri

### Örnek Olay: DeepSeek V3 → Phi-4-mini ile Müşteri Desteği

Bir teknoloji şirketi, müşteri destek sistemi için damıtmayı uyguladı:

**Uygulama Detayları:**
- **Öğretmen Modeli**: DeepSeek V3 (671B parametre) - karmaşık müşteri soruları için mükemmel akıl yürütme
- **Öğrenci Modeli**: Phi-4-mini - hızlı çıkarım ve dağıtım için optimize edilmiş
- **Eğitim Verisi**: 50,000 müşteri destek konuşması
- **Görev**: Teknik sorun çözme ile çok aşamalı konuşma desteği

**Elde Edilen Sonuçlar:**
- **%85 zaman tasarrufu** çıkarım süresinde (3.2s'den 0.48s'ye yanıt başına)
- **%95 bellek gereksinimi azalması** (1.2TB'den 60GB'ye)
- **%92 doğruluk korunması** destek görevlerinde orijinal model doğruluğunun
- **%60 operasyonel maliyet azalması**
- **Geliştirilmiş ölçeklenebilirlik** - artık 10 kat daha fazla eşzamanlı kullanıcı desteklenebilir

**Performans Dağılımı:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Sonuç {#conclusion}

Model damıtma, gelişmiş yapay zeka yeteneklerine erişimi demokratikleştiren kritik bir teknik olarak öne çıkıyor. Daha büyük modellerin performansını koruyan daha küçük ve verimli modeller oluşturmayı mümkün kılarak, damıtma pratik yapay zeka dağıtımına yönelik artan ihtiyacı karşılar.

### Temel Çıkarımlar

1. **Damıtma, performans ve pratik kısıtlamalar arasındaki boşluğu kapatır**
2. **İki aşamalı süreç**, öğretmenden öğrenciye etkili bilgi aktarımını sağlar
3. **Azure ML, damıtma iş akışlarını uygulamak için sağlam bir altyapı sunar**
4. **Uygun değerlendirme ve optimizasyon**, başarılı damıtma için gereklidir
5. **Gerçek dünya uygulamaları**, maliyet, hız ve erişilebilirlikte önemli faydalar sağlar

### Gelecek Yönelimler

Alan geliştikçe, şunları bekleyebiliriz:
- **Gelişmiş damıtma teknikleri** ile daha iyi bilgi aktarım yöntemleri
- **Çoklu öğretmen damıtma** ile geliştirilmiş öğrenci model yetenekleri
- **Damıtma sürecinin otomatik optimizasyonu**
- **Farklı mimariler ve alanlar arasında daha geniş model desteği**

Model damıtma, kuruluşların gelişmiş dil modeli yeteneklerinden yararlanmasını sağlarken, pratik dağıtım kısıtlamalarını koruyarak, gelişmiş yapay zeka modellerini çeşitli uygulamalar ve ortamlar için erişilebilir hale getirir.

## ➡️ Sıradaki Adım

- [03: İnce Ayar - Modelleri Belirli Görevler İçin Özelleştirme](./03.SLMOps-Finetuing.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.