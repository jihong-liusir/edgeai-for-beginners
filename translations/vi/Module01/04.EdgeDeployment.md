<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1f5ed7b55962c3dccafd3da6f9ec252",
  "translation_date": "2025-09-18T12:43:02+00:00",
  "source_file": "Module01/04.EdgeDeployment.md",
  "language_code": "vi"
}
-->
# Phần 4: Nền tảng phần cứng triển khai AI tại biên

Triển khai AI tại biên là bước cuối cùng của việc tối ưu hóa mô hình và lựa chọn phần cứng, mang lại khả năng thông minh trực tiếp cho các thiết bị nơi dữ liệu được tạo ra. Phần này khám phá các yếu tố thực tiễn, yêu cầu phần cứng, và lợi ích chiến lược của việc triển khai AI tại biên trên nhiều nền tảng khác nhau, tập trung vào các giải pháp phần cứng hàng đầu từ Intel, Qualcomm, NVIDIA, và Windows AI PCs.

## Tài nguyên dành cho nhà phát triển

### Tài liệu và tài nguyên học tập
- [Microsoft Learn: Phát triển AI tại biên](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)
- [Tài nguyên AI tại biên của Intel](https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/edge-ai.html)
- [Tài nguyên dành cho nhà phát triển AI của Qualcomm](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- [Tài liệu NVIDIA Jetson](https://developer.nvidia.com/embedded/learn/getting-started-jetson)
- [Tài liệu Windows AI](https://learn.microsoft.com/windows/ai/)

### Công cụ và SDK
- [ONNX Runtime](https://onnxruntime.ai/) - Framework suy luận đa nền tảng
- [OpenVINO Toolkit](https://docs.openvino.ai/) - Bộ công cụ tối ưu hóa của Intel
- [TensorRT](https://developer.nvidia.com/tensorrt) - SDK suy luận hiệu suất cao của NVIDIA
- [DirectML](https://learn.microsoft.com/windows/ai/directml/dml-intro) - API ML tăng tốc phần cứng của Microsoft

## Giới thiệu

Trong phần này, chúng ta sẽ khám phá các khía cạnh thực tiễn của việc triển khai mô hình AI lên các thiết bị tại biên. Chúng ta sẽ đề cập đến các yếu tố cần thiết để triển khai thành công, lựa chọn nền tảng phần cứng, và các chiến lược tối ưu hóa phù hợp với các kịch bản tính toán tại biên khác nhau.

## Mục tiêu học tập

Kết thúc phần này, bạn sẽ có thể:

- Hiểu các yếu tố chính để triển khai AI tại biên thành công
- Xác định nền tảng phần cứng phù hợp cho các khối lượng công việc AI tại biên khác nhau
- Nhận biết các đánh đổi giữa các giải pháp phần cứng AI tại biên
- Áp dụng các kỹ thuật tối ưu hóa phù hợp với các nền tảng phần cứng AI tại biên khác nhau

## Các yếu tố cần cân nhắc khi triển khai AI tại biên

Triển khai AI lên các thiết bị tại biên mang lại những thách thức và yêu cầu đặc thù so với triển khai trên đám mây. Việc triển khai AI tại biên thành công đòi hỏi phải cân nhắc kỹ lưỡng các yếu tố sau:

### Hạn chế về tài nguyên phần cứng

Các thiết bị tại biên thường có tài nguyên tính toán hạn chế so với hạ tầng đám mây:

- **Hạn chế về bộ nhớ**: Nhiều thiết bị tại biên chỉ có RAM từ vài MB đến vài GB
- **Hạn chế về lưu trữ**: Dung lượng lưu trữ hạn chế ảnh hưởng đến kích thước mô hình và quản lý dữ liệu
- **Sức mạnh xử lý**: Khả năng CPU/GPU/NPU bị giới hạn ảnh hưởng đến tốc độ suy luận
- **Tiêu thụ năng lượng**: Nhiều thiết bị tại biên hoạt động bằng pin hoặc có giới hạn nhiệt

### Yếu tố kết nối

AI tại biên phải hoạt động hiệu quả với kết nối không ổn định:

- **Kết nối gián đoạn**: Hoạt động phải tiếp tục khi mạng bị gián đoạn
- **Hạn chế băng thông**: Khả năng truyền dữ liệu thấp hơn so với trung tâm dữ liệu
- **Yêu cầu độ trễ**: Nhiều ứng dụng yêu cầu xử lý thời gian thực hoặc gần thời gian thực
- **Đồng bộ hóa dữ liệu**: Quản lý xử lý cục bộ với việc đồng bộ hóa định kỳ với đám mây

### Yêu cầu về bảo mật và quyền riêng tư

AI tại biên mang lại những thách thức bảo mật đặc thù:

- **Bảo mật vật lý**: Thiết bị có thể được triển khai ở các vị trí dễ tiếp cận
- **Bảo vệ dữ liệu**: Xử lý dữ liệu nhạy cảm trên các thiết bị có thể dễ bị tổn thương
- **Xác thực**: Kiểm soát truy cập an toàn cho chức năng của thiết bị tại biên
- **Quản lý cập nhật**: Cơ chế an toàn để cập nhật mô hình và phần mềm

### Triển khai và quản lý

Các yếu tố triển khai thực tiễn bao gồm:

- **Quản lý đội thiết bị**: Nhiều triển khai tại biên liên quan đến các thiết bị phân tán
- **Kiểm soát phiên bản**: Quản lý phiên bản mô hình trên các thiết bị phân tán
- **Giám sát**: Theo dõi hiệu suất và phát hiện bất thường tại biên
- **Quản lý vòng đời**: Từ triển khai ban đầu đến cập nhật và ngừng hoạt động

## Các tùy chọn nền tảng phần cứng cho AI tại biên

### Giải pháp AI tại biên của Intel

Intel cung cấp nhiều nền tảng phần cứng được tối ưu hóa cho triển khai AI tại biên:

#### Intel NUC

Intel NUC (Next Unit of Computing) mang lại hiệu suất cấp máy tính để bàn trong một thiết kế nhỏ gọn:

- **Bộ xử lý Intel Core** với đồ họa tích hợp Iris Xe
- **RAM**: Hỗ trợ lên đến 64GB DDR4
- **Tương thích với Neural Compute Stick 2** để tăng tốc AI bổ sung
- **Phù hợp nhất cho**: Khối lượng công việc AI tại biên từ trung bình đến phức tạp ở các vị trí cố định với nguồn điện sẵn có

[Intel NUC cho AI tại biên](https://www.intel.com/content/www/us/en/products/details/nuc.html)

#### Intel Movidius Vision Processing Units (VPUs)

Phần cứng chuyên dụng cho thị giác máy tính và tăng tốc mạng nơ-ron:

- **Tiêu thụ năng lượng cực thấp** (1-3W điển hình)
- **Tăng tốc mạng nơ-ron chuyên dụng**
- **Thiết kế nhỏ gọn** để tích hợp vào camera và cảm biến
- **Phù hợp nhất cho**: Ứng dụng thị giác máy tính với yêu cầu nghiêm ngặt về năng lượng

[Intel Movidius VPU](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html)

#### Intel Neural Compute Stick 2

Bộ tăng tốc mạng nơ-ron cắm và chạy qua USB:

- **Intel Movidius Myriad X VPU**
- **Hiệu suất lên đến 4 TOPS**
- **Giao diện USB 3.0** để tích hợp dễ dàng
- **Phù hợp nhất cho**: Tạo mẫu nhanh và thêm khả năng AI vào các hệ thống hiện có

[Intel Neural Compute Stick 2](https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html)

#### Phương pháp phát triển

Intel cung cấp bộ công cụ OpenVINO để tối ưu hóa và triển khai mô hình:

```python
# Example: Using OpenVINO for edge deployment
from openvino.inference_engine import IECore

# Initialize the Inference Engine
ie = IECore()

# Read the network from IR files
net = ie.read_network(model="optimized_model.xml", weights="optimized_model.bin")

# Prepare input and output blobs
input_blob = next(iter(net.input_info))
output_blob = next(iter(net.outputs))

# Load the network to the device (CPU, GPU, MYRIAD, etc.)
exec_net = ie.load_network(network=net, device_name="CPU")

# Prepare input
input_data = preprocess_image("sample.jpg")

# Run inference
result = exec_net.infer(inputs={input_blob: input_data})

# Process output
output = result[output_blob]
```

### Giải pháp AI của Qualcomm

Các nền tảng của Qualcomm tập trung vào ứng dụng di động và nhúng:

#### Qualcomm Snapdragon

Hệ thống trên chip (SoCs) Snapdragon tích hợp:

- **Qualcomm AI Engine** với Hexagon DSP
- **Adreno GPU** cho đồ họa và tính toán song song
- **Lõi CPU Kryo** cho xử lý chung
- **Phù hợp nhất cho**: Điện thoại thông minh, máy tính bảng, tai nghe XR, và camera thông minh

[Qualcomm Snapdragon cho AI tại biên](https://www.qualcomm.com/products/mobile/snapdragon/pcs/product-list)

#### Qualcomm Cloud AI 100

Bộ tăng tốc suy luận AI chuyên dụng tại biên:

- **Hiệu suất AI lên đến 400 TOPS**
- **Hiệu quả năng lượng** được tối ưu hóa cho trung tâm dữ liệu và triển khai tại biên
- **Kiến trúc có thể mở rộng** cho các kịch bản triển khai khác nhau
- **Phù hợp nhất cho**: Ứng dụng AI tại biên có thông lượng cao trong môi trường kiểm soát

[Qualcomm Cloud AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence)

#### Qualcomm RB5/RB6 Robotics Platform

Được thiết kế đặc biệt cho robot và tính toán tại biên tiên tiến:

- **Kết nối 5G tích hợp**
- **Khả năng AI và thị giác máy tính tiên tiến**
- **Hỗ trợ cảm biến toàn diện**
- **Phù hợp nhất cho**: Robot tự động, máy bay không người lái, và hệ thống công nghiệp thông minh

[Nền tảng Robotics của Qualcomm](https://www.qualcomm.com/products/application/robotics/qualcomm-robotics-rb6-platform)

#### Phương pháp phát triển

Qualcomm cung cấp Neural Processing SDK và AI Model Efficiency Toolkit:

```python
# Example: Using Qualcomm Neural Processing SDK
from qti.aisw.dlc_utils import modeltools

# Convert your model to DLC format
converter = modeltools.DlcConverter()
converter.convert(
    input_network="model.tflite",
    input_dim=[1, 224, 224, 3],
    output_path="optimized_model.dlc"
)

# Use SNPE runtime for inference
from qti.aisw.snpe_runtime import SnpeRuntime

# Initialize runtime
runtime = SnpeRuntime()
runtime.load("optimized_model.dlc")

# Prepare input
input_tensor = preprocess_image("sample.jpg")

# Run inference
outputs = runtime.execute(input_tensor)

# Process results
predictions = postprocess_output(outputs)
```

### 🎮 Giải pháp AI tại biên của NVIDIA

NVIDIA cung cấp các nền tảng tăng tốc GPU mạnh mẽ cho triển khai tại biên:

#### Dòng NVIDIA Jetson

Các nền tảng tính toán AI tại biên được thiết kế đặc biệt:

##### Dòng Jetson Orin
- **Hiệu suất AI lên đến 275 TOPS**
- **GPU kiến trúc NVIDIA Ampere**
- **Cấu hình năng lượng** từ 5W đến 60W
- **Phù hợp nhất cho**: Robot tiên tiến, phân tích video thông minh, và thiết bị y tế

##### Jetson Nano
- **Tính toán AI cấp nhập môn** (472 GFLOPS)
- **GPU Maxwell 128 lõi**
- **Tiết kiệm năng lượng** (5-10W)
- **Phù hợp nhất cho**: Dự án sở thích, ứng dụng giáo dục, và triển khai AI đơn giản

[Nền tảng NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)

#### NVIDIA Clara Guardian

Nền tảng cho ứng dụng AI trong chăm sóc sức khỏe:

- **Cảm biến thời gian thực** để giám sát bệnh nhân
- **Dựa trên Jetson** hoặc máy chủ tăng tốc GPU
- **Tối ưu hóa dành riêng cho chăm sóc sức khỏe**
- **Phù hợp nhất cho**: Bệnh viện thông minh, giám sát bệnh nhân, và hình ảnh y tế

[NVIDIA Clara Guardian](https://developer.nvidia.com/clara)

#### Nền tảng NVIDIA EGX

Giải pháp tính toán tại biên cấp doanh nghiệp:

- **Có thể mở rộng từ GPU NVIDIA A100 đến T4**
- **Giải pháp máy chủ được chứng nhận** từ các đối tác OEM
- **Bao gồm bộ phần mềm NVIDIA AI Enterprise**
- **Phù hợp nhất cho**: Triển khai AI tại biên quy mô lớn trong môi trường công nghiệp và doanh nghiệp

[Nền tảng NVIDIA EGX](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)

#### Phương pháp phát triển

NVIDIA cung cấp TensorRT để triển khai mô hình tối ưu hóa:

```python
# Example: Using TensorRT for optimized inference
import tensorrt as trt
import numpy as np

# Create builder and network
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

# Parse ONNX model
parser = trt.OnnxParser(network, logger)
with open("model.onnx", "rb") as f:
    parser.parse(f.read())

# Create optimization config
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB
config.set_flag(trt.BuilderFlag.FP16)

# Build and serialize engine
engine = builder.build_engine(network, config)
with open("model.trt", "wb") as f:
    f.write(engine.serialize())

# Create runtime and execute
runtime = trt.Runtime(logger)
engine = runtime.deserialize_cuda_engine(open("model.trt", "rb").read())
context = engine.create_execution_context()

# Run inference
input_data = preprocess_image("sample.jpg")
output = run_inference(context, input_data, engine)
```

### Windows AI PCs

Windows AI PCs đại diện cho danh mục phần cứng AI tại biên mới nhất, với các Neural Processing Units (NPUs) chuyên dụng:

#### Qualcomm Snapdragon X Elite/Plus

Thế hệ đầu tiên của Windows Copilot+ PCs có:

- **Hexagon NPU** với hiệu suất AI hơn 45 TOPS
- **CPU Qualcomm Oryon** với tối đa 12 lõi
- **GPU Adreno** cho đồ họa và tăng tốc AI bổ sung
- **Phù hợp nhất cho**: Năng suất tăng cường AI, sáng tạo nội dung, và phát triển phần mềm

[Qualcomm Snapdragon X Elite](https://www.qualcomm.com/snapdragon/laptops)

#### Intel Core Ultra (Meteor Lake và các thế hệ sau)

Bộ xử lý AI PC của Intel có:

- **Intel AI Boost (NPU)** cung cấp hiệu suất lên đến 10 TOPS
- **GPU Intel Arc** mang lại khả năng tăng tốc AI bổ sung
- **Lõi CPU hiệu suất và hiệu quả**
- **Phù hợp nhất cho**: Máy tính xách tay doanh nghiệp, máy trạm sáng tạo, và tính toán AI hàng ngày

[Bộ xử lý Intel Core Ultra](https://www.intel.com/content/www/us/en/products/details/processors/core/ultra.html)

#### AMD Ryzen AI Series

Các bộ xử lý tập trung vào AI của AMD bao gồm:

- **NPU dựa trên XDNA** cung cấp hiệu suất lên đến 16 TOPS
- **Lõi CPU Zen 4** cho xử lý chung
- **Đồ họa RDNA 3** cho khả năng tính toán bổ sung
- **Phù hợp nhất cho**: Chuyên gia sáng tạo, nhà phát triển, và người dùng cao cấp

[Bộ xử lý AMD Ryzen AI](https://www.amd.com/en/processors/ryzen-ai.html)

#### Phương pháp phát triển

Windows AI PCs tận dụng Windows Developer Platform và DirectML:

```csharp
// Example: Using Windows App SDK and DirectML
using Microsoft.AI.DirectML;
using Microsoft.Windows.AI;

// Load model
var modelPath = "optimized_model.onnx";
var modelOptions = new OnnxModelOptions
{
    InterOpNumThreads = 4,
    IntraOpNumThreads = 4
};

// Create model
var model = await OnnxModel.CreateFromFileAsync(modelPath, modelOptions);

// Prepare input
var inputFeatures = new List<InputFeature>
{
    new InputFeature
    {
        Name = "input",
        Value = imageData
    }
};

// Run inference
var results = await model.EvaluateAsync(inputFeatures);

// Process output
var output = results.First().Value;
```

## ⚡ Kỹ thuật tối ưu hóa phần cứng cụ thể

### 🔍 Phương pháp lượng hóa

Các nền tảng phần cứng khác nhau hưởng lợi từ các kỹ thuật lượng hóa cụ thể:

#### Tối ưu hóa OpenVINO của Intel
- **Lượng hóa INT8** cho CPU và GPU tích hợp
- **Độ chính xác FP16** để cải thiện hiệu suất với tổn thất độ chính xác tối thiểu
- **Lượng hóa không đối xứng** để xử lý phân phối kích hoạt

#### Tối ưu hóa Qualcomm AI Engine
- **Lượng hóa UINT8** cho Hexagon DSP
- **Độ chính xác hỗn hợp** tận dụng tất cả các đơn vị tính toán có sẵn
- **Lượng hóa theo kênh** để cải thiện độ chính xác

#### Tối ưu hóa TensorRT của NVIDIA
- **Độ chính xác INT8 và FP16** cho tăng tốc GPU
- **Hợp nhất lớp** để giảm chuyển giao bộ nhớ
- **Tự động điều chỉnh kernel** cho các kiến trúc GPU cụ thể

#### Tối ưu hóa NPU của Windows
- **Lượng hóa INT8/INT4** cho thực thi NPU
- **Tối ưu hóa đồ thị DirectML**
- **Tăng tốc runtime Windows ML**

### Thích ứng theo kiến trúc

Phần cứng khác nhau yêu cầu các cân nhắc kiến trúc cụ thể:

- **Intel**: Tối ưu hóa cho hướng vector AVX-512 và Intel Deep Learning Boost
- **Qualcomm**: Tận dụng tính toán dị thể trên Hexagon DSP, Adreno GPU, và Kryo CPU
- **NVIDIA**: Tối đa hóa tính song song GPU và sử dụng lõi CUDA
- **Windows NPU**: Thiết kế cho xử lý hợp tác giữa NPU-CPU-GPU

### Chiến lược quản lý bộ nhớ

Xử lý bộ nhớ hiệu quả thay đổi theo nền tảng:

- **Intel**: Tối ưu hóa cho sử dụng bộ nhớ đệm và mẫu truy cập bộ nhớ
- **Qualcomm**: Quản lý bộ nhớ chia sẻ trên các bộ xử lý dị thể
- **NVIDIA**: Sử dụng bộ nhớ hợp nhất CUDA và tối ưu hóa sử dụng VRAM
- **Windows NPU**: Cân bằng khối lượng công việc giữa bộ nhớ NPU chuyên dụng và RAM hệ thống

## Đánh giá hiệu suất và các chỉ số

Khi đánh giá triển khai AI tại biên, hãy xem xét các chỉ số chính sau:

### Chỉ số hiệu suất

- **Thời gian suy luận**: Milliseconds mỗi lần suy luận (càng thấp càng tốt)
- **Thông lượng**: Suy luận mỗi giây (càng cao càng tốt)
- **Độ trễ**: Thời gian phản hồi từ đầu đến cuối (càng thấp càng tốt)
- **FPS**: Khung hình mỗi giây cho ứng dụng thị giác (càng cao càng tốt)

### Chỉ số hiệu quả

- **Hiệu suất trên mỗi watt**: TOPS/W hoặc suy luận/giây/watt
- **Năng lượng mỗi lần suy luận**: Joules tiêu thụ mỗi lần suy luận
- **Ảnh hưởng đến pin**: Giảm thời gian hoạt động khi chạy khối lượng công việc AI
- **Hiệu quả nhiệt**: Tăng nhiệt độ trong quá trình hoạt động liên tục

### Chỉ số độ chính xác

- **Độ chính xác Top-1/Top-5**: Tỷ lệ đúng của phân loại
- **mAP**: Độ chính xác trung bình cho phát hiện đối tượng
- **Điểm F1**: Cân bằng giữa độ chính xác và độ hồi tưởng
- **Ảnh hưởng của lượng hóa**: Sự khác biệt độ chính xác giữa mô hình đầy đủ và mô hình lượng hóa

## Mô hình triển khai và thực tiễn tốt nhất

### Chiến lược triển khai doanh nghiệp

- **Container hóa**: Sử dụng Docker hoặc tương tự để triển khai nhất quán
- **Quản lý đội thiết bị**: Các giải pháp như Azure IoT Edge để quản lý thiết bị
- **Giám sát**: Thu thập dữ liệu từ xa và theo dõi hiệu suất
- **Quản lý cập nhật**: Cơ chế cập nhật OTA cho mô hình và phần mềm

### Mô hình kết hợp giữa đám mây và thiết bị biên

- **Huấn luyện trên đám mây, suy luận trên thiết bị biên**: Huấn luyện trên đám mây, triển khai trên thiết bị biên
- **Xử lý sơ bộ trên thiết bị biên, phân tích trên đám mây**: Xử lý cơ bản trên thiết bị biên, phân tích phức tạp trên đám mây
- **Học liên kết**: Cải thiện mô hình phân tán mà không cần tập trung dữ liệu
- **Học gia tăng**: Cải thiện mô hình liên tục từ dữ liệu thiết bị biên

### Mô hình tích hợp

- **Tích hợp cảm biến**: Kết nối trực tiếp với camera, micro và các cảm biến khác
- **Điều khiển bộ truyền động**: Điều khiển thời gian thực các động cơ, màn hình và các đầu ra khác
- **Tích hợp hệ thống**: Giao tiếp với các hệ thống doanh nghiệp hiện có
- **Tích hợp IoT**: Kết nối với hệ sinh thái IoT rộng hơn

## Các yếu tố triển khai theo ngành

### Y tế

- **Bảo mật thông tin bệnh nhân**: Tuân thủ HIPAA đối với dữ liệu y tế
- **Quy định về thiết bị y tế**: Yêu cầu của FDA và các cơ quan quản lý khác
- **Yêu cầu về độ tin cậy**: Khả năng chịu lỗi cho các ứng dụng quan trọng
- **Tiêu chuẩn tích hợp**: FHIR, HL7 và các tiêu chuẩn tương tác trong ngành y tế

### Sản xuất

- **Môi trường công nghiệp**: Chống chịu trong điều kiện khắc nghiệt
- **Yêu cầu thời gian thực**: Hiệu suất xác định cho các hệ thống điều khiển
- **Hệ thống an toàn**: Tích hợp với các giao thức an toàn công nghiệp
- **Tích hợp hệ thống cũ**: Kết nối với cơ sở hạ tầng OT hiện có

### Ô tô

- **An toàn chức năng**: Tuân thủ ISO 26262
- **Chống chịu môi trường**: Hoạt động trong các điều kiện nhiệt độ khắc nghiệt
- **Quản lý năng lượng**: Hoạt động tiết kiệm pin
- **Quản lý vòng đời**: Hỗ trợ dài hạn cho tuổi thọ của xe

### Thành phố thông minh

- **Triển khai ngoài trời**: Chống chịu thời tiết và bảo mật vật lý
- **Quản lý quy mô**: Từ hàng nghìn đến hàng triệu thiết bị phân tán
- **Biến đổi mạng**: Hoạt động với kết nối không ổn định
- **Cân nhắc về quyền riêng tư**: Xử lý dữ liệu không gian công cộng một cách có trách nhiệm

## Xu hướng tương lai trong phần cứng AI thiết bị biên

### Phát triển phần cứng mới nổi

- **Silicon chuyên dụng cho AI**: Các NPU và bộ tăng tốc AI chuyên biệt hơn
- **Tính toán thần kinh**: Kiến trúc lấy cảm hứng từ não bộ để cải thiện hiệu suất
- **Tính toán trong bộ nhớ**: Giảm di chuyển dữ liệu cho các hoạt động AI
- **Đóng gói đa chip**: Tích hợp không đồng nhất các bộ xử lý AI chuyên biệt

### Đồng tiến hóa phần mềm và phần cứng

- **Tìm kiếm kiến trúc thần kinh dựa trên phần cứng**: Mô hình tối ưu hóa cho phần cứng cụ thể
- **Cải tiến trình biên dịch**: Dịch mô hình sang hướng dẫn phần cứng tốt hơn
- **Tối ưu hóa đồ thị chuyên biệt**: Chuyển đổi mạng phù hợp với phần cứng
- **Thích ứng động**: Tối ưu hóa thời gian chạy dựa trên tài nguyên sẵn có

### Nỗ lực tiêu chuẩn hóa

- **ONNX và ONNX Runtime**: Khả năng tương tác mô hình đa nền tảng
- **MLIR**: Biểu diễn trung gian đa cấp cho ML
- **OpenXLA**: Biên dịch đại số tuyến tính tăng tốc
- **TMUL**: Các lớp trừu tượng bộ xử lý tensor

## Bắt đầu triển khai AI thiết bị biên

### Thiết lập môi trường phát triển

1. **Chọn phần cứng mục tiêu**: Chọn nền tảng phù hợp với trường hợp sử dụng của bạn
2. **Cài đặt SDK và công cụ**: Thiết lập bộ công cụ phát triển của nhà sản xuất
3. **Cấu hình công cụ tối ưu hóa**: Cài đặt phần mềm lượng tử hóa và biên dịch
4. **Thiết lập quy trình CI/CD**: Xây dựng quy trình kiểm thử và triển khai tự động

### Danh sách kiểm tra triển khai

- **Tối ưu hóa mô hình**: Lượng tử hóa, cắt tỉa và tối ưu hóa kiến trúc
- **Kiểm tra hiệu suất**: Đo hiệu suất trên phần cứng mục tiêu trong điều kiện thực tế
- **Phân tích năng lượng**: Đo lường các mẫu tiêu thụ năng lượng
- **Kiểm tra bảo mật**: Xác minh bảo vệ dữ liệu và kiểm soát truy cập
- **Cơ chế cập nhật**: Triển khai khả năng cập nhật an toàn
- **Thiết lập giám sát**: Triển khai thu thập dữ liệu và cảnh báo từ xa

## ➡️ Tiếp theo

- Xem lại [Tổng quan Module 1](./README.md)
- Khám phá [Module 2: Nền tảng mô hình ngôn ngữ nhỏ](../Module02/README.md)
- Tiếp tục với [Module 3: Chiến lược triển khai SLM](../Module03/README.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.