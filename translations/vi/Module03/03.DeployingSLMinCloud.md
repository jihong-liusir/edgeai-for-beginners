<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T13:14:04+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "vi"
}
-->
# Triển khai Đám mây với Container - Giải pháp Quy mô Sản xuất

Hướng dẫn toàn diện này bao gồm ba phương pháp chính để triển khai mô hình Phi-4-mini-instruct của Microsoft trong môi trường container: vLLM, Ollama, và SLM Engine với ONNX Runtime. Mô hình 3.8 tỷ tham số này là lựa chọn tối ưu cho các nhiệm vụ suy luận, đồng thời duy trì hiệu quả cho triển khai tại thiết bị biên.

## Mục lục

1. [Giới thiệu về Triển khai Container Phi-4-mini](../../../Module03)
2. [Mục tiêu học tập](../../../Module03)
3. [Hiểu về Phân loại Phi-4-mini](../../../Module03)
4. [Triển khai Container vLLM](../../../Module03)
5. [Triển khai Container Ollama](../../../Module03)
6. [SLM Engine với ONNX Runtime](../../../Module03)
7. [Khung So sánh](../../../Module03)
8. [Thực hành tốt nhất](../../../Module03)

## Giới thiệu về Triển khai Container Phi-4-mini

Các Mô hình Ngôn ngữ Nhỏ (SLM) là một bước tiến quan trọng trong EdgeAI, cho phép các khả năng xử lý ngôn ngữ tự nhiên tiên tiến trên các thiết bị có tài nguyên hạn chế. Hướng dẫn này tập trung vào các chiến lược triển khai container cho Phi-4-mini-instruct của Microsoft, một mô hình suy luận tiên tiến cân bằng giữa khả năng và hiệu quả.

### Mô hình nổi bật: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 tỷ tham số)**: Mô hình mới nhất của Microsoft được tinh chỉnh theo hướng dẫn, thiết kế cho môi trường hạn chế về bộ nhớ/tính toán với các khả năng vượt trội trong:
- **Suy luận toán học và tính toán phức tạp**
- **Tạo mã, gỡ lỗi và phân tích**
- **Giải quyết vấn đề logic và suy luận từng bước**
- **Ứng dụng giáo dục yêu cầu giải thích chi tiết**
- **Gọi hàm và tích hợp công cụ**

Thuộc danh mục "SLM nhỏ" (1.5B - 13.9B tham số), Phi-4-mini đạt được sự cân bằng tối ưu giữa khả năng suy luận và hiệu quả tài nguyên.

### Lợi ích của Triển khai Container Phi-4-mini

- **Hiệu quả vận hành**: Suy luận nhanh cho các nhiệm vụ với yêu cầu tính toán thấp hơn
- **Linh hoạt triển khai**: Khả năng AI trên thiết bị với quyền riêng tư được nâng cao thông qua xử lý cục bộ
- **Hiệu quả chi phí**: Giảm chi phí vận hành so với các mô hình lớn hơn trong khi vẫn duy trì chất lượng
- **Cách ly**: Phân tách rõ ràng giữa các phiên bản mô hình và môi trường thực thi an toàn
- **Khả năng mở rộng**: Dễ dàng mở rộng ngang để tăng thông lượng suy luận

## Mục tiêu học tập

Sau khi hoàn thành hướng dẫn này, bạn sẽ có thể:

- Triển khai và tối ưu hóa Phi-4-mini-instruct trong các môi trường container khác nhau
- Thực hiện các chiến lược lượng hóa và nén tiên tiến cho các kịch bản triển khai khác nhau
- Cấu hình điều phối container sẵn sàng sản xuất cho các khối lượng công việc suy luận
- Đánh giá và chọn khung triển khai phù hợp dựa trên yêu cầu trường hợp sử dụng cụ thể
- Áp dụng các thực hành tốt nhất về bảo mật, giám sát và mở rộng cho triển khai SLM trong container

## Hiểu về Phân loại Phi-4-mini

### Thông số kỹ thuật của mô hình

**Chi tiết kỹ thuật:**
- **Tham số**: 3.8 tỷ (Danh mục SLM nhỏ)
- **Kiến trúc**: Transformer giải mã dày đặc với chú ý nhóm truy vấn
- **Độ dài ngữ cảnh**: 128K token (khuyến nghị 32K để đạt hiệu suất tối ưu)
- **Từ vựng**: 200K token với hỗ trợ đa ngôn ngữ
- **Dữ liệu huấn luyện**: 5T token nội dung chất lượng cao, giàu suy luận

### Yêu cầu tài nguyên

| Loại triển khai | RAM tối thiểu | RAM khuyến nghị | VRAM (GPU) | Lưu trữ | Trường hợp sử dụng điển hình |
|-----------------|---------------|-----------------|------------|---------|-----------------------------|
| **Phát triển** | 6GB | 8GB | - | 8GB | Kiểm thử cục bộ, tạo mẫu |
| **Sản xuất CPU** | 8GB | 12GB | - | 10GB | Máy chủ biên, triển khai tối ưu chi phí |
| **Sản xuất GPU** | 6GB | 8GB | 4-6GB | 8GB | Dịch vụ suy luận thông lượng cao |
| **Tối ưu hóa biên** | 4GB | 6GB | - | 6GB | Triển khai lượng hóa, cổng IoT |

### Khả năng của Phi-4-mini

- **Xuất sắc về toán học**: Giải quyết các vấn đề về số học, đại số và giải tích tiên tiến
- **Thông minh về mã**: Tạo mã Python, JavaScript và đa ngôn ngữ với khả năng gỡ lỗi
- **Suy luận logic**: Phân tích vấn đề từng bước và xây dựng giải pháp
- **Hỗ trợ giáo dục**: Giải thích chi tiết phù hợp cho học tập và giảng dạy
- **Gọi hàm**: Hỗ trợ tích hợp công cụ và tương tác API một cách tự nhiên

## Triển khai Container vLLM

vLLM cung cấp hỗ trợ tuyệt vời cho Phi-4-mini-instruct với hiệu suất suy luận tối ưu và API tương thích với OpenAI, lý tưởng cho các dịch vụ suy luận sản xuất.

### Ví dụ khởi động nhanh

#### Triển khai CPU cơ bản (Phát triển)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Triển khai sản xuất tăng tốc GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Cấu hình sản xuất

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Kiểm thử khả năng suy luận của Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Triển khai Container Ollama

Ollama cung cấp hỗ trợ tuyệt vời cho Phi-4-mini-instruct với triển khai và quản lý đơn giản, lý tưởng cho phát triển và triển khai sản xuất cân bằng.

### Thiết lập nhanh

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Cấu hình sản xuất

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Tối ưu hóa mô hình và biến thể

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Ví dụ sử dụng API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine với ONNX Runtime

ONNX Runtime cung cấp hiệu suất tối ưu cho triển khai biên của Phi-4-mini-instruct với tối ưu hóa tiên tiến và khả năng tương thích đa nền tảng.

### Thiết lập cơ bản

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Triển khai máy chủ đơn giản hóa

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script chuyển đổi mô hình

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Cấu hình sản xuất

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Kiểm thử triển khai ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Khung So sánh

### So sánh khung cho Phi-4-mini

| Tính năng | vLLM | Ollama | ONNX Runtime |
|-----------|------|--------|--------------|
| **Độ phức tạp thiết lập** | Trung bình | Dễ | Phức tạp |
| **Hiệu suất (GPU)** | Xuất sắc (~25 tok/s) | Rất tốt (~20 tok/s) | Tốt (~15 tok/s) |
| **Hiệu suất (CPU)** | Tốt (~8 tok/s) | Rất tốt (~12 tok/s) | Xuất sắc (~15 tok/s) |
| **Sử dụng bộ nhớ** | 8-12GB | 6-10GB | 4-8GB |
| **Tương thích API** | Tương thích OpenAI | REST tùy chỉnh | FastAPI tùy chỉnh |
| **Gọi hàm** | ✅ Tự nhiên | ✅ Hỗ trợ | ⚠️ Triển khai tùy chỉnh |
| **Hỗ trợ lượng hóa** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Lượng hóa ONNX |
| **Sẵn sàng sản xuất** | ✅ Xuất sắc | ✅ Rất tốt | ✅ Tốt |
| **Triển khai biên** | Tốt | Xuất sắc | Nổi bật |

## Tài nguyên bổ sung

### Tài liệu chính thức
- **Phi-4 Model Card của Microsoft**: Thông số chi tiết và hướng dẫn sử dụng
- **Tài liệu vLLM**: Tùy chọn cấu hình và tối ưu hóa nâng cao
- **Thư viện mô hình Ollama**: Mô hình cộng đồng và ví dụ tùy chỉnh
- **Hướng dẫn ONNX Runtime**: Chiến lược tối ưu hóa hiệu suất và triển khai

### Công cụ phát triển
- **Hugging Face Transformers**: Tương tác và tùy chỉnh mô hình
- **Đặc tả API OpenAI**: Kiểm thử khả năng tương thích vLLM
- **Thực hành tốt nhất về Docker**: Bảo mật và tối ưu hóa container
- **Triển khai Kubernetes**: Mẫu điều phối cho mở rộng sản xuất

### Tài nguyên học tập
- **Đánh giá hiệu suất SLM**: Phương pháp phân tích so sánh
- **Triển khai Edge AI**: Thực hành tốt nhất cho môi trường hạn chế tài nguyên
- **Tối ưu hóa nhiệm vụ suy luận**: Chiến lược nhắc nhở cho các vấn đề toán học và logic
- **Bảo mật container**: Thực hành tăng cường cho triển khai mô hình AI

## Kết quả học tập

Sau khi hoàn thành module này, bạn sẽ có thể:

1. Triển khai mô hình Phi-4-mini-instruct trong môi trường container bằng nhiều khung khác nhau
2. Cấu hình và tối ưu hóa triển khai SLM cho các môi trường phần cứng khác nhau
3. Thực hiện các thực hành tốt nhất về bảo mật cho triển khai AI trong container
4. So sánh và chọn khung triển khai phù hợp dựa trên yêu cầu trường hợp sử dụng cụ thể
5. Áp dụng chiến lược giám sát và mở rộng cho dịch vụ SLM cấp sản xuất

## Tiếp theo

- Quay lại [Module 1](../Module01/README.md)
- Quay lại [Module 2](../Module02/README.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.