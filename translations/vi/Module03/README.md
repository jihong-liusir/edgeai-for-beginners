<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-18T13:06:44+00:00",
  "source_file": "Module03/README.md",
  "language_code": "vi"
}
-->
# Chương 03: Triển khai Mô hình Ngôn ngữ Nhỏ (SLMs)

Chương này cung cấp một cái nhìn toàn diện về toàn bộ vòng đời triển khai Mô hình Ngôn ngữ Nhỏ (SLMs), bao gồm các nền tảng lý thuyết, chiến lược triển khai thực tiễn, và các giải pháp container hóa sẵn sàng cho sản xuất. Chương được cấu trúc thành ba phần tiến bộ, dẫn dắt người đọc từ các khái niệm cơ bản đến các kịch bản triển khai nâng cao.

## Cấu trúc Chương và Hành trình Học tập

### **[Phần 1: Học nâng cao về SLM - Nền tảng và Tối ưu hóa](./01.SLMAdvancedLearning.md)**
Phần mở đầu thiết lập nền tảng lý thuyết để hiểu về Mô hình Ngôn ngữ Nhỏ và tầm quan trọng chiến lược của chúng trong các triển khai AI tại biên. Phần này bao gồm:

- **Khung phân loại tham số**: Phân tích chi tiết các loại SLM từ Micro SLMs (100M-1.4B tham số) đến Medium SLMs (14B-30B tham số), tập trung vào các mô hình như Phi-4-mini-3.8B, dòng Qwen3, và Google Gemma3, bao gồm yêu cầu phần cứng và phân tích bộ nhớ cho từng cấp độ mô hình
- **Kỹ thuật tối ưu hóa nâng cao**: Bao quát các phương pháp lượng tử hóa sử dụng các framework như Llama.cpp, Microsoft Olive, và Apple MLX, bao gồm kỹ thuật lượng tử hóa BitNET 1-bit tiên tiến với các ví dụ mã thực tiễn minh họa quy trình lượng tử hóa và kết quả đánh giá hiệu năng
- **Chiến lược thu thập mô hình**: Phân tích sâu về hệ sinh thái Hugging Face và Azure AI Foundry Model Catalog cho triển khai SLM cấp doanh nghiệp, với các ví dụ mã để tải xuống, xác thực và chuyển đổi định dạng mô hình một cách lập trình
- **API dành cho nhà phát triển**: Các ví dụ mã trong Python, C++, và C# minh họa cách tải mô hình, thực hiện suy luận, và tích hợp với các framework phổ biến như PyTorch, TensorFlow, và ONNX Runtime

Phần nền tảng này nhấn mạnh sự cân bằng giữa hiệu quả vận hành, tính linh hoạt trong triển khai, và chi phí hợp lý, làm cho SLM trở thành lựa chọn lý tưởng cho các kịch bản tính toán tại biên, với các ví dụ mã thực tiễn mà nhà phát triển có thể áp dụng trực tiếp vào dự án của mình.

### **[Phần 2: Triển khai trong môi trường cục bộ - Giải pháp ưu tiên quyền riêng tư](./02.DeployingSLMinLocalEnv.md)**
Phần thứ hai chuyển từ lý thuyết sang triển khai thực tiễn, tập trung vào các chiến lược triển khai cục bộ ưu tiên chủ quyền dữ liệu và tính độc lập trong vận hành. Các nội dung chính bao gồm:

- **Nền tảng Ollama Universal**: Khám phá toàn diện về triển khai đa nền tảng với trọng tâm là quy trình làm việc thân thiện với nhà phát triển, quản lý vòng đời mô hình, và tùy chỉnh thông qua Modelfiles, bao gồm các ví dụ tích hợp REST API hoàn chỉnh và script tự động hóa CLI
- **Microsoft Foundry Local**: Các giải pháp triển khai cấp doanh nghiệp với tối ưu hóa dựa trên ONNX, tích hợp Windows ML, và các tính năng bảo mật toàn diện, với các ví dụ mã C# và Python để tích hợp ứng dụng gốc
- **Phân tích so sánh**: So sánh chi tiết các framework bao gồm kiến trúc kỹ thuật, đặc điểm hiệu năng, và hướng dẫn tối ưu hóa cho từng trường hợp sử dụng, với mã benchmark để đánh giá tốc độ suy luận và sử dụng bộ nhớ trên các phần cứng khác nhau
- **Tích hợp API**: Các ứng dụng mẫu minh họa cách xây dựng dịch vụ web, ứng dụng trò chuyện, và các pipeline xử lý dữ liệu sử dụng triển khai SLM cục bộ, với các ví dụ mã trong Node.js, Python Flask/FastAPI, và ASP.NET Core
- **Khung kiểm thử**: Các phương pháp kiểm thử tự động để đảm bảo chất lượng mô hình, bao gồm các ví dụ kiểm thử đơn vị và kiểm thử tích hợp cho các triển khai SLM

Phần này cung cấp hướng dẫn thực tiễn cho các tổ chức muốn triển khai các giải pháp AI bảo vệ quyền riêng tư trong khi vẫn duy trì toàn quyền kiểm soát môi trường triển khai, với các ví dụ mã sẵn sàng sử dụng mà nhà phát triển có thể điều chỉnh theo yêu cầu cụ thể.

### **[Phần 3: Triển khai container hóa trên đám mây - Giải pháp quy mô sản xuất](./03.DeployingSLMinCloud.md)**
Phần cuối cùng tập trung vào các chiến lược triển khai container hóa nâng cao, với Microsoft Phi-4-mini-instruct được chọn làm nghiên cứu điển hình chính. Phần này bao gồm:

- **Triển khai vLLM**: Tối ưu hóa suy luận hiệu năng cao với các API tương thích OpenAI, tăng tốc GPU tiên tiến, và cấu hình sẵn sàng cho sản xuất, bao gồm Dockerfiles hoàn chỉnh, Kubernetes manifests, và các tham số tinh chỉnh hiệu năng
- **Điều phối container Ollama**: Quy trình triển khai đơn giản hóa với Docker Compose, các biến thể tối ưu hóa mô hình, và tích hợp giao diện web, với các ví dụ pipeline CI/CD để tự động hóa triển khai và kiểm thử
- **Triển khai ONNX Runtime**: Triển khai tối ưu hóa cho biên với chuyển đổi mô hình toàn diện, chiến lược lượng tử hóa, và khả năng tương thích đa nền tảng, bao gồm các ví dụ mã chi tiết cho tối ưu hóa và triển khai mô hình
- **Giám sát & Quan sát**: Triển khai bảng điều khiển Prometheus/Grafana với các chỉ số tùy chỉnh để giám sát hiệu năng SLM, bao gồm cấu hình cảnh báo và tổng hợp log
- **Cân bằng tải & Mở rộng quy mô**: Các ví dụ thực tiễn về chiến lược mở rộng ngang và dọc với cấu hình tự động mở rộng dựa trên mức sử dụng CPU/GPU và mẫu yêu cầu
- **Tăng cường bảo mật**: Các thực hành tốt nhất về bảo mật container bao gồm giảm quyền, chính sách mạng, và quản lý bí mật cho khóa API và thông tin xác thực truy cập mô hình

Mỗi phương pháp triển khai được trình bày với các ví dụ cấu hình hoàn chỉnh, quy trình kiểm thử, danh sách kiểm tra sẵn sàng cho sản xuất, và các mẫu hạ tầng dưới dạng mã mà nhà phát triển có thể áp dụng trực tiếp vào quy trình triển khai của mình.

## Kết quả Học tập Chính

Hoàn thành chương này, người đọc sẽ nắm vững:

1. **Lựa chọn mô hình chiến lược**: Hiểu rõ giới hạn tham số và lựa chọn SLM phù hợp dựa trên các ràng buộc tài nguyên và yêu cầu hiệu năng
2. **Thành thạo tối ưu hóa**: Triển khai các kỹ thuật lượng tử hóa nâng cao trên các framework khác nhau để đạt được sự cân bằng tối ưu giữa hiệu năng và hiệu quả
3. **Tính linh hoạt trong triển khai**: Lựa chọn giữa các giải pháp ưu tiên quyền riêng tư cục bộ và triển khai container hóa có khả năng mở rộng dựa trên nhu cầu tổ chức
4. **Sẵn sàng cho sản xuất**: Cấu hình hệ thống giám sát, bảo mật, và mở rộng quy mô cho các triển khai SLM cấp doanh nghiệp

## Tập trung Thực tiễn và Ứng dụng Thực tế

Chương duy trì định hướng thực tiễn mạnh mẽ xuyên suốt, bao gồm:

- **Ví dụ thực hành**: Các tệp cấu hình hoàn chỉnh, quy trình kiểm thử API, và script triển khai
- **Đánh giá hiệu năng**: So sánh chi tiết về tốc độ suy luận, sử dụng bộ nhớ, và yêu cầu tài nguyên
- **Cân nhắc bảo mật**: Các thực hành bảo mật cấp doanh nghiệp, khung tuân thủ, và chiến lược bảo vệ dữ liệu
- **Thực hành tốt nhất**: Các hướng dẫn đã được chứng minh trong sản xuất về giám sát, mở rộng quy mô, và bảo trì

## Góc nhìn Sẵn sàng cho Tương lai

Chương kết thúc với những hiểu biết hướng tới tương lai về các xu hướng nổi bật bao gồm:

- Các kiến trúc mô hình tiên tiến với tỷ lệ hiệu quả được cải thiện
- Tích hợp phần cứng sâu hơn với các bộ tăng tốc AI chuyên dụng
- Sự phát triển của hệ sinh thái hướng tới tiêu chuẩn hóa và khả năng tương tác
- Các mô hình áp dụng doanh nghiệp được thúc đẩy bởi yêu cầu về quyền riêng tư và tuân thủ

Cách tiếp cận toàn diện này đảm bảo người đọc được trang bị đầy đủ để điều hướng cả những thách thức triển khai SLM hiện tại và các phát triển công nghệ trong tương lai, đưa ra các quyết định sáng suốt phù hợp với yêu cầu và ràng buộc cụ thể của tổ chức mình.

Chương này vừa là hướng dẫn thực tiễn để triển khai ngay lập tức, vừa là tài liệu chiến lược cho việc lập kế hoạch triển khai AI dài hạn, nhấn mạnh sự cân bằng quan trọng giữa khả năng, hiệu quả, và xuất sắc trong vận hành, điều làm nên thành công của các triển khai SLM.

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.