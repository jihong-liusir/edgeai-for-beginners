<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T12:32:24+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "vi"
}
-->
# Phần 03 - Tích hợp Giao thức Ngữ cảnh Mô hình (MCP)

## Giới thiệu về MCP (Giao thức Ngữ cảnh Mô hình)

Giao thức Ngữ cảnh Mô hình (MCP) là một khung làm việc mang tính cách mạng, cho phép các mô hình ngôn ngữ tương tác với các công cụ và hệ thống bên ngoài theo cách tiêu chuẩn hóa. Khác với các phương pháp truyền thống, nơi các mô hình bị cô lập, MCP tạo ra một cầu nối giữa các mô hình AI và thế giới thực thông qua một giao thức được định nghĩa rõ ràng.

### MCP là gì?

MCP hoạt động như một giao thức giao tiếp, cho phép các mô hình ngôn ngữ:
- Kết nối với các nguồn dữ liệu bên ngoài
- Thực thi các công cụ và chức năng
- Tương tác với API và dịch vụ
- Truy cập thông tin thời gian thực
- Thực hiện các thao tác phức tạp nhiều bước

Giao thức này biến các mô hình ngôn ngữ tĩnh thành các tác nhân động, có khả năng thực hiện các nhiệm vụ thực tế vượt ra ngoài việc tạo văn bản.

## Các Mô hình Ngôn ngữ Nhỏ (SLMs) trong MCP

Các Mô hình Ngôn ngữ Nhỏ đại diện cho một cách tiếp cận hiệu quả trong triển khai AI, mang lại nhiều lợi ích:

### Lợi ích của SLMs
- **Hiệu quả tài nguyên**: Yêu cầu tính toán thấp hơn
- **Thời gian phản hồi nhanh hơn**: Giảm độ trễ cho các ứng dụng thời gian thực  
- **Hiệu quả chi phí**: Cần ít cơ sở hạ tầng hơn
- **Bảo mật**: Có thể chạy cục bộ mà không cần truyền dữ liệu
- **Tùy chỉnh**: Dễ dàng tinh chỉnh cho các lĩnh vực cụ thể

### Tại sao SLMs hoạt động tốt với MCP

SLMs kết hợp với MCP tạo thành một sự kết hợp mạnh mẽ, nơi khả năng suy luận của mô hình được tăng cường bởi các công cụ bên ngoài, bù đắp cho số lượng tham số nhỏ hơn bằng chức năng nâng cao.

## Tổng quan về Python MCP SDK

Python MCP SDK cung cấp nền tảng để xây dựng các ứng dụng hỗ trợ MCP. SDK bao gồm:

- **Thư viện khách hàng**: Để kết nối với các máy chủ MCP
- **Khung máy chủ**: Để tạo các máy chủ MCP tùy chỉnh
- **Trình xử lý giao thức**: Để quản lý giao tiếp
- **Tích hợp công cụ**: Để thực thi các chức năng bên ngoài

## Triển khai thực tế: Khách hàng MCP Phi-4

Hãy khám phá một triển khai thực tế sử dụng mô hình mini Phi-4 của Microsoft được tích hợp với các khả năng MCP.

### Kiến trúc hệ thống

Triển khai này tuân theo kiến trúc phân lớp:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Các thành phần cốt lõi

#### 1. Các lớp khách hàng MCP

**BaseMCPClient**: Nền tảng trừu tượng cung cấp chức năng chung
- Giao thức quản lý ngữ cảnh bất đồng bộ
- Định nghĩa giao diện tiêu chuẩn
- Quản lý tài nguyên

**Phi4MiniMCPClient**: Triển khai dựa trên STDIO
- Giao tiếp quy trình cục bộ
- Xử lý đầu vào/đầu ra tiêu chuẩn
- Quản lý quy trình con

**Phi4MiniSSEMCPClient**: Triển khai sự kiện do máy chủ gửi
- Giao tiếp HTTP streaming
- Xử lý sự kiện thời gian thực
- Kết nối máy chủ dựa trên web

#### 2. Tích hợp LLM

**OllamaClient**: Lưu trữ mô hình cục bộ
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Phục vụ hiệu suất cao
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Quy trình xử lý công cụ

Quy trình xử lý công cụ chuyển đổi các công cụ MCP thành các định dạng tương thích với mô hình ngôn ngữ:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Bắt đầu: Hướng dẫn từng bước

### Bước 1: Thiết lập môi trường

Cài đặt các phụ thuộc cần thiết:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Bước 2: Cấu hình cơ bản

Thiết lập các biến môi trường:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Bước 3: Chạy khách hàng MCP đầu tiên của bạn

**Thiết lập Ollama cơ bản:**
```bash
python ghmodel_mcp_demo.py
```

**Sử dụng backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Kết nối sự kiện do máy chủ gửi:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Máy chủ MCP tùy chỉnh:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Bước 4: Sử dụng lập trình

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Các tính năng nâng cao

### Hỗ trợ nhiều backend

Triển khai hỗ trợ cả backend Ollama và vLLM, cho phép bạn lựa chọn dựa trên yêu cầu của mình:

- **Ollama**: Tốt hơn cho phát triển và thử nghiệm cục bộ
- **vLLM**: Tối ưu hóa cho sản xuất và các kịch bản thông lượng cao

### Các giao thức kết nối linh hoạt

Hai chế độ kết nối được hỗ trợ:

**Chế độ STDIO**: Giao tiếp quy trình trực tiếp
- Độ trễ thấp hơn
- Phù hợp với các công cụ cục bộ
- Thiết lập đơn giản

**Chế độ SSE**: Streaming dựa trên HTTP
- Có khả năng mạng
- Tốt hơn cho các hệ thống phân tán
- Cập nhật thời gian thực

### Khả năng tích hợp công cụ

Hệ thống có thể tích hợp với nhiều công cụ:
- Tự động hóa web (Playwright)
- Thao tác tệp
- Tương tác API
- Lệnh hệ thống
- Chức năng tùy chỉnh

## Xử lý lỗi và các thực hành tốt nhất

### Quản lý lỗi toàn diện

Triển khai bao gồm xử lý lỗi mạnh mẽ cho:

**Lỗi kết nối:**
- Máy chủ MCP gặp sự cố
- Hết thời gian mạng
- Vấn đề kết nối

**Lỗi thực thi công cụ:**
- Thiếu công cụ
- Xác thực tham số
- Thực thi thất bại

**Lỗi xử lý phản hồi:**
- Vấn đề phân tích JSON
- Không nhất quán định dạng
- Bất thường trong phản hồi LLM

### Các thực hành tốt nhất

1. **Quản lý tài nguyên**: Sử dụng các trình quản lý ngữ cảnh bất đồng bộ
2. **Xử lý lỗi**: Triển khai các khối try-catch toàn diện
3. **Ghi nhật ký**: Kích hoạt mức ghi nhật ký phù hợp
4. **Bảo mật**: Xác thực đầu vào và làm sạch đầu ra
5. **Hiệu suất**: Sử dụng kết nối pooling và caching

## Ứng dụng thực tế

### Tự động hóa web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Xử lý dữ liệu
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Tích hợp API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Tối ưu hóa hiệu suất

### Quản lý bộ nhớ
- Xử lý lịch sử tin nhắn hiệu quả
- Dọn dẹp tài nguyên đúng cách
- Kết nối pooling

### Tối ưu hóa mạng
- Hoạt động HTTP bất đồng bộ
- Thời gian chờ có thể cấu hình
- Khôi phục lỗi một cách nhẹ nhàng

### Xử lý đồng thời
- I/O không chặn
- Thực thi công cụ song song
- Mẫu bất đồng bộ hiệu quả

## Cân nhắc về bảo mật

### Bảo vệ dữ liệu
- Quản lý khóa API an toàn
- Xác thực đầu vào
- Làm sạch đầu ra

### Bảo mật mạng
- Hỗ trợ HTTPS
- Mặc định điểm cuối cục bộ
- Xử lý token an toàn

### An toàn thực thi
- Lọc công cụ
- Môi trường sandboxed
- Ghi nhật ký kiểm toán

## Kết luận

SLMs tích hợp với MCP đại diện cho một sự thay đổi mô hình trong phát triển ứng dụng AI. Bằng cách kết hợp hiệu quả của các mô hình nhỏ với sức mạnh của các công cụ bên ngoài, các nhà phát triển có thể tạo ra các hệ thống thông minh vừa hiệu quả về tài nguyên vừa có khả năng cao.

Triển khai khách hàng MCP Phi-4 minh họa cách tích hợp này có thể được thực hiện trong thực tế, cung cấp nền tảng vững chắc để xây dựng các ứng dụng AI tiên tiến.

Những điểm chính:
- MCP tạo cầu nối giữa các mô hình ngôn ngữ và hệ thống bên ngoài
- SLMs mang lại hiệu quả mà không làm giảm khả năng khi được tăng cường bởi các công cụ
- Kiến trúc mô-đun cho phép mở rộng và tùy chỉnh dễ dàng
- Xử lý lỗi và các biện pháp bảo mật là rất quan trọng cho việc sử dụng trong sản xuất

Hướng dẫn này cung cấp nền tảng để xây dựng các ứng dụng MCP hỗ trợ SLM của riêng bạn, mở ra các khả năng cho tự động hóa, xử lý dữ liệu và tích hợp hệ thống thông minh.

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.