<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T13:04:16+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "vi"
}
-->
# Phần 3: Tinh Chỉnh - Tùy Chỉnh Mô Hình Cho Các Nhiệm Vụ Cụ Thể

## Mục Lục
1. [Giới Thiệu Về Tinh Chỉnh](../../../Module05)
2. [Tại Sao Tinh Chỉnh Quan Trọng](../../../Module05)
3. [Các Loại Tinh Chỉnh](../../../Module05)
4. [Tinh Chỉnh Với Microsoft Olive](../../../Module05)
5. [Ví Dụ Thực Hành](../../../Module05)
6. [Các Thực Hành Tốt Nhất Và Hướng Dẫn](../../../Module05)
7. [Kỹ Thuật Nâng Cao](../../../Module05)
8. [Đánh Giá Và Giám Sát](../../../Module05)
9. [Những Thách Thức Thường Gặp Và Giải Pháp](../../../Module05)
10. [Kết Luận](../../../Module05)

## Giới Thiệu Về Tinh Chỉnh

**Tinh chỉnh** là một kỹ thuật học máy mạnh mẽ, cho phép điều chỉnh một mô hình đã được huấn luyện trước để thực hiện các nhiệm vụ cụ thể hoặc làm việc với các tập dữ liệu chuyên biệt. Thay vì huấn luyện một mô hình từ đầu, tinh chỉnh tận dụng kiến thức đã học từ mô hình huấn luyện trước và điều chỉnh nó cho trường hợp sử dụng cụ thể của bạn.

### Tinh Chỉnh Là Gì?

Tinh chỉnh là một dạng của **học chuyển giao**, trong đó bạn:
- Bắt đầu với một mô hình đã được huấn luyện trước, mô hình này đã học các mẫu chung từ các tập dữ liệu lớn
- Điều chỉnh các tham số bên trong của mô hình bằng tập dữ liệu cụ thể của bạn
- Giữ lại kiến thức giá trị trong khi chuyên biệt hóa mô hình cho nhiệm vụ của bạn

Hãy nghĩ về nó như việc dạy một đầu bếp giỏi nấu một món ăn mới - họ đã hiểu rõ các nguyên tắc cơ bản của nấu ăn, nhưng cần học các kỹ thuật và hương vị cụ thể cho phong cách mới.

### Lợi Ích Chính

- **Tiết Kiệm Thời Gian**: Nhanh hơn đáng kể so với huấn luyện từ đầu
- **Hiệu Quả Dữ Liệu**: Cần ít dữ liệu hơn để đạt hiệu suất tốt
- **Tiết Kiệm Chi Phí**: Yêu cầu ít tài nguyên tính toán hơn
- **Hiệu Suất Tốt Hơn**: Thường đạt kết quả vượt trội so với huấn luyện từ đầu
- **Tối Ưu Hóa Tài Nguyên**: Giúp các đội nhóm và tổ chức nhỏ tiếp cận AI mạnh mẽ

## Tại Sao Tinh Chỉnh Quan Trọng

### Ứng Dụng Thực Tiễn

Tinh chỉnh rất cần thiết trong nhiều tình huống:

**1. Thích Nghi Theo Lĩnh Vực**
- AI Y Tế: Điều chỉnh các mô hình ngôn ngữ chung cho thuật ngữ y khoa và ghi chú lâm sàng
- Công Nghệ Pháp Lý: Chuyên biệt hóa mô hình để phân tích tài liệu pháp lý và đánh giá hợp đồng
- Dịch Vụ Tài Chính: Tùy chỉnh mô hình để phân tích báo cáo tài chính và đánh giá rủi ro

**2. Chuyên Biệt Hóa Nhiệm Vụ**
- Tạo Nội Dung: Tinh chỉnh cho các phong cách viết hoặc tông giọng cụ thể
- Sinh Mã: Điều chỉnh mô hình cho các ngôn ngữ lập trình hoặc khung làm việc cụ thể
- Dịch Thuật: Cải thiện hiệu suất cho các cặp ngôn ngữ hoặc lĩnh vực kỹ thuật cụ thể

**3. Ứng Dụng Doanh Nghiệp**
- Dịch Vụ Khách Hàng: Tạo chatbot hiểu thuật ngữ đặc thù của công ty
- Tài Liệu Nội Bộ: Xây dựng trợ lý AI quen thuộc với các quy trình tổ chức
- Giải Pháp Theo Ngành: Phát triển mô hình hiểu biệt ngữ và quy trình làm việc đặc thù của ngành

## Các Loại Tinh Chỉnh

### 1. Tinh Chỉnh Toàn Bộ (Instruction Fine-Tuning)

Trong tinh chỉnh toàn bộ, tất cả các tham số của mô hình được cập nhật trong quá trình huấn luyện. Phương pháp này:
- Cung cấp sự linh hoạt tối đa và tiềm năng hiệu suất cao nhất
- Yêu cầu tài nguyên tính toán đáng kể
- Tạo ra một phiên bản hoàn toàn mới của mô hình
- Phù hợp nhất cho các trường hợp có nhiều dữ liệu huấn luyện và tài nguyên tính toán

### 2. Tinh Chỉnh Hiệu Quả Tham Số (PEFT)

Các phương pháp PEFT chỉ cập nhật một phần nhỏ các tham số, giúp quá trình hiệu quả hơn:

#### Low-Rank Adaptation (LoRA)
- Thêm các ma trận phân rã hạng nhỏ có thể huấn luyện vào các trọng số hiện có
- Giảm đáng kể số lượng tham số cần huấn luyện
- Duy trì hiệu suất gần với tinh chỉnh toàn bộ
- Cho phép dễ dàng chuyển đổi giữa các điều chỉnh khác nhau

#### QLoRA (Quantized LoRA)
- Kết hợp LoRA với các kỹ thuật lượng tử hóa
- Giảm thêm yêu cầu bộ nhớ
- Cho phép tinh chỉnh các mô hình lớn trên phần cứng tiêu dùng
- Cân bằng giữa hiệu quả và hiệu suất

#### Adapters
- Chèn các mạng nơ-ron nhỏ giữa các lớp hiện có
- Cho phép tinh chỉnh mục tiêu trong khi giữ nguyên mô hình cơ bản
- Cung cấp cách tiếp cận mô-đun để tùy chỉnh mô hình

### 3. Tinh Chỉnh Theo Nhiệm Vụ Cụ Thể

Tập trung vào việc điều chỉnh mô hình cho các nhiệm vụ hạ nguồn cụ thể:
- **Phân Loại**: Điều chỉnh mô hình cho các nhiệm vụ phân loại
- **Tạo Nội Dung**: Tối ưu hóa cho việc tạo nội dung và sinh văn bản
- **Trích Xuất**: Tinh chỉnh để trích xuất thông tin và nhận diện thực thể có tên
- **Tóm Tắt**: Chuyên biệt hóa mô hình để tóm tắt tài liệu

## Tinh Chỉnh Với Microsoft Olive

Microsoft Olive là một bộ công cụ tối ưu hóa mô hình toàn diện, giúp đơn giản hóa quá trình tinh chỉnh đồng thời cung cấp các tính năng cấp doanh nghiệp.

### Microsoft Olive Là Gì?

Microsoft Olive là một công cụ tối ưu hóa mô hình mã nguồn mở, giúp:
- Hợp lý hóa quy trình tinh chỉnh cho các mục tiêu phần cứng khác nhau
- Hỗ trợ sẵn các kiến trúc mô hình phổ biến (Llama, Phi, Qwen, Gemma)
- Cung cấp các tùy chọn triển khai trên đám mây và cục bộ
- Tích hợp liền mạch với Azure ML và các dịch vụ AI khác của Microsoft
- Hỗ trợ tối ưu hóa và lượng tử hóa tự động

### Các Tính Năng Chính

- **Tối Ưu Hóa Theo Phần Cứng**: Tự động tối ưu hóa mô hình cho phần cứng cụ thể (CPU, GPU, NPU)
- **Hỗ Trợ Đa Định Dạng**: Làm việc với các mô hình PyTorch, Hugging Face và ONNX
- **Quy Trình Tự Động**: Giảm cấu hình thủ công và thử nghiệm
- **Tích Hợp Doanh Nghiệp**: Hỗ trợ sẵn cho Azure ML và triển khai trên đám mây
- **Kiến Trúc Mở Rộng**: Cho phép các kỹ thuật tối ưu hóa tùy chỉnh

### Cài Đặt Và Thiết Lập

#### Cài Đặt Cơ Bản

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Các Phụ Thuộc Tùy Chọn

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Xác Minh Cài Đặt

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Ví Dụ Thực Hành

### Ví Dụ 1: Tinh Chỉnh Cơ Bản Với Olive CLI

Ví dụ này minh họa cách tinh chỉnh một mô hình ngôn ngữ nhỏ để phân loại cụm từ:

#### Bước 1: Chuẩn Bị Môi Trường

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Bước 2: Tinh Chỉnh Mô Hình

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Bước 3: Tối Ưu Hóa Để Triển Khai

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Ví Dụ 2: Cấu Hình Nâng Cao Với Tập Dữ Liệu Tùy Chỉnh

#### Bước 1: Chuẩn Bị Tập Dữ Liệu Tùy Chỉnh

Tạo một tệp JSON với dữ liệu huấn luyện của bạn:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Bước 2: Tạo Tệp Cấu Hình

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Bước 3: Thực Thi Tinh Chỉnh

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Ví Dụ 3: Tinh Chỉnh QLoRA Để Tiết Kiệm Bộ Nhớ

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Các Thực Hành Tốt Nhất Và Hướng Dẫn

### Chuẩn Bị Dữ Liệu

**1. Chất Lượng Dữ Liệu Quan Trọng Hơn Số Lượng**
- Ưu tiên các ví dụ chất lượng cao, đa dạng thay vì khối lượng lớn dữ liệu kém chất lượng
- Đảm bảo dữ liệu đại diện cho trường hợp sử dụng mục tiêu của bạn
- Làm sạch và tiền xử lý dữ liệu một cách nhất quán

**2. Định Dạng Và Mẫu Dữ Liệu**
- Sử dụng định dạng nhất quán trên tất cả các ví dụ huấn luyện
- Tạo các mẫu đầu vào-đầu ra rõ ràng phù hợp với trường hợp sử dụng của bạn
- Bao gồm định dạng hướng dẫn phù hợp cho các mô hình được tinh chỉnh theo hướng dẫn

**3. Chia Tập Dữ Liệu**
- Dành 10-20% dữ liệu cho xác thực
- Duy trì phân phối tương tự giữa các tập huấn luyện/xác thực
- Cân nhắc lấy mẫu phân tầng cho các nhiệm vụ phân loại

### Cấu Hình Huấn Luyện

**1. Lựa Chọn Tốc Độ Học**
- Bắt đầu với tốc độ học nhỏ hơn (1e-5 đến 1e-4) khi tinh chỉnh
- Sử dụng lịch trình tốc độ học để hội tụ tốt hơn
- Theo dõi đồ thị mất mát để điều chỉnh tốc độ học phù hợp

**2. Tối Ưu Hóa Kích Thước Lô**
- Cân bằng kích thước lô với bộ nhớ sẵn có
- Sử dụng tích lũy gradient để tăng kích thước lô hiệu quả
- Cân nhắc mối quan hệ giữa kích thước lô và tốc độ học

**3. Thời Gian Huấn Luyện**
- Theo dõi các chỉ số xác thực để tránh quá khớp
- Sử dụng dừng sớm khi hiệu suất xác thực đạt đỉnh
- Lưu các điểm kiểm tra thường xuyên để khôi phục và phân tích

### Lựa Chọn Mô Hình

**1. Chọn Mô Hình Cơ Bản**
- Chọn các mô hình được huấn luyện trước trên các lĩnh vực tương tự nếu có thể
- Cân nhắc kích thước mô hình so với hạn chế tính toán của bạn
- Đánh giá các yêu cầu cấp phép cho mục đích thương mại

**2. Chọn Phương Pháp Tinh Chỉnh**
- Sử dụng LoRA/QLoRA cho các môi trường hạn chế tài nguyên
- Chọn tinh chỉnh toàn bộ khi hiệu suất tối đa là quan trọng
- Cân nhắc các phương pháp dựa trên adapter cho các kịch bản đa nhiệm

### Quản Lý Tài Nguyên

**1. Tối Ưu Hóa Phần Cứng**
- Chọn phần cứng phù hợp với kích thước mô hình và phương pháp của bạn
- Sử dụng bộ nhớ GPU hiệu quả với kiểm tra gradient
- Cân nhắc các giải pháp dựa trên đám mây cho các mô hình lớn

**2. Quản Lý Bộ Nhớ**
- Sử dụng huấn luyện độ chính xác hỗn hợp khi có thể
- Thực hiện tích lũy gradient cho các hạn chế bộ nhớ
- Theo dõi việc sử dụng bộ nhớ GPU trong suốt quá trình huấn luyện

## Kỹ Thuật Nâng Cao

### Huấn Luyện Đa Adapter

Huấn luyện nhiều adapter cho các nhiệm vụ khác nhau trong khi chia sẻ mô hình cơ bản:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Tối Ưu Hóa Siêu Tham Số

Thực hiện điều chỉnh siêu tham số một cách hệ thống:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Hàm Mất Mát Tùy Chỉnh

Triển khai các hàm mất mát dành riêng cho lĩnh vực:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Đánh Giá Và Giám Sát

### Các Chỉ Số Và Đánh Giá

**1. Chỉ Số Chuẩn**
- **Độ Chính Xác**: Độ đúng tổng thể cho các nhiệm vụ phân loại
- **Perplexity**: Thước đo chất lượng mô hình ngôn ngữ
- **BLEU/ROUGE**: Chất lượng tạo văn bản và tóm tắt
- **F1 Score**: Cân bằng giữa độ chính xác và độ nhạy cho phân loại

**2. Chỉ Số Theo Lĩnh Vực**
- **Tiêu Chuẩn Nhiệm Vụ Cụ Thể**: Sử dụng các tiêu chuẩn đã được thiết lập cho lĩnh vực của bạn
- **Đánh Giá Con Người**: Bao gồm đánh giá của con người cho các nhiệm vụ mang tính chủ quan
- **Chỉ Số Kinh Doanh**: Phù hợp với các mục tiêu kinh doanh thực tế

**3. Thiết Lập Đánh Giá**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Giám Sát Tiến Trình Huấn Luyện

**1. Theo Dõi Mất Mát**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Giám Sát Xác Thực**
- Theo dõi mất mát xác thực cùng với mất mát huấn luyện
- Giám sát các dấu hiệu của quá khớp (mất mát xác thực tăng trong khi mất mát huấn luyện giảm)
- Sử dụng dừng sớm dựa trên các chỉ số xác thực

**3. Giám Sát Tài Nguyên**
- Theo dõi việc sử dụng GPU/CPU
- Theo dõi các mẫu sử dụng bộ nhớ
- Theo dõi tốc độ và thông lượng huấn luyện

## Những Thách Thức Thường Gặp Và Giải Pháp

### Thách Thức 1: Quá Khớp

**Triệu Chứng:**
- Mất mát huấn luyện tiếp tục giảm trong khi mất mát xác thực tăng
- Khoảng cách lớn giữa hiệu suất huấn luyện và xác thực
- Khả năng tổng quát hóa kém đối với dữ liệu mới

**Giải Pháp:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Thách Thức 2: Hạn Chế Bộ Nhớ

**Giải Pháp:**
- Sử dụng kiểm tra gradient
- Thực hiện tích lũy gradient
- Chọn các phương pháp hiệu quả tham số (LoRA, QLoRA)
- Sử dụng phân chia mô hình cho các mô hình lớn

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Thách Thức 3: Huấn Luyện Chậm

**Giải Pháp:**
- Tối ưu hóa các đường dẫn tải dữ liệu
- Sử dụng huấn luyện độ chính xác hỗn hợp
- Thực hiện các chiến lược tạo lô hiệu quả
- Cân nhắc huấn luyện phân tán cho các tập dữ liệu lớn

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Thách Thức 4: Hiệu Suất Kém

**Các Bước Chẩn Đoán:**
1. Xác minh chất lượng và định dạng dữ liệu
2. Kiểm tra tốc độ học và thời gian huấn luyện
3. Đánh giá lựa chọn mô hình cơ bản
4. Xem xét tiền xử lý và mã hóa

**Giải Pháp:**
- Tăng tính đa dạng của dữ liệu huấn luyện
- Điều chỉnh lịch trình tốc độ học
- Thử các mô hình cơ bản khác nhau
- Thực hiện các kỹ thuật tăng cường dữ liệu

## Kết Luận

Tinh chỉnh là một kỹ thuật mạnh mẽ giúp dân chủ hóa khả năng tiếp cận các khả năng AI tiên tiến. Bằng cách tận dụng các công cụ như Microsoft Olive, các tổ chức có thể hiệu quả điều chỉnh các mô hình đã được huấn luyện trước cho nhu cầu cụ thể của họ trong khi tối ưu hóa hiệu suất và hạn chế tài nguyên.

### Những Điểm Chính

1. **Chọn Phương Pháp Phù Hợp**: Chọn các phương pháp tinh chỉnh dựa trên tài nguyên tính toán và yêu cầu hiệu suất của bạn
2. **Chất Lượng Dữ Liệu Quan Trọng**: Đầu tư vào dữ liệu huấn luyện chất lượng cao, đại diện
3. **Giám Sát Và Lặp Lại**: Liên tục đánh giá và cải thiện các mô hình của bạn
4. **Tận Dụng Công Cụ**: Sử dụng các khung làm việc như Olive để đơn giản hóa và tối ưu hóa quy trình
5. **Cân Nhắc Triển Khai**: Lên kế hoạch tối ưu hóa và triển khai mô hình ngay từ đầu

## ➡️ Tiếp Theo Là Gì

- [04: Triển Khai - Thực Hiện Mô Hình Sẵn Sàng Sản Xuất](./04.SLMOps.Deployment.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.