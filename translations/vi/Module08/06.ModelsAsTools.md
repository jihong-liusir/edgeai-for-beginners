<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T21:51:52+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "vi"
}
-->
# Buổi 6: Foundry Local – Mô hình như công cụ

## Tổng quan

Xem các mô hình AI như những công cụ mô-đun, tùy chỉnh chạy trực tiếp trên thiết bị với Foundry Local. Buổi học này nhấn mạnh các quy trình thực tế để suy luận bảo vệ quyền riêng tư, độ trễ thấp và cách tích hợp các công cụ này qua SDK, API hoặc CLI. Bạn cũng sẽ học cách mở rộng quy mô lên Azure AI Foundry khi cần thiết.

Tài liệu tham khảo:
- Tài liệu Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Tích hợp với SDK suy luận: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Biên dịch mô hình Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Mục tiêu học tập
- Thiết kế các mẫu mô hình như công cụ trên thiết bị
- Tích hợp qua API REST tương thích OpenAI hoặc SDK
- Tùy chỉnh mô hình cho các trường hợp sử dụng cụ thể theo lĩnh vực
- Lập kế hoạch mở rộng quy mô lai lên Azure AI Foundry

## Phần 1: Trừu tượng hóa công cụ (Từng bước)

Mục tiêu: Đại diện cho các mô hình như công cụ với các hợp đồng rõ ràng và một bộ định tuyến đơn giản.

Bước 1) Định nghĩa giao diện công cụ và đăng ký
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```

Bước 2) Triển khai hai công cụ dựa trên Foundry Local
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```

Bước 3) Bộ định tuyến theo nhiệm vụ
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```

## Phần 2: Tích hợp SDK và API (Từng bước)

Mục tiêu: Sử dụng OpenAI Python SDK với điểm cuối Foundry Local.

Bước 1) Cài đặt
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```

Bước 2) Cấu hình biến môi trường
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```

Bước 3) Gọi API chat
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```

## Phần 3: Tùy chỉnh theo lĩnh vực (Từng bước)

Mục tiêu: Điều chỉnh đầu ra cho một lĩnh vực bằng mẫu gợi ý và JSON schema.

Bước 1) Tạo mẫu gợi ý theo lĩnh vực
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```

Bước 2) Áp dụng đầu ra JSON
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```

## Phần 4: Hoạt động ngoại tuyến và tư thế bảo mật (Từng bước)

Mục tiêu: Đảm bảo quyền riêng tư và khả năng phục hồi khi chạy mô hình như công cụ cục bộ.

Bước 1) Làm nóng trước và xác thực điểm cuối cục bộ
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```

Bước 2) Làm sạch đầu vào
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

Bước 3) Cờ chỉ cục bộ và ghi nhật ký
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```

## Phần 5: Mở rộng quy mô lên Azure AI Foundry (Từng bước)

Mục tiêu: Phản chiếu các mô hình cục bộ với các điểm cuối Azure để tăng khả năng xử lý.

Bước 1) Quyết định chiến lược định tuyến
- Ưu tiên cục bộ để bảo vệ quyền riêng tư/độ trễ, sử dụng Azure khi gặp lỗi hoặc yêu cầu lớn

Bước 2) Triển khai một bộ định tuyến stub đơn giản
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```

## Danh sách kiểm tra thực hành
- [ ] Đăng ký ít nhất hai công cụ và định tuyến yêu cầu
- [ ] Gọi Foundry Local qua OpenAI SDK và REST thô
- [ ] Áp dụng đầu ra JSON cho mẫu lĩnh vực
- [ ] Làm sạch và ghi nhật ký các cuộc gọi cục bộ
- [ ] Triển khai một bộ định tuyến lai đơn giản với Azure fallback

## Kết luận

Foundry Local cho phép AI mạnh mẽ trên thiết bị, nơi các mô hình trở thành công cụ có thể kết hợp. Với các giao diện rõ ràng, quản trị và mở rộng quy mô lai, các nhóm có thể triển khai ứng dụng AI thời gian thực, bảo mật, tôn trọng quyền riêng tư của người dùng đồng thời sẵn sàng cho doanh nghiệp.

---

