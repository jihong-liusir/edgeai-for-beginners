<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T21:52:33+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "vi"
}
-->
# Buổi 4: Các Mô Hình Tiên Tiến – LLMs, SLMs, và Suy Luận Trên Thiết Bị

## Tổng Quan

So sánh LLMs và SLMs, đánh giá các lợi ích và hạn chế giữa suy luận cục bộ và trên đám mây, và triển khai các demo minh họa các kịch bản EdgeAI sử dụng Phi và ONNX Runtime. Chúng ta cũng sẽ nhấn mạnh Chainlit RAG, các tùy chọn suy luận WebGPU, và tích hợp Open WebUI.

Tài liệu tham khảo:
- Tài liệu Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Hướng dẫn Open WebUI (ứng dụng chat với Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Mục Tiêu Học Tập
- Hiểu các lợi ích và hạn chế giữa LLM và SLM về chi phí, độ trễ, và độ chính xác
- Lựa chọn giữa suy luận cục bộ và trên đám mây phù hợp với nhu cầu kinh doanh cụ thể
- Triển khai một demo RAG nhỏ với Chainlit
- Khám phá WebGPU để tăng tốc trên trình duyệt
- Kết nối Open WebUI với Foundry Local

## Phần 1: LLM vs SLM – Ma Trận Quyết Định

Cân nhắc:
- Độ trễ: SLMs trên thiết bị thường cung cấp phản hồi dưới một giây
- Chi phí: suy luận cục bộ giảm chi phí đám mây
- Bảo mật: dữ liệu nhạy cảm được giữ trên thiết bị
- Khả năng: LLMs có thể vượt trội hơn SLMs trong các nhiệm vụ phức tạp
- Độ tin cậy: chiến lược kết hợp giảm rủi ro thời gian chết

## Phần 2: Cục Bộ vs Đám Mây – Các Mô Hình Kết Hợp

- Ưu tiên cục bộ với dự phòng đám mây cho các yêu cầu lớn/phức tạp
- Ưu tiên đám mây với cục bộ cho các kịch bản nhạy cảm về bảo mật hoặc ngoại tuyến
- Phân loại theo loại nhiệm vụ (tạo mã cho DeepSeek, trò chuyện chung cho Phi/Qwen)

## Phần 3: Ứng Dụng Chat RAG với Chainlit (Tối Giản)

Cài đặt các phụ thuộc:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Chạy:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Mở rộng: thêm một trình truy xuất đơn giản (tệp cục bộ) và thêm ngữ cảnh truy xuất vào trước lời nhắc của người dùng.

## Phần 4: Suy Luận WebGPU (Lưu Ý)

Chạy các mô hình nhỏ trực tiếp trên trình duyệt sử dụng WebGPU. Đây là lý tưởng cho các demo ưu tiên bảo mật và trải nghiệm không cần cài đặt. Dưới đây là một ví dụ tối giản, từng bước sử dụng ONNX Runtime Web với nhà cung cấp thực thi WebGPU.

1) Kiểm tra hỗ trợ WebGPU
- Trình duyệt Chromium: chrome://gpu → xác nhận “WebGPU” đã được bật
- Kiểm tra bằng mã (chúng ta cũng sẽ kiểm tra trong mã): `if (!('gpu' in navigator)) { /* không có WebGPU */ }`

2) Tạo một dự án tối giản
Tạo một thư mục và hai tệp: `index.html` và `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Phục vụ cục bộ (Windows cmd.exe)
Sử dụng một máy chủ tĩnh đơn giản để trình duyệt có thể lấy mô hình.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Mở http://localhost:5173 trên trình duyệt của bạn. Bạn sẽ thấy nhật ký khởi tạo, tạo phiên với WebGPU, và một dự đoán argmax.

4) Khắc phục sự cố
- Nếu WebGPU không khả dụng: cập nhật Chrome/Edge và đảm bảo trình điều khiển GPU hiện tại, sau đó kiểm tra chrome://flags để bật “Enable WebGPU”.
- Nếu xảy ra lỗi CORS hoặc fetch: đảm bảo bạn phục vụ tệp qua http:// (không phải file://) và URL mô hình cho phép yêu cầu cross-origin.
- Dự phòng sang CPU: thay đổi `executionProviders: ['wasm']` để kiểm tra hành vi cơ bản.

5) Bước tiếp theo
- Thay thế bằng một mô hình ONNX cụ thể theo lĩnh vực (ví dụ: phân loại hình ảnh hoặc mô hình văn bản nhỏ).
- Thêm logic tiền xử lý/hậu xử lý cho các đầu vào thực tế.
- Đối với các mô hình lớn hơn hoặc độ trễ sản xuất, ưu tiên Foundry Local hoặc ONNX Runtime Server.

## Phần 5: Open WebUI + Foundry Local (Từng Bước)

Điều này kết nối Open WebUI với điểm cuối tương thích OpenAI của Foundry Local để có giao diện chat cục bộ.

1) Yêu cầu
- Foundry Local đã được cài đặt và hoạt động (`foundry --version`)
- Một mô hình sẵn sàng chạy cục bộ (ví dụ: `phi-4-mini`)
- Docker Desktop đã được cài đặt (khuyến nghị cho Open WebUI)

2) Khởi động một mô hình với Foundry Local
```powershell
foundry model run phi-4-mini
```
Điều này cung cấp một API tương thích OpenAI tại `http://localhost:8000`.

3) Khởi động Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Lưu ý:
- Trên Windows, `host.docker.internal` cho phép container truy cập máy chủ của bạn tại `localhost`.
- Chúng ta đặt `OPENAI_API_BASE_URL` thành điểm cuối của Foundry Local và một `OPENAI_API_KEY` giả.

4) Cấu hình từ giao diện Open WebUI (lựa chọn thay thế)
- Truy cập http://localhost:3000
- Hoàn thành thiết lập ban đầu (người dùng quản trị)
- Đi tới Cài đặt → Mô hình/Nhà cung cấp
- Đặt URL cơ sở: `http://host.docker.internal:8000/v1`
- Đặt API Key: `local-key` (giá trị giả)
- Lưu

5) Chạy một lời nhắc thử nghiệm
- Trong chat Open WebUI, chọn hoặc nhập tên mô hình `phi-4-mini`
- Lời nhắc: “Liệt kê năm lợi ích của suy luận AI trên thiết bị.”
- Bạn sẽ thấy phản hồi được truyền từ mô hình cục bộ của bạn

6) Khắc phục sự cố
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Tùy chọn: Lưu dữ liệu Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Danh Sách Kiểm Tra Thực Hành
- [ ] So sánh phản hồi/độ trễ giữa SLM và LLM cục bộ
- [ ] Chạy demo Chainlit với ít nhất hai mô hình
- [ ] Kết nối Open WebUI với điểm cuối cục bộ của bạn và kiểm tra

## Bước Tiếp Theo
- Chuẩn bị cho các quy trình tác nhân trong Buổi 5
- Xác định các kịch bản nơi kết hợp cục bộ/đám mây cải thiện ROI

---

