<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:54:48+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "vi"
}
-->
# Buổi 4: Xây dựng ứng dụng chat sản xuất với Chainlit

## Tổng quan

Buổi học này tập trung vào việc xây dựng các ứng dụng chat sẵn sàng cho sản xuất bằng Chainlit và Microsoft Foundry Local. Bạn sẽ học cách tạo giao diện web hiện đại cho các cuộc trò chuyện AI, triển khai phản hồi theo luồng và triển khai các ứng dụng chat mạnh mẽ với xử lý lỗi và thiết kế trải nghiệm người dùng phù hợp.

**Những gì bạn sẽ xây dựng:**
- **Ứng dụng Chat Chainlit**: Giao diện web hiện đại với phản hồi theo luồng
- **Demo WebGPU**: Suy luận trên trình duyệt cho các ứng dụng ưu tiên quyền riêng tư  
- **Tích hợp Open WebUI**: Giao diện chat chuyên nghiệp với Foundry Local
- **Mô hình sản xuất**: Xử lý lỗi, giám sát và chiến lược triển khai

## Mục tiêu học tập

- Xây dựng ứng dụng chat sẵn sàng cho sản xuất với Chainlit
- Triển khai phản hồi theo luồng để cải thiện trải nghiệm người dùng
- Làm chủ các mô hình tích hợp SDK Foundry Local
- Áp dụng xử lý lỗi phù hợp và giảm thiểu tác động một cách nhẹ nhàng
- Triển khai và cấu hình ứng dụng chat cho các môi trường khác nhau
- Hiểu các mẫu giao diện web hiện đại cho AI hội thoại

## Yêu cầu trước

- **Foundry Local**: Đã cài đặt và chạy ([Hướng dẫn cài đặt](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Phiên bản 3.10 hoặc mới hơn với khả năng tạo môi trường ảo
- **Mô hình**: Ít nhất một mô hình đã được tải (`foundry model run phi-4-mini`)
- **Trình duyệt**: Trình duyệt web hiện đại hỗ trợ WebGPU (Chrome/Edge)
- **Docker**: Để tích hợp Open WebUI (tùy chọn)

## Phần 1: Hiểu về ứng dụng chat hiện đại

### Tổng quan kiến trúc

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Công nghệ chính

**Mẫu SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Quản lý dịch vụ tự động
- `manager.endpoint` và `manager.api_key`: Chi tiết kết nối
- `manager.get_model_info(alias).id`: Nhận diện mô hình

**Framework Chainlit:**
- `@cl.on_chat_start`: Khởi tạo phiên chat
- `@cl.on_message`: Xử lý tin nhắn từ người dùng  
- `cl.Message().stream_token()`: Phát trực tiếp theo thời gian thực
- Tự động tạo giao diện người dùng và quản lý WebSocket

## Phần 2: Ma trận quyết định giữa Local và Cloud

### Đặc điểm hiệu suất

| Khía cạnh | Local (Foundry) | Cloud (Azure OpenAI) |
|-----------|-----------------|----------------------|
| **Độ trễ** | 🚀 50-200ms (không mạng) | ⏱️ 200-2000ms (phụ thuộc mạng) |
| **Quyền riêng tư** | 🔒 Dữ liệu không rời khỏi thiết bị | ⚠️ Dữ liệu gửi lên cloud |
| **Chi phí** | 💰 Miễn phí sau phần cứng | 💸 Tính phí theo token |
| **Offline** | ✅ Hoạt động không cần internet | ❌ Yêu cầu internet |
| **Kích thước mô hình** | ⚠️ Giới hạn bởi phần cứng | ✅ Truy cập mô hình lớn nhất |
| **Khả năng mở rộng** | ⚠️ Phụ thuộc phần cứng | ✅ Mở rộng không giới hạn |

### Mẫu chiến lược lai

**Ưu tiên Local với dự phòng:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Định tuyến theo nhiệm vụ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Phần 3: Mẫu 04 - Ứng dụng Chat Chainlit

### Bắt đầu nhanh

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Ứng dụng tự động mở tại `http://localhost:8080` với giao diện chat hiện đại.

### Triển khai cốt lõi

Ứng dụng Mẫu 04 minh họa các mẫu sẵn sàng cho sản xuất:

**Khám phá dịch vụ tự động:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Xử lý chat theo luồng:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Tùy chọn cấu hình

**Biến môi trường:**

| Biến | Mô tả | Mặc định | Ví dụ |
|------|-------|----------|-------|
| `MODEL` | Bí danh mô hình sử dụng | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint Foundry Local | Tự động phát hiện | `http://localhost:51211` |
| `API_KEY` | API key (tùy chọn cho local) | `""` | `your-api-key` |

**Sử dụng nâng cao:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Phần 4: Tạo và sử dụng Jupyter Notebooks

### Tổng quan về hỗ trợ Notebook

Mẫu 04 bao gồm một notebook Jupyter toàn diện (`chainlit_app.ipynb`) cung cấp:

- **📚 Nội dung giáo dục**: Tài liệu học tập từng bước
- **🔬 Khám phá tương tác**: Chạy và thử nghiệm với các ô mã
- **📊 Minh họa trực quan**: Biểu đồ, sơ đồ và hình ảnh đầu ra
- **🛠️ Công cụ phát triển**: Kiểm tra và gỡ lỗi

### Tạo notebook của riêng bạn

#### Bước 1: Thiết lập môi trường Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Bước 2: Tạo notebook mới

**Sử dụng VS Code:**
1. Mở VS Code trong thư mục Module08
2. Tạo tệp mới với phần mở rộng `.ipynb`
3. Chọn kernel "Foundry Local" khi được nhắc
4. Bắt đầu thêm các ô với nội dung của bạn

**Sử dụng Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Thực hành cấu trúc notebook

#### Tổ chức ô

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Ví dụ và bài tập tương tác

#### Bài tập 1: Kiểm tra cấu hình client

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Bài tập 2: Mô phỏng phản hồi theo luồng

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Phần 5: Demo suy luận trình duyệt WebGPU

### Tổng quan

WebGPU cho phép chạy các mô hình AI trực tiếp trên trình duyệt để tối đa hóa quyền riêng tư và trải nghiệm không cần cài đặt. Mẫu này minh họa ONNX Runtime Web với thực thi WebGPU.

### Bước 1: Kiểm tra hỗ trợ WebGPU

**Yêu cầu trình duyệt:**
- Chrome/Edge 113+ với WebGPU được bật
- Kiểm tra: `chrome://gpu` → xác nhận trạng thái "WebGPU"
- Kiểm tra bằng mã: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Bước 2: Tạo demo WebGPU

Tạo thư mục: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Bước 3: Chạy demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Phần 6: Tích hợp Open WebUI

### Tổng quan

Open WebUI cung cấp giao diện chuyên nghiệp giống ChatGPT kết nối với API tương thích OpenAI của Foundry Local.

### Bước 1: Yêu cầu trước

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Bước 2: Thiết lập Docker (Khuyến nghị)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Lưu ý:** `host.docker.internal` cho phép các container Docker truy cập máy chủ trên Windows.

### Bước 3: Cấu hình

1. **Mở trình duyệt:** Điều hướng đến `http://localhost:3000`
2. **Thiết lập ban đầu:** Tạo tài khoản admin
3. **Cấu hình mô hình:**
   - Settings → Models → OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (bất kỳ giá trị nào đều được)
4. **Kiểm tra kết nối:** Các mô hình sẽ xuất hiện trong danh sách thả xuống

### Xử lý sự cố

**Các vấn đề thường gặp:**

1. **Kết nối bị từ chối:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Mô hình không xuất hiện:**
   - Xác minh mô hình đã được tải: `foundry model list`
   - Kiểm tra phản hồi API: `curl http://localhost:51211/v1/models`
   - Khởi động lại container Open WebUI

## Phần 7: Cân nhắc triển khai sản xuất

### Cấu hình môi trường

**Thiết lập phát triển:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Triển khai sản xuất:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Các vấn đề cổng thường gặp và giải pháp

**Ngăn chặn xung đột cổng 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Giám sát hiệu suất

**Triển khai kiểm tra sức khỏe:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Tóm tắt

Buổi 4 đã đề cập đến việc xây dựng các ứng dụng Chainlit sẵn sàng cho sản xuất dành cho AI hội thoại. Bạn đã học về:

- ✅ **Framework Chainlit**: Giao diện hiện đại và hỗ trợ phát trực tiếp cho ứng dụng chat
- ✅ **Tích hợp Foundry Local**: Sử dụng SDK và các mẫu cấu hình  
- ✅ **Suy luận WebGPU**: AI trên trình duyệt để tối đa hóa quyền riêng tư
- ✅ **Thiết lập Open WebUI**: Triển khai giao diện chat chuyên nghiệp
- ✅ **Mô hình sản xuất**: Xử lý lỗi, giám sát và mở rộng

Ứng dụng Mẫu 04 minh họa các thực tiễn tốt nhất để xây dựng giao diện chat mạnh mẽ tận dụng các mô hình AI cục bộ thông qua Microsoft Foundry Local đồng thời mang lại trải nghiệm người dùng xuất sắc.

## Tài liệu tham khảo

- **[Mẫu 04: Ứng dụng Chainlit](samples/04/README.md)**: Ứng dụng hoàn chỉnh với tài liệu
- **[Notebook giáo dục Chainlit](samples/04/chainlit_app.ipynb)**: Tài liệu học tập tương tác
- **[Tài liệu Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Tài liệu nền tảng đầy đủ
- **[Tài liệu Chainlit](https://docs.chainlit.io/)**: Tài liệu chính thức của framework
- **[Hướng dẫn tích hợp Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Hướng dẫn chính thức

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.