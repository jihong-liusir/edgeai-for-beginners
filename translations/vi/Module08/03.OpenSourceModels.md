<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T21:50:39+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "vi"
}
-->
# Buổi 3: Mô hình mã nguồn mở với Foundry Local

## Tổng quan

Buổi học này khám phá cách đưa các mô hình mã nguồn mở vào Foundry Local: lựa chọn mô hình từ cộng đồng, tích hợp nội dung từ Hugging Face, và áp dụng chiến lược “mang theo mô hình của bạn” (BYOM). Bạn cũng sẽ tìm hiểu về chuỗi Model Mondays để học hỏi liên tục và khám phá mô hình.

Tham khảo:
- Tài liệu Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Biên dịch mô hình Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Mục tiêu học tập
- Khám phá và đánh giá các mô hình mã nguồn mở để suy luận cục bộ
- Biên dịch và chạy các mô hình Hugging Face được chọn trong Foundry Local
- Áp dụng chiến lược lựa chọn mô hình dựa trên độ chính xác, độ trễ và nhu cầu tài nguyên
- Quản lý mô hình cục bộ với bộ nhớ đệm và phiên bản hóa

## Phần 1: Khám phá và lựa chọn mô hình (Hướng dẫn từng bước)

Bước 1) Liệt kê các mô hình có sẵn trong danh mục cục bộ  
```cmd
foundry model list
```
  
Bước 2) Thử nhanh hai mô hình ứng viên (tự động tải xuống khi chạy lần đầu)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Bước 3) Ghi lại các chỉ số cơ bản  
- Quan sát độ trễ (chủ quan) và chất lượng với một prompt cố định  
- Theo dõi mức sử dụng bộ nhớ qua Task Manager khi mỗi mô hình chạy  

## Phần 2: Chạy mô hình từ danh mục qua CLI (Hướng dẫn từng bước)

Bước 1) Khởi động một mô hình  
```cmd
foundry model run llama-3.2
```
  
Bước 2) Gửi một prompt thử nghiệm qua endpoint tương thích OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Phần 3: BYOM – Biên dịch mô hình Hugging Face (Hướng dẫn từng bước)

Làm theo hướng dẫn chính thức để biên dịch mô hình. Dưới đây là quy trình tổng quan—xem bài viết trên Microsoft Learn để biết các lệnh chính xác và cấu hình được hỗ trợ.

Bước 1) Chuẩn bị thư mục làm việc  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Bước 2) Biên dịch một mô hình HF được hỗ trợ  
- Sử dụng các bước từ tài liệu Learn để chuyển đổi và đặt mô hình ONNX đã biên dịch vào thư mục `models` của bạn  
- Xác nhận với:  
```cmd
foundry cache ls
```
  
Bạn sẽ thấy tên mô hình đã biên dịch của mình (ví dụ, `llama-3.2`).  

Bước 3) Chạy mô hình đã biên dịch  
```cmd
foundry model run llama-3.2 --verbose
```
  
Lưu ý:  
- Đảm bảo đủ dung lượng đĩa và RAM để biên dịch và chạy  
- Bắt đầu với các mô hình nhỏ hơn để xác nhận quy trình, sau đó mở rộng quy mô  

## Phần 4: Quản lý mô hình thực tiễn (Hướng dẫn từng bước)

Bước 1) Tạo một registry `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Bước 2) Script chọn lọc nhỏ  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Phần 5: Đánh giá hiệu năng thực tế (Hướng dẫn từng bước)

Bước 1) Đánh giá độ trễ đơn giản  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Bước 2) Kiểm tra chất lượng nhanh  
- Sử dụng một bộ prompt cố định, lưu kết quả đầu ra vào CSV/JSON  
- Đánh giá thủ công độ trôi chảy, mức độ liên quan, và độ chính xác (1–5)  

## Phần 6: Bước tiếp theo
- Đăng ký Model Mondays để nhận mô hình và mẹo mới: https://aka.ms/model-mondays  
- Đóng góp kết quả vào `models.json` của nhóm bạn  
- Chuẩn bị cho Buổi 4: so sánh LLMs và SLMs, suy luận cục bộ và trên đám mây, cùng các demo thực hành  

---

