<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:54:21+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "vi"
}
-->
# Buổi 3: Khám phá và Quản lý Mô hình Mã nguồn Mở

## Tổng quan

Buổi học này tập trung vào việc khám phá và quản lý mô hình thực tế với Foundry Local. Bạn sẽ học cách liệt kê các mô hình có sẵn, thử nghiệm các tùy chọn khác nhau và hiểu các đặc điểm hiệu suất cơ bản. Phương pháp này nhấn mạnh vào việc thực hành với công cụ dòng lệnh Foundry CLI để giúp bạn chọn mô hình phù hợp với các trường hợp sử dụng của mình.

## Mục tiêu học tập

- Thành thạo các lệnh Foundry CLI để khám phá và quản lý mô hình
- Hiểu cách lưu trữ và quản lý bộ nhớ đệm mô hình
- Học cách thử nghiệm và so sánh nhanh các mô hình khác nhau
- Xây dựng quy trình làm việc thực tế để chọn và đánh giá mô hình
- Khám phá hệ sinh thái mô hình ngày càng phát triển thông qua Foundry Local

## Điều kiện tiên quyết

- Hoàn thành Buổi 1: Bắt đầu với Foundry Local
- Đã cài đặt và truy cập được Foundry Local CLI
- Có đủ dung lượng lưu trữ để tải xuống mô hình (kích thước mô hình có thể từ 1GB đến hơn 20GB)
- Hiểu cơ bản về các loại mô hình và trường hợp sử dụng

## Tổng quan

Buổi học này khám phá cách đưa các mô hình mã nguồn mở vào Foundry Local.

## Phần 6: Bài tập thực hành

### Bài tập: Khám phá và So sánh Mô hình

Tạo script đánh giá mô hình của riêng bạn dựa trên Mẫu 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### Nhiệm vụ của bạn

1. **Chạy script Mẫu 03**: `samples\03\list_and_bench.cmd`
2. **Thử nghiệm các mô hình khác nhau**: Kiểm tra ít nhất 3 mô hình khác nhau
3. **So sánh hiệu suất**: Ghi lại sự khác biệt về tốc độ và chất lượng phản hồi
4. **Ghi chép kết quả**: Tạo biểu đồ so sánh đơn giản

### Định dạng So sánh Ví dụ

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## Phần 7: Xử lý sự cố và Thực hành tốt nhất

### Các vấn đề thường gặp và giải pháp

**Mô hình không khởi động được:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**Bộ nhớ không đủ:**
- Bắt đầu với các mô hình nhỏ hơn (`phi-4-mini`)
- Đóng các ứng dụng khác
- Nâng cấp RAM nếu thường xuyên gặp giới hạn

**Hiệu suất chậm:**
- Đảm bảo mô hình đã tải đầy đủ (kiểm tra đầu ra chi tiết)
- Đóng các ứng dụng nền không cần thiết
- Cân nhắc sử dụng ổ lưu trữ nhanh hơn (SSD)

### Thực hành tốt nhất

1. **Bắt đầu nhỏ**: Bắt đầu với `phi-4-mini` để kiểm tra thiết lập
2. **Chỉ chạy một mô hình tại một thời điểm**: Dừng các mô hình trước đó trước khi khởi động mô hình mới
3. **Theo dõi tài nguyên**: Giám sát việc sử dụng bộ nhớ
4. **Kiểm tra nhất quán**: Sử dụng cùng một lời nhắc để so sánh công bằng
5. **Ghi chép kết quả**: Lưu lại các ghi chú về hiệu suất mô hình cho các trường hợp sử dụng của bạn

## Phần 8: Bước tiếp theo và Tài liệu tham khảo

### Chuẩn bị cho Buổi 4

- **Trọng tâm Buổi 4**: Các công cụ và kỹ thuật tối ưu hóa
- **Điều kiện tiên quyết**: Thành thạo việc chuyển đổi mô hình và kiểm tra hiệu suất cơ bản
- **Khuyến nghị**: Xác định 2-3 mô hình yêu thích từ buổi học này

### Tài liệu tham khảo bổ sung

- **[Tài liệu Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Tài liệu chính thức
- **[Tham chiếu CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Tham chiếu lệnh đầy đủ
- **[Model Mondays](https://aka.ms/model-mondays)**: Điểm nhấn mô hình hàng tuần
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Cộng đồng và các vấn đề
- **[Mẫu 03: Khám phá Mô hình](samples/03/README.md)**: Script ví dụ thực hành

### Những điểm chính cần nhớ

✅ **Khám phá Mô hình**: Sử dụng `foundry model list` để khám phá các mô hình có sẵn  
✅ **Thử nghiệm nhanh**: Mẫu `list_and_bench.cmd` để đánh giá nhanh  
✅ **Theo dõi hiệu suất**: Đo lường việc sử dụng tài nguyên cơ bản và thời gian phản hồi  
✅ **Chọn mô hình**: Hướng dẫn thực tế để chọn mô hình theo trường hợp sử dụng  
✅ **Quản lý bộ nhớ đệm**: Hiểu cách lưu trữ và quy trình dọn dẹp  

Giờ đây, bạn đã có kỹ năng thực hành để khám phá, thử nghiệm và chọn các mô hình phù hợp cho ứng dụng AI của mình bằng cách sử dụng cách tiếp cận CLI đơn giản của Foundry Local.

## Mục tiêu học tập

- Khám phá và đánh giá các mô hình mã nguồn mở cho suy luận cục bộ
- Biên dịch và chạy các mô hình Hugging Face được chọn trong Foundry Local
- Áp dụng các chiến lược chọn mô hình dựa trên độ chính xác, độ trễ và nhu cầu tài nguyên
- Quản lý mô hình cục bộ với bộ nhớ đệm và phiên bản hóa

## Phần 1: Khám phá Mô hình với Foundry CLI

### Các lệnh quản lý mô hình cơ bản

Foundry CLI cung cấp các lệnh đơn giản để khám phá và quản lý mô hình:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### Chạy các mô hình đầu tiên của bạn

Bắt đầu với các mô hình phổ biến, đã được kiểm tra kỹ để hiểu các đặc điểm hiệu suất:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**Lưu ý:** Cờ `--verbose` cung cấp thông tin khởi động chi tiết, bao gồm:
- Tiến trình tải xuống mô hình (lần chạy đầu tiên)
- Chi tiết phân bổ bộ nhớ
- Thông tin liên kết dịch vụ
- Các chỉ số khởi tạo hiệu suất

### Hiểu các loại mô hình

**Mô hình ngôn ngữ nhỏ (SLMs):**
- `phi-4-mini`: Nhanh, hiệu quả, phù hợp cho trò chuyện chung
- `phi-4`: Phiên bản mạnh hơn với khả năng suy luận tốt hơn

**Mô hình trung bình:**
- `qwen2.5-7b`: Suy luận xuất sắc và ngữ cảnh dài hơn
- `deepseek-r1-7b`: Tối ưu hóa cho việc tạo mã

**Mô hình lớn:**
- `llama-3.2`: Mô hình mã nguồn mở mới nhất của Meta
- `qwen2.5-14b`: Suy luận cấp doanh nghiệp

## Phần 2: Thử nghiệm và So sánh Mô hình Nhanh

### Phương pháp Mẫu 03: Danh sách và Đánh giá đơn giản

Dựa trên mẫu Mẫu 03, đây là quy trình tối thiểu:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### Kiểm tra hiệu suất mô hình

Khi một mô hình đang chạy, thử nghiệm với các lời nhắc nhất quán:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### Thử nghiệm thay thế bằng PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## Phần 3: Quản lý Bộ nhớ đệm và Lưu trữ Mô hình

### Hiểu về Bộ nhớ đệm Mô hình

Foundry Local tự động quản lý việc tải xuống và lưu trữ bộ nhớ đệm mô hình:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### Cân nhắc về Lưu trữ Mô hình

**Kích thước mô hình điển hình:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Thực hành lưu trữ tốt nhất:**
- Giữ 2-3 mô hình trong bộ nhớ đệm để chuyển đổi nhanh
- Xóa các mô hình không sử dụng để giải phóng dung lượng: `foundry cache clean`
- Giám sát việc sử dụng ổ đĩa, đặc biệt trên các ổ SSD nhỏ hơn
- Cân nhắc giữa kích thước mô hình và khả năng

### Theo dõi hiệu suất mô hình

Khi các mô hình đang chạy, giám sát tài nguyên hệ thống:

**Trình quản lý tác vụ Windows:**
- Theo dõi việc sử dụng bộ nhớ (mô hình được giữ trong RAM)
- Giám sát việc sử dụng CPU trong quá trình suy luận
- Kiểm tra I/O ổ đĩa trong quá trình tải mô hình ban đầu

**Giám sát dòng lệnh:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## Phần 4: Hướng dẫn Chọn Mô hình Thực tế

### Chọn mô hình theo trường hợp sử dụng

**Đối với trò chuyện chung và Hỏi & Đáp:**
- Bắt đầu với: `phi-4-mini` (nhanh, hiệu quả)
- Nâng cấp lên: `phi-4` (suy luận tốt hơn)
- Nâng cao: `qwen2.5-7b` (ngữ cảnh dài hơn)

**Đối với tạo mã:**
- Khuyến nghị: `deepseek-r1-7b`
- Lựa chọn thay thế: `qwen2.5-7b` (cũng tốt cho mã)

**Đối với suy luận phức tạp:**
- Tốt nhất: `qwen2.5-7b` hoặc `qwen2.5-14b`
- Lựa chọn tiết kiệm: `phi-4`

### Hướng dẫn yêu cầu phần cứng

**Yêu cầu hệ thống tối thiểu:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**Khuyến nghị để có hiệu suất tốt nhất:**
- RAM 32GB+ để chuyển đổi mô hình thoải mái
- Lưu trữ SSD để tải mô hình nhanh hơn
- CPU hiện đại với hiệu suất đơn luồng tốt
- Hỗ trợ NPU (PC Windows 11 Copilot+) để tăng tốc

### Quy trình chuyển đổi mô hình

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## Phần 5: Đánh giá Mô hình Đơn giản

### Kiểm tra hiệu suất cơ bản

Đây là cách tiếp cận đơn giản để so sánh hiệu suất mô hình:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### Đánh giá chất lượng thủ công

Đối với mỗi mô hình, thử nghiệm với các lời nhắc nhất quán và đánh giá thủ công:

**Lời nhắc kiểm tra:**
1. "Giải thích điện toán lượng tử một cách đơn giản."
2. "Viết một hàm Python để sắp xếp một danh sách."
3. "Ưu và nhược điểm của làm việc từ xa là gì?"
4. "Tóm tắt lợi ích của AI biên."

**Tiêu chí đánh giá:**
- **Độ chính xác**: Thông tin có đúng không?
- **Rõ ràng**: Giải thích có dễ hiểu không?
- **Đầy đủ**: Có trả lời đầy đủ câu hỏi không?
- **Tốc độ**: Phản hồi nhanh như thế nào?

### Giám sát việc sử dụng tài nguyên

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## Phần 6: Bước tiếp theo

- Đăng ký Model Mondays để nhận mô hình mới và mẹo: https://aka.ms/model-mondays
- Đóng góp kết quả cho `models.json` của nhóm bạn
- Chuẩn bị cho Buổi 4: so sánh LLMs với SLMs, suy luận cục bộ so với đám mây, và các demo thực hành

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.