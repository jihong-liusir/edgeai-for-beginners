<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-25T00:07:48+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "vi"
}
-->
# Bu·ªïi 1: B·∫Øt ƒë·∫ßu v·ªõi Foundry Local

## T·ªïng quan

Microsoft Foundry Local mang c√°c kh·∫£ nƒÉng c·ªßa Azure AI Foundry tr·ª±c ti·∫øp ƒë·∫øn m√¥i tr∆∞·ªùng ph√°t tri·ªÉn Windows 11 c·ªßa b·∫°n, cho ph√©p ph√°t tri·ªÉn AI v·ªõi ƒë·ªô tr·ªÖ th·∫•p v√† b·∫£o v·ªá quy·ªÅn ri√™ng t∆∞ b·∫±ng c√°c c√¥ng c·ª• c·∫•p doanh nghi·ªáp. Bu·ªïi h·ªçc n√†y bao g·ªìm to√†n b·ªô qu√° tr√¨nh c√†i ƒë·∫∑t, c·∫•u h√¨nh v√† tri·ªÉn khai th·ª±c t·∫ø c√°c m√¥ h√¨nh ph·ªï bi·∫øn nh∆∞ phi, qwen, deepseek, v√† GPT-OSS-20B.

## M·ª•c ti√™u h·ªçc t·∫≠p

Sau bu·ªïi h·ªçc n√†y, b·∫°n s·∫Ω:
- C√†i ƒë·∫∑t v√† c·∫•u h√¨nh Foundry Local tr√™n Windows 11
- Th√†nh th·∫°o c√°c l·ªánh CLI v√† t√πy ch·ªçn c·∫•u h√¨nh
- Hi·ªÉu chi·∫øn l∆∞·ª£c l∆∞u tr·ªØ m√¥ h√¨nh ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
- Ch·∫°y th√†nh c√¥ng c√°c m√¥ h√¨nh phi, qwen, deepseek, v√† GPT-OSS-20B
- T·∫°o ·ª©ng d·ª•ng AI ƒë·∫ßu ti√™n c·ªßa b·∫°n b·∫±ng Foundry Local

## Y√™u c·∫ßu tr∆∞·ªõc

### Y√™u c·∫ßu h·ªá th·ªëng
- **Windows 11**: Phi√™n b·∫£n 22H2 ho·∫∑c m·ªõi h∆°n
- **RAM**: T·ªëi thi·ªÉu 16GB, khuy·∫øn ngh·ªã 32GB
- **Dung l∆∞·ª£ng l∆∞u tr·ªØ**: 50GB tr·ªëng cho m√¥ h√¨nh v√† b·ªô nh·ªõ ƒë·ªám
- **Ph·∫ßn c·ª©ng**: Thi·∫øt b·ªã h·ªó tr·ª£ NPU ho·∫∑c GPU (PC Copilot+ ho·∫∑c GPU NVIDIA ƒë∆∞·ª£c ∆∞u ti√™n)
- **M·∫°ng**: Internet t·ªëc ƒë·ªô cao ƒë·ªÉ t·∫£i m√¥ h√¨nh

### M√¥i tr∆∞·ªùng ph√°t tri·ªÉn
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Ph·∫ßn 1: C√†i ƒë·∫∑t v√† thi·∫øt l·∫≠p

### B∆∞·ªõc 1: C√†i ƒë·∫∑t Foundry Local

C√†i ƒë·∫∑t Foundry Local b·∫±ng Winget ho·∫∑c t·∫£i tr√¨nh c√†i ƒë·∫∑t t·ª´ GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### B∆∞·ªõc 2: X√°c minh c√†i ƒë·∫∑t

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Ph·∫ßn 2: Hi·ªÉu v·ªÅ CLI

### C·∫•u tr√∫c l·ªánh c·ªët l√µi

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Ph·∫ßn 3: Qu·∫£n l√Ω v√† l∆∞u tr·ªØ m√¥ h√¨nh

Foundry Local tri·ªÉn khai l∆∞u tr·ªØ m√¥ h√¨nh th√¥ng minh ƒë·ªÉ t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v√† dung l∆∞·ª£ng:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Ph·∫ßn 4: Tri·ªÉn khai m√¥ h√¨nh th·ª±c t·∫ø

### Ch·∫°y m√¥ h√¨nh Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### L√†m vi·ªác v·ªõi m√¥ h√¨nh Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### Ch·∫°y m√¥ h√¨nh DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### Ch·∫°y GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Ph·∫ßn 5: T·∫°o ·ª©ng d·ª•ng ƒë·∫ßu ti√™n c·ªßa b·∫°n

### ·ª®ng d·ª•ng tr√≤ chuy·ªán hi·ªán ƒë·∫°i (OpenAI SDK + Foundry Local)

T·∫°o ·ª©ng d·ª•ng tr√≤ chuy·ªán s·∫µn s√†ng s·∫£n xu·∫•t b·∫±ng c√°ch t√≠ch h·ª£p OpenAI SDK v·ªõi Foundry Local, theo c√°c m·∫´u t·ª´ Sample 01.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("‚ö†Ô∏è Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"üåê Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"üè† Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"‚ö†Ô∏è Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"üîß Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Ch·∫°y ·ª©ng d·ª•ng tr√≤ chuy·ªán

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Ph·∫ßn 6: Kh·∫Øc ph·ª•c s·ª± c·ªë v√† th·ª±c h√†nh t·ªët nh·∫•t

### C√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p v√† gi·∫£i ph√°p

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Gi√°m s√°t t√†i nguy√™n h·ªá th·ªëng (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Bi·∫øn m√¥i tr∆∞·ªùng

| Bi·∫øn       | M√¥ t·∫£                        | M·∫∑c ƒë·ªãnh            | B·∫Øt bu·ªôc |
|------------|------------------------------|---------------------|----------|
| `MODEL`    | B√≠ danh ho·∫∑c t√™n m√¥ h√¨nh     | `phi-4-mini`        | Kh√¥ng    |
| `BASE_URL` | URL c∆° s·ªü c·ªßa Foundry Local  | `http://localhost:8000` | Kh√¥ng    |
| `API_KEY`  | Kh√≥a API (th∆∞·ªùng kh√¥ng c·∫ßn cho local) | `""`         | Kh√¥ng    |
| `AZURE_OPENAI_ENDPOINT` | ƒêi·ªÉm cu·ªëi Azure OpenAI | -                   | Cho Azure |
| `AZURE_OPENAI_API_KEY`  | Kh√≥a API Azure OpenAI  | -                   | Cho Azure |
| `AZURE_OPENAI_API_VERSION` | Phi√™n b·∫£n API Azure | `2024-08-01-preview` | Kh√¥ng    |

### Th·ª±c h√†nh t·ªët nh·∫•t

- **S·ª≠ d·ª•ng OpenAI SDK**: ∆Øu ti√™n OpenAI SDK thay v√¨ y√™u c·∫ßu HTTP th√¥ ƒë·ªÉ d·ªÖ b·∫£o tr√¨ h∆°n
- **FoundryLocalManager**: S·ª≠ d·ª•ng SDK ch√≠nh th·ª©c ƒë·ªÉ qu·∫£n l√Ω d·ªãch v·ª• khi c√≥ s·∫µn
- **X·ª≠ l√Ω l·ªói**: Tri·ªÉn khai chi·∫øn l∆∞·ª£c d·ª± ph√≤ng ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng s·∫£n xu·∫•t
- **C·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n**: Gi·ªØ Foundry Local lu√¥n ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ truy c·∫≠p c√°c m√¥ h√¨nh v√† s·ª≠a l·ªói m·ªõi
- **B·∫Øt ƒë·∫ßu nh·ªè**: B·∫Øt ƒë·∫ßu v·ªõi c√°c m√¥ h√¨nh nh·ªè (Phi mini, Qwen 7B) v√† m·ªü r·ªông d·∫ßn
- **Gi√°m s√°t t√†i nguy√™n**: Theo d√µi CPU/GPU/b·ªô nh·ªõ khi ƒëi·ªÅu ch·ªânh prompt v√† c√†i ƒë·∫∑t

## Ph·∫ßn 7: B√†i t·∫≠p th·ª±c h√†nh

### B√†i t·∫≠p 1: Ki·ªÉm tra nhanh ƒëa m√¥ h√¨nh

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### B√†i t·∫≠p 2: Ki·ªÉm tra t√≠ch h·ª£p OpenAI SDK

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"‚úÖ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"‚ùå {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### B√†i t·∫≠p 3: Ki·ªÉm tra to√†n di·ªán s·ª©c kh·ªèe d·ªãch v·ª•

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"‚úÖ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"‚úÖ {model_id}: Working")
            except Exception as e:
                print(f"‚ùå {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"‚ùå Service check failed: {e}")
        return False

comprehensive_health_check()
```

## T√†i li·ªáu tham kh·∫£o

- **B·∫Øt ƒë·∫ßu v·ªõi Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **Tham kh·∫£o CLI v√† t·ªïng quan l·ªánh**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **T√≠ch h·ª£p OpenAI SDK**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Bi√™n d·ªãch m√¥ h√¨nh Hugging Face**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: Tr√≤ chuy·ªán nhanh qua OpenAI SDK**: samples/01/README.md
- **Sample 02: T√≠ch h·ª£p SDK n√¢ng cao**: samples/02/README.md

---

