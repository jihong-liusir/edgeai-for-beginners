<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T12:46:34+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "vi"
}
-->
# Phần 4: Bộ Công Cụ Tối Ưu OpenVINO

## Mục Lục
1. [Giới thiệu](../../../Module04)
2. [OpenVINO là gì?](../../../Module04)
3. [Cài đặt](../../../Module04)
4. [Hướng dẫn bắt đầu nhanh](../../../Module04)
5. [Ví dụ: Chuyển đổi và tối ưu hóa mô hình với OpenVINO](../../../Module04)
6. [Sử dụng nâng cao](../../../Module04)
7. [Thực hành tốt nhất](../../../Module04)
8. [Khắc phục sự cố](../../../Module04)
9. [Tài nguyên bổ sung](../../../Module04)

## Giới thiệu

OpenVINO (Open Visual Inference and Neural Network Optimization) là bộ công cụ mã nguồn mở của Intel dành cho việc triển khai các giải pháp AI hiệu suất cao trên môi trường đám mây, tại chỗ và thiết bị biên. Dù bạn đang nhắm đến CPU, GPU, VPU hay các bộ tăng tốc AI chuyên dụng, OpenVINO cung cấp khả năng tối ưu hóa toàn diện trong khi vẫn duy trì độ chính xác của mô hình và hỗ trợ triển khai đa nền tảng.

## OpenVINO là gì?

OpenVINO là một bộ công cụ mã nguồn mở giúp các nhà phát triển tối ưu hóa, chuyển đổi và triển khai mô hình AI một cách hiệu quả trên nhiều nền tảng phần cứng khác nhau. Bộ công cụ này bao gồm ba thành phần chính: OpenVINO Runtime để suy luận, Neural Network Compression Framework (NNCF) để tối ưu hóa mô hình, và OpenVINO Model Server để triển khai quy mô lớn.

### Các tính năng chính

- **Triển khai đa nền tảng**: Hỗ trợ Linux, Windows và macOS với các API Python, C++, và C
- **Tăng tốc phần cứng**: Tự động phát hiện thiết bị và tối ưu hóa cho CPU, GPU, VPU và các bộ tăng tốc AI
- **Khung nén mô hình**: Kỹ thuật lượng hóa, cắt tỉa và tối ưu hóa tiên tiến thông qua NNCF
- **Tương thích với các khung framework**: Hỗ trợ trực tiếp các mô hình từ TensorFlow, ONNX, PaddlePaddle và PyTorch
- **Hỗ trợ AI tạo sinh**: OpenVINO GenAI chuyên dụng để triển khai các mô hình ngôn ngữ lớn và ứng dụng AI tạo sinh

### Lợi ích

- **Tối ưu hóa hiệu suất**: Cải thiện tốc độ đáng kể với độ mất chính xác tối thiểu
- **Giảm kích thước triển khai**: Giảm thiểu các phụ thuộc bên ngoài, đơn giản hóa việc cài đặt và triển khai
- **Thời gian khởi động nhanh hơn**: Tải và lưu trữ mô hình được tối ưu hóa để khởi tạo ứng dụng nhanh hơn
- **Triển khai quy mô lớn**: Từ thiết bị biên đến hạ tầng đám mây với các API nhất quán
- **Sẵn sàng cho sản xuất**: Độ tin cậy cấp doanh nghiệp với tài liệu đầy đủ và hỗ trợ cộng đồng

## Cài đặt

### Yêu cầu trước

- Python 3.8 trở lên
- Trình quản lý gói pip
- Môi trường ảo (khuyến nghị)
- Phần cứng tương thích (khuyến nghị CPU Intel, nhưng hỗ trợ nhiều kiến trúc khác)

### Cài đặt cơ bản

Tạo và kích hoạt môi trường ảo:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Cài đặt OpenVINO Runtime:

```bash
pip install openvino
```

Cài đặt NNCF để tối ưu hóa mô hình:

```bash
pip install nncf
```

### Cài đặt OpenVINO GenAI

Dành cho các ứng dụng AI tạo sinh:

```bash
pip install openvino-genai
```

### Các phụ thuộc tùy chọn

Các gói bổ sung cho các trường hợp sử dụng cụ thể:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Xác minh cài đặt

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Nếu thành công, bạn sẽ thấy thông tin phiên bản OpenVINO.

## Hướng dẫn bắt đầu nhanh

### Tối ưu hóa mô hình đầu tiên của bạn

Hãy chuyển đổi và tối ưu hóa một mô hình Hugging Face bằng OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Quy trình này làm gì?

Quy trình tối ưu hóa bao gồm: tải mô hình gốc từ Hugging Face, chuyển đổi sang định dạng OpenVINO Intermediate Representation (IR), áp dụng các tối ưu hóa mặc định, và biên dịch cho phần cứng mục tiêu.

### Giải thích các tham số chính

- `export=True`: Chuyển đổi mô hình sang định dạng IR của OpenVINO
- `compile=False`: Trì hoãn việc biên dịch đến thời gian chạy để linh hoạt hơn
- `device`: Phần cứng mục tiêu ("CPU", "GPU", "AUTO" để tự động chọn)
- `save_pretrained()`: Lưu mô hình đã tối ưu hóa để tái sử dụng

## Ví dụ: Chuyển đổi và tối ưu hóa mô hình với OpenVINO

### Bước 1: Chuyển đổi mô hình với lượng hóa NNCF

Cách áp dụng lượng hóa sau huấn luyện bằng NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Bước 2: Tối ưu hóa nâng cao với nén trọng số

Đối với các mô hình dựa trên transformer, áp dụng nén trọng số:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Bước 3: Suy luận với mô hình đã tối ưu hóa

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Cấu trúc đầu ra

Sau khi tối ưu hóa, thư mục mô hình của bạn sẽ chứa:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Sử dụng nâng cao

### Cấu hình với NNCF YAML

Đối với các quy trình tối ưu hóa phức tạp, sử dụng tệp cấu hình NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Áp dụng cấu hình:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Tối ưu hóa GPU

Để tăng tốc GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Tối ưu hóa xử lý theo lô

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Triển khai máy chủ mô hình

Triển khai mô hình đã tối ưu hóa với OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Mã khách hàng cho máy chủ mô hình:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Thực hành tốt nhất

### 1. Lựa chọn và chuẩn bị mô hình
- Sử dụng các mô hình từ các khung framework được hỗ trợ (PyTorch, TensorFlow, ONNX)
- Đảm bảo đầu vào của mô hình có hình dạng cố định hoặc động đã biết
- Kiểm tra với các tập dữ liệu đại diện để hiệu chỉnh

### 2. Lựa chọn chiến lược tối ưu hóa
- **Lượng hóa sau huấn luyện**: Bắt đầu từ đây để tối ưu hóa nhanh
- **Nén trọng số**: Lý tưởng cho các mô hình ngôn ngữ lớn và transformer
- **Huấn luyện nhận thức lượng hóa**: Sử dụng khi độ chính xác là yếu tố quan trọng

### 3. Tối ưu hóa theo phần cứng
- **CPU**: Sử dụng lượng hóa INT8 để cân bằng hiệu suất
- **GPU**: Tận dụng độ chính xác FP16 và xử lý theo lô
- **VPU**: Tập trung vào đơn giản hóa mô hình và hợp nhất lớp

### 4. Tinh chỉnh hiệu suất
- **Chế độ thông lượng**: Dành cho xử lý theo lô khối lượng lớn
- **Chế độ độ trễ**: Dành cho các ứng dụng tương tác thời gian thực
- **Thiết bị AUTO**: Để OpenVINO tự chọn phần cứng tối ưu

### 5. Quản lý bộ nhớ
- Sử dụng hình dạng động một cách hợp lý để tránh chi phí bộ nhớ
- Triển khai lưu trữ mô hình để tải nhanh hơn
- Theo dõi việc sử dụng bộ nhớ trong quá trình tối ưu hóa

### 6. Xác thực độ chính xác
- Luôn xác thực mô hình đã tối ưu hóa so với hiệu suất ban đầu
- Sử dụng các tập dữ liệu kiểm tra đại diện để đánh giá
- Cân nhắc tối ưu hóa dần dần (bắt đầu với các cài đặt bảo thủ)

## Khắc phục sự cố

### Các vấn đề thường gặp

#### 1. Vấn đề cài đặt
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Lỗi chuyển đổi mô hình
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Vấn đề hiệu suất
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Vấn đề bộ nhớ
- Giảm kích thước lô mô hình trong quá trình tối ưu hóa
- Sử dụng streaming cho các tập dữ liệu lớn
- Bật lưu trữ mô hình: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Suy giảm độ chính xác
- Sử dụng độ chính xác cao hơn (INT8 thay vì INT4)
- Tăng kích thước tập dữ liệu hiệu chỉnh
- Áp dụng tối ưu hóa độ chính xác hỗn hợp

### Theo dõi hiệu suất

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Nhận hỗ trợ

- **Tài liệu**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Vấn đề trên GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Diễn đàn cộng đồng**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Tài nguyên bổ sung

### Liên kết chính thức
- **Trang chủ OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Kho GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Kho NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Tài liệu học tập
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Hướng dẫn bắt đầu nhanh**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Hướng dẫn tối ưu hóa**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Công cụ tích hợp
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Các tiêu chuẩn hiệu suất
- **Tiêu chuẩn chính thức**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Ví dụ từ cộng đồng
- **Jupyter Notebooks**: [Kho OpenVINO Notebooks](https://github.com/openvinotoolkit/openvino_notebooks) - Các hướng dẫn chi tiết có sẵn trong kho OpenVINO notebooks
- **Ứng dụng mẫu**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Các ví dụ thực tế cho nhiều lĩnh vực (thị giác máy tính, NLP, âm thanh)
- **Bài viết blog**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Các bài viết blog của Intel AI và cộng đồng với các trường hợp sử dụng chi tiết

### Công cụ liên quan
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Các kỹ thuật tối ưu hóa bổ sung cho phần cứng Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Để so sánh triển khai trên thiết bị di động và biên
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Các lựa chọn thay thế cho công cụ suy luận đa nền tảng

## ➡️ Tiếp theo

- [05: Tìm hiểu sâu về Apple MLX Framework](./05.AppleMLX.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.