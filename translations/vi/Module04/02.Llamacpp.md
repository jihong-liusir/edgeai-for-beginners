<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T12:55:16+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "vi"
}
-->
# Phần 2: Hướng Dẫn Triển Khai Llama.cpp

## Mục Lục
1. [Giới thiệu](../../../Module04)
2. [Llama.cpp là gì?](../../../Module04)
3. [Cài đặt](../../../Module04)
4. [Xây dựng từ mã nguồn](../../../Module04)
5. [Lượng tử hóa mô hình](../../../Module04)
6. [Sử dụng cơ bản](../../../Module04)
7. [Tính năng nâng cao](../../../Module04)
8. [Tích hợp Python](../../../Module04)
9. [Khắc phục sự cố](../../../Module04)
10. [Thực hành tốt nhất](../../../Module04)

## Giới thiệu

Hướng dẫn toàn diện này sẽ giúp bạn nắm vững mọi thứ về Llama.cpp, từ cài đặt cơ bản đến các kịch bản sử dụng nâng cao. Llama.cpp là một triển khai mạnh mẽ bằng C++ cho phép suy luận hiệu quả các Mô hình Ngôn ngữ Lớn (LLMs) với thiết lập tối thiểu và hiệu suất xuất sắc trên nhiều cấu hình phần cứng khác nhau.

## Llama.cpp là gì?

Llama.cpp là một khung suy luận LLM được viết bằng C/C++ cho phép chạy các mô hình ngôn ngữ lớn cục bộ với thiết lập tối thiểu và hiệu suất tiên tiến trên nhiều loại phần cứng. Các tính năng chính bao gồm:

### Tính năng cốt lõi
- **Triển khai thuần C/C++** không có phụ thuộc
- **Tương thích đa nền tảng** (Windows, macOS, Linux)
- **Tối ưu hóa phần cứng** cho nhiều kiến trúc
- **Hỗ trợ lượng tử hóa** (từ 1.5-bit đến 8-bit số nguyên)
- **Hỗ trợ tăng tốc CPU và GPU**
- **Hiệu quả bộ nhớ** cho các môi trường hạn chế

### Ưu điểm
- Chạy hiệu quả trên CPU mà không cần phần cứng chuyên dụng
- Hỗ trợ nhiều backend GPU (CUDA, Metal, OpenCL, Vulkan)
- Nhẹ và dễ di chuyển
- Apple silicon là công dân hạng nhất - được tối ưu hóa qua ARM NEON, Accelerate và Metal frameworks
- Hỗ trợ nhiều mức lượng tử hóa để giảm sử dụng bộ nhớ

## Cài đặt

### Phương pháp 1: Tệp nhị phân dựng sẵn (Khuyến nghị cho người mới bắt đầu)

#### Tải xuống từ GitHub Releases
1. Truy cập [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Tải xuống tệp nhị phân phù hợp với hệ thống của bạn:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` cho Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` cho macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` cho Linux

3. Giải nén tệp và thêm thư mục vào PATH của hệ thống

#### Sử dụng Trình Quản Lý Gói

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Nhiều bản phân phối):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Phương pháp 2: Gói Python (llama-cpp-python)

#### Cài đặt cơ bản
```bash
pip install llama-cpp-python
```

#### Với Tăng Tốc Phần Cứng
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Xây dựng từ mã nguồn

### Yêu cầu

**Yêu cầu hệ thống:**
- Trình biên dịch C++ (GCC, Clang, hoặc MSVC)
- CMake (phiên bản 3.14 trở lên)
- Git
- Công cụ xây dựng cho nền tảng của bạn

**Cài đặt các yêu cầu:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Cài đặt Visual Studio 2022 với công cụ phát triển C++
- Cài đặt CMake từ trang web chính thức
- Cài đặt Git

### Quy trình xây dựng cơ bản

1. **Clone kho lưu trữ:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Cấu hình xây dựng:**
```bash
cmake -B build
```

3. **Xây dựng dự án:**
```bash
cmake --build build --config Release
```

Để biên dịch nhanh hơn, sử dụng các job song song:
```bash
cmake --build build --config Release -j 8
```

### Xây dựng dành riêng cho phần cứng

#### Hỗ trợ CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Hỗ trợ Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Hỗ trợ OpenBLAS (Tối ưu hóa CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Hỗ trợ Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Tùy chọn xây dựng nâng cao

#### Xây dựng Debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Với các tính năng bổ sung
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Lượng tử hóa mô hình

### Hiểu định dạng GGUF

GGUF (Generalized GGML Unified Format) là một định dạng tệp được tối ưu hóa để chạy các mô hình ngôn ngữ lớn hiệu quả bằng Llama.cpp và các khung khác. Nó cung cấp:

- Lưu trữ trọng số mô hình tiêu chuẩn
- Cải thiện khả năng tương thích trên các nền tảng
- Hiệu suất nâng cao
- Xử lý metadata hiệu quả

### Các loại lượng tử hóa

Llama.cpp hỗ trợ nhiều mức lượng tử hóa:

| Loại | Số bit | Mô tả | Trường hợp sử dụng |
|------|--------|-------|--------------------|
| F16  | 16     | Độ chính xác nửa | Chất lượng cao, bộ nhớ lớn |
| Q8_0 | 8      | Lượng tử hóa 8-bit | Cân bằng tốt |
| Q4_0 | 4      | Lượng tử hóa 4-bit | Chất lượng vừa phải, kích thước nhỏ hơn |
| Q2_K | 2      | Lượng tử hóa 2-bit | Kích thước nhỏ nhất, chất lượng thấp hơn |

### Chuyển đổi mô hình

#### Từ PyTorch sang GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Tải trực tiếp từ Hugging Face
Nhiều mô hình có sẵn ở định dạng GGUF trên Hugging Face:
- Tìm kiếm các mô hình có "GGUF" trong tên
- Tải xuống mức lượng tử hóa phù hợp
- Sử dụng trực tiếp với llama.cpp

## Sử dụng cơ bản

### Giao diện dòng lệnh

#### Sinh văn bản đơn giản
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Sử dụng mô hình từ Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Chế độ máy chủ
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Các tham số phổ biến

| Tham số | Mô tả | Ví dụ |
|---------|-------|-------|
| `-m`    | Đường dẫn tệp mô hình | `-m model.gguf` |
| `-p`    | Văn bản gợi ý | `-p "Hello world"` |
| `-n`    | Số lượng token cần sinh | `-n 100` |
| `-c`    | Kích thước ngữ cảnh | `-c 4096` |
| `-t`    | Số luồng | `-t 8` |
| `-ngl`  | Lớp GPU | `-ngl 32` |
| `-temp` | Nhiệt độ | `-temp 0.7` |

### Chế độ tương tác

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Tính năng nâng cao

### API máy chủ

#### Khởi động máy chủ
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Sử dụng API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Tối ưu hóa hiệu suất

#### Quản lý bộ nhớ
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Đa luồng
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Tăng tốc GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Tích hợp Python

### Sử dụng cơ bản với llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Giao diện trò chuyện

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Phản hồi dạng luồng

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Tích hợp với LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Khắc phục sự cố

### Các vấn đề phổ biến và giải pháp

#### Lỗi xây dựng

**Vấn đề: Không tìm thấy CMake**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Vấn đề: Không tìm thấy trình biên dịch**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Vấn đề khi chạy

**Vấn đề: Tải mô hình thất bại**
- Kiểm tra đường dẫn tệp mô hình
- Kiểm tra quyền truy cập tệp
- Đảm bảo đủ RAM
- Thử các mức lượng tử hóa khác nhau

**Vấn đề: Hiệu suất kém**
- Bật tăng tốc phần cứng
- Tăng số lượng luồng
- Sử dụng mức lượng tử hóa phù hợp
- Kiểm tra sử dụng bộ nhớ GPU

#### Vấn đề bộ nhớ

**Vấn đề: Hết bộ nhớ**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Vấn đề cụ thể theo nền tảng

#### Windows
- Sử dụng trình biên dịch MinGW hoặc Visual Studio
- Đảm bảo cấu hình PATH đúng
- Kiểm tra sự can thiệp của phần mềm diệt virus

#### macOS
- Bật Metal cho Apple Silicon
- Sử dụng Rosetta 2 để tương thích nếu cần
- Kiểm tra công cụ dòng lệnh Xcode

#### Linux
- Cài đặt các gói phát triển
- Kiểm tra phiên bản trình điều khiển GPU
- Xác minh cài đặt bộ công cụ CUDA

## Thực hành tốt nhất

### Lựa chọn mô hình
1. **Chọn mức lượng tử hóa phù hợp** với phần cứng của bạn
2. **Cân nhắc kích thước mô hình** so với chất lượng
3. **Thử nghiệm các mô hình khác nhau** cho trường hợp sử dụng cụ thể của bạn

### Tối ưu hóa hiệu suất
1. **Sử dụng tăng tốc GPU** khi có thể
2. **Tối ưu hóa số lượng luồng** cho CPU của bạn
3. **Đặt kích thước ngữ cảnh phù hợp** với trường hợp sử dụng
4. **Bật ánh xạ bộ nhớ** cho các mô hình lớn

### Triển khai sản xuất
1. **Sử dụng chế độ máy chủ** để truy cập API
2. **Triển khai xử lý lỗi đúng cách**
3. **Theo dõi sử dụng tài nguyên**
4. **Thiết lập ghi nhật ký và giám sát**

### Quy trình phát triển
1. **Bắt đầu với các mô hình nhỏ hơn** để thử nghiệm
2. **Sử dụng kiểm soát phiên bản** cho cấu hình mô hình
3. **Ghi lại cấu hình của bạn**
4. **Thử nghiệm trên các nền tảng khác nhau**

### Cân nhắc về bảo mật
1. **Xác thực các gợi ý đầu vào**
2. **Triển khai giới hạn tốc độ**
3. **Bảo mật các điểm cuối API**
4. **Theo dõi các mẫu lạm dụng**

## Kết luận

Llama.cpp cung cấp một cách mạnh mẽ và hiệu quả để chạy các mô hình ngôn ngữ lớn cục bộ trên nhiều cấu hình phần cứng khác nhau. Dù bạn đang phát triển ứng dụng AI, nghiên cứu, hay chỉ đơn giản là thử nghiệm với LLMs, khung này mang lại sự linh hoạt và hiệu suất cần thiết cho nhiều trường hợp sử dụng.

Những điểm chính:
- Chọn phương pháp cài đặt phù hợp nhất với nhu cầu của bạn
- Tối ưu hóa cho cấu hình phần cứng cụ thể của bạn
- Bắt đầu với sử dụng cơ bản và dần dần khám phá các tính năng nâng cao
- Cân nhắc sử dụng các binding Python để tích hợp dễ dàng hơn
- Tuân thủ các thực hành tốt nhất khi triển khai sản xuất

Để biết thêm thông tin và cập nhật, hãy truy cập [kho lưu trữ chính thức của Llama.cpp](https://github.com/ggml-org/llama.cpp) và tham khảo tài liệu toàn diện cùng các tài nguyên cộng đồng có sẵn.

## ➡️ Tiếp theo

- [03: Bộ Tối Ưu Hóa Microsoft Olive](./03.MicrosoftOlive.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, chúng tôi khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.