<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T12:51:35+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "vi"
}
-->
# Mục 3: Bộ công cụ tối ưu hóa Microsoft Olive

## Mục lục
1. [Giới thiệu](../../../Module04)
2. [Microsoft Olive là gì?](../../../Module04)
3. [Cài đặt](../../../Module04)
4. [Hướng dẫn bắt đầu nhanh](../../../Module04)
5. [Ví dụ: Chuyển đổi Qwen3 sang ONNX INT4](../../../Module04)
6. [Sử dụng nâng cao](../../../Module04)
7. [Thực hành tốt nhất](../../../Module04)
8. [Khắc phục sự cố](../../../Module04)
9. [Tài nguyên bổ sung](../../../Module04)

## Giới thiệu

Microsoft Olive là một bộ công cụ tối ưu hóa mô hình mạnh mẽ, dễ sử dụng, được thiết kế để tối ưu hóa mô hình học máy cho việc triển khai trên các nền tảng phần cứng khác nhau. Dù bạn đang nhắm đến CPU, GPU hay các bộ tăng tốc AI chuyên dụng, Olive giúp bạn đạt được hiệu suất tối ưu mà vẫn duy trì độ chính xác của mô hình.

## Microsoft Olive là gì?

Olive là một công cụ tối ưu hóa mô hình dễ sử dụng, có khả năng nhận biết phần cứng, kết hợp các kỹ thuật hàng đầu trong ngành về nén mô hình, tối ưu hóa và biên dịch. Nó hoạt động với ONNX Runtime như một giải pháp tối ưu hóa suy luận từ đầu đến cuối.

### Các tính năng chính

- **Tối ưu hóa nhận biết phần cứng**: Tự động chọn các kỹ thuật tối ưu hóa tốt nhất cho phần cứng mục tiêu của bạn
- **Hơn 40 thành phần tối ưu hóa tích hợp**: Bao gồm nén mô hình, lượng tử hóa, tối ưu hóa đồ thị và nhiều hơn nữa
- **Giao diện CLI dễ sử dụng**: Các lệnh đơn giản cho các tác vụ tối ưu hóa phổ biến
- **Hỗ trợ đa khung framework**: Hoạt động với PyTorch, mô hình Hugging Face và ONNX
- **Hỗ trợ mô hình phổ biến**: Olive có thể tự động tối ưu hóa các kiến trúc mô hình phổ biến như Llama, Phi, Qwen, Gemma, v.v. ngay từ đầu

### Lợi ích

- **Giảm thời gian phát triển**: Không cần thử nghiệm thủ công với các kỹ thuật tối ưu hóa khác nhau
- **Cải thiện hiệu suất**: Tăng tốc đáng kể (lên đến 6 lần trong một số trường hợp)
- **Triển khai đa nền tảng**: Các mô hình được tối ưu hóa hoạt động trên các phần cứng và hệ điều hành khác nhau
- **Duy trì độ chính xác**: Các tối ưu hóa bảo toàn chất lượng mô hình trong khi cải thiện hiệu suất

## Cài đặt

### Yêu cầu trước

- Python 3.8 trở lên
- Trình quản lý gói pip
- Môi trường ảo (khuyến nghị)

### Cài đặt cơ bản

Tạo và kích hoạt môi trường ảo:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Cài đặt Olive với các tính năng tối ưu hóa tự động:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Các phụ thuộc tùy chọn

Olive cung cấp các phụ thuộc tùy chọn cho các tính năng bổ sung:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Xác minh cài đặt

```bash
olive --help
```

Nếu thành công, bạn sẽ thấy thông báo trợ giúp CLI của Olive.

## Hướng dẫn bắt đầu nhanh

### Tối ưu hóa đầu tiên của bạn

Hãy tối ưu hóa một mô hình ngôn ngữ nhỏ bằng tính năng tối ưu hóa tự động của Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Lệnh này làm gì

Quá trình tối ưu hóa bao gồm: lấy mô hình từ bộ nhớ cache cục bộ, chụp đồ thị ONNX và lưu trọng số vào tệp dữ liệu ONNX, tối ưu hóa đồ thị ONNX, và lượng tử hóa mô hình sang int4 bằng phương pháp RTN.

### Giải thích các tham số lệnh

- `--model_name_or_path`: Định danh mô hình Hugging Face hoặc đường dẫn cục bộ
- `--output_path`: Thư mục nơi mô hình được tối ưu hóa sẽ được lưu
- `--device`: Thiết bị mục tiêu (cpu, gpu)
- `--provider`: Nhà cung cấp thực thi (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Sử dụng ONNX Runtime Generate AI cho suy luận
- `--precision`: Độ chính xác lượng tử hóa (int4, int8, fp16)
- `--log_level`: Mức độ chi tiết của nhật ký (0=tối thiểu, 1=chi tiết)

## Ví dụ: Chuyển đổi Qwen3 sang ONNX INT4

Dựa trên ví dụ Hugging Face được cung cấp tại [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), đây là cách tối ưu hóa mô hình Qwen3:

### Bước 1: Tải xuống mô hình (Tùy chọn)

Để giảm thời gian tải xuống, chỉ lưu vào bộ nhớ cache các tệp cần thiết:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Bước 2: Tối ưu hóa mô hình Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Bước 3: Kiểm tra mô hình đã tối ưu hóa

Tạo một tập lệnh Python đơn giản để kiểm tra mô hình đã tối ưu hóa của bạn:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Cấu trúc đầu ra

Sau khi tối ưu hóa, thư mục đầu ra của bạn sẽ chứa:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Sử dụng nâng cao

### Tệp cấu hình

Đối với các quy trình tối ưu hóa phức tạp hơn, bạn có thể sử dụng tệp cấu hình JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Chạy với cấu hình:

```bash
olive run --config config.json
```

### Tối ưu hóa GPU

Đối với tối ưu hóa GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Đối với DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Tinh chỉnh với Olive

Olive cũng hỗ trợ tinh chỉnh mô hình:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Thực hành tốt nhất

### 1. Lựa chọn mô hình
- Bắt đầu với các mô hình nhỏ để thử nghiệm (ví dụ: 0.5B-7B tham số)
- Đảm bảo kiến trúc mô hình mục tiêu của bạn được Olive hỗ trợ

### 2. Cân nhắc phần cứng
- Khớp mục tiêu tối ưu hóa của bạn với phần cứng triển khai
- Sử dụng tối ưu hóa GPU nếu bạn có phần cứng tương thích CUDA
- Cân nhắc DirectML cho máy Windows với đồ họa tích hợp

### 3. Lựa chọn độ chính xác
- **INT4**: Nén tối đa, mất độ chính xác nhẹ
- **INT8**: Cân bằng tốt giữa kích thước và độ chính xác
- **FP16**: Mất độ chính xác tối thiểu, giảm kích thước vừa phải

### 4. Kiểm tra và xác thực
- Luôn kiểm tra các mô hình đã tối ưu hóa với các trường hợp sử dụng cụ thể của bạn
- So sánh các chỉ số hiệu suất (độ trễ, thông lượng, độ chính xác)
- Sử dụng dữ liệu đầu vào đại diện để đánh giá

### 5. Tối ưu hóa lặp lại
- Bắt đầu với tối ưu hóa tự động để có kết quả nhanh
- Sử dụng tệp cấu hình để kiểm soát chi tiết
- Thử nghiệm với các lần tối ưu hóa khác nhau

## Khắc phục sự cố

### Các vấn đề phổ biến

#### 1. Vấn đề cài đặt
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Vấn đề CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Vấn đề bộ nhớ
- Sử dụng kích thước batch nhỏ hơn trong quá trình tối ưu hóa
- Thử lượng tử hóa với độ chính xác cao hơn trước (int8 thay vì int4)
- Đảm bảo đủ dung lượng đĩa để lưu vào bộ nhớ cache mô hình

#### 4. Lỗi tải mô hình
- Xác minh đường dẫn mô hình và quyền truy cập
- Kiểm tra xem mô hình có yêu cầu `trust_remote_code=True` không
- Đảm bảo tất cả các tệp mô hình cần thiết đã được tải xuống

### Nhận trợ giúp

- **Tài liệu**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Vấn đề trên GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Ví dụ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Tài nguyên bổ sung

### Liên kết chính thức
- **Kho GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Tài liệu ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Ví dụ Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Ví dụ cộng đồng
- **Jupyter Notebooks**: Có sẵn trong kho GitHub của Olive
- **Tiện ích mở rộng VS Code**: Tiện ích AI Toolkit sử dụng Olive để tối ưu hóa mô hình
- **Bài viết blog**: Blog Microsoft Open Source có các hướng dẫn chi tiết về Olive

### Công cụ liên quan
- **ONNX Runtime**: Công cụ suy luận hiệu suất cao
- **Hugging Face Transformers**: Nguồn của nhiều mô hình tương thích
- **Azure Machine Learning**: Quy trình tối ưu hóa dựa trên đám mây

## ➡️ Tiếp theo

- [04: Bộ công cụ tối ưu hóa OpenVINO Toolkit](./04.openvino.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.