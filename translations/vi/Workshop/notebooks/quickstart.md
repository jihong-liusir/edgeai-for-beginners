<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T17:07:19+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "vi"
}
-->
# S·ªï Tay Workshop - H∆∞·ªõng D·∫´n Nhanh

## M·ª•c L·ª•c

- [Y√™u C·∫ßu Tr∆∞·ªõc](../../../../Workshop/notebooks)
- [C√†i ƒê·∫∑t Ban ƒê·∫ßu](../../../../Workshop/notebooks)
- [Phi√™n 04: So S√°nh M√¥ H√¨nh](../../../../Workshop/notebooks)
- [Phi√™n 05: ƒêi·ªÅu Ph·ªëi ƒêa T√°c Nh√¢n](../../../../Workshop/notebooks)
- [Phi√™n 06: ƒê·ªãnh Tuy·∫øn M√¥ H√¨nh D·ª±a Tr√™n √ù ƒê·ªãnh](../../../../Workshop/notebooks)
- [Bi·∫øn M√¥i Tr∆∞·ªùng](../../../../Workshop/notebooks)
- [L·ªánh Th√¥ng D·ª•ng](../../../../Workshop/notebooks)

---

## Y√™u C·∫ßu Tr∆∞·ªõc

### 1. C√†i ƒê·∫∑t Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**X√°c Minh C√†i ƒê·∫∑t:**
```bash
foundry --version
```

### 2. C√†i ƒê·∫∑t C√°c Th∆∞ Vi·ªán Python

```bash
cd Workshop
pip install -r requirements.txt
```

Ho·∫∑c c√†i ƒë·∫∑t t·ª´ng th∆∞ vi·ªán ri√™ng l·∫ª:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## C√†i ƒê·∫∑t Ban ƒê·∫ßu

### Kh·ªüi ƒê·ªông D·ªãch V·ª• Foundry Local

**B·∫Øt bu·ªôc tr∆∞·ªõc khi ch·∫°y b·∫•t k·ª≥ s·ªï tay n√†o:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### T·∫£i Xu·ªëng v√† N·∫°p M√¥ H√¨nh

C√°c s·ªï tay s·ª≠ d·ª•ng c√°c m√¥ h√¨nh sau theo m·∫∑c ƒë·ªãnh:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### X√°c Minh C√†i ƒê·∫∑t

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Phi√™n 04: So S√°nh M√¥ H√¨nh

### M·ª•c ƒê√≠ch
So s√°nh hi·ªáu su·∫•t gi·ªØa M√¥ H√¨nh Ng√¥n Ng·ªØ Nh·ªè (SLM) v√† M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn (LLM).

### C√†i ƒê·∫∑t Nhanh

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Ch·∫°y S·ªï Tay

1. **M·ªü** `session04_model_compare.ipynb` trong VS Code ho·∫∑c Jupyter
2. **Kh·ªüi ƒë·ªông l·∫°i kernel** (Kernel ‚Üí Restart Kernel)
3. **Ch·∫°y t·∫•t c·∫£ c√°c √¥ l·ªánh** theo th·ª© t·ª±

### C·∫•u H√¨nh Ch√≠nh

**M√¥ H√¨nh M·∫∑c ƒê·ªãnh:**
- **SLM:** `phi-4-mini` (~4GB RAM, nhanh h∆°n)
- **LLM:** `qwen2.5-3b` (~3GB RAM, t·ªëi ∆∞u b·ªô nh·ªõ)

**Bi·∫øn M√¥i Tr∆∞·ªùng (t√πy ch·ªçn):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### K·∫øt Qu·∫£ Mong ƒê·ª£i

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### T√πy Ch·ªânh

**S·ª≠ d·ª•ng m√¥ h√¨nh kh√°c:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**T√πy ch·ªânh prompt:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Danh S√°ch Ki·ªÉm Tra X√°c Minh

- [ ] √î 12 hi·ªÉn th·ªã ƒë√∫ng m√¥ h√¨nh (phi-4-mini, qwen2.5-3b)
- [ ] √î 12 hi·ªÉn th·ªã ƒë√∫ng endpoint (c·ªïng 59959)
- [ ] √î 16 ki·ªÉm tra ch·∫©n ƒëo√°n th√†nh c√¥ng (‚úÖ D·ªãch v·ª• ƒëang ch·∫°y)
- [ ] √î 20 ki·ªÉm tra tr∆∞·ªõc khi ch·∫°y th√†nh c√¥ng (c·∫£ hai m√¥ h√¨nh ƒë·ªÅu ·ªïn)
- [ ] √î 22 so s√°nh ho√†n t·∫•t v·ªõi c√°c gi√° tr·ªã ƒë·ªô tr·ªÖ
- [ ] √î 24 x√°c minh hi·ªÉn th·ªã üéâ T·∫§T C·∫¢ KI·ªÇM TRA ƒê√É TH√ÄNH C√îNG!

### Th·ªùi Gian ∆Ø·ªõc T√≠nh
- **L·∫ßn ch·∫°y ƒë·∫ßu ti√™n:** 5-10 ph√∫t (bao g·ªìm t·∫£i xu·ªëng m√¥ h√¨nh)
- **C√°c l·∫ßn ch·∫°y sau:** 1-2 ph√∫t

---

## Phi√™n 05: ƒêi·ªÅu Ph·ªëi ƒêa T√°c Nh√¢n

### M·ª•c ƒê√≠ch
Tr√¨nh di·ªÖn s·ª± h·ª£p t√°c gi·ªØa c√°c t√°c nh√¢n ƒëa nhi·ªám s·ª≠ d·ª•ng Foundry Local SDK - c√°c t√°c nh√¢n l√†m vi·ªác c√πng nhau ƒë·ªÉ t·∫°o ra k·∫øt qu·∫£ tinh ch·ªânh.

### C√†i ƒê·∫∑t Nhanh

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Ch·∫°y S·ªï Tay

1. **M·ªü** `session05_agents_orchestrator.ipynb`
2. **Kh·ªüi ƒë·ªông l·∫°i kernel**
3. **Ch·∫°y t·∫•t c·∫£ c√°c √¥ l·ªánh** theo th·ª© t·ª±

### C·∫•u H√¨nh Ch√≠nh

**C√†i ƒê·∫∑t M·∫∑c ƒê·ªãnh (C√πng M√¥ H√¨nh Cho C·∫£ Hai T√°c Nh√¢n):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**C√†i ƒê·∫∑t N√¢ng Cao (M√¥ H√¨nh Kh√°c Nhau):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Ki·∫øn Tr√∫c

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### K·∫øt Qu·∫£ Mong ƒê·ª£i

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### M·ªü R·ªông

**Th√™m nhi·ªÅu t√°c nh√¢n h∆°n:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Ki·ªÉm tra theo l√¥:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Th·ªùi Gian ∆Ø·ªõc T√≠nh
- **L·∫ßn ch·∫°y ƒë·∫ßu ti√™n:** 3-5 ph√∫t
- **C√°c l·∫ßn ch·∫°y sau:** 1-2 ph√∫t m·ªói c√¢u h·ªèi

---

## Phi√™n 06: ƒê·ªãnh Tuy·∫øn M√¥ H√¨nh D·ª±a Tr√™n √ù ƒê·ªãnh

### M·ª•c ƒê√≠ch
ƒê·ªãnh tuy·∫øn th√¥ng minh c√°c prompt ƒë·∫øn c√°c m√¥ h√¨nh chuy√™n bi·ªát d·ª±a tr√™n √Ω ƒë·ªãnh ƒë∆∞·ª£c ph√°t hi·ªán.

### C√†i ƒê·∫∑t Nhanh

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**L∆∞u √Ω:** Phi√™n 06 m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng c√°c m√¥ h√¨nh CPU ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch t·ªëi ƒëa.

### Ch·∫°y S·ªï Tay

1. **M·ªü** `session06_models_router.ipynb`
2. **Kh·ªüi ƒë·ªông l·∫°i kernel**
3. **Ch·∫°y t·∫•t c·∫£ c√°c √¥ l·ªánh** theo th·ª© t·ª±

### C·∫•u H√¨nh Ch√≠nh

**Danh M·ª•c M·∫∑c ƒê·ªãnh (M√¥ H√¨nh CPU):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Thay Th·∫ø (M√¥ H√¨nh GPU):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Ph√°t Hi·ªán √ù ƒê·ªãnh

B·ªô ƒë·ªãnh tuy·∫øn s·ª≠ d·ª•ng c√°c m·∫´u regex ƒë·ªÉ ph√°t hi·ªán √Ω ƒë·ªãnh:

| √ù ƒê·ªãnh | V√≠ D·ª• M·∫´u | ƒê·ªãnh Tuy·∫øn ƒê·∫øn |
|--------|-----------------|-----------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | T·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p kh√°c | phi-4-mini-cpu |

### K·∫øt Qu·∫£ Mong ƒê·ª£i

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### T√πy Ch·ªânh

**Th√™m √Ω ƒë·ªãnh t√πy ch·ªânh:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**B·∫≠t theo d√µi token:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### Chuy·ªÉn Sang M√¥ H√¨nh GPU

N·∫øu b·∫°n c√≥ VRAM 8GB+:

1. Trong **√î #6**, b√¨nh lu·∫≠n danh m·ª•c CPU
2. B·ªè b√¨nh lu·∫≠n danh m·ª•c GPU
3. T·∫£i m√¥ h√¨nh GPU:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Kh·ªüi ƒë·ªông l·∫°i kernel v√† ch·∫°y l·∫°i s·ªï tay

### Th·ªùi Gian ∆Ø·ªõc T√≠nh
- **L·∫ßn ch·∫°y ƒë·∫ßu ti√™n:** 5-10 ph√∫t (t·∫£i m√¥ h√¨nh)
- **C√°c l·∫ßn ch·∫°y sau:** 30-60 gi√¢y m·ªói l·∫ßn ki·ªÉm tra

---

## Bi·∫øn M√¥i Tr∆∞·ªùng

### C·∫•u H√¨nh To√†n C·ª•c

Thi·∫øt l·∫≠p tr∆∞·ªõc khi kh·ªüi ƒë·ªông Jupyter/VS Code:

**Windows (Command Prompt):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### C·∫•u H√¨nh Trong S·ªï Tay

Thi·∫øt l·∫≠p ·ªü ƒë·∫ßu b·∫•t k·ª≥ s·ªï tay n√†o:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## L·ªánh Th√¥ng D·ª•ng

### Qu·∫£n L√Ω D·ªãch V·ª•

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Qu·∫£n L√Ω M√¥ H√¨nh

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Ki·ªÉm Tra Endpoint

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### L·ªánh Ch·∫©n ƒêo√°n

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Th·ª±c H√†nh T·ªët Nh·∫•t

### Tr∆∞·ªõc Khi B·∫Øt ƒê·∫ßu B·∫•t K·ª≥ S·ªï Tay N√†o

1. **Ki·ªÉm tra d·ªãch v·ª• ƒëang ch·∫°y:**
   ```bash
   foundry service status
   ```

2. **X√°c minh c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c n·∫°p:**
   ```bash
   foundry model ls
   ```

3. **Kh·ªüi ƒë·ªông l·∫°i kernel c·ªßa s·ªï tay** n·∫øu ch·∫°y l·∫°i

4. **X√≥a t·∫•t c·∫£ k·∫øt qu·∫£ ƒë·∫ßu ra** ƒë·ªÉ ch·∫°y s·∫°ch

### Qu·∫£n L√Ω T√†i Nguy√™n

1. **S·ª≠ d·ª•ng m√¥ h√¨nh CPU theo m·∫∑c ƒë·ªãnh** ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch
2. **Chuy·ªÉn sang m√¥ h√¨nh GPU** ch·ªâ khi b·∫°n c√≥ VRAM 8GB+
3. **ƒê√≥ng c√°c ·ª©ng d·ª•ng GPU kh√°c** tr∆∞·ªõc khi ch·∫°y
4. **Gi·ªØ d·ªãch v·ª• ch·∫°y** gi·ªØa c√°c phi√™n s·ªï tay
5. **Theo d√µi s·ª≠ d·ª•ng t√†i nguy√™n** b·∫±ng Task Manager / nvidia-smi

### X·ª≠ L√Ω S·ª± C·ªë

1. **Lu√¥n ki·ªÉm tra d·ªãch v·ª• tr∆∞·ªõc** khi g·ª° l·ªói m√£
2. **Kh·ªüi ƒë·ªông l·∫°i kernel** n·∫øu b·∫°n th·∫•y c·∫•u h√¨nh c≈©
3. **Ch·∫°y l·∫°i c√°c √¥ ch·∫©n ƒëo√°n** sau b·∫•t k·ª≥ thay ƒë·ªïi n√†o
4. **Ki·ªÉm tra t√™n m√¥ h√¨nh** kh·ªõp v·ªõi nh·ªØng g√¨ ƒë√£ n·∫°p
5. **X√°c minh c·ªïng endpoint** kh·ªõp v·ªõi tr·∫°ng th√°i d·ªãch v·ª•

---

## Tham Kh·∫£o Nhanh: B√≠ Danh M√¥ H√¨nh

### C√°c M√¥ H√¨nh Th√¥ng D·ª•ng

| B√≠ Danh | K√≠ch Th∆∞·ªõc | T·ªët Nh·∫•t Cho | RAM/VRAM | Bi·∫øn Th·ªÉ |
|-------|------|----------|----------|----------|
| `phi-4-mini` | ~4B | Chat t·ªïng qu√°t, t√≥m t·∫Øt | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Sinh m√£, t√°i c·∫•u tr√∫c | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Nhi·ªám v·ª• t·ªïng qu√°t, hi·ªáu qu·∫£ | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Nhanh, √≠t t√†i nguy√™n | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Ph√¢n lo·∫°i, √≠t t√†i nguy√™n | 1-2GB | `-cpu`, `-cuda-gpu` |

### Quy ∆Ø·ªõc ƒê·∫∑t T√™n Bi·∫øn Th·ªÉ

- **T√™n c∆° b·∫£n** (v√≠ d·ª•: `phi-4-mini`): T·ª± ƒë·ªông ch·ªçn bi·∫øn th·ªÉ t·ªët nh·∫•t cho ph·∫ßn c·ª©ng c·ªßa b·∫°n
- **`-cpu`**: T·ªëi ∆∞u h√≥a cho CPU, ho·∫°t ƒë·ªông ·ªü m·ªçi n∆°i
- **`-cuda-gpu`**: T·ªëi ∆∞u h√≥a cho GPU NVIDIA, y√™u c·∫ßu VRAM 8GB+
- **`-npu`**: T·ªëi ∆∞u h√≥a cho Qualcomm NPU, y√™u c·∫ßu tr√¨nh ƒëi·ªÅu khi·ªÉn NPU

**Khuy·∫øn ngh·ªã:** S·ª≠ d·ª•ng t√™n c∆° b·∫£n (kh√¥ng c√≥ h·∫≠u t·ªë) v√† ƒë·ªÉ Foundry Local t·ª± ƒë·ªông ch·ªçn bi·∫øn th·ªÉ t·ªët nh·∫•t.

---

## Ch·ªâ S·ªë Th√†nh C√¥ng

B·∫°n ƒë√£ s·∫µn s√†ng khi th·∫•y:

‚úÖ `foundry service status` hi·ªÉn th·ªã "running"  
‚úÖ `foundry model ls` hi·ªÉn th·ªã c√°c m√¥ h√¨nh b·∫°n c·∫ßn  
‚úÖ D·ªãch v·ª• truy c·∫≠p ƒë∆∞·ª£c t·∫°i endpoint ch√≠nh x√°c  
‚úÖ Ki·ªÉm tra s·ª©c kh·ªèe tr·∫£ v·ªÅ 200 OK  
‚úÖ C√°c √¥ ch·∫©n ƒëo√°n trong s·ªï tay th√†nh c√¥ng  
‚úÖ Kh√¥ng c√≥ l·ªói k·∫øt n·ªëi trong k·∫øt qu·∫£ ƒë·∫ßu ra  

---

## Nh·∫≠n Tr·ª£ Gi√∫p

### T√†i Li·ªáu
- **Kho Ch√≠nh**: https://github.com/microsoft/Foundry-Local  
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python  
- **Tham Kh·∫£o CLI**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md  
- **X·ª≠ L√Ω S·ª± C·ªë**: Xem `troubleshooting.md` trong th∆∞ m·ª•c n√†y  

### V·∫•n ƒê·ªÅ GitHub
- https://github.com/microsoft/Foundry-Local/issues  
- https://github.com/microsoft/edgeai-for-beginners/issues  

---

**C·∫≠p Nh·∫≠t L·∫ßn Cu·ªëi:** 8 Th√°ng 10, 2025  
**Phi√™n B·∫£n:** Workshop Notebooks 2.0  

---

**Tuy√™n b·ªë mi·ªÖn tr·ª´ tr√°ch nhi·ªám**:  
T√†i li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c d·ªãch b·∫±ng d·ªãch v·ª• d·ªãch thu·∫≠t AI [Co-op Translator](https://github.com/Azure/co-op-translator). M·∫∑c d√π ch√∫ng t√¥i c·ªë g·∫Øng ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c, xin l∆∞u √Ω r·∫±ng c√°c b·∫£n d·ªãch t·ª± ƒë·ªông c√≥ th·ªÉ ch·ª©a l·ªói ho·∫∑c kh√¥ng ch√≠nh x√°c. T√†i li·ªáu g·ªëc b·∫±ng ng√¥n ng·ªØ b·∫£n ƒë·ªãa n√™n ƒë∆∞·ª£c coi l√† ngu·ªìn th√¥ng tin ch√≠nh th·ª©c. ƒê·ªëi v·ªõi c√°c th√¥ng tin quan tr·ªçng, khuy·∫øn ngh·ªã s·ª≠ d·ª•ng d·ªãch v·ª• d·ªãch thu·∫≠t chuy√™n nghi·ªáp b·ªüi con ng∆∞·ªùi. Ch√∫ng t√¥i kh√¥ng ch·ªãu tr√°ch nhi·ªám cho b·∫•t k·ª≥ s·ª± hi·ªÉu l·∫ßm ho·∫∑c di·ªÖn gi·∫£i sai n√†o ph√°t sinh t·ª´ vi·ªác s·ª≠ d·ª•ng b·∫£n d·ªãch n√†y.