<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T07:30:35+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "th"
}
-->
# ส่วนที่ 03 - การผสานรวม Model Context Protocol (MCP)

## บทนำสู่ MCP (Model Context Protocol)

Model Context Protocol (MCP) เป็นกรอบการทำงานที่ปฏิวัติวงการ ซึ่งช่วยให้โมเดลภาษาสามารถโต้ตอบกับเครื่องมือและระบบภายนอกได้ในรูปแบบมาตรฐาน แตกต่างจากวิธีการแบบดั้งเดิมที่โมเดลถูกแยกออกจากกัน MCP สร้างสะพานเชื่อมระหว่างโมเดล AI และโลกจริงผ่านโปรโตคอลที่กำหนดไว้อย่างชัดเจน

### MCP คืออะไร?

MCP ทำหน้าที่เป็นโปรโตคอลการสื่อสารที่ช่วยให้โมเดลภาษา:
- เชื่อมต่อกับแหล่งข้อมูลภายนอก
- เรียกใช้เครื่องมือและฟังก์ชัน
- โต้ตอบกับ API และบริการต่างๆ
- เข้าถึงข้อมูลแบบเรียลไทม์
- ดำเนินการที่ซับซ้อนหลายขั้นตอน

โปรโตคอลนี้เปลี่ยนโมเดลภาษาที่นิ่งเฉยให้กลายเป็นตัวแทนที่มีความสามารถในการทำงานจริงที่มากกว่าการสร้างข้อความ

## Small Language Models (SLMs) ใน MCP

Small Language Models เป็นแนวทางที่มีประสิทธิภาพในการใช้งาน AI โดยมีข้อดีหลายประการ:

### ข้อดีของ SLMs
- **ประหยัดทรัพยากร**: ต้องการการประมวลผลน้อยลง
- **ตอบสนองเร็วขึ้น**: ลดความล่าช้าสำหรับการใช้งานแบบเรียลไทม์  
- **คุ้มค่า**: ต้องการโครงสร้างพื้นฐานน้อย
- **ความเป็นส่วนตัว**: สามารถทำงานในเครื่องโดยไม่ต้องส่งข้อมูลออกไป
- **ปรับแต่งได้ง่าย**: ปรับแต่งให้เหมาะกับโดเมนเฉพาะได้ง่ายขึ้น

### เหตุผลที่ SLMs ทำงานได้ดีร่วมกับ MCP

SLMs เมื่อจับคู่กับ MCP จะสร้างการผสมผสานที่ทรงพลัง โดยความสามารถในการวิเคราะห์ของโมเดลจะได้รับการเสริมด้วยเครื่องมือภายนอก ชดเชยจำนวนพารามิเตอร์ที่น้อยลงด้วยฟังก์ชันที่เพิ่มขึ้น

## ภาพรวม Python MCP SDK

Python MCP SDK เป็นพื้นฐานสำหรับการสร้างแอปพลิเคชันที่รองรับ MCP SDK ประกอบด้วย:

- **ไลบรารีลูกค้า**: สำหรับการเชื่อมต่อกับเซิร์ฟเวอร์ MCP
- **กรอบงานเซิร์ฟเวอร์**: สำหรับการสร้างเซิร์ฟเวอร์ MCP แบบกำหนดเอง
- **ตัวจัดการโปรโตคอล**: สำหรับการจัดการการสื่อสาร
- **การผสานรวมเครื่องมือ**: สำหรับการเรียกใช้ฟังก์ชันภายนอก

## การใช้งานจริง: Phi-4 MCP Client

มาดูการใช้งานจริงโดยใช้โมเดลขนาดเล็ก Phi-4 ของ Microsoft ที่ผสานรวมความสามารถ MCP

### สถาปัตยกรรมระบบ

การใช้งานนี้ใช้สถาปัตยกรรมแบบเลเยอร์:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### ส่วนประกอบหลัก

#### 1. คลาส MCP Client

**BaseMCPClient**: พื้นฐานแบบนามธรรมที่ให้ฟังก์ชันทั่วไป
- โปรโตคอลตัวจัดการบริบทแบบอะซิงโครนัส
- นิยามอินเทอร์เฟซมาตรฐาน
- การจัดการทรัพยากร

**Phi4MiniMCPClient**: การใช้งานแบบ STDIO
- การสื่อสารกับกระบวนการในเครื่อง
- การจัดการอินพุต/เอาต์พุตมาตรฐาน
- การจัดการ subprocess

**Phi4MiniSSEMCPClient**: การใช้งาน Server-Sent Events
- การสื่อสารแบบสตรีม HTTP
- การจัดการเหตุการณ์แบบเรียลไทม์
- การเชื่อมต่อเซิร์ฟเวอร์ผ่านเว็บ

#### 2. การผสานรวม LLM

**OllamaClient**: การโฮสต์โมเดลในเครื่อง
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: การให้บริการที่มีประสิทธิภาพสูง
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. ท่อประมวลผลเครื่องมือ

ท่อประมวลผลเครื่องมือแปลง MCP tools ให้เข้ากันได้กับโมเดลภาษา:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## เริ่มต้นใช้งาน: คู่มือทีละขั้นตอน

### ขั้นตอนที่ 1: การตั้งค่าสภาพแวดล้อม

ติดตั้ง dependencies ที่จำเป็น:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### ขั้นตอนที่ 2: การตั้งค่าพื้นฐาน

ตั้งค่าตัวแปรสภาพแวดล้อม:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### ขั้นตอนที่ 3: การเรียกใช้ MCP Client ครั้งแรก

**การตั้งค่า Ollama แบบพื้นฐาน:**
```bash
python ghmodel_mcp_demo.py
```

**การใช้ vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**การเชื่อมต่อ Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**เซิร์ฟเวอร์ MCP แบบกำหนดเอง:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### ขั้นตอนที่ 4: การใช้งานแบบโปรแกรม

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## ฟีเจอร์ขั้นสูง

### การรองรับหลาย Backend

การใช้งานรองรับทั้ง Ollama และ vLLM backend ช่วยให้คุณเลือกได้ตามความต้องการ:

- **Ollama**: เหมาะสำหรับการพัฒนาและทดสอบในเครื่อง
- **vLLM**: ปรับแต่งสำหรับการใช้งานในระบบผลิตและสถานการณ์ที่ต้องการปริมาณงานสูง

### โปรโตคอลการเชื่อมต่อที่ยืดหยุ่น

รองรับโหมดการเชื่อมต่อสองแบบ:

**โหมด STDIO**: การสื่อสารกับกระบวนการโดยตรง
- ความล่าช้าต่ำ
- เหมาะสำหรับเครื่องมือในเครื่อง
- การตั้งค่าง่าย

**โหมด SSE**: การสตรีม HTTP
- รองรับเครือข่าย
- เหมาะสำหรับระบบกระจาย
- อัปเดตแบบเรียลไทม์

### ความสามารถในการผสานรวมเครื่องมือ

ระบบสามารถผสานรวมกับเครื่องมือหลากหลาย:
- การทำงานอัตโนมัติบนเว็บ (Playwright)
- การดำเนินการกับไฟล์
- การโต้ตอบกับ API
- คำสั่งระบบ
- ฟังก์ชันแบบกำหนดเอง

## การจัดการข้อผิดพลาดและแนวทางปฏิบัติที่ดีที่สุด

### การจัดการข้อผิดพลาดอย่างครอบคลุม

การใช้งานนี้มีการจัดการข้อผิดพลาดที่แข็งแกร่งสำหรับ:

**ข้อผิดพลาดการเชื่อมต่อ:**
- เซิร์ฟเวอร์ MCP ล้มเหลว
- การหมดเวลาของเครือข่าย
- ปัญหาการเชื่อมต่อ

**ข้อผิดพลาดการเรียกใช้เครื่องมือ:**
- เครื่องมือที่หายไป
- การตรวจสอบพารามิเตอร์
- การเรียกใช้ล้มเหลว

**ข้อผิดพลาดการประมวลผลการตอบกลับ:**
- ปัญหาการแปลง JSON
- ความไม่สอดคล้องของรูปแบบ
- ความผิดปกติของการตอบกลับ LLM

### แนวทางปฏิบัติที่ดีที่สุด

1. **การจัดการทรัพยากร**: ใช้ตัวจัดการบริบทแบบอะซิงโครนัส
2. **การจัดการข้อผิดพลาด**: ใช้บล็อก try-catch อย่างครอบคลุม
3. **การบันทึก**: เปิดใช้งานระดับการบันทึกที่เหมาะสม
4. **ความปลอดภัย**: ตรวจสอบความถูกต้องของอินพุตและทำความสะอาดเอาต์พุต
5. **ประสิทธิภาพ**: ใช้การ pooling การเชื่อมต่อและ caching

## การใช้งานในโลกจริง

### การทำงานอัตโนมัติบนเว็บ
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### การประมวลผลข้อมูล
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### การผสานรวม API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## การปรับปรุงประสิทธิภาพ

### การจัดการหน่วยความจำ
- การจัดการประวัติข้อความอย่างมีประสิทธิภาพ
- การทำความสะอาดทรัพยากรอย่างเหมาะสม
- การ pooling การเชื่อมต่อ

### การปรับปรุงเครือข่าย
- การดำเนินการ HTTP แบบอะซิงโครนัส
- การตั้งค่าการหมดเวลาที่กำหนดค่าได้
- การกู้คืนข้อผิดพลาดอย่างราบรื่น

### การประมวลผลพร้อมกัน
- I/O แบบไม่บล็อก
- การเรียกใช้เครื่องมือแบบขนาน
- รูปแบบอะซิงโครนัสที่มีประสิทธิภาพ

## การพิจารณาด้านความปลอดภัย

### การปกป้องข้อมูล
- การจัดการคีย์ API อย่างปลอดภัย
- การตรวจสอบความถูกต้องของอินพุต
- การทำความสะอาดเอาต์พุต

### ความปลอดภัยของเครือข่าย
- รองรับ HTTPS
- ค่าเริ่มต้นของ endpoint ในเครื่อง
- การจัดการโทเค็นอย่างปลอดภัย

### ความปลอดภัยในการดำเนินการ
- การกรองเครื่องมือ
- สภาพแวดล้อมที่มีการจำกัด
- การบันทึกการตรวจสอบ

## สรุป

SLMs ที่ผสานรวมกับ MCP เป็นการเปลี่ยนแปลงแนวทางการพัฒนาแอปพลิเคชัน AI โดยการรวมประสิทธิภาพของโมเดลขนาดเล็กเข้ากับพลังของเครื่องมือภายนอก นักพัฒนาสามารถสร้างระบบอัจฉริยะที่ทั้งประหยัดทรัพยากรและมีความสามารถสูง

การใช้งาน Phi-4 MCP client แสดงให้เห็นว่าการผสานรวมนี้สามารถทำได้ในทางปฏิบัติ โดยให้พื้นฐานที่มั่นคงสำหรับการสร้างแอปพลิเคชันที่ขับเคลื่อนด้วย AI ที่ซับซ้อน

ประเด็นสำคัญ:
- MCP สร้างสะพานเชื่อมระหว่างโมเดลภาษาและระบบภายนอก
- SLMs ให้ประสิทธิภาพโดยไม่ลดทอนความสามารถเมื่อเสริมด้วยเครื่องมือ
- สถาปัตยกรรมแบบโมดูลช่วยให้ขยายและปรับแต่งได้ง่าย
- การจัดการข้อผิดพลาดและมาตรการรักษาความปลอดภัยเป็นสิ่งสำคัญสำหรับการใช้งานในระบบผลิต

บทแนะนำนี้ให้พื้นฐานสำหรับการสร้างแอปพลิเคชัน MCP ที่ขับเคลื่อนด้วย SLM ของคุณเอง เปิดโอกาสสำหรับการทำงานอัตโนมัติ การประมวลผลข้อมูล และการผสานรวมระบบอัจฉริยะ

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้