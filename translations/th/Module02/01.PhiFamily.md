<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20892f7994d9e5d2473ad19dfa93cf6d",
  "translation_date": "2025-09-18T06:55:55+00:00",
  "source_file": "Module02/01.PhiFamily.md",
  "language_code": "th"
}
-->
# ส่วนที่ 1: พื้นฐานของตระกูลโมเดล Microsoft Phi

ตระกูลโมเดล Microsoft Phi เป็นการเปลี่ยนแปลงครั้งสำคัญในปัญญาประดิษฐ์ โดยแสดงให้เห็นว่าโมเดลที่มีขนาดเล็กและมีประสิทธิภาพสามารถให้ผลลัพธ์ที่ยอดเยี่ยมได้ ในขณะที่ใช้ทรัพยากรน้อยกว่ามากเมื่อเทียบกับโมเดลภาษาขนาดใหญ่แบบดั้งเดิม สิ่งสำคัญคือการเข้าใจว่าตระกูล Phi สามารถมอบความสามารถ AI ที่ทรงพลังได้อย่างไร โดยลดความต้องการด้านการประมวลผลลง แต่ยังคงรักษาประสิทธิภาพสูงในงานต่าง ๆ

## แหล่งข้อมูลสำหรับนักพัฒนา

### Azure AI Foundry Model Catalog
ตระกูลโมเดล Phi (ยกเว้น Phi-silica) สามารถเข้าถึงได้ผ่าน [Azure AI Foundry Model Catalog](https://ai.azure.com/explore/models?q=phi) ซึ่งช่วยให้นักพัฒนาสามารถเข้าถึง ปรับแต่ง และนำโมเดลเหล่านี้ไปใช้ในแอปพลิเคชันของตนได้อย่างง่ายดาย แคตตาล็อกนี้ช่วยให้ทดลองใช้โมเดล Phi แบบต่าง ๆ และผสานรวมเข้ากับโปรเจกต์ของคุณได้อย่างสะดวก

### Azure AI Foundry
คุณสามารถนำโมเดล Phi ไปใช้งานและทดลองได้ผ่าน [Azure AI Foundry](https://ai.azure.com) ซึ่งเป็นสภาพแวดล้อมที่ครอบคลุมสำหรับการสร้าง ทดสอบ และนำโซลูชัน AI ไปใช้งานโดยมีการตั้งค่าที่น้อยที่สุด

### Foundry Local
สำหรับการพัฒนาและการใช้งานในเครื่อง ลองดู [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) ซึ่งช่วยให้คุณสามารถรันโมเดล Phi บนเครื่องพัฒนาของคุณด้วยการตั้งค่าที่ปรับแต่งมาอย่างเหมาะสม

### แหล่งข้อมูลเอกสาร
- [Microsoft Research: Phi Model Technical Reports](https://ai.azure.com/labs/projects/phi-4)
- [Phi Cookbook](https://aka.ms/phicookbook)

## บทนำ

ในบทเรียนนี้ เราจะสำรวจตระกูลโมเดล Phi ของ Microsoft และแนวคิดพื้นฐาน เราจะครอบคลุมถึงวิวัฒนาการของตระกูล Phi วิธีการฝึกอบรมที่เป็นนวัตกรรมที่ทำให้โมเดล Phi มีประสิทธิภาพ ตัวแปรสำคัญในตระกูล และการใช้งานจริงในสถานการณ์ต่าง ๆ

## วัตถุประสงค์การเรียนรู้

เมื่อจบบทเรียนนี้ คุณจะสามารถ:

- เข้าใจปรัชญาการออกแบบและวิวัฒนาการของตระกูลโมเดล Phi ของ Microsoft
- ระบุนวัตกรรมสำคัญที่ช่วยให้โมเดล Phi มีประสิทธิภาพสูงด้วยจำนวนพารามิเตอร์ที่น้อยลง
- รับรู้ถึงข้อดีและข้อจำกัดของตัวแปรโมเดล Phi ต่าง ๆ
- ใช้ความรู้เกี่ยวกับโมเดล Phi เพื่อเลือกตัวแปรที่เหมาะสมสำหรับสถานการณ์จริง

## การทำความเข้าใจแนวทางโมเดล AI แบบดั้งเดิม

โดยทั่วไป การบรรลุประสิทธิภาพสูงในงานประมวลผลภาษาธรรมชาติต้องใช้โมเดลภาษาขนาดใหญ่ที่มีพารามิเตอร์นับพันล้านหรือหลายแสนล้าน องค์กรมักจะนำโมเดลเหล่านี้ไปใช้งานบนคลัสเตอร์ GPU ที่ทรงพลัง โดยเข้าถึงความสามารถของโมเดลผ่านอินเทอร์เฟซ API หรือโครงสร้างพื้นฐานฮาร์ดแวร์เฉพาะทาง

วิธีการนี้ทำงานได้ดีสำหรับแอปพลิเคชันหลายประเภท แต่มีข้อจำกัดโดยธรรมชาติเมื่อพูดถึงสถานการณ์การใช้งานจริง วิธีการแบบดั้งเดิมเกี่ยวข้องกับการใช้โมเดลที่ต้องการทรัพยากรการประมวลผลจำนวนมาก หน่วยความจำขนาดใหญ่ และการใช้พลังงานที่สูง แม้ว่าวิธีนี้จะให้การเข้าถึงความสามารถที่ล้ำสมัย แต่ก็สร้างการพึ่งพาฮาร์ดแวร์ราคาแพง เพิ่มต้นทุนการดำเนินงาน และจำกัดความยืดหยุ่นในการใช้งาน

## ความท้าทายของการใช้งาน AI อย่างมีประสิทธิภาพ

ความต้องการ AI ที่มีประสิทธิภาพมากขึ้นกลายเป็นสิ่งสำคัญในสถานการณ์ต่าง ๆ ลองพิจารณาแอปพลิเคชันที่ต้องการการใช้งานในเครื่องเพื่อเหตุผลด้านความเป็นส่วนตัว การใช้งานที่คำนึงถึงต้นทุนซึ่งค่าใช้จ่าย API บนคลาวด์กลายเป็นสิ่งที่ไม่สามารถรับได้ สถานการณ์การประมวลผลที่อุปกรณ์ปลายทางมีทรัพยากรฮาร์ดแวร์จำกัด หรือแอปพลิเคชันแบบเรียลไทม์ที่ความล่าช้าเป็นสิ่งสำคัญ

### ข้อจำกัดสำคัญในการใช้งาน

การใช้งานโมเดลขนาดใหญ่แบบดั้งเดิมมีข้อจำกัดพื้นฐานหลายประการที่จำกัดการใช้งานจริง:

- **ข้อจำกัดด้านต้นทุน**: ค่าใช้จ่ายในการประมวลผลสูงทำให้การใช้งานต่อเนื่องมีราคาแพงสำหรับองค์กรหลายแห่ง
- **ข้อจำกัดด้านทรัพยากร**: การเข้าถึงโครงสร้างพื้นฐาน GPU ระดับสูงที่จำกัดทำให้ตัวเลือกการใช้งานถูกจำกัด
- **ข้อกำหนดด้านความเป็นส่วนตัว**: แอปพลิเคชันที่มีความอ่อนไหวต้องการการประมวลผลในเครื่องเพื่อรักษาความเป็นส่วนตัวของข้อมูล
- **ความไวต่อความล่าช้า**: แอปพลิเคชันแบบเรียลไทม์ต้องการการตอบสนองทันทีโดยไม่มีความล่าช้าจากการส่งข้อมูลไปกลับบนคลาวด์

## ปรัชญาของโมเดล Microsoft Phi

ตระกูลโมเดล Microsoft Phi เป็นการเปลี่ยนแปลงพื้นฐานในปรัชญาการออกแบบโมเดล AI โดยให้ความสำคัญกับประสิทธิภาพและการใช้งานจริง ในขณะที่ยังคงรักษาลักษณะการทำงานที่แข็งแกร่ง โมเดล Phi บรรลุเป้าหมายนี้ผ่านสถาปัตยกรรมที่เป็นนวัตกรรม วิธีการฝึกอบรมคุณภาพสูง และเทคนิคการปรับแต่งเฉพาะทาง

ตระกูล Phi ครอบคลุมแนวทางต่าง ๆ ที่ออกแบบมาเพื่อเพิ่มประสิทธิภาพต่อพารามิเตอร์สูงสุด ช่วยให้สามารถใช้งานบนฮาร์ดแวร์มาตรฐานได้ ในขณะที่ยังคงมอบความสามารถ AI ที่มีความหมาย เป้าหมายคือการรักษาประสิทธิภาพที่แข่งขันได้ ในขณะที่ลดความต้องการด้านการประมวลผล การใช้หน่วยความจำ และต้นทุนการดำเนินงานลงอย่างมาก

### หลักการออกแบบ Phi ที่สำคัญ

โมเดล Phi ถูกสร้างขึ้นบนหลักการพื้นฐานหลายประการที่ทำให้แตกต่างจากโมเดลภาษาขนาดใหญ่แบบดั้งเดิม:

- **ประสิทธิภาพเป็นอันดับแรก**: ปรับแต่งเพื่อประสิทธิภาพสูงสุดต่อพารามิเตอร์แทนที่จะเน้นที่ขนาดโดยรวม
- **การฝึกอบรมคุณภาพสูง**: มุ่งเน้นที่ข้อมูลการฝึกอบรมที่มีคุณภาพสูงและคัดสรรมาอย่างดีแทนที่จะใช้ชุดข้อมูลขนาดใหญ่
- **ความยืดหยุ่นในการใช้งาน**: ออกแบบมาให้ทำงานได้อย่างมีประสิทธิภาพบนการกำหนดค่าฮาร์ดแวร์ที่หลากหลาย
- **ความสามารถเฉพาะทาง**: มักจะปรับแต่งเพื่อการใช้งานเฉพาะด้านหรือโดเมนเพื่อเพิ่มประสิทธิภาพสูงสุด

## เทคโนโลยีสำคัญที่ช่วยให้ตระกูล Phi เป็นไปได้

### วิธีการฝึกอบรมแบบ "Textbook"

หนึ่งในแง่มุมที่ปฏิวัติวงการที่สุดของตระกูล Phi คือวิธีการฝึกอบรมแบบ "คุณภาพตำราเรียน" แทนที่จะฝึกอบรมด้วยข้อมูลอินเทอร์เน็ตที่ไม่ได้กรองจำนวนมหาศาล โมเดล Phi ใช้เนื้อหาการศึกษาที่คัดสรรมาอย่างดีและมีคุณภาพสูง ซึ่งออกแบบมาเพื่อสอนการให้เหตุผล คณิตศาสตร์ การเขียนโค้ด และความรู้ทั่วไปอย่างมีประสิทธิภาพ

วิธีการนี้ทำงานโดยการสร้างเนื้อหาการศึกษาสังเคราะห์ที่สะท้อนถึงตำราเรียนและวัสดุทางวิชาการคุณภาพสูง ข้อมูลการฝึกอบรมได้รับการออกแบบมาให้มีความเหมาะสมทางการศึกษา โดยมุ่งเน้นที่คำอธิบายที่ชัดเจน การให้เหตุผลทีละขั้นตอน และการนำเสนอความรู้ที่มีโครงสร้าง

### การฝึกอบรมการให้เหตุผลขั้นสูง

โมเดล Phi รุ่นล่าสุดรวมวิธีการฝึกอบรมการให้เหตุผลที่ซับซ้อน ซึ่งช่วยให้สามารถแก้ปัญหาหลายขั้นตอนที่ซับซ้อนได้ เทคนิคเหล่านี้รวมถึง:

**การฝึกอบรมแบบ Chain-of-Thought**: โมเดลเรียนรู้ที่จะแบ่งปัญหาที่ซับซ้อนออกเป็นขั้นตอนการให้เหตุผลระดับกลาง ทำให้กระบวนการแก้ปัญหามีความโปร่งใสและเชื่อถือได้มากขึ้น

**Inference-Time Scaling**: โมเดลสร้างห่วงโซ่การให้เหตุผลโดยละเอียดที่ใช้ทรัพยากรการประมวลผลเพิ่มเติมในระหว่างการสร้างคำตอบเพื่อความแม่นยำที่ดีขึ้น

**Edge-of-Capability Training**: ข้อมูลการฝึกอบรมได้รับการคัดเลือกมาโดยเฉพาะเพื่อท้าทายโมเดลที่ขอบเขตความสามารถในปัจจุบัน ส่งเสริมการเรียนรู้รูปแบบการให้เหตุผลที่ซับซ้อน

### นวัตกรรมด้านสถาปัตยกรรม

ตระกูล Phi รวมการปรับแต่งสถาปัตยกรรมหลายอย่างที่ออกแบบมาโดยเฉพาะเพื่อประสิทธิภาพ:

**Parameter Efficiency**: การเลือกสถาปัตยกรรมอย่างรอบคอบที่เพิ่มผลกระทบของแต่ละพารามิเตอร์ในโมเดล

**Multi-Modal Integration**: การผสานรวมการประมวลผลข้อความ ภาพ และเสียงอย่างมีประสิทธิภาพในสถาปัตยกรรมที่กะทัดรัด

**Hardware Optimization**: ตัวแปรเฉพาะที่ปรับแต่งสำหรับแพลตฟอร์มฮาร์ดแวร์และสถานการณ์การใช้งานเฉพาะทาง

## การปรับแต่งฮาร์ดแวร์สำหรับโมเดล Phi

สภาพแวดล้อมการใช้งานสมัยใหม่ได้รับประโยชน์จากประสิทธิภาพของโมเดล Phi ในการกำหนดค่าฮาร์ดแวร์ต่าง ๆ:

### การใช้งานที่ปรับแต่งสำหรับ CPU

โมเดล Phi ถูกออกแบบมาให้ทำงานได้อย่างมีประสิทธิภาพบนฮาร์ดแวร์ที่ใช้ CPU เท่านั้น ทำให้สามารถใช้งานได้บนโครงสร้างพื้นฐานการประมวลผลมาตรฐานโดยไม่ต้องใช้ตัวเร่ง AI เฉพาะทาง

### การเร่งความเร็วด้วย GPU

แม้ว่าโมเดล Phi จะไม่ต้องการ GPU ที่ทรงพลัง แต่ก็สามารถใช้ทรัพยากร GPU ที่มีอยู่เพื่อเพิ่มประสิทธิภาพได้ ซึ่งให้ความยืดหยุ่นในการกำหนดค่าการใช้งาน

### การผสานรวมกับอุปกรณ์ปลายทาง

ตัวแปรเฉพาะ เช่น Phi-3-Silica ได้รับการปรับแต่งสำหรับแพลตฟอร์มการประมวลผลที่อุปกรณ์ปลายทาง โดยให้ประสิทธิภาพที่โดดเด่น เช่น 650 โทเค็นต่อวินาที โดยใช้พลังงานเพียง 1.5W

## ข้อดีของตระกูลโมเดล Phi

### ประสิทธิภาพด้านต้นทุน

โมเดล Phi ลดต้นทุนการดำเนินงานลงอย่างมากโดยต้องการโครงสร้างพื้นฐานการประมวลผลที่น้อยลง ในขณะที่ยังคงรักษาประสิทธิภาพที่แข่งขันได้ สิ่งนี้ทำให้ AI สามารถเข้าถึงได้สำหรับองค์กรที่มีงบประมาณจำกัด หรือแอปพลิเคชันที่มีปริมาณการใช้งานสูงซึ่งต้นทุนต่อการประมวลผลมีความสำคัญ

### ความยืดหยุ่นในการใช้งาน

ประสิทธิภาพของโมเดล Phi ช่วยให้สามารถใช้งานได้บนการกำหนดค่าฮาร์ดแวร์ที่หลากหลาย ตั้งแต่แล็ปท็อปส่วนบุคคลไปจนถึงเซิร์ฟเวอร์ระดับองค์กร ทำให้องค์กรมีตัวเลือกที่หลากหลายในโครงสร้างพื้นฐาน AI

### ความเป็นส่วนตัวและความปลอดภัย

ประสิทธิภาพของโมเดล Phi ช่วยให้สามารถใช้งานในเครื่องสำหรับแอปพลิเคชันที่มีความอ่อนไหวต่อความเป็นส่วนตัว โดยมั่นใจได้ว่าข้อมูลที่มีความอ่อนไหวจะไม่ออกจากการควบคุมขององค์กร ในขณะที่ยังคงมอบความสามารถ AI ที่ทรงพลัง

### ประสิทธิภาพแบบเรียลไทม์

ความต้องการด้านการประมวลผลที่ลดลงแปลเป็นเวลาการประมวลผลที่เร็วขึ้น ทำให้สามารถใช้งานแอปพลิเคชันแบบเรียลไทม์ที่ต้องการการตอบสนองทันทีโดยไม่ลดทอนคุณภาพของความสามารถ AI

### AI ที่เข้าถึงได้

โดยการลดอุปสรรคในการใช้งาน AI โมเดล Phi ทำให้ความสามารถ AI ขั้นสูงสามารถเข้าถึงได้สำหรับองค์กรที่มีทรัพยากรจำกัด ช่วยให้ทีมขนาดเล็กและนักพัฒนารายบุคคลสามารถใช้ความเข้าใจภาษาที่ซับซ้อนได้

## ตัวอย่างและกรณีการใช้งานจริง

ก่อนที่จะเข้าสู่วิวัฒนาการทางเทคนิค ลองสำรวจตัวอย่างที่เป็นรูปธรรมของสิ่งที่โมเดล Phi สามารถทำได้:

### ตัวอย่างการให้เหตุผลทางคณิตศาสตร์

Phi-4-mini-reasoning มีความโดดเด่นในการแก้ปัญหาทางคณิตศาสตร์ทีละขั้นตอน ตัวอย่างเช่น เมื่อถามว่า "วิธีแก้สมการ 3*x^2+4*x+5=1?" โมเดลจะให้ขั้นตอนการให้เหตุผลโดยละเอียด:

```
<|system|>Your name is Phi, an AI math expert developed by Microsoft.<|end|>
<|user|>How to solve 3*x^2+4*x+5=1?<|end|>
<|assistant|>
I need to solve the quadratic equation 3x² + 4x + 5 = 1.

First, let me rearrange this to standard form:
3x² + 4x + 5 = 1
3x² + 4x + 5 - 1 = 0  
3x² + 4x + 4 = 0

Now I can use the quadratic formula: x = (-b ± √(b² - 4ac)) / (2a)
Where a = 3, b = 4, c = 4

Let me calculate the discriminant:
b² - 4ac = 4² - 4(3)(4) = 16 - 48 = -32

Since the discriminant is negative (-32), this equation has no real solutions.
The solutions are complex: x = (-4 ± √(-32)) / 6 = (-4 ± 4i√2) / 6 = (-2 ± 2i√2) / 3
```

### ตัวอย่างการสนับสนุนหลายภาษา

Phi-3.5-mini-instruct แสดงความสามารถหลายภาษาที่แข็งแกร่งในหลากหลายภาษา นี่คือตัวอย่างรูปแบบคำถาม:

```
<|system|>You are a helpful assistant.<|end|>
<|user|>How to explain Internet for a medieval knight?<|end|>
<|assistant|>
Imagine, good sir knight, a vast network of invisible roads that span the entire realm and beyond. The Internet is like a magical system of messenger birds that can carry any message - be it text, images, or even moving pictures - instantly to any castle, village, or hamlet in the world.

Think of it as if every scribe's work could be copied and shared with thousands of people simultaneously, as if you could visit any library in any kingdom without leaving your chambers, or as if you could hold council with knights from distant lands as if they were sitting at your very table.

This "Internet" allows people to:
- Send messages faster than the swiftest horse
- Access knowledge from scholars across all kingdoms
- Trade goods without traveling to distant markets
- Share stories and news from lands you've never seen

'Tis like having a thousand scribes, merchants, and storytellers all working for you at once, good knight!
```

### ตัวอย่างความสามารถหลายรูปแบบ

Phi-4-multimodal สามารถประมวลผลข้อความ ภาพ และเสียงพร้อมกัน นี่คือตัวอย่างการใช้งานจริง:

**การวางแผนการเดินทางด้วยการป้อนข้อมูลเสียง:**
ดู Phi-4 Multimodal วิเคราะห์ภาษาพูดเพื่อช่วยวางแผนการเดินทางไปซีแอตเทิล โดยแสดงความสามารถในการประมวลผลเสียงขั้นสูงและการแนะนำ

**การแก้ปัญหาทางคณิตศาสตร์จากภาพ:**
ดู Phi-4 Multimodal จัดการกับปัญหาทางคณิตศาสตร์ที่ซับซ้อนผ่านการป้อนข้อมูลภาพ โดยแสดงความสามารถในการประมวลผลและแก้สมการที่นำเสนอในภาพ

**ตัวอย่างการเรียกฟังก์ชัน:**
ด้วยการเรียกฟังก์ชัน Phi-4-mini และ Phi-4-multimodal สามารถขยายความสามารถในการประมวลผลข้อความโดยการผสานรวมเครื่องมือค้นหา เชื่อมต่อเครื่องมือหลากหลาย และอื่น ๆ ดังที่แสดง โมเดลสามารถดึงข้อมูลการแข่งขันพรีเมียร์ลีกผ่าน Phi-4-mini โดยแสดงความสามารถในการโต้ตอบกับแหล่งข้อมูลภายนอกได้อย่างราบรื่น

### ตัวอย่างการสร้างโค้ด

Phi-4-multimodal สามารถสร้างโครงสร้างโค้ดโปรเจกต์ตามเนื้อหาภาพและคำถามที่ให้มา ดังที่แสดงในเวิร์กโฟลว์จริง:

1. อัปโหลดภาพของโครงร่างหรือการออกแบบ
2. ให้บริบทเกี่ยวกับข้อกำหนดของโปรเจกต์
3. โมเดลสร้างโครงสร้างโค้ดที่สมบูรณ์และใช้งานได้
4. โค้ดสามารถปรับแต่งได้ตามเฟรมเวิร์กหรือภาษาที่เฉพาะเจาะจง

### ตัวอย่างการใช้งานที่อุปกรณ์ปลายทาง

เราสามารถนำโมเดลที่ปรับแต่งแล้วไปใช้งานบนอุปกรณ์ปลายทาง โดยการผสาน Microsoft Olive และ ONNX GenAI Runtime เราสามารถนำ Phi-4-mini ไปใช้งานบน Windows, iPhone, Android และอุปกรณ์อื่น ๆ นี่คือตัวอย่างที่รันบน iPhone 12 Pro

กระบวนการใช้งานประกอบด้วย:
- การปรับแต่งโมเดลเพื่อการเพิ่มประสิทธิภาพบนมือถือ
- การผสาน ONNX runtime เพื่อความเข้ากันได้ข้ามแพลตฟอร์ม
- การประมวลผลในเครื่องโดยไม่ต้องเชื่อมต่ออินเทอร์เน็ต
- ประสิทธิภาพแบบเรียลไทม์ด้วยการใช้พลังงานที่น
ครอบครัว Phi แสดงให้เห็นว่าอนาคตของการใช้งาน AI ไม่ได้ขึ้นอยู่กับการสร้างโมเดลที่ใหญ่ขึ้นเท่านั้น แต่ยังรวมถึงการสร้างโมเดลที่ฉลาดและมีประสิทธิภาพมากขึ้น ซึ่งสามารถทำงานได้อย่างมีประสิทธิภาพในสภาพแวดล้อมฮาร์ดแวร์ที่หลากหลาย พร้อมทั้งรักษามาตรฐานประสิทธิภาพสูงไว้ได้

## ตัวอย่างการพัฒนาและการผสานรวม

### เริ่มต้นใช้งานอย่างรวดเร็วกับ Transformers

นี่คือวิธีเริ่มต้นใช้งานโมเดล Phi โดยใช้ไลบรารี Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Phi-4-mini-instruct
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation="flash_attention_2"  # For optimized performance
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")

# Format your prompt using the chat template
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

# Apply chat template
input_text = tokenizer.apply_chat_template(messages, tokenize=False)
inputs = tokenizer(input_text, return_tensors="pt")

# Generate response
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
    
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### ตัวอย่างการปรับแต่งโมเดล

ตัวอย่างต่อไปนี้แสดงวิธีปรับแต่ง Phi-4-mini-instruct สำหรับงานเฉพาะ:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig
from trl import SFTTrainer
from datasets import load_dataset

# Configuration for LoRA fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules="all-linear"
)

# Training configuration optimized for efficiency
training_config = TrainingArguments(
    output_dir="./phi-4-mini-finetuned",
    learning_rate=5.0e-06,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    num_train_epochs=1,
    bf16=True,
    gradient_checkpointing=True,
    warmup_ratio=0.2,
    save_steps=100
)

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = 'right'

# Load and process dataset
train_dataset = load_dataset("HuggingFaceH4/ultrachat_200k", split="train_sft")

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_config,
    peft_config=peft_config,
    train_dataset=train_dataset,
    max_seq_length=2048,
    tokenizer=tokenizer,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### รูปแบบ Prompt เฉพาะทาง

**สำหรับงานด้านการให้เหตุผล (Phi-4-reasoning-plus):**
```
<|im_start|>system<|im_sep|>
You are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions.

Please structure your response into two main sections:
<think>
{Thought section}
</think>
{Solution section}
<|im_end|>
<|im_start|>user<|im_sep|>
What is the derivative of x^2?
<|im_end|>
<|im_start|>assistant<|im_sep|>
```

**สำหรับงานด้านคณิตศาสตร์ (Phi-4-mini-reasoning):**
```
<|system|>
Your name is Phi, an AI math expert developed by Microsoft.
<|end|>
<|user|>
Solve this calculus problem: Find the integral of 2x + 3
<|end|>
<|assistant|>
```

### การใช้งานบนมือถือด้วย ONNX

```python
import onnxruntime as ort
import numpy as np

# Load ONNX model for mobile deployment
session = ort.InferenceSession("phi-4-mini-quantized.onnx")

# Prepare input
input_ids = tokenizer.encode("Hello, how are you?", return_tensors="np")

# Run inference
outputs = session.run(None, {"input_ids": input_ids})
predicted_ids = outputs[0]

# Decode response
response = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
```

## การวัดประสิทธิภาพและความสำเร็จ

ครอบครัวโมเดล Phi ได้สร้างความสำเร็จที่โดดเด่นในหลากหลายการวัดประสิทธิภาพ โดยมักจะมีประสิทธิภาพเหนือกว่าโมเดลที่ใหญ่กว่ามาก:

### ไฮไลต์สำคัญด้านประสิทธิภาพ

**ความเป็นเลิศด้านการให้เหตุผลทางคณิตศาสตร์:**
- Phi-4 ทำได้ 82.5% ความแม่นยำใน AIME 2025 (การแข่งขันคณิตศาสตร์ระดับโอลิมปิก)
- Phi-4-reasoning (14B) มีประสิทธิภาพเหนือกว่า DeepSeek-R1-Distill-70B (ใหญ่กว่า 5 เท่า) ในการวัดประสิทธิภาพด้านการให้เหตุผล
- Phi-4-mini-reasoning (3.8B) มีประสิทธิภาพเทียบเท่ากับโมเดลที่ใหญ่กว่าสองเท่าในงานด้านการให้เหตุผลทางคณิตศาสตร์

**ความสำเร็จด้านประสิทธิภาพ:**
- Phi-3-Silica ประมวลผลได้ 650 โทเค็นต่อวินาที โดยใช้พลังงานเพียง 1.5W
- Phi-4-mini (3.8B) มีประสิทธิภาพใกล้เคียงกับโมเดลที่ใหญ่กว่ามาก

**การวัดประสิทธิภาพ:**
- **MMLU (Massive Multitask Language Understanding)**: ประสิทธิภาพที่แข่งขันได้ใน 57 วิชาการ
- **HumanEval**: ความสามารถในการสร้างโค้ดที่แข็งแกร่ง โดยเฉพาะใน Python
- **MGSM**: การแก้ปัญหาคณิตศาสตร์ระดับประถมศึกษาแบบหลายภาษา
- **DROP**: งานด้านการทำความเข้าใจและการให้เหตุผลที่ซับซ้อน
- **SimpleQA**: ความแม่นยำในการตอบคำถามเชิงข้อเท็จจริง

### 📊 ตารางเปรียบเทียบโมเดล

| โมเดล | พารามิเตอร์ | ความยาวบริบท | จุดเด่น | กรณีการใช้งานที่เหมาะสม |
|-------|-------------|----------------|----------|---------------------------|
| **Phi-3-mini** | 3.8B | 4K/128K | ประสิทธิภาพทั่วไป | แอปมือถือ, แชทบอทพื้นฐาน |
| **Phi-3.5-mini** | 3.8B | 128K | รองรับหลายภาษา | แอปพลิเคชันระหว่างประเทศ |
| **Phi-4-mini** | 3.8B | 128K | การให้เหตุผลที่ดีขึ้น, การเรียกฟังก์ชัน | ระบบอัตโนมัติในธุรกิจ |
| **Phi-4-mini-reasoning** | 3.8B | 128K | การให้เหตุผลทางคณิตศาสตร์ | แพลตฟอร์มการศึกษา |
| **Phi-4** | 14B | 32K | การให้เหตุผลที่ซับซ้อน | การวิจัย, การวิเคราะห์ขั้นสูง |
| **Phi-4-reasoning** | 14B | 32K/64K | การให้เหตุผลหลายขั้นตอน | การคำนวณทางวิทยาศาสตร์ |
| **Phi-4-reasoning-plus** | 14B | 32K | ความแม่นยำสูงสุดในการให้เหตุผล | การตัดสินใจที่สำคัญ |
| **Phi-4-multimodal** | 5.6B | เปลี่ยนแปลงได้ | เสียง, ภาพ, ข้อความ | แอปพลิเคชันมัลติมีเดีย |

## คู่มือการเลือกโมเดล

### สำหรับแอปพลิเคชันพื้นฐาน
- **Phi-3-mini**: การสร้างข้อความง่าย ๆ, Q&A พื้นฐาน, การตอบสนองรวดเร็ว
- **Phi-4-mini**: การให้เหตุผลที่ดีขึ้นพร้อมความสามารถในการเรียกฟังก์ชัน

### สำหรับงานด้านคณิตศาสตร์และการให้เหตุผล
- **Phi-4**: การแก้ปัญหาคณิตศาสตร์ที่ซับซ้อนและการให้เหตุผล
- **Phi-4-reasoning**: การให้เหตุผลหลายขั้นตอนพร้อมคำอธิบายละเอียด
- **Phi-4-reasoning-plus**: ความแม่นยำสูงสุดสำหรับงานการให้เหตุผลที่สำคัญ
- **Phi-4-mini-reasoning**: การให้เหตุผลทางคณิตศาสตร์ที่มีประสิทธิภาพสำหรับสภาพแวดล้อมที่มีทรัพยากรจำกัด

### สำหรับแอปพลิเคชันมัลติมีเดีย
- **Phi-3-vision**: การประมวลผลภาพและข้อความร่วมกัน
- **Phi-4-multimodal**: ความสามารถครอบคลุมเสียง, ภาพ, และข้อความ

### สำหรับการใช้งานในองค์กร
- **Phi-3-medium**: ความเข้าใจภาษาขั้นสูงสำหรับแอปพลิเคชันธุรกิจ
- **Phi-3-Silica**: ปรับแต่งสำหรับแพลตฟอร์มฮาร์ดแวร์เฉพาะ

## แพลตฟอร์มการใช้งานและการเข้าถึง

### แพลตฟอร์มคลาวด์
- **Azure AI Foundry**: การใช้งานเต็มรูปแบบพร้อมเครื่องมือสำหรับองค์กร
- **Hugging Face**: คลังโมเดลโอเพ่นซอร์สและทรัพยากรชุมชน
- **NVIDIA API Catalog**: ตัวเลือกการใช้งานแบบไมโครเซอร์วิส

### เฟรมเวิร์กการพัฒนาในเครื่อง
- **Ollama**: เฟรมเวิร์กน้ำหนักเบาสำหรับการใช้งานโมเดลในเครื่อง
- **ONNX Runtime**: ปรับแต่งสำหรับการกำหนดค่าฮาร์ดแวร์ต่าง ๆ  
- **DirectML**: ประสิทธิภาพที่ปรับแต่งสำหรับ Windows
- **llama.cpp**: เอนจินการอนุมานข้ามแพลตฟอร์ม

### แหล่งเรียนรู้
- **Phi Portal**: ศูนย์เอกสารทางการของ Microsoft Phi
- **Phi Cookbook**: ตัวอย่างและบทเรียนที่ครอบคลุม
- **Technical Reports**: รายงานการวิจัยเชิงลึกบน arxiv
- **Community Spaces**: เดโมแบบโต้ตอบบน Hugging Face

### เริ่มต้นใช้งานโมเดล Phi

#### แพลตฟอร์มการพัฒนา
1. **Azure AI Foundry**: CLI ในเครื่องที่เรียบง่ายและการจัดการโมเดล
2. **Hugging Face Transformers**: การทดลองในเครื่องอย่างรวดเร็ว
3. **Ollama**: การใช้งานในเครื่องที่ง่ายสำหรับการทดสอบ

#### เส้นทางการเรียนรู้
1. **เข้าใจแนวคิดหลัก**: ศึกษาหลักการออกแบบพื้นฐาน
2. **ทดลองใช้ตัวแปรต่าง ๆ**: ลองใช้โมเดล Phi ต่าง ๆ เพื่อเข้าใจความสามารถ
3. **ฝึกการใช้งาน**: ใช้งานโมเดลในสภาพแวดล้อมทดสอบ
4. **ขยายการใช้งาน**: ขยายการใช้งานทีละน้อยตามผลลัพธ์ที่ประสบความสำเร็จ

#### แนวทางปฏิบัติที่ดีที่สุด
- **เริ่มต้นเล็ก ๆ**: เริ่มต้นด้วยโมเดล Phi-mini สำหรับการพัฒนาเบื้องต้น
- **ปรับแต่ง Prompt**: ใช้รูปแบบการแชทที่เหมาะสมเพื่อผลลัพธ์ที่ดีที่สุด
- **ติดตามประสิทธิภาพ**: ติดตามความเร็วในการอนุมานและตัวชี้วัดความแม่นยำ
- **พิจารณาฮาร์ดแวร์**: จับคู่ขนาดโมเดลกับทรัพยากรการคำนวณที่มีอยู่

## สรุป

ครอบครัวโมเดล Microsoft Phi เป็นตัวแทนของแนวทางปฏิวัติการออกแบบโมเดล AI โดยแสดงให้เห็นว่าโมเดลที่เล็กกว่าและมีประสิทธิภาพมากกว่าสามารถสร้างผลลัพธ์ที่น่าทึ่งในหลากหลายงานได้ ด้วยการมุ่งเน้นที่ข้อมูลการฝึกอบรมคุณภาพสูงและการปรับแต่งสถาปัตยกรรม ครอบครัว Phi มอบความสามารถที่ยอดเยี่ยมพร้อมความต้องการการคำนวณที่ลดลงอย่างมากเมื่อเทียบกับโมเดลภาษาขนาดใหญ่แบบดั้งเดิม

## วัตถุประสงค์การเรียนรู้ที่สำคัญ

1. เข้าใจปรัชญาการออกแบบและวิวัฒนาการของครอบครัวโมเดล Microsoft Phi ตั้งแต่ Phi-1 ถึง Phi-4
2. ระบุนวัตกรรมสำคัญ รวมถึงการฝึกอบรม "คุณภาพระดับตำราเรียน" และการปรับแต่งสถาปัตยกรรม
3. ตระหนักถึงประโยชน์และข้อจำกัดของตัวแปร Phi ต่าง ๆ ในสถานการณ์การใช้งานที่แตกต่างกัน
4. ใช้ความรู้เพื่อเลือกโมเดล Phi ที่เหมาะสมสำหรับกรณีการใช้งานและข้อจำกัดฮาร์ดแวร์เฉพาะ
5. ใช้เทคนิคการปรับแต่งเพื่อใช้งานโมเดล Phi บนอุปกรณ์ที่มีทรัพยากรจำกัด
6. อธิบายข้อได้เปรียบด้านสถาปัตยกรรมของครอบครัวโมเดล Phi เมื่อเทียบกับโมเดลภาษาขนาดใหญ่แบบดั้งเดิม
7. เลือกตัวแปร Phi ที่เหมาะสมตามข้อกำหนดของแอปพลิเคชันเฉพาะและข้อจำกัดฮาร์ดแวร์
8. ใช้งานโมเดล Phi ในทั้งสถานการณ์การใช้งานบนคลาวด์และขอบเครือข่ายด้วยการกำหนดค่าที่ปรับแต่ง
9. ใช้เทคนิคการลดขนาดและการปรับแต่งเพื่อปรับปรุงประสิทธิภาพโมเดล Phi บนอุปกรณ์เป้าหมาย
10. ประเมินการแลกเปลี่ยนระหว่างขนาดโมเดล ประสิทธิภาพ และความสามารถในครอบครัว Phi

## สิ่งที่จะเกิดขึ้นต่อไป

- [02: พื้นฐานครอบครัว Qwen](02.QwenFamily.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้