<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:06:59+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "th"
}
-->
# ส่วนที่ 2: พื้นฐานของตระกูล Qwen

ตระกูลโมเดล Qwen เป็นตัวแทนของแนวทางที่ครอบคลุมของ Alibaba Cloud ในการพัฒนาโมเดลภาษาขนาดใหญ่และ AI แบบมัลติโมดอล โดยแสดงให้เห็นว่าโมเดลโอเพ่นซอร์สสามารถมีประสิทธิภาพที่ยอดเยี่ยมและสามารถใช้งานได้ในหลากหลายสถานการณ์การปรับใช้ สิ่งสำคัญคือการเข้าใจว่าตระกูล Qwen ช่วยให้เกิดความสามารถ AI ที่ทรงพลังด้วยตัวเลือกการปรับใช้ที่ยืดหยุ่น ในขณะที่ยังคงรักษาประสิทธิภาพที่แข่งขันได้ในงานที่หลากหลาย

## ทรัพยากรสำหรับนักพัฒนา

### คลังโมเดล Hugging Face
โมเดลบางส่วนในตระกูล Qwen มีให้ใช้งานผ่าน [Hugging Face](https://huggingface.co/models?search=qwen) ซึ่งช่วยให้คุณเข้าถึงโมเดลบางเวอร์ชัน คุณสามารถสำรวจเวอร์ชันที่มีอยู่ ปรับแต่งให้เหมาะกับการใช้งานเฉพาะ และปรับใช้ผ่านเฟรมเวิร์กต่าง ๆ

### เครื่องมือพัฒนาท้องถิ่น
สำหรับการพัฒนาและทดสอบในเครื่อง คุณสามารถใช้ [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) เพื่อรันโมเดล Qwen ที่มีอยู่บนเครื่องพัฒนาของคุณด้วยประสิทธิภาพที่ปรับแต่งมาอย่างดี

### แหล่งข้อมูลเอกสาร
- [เอกสารโมเดล Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [การปรับแต่งโมเดล Qwen สำหรับการปรับใช้ที่ Edge](https://github.com/microsoft/olive)

## บทนำ

ในบทเรียนนี้ เราจะสำรวจตระกูลโมเดล Qwen ของ Alibaba และแนวคิดพื้นฐาน เราจะครอบคลุมถึงวิวัฒนาการของตระกูล Qwen วิธีการฝึกอบรมที่เป็นนวัตกรรมที่ทำให้โมเดล Qwen มีประสิทธิภาพ เวอร์ชันสำคัญในตระกูล และการใช้งานจริงในสถานการณ์ต่าง ๆ

## วัตถุประสงค์การเรียนรู้

เมื่อจบบทเรียนนี้ คุณจะสามารถ:

- เข้าใจปรัชญาการออกแบบและวิวัฒนาการของตระกูลโมเดล Qwen ของ Alibaba
- ระบุนวัตกรรมสำคัญที่ช่วยให้โมเดล Qwen มีประสิทธิภาพสูงในขนาดพารามิเตอร์ต่าง ๆ
- รับรู้ถึงข้อดีและข้อจำกัดของเวอร์ชันโมเดล Qwen ต่าง ๆ
- ใช้ความรู้เกี่ยวกับโมเดล Qwen เพื่อเลือกเวอร์ชันที่เหมาะสมสำหรับสถานการณ์ในโลกจริง

## การทำความเข้าใจภูมิทัศน์โมเดล AI สมัยใหม่

ภูมิทัศน์ AI ได้พัฒนาไปอย่างมาก โดยองค์กรต่าง ๆ ใช้แนวทางที่หลากหลายในการพัฒนาโมเดลภาษา ในขณะที่บางองค์กรมุ่งเน้นไปที่โมเดลปิดที่เป็นกรรมสิทธิ์ อื่น ๆ เน้นการเข้าถึงและความโปร่งใสแบบโอเพ่นซอร์ส วิธีการแบบดั้งเดิมเกี่ยวข้องกับโมเดลขนาดใหญ่ที่เป็นกรรมสิทธิ์ซึ่งเข้าถึงได้ผ่าน API เท่านั้น หรือโมเดลโอเพ่นซอร์สที่อาจมีความสามารถตามหลัง

แนวทางนี้สร้างความท้าทายให้กับองค์กรที่ต้องการความสามารถ AI ที่ทรงพลัง ในขณะที่ยังคงควบคุมข้อมูล ค่าใช้จ่าย และความยืดหยุ่นในการปรับใช้ วิธีการแบบดั้งเดิมมักต้องเลือกระหว่างประสิทธิภาพที่ล้ำสมัยและการพิจารณาการปรับใช้ในทางปฏิบัติ

## ความท้าทายของ AI ที่เข้าถึงได้และยอดเยี่ยม

ความต้องการ AI ที่มีคุณภาพสูงและเข้าถึงได้กลายเป็นสิ่งสำคัญมากขึ้นในสถานการณ์ต่าง ๆ ลองพิจารณาการใช้งานที่ต้องการตัวเลือกการปรับใช้ที่ยืดหยุ่นสำหรับความต้องการขององค์กรต่าง ๆ การดำเนินการที่คุ้มค่าเมื่อค่าใช้จ่าย API อาจสูงขึ้น ความสามารถหลายภาษาเพื่อการใช้งานทั่วโลก หรือความเชี่ยวชาญเฉพาะด้านในพื้นที่เช่นการเขียนโค้ดและคณิตศาสตร์

### ข้อกำหนดการปรับใช้ที่สำคัญ

การปรับใช้ AI สมัยใหม่เผชิญกับข้อกำหนดพื้นฐานหลายประการที่จำกัดการใช้งานในทางปฏิบัติ:

- **การเข้าถึง**: ความพร้อมใช้งานแบบโอเพ่นซอร์สเพื่อความโปร่งใสและการปรับแต่ง
- **ความคุ้มค่า**: ความต้องการคอมพิวเตอร์ที่เหมาะสมสำหรับงบประมาณต่าง ๆ
- **ความยืดหยุ่น**: ขนาดโมเดลที่หลากหลายสำหรับสถานการณ์การปรับใช้ต่าง ๆ
- **การเข้าถึงทั่วโลก**: ความสามารถหลายภาษาและข้ามวัฒนธรรมที่แข็งแกร่ง
- **ความเชี่ยวชาญเฉพาะด้าน**: เวอร์ชันเฉพาะด้านสำหรับการใช้งานเฉพาะ

## ปรัชญาของโมเดล Qwen

ตระกูลโมเดล Qwen เป็นตัวแทนของแนวทางที่ครอบคลุมในการพัฒนาโมเดล AI โดยให้ความสำคัญกับการเข้าถึงแบบโอเพ่นซอร์ส ความสามารถหลายภาษา และการปรับใช้ในทางปฏิบัติ ในขณะที่ยังคงรักษาลักษณะประสิทธิภาพที่แข่งขันได้ โมเดล Qwen บรรลุเป้าหมายนี้ผ่านขนาดโมเดลที่หลากหลาย วิธีการฝึกอบรมคุณภาพสูง และเวอร์ชันเฉพาะด้านสำหรับโดเมนต่าง ๆ

ตระกูล Qwen ครอบคลุมแนวทางที่หลากหลายซึ่งออกแบบมาเพื่อให้ตัวเลือกในสเปกตรัมประสิทธิภาพ-ประสิทธิภาพ ช่วยให้การปรับใช้ตั้งแต่โทรศัพท์มือถือไปจนถึงเซิร์ฟเวอร์องค์กร ในขณะที่ให้ความสามารถ AI ที่มีความหมาย เป้าหมายคือการทำให้การเข้าถึง AI คุณภาพสูงเป็นประชาธิปไตย ในขณะที่ให้ความยืดหยุ่นในตัวเลือกการปรับใช้

### หลักการออกแบบหลักของ Qwen

โมเดล Qwen ถูกสร้างขึ้นบนหลักการพื้นฐานหลายประการที่ทำให้แตกต่างจากตระกูลโมเดลภาษาอื่น ๆ:

- **โอเพ่นซอร์สเป็นอันดับแรก**: ความโปร่งใสและการเข้าถึงที่สมบูรณ์สำหรับการวิจัยและการใช้งานเชิงพาณิชย์
- **การฝึกอบรมที่ครอบคลุม**: การฝึกอบรมบนชุดข้อมูลขนาดใหญ่และหลากหลายที่ครอบคลุมหลายภาษาและโดเมน
- **สถาปัตยกรรมที่ปรับขนาดได้**: ขนาดโมเดลที่หลากหลายเพื่อให้ตรงกับความต้องการคอมพิวเตอร์ที่แตกต่างกัน
- **ความเป็นเลิศเฉพาะด้าน**: เวอร์ชันเฉพาะด้านที่ปรับแต่งสำหรับงานเฉพาะ

## เทคโนโลยีสำคัญที่ช่วยให้ตระกูล Qwen เป็นไปได้

### การฝึกอบรมในระดับใหญ่

หนึ่งในลักษณะสำคัญของตระกูล Qwen คือขนาดใหญ่ของชุดข้อมูลการฝึกอบรมและทรัพยากรคอมพิวเตอร์ที่ลงทุนในการพัฒนาโมเดล โมเดล Qwen ใช้ชุดข้อมูลหลายภาษาที่คัดสรรมาอย่างดีซึ่งครอบคลุมหลายล้านล้านโทเค็น ออกแบบมาเพื่อให้ความรู้เกี่ยวกับโลกและความสามารถในการให้เหตุผลที่ครอบคลุม

วิธีการนี้ทำงานโดยการรวมเนื้อหาเว็บคุณภาพสูง วรรณกรรมทางวิชาการ คลังโค้ด และทรัพยากรหลายภาษา วิธีการฝึกอบรมเน้นทั้งความกว้างของความรู้และความลึกของความเข้าใจในโดเมนและภาษาต่าง ๆ

### การให้เหตุผลและการคิดขั้นสูง

โมเดล Qwen รุ่นล่าสุดรวมความสามารถในการให้เหตุผลที่ซับซ้อนซึ่งช่วยให้การแก้ปัญหาหลายขั้นตอนที่ซับซ้อน:

**โหมดการคิด (Qwen3)**: โมเดลสามารถมีส่วนร่วมในกระบวนการให้เหตุผลทีละขั้นตอนก่อนที่จะให้คำตอบสุดท้าย คล้ายกับวิธีการแก้ปัญหาของมนุษย์

**การดำเนินการแบบสองโหมด**: ความสามารถในการสลับระหว่างโหมดตอบสนองเร็วสำหรับคำถามง่าย ๆ และโหมดการคิดลึกสำหรับปัญหาที่ซับซ้อน

**การรวมการคิดเป็นลำดับ**: การรวมขั้นตอนการให้เหตุผลตามธรรมชาติที่ช่วยปรับปรุงความโปร่งใสและความแม่นยำในงานที่ซับซ้อน

### นวัตกรรมทางสถาปัตยกรรม

ตระกูล Qwen รวมการปรับแต่งทางสถาปัตยกรรมหลายประการที่ออกแบบมาเพื่อทั้งประสิทธิภาพและประสิทธิผล:

**การออกแบบที่ปรับขนาดได้**: สถาปัตยกรรมที่สอดคล้องกันในขนาดโมเดลที่ช่วยให้การปรับขนาดและการเปรียบเทียบง่ายขึ้น

**การรวมมัลติโมดอล**: การรวมการประมวลผลข้อความ ภาพ และเสียงอย่างไร้รอยต่อในสถาปัตยกรรมที่เป็นหนึ่งเดียว

**การปรับแต่งการปรับใช้**: ตัวเลือกการควอนไทซ์และรูปแบบการปรับใช้ที่หลากหลายสำหรับการกำหนดค่าฮาร์ดแวร์ต่าง ๆ

## ขนาดโมเดลและตัวเลือกการปรับใช้

สภาพแวดล้อมการปรับใช้สมัยใหม่ได้รับประโยชน์จากความยืดหยุ่นของโมเดล Qwen ในความต้องการคอมพิวเตอร์ที่หลากหลาย:

### โมเดลขนาดเล็ก (0.5B-3B)

Qwen มีโมเดลขนาดเล็กที่มีประสิทธิภาพเหมาะสำหรับการปรับใช้ที่ Edge แอปพลิเคชันมือถือ และสภาพแวดล้อมที่มีทรัพยากรจำกัด ในขณะที่ยังคงรักษาความสามารถที่น่าประทับใจ

### โมเดลขนาดกลาง (7B-32B)

โมเดลขนาดกลางให้ความสามารถที่เพิ่มขึ้นสำหรับการใช้งานระดับมืออาชีพ โดยให้สมดุลที่ยอดเยี่ยมระหว่างประสิทธิภาพและความต้องการคอมพิวเตอร์

### โมเดลขนาดใหญ่ (72B+)

โมเดลขนาดใหญ่ให้ประสิทธิภาพที่ล้ำสมัยสำหรับการใช้งานที่ต้องการ การวิจัย และการปรับใช้ในองค์กรที่ต้องการความสามารถสูงสุด

## ข้อดีของตระกูลโมเดล Qwen

### การเข้าถึงแบบโอเพ่นซอร์ส

โมเดล Qwen ให้ความโปร่งใสและความสามารถในการปรับแต่งอย่างสมบูรณ์ ช่วยให้องค์กรเข้าใจ ปรับเปลี่ยน และปรับแต่งโมเดลให้เหมาะกับความต้องการเฉพาะโดยไม่ต้องพึ่งพาผู้ขาย

### ความยืflexibility
- Qwen3-235B-A22B ทำผลงานได้ดีในด้านการเขียนโค้ด คณิตศาสตร์ และความสามารถทั่วไป เมื่อเปรียบเทียบกับโมเดลชั้นนำอื่น ๆ เช่น DeepSeek-R1, o1, o3-mini, Grok-3 และ Gemini-2.5-Pro  
- Qwen3-30B-A3B มีประสิทธิภาพเหนือกว่า QwQ-32B แม้จะใช้พารามิเตอร์ที่เปิดใช้งานมากกว่า 10 เท่า  
- Qwen3-4B สามารถเทียบเคียงประสิทธิภาพกับ Qwen2.5-72B-Instruct  

**ความสำเร็จด้านประสิทธิภาพ:**  
- โมเดลพื้นฐาน Qwen3-MoE มีประสิทธิภาพใกล้เคียงกับโมเดลพื้นฐาน Qwen2.5 แบบ dense แต่ใช้พารามิเตอร์ที่เปิดใช้งานเพียง 10%  
- ประหยัดค่าใช้จ่ายอย่างมากทั้งในด้านการฝึกฝนและการใช้งานเมื่อเทียบกับโมเดลแบบ dense  

**ความสามารถด้านหลายภาษา:**  
- โมเดล Qwen3 รองรับ 119 ภาษาและสำเนียง  
- มีประสิทธิภาพที่แข็งแกร่งในบริบททางภาษาและวัฒนธรรมที่หลากหลาย  

**ขนาดการฝึกฝน:**  
- Qwen3 ใช้ข้อมูลการฝึกฝนเกือบสองเท่าของ Qwen2.5 โดยมีประมาณ 36 ล้านล้านโทเค็นครอบคลุม 119 ภาษาและสำเนียง เทียบกับ Qwen2.5 ที่มี 18 ล้านล้านโทเค็น  

### ตารางเปรียบเทียบโมเดล  

| ซีรีส์โมเดล | ช่วงพารามิเตอร์ | ความยาวบริบท | จุดเด่น | กรณีการใช้งานที่เหมาะสม |  
|--------------|------------------|----------------|---------------|----------------|  
| **Qwen2.5** | 0.5B-72B | 32K-128K | ประสิทธิภาพสมดุล รองรับหลายภาษา | การใช้งานทั่วไป การปรับใช้ในระบบผลิต |  
| **Qwen2.5-Coder** | 1.5B-32B | 128K | การสร้างโค้ด การเขียนโปรแกรม | การพัฒนาซอฟต์แวร์ ผู้ช่วยเขียนโค้ด |  
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | การวิเคราะห์ทางคณิตศาสตร์ | แพลตฟอร์มการศึกษา การใช้งาน STEM |  
| **Qwen2.5-VL** | หลากหลาย | เปลี่ยนแปลงได้ | ความเข้าใจด้านภาพและภาษา | การใช้งานมัลติโหมด การวิเคราะห์ภาพ |  
| **Qwen3** | 0.6B-235B | เปลี่ยนแปลงได้ | การวิเคราะห์ขั้นสูง โหมดการคิด | การวิเคราะห์ที่ซับซ้อน การวิจัย |  
| **Qwen3 MoE** | 30B-235B รวม | เปลี่ยนแปลงได้ | ประสิทธิภาพขนาดใหญ่ที่มีประสิทธิภาพ | การใช้งานในองค์กร ความต้องการประสิทธิภาพสูง |  

## คู่มือการเลือกโมเดล  

### สำหรับการใช้งานพื้นฐาน  
- **Qwen2.5-0.5B/1.5B**: แอปพลิเคชันมือถือ อุปกรณ์ edge การใช้งานแบบเรียลไทม์  
- **Qwen2.5-3B/7B**: แชทบอททั่วไป การสร้างเนื้อหา ระบบถามตอบ  

### สำหรับงานคณิตศาสตร์และการวิเคราะห์  
- **Qwen2.5-Math**: การแก้ปัญหาทางคณิตศาสตร์และการศึกษา STEM  
- **Qwen3 พร้อมโหมดการคิด**: การวิเคราะห์ที่ซับซ้อนที่ต้องการการวิเคราะห์ทีละขั้นตอน  

### สำหรับการเขียนโปรแกรมและการพัฒนา  
- **Qwen2.5-Coder**: การสร้างโค้ด การดีบัก ผู้ช่วยเขียนโปรแกรม  
- **Qwen3**: งานเขียนโปรแกรมขั้นสูงที่ต้องการความสามารถในการวิเคราะห์  

### สำหรับการใช้งานมัลติโหมด  
- **Qwen2.5-VL**: ความเข้าใจภาพ การตอบคำถามด้วยภาพ  
- **Qwen-Audio**: การประมวลผลเสียงและความเข้าใจคำพูด  

### สำหรับการปรับใช้ในองค์กร  
- **Qwen2.5-32B/72B**: ความเข้าใจภาษาที่มีประสิทธิภาพสูง  
- **Qwen3-235B-A22B**: ความสามารถสูงสุดสำหรับการใช้งานที่ต้องการ  

## แพลตฟอร์มการปรับใช้และการเข้าถึง  
### แพลตฟอร์มคลาวด์  
- **Hugging Face Hub**: คลังโมเดลที่ครอบคลุมพร้อมการสนับสนุนจากชุมชน  
- **ModelScope**: แพลตฟอร์มโมเดลของ Alibaba พร้อมเครื่องมือปรับแต่ง  
- **ผู้ให้บริการคลาวด์ต่าง ๆ**: รองรับผ่านแพลตฟอร์ม ML มาตรฐาน  

### เฟรมเวิร์กการพัฒนาในเครื่อง  
- **Transformers**: การผสานรวม Hugging Face มาตรฐานสำหรับการปรับใช้ที่ง่าย  
- **vLLM**: การให้บริการที่มีประสิทธิภาพสูงสำหรับสภาพแวดล้อมการผลิต  
- **Ollama**: การปรับใช้และการจัดการในเครื่องที่ง่าย  
- **ONNX Runtime**: การปรับแต่งข้ามแพลตฟอร์มสำหรับฮาร์ดแวร์หลากหลาย  
- **llama.cpp**: การใช้งาน C++ ที่มีประสิทธิภาพสำหรับแพลตฟอร์มหลากหลาย  

### แหล่งเรียนรู้  
- **เอกสาร Qwen**: เอกสารทางการและการ์ดโมเดล  
- **Hugging Face Model Hub**: เดโมแบบโต้ตอบและตัวอย่างจากชุมชน  
- **งานวิจัย**: เอกสารทางเทคนิคบน arxiv เพื่อความเข้าใจเชิงลึก  
- **ฟอรัมชุมชน**: การสนับสนุนและการอภิปรายจากชุมชนที่มีความเคลื่อนไหว  

### เริ่มต้นใช้งานโมเดล Qwen  

#### แพลตฟอร์มการพัฒนา  
1. **Hugging Face Transformers**: เริ่มต้นด้วยการผสานรวม Python มาตรฐาน  
2. **ModelScope**: สำรวจเครื่องมือปรับใช้ที่ปรับแต่งของ Alibaba  
3. **การปรับใช้ในเครื่อง**: ใช้ Ollama หรือ Transformers โดยตรงสำหรับการทดสอบในเครื่อง  

#### เส้นทางการเรียนรู้  
1. **เข้าใจแนวคิดหลัก**: ศึกษาสถาปัตยกรรมและความสามารถของตระกูล Qwen  
2. **ทดลองกับตัวแปรต่าง ๆ**: ลองใช้โมเดลขนาดต่าง ๆ เพื่อเข้าใจการแลกเปลี่ยนประสิทธิภาพ  
3. **ฝึกฝนการใช้งาน**: ปรับใช้โมเดลในสภาพแวดล้อมการพัฒนา  
4. **ปรับแต่งการปรับใช้**: ปรับแต่งสำหรับกรณีการใช้งานในระบบผลิต  

#### แนวทางปฏิบัติที่ดีที่สุด  
- **เริ่มต้นเล็ก ๆ**: เริ่มต้นด้วยโมเดลขนาดเล็ก (1.5B-7B) สำหรับการพัฒนาเบื้องต้น  
- **ใช้เทมเพลตแชท**: ใช้รูปแบบที่เหมาะสมเพื่อผลลัพธ์ที่ดีที่สุด  
- **ติดตามทรัพยากร**: ตรวจสอบการใช้งานหน่วยความจำและความเร็วในการใช้งาน  
- **พิจารณาความเชี่ยวชาญเฉพาะด้าน**: เลือกตัวแปรที่เฉพาะเจาะจงตามโดเมนเมื่อเหมาะสม  

## รูปแบบการใช้งานขั้นสูง  

### ตัวอย่างการปรับแต่ง  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```
  
### การออกแบบพรอมต์เฉพาะทาง  

**สำหรับงานวิเคราะห์ที่ซับซ้อน:**  
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```
  
**สำหรับการสร้างโค้ดพร้อมบริบท:**  
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```
  
### การใช้งานหลายภาษา  

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```
  
### 🔧 รูปแบบการปรับใช้ในระบบผลิต  

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```
  
## กลยุทธ์การปรับแต่งประสิทธิภาพ  

### การปรับแต่งหน่วยความจำ  

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```
  
### การปรับแต่งการใช้งาน  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```
  
## แนวทางปฏิบัติและคำแนะนำ  

### ความปลอดภัยและความเป็นส่วนตัว  

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```
  
### การติดตามและการประเมินผล  

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```
  
## สรุป  

ตระกูลโมเดล Qwen เป็นตัวแทนของแนวทางที่ครอบคลุมในการทำให้เทคโนโลยี AI เข้าถึงได้ง่ายขึ้น พร้อมทั้งรักษาประสิทธิภาพที่แข่งขันได้ในหลากหลายการใช้งาน ด้วยความมุ่งมั่นในด้านการเข้าถึงแบบโอเพ่นซอร์ส ความสามารถหลายภาษา และตัวเลือกการปรับใช้ที่ยืดหยุ่น Qwen ช่วยให้องค์กรและนักพัฒนาสามารถใช้ประโยชน์จากความสามารถ AI ที่ทรงพลังได้โดยไม่คำนึงถึงทรัพยากรหรือความต้องการเฉพาะ  

### ประเด็นสำคัญ  

**ความเป็นเลิศด้านโอเพ่นซอร์ส**: Qwen แสดงให้เห็นว่าโมเดลโอเพ่นซอร์สสามารถมีประสิทธิภาพที่แข่งขันได้กับทางเลือกที่เป็นกรรมสิทธิ์ พร้อมทั้งให้ความโปร่งใส การปรับแต่ง และการควบคุม  

**สถาปัตยกรรมที่ปรับขนาดได้**: ช่วงพารามิเตอร์ตั้งแต่ 0.5B ถึง 235B ช่วยให้สามารถปรับใช้ได้ในทุกสภาพแวดล้อมการคำนวณ ตั้งแต่อุปกรณ์มือถือไปจนถึงคลัสเตอร์องค์กร  

**ความสามารถเฉพาะทาง**: ตัวแปรเฉพาะด้าน เช่น Qwen-Coder, Qwen-Math และ Qwen-VL ให้ความเชี่ยวชาญเฉพาะทางพร้อมทั้งรักษาความเข้าใจภาษาทั่วไป  

**การเข้าถึงทั่วโลก**: การสนับสนุนหลายภาษาที่แข็งแกร่งในกว่า 119 ภาษา ทำให้ Qwen เหมาะสำหรับการใช้งานระดับนานาชาติและฐานผู้ใช้ที่หลากหลาย  

**นวัตกรรมอย่างต่อเนื่อง**: การพัฒนาจาก Qwen 1.0 สู่ Qwen3 แสดงให้เห็นถึงการปรับปรุงอย่างต่อเนื่องในด้านความสามารถ ประสิทธิภาพ และตัวเลือกการปรับใช้  

### มุมมองในอนาคต  

เมื่อตระกูล Qwen ยังคงพัฒนา เราสามารถคาดหวังได้ว่า:  
- **ประสิทธิภาพที่ดีขึ้น**: การปรับแต่งอย่างต่อเนื่องเพื่ออัตราส่วนประสิทธิภาพต่อพารามิเตอร์ที่ดียิ่งขึ้น  
- **ความสามารถมัลติโหมดที่ขยายตัว**: การผสานรวมการประมวลผลภาพ เสียง และข้อความที่ซับซ้อนมากขึ้น  
- **การวิเคราะห์ที่ดีขึ้น**: กลไกการคิดขั้นสูงและความสามารถในการแก้ปัญหาหลายขั้นตอน  
- **เครื่องมือปรับใช้ที่ดีขึ้น**: เฟรมเวิร์กและเครื่องมือปรับแต่งที่ปรับปรุงสำหรับสถานการณ์การปรับใช้ที่หลากหลาย  
- **การเติบโตของชุมชน**: ระบบนิเวศที่ขยายตัวของเครื่องมือ แอปพลิเคชัน และการสนับสนุนจากชุมชน  

### ขั้นตอนถัดไป  

ไม่ว่าคุณจะสร้างแชทบอท พัฒนาเครื่องมือการศึกษา สร้างผู้ช่วยเขียนโค้ด หรือทำงานเกี่ยวกับการใช้งานหลายภาษา ตระกูล Qwen มีโซลูชันที่ปรับขนาดได้พร้อมการสนับสนุนจากชุมชนที่แข็งแกร่งและเอกสารที่ครอบคลุม  

สำหรับข้อมูลล่าสุด การเปิดตัวโมเดล และเอกสารทางเทคนิคโดยละเอียด โปรดเยี่ยมชมที่เก็บ Qwen อย่างเป็นทางการบน Hugging Face และสำรวจการอภิปรายและตัวอย่างจากชุมชนที่มีความเคลื่อนไหว  

อนาคตของการพัฒนา AI อยู่ที่เครื่องมือที่เข้าถึงได้ โปร่งใส และทรงพลังที่ช่วยให้เกิดนวัตกรรมในทุกภาคส่วนและทุกระดับ ตระกูล Qwen เป็นตัวอย่างของวิสัยทัศน์นี้ โดยให้องค์กรและนักพัฒนามีพื้นฐานในการสร้างแอปพลิเคชันที่ขับเคลื่อนด้วย AI รุ่นต่อไป  

## แหล่งข้อมูลเพิ่มเติม  

- **เอกสารทางการ**: [Qwen Documentation](https://qwen.readthedocs.io/)  
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)  
- **เอกสารทางเทคนิค**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)  
- **ชุมชน**: [GitHub Discussions and Issues](https://github.com/QwenLM/)  
- **แพลตฟอร์ม ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)  

## ผลลัพธ์การเรียนรู้  

หลังจากจบโมดูลนี้ คุณจะสามารถ:  
1. อธิบายข้อดีด้านสถาปัตยกรรมของตระกูลโมเดล Qwen และแนวทางโอเพ่นซอร์ส  
2. เลือกตัวแปร Qwen ที่เหมาะสมตามความต้องการของแอปพลิเคชันและข้อจำกัดด้านทรัพยากร  
3. ใช้โมเดล Qwen ในสถานการณ์การปรับใช้ต่าง ๆ ด้วยการกำหนดค่าที่ปรับแต่ง  
4. ใช้เทคนิคการปรับแต่งและการปรับแต่งเพื่อปรับปรุงประสิทธิภาพของโมเดล Qwen  
5. ประเมินการแลกเปลี่ยนระหว่างขนาดโมเดล ประสิทธิภาพ และความสามารถในตระกูล Qwen  

## สิ่งที่จะเกิดขึ้นต่อไป  

- [03: พื้นฐานตระกูล Gemma](03.GemmaFamily.md)  

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์ที่มีความเชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้