{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# เซสชันที่ 5 – การจัดการหลายเอเจนต์\n",
    "\n",
    "แสดงตัวอย่างการทำงานของสายงานแบบสองเอเจนต์อย่างง่าย (นักวิจัย -> บรรณาธิการ) โดยใช้ Foundry Local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### คำอธิบาย: การติดตั้ง Dependency\n",
    "ติดตั้ง `foundry-local-sdk` และ `openai` ซึ่งจำเป็นสำหรับการเข้าถึงโมเดลในเครื่องและการสร้างข้อความแชท ผลลัพธ์จะเหมือนเดิมทุกครั้งที่ดำเนินการ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# สถานการณ์\n",
    "การสร้างรูปแบบการทำงานแบบผู้ประสานงานสองตัวแทนที่เรียบง่าย:\n",
    "- **ตัวแทนนักวิจัย** รวบรวมข้อมูลข้อเท็จจริงที่กระชับ\n",
    "- **ตัวแทนบรรณาธิการ** เขียนใหม่ให้ชัดเจนสำหรับผู้บริหาร\n",
    "\n",
    "แสดงให้เห็นถึงการใช้หน่วยความจำร่วมกันในแต่ละตัวแทน การส่งผ่านผลลัพธ์ระหว่างกลางแบบลำดับ และฟังก์ชันการทำงานแบบไปป์ไลน์ที่เรียบง่าย สามารถขยายไปยังบทบาทเพิ่มเติม (เช่น นักวิจารณ์, ผู้ตรวจสอบ) หรือสาขาคู่ขนานได้\n",
    "\n",
    "**ตัวแปรสภาพแวดล้อม:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - โมเดลเริ่มต้นที่จะใช้ (ค่าเริ่มต้น: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - โมเดลตัวแทนหลัก (แทนที่ ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - โมเดลตัวแทนบรรณาธิการ (ค่าเริ่มต้นคือโมเดลหลัก)\n",
    "\n",
    "**เอกสารอ้างอิง SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**วิธีการทำงาน:**\n",
    "1. **FoundryLocalManager** เริ่มต้นบริการ Foundry Local โดยอัตโนมัติ\n",
    "2. ดาวน์โหลดและโหลดโมเดลที่ระบุ (หรือใช้เวอร์ชันที่แคชไว้)\n",
    "3. ให้จุดเชื่อมต่อที่เข้ากันได้กับ OpenAI สำหรับการโต้ตอบ\n",
    "4. ตัวแทนแต่ละตัวสามารถใช้โมเดลที่แตกต่างกันสำหรับงานเฉพาะทาง\n",
    "5. มีตรรกะการลองใหม่ในตัวเพื่อจัดการกับความล้มเหลวชั่วคราวอย่างราบรื่น\n",
    "\n",
    "**คุณสมบัติเด่น:**\n",
    "- ✅ การค้นหาและเริ่มต้นบริการอัตโนมัติ\n",
    "- ✅ การจัดการวงจรชีวิตของโมเดล (ดาวน์โหลด, แคช, โหลด)\n",
    "- ✅ ความเข้ากันได้กับ OpenAI SDK สำหรับ API ที่คุ้นเคย\n",
    "- ✅ รองรับหลายโมเดลสำหรับการทำงานเฉพาะทางของตัวแทน\n",
    "- ✅ การจัดการข้อผิดพลาดที่แข็งแกร่งด้วยตรรกะการลองใหม่\n",
    "- ✅ การประมวลผลในเครื่อง (ไม่ต้องใช้ API บนคลาวด์)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### คำอธิบาย: การนำเข้า Core & การใช้ Typing\n",
    "แนะนำ dataclasses สำหรับการจัดเก็บข้อความของตัวแทนและการใช้ typing hints เพื่อความชัดเจน นำเข้า Foundry Local manager และ OpenAI client สำหรับการดำเนินการของตัวแทนในขั้นตอนถัดไป\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### คำอธิบาย: การเริ่มต้นใช้งานโมเดล (รูปแบบ SDK)\n",
    "ใช้ Foundry Local Python SDK เพื่อการจัดการโมเดลที่มีประสิทธิภาพ:\n",
    "- **FoundryLocalManager(alias)** - เริ่มต้นบริการอัตโนมัติและโหลดโมเดลตาม alias\n",
    "- **get_model_info(alias)** - แปลง alias เป็น ID โมเดลที่เฉพาะเจาะจง\n",
    "- **manager.endpoint** - ให้บริการ endpoint สำหรับ OpenAI client\n",
    "- **manager.api_key** - ให้ API key (ไม่จำเป็นสำหรับการใช้งานในเครื่อง)\n",
    "- รองรับโมเดลแยกสำหรับตัวแทนต่าง ๆ (หลัก vs ตัวแก้ไข)\n",
    "- มีตรรกะการลองใหม่ในตัวพร้อมการเพิ่มเวลารอแบบทวีคูณเพื่อความยืดหยุ่น\n",
    "- ตรวจสอบการเชื่อมต่อเพื่อให้แน่ใจว่าบริการพร้อมใช้งาน\n",
    "\n",
    "**รูปแบบ SDK ที่สำคัญ:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**การจัดการวงจรชีวิต:**\n",
    "- ผู้จัดการจะถูกเก็บไว้ในระดับโลกเพื่อการทำความสะอาดที่เหมาะสม\n",
    "- ตัวแทนแต่ละตัวสามารถใช้โมเดลที่แตกต่างกันเพื่อความเชี่ยวชาญเฉพาะด้าน\n",
    "- การค้นหาบริการและการจัดการการเชื่อมต่ออัตโนมัติ\n",
    "- ลองใหม่อย่างราบรื่นพร้อมการเพิ่มเวลารอแบบทวีคูณเมื่อเกิดข้อผิดพลาด\n",
    "\n",
    "สิ่งนี้ช่วยให้มั่นใจว่าการเริ่มต้นใช้งานถูกต้องก่อนที่การจัดการตัวแทนจะเริ่มต้นขึ้น\n",
    "\n",
    "**อ้างอิง:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### คำอธิบาย: คลาส Agent & Memory\n",
    "กำหนด `AgentMsg` ที่มีน้ำหนักเบาสำหรับรายการหน่วยความจำ และ `Agent` ที่ครอบคลุม:\n",
    "- **บทบาทของระบบ** - บุคลิกและคำแนะนำของ Agent\n",
    "- **ประวัติข้อความ** - รักษาบริบทของการสนทนา\n",
    "- **เมธอด act()** - ดำเนินการด้วยการจัดการข้อผิดพลาดอย่างเหมาะสม\n",
    "\n",
    "Agent สามารถใช้โมเดลที่แตกต่างกัน (หลัก vs ตัวแก้ไข) และรักษาบริบทแยกกันสำหรับแต่ละ Agent รูปแบบนี้ช่วยให้:\n",
    "- การคงหน่วยความจำระหว่างการดำเนินการ\n",
    "- การกำหนดโมเดลที่ยืดหยุ่นสำหรับแต่ละ Agent\n",
    "- การแยกข้อผิดพลาดและการกู้คืน\n",
    "- การเชื่อมโยงและการจัดการที่ง่ายดาย\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### คำอธิบาย: กระบวนการแบบมีการจัดการ\n",
    "สร้างตัวแทนเฉพาะทางสองตัว:\n",
    "- **นักวิจัย**: ใช้โมเดลหลัก รวบรวมข้อมูลข้อเท็จจริง\n",
    "- **บรรณาธิการ**: สามารถใช้โมเดลแยกต่างหาก (ถ้ากำหนดค่าไว้) ปรับปรุงและเขียนใหม่\n",
    "\n",
    "ฟังก์ชัน `pipeline`:\n",
    "1. นักวิจัยรวบรวมข้อมูลดิบ\n",
    "2. บรรณาธิการปรับปรุงข้อมูลให้พร้อมสำหรับการใช้งานระดับผู้บริหาร\n",
    "3. ส่งคืนผลลัพธ์ทั้งแบบขั้นกลางและขั้นสุดท้าย\n",
    "\n",
    "รูปแบบนี้ช่วยให้:\n",
    "- การปรับแต่งโมเดล (ใช้โมเดลต่างกันสำหรับบทบาทต่างๆ)\n",
    "- การปรับปรุงคุณภาพผ่านการประมวลผลหลายขั้นตอน\n",
    "- การตรวจสอบย้อนกลับของการเปลี่ยนแปลงข้อมูล\n",
    "- การขยายตัวที่ง่ายขึ้นสำหรับตัวแทนเพิ่มเติมหรือการประมวลผลแบบขนาน\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### คำอธิบาย: การดำเนินการและผลลัพธ์ของ Pipeline\n",
    "ดำเนินการ pipeline แบบหลายตัวแทนบนคำถามที่เกี่ยวกับการปฏิบัติตามกฎระเบียบและความหน่วงเวลา เพื่อแสดงให้เห็นถึง:\n",
    "- การเปลี่ยนแปลงข้อมูลหลายขั้นตอน\n",
    "- ความเชี่ยวชาญและการทำงานร่วมกันของตัวแทน\n",
    "- การปรับปรุงคุณภาพผลลัพธ์ผ่านการกลั่นกรอง\n",
    "- การตรวจสอบย้อนกลับ (ทั้งผลลัพธ์ระหว่างทางและผลลัพธ์สุดท้ายถูกเก็บรักษาไว้)\n",
    "\n",
    "**โครงสร้างผลลัพธ์:**\n",
    "- `question` - คำถามต้นฉบับจากผู้ใช้\n",
    "- `research` - ผลลัพธ์การวิจัยดิบ (ข้อมูลข้อเท็จจริงแบบหัวข้อย่อย)\n",
    "- `final` - สรุปผลที่ผ่านการกลั่นกรอง\n",
    "- `models` - โมเดลที่ใช้ในแต่ละขั้นตอน\n",
    "\n",
    "**แนวคิดการขยายเพิ่มเติม:**\n",
    "1. เพิ่มตัวแทน Critic สำหรับการตรวจสอบคุณภาพ\n",
    "2. ใช้ตัวแทนวิจัยแบบขนานสำหรับแง่มุมต่าง ๆ\n",
    "3. เพิ่มตัวแทน Verifier สำหรับการตรวจสอบข้อเท็จจริง\n",
    "4. ใช้โมเดลที่แตกต่างกันสำหรับระดับความซับซ้อนที่ต่างกัน\n",
    "5. ใช้กระบวนการป้อนกลับเพื่อปรับปรุงแบบวนซ้ำ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### ขั้นสูง: การตั้งค่าตัวแทนแบบกำหนดเอง\n",
    "\n",
    "ลองปรับแต่งพฤติกรรมของตัวแทนโดยการแก้ไขตัวแปรสภาพแวดล้อมก่อนที่จะรันเซลล์การเริ่มต้น:\n",
    "\n",
    "**โมเดลที่มีให้ใช้งาน:**\n",
    "- ใช้ `foundry model ls` ในเทอร์มินัลเพื่อดูโมเดลทั้งหมดที่มีให้ใช้งาน\n",
    "- ตัวอย่าง: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b เป็นต้น\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์ที่มีความเชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-09T13:29:47+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}