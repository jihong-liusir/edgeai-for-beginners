<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:30:06+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "th"
}
-->
# ส่วนที่ 3: คู่มือการใช้งานจริง

## ภาพรวม

คู่มือฉบับสมบูรณ์นี้จะช่วยคุณเตรียมตัวสำหรับหลักสูตร EdgeAI ซึ่งเน้นการสร้างโซลูชัน AI ที่สามารถทำงานได้อย่างมีประสิทธิภาพบนอุปกรณ์ Edge หลักสูตรนี้ให้ความสำคัญกับการพัฒนาผ่านการลงมือทำจริง โดยใช้เฟรมเวิร์กที่ทันสมัยและโมเดลล้ำสมัยที่ได้รับการปรับแต่งสำหรับการใช้งานบน Edge

## 1. การตั้งค่าสภาพแวดล้อมการพัฒนา

### ภาษาโปรแกรมและเฟรมเวิร์ก

**สภาพแวดล้อม Python**
- **เวอร์ชัน**: Python 3.10 หรือสูงกว่า (แนะนำ: Python 3.11)
- **ตัวจัดการแพ็กเกจ**: pip หรือ conda
- **สภาพแวดล้อมเสมือน**: ใช้ venv หรือ conda environments เพื่อแยกการทำงาน
- **ไลบรารีหลัก**: เราจะติดตั้งไลบรารี EdgeAI เฉพาะในระหว่างหลักสูตร

**สภาพแวดล้อม Microsoft .NET**
- **เวอร์ชัน**: .NET 8 หรือสูงกว่า
- **IDE**: Visual Studio 2022, Visual Studio Code หรือ JetBrains Rider
- **SDK**: ตรวจสอบให้แน่ใจว่าได้ติดตั้ง .NET SDK สำหรับการพัฒนาข้ามแพลตฟอร์ม

### เครื่องมือพัฒนา

**ตัวแก้ไขโค้ดและ IDE**
- Visual Studio Code (แนะนำสำหรับการพัฒนาข้ามแพลตฟอร์ม)
- PyCharm หรือ Visual Studio (สำหรับการพัฒนาเฉพาะภาษา)
- Jupyter Notebooks สำหรับการพัฒนาและการสร้างต้นแบบแบบโต้ตอบ

**การควบคุมเวอร์ชัน**
- Git (เวอร์ชันล่าสุด)
- บัญชี GitHub สำหรับการเข้าถึง repository และการทำงานร่วมกัน

## 2. ความต้องการและคำแนะนำด้านฮาร์ดแวร์

### ความต้องการระบบขั้นต่ำ
- **CPU**: โปรเซสเซอร์แบบหลายคอร์ (Intel i5/AMD Ryzen 5 หรือเทียบเท่า)
- **RAM**: ขั้นต่ำ 8GB, แนะนำ 16GB
- **พื้นที่จัดเก็บ**: พื้นที่ว่าง 50GB สำหรับโมเดลและเครื่องมือพัฒนา
- **OS**: Windows 10/11, macOS 10.15+ หรือ Linux (Ubuntu 20.04+)

### กลยุทธ์ทรัพยากรคอมพิวเตอร์
หลักสูตรนี้ออกแบบมาให้เข้าถึงได้ในฮาร์ดแวร์ที่หลากหลาย:

**การพัฒนาในเครื่อง (เน้น CPU/NPU)**
- การพัฒนาหลักจะใช้ CPU และการเร่งความเร็ว NPU
- เหมาะสำหรับแล็ปท็อปและเดสก์ท็อปสมัยใหม่ส่วนใหญ่
- เน้นประสิทธิภาพและสถานการณ์การใช้งานจริง

**ทรัพยากร GPU บนคลาวด์ (ตัวเลือกเสริม)**
- **Azure Machine Learning**: สำหรับการฝึกอบรมและการทดลองที่เข้มข้น
- **Google Colab**: มีระดับฟรีสำหรับการศึกษา
- **Kaggle Notebooks**: แพลตฟอร์มการประมวลผลบนคลาวด์ทางเลือก

### ข้อควรพิจารณาเกี่ยวกับอุปกรณ์ Edge
- ความเข้าใจเกี่ยวกับโปรเซสเซอร์ ARM
- ความรู้เกี่ยวกับข้อจำกัดของฮาร์ดแวร์มือถือและ IoT
- ความคุ้นเคยกับการปรับแต่งการใช้พลังงาน

## 3. กลุ่มโมเดลหลักและทรัพยากร

### กลุ่มโมเดลหลัก

**Microsoft Phi-4 Family**
- **คำอธิบาย**: โมเดลขนาดกะทัดรัดและมีประสิทธิภาพที่ออกแบบมาสำหรับการใช้งานบน Edge
- **จุดเด่น**: อัตราส่วนประสิทธิภาพต่อขนาดที่ยอดเยี่ยม ปรับแต่งสำหรับงานด้านการให้เหตุผล
- **ทรัพยากร**: [Phi-4 Collection บน Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **กรณีการใช้งาน**: การสร้างโค้ด การให้เหตุผลทางคณิตศาสตร์ การสนทนาทั่วไป

**Qwen-3 Family**
- **คำอธิบาย**: โมเดลรุ่นล่าสุดของ Alibaba ที่รองรับหลายภาษา
- **จุดเด่น**: ความสามารถหลายภาษาที่แข็งแกร่ง สถาปัตยกรรมที่มีประสิทธิภาพ
- **ทรัพยากร**: [Qwen-3 Collection บน Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **กรณีการใช้งาน**: แอปพลิเคชันหลายภาษา โซลูชัน AI ข้ามวัฒนธรรม

**Google Gemma-3n Family**
- **คำอธิบาย**: โมเดลน้ำหนักเบาของ Google ที่ปรับแต่งสำหรับการใช้งานบน Edge
- **จุดเด่น**: การประมวลผลที่รวดเร็ว สถาปัตยกรรมที่เหมาะกับมือถือ
- **ทรัพยากร**: [Gemma-3n Collection บน Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **กรณีการใช้งาน**: แอปพลิเคชันมือถือ การประมวลผลแบบเรียลไทม์

### เกณฑ์การเลือกโมเดล
- **การแลกเปลี่ยนระหว่างประสิทธิภาพและขนาด**: ความเข้าใจว่าเมื่อใดควรเลือกโมเดลขนาดเล็กหรือใหญ่
- **การปรับแต่งเฉพาะงาน**: การจับคู่โมเดลกับกรณีการใช้งานเฉพาะ
- **ข้อจำกัดในการใช้งาน**: หน่วยความจำ ความหน่วง และการใช้พลังงาน

## 4. เครื่องมือการปรับแต่งและการเพิ่มประสิทธิภาพ

### เฟรมเวิร์ก Llama.cpp
- **Repository**: [Llama.cpp บน GitHub](https://github.com/ggml-org/llama.cpp)
- **วัตถุประสงค์**: เอนจินการอนุมานประสิทธิภาพสูงสำหรับ LLMs
- **คุณสมบัติหลัก**:
  - การอนุมานที่ปรับแต่งสำหรับ CPU
  - รูปแบบการปรับแต่งหลายรูปแบบ (Q4, Q5, Q8)
  - ความเข้ากันได้ข้ามแพลตฟอร์ม
  - การดำเนินการที่มีประสิทธิภาพด้านหน่วยความจำ
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repository**: [Microsoft Olive บน GitHub](https://github.com/microsoft/olive)
- **วัตถุประสงค์**: เครื่องมือปรับแต่งโมเดลสำหรับการใช้งานบน Edge
- **คุณสมบัติหลัก**:
  - เวิร์กโฟลว์การปรับแต่งโมเดลอัตโนมัติ
  - การปรับแต่งที่คำนึงถึงฮาร์ดแวร์
  - การรวมเข้ากับ ONNX Runtime
  - เครื่องมือวัดประสิทธิภาพ
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # ตัวอย่างสคริปต์ Python สำหรับการปรับแต่งโมเดล
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (สำหรับผู้ใช้ macOS)
- **Repository**: [Apple MLX บน GitHub](https://github.com/ml-explore/mlx)
- **วัตถุประสงค์**: เฟรมเวิร์กการเรียนรู้ของเครื่องสำหรับ Apple Silicon
- **คุณสมบัติหลัก**:
  - การปรับแต่งที่เหมาะสมกับ Apple Silicon
  - การดำเนินการที่มีประสิทธิภาพด้านหน่วยความจำ
  - API ที่คล้าย PyTorch
  - รองรับสถาปัตยกรรมหน่วยความจำแบบรวม
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repository**: [ONNX Runtime บน GitHub](https://github.com/microsoft/onnxruntime)
- **วัตถุประสงค์**: การเร่งการอนุมานข้ามแพลตฟอร์มสำหรับโมเดล ONNX
- **คุณสมบัติหลัก**:
  - การปรับแต่งเฉพาะฮาร์ดแวร์ (CPU, GPU, NPU)
  - การปรับแต่งกราฟสำหรับการอนุมาน
  - รองรับการปรับแต่ง
  - รองรับหลายภาษา (Python, C++, C#, JavaScript)
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. การอ่านและทรัพยากรที่แนะนำ

### เอกสารสำคัญ
- **เอกสาร ONNX Runtime**: ความเข้าใจเกี่ยวกับการอนุมานข้ามแพลตฟอร์ม
- **คู่มือ Hugging Face Transformers**: การโหลดโมเดลและการอนุมาน
- **รูปแบบการออกแบบ Edge AI**: แนวปฏิบัติที่ดีที่สุดสำหรับการใช้งานบน Edge

### เอกสารทางเทคนิค
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### ทรัพยากรชุมชน
- **ชุมชน EdgeAI บน Slack/Discord**: การสนับสนุนและการอภิปรายจากเพื่อนร่วมงาน
- **Repository บน GitHub**: ตัวอย่างการใช้งานและบทเรียน
- **ช่อง YouTube**: การเจาะลึกทางเทคนิคและบทเรียน

## 6. การประเมินและการตรวจสอบ

### รายการตรวจสอบก่อนเริ่มหลักสูตร
- [ ] ติดตั้งและตรวจสอบ Python 3.10+ แล้ว
- [ ] ติดตั้งและตรวจสอบ .NET 8+ แล้ว
- [ ] ตั้งค่าสภาพแวดล้อมการพัฒนาแล้ว
- [ ] สร้างบัญชี Hugging Face แล้ว
- [ ] มีความคุ้นเคยพื้นฐานกับกลุ่มโมเดลเป้าหมาย
- [ ] ติดตั้งและทดสอบเครื่องมือปรับแต่งแล้ว
- [ ] ตรงตามความต้องการด้านฮาร์ดแวร์แล้ว
- [ ] ตั้งค่าบัญชีคลาวด์คอมพิวติ้ง (ถ้าจำเป็น)

## วัตถุประสงค์การเรียนรู้ที่สำคัญ

เมื่อจบคู่มือนี้ คุณจะสามารถ:

1. ตั้งค่าสภาพแวดล้อมการพัฒนาแบบสมบูรณ์สำหรับการพัฒนาแอปพลิเคชัน EdgeAI
2. ติดตั้งและกำหนดค่าเครื่องมือและเฟรมเวิร์กที่จำเป็นสำหรับการปรับแต่งโมเดล
3. เลือกการกำหนดค่าฮาร์ดแวร์และซอฟต์แวร์ที่เหมาะสมสำหรับโครงการ EdgeAI ของคุณ
4. เข้าใจข้อควรพิจารณาที่สำคัญสำหรับการใช้งานโมเดล AI บนอุปกรณ์ Edge
5. เตรียมระบบของคุณสำหรับการลงมือทำในหลักสูตร

## ทรัพยากรเพิ่มเติม

### เอกสารทางการ
- **เอกสาร Python**: เอกสารทางการของภาษา Python
- **เอกสาร Microsoft .NET**: ทรัพยากรการพัฒนา .NET อย่างเป็นทางการ
- **เอกสาร ONNX Runtime**: คู่มือครอบคลุมเกี่ยวกับ ONNX Runtime
- **เอกสาร TensorFlow Lite**: เอกสารทางการของ TensorFlow Lite

### เครื่องมือพัฒนา
- **Visual Studio Code**: ตัวแก้ไขโค้ดน้ำหนักเบาพร้อมส่วนขยายสำหรับการพัฒนา AI
- **Jupyter Notebooks**: สภาพแวดล้อมการคำนวณแบบโต้ตอบสำหรับการทดลอง ML
- **Docker**: แพลตฟอร์มคอนเทนเนอร์สำหรับสภาพแวดล้อมการพัฒนาที่สอดคล้องกัน
- **Git**: ระบบควบคุมเวอร์ชันสำหรับการจัดการโค้ด

### ทรัพยากรการเรียนรู้
- **เอกสารวิจัย EdgeAI**: งานวิจัยทางวิชาการล่าสุดเกี่ยวกับโมเดลที่มีประสิทธิภาพ
- **หลักสูตรออนไลน์**: วัสดุการเรียนรู้เสริมเกี่ยวกับการปรับแต่ง AI
- **ฟอรัมชุมชน**: แพลตฟอร์มถามตอบสำหรับความท้าทายในการพัฒนา EdgeAI
- **ชุดข้อมูลมาตรฐาน**: ชุดข้อมูลมาตรฐานสำหรับการประเมินประสิทธิภาพโมเดล

## ผลลัพธ์การเรียนรู้

หลังจากจบคู่มือเตรียมความพร้อมนี้ คุณจะ:

1. มีสภาพแวดล้อมการพัฒนาที่ตั้งค่าไว้อย่างสมบูรณ์สำหรับการพัฒนา EdgeAI
2. เข้าใจความต้องการด้านฮาร์ดแวร์และซอฟต์แวร์สำหรับสถานการณ์การใช้งานที่แตกต่างกัน
3. คุ้นเคยกับเฟรมเวิร์กและเครื่องมือสำคัญที่ใช้ตลอดหลักสูตร
4. สามารถเลือกโมเดลที่เหมาะสมตามข้อจำกัดและความต้องการของอุปกรณ์
5. มีความรู้พื้นฐานเกี่ยวกับเทคนิคการปรับแต่งสำหรับการใช้งานบน Edge

## ➡️ ขั้นตอนถัดไป

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้