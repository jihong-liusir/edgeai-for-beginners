<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-18T07:33:10+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "th"
}
-->
# ส่วนที่ 3: คู่มือการใช้งานจริง

## ภาพรวม

คู่มือฉบับสมบูรณ์นี้จะช่วยคุณเตรียมตัวสำหรับหลักสูตร EdgeAI ซึ่งเน้นการสร้างโซลูชัน AI ที่สามารถทำงานได้อย่างมีประสิทธิภาพบนอุปกรณ์ Edge หลักสูตรนี้ให้ความสำคัญกับการพัฒนาผ่านการลงมือทำจริง โดยใช้เฟรมเวิร์คที่ทันสมัยและโมเดลที่ล้ำสมัยซึ่งได้รับการปรับแต่งสำหรับการใช้งานบนอุปกรณ์ Edge

## 1. การตั้งค่าสภาพแวดล้อมการพัฒนา

### ภาษาโปรแกรมและเฟรมเวิร์ค

**สภาพแวดล้อม Python**
- **เวอร์ชัน**: Python 3.10 หรือสูงกว่า (แนะนำ: Python 3.11)
- **ตัวจัดการแพ็กเกจ**: pip หรือ conda
- **สภาพแวดล้อมเสมือน**: ใช้ venv หรือ conda environments เพื่อแยกการทำงาน
- **ไลบรารีสำคัญ**: เราจะติดตั้งไลบรารี EdgeAI เฉพาะในระหว่างหลักสูตร

**สภาพแวดล้อม Microsoft .NET**
- **เวอร์ชัน**: .NET 8 หรือสูงกว่า
- **IDE**: Visual Studio 2022, Visual Studio Code หรือ JetBrains Rider
- **SDK**: ตรวจสอบให้แน่ใจว่าได้ติดตั้ง .NET SDK สำหรับการพัฒนาข้ามแพลตฟอร์ม

### เครื่องมือพัฒนา

**ตัวแก้ไขโค้ดและ IDE**
- Visual Studio Code (แนะนำสำหรับการพัฒนาข้ามแพลตฟอร์ม)
- PyCharm หรือ Visual Studio (สำหรับการพัฒนาเฉพาะภาษา)
- Jupyter Notebooks สำหรับการพัฒนาแบบโต้ตอบและการสร้างต้นแบบ

**การควบคุมเวอร์ชัน**
- Git (เวอร์ชันล่าสุด)
- บัญชี GitHub สำหรับการเข้าถึง repository และการทำงานร่วมกัน

## 2. ความต้องการและคำแนะนำด้านฮาร์ดแวร์

### ความต้องการระบบขั้นต่ำ
- **CPU**: โปรเซสเซอร์แบบหลายคอร์ (Intel i5/AMD Ryzen 5 หรือเทียบเท่า)
- **RAM**: ขั้นต่ำ 8GB, แนะนำ 16GB
- **พื้นที่จัดเก็บ**: พื้นที่ว่าง 50GB สำหรับโมเดลและเครื่องมือพัฒนา
- **OS**: Windows 10/11, macOS 10.15+ หรือ Linux (Ubuntu 20.04+)

### กลยุทธ์ทรัพยากรการประมวลผล
หลักสูตรนี้ออกแบบมาให้เข้าถึงได้ในฮาร์ดแวร์ที่หลากหลาย:

**การพัฒนาในเครื่อง (เน้น CPU/NPU)**
- การพัฒนาหลักจะใช้การเร่งความเร็วผ่าน CPU และ NPU
- เหมาะสำหรับแล็ปท็อปและเดสก์ท็อปสมัยใหม่ส่วนใหญ่
- เน้นประสิทธิภาพและสถานการณ์การใช้งานจริง

**ทรัพยากร GPU บนคลาวด์ (ตัวเลือกเสริม)**
- **Azure Machine Learning**: สำหรับการฝึกอบรมและการทดลองที่เข้มข้น
- **Google Colab**: มีให้ใช้งานฟรีสำหรับการศึกษา
- **Kaggle Notebooks**: แพลตฟอร์มการประมวลผลบนคลาวด์ทางเลือก

### ข้อควรพิจารณาเกี่ยวกับอุปกรณ์ Edge
- ความเข้าใจเกี่ยวกับโปรเซสเซอร์ ARM
- ความรู้เกี่ยวกับข้อจำกัดของฮาร์ดแวร์มือถือและ IoT
- ความคุ้นเคยกับการปรับแต่งการใช้พลังงาน

## 3. กลุ่มโมเดลหลักและทรัพยากร

### กลุ่มโมเดลหลัก

**Microsoft Phi-4 Family**
- **คำอธิบาย**: โมเดลที่กะทัดรัดและมีประสิทธิภาพ ออกแบบมาสำหรับการใช้งานบนอุปกรณ์ Edge
- **จุดเด่น**: อัตราส่วนประสิทธิภาพต่อขนาดที่ยอดเยี่ยม ปรับแต่งสำหรับงานด้านการให้เหตุผล
- **ทรัพยากร**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **กรณีการใช้งาน**: การสร้างโค้ด การให้เหตุผลทางคณิตศาสตร์ การสนทนาทั่วไป

**Qwen-3 Family**
- **คำอธิบาย**: โมเดลรุ่นล่าสุดของ Alibaba ที่รองรับหลายภาษา
- **จุดเด่น**: ความสามารถด้านหลายภาษา โครงสร้างที่มีประสิทธิภาพ
- **ทรัพยากร**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **กรณีการใช้งาน**: แอปพลิเคชันหลายภาษา โซลูชัน AI ข้ามวัฒนธรรม

**Google Gemma-3n Family**
- **คำอธิบาย**: โมเดลน้ำหนักเบาของ Google ที่ปรับแต่งสำหรับการใช้งานบนอุปกรณ์ Edge
- **จุดเด่น**: การประมวลผลที่รวดเร็ว โครงสร้างที่เหมาะกับมือถือ
- **ทรัพยากร**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **กรณีการใช้งาน**: แอปพลิเคชันมือถือ การประมวลผลแบบเรียลไทม์

### เกณฑ์การเลือกโมเดล
- **การแลกเปลี่ยนระหว่างประสิทธิภาพและขนาด**: เข้าใจว่าเมื่อใดควรเลือกโมเดลขนาดเล็กหรือใหญ่
- **การปรับแต่งเฉพาะงาน**: การจับคู่โมเดลกับกรณีการใช้งานเฉพาะ
- **ข้อจำกัดในการใช้งาน**: หน่วยความจำ เวลาแฝง และการใช้พลังงาน

## 4. เครื่องมือการปรับแต่งและการเพิ่มประสิทธิภาพ

### เฟรมเวิร์ค Llama.cpp
- **Repository**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **วัตถุประสงค์**: เอนจินการอนุมานประสิทธิภาพสูงสำหรับ LLMs
- **คุณสมบัติสำคัญ**:
  - การอนุมานที่ปรับแต่งสำหรับ CPU
  - รูปแบบการปรับแต่งหลายรูปแบบ (Q4, Q5, Q8)
  - ความเข้ากันได้ข้ามแพลตฟอร์ม
  - การดำเนินการที่ประหยัดหน่วยความจำ
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repository**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **วัตถุประสงค์**: เครื่องมือปรับแต่งโมเดลสำหรับการใช้งานบนอุปกรณ์ Edge
- **คุณสมบัติสำคัญ**:
  - เวิร์กโฟลว์การปรับแต่งโมเดลอัตโนมัติ
  - การปรับแต่งที่คำนึงถึงฮาร์ดแวร์
  - การรวมเข้ากับ ONNX Runtime
  - เครื่องมือวัดประสิทธิภาพ
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # กำหนดโมเดลและการตั้งค่าการปรับแต่ง
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # รันเวิร์กโฟลว์การปรับแต่ง
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # บันทึกโมเดลที่ปรับแต่งแล้ว
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # ติดตั้ง MLX
  pip install mlx
  
  # ตัวอย่างสคริปต์ Python สำหรับโหลดและปรับแต่งโมเดล
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repository**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **วัตถุประสงค์**: การเร่งความเร็วการอนุมานข้ามแพลตฟอร์มสำหรับโมเดล ONNX
- **คุณสมบัติสำคัญ**:
  - การปรับแต่งเฉพาะฮาร์ดแวร์ (CPU, GPU, NPU)
  - การปรับแต่งกราฟสำหรับการอนุมาน
  - รองรับการปรับแต่ง
  - รองรับหลายภาษา (Python, C++, C#, JavaScript)
- **การติดตั้งและการใช้งานพื้นฐาน**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. การอ่านและทรัพยากรที่แนะนำ

### เอกสารสำคัญ
- **เอกสาร ONNX Runtime**: ความเข้าใจเกี่ยวกับการอนุมานข้ามแพลตฟอร์ม
- **คู่มือ Hugging Face Transformers**: การโหลดและการอนุมานโมเดล
- **รูปแบบการออกแบบ Edge AI**: แนวปฏิบัติที่ดีที่สุดสำหรับการใช้งานบนอุปกรณ์ Edge

### เอกสารทางเทคนิค
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### ทรัพยากรชุมชน
- **ชุมชน EdgeAI บน Slack/Discord**: การสนับสนุนและการพูดคุยกับเพื่อนร่วมงาน
- **GitHub Repositories**: ตัวอย่างการใช้งานและบทเรียน
- **ช่อง YouTube**: การเจาะลึกทางเทคนิคและบทเรียน

## 6. การประเมินและการตรวจสอบ

### รายการตรวจสอบก่อนเริ่มหลักสูตร
- [ ] ติดตั้งและตรวจสอบ Python 3.10+
- [ ] ติดตั้งและตรวจสอบ .NET 8+
- [ ] ตั้งค่าสภาพแวดล้อมการพัฒนา
- [ ] สร้างบัญชี Hugging Face
- [ ] มีความคุ้นเคยพื้นฐานกับกลุ่มโมเดลเป้าหมาย
- [ ] ติดตั้งและทดสอบเครื่องมือปรับแต่ง
- [ ] ตรงตามความต้องการด้านฮาร์ดแวร์
- [ ] ตั้งค่าบัญชีคลาวด์ (ถ้าจำเป็น)

## วัตถุประสงค์การเรียนรู้หลัก

เมื่อจบคู่มือนี้ คุณจะสามารถ:

1. ตั้งค่าสภาพแวดล้อมการพัฒนาที่สมบูรณ์สำหรับการพัฒนาแอปพลิเคชัน EdgeAI
2. ติดตั้งและกำหนดค่าเครื่องมือและเฟรมเวิร์คที่จำเป็นสำหรับการปรับแต่งโมเดล
3. เลือกการกำหนดค่าฮาร์ดแวร์และซอฟต์แวร์ที่เหมาะสมสำหรับโครงการ EdgeAI ของคุณ
4. เข้าใจข้อควรพิจารณาที่สำคัญสำหรับการใช้งานโมเดล AI บนอุปกรณ์ Edge
5. เตรียมระบบของคุณสำหรับการฝึกปฏิบัติในหลักสูตร

## ทรัพยากรเพิ่มเติม

### เอกสารอย่างเป็นทางการ
- **เอกสาร Python**: เอกสารภาษา Python อย่างเป็นทางการ
- **เอกสาร Microsoft .NET**: ทรัพยากรการพัฒนา .NET อย่างเป็นทางการ
- **เอกสาร ONNX Runtime**: คู่มือครอบคลุมเกี่ยวกับ ONNX Runtime
- **เอกสาร TensorFlow Lite**: เอกสาร TensorFlow Lite อย่างเป็นทางการ

### เครื่องมือพัฒนา
- **Visual Studio Code**: ตัวแก้ไขโค้ดน้ำหนักเบาพร้อมส่วนขยายสำหรับการพัฒนา AI
- **Jupyter Notebooks**: สภาพแวดล้อมการคำนวณแบบโต้ตอบสำหรับการทดลอง ML
- **Docker**: แพลตฟอร์มคอนเทนเนอร์สำหรับสภาพแวดล้อมการพัฒนาที่สอดคล้องกัน
- **Git**: ระบบควบคุมเวอร์ชันสำหรับการจัดการโค้ด

### ทรัพยากรการเรียนรู้
- **เอกสารวิจัย EdgeAI**: งานวิจัยทางวิชาการล่าสุดเกี่ยวกับโมเดลที่มีประสิทธิภาพ
- **หลักสูตรออนไลน์**: วัสดุการเรียนรู้เพิ่มเติมเกี่ยวกับการปรับแต่ง AI
- **ฟอรัมชุมชน**: แพลตฟอร์มถามตอบสำหรับความท้าทายในการพัฒนา EdgeAI
- **ชุดข้อมูลมาตรฐาน**: ชุดข้อมูลมาตรฐานสำหรับการประเมินประสิทธิภาพโมเดล

## ผลลัพธ์การเรียนรู้

หลังจากจบคู่มือเตรียมความพร้อมนี้ คุณจะ:

1. มีสภาพแวดล้อมการพัฒนาที่ตั้งค่าไว้อย่างสมบูรณ์สำหรับการพัฒนา EdgeAI
2. เข้าใจความต้องการด้านฮาร์ดแวร์และซอฟต์แวร์สำหรับสถานการณ์การใช้งานที่แตกต่างกัน
3. คุ้นเคยกับเฟรมเวิร์คและเครื่องมือสำคัญที่ใช้ตลอดหลักสูตร
4. สามารถเลือกโมเดลที่เหมาะสมตามข้อจำกัดของอุปกรณ์และความต้องการ
5. มีความรู้พื้นฐานเกี่ยวกับเทคนิคการปรับแต่งสำหรับการใช้งานบนอุปกรณ์ Edge

## ➡️ ขั้นตอนถัดไป

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้