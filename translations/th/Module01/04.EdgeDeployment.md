<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1f5ed7b55962c3dccafd3da6f9ec252",
  "translation_date": "2025-09-18T07:46:29+00:00",
  "source_file": "Module01/04.EdgeDeployment.md",
  "language_code": "th"
}
-->
# ส่วนที่ 4: แพลตฟอร์มฮาร์ดแวร์สำหรับการปรับใช้ Edge AI

การปรับใช้ Edge AI เป็นขั้นตอนสุดท้ายของการปรับแต่งโมเดลและการเลือกฮาร์ดแวร์ เพื่อนำความสามารถอัจฉริยะไปยังอุปกรณ์ที่สร้างข้อมูลโดยตรง ส่วนนี้จะสำรวจข้อควรพิจารณาในทางปฏิบัติ ความต้องการด้านฮาร์ดแวร์ และประโยชน์เชิงกลยุทธ์ของการปรับใช้ Edge AI บนแพลตฟอร์มต่าง ๆ โดยเน้นที่โซลูชันฮาร์ดแวร์ชั้นนำจาก Intel, Qualcomm, NVIDIA และ Windows AI PCs

## ทรัพยากรสำหรับนักพัฒนา

### เอกสารและแหล่งการเรียนรู้
- [Microsoft Learn: การพัฒนา Edge AI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)
- [ทรัพยากร Edge AI จาก Intel](https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/edge-ai.html)
- [ทรัพยากรสำหรับนักพัฒนาจาก Qualcomm](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- [เอกสาร NVIDIA Jetson](https://developer.nvidia.com/embedded/learn/getting-started-jetson)
- [เอกสาร Windows AI](https://learn.microsoft.com/windows/ai/)

### เครื่องมือและ SDKs
- [ONNX Runtime](https://onnxruntime.ai/) - เฟรมเวิร์กสำหรับการอนุมานข้ามแพลตฟอร์ม
- [OpenVINO Toolkit](https://docs.openvino.ai/) - ชุดเครื่องมือปรับแต่งจาก Intel
- [TensorRT](https://developer.nvidia.com/tensorrt) - SDK สำหรับการอนุมานประสิทธิภาพสูงจาก NVIDIA
- [DirectML](https://learn.microsoft.com/windows/ai/directml/dml-intro) - API สำหรับ ML ที่เร่งด้วยฮาร์ดแวร์จาก Microsoft

## บทนำ

ในส่วนนี้ เราจะสำรวจแง่มุมในทางปฏิบัติของการปรับใช้โมเดล AI บนอุปกรณ์ Edge โดยจะครอบคลุมข้อควรพิจารณาที่สำคัญสำหรับการปรับใช้ Edge ให้ประสบความสำเร็จ การเลือกแพลตฟอร์มฮาร์ดแวร์ และกลยุทธ์การปรับแต่งที่เหมาะสมกับสถานการณ์การประมวลผล Edge ต่าง ๆ

## วัตถุประสงค์การเรียนรู้

เมื่อจบส่วนนี้ คุณจะสามารถ:

- เข้าใจข้อควรพิจารณาที่สำคัญสำหรับการปรับใช้ Edge AI ให้ประสบความสำเร็จ
- ระบุแพลตฟอร์มฮาร์ดแวร์ที่เหมาะสมสำหรับงาน Edge AI ที่แตกต่างกัน
- ตระหนักถึงข้อแลกเปลี่ยนระหว่างโซลูชันฮาร์ดแวร์ Edge AI ต่าง ๆ
- ใช้เทคนิคการปรับแต่งที่เฉพาะเจาะจงสำหรับแพลตฟอร์มฮาร์ดแวร์ Edge AI ต่าง ๆ

## ข้อควรพิจารณาสำหรับการปรับใช้ Edge AI

การปรับใช้ AI บนอุปกรณ์ Edge มีความท้าทายและความต้องการเฉพาะที่แตกต่างจากการปรับใช้บนคลาวด์ การปรับใช้ Edge AI ให้ประสบความสำเร็จต้องพิจารณาปัจจัยหลายประการ:

### ข้อจำกัดของทรัพยากรฮาร์ดแวร์

อุปกรณ์ Edge มักมีทรัพยากรการประมวลผลที่จำกัดเมื่อเทียบกับโครงสร้างพื้นฐานบนคลาวด์:

- **ข้อจำกัดด้านหน่วยความจำ**: อุปกรณ์ Edge หลายตัวมี RAM จำกัด (ตั้งแต่ไม่กี่ MB ถึงไม่กี่ GB)
- **ข้อจำกัดด้านพื้นที่จัดเก็บ**: พื้นที่จัดเก็บถาวรที่จำกัดส่งผลต่อขนาดโมเดลและการจัดการข้อมูล
- **พลังการประมวลผล**: ความสามารถของ CPU/GPU/NPU ที่จำกัดส่งผลต่อความเร็วในการอนุมาน
- **การใช้พลังงาน**: อุปกรณ์ Edge หลายตัวทำงานด้วยพลังงานจากแบตเตอรี่หรือมีข้อจำกัดด้านความร้อน

### ข้อควรพิจารณาด้านการเชื่อมต่อ

Edge AI ต้องทำงานได้อย่างมีประสิทธิภาพแม้ในสภาพการเชื่อมต่อที่แปรปรวน:

- **การเชื่อมต่อที่ไม่ต่อเนื่อง**: การทำงานต้องดำเนินต่อไปในช่วงที่เครือข่ายขัดข้อง
- **ข้อจำกัดด้านแบนด์วิดท์**: ความสามารถในการถ่ายโอนข้อมูลที่ลดลงเมื่อเทียบกับศูนย์ข้อมูล
- **ความต้องการด้านความหน่วงต่ำ**: แอปพลิเคชันหลายตัวต้องการการประมวลผลแบบเรียลไทม์หรือใกล้เคียงเรียลไทม์
- **การซิงโครไนซ์ข้อมูล**: การจัดการการประมวลผลในพื้นที่พร้อมการซิงโครไนซ์กับคลาวด์เป็นระยะ

### ความต้องการด้านความปลอดภัยและความเป็นส่วนตัว

Edge AI มีความท้าทายด้านความปลอดภัยเฉพาะ:

- **ความปลอดภัยทางกายภาพ**: อุปกรณ์อาจถูกติดตั้งในสถานที่ที่เข้าถึงได้ทางกายภาพ
- **การปกป้องข้อมูล**: การประมวลผลข้อมูลที่ละเอียดอ่อนบนอุปกรณ์ที่อาจมีความเสี่ยง
- **การตรวจสอบสิทธิ์**: การควบคุมการเข้าถึงฟังก์ชันของอุปกรณ์ Edge อย่างปลอดภัย
- **การจัดการการอัปเดต**: กลไกที่ปลอดภัยสำหรับการอัปเดตโมเดลและซอฟต์แวร์

### การปรับใช้และการจัดการ

ข้อควรพิจารณาในทางปฏิบัติสำหรับการปรับใช้ ได้แก่:

- **การจัดการอุปกรณ์จำนวนมาก**: การปรับใช้ Edge หลายครั้งเกี่ยวข้องกับอุปกรณ์ที่กระจายตัวจำนวนมาก
- **การควบคุมเวอร์ชัน**: การจัดการเวอร์ชันโมเดลในอุปกรณ์ที่กระจายตัว
- **การตรวจสอบ**: การติดตามประสิทธิภาพและการตรวจจับความผิดปกติที่ Edge
- **การจัดการวงจรชีวิต**: ตั้งแต่การปรับใช้ครั้งแรกจนถึงการอัปเดตและการเลิกใช้งาน

## ตัวเลือกแพลตฟอร์มฮาร์ดแวร์สำหรับ Edge AI

### โซลูชัน Edge AI จาก Intel

Intel มีแพลตฟอร์มฮาร์ดแวร์หลายตัวที่ปรับแต่งสำหรับการปรับใช้ Edge AI:

#### Intel NUC

Intel NUC (Next Unit of Computing) ให้ประสิทธิภาพระดับเดสก์ท็อปในรูปแบบที่กะทัดรัด:

- **โปรเซสเซอร์ Intel Core** พร้อมกราฟิก Iris Xe ในตัว
- **RAM**: รองรับสูงสุด 64GB DDR4
- **ความเข้ากันได้กับ Neural Compute Stick 2** สำหรับการเร่งความเร็ว AI เพิ่มเติม
- **เหมาะสำหรับ**: งาน Edge AI ระดับปานกลางถึงซับซ้อนในสถานที่ที่มีแหล่งพลังงาน

[Intel NUC สำหรับ Edge AI](https://www.intel.com/content/www/us/en/products/details/nuc.html)

#### Intel Movidius Vision Processing Units (VPUs)

ฮาร์ดแวร์เฉพาะสำหรับการประมวลผลภาพและการเร่งเครือข่ายประสาท:

- **การใช้พลังงานต่ำมาก** (1-3W โดยทั่วไป)
- **การเร่งเครือข่ายประสาทโดยเฉพาะ**
- **รูปแบบที่กะทัดรัด** สำหรับการรวมเข้ากับกล้องและเซ็นเซอร์
- **เหมาะสำหรับ**: แอปพลิเคชันการประมวลผลภาพที่มีข้อจำกัดด้านพลังงาน

[Intel Movidius VPU](https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html)

#### Intel Neural Compute Stick 2

ตัวเร่งเครือข่ายประสาทแบบ USB ที่เสียบใช้งานได้ทันที:

- **Intel Movidius Myriad X VPU**
- **ประสิทธิภาพสูงสุด 4 TOPS**
- **อินเทอร์เฟซ USB 3.0** สำหรับการรวมที่ง่ายดาย
- **เหมาะสำหรับ**: การสร้างต้นแบบอย่างรวดเร็วและการเพิ่มความสามารถ AI ให้กับระบบที่มีอยู่

[Intel Neural Compute Stick 2](https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html)

#### แนวทางการพัฒนา

Intel มีชุดเครื่องมือ OpenVINO สำหรับการปรับแต่งและปรับใช้โมเดล:

```python
# Example: Using OpenVINO for edge deployment
from openvino.inference_engine import IECore

# Initialize the Inference Engine
ie = IECore()

# Read the network from IR files
net = ie.read_network(model="optimized_model.xml", weights="optimized_model.bin")

# Prepare input and output blobs
input_blob = next(iter(net.input_info))
output_blob = next(iter(net.outputs))

# Load the network to the device (CPU, GPU, MYRIAD, etc.)
exec_net = ie.load_network(network=net, device_name="CPU")

# Prepare input
input_data = preprocess_image("sample.jpg")

# Run inference
result = exec_net.infer(inputs={input_blob: input_data})

# Process output
output = result[output_blob]
```

### โซลูชัน AI จาก Qualcomm

แพลตฟอร์มของ Qualcomm มุ่งเน้นที่แอปพลิเคชันมือถือและแบบฝังตัว:

#### Qualcomm Snapdragon

Snapdragon Systems-on-Chip (SoCs) รวม:

- **Qualcomm AI Engine** พร้อม Hexagon DSP
- **Adreno GPU** สำหรับกราฟิกและการประมวลผลแบบขนาน
- **Kryo CPU** สำหรับการประมวลผลทั่วไป
- **เหมาะสำหรับ**: สมาร์ทโฟน แท็บเล็ต ชุด XR และกล้องอัจฉริยะ

[Qualcomm Snapdragon สำหรับ Edge AI](https://www.qualcomm.com/products/mobile/snapdragon/pcs/product-list)

#### Qualcomm Cloud AI 100

ตัวเร่งการอนุมาน AI สำหรับ Edge โดยเฉพาะ:

- **ประสิทธิภาพ AI สูงสุด 400 TOPS**
- **การใช้พลังงานอย่างมีประสิทธิภาพ** ที่ปรับให้เหมาะสมสำหรับศูนย์ข้อมูลและการปรับใช้ Edge
- **สถาปัตยกรรมที่ปรับขนาดได้** สำหรับสถานการณ์การปรับใช้ต่าง ๆ
- **เหมาะสำหรับ**: แอปพลิเคชัน Edge AI ที่มีปริมาณงานสูงในสภาพแวดล้อมที่ควบคุมได้

[Qualcomm Cloud AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence)

#### Qualcomm RB5/RB6 Robotics Platform

สร้างขึ้นเพื่อหุ่นยนต์และการประมวลผล Edge ขั้นสูง:

- **การเชื่อมต่อ 5G ในตัว**
- **ความสามารถ AI และการประมวลผลภาพขั้นสูง**
- **การรองรับเซ็นเซอร์อย่างครอบคลุม**
- **เหมาะสำหรับ**: หุ่นยนต์อัตโนมัติ โดรน และระบบอุตสาหกรรมอัจฉริยะ

[Qualcomm Robotics Platform](https://www.qualcomm.com/products/application/robotics/qualcomm-robotics-rb6-platform)

#### แนวทางการพัฒนา

Qualcomm มี Neural Processing SDK และ AI Model Efficiency Toolkit:

```python
# Example: Using Qualcomm Neural Processing SDK
from qti.aisw.dlc_utils import modeltools

# Convert your model to DLC format
converter = modeltools.DlcConverter()
converter.convert(
    input_network="model.tflite",
    input_dim=[1, 224, 224, 3],
    output_path="optimized_model.dlc"
)

# Use SNPE runtime for inference
from qti.aisw.snpe_runtime import SnpeRuntime

# Initialize runtime
runtime = SnpeRuntime()
runtime.load("optimized_model.dlc")

# Prepare input
input_tensor = preprocess_image("sample.jpg")

# Run inference
outputs = runtime.execute(input_tensor)

# Process results
predictions = postprocess_output(outputs)
```

### 🎮 โซลูชัน Edge AI จาก NVIDIA

NVIDIA มีแพลตฟอร์มที่เร่งด้วย GPU สำหรับการปรับใช้ Edge:

#### NVIDIA Jetson Family

แพลตฟอร์มการประมวลผล Edge AI ที่สร้างขึ้นโดยเฉพาะ:

##### Jetson Orin Series
- **ประสิทธิภาพ AI สูงสุด 275 TOPS**
- **GPU สถาปัตยกรรม NVIDIA Ampere**
- **การกำหนดค่าพลังงาน** ตั้งแต่ 5W ถึง 60W
- **เหมาะสำหรับ**: หุ่นยนต์ขั้นสูง การวิเคราะห์วิดีโออัจฉริยะ และอุปกรณ์ทางการแพทย์

##### Jetson Nano
- **การประมวลผล AI ระดับเริ่มต้น** (472 GFLOPS)
- **GPU Maxwell 128 คอร์**
- **ประหยัดพลังงาน** (5-10W)
- **เหมาะสำหรับ**: โครงการสำหรับผู้ที่ชื่นชอบ การศึกษา และการปรับใช้ AI อย่างง่าย

[แพลตฟอร์ม NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)

#### NVIDIA Clara Guardian

แพลตฟอร์มสำหรับแอปพลิเคชัน AI ด้านการดูแลสุขภาพ:

- **การตรวจจับแบบเรียลไทม์** สำหรับการติดตามผู้ป่วย
- **สร้างบน Jetson** หรือเซิร์ฟเวอร์ที่เร่งด้วย GPU
- **การปรับแต่งเฉพาะด้านการดูแลสุขภาพ**
- **เหมาะสำหรับ**: โรงพยาบาลอัจฉริยะ การติดตามผู้ป่วย และการถ่ายภาพทางการแพทย์

[NVIDIA Clara Guardian](https://developer.nvidia.com/clara)

#### NVIDIA EGX Platform

โซลูชันการประมวลผล Edge ระดับองค์กร:

- **ปรับขนาดได้ตั้งแต่ NVIDIA A100 ถึง T4 GPUs**
- **โซลูชันเซิร์ฟเวอร์ที่ได้รับการรับรอง** จากพันธมิตร OEM
- **ชุดซอฟต์แวร์ NVIDIA AI Enterprise** รวมอยู่ด้วย
- **เหมาะสำหรับ**: การปรับใช้ Edge AI ขนาดใหญ่ในอุตสาหกรรมและองค์กร

[แพลตฟอร์ม NVIDIA EGX](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)

#### แนวทางการพัฒนา

NVIDIA มี TensorRT สำหรับการปรับใช้โมเดลที่ปรับแต่ง:

```python
# Example: Using TensorRT for optimized inference
import tensorrt as trt
import numpy as np

# Create builder and network
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

# Parse ONNX model
parser = trt.OnnxParser(network, logger)
with open("model.onnx", "rb") as f:
    parser.parse(f.read())

# Create optimization config
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB
config.set_flag(trt.BuilderFlag.FP16)

# Build and serialize engine
engine = builder.build_engine(network, config)
with open("model.trt", "wb") as f:
    f.write(engine.serialize())

# Create runtime and execute
runtime = trt.Runtime(logger)
engine = runtime.deserialize_cuda_engine(open("model.trt", "rb").read())
context = engine.create_execution_context()

# Run inference
input_data = preprocess_image("sample.jpg")
output = run_inference(context, input_data, engine)
```

### Windows AI PCs

Windows AI PCs เป็นหมวดหมู่ใหม่ของฮาร์ดแวร์ Edge AI ที่มี Neural Processing Units (NPUs) เฉพาะ:

#### Qualcomm Snapdragon X Elite/Plus

Windows Copilot+ PCs รุ่นแรกมี:

- **Hexagon NPU** พร้อมประสิทธิภาพ AI 45+ TOPS
- **Qualcomm Oryon CPU** สูงสุด 12 คอร์
- **Adreno GPU** สำหรับกราฟิกและการเร่ง AI เพิ่มเติม
- **เหมาะสำหรับ**: การเพิ่มประสิทธิภาพด้วย AI การสร้างเนื้อหา และการพัฒนาซอฟต์แวร์

[Qualcomm Snapdragon X Elite](https://www.qualcomm.com/snapdragon/laptops)

#### Intel Core Ultra (Meteor Lake และรุ่นถัดไป)

โปรเซสเซอร์ AI PC ของ Intel มี:

- **Intel AI Boost (NPU)** ให้ประสิทธิภาพสูงสุด 10 TOPS
- **Intel Arc GPU** ให้การเร่ง AI เพิ่มเติม
- **คอร์ CPU ประสิทธิภาพและประหยัดพลังงาน**
- **เหมาะสำหรับ**: แล็ปท็อปธุรกิจ เวิร์กสเตชันสำหรับงานสร้างสรรค์ และการประมวลผล AI ในชีวิตประจำวัน

[โปรเซสเซอร์ Intel Core Ultra](https://www.intel.com/content/www/us/en/products/details/processors/core/ultra.html)

#### AMD Ryzen AI Series

โปรเซสเซอร์ที่เน้น AI ของ AMD รวมถึง:

- **XDNA-based NPU** ให้ประสิทธิภาพสูงสุด 16 TOPS
- **คอร์ CPU Zen 4** สำหรับการประมวลผลทั่วไป
- **กราฟิก RDNA 3** สำหรับความสามารถในการประมวลผลเพิ่มเติม
- **เหมาะสำหรับ**: มืออาชีพด้านการสร้างสรรค์ นักพัฒนา และผู้ใช้ที่ต้องการประสิทธิภาพสูง

[โปรเซสเซอร์ AMD Ryzen AI](https://www.amd.com/en/processors/ryzen-ai.html)

#### แนวทางการพัฒนา

Windows AI PCs ใช้ Windows Developer Platform และ DirectML:

```csharp
// Example: Using Windows App SDK and DirectML
using Microsoft.AI.DirectML;
using Microsoft.Windows.AI;

// Load model
var modelPath = "optimized_model.onnx";
var modelOptions = new OnnxModelOptions
{
    InterOpNumThreads = 4,
    IntraOpNumThreads = 4
};

// Create model
var model = await OnnxModel.CreateFromFileAsync(modelPath, modelOptions);

// Prepare input
var inputFeatures = new List<InputFeature>
{
    new InputFeature
    {
        Name = "input",
        Value = imageData
    }
};

// Run inference
var results = await model.EvaluateAsync(inputFeatures);

// Process output
var output = results.First().Value;
```

## ⚡ เทคนิคการปรับแต่งเฉพาะฮาร์ดแวร์

### 🔍 แนวทางการปรับลดความแม่นยำ (Quantization)

แพลตฟอร์มฮาร์ดแวร์ต่าง ๆ ได้ประโยชน์จากเทคนิคการปรับลดความแม่นยำที่เฉพาะเจาะจง:

#### การปรับแต่งด้วย Intel OpenVINO
- **การปรับลดความแม่นยำเป็น INT8** สำหรับ CPU และ GPU ในตัว
- **ความแม่นยำ FP16** เพื่อเพิ่มประสิทธิภาพโดยสูญเสียความแม่นยำน้อยที่สุด
- **การปรับลดความแม่นยำแบบไม่สมมาตร** สำหรับการจัดการการกระจายของการกระตุ้น

#### การปรับแต่งด้วย Qualcomm AI Engine
- **การปรับลดความแม่นยำเป็น UINT8** สำหรับ Hexagon DSP
- **ความแม่นยำแบบผสม** ที่ใช้หน่วยประมวลผลทั้งหมดที่มี
- **การปรับลดความแม่นยำแบบต่อช่องสัญญาณ** เพื่อเพิ่มความแม่นยำ

#### การปรับแต่งด้วย NVIDIA TensorRT
- **ความแม่นยำ INT8 และ FP16** สำหรับการเร่ง GPU
- **การรวมเลเยอร์** เพื่อลดการถ่ายโอนหน่วยความจำ
- **การปรับแต่งเคอร์เนลอัตโนมัติ** สำหรับสถาปัตยกรรม GPU เฉพาะ

#### การปรับแต่งด้วย Windows NPU
- **การปรับลดความแม่นยำเป็น INT8/INT4** สำหรับการประมวลผลบน NPU
- **การปรับแต่งกราฟด้วย DirectML**
- **การเร่งด้วย Windows ML runtime**

### การปรับตัวตามสถาปัตยกรรม

ฮาร์ดแวร์ต่าง ๆ ต้องการการพิจารณาด้านสถาปัตยกรรมที่เฉพาะเจาะจง:

- **Intel**: ปรับให้เหมาะสมสำหรับคำสั่งเวกเตอร์ AVX-512 และ Intel Deep Learning Boost
- **Qualcomm**: ใช้การประมวลผลแบบเฮเทอโรจีนัสระหว่าง Hexagon DSP, Adreno GPU และ Kryo CPU
- **NVIDIA**: เพิ่มการประมวล
- **การจัดการการอัปเดต**: กลไกการอัปเดต OTA สำหรับโมเดลและซอฟต์แวร์

### รูปแบบการทำงานแบบไฮบริดระหว่างคลาวด์และเอดจ์

- **การฝึกในคลาวด์, การวิเคราะห์ในเอดจ์**: ฝึกโมเดลในคลาวด์และนำไปใช้งานที่เอดจ์
- **การประมวลผลเบื้องต้นในเอดจ์, การวิเคราะห์ในคลาวด์**: ประมวลผลพื้นฐานในเอดจ์และวิเคราะห์เชิงลึกในคลาวด์
- **การเรียนรู้แบบกระจาย**: พัฒนาปรับปรุงโมเดลโดยไม่ต้องรวมข้อมูลไว้ที่ศูนย์กลาง
- **การเรียนรู้แบบเพิ่มพูน**: ปรับปรุงโมเดลอย่างต่อเนื่องจากข้อมูลที่เอดจ์

### รูปแบบการบูรณาการ

- **การบูรณาการเซ็นเซอร์**: เชื่อมต่อโดยตรงกับกล้อง, ไมโครโฟน และเซ็นเซอร์อื่น ๆ
- **การควบคุมแอคชูเอเตอร์**: ควบคุมมอเตอร์, หน้าจอ และอุปกรณ์เอาต์พุตอื่น ๆ แบบเรียลไทม์
- **การบูรณาการระบบ**: สื่อสารกับระบบองค์กรที่มีอยู่
- **การบูรณาการ IoT**: เชื่อมต่อกับระบบนิเวศ IoT ที่กว้างขึ้น

## ข้อควรพิจารณาในการปรับใช้เฉพาะอุตสาหกรรม

### การดูแลสุขภาพ

- **ความเป็นส่วนตัวของผู้ป่วย**: ปฏิบัติตามข้อกำหนด HIPAA สำหรับข้อมูลทางการแพทย์
- **ข้อกำหนดด้านอุปกรณ์ทางการแพทย์**: ข้อกำหนดของ FDA และหน่วยงานกำกับดูแลอื่น ๆ
- **ข้อกำหนดด้านความน่าเชื่อถือ**: ความทนทานต่อความผิดพลาดสำหรับแอปพลิเคชันที่สำคัญ
- **มาตรฐานการบูรณาการ**: FHIR, HL7 และมาตรฐานการทำงานร่วมกันในด้านการดูแลสุขภาพอื่น ๆ

### การผลิต

- **สภาพแวดล้อมอุตสาหกรรม**: การออกแบบให้ทนทานต่อสภาพแวดล้อมที่รุนแรง
- **ข้อกำหนดแบบเรียลไทม์**: ประสิทธิภาพที่แน่นอนสำหรับระบบควบคุม
- **ระบบความปลอดภัย**: การบูรณาการกับโปรโตคอลความปลอดภัยในอุตสาหกรรม
- **การบูรณาการระบบเดิม**: เชื่อมต่อกับโครงสร้างพื้นฐาน OT ที่มีอยู่

### ยานยนต์

- **ความปลอดภัยในการทำงาน**: ปฏิบัติตามข้อกำหนด ISO 26262
- **การปรับตัวต่อสภาพแวดล้อม**: การทำงานในสภาพอุณหภูมิที่หลากหลาย
- **การจัดการพลังงาน**: การทำงานที่ประหยัดแบตเตอรี่
- **การจัดการวงจรชีวิต**: การสนับสนุนระยะยาวสำหรับอายุการใช้งานของยานพาหนะ

### เมืองอัจฉริยะ

- **การปรับใช้กลางแจ้ง**: ความทนทานต่อสภาพอากาศและความปลอดภัยทางกายภาพ
- **การจัดการขนาด**: อุปกรณ์ที่กระจายตั้งแต่หลักพันถึงหลักล้าน
- **ความหลากหลายของเครือข่าย**: การทำงานในสภาพการเชื่อมต่อที่ไม่แน่นอน
- **ข้อควรพิจารณาด้านความเป็นส่วนตัว**: การจัดการข้อมูลในพื้นที่สาธารณะอย่างมีความรับผิดชอบ

## แนวโน้มในอนาคตของฮาร์ดแวร์ Edge AI

### การพัฒนาฮาร์ดแวร์ที่เกิดขึ้นใหม่

- **ซิลิคอนเฉพาะ AI**: NPU และตัวเร่ง AI ที่มีความเชี่ยวชาญมากขึ้น
- **การคำนวณแบบประสาทเทียม**: สถาปัตยกรรมที่ได้รับแรงบันดาลใจจากสมองเพื่อเพิ่มประสิทธิภาพ
- **การคำนวณในหน่วยความจำ**: ลดการเคลื่อนย้ายข้อมูลสำหรับการดำเนินการ AI
- **การบรรจุหลายชิป**: การบูรณาการที่หลากหลายของตัวประมวลผล AI เฉพาะทาง

### การพัฒนาร่วมกันระหว่างซอฟต์แวร์และฮาร์ดแวร์

- **การค้นหาสถาปัตยกรรมประสาทที่คำนึงถึงฮาร์ดแวร์**: โมเดลที่ปรับให้เหมาะสมกับฮาร์ดแวร์เฉพาะ
- **ความก้าวหน้าของคอมไพเลอร์**: การแปลงโมเดลเป็นคำสั่งฮาร์ดแวร์ที่ดีขึ้น
- **การปรับแต่งกราฟเฉพาะทาง**: การเปลี่ยนแปลงเครือข่ายที่เหมาะสมกับฮาร์ดแวร์
- **การปรับตัวแบบไดนามิก**: การปรับแต่งในระหว่างการทำงานตามทรัพยากรที่มีอยู่

### ความพยายามในการสร้างมาตรฐาน

- **ONNX และ ONNX Runtime**: การทำงานร่วมกันของโมเดลข้ามแพลตฟอร์ม
- **MLIR**: การแสดงผลระดับกลางหลายระดับสำหรับ ML
- **OpenXLA**: การคอมไพล์พีชคณิตเชิงเส้นที่เร่งความเร็ว
- **TMUL**: ชั้นนามธรรมของตัวประมวลผลเทนเซอร์

## เริ่มต้นใช้งานการปรับใช้ Edge AI

### การตั้งค่าสภาพแวดล้อมการพัฒนา

1. **เลือกฮาร์ดแวร์เป้าหมาย**: เลือกแพลตฟอร์มที่เหมาะสมกับกรณีการใช้งานของคุณ
2. **ติดตั้ง SDK และเครื่องมือ**: ตั้งค่าชุดพัฒนาของผู้ผลิต
3. **กำหนดค่าเครื่องมือเพิ่มประสิทธิภาพ**: ติดตั้งซอฟต์แวร์การปรับลดขนาดและการคอมไพล์
4. **ตั้งค่าท่อ CI/CD**: สร้างเวิร์กโฟลว์การทดสอบและการปรับใช้อัตโนมัติ

### รายการตรวจสอบการปรับใช้

- **การเพิ่มประสิทธิภาพโมเดล**: การลดขนาด, การตัดแต่ง และการปรับแต่งสถาปัตยกรรม
- **การทดสอบประสิทธิภาพ**: ทดสอบบนฮาร์ดแวร์เป้าหมายภายใต้เงื่อนไขที่สมจริง
- **การวิเคราะห์พลังงาน**: วัดรูปแบบการใช้พลังงาน
- **การตรวจสอบความปลอดภัย**: ตรวจสอบการปกป้องข้อมูลและการควบคุมการเข้าถึง
- **กลไกการอัปเดต**: ใช้ความสามารถในการอัปเดตที่ปลอดภัย
- **การตั้งค่าการตรวจสอบ**: ติดตั้งการรวบรวมข้อมูลและการแจ้งเตือน

## ➡️ ขั้นตอนต่อไป

- ทบทวน [Module 1 Overview](./README.md)
- สำรวจ [Module 2: Small Language Model Foundations](../Module02/README.md)
- ดำเนินการต่อที่ [Module 3: SLM Deployment Strategies](../Module03/README.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้