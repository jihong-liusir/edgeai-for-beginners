<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T08:01:40+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "th"
}
-->
# ส่วนที่ 2 : คู่มือการใช้งาน Llama.cpp

## สารบัญ
1. [บทนำ](../../../Module04)
2. [Llama.cpp คืออะไร?](../../../Module04)
3. [การติดตั้ง](../../../Module04)
4. [การสร้างจากซอร์สโค้ด](../../../Module04)
5. [การลดขนาดโมเดล](../../../Module04)
6. [การใช้งานพื้นฐาน](../../../Module04)
7. [ฟีเจอร์ขั้นสูง](../../../Module04)
8. [การใช้งานร่วมกับ Python](../../../Module04)
9. [การแก้ไขปัญหา](../../../Module04)
10. [แนวทางปฏิบัติที่ดีที่สุด](../../../Module04)

## บทนำ

คู่มือฉบับสมบูรณ์นี้จะช่วยให้คุณเข้าใจทุกสิ่งเกี่ยวกับ Llama.cpp ตั้งแต่การติดตั้งพื้นฐานไปจนถึงการใช้งานขั้นสูง Llama.cpp เป็นการพัฒนาในภาษา C++ ที่ทรงพลัง ซึ่งช่วยให้การประมวลผลโมเดลภาษาขนาดใหญ่ (LLMs) มีประสิทธิภาพสูง โดยใช้การตั้งค่าที่น้อยที่สุดและรองรับการทำงานบนฮาร์ดแวร์หลากหลายประเภท

## Llama.cpp คืออะไร?

Llama.cpp เป็นเฟรมเวิร์กสำหรับการประมวลผลโมเดลภาษาขนาดใหญ่ (LLM) ที่เขียนด้วยภาษา C/C++ ซึ่งช่วยให้สามารถรันโมเดลภาษาขนาดใหญ่ได้ในเครื่องของคุณโดยไม่ต้องตั้งค่าซับซ้อน และยังมีประสิทธิภาพสูงสุดบนฮาร์ดแวร์หลากหลายประเภท

### ฟีเจอร์หลัก
- **การพัฒนาในภาษา C/C++ แบบเรียบง่าย** โดยไม่มีการพึ่งพาไลบรารีอื่น
- **รองรับการทำงานข้ามแพลตฟอร์ม** (Windows, macOS, Linux)
- **การปรับแต่งฮาร์ดแวร์** สำหรับสถาปัตยกรรมต่าง ๆ
- **รองรับการลดขนาดโมเดล** (การลดขนาดข้อมูลตั้งแต่ 1.5-bit ถึง 8-bit)
- **การเร่งความเร็วด้วย CPU และ GPU**
- **การใช้หน่วยความจำอย่างมีประสิทธิภาพ** สำหรับสภาพแวดล้อมที่มีข้อจำกัด

### ข้อดี
- ทำงานได้ดีบน CPU โดยไม่ต้องใช้ฮาร์ดแวร์เฉพาะทาง
- รองรับ GPU หลายประเภท (CUDA, Metal, OpenCL, Vulkan)
- น้ำหนักเบาและพกพาได้ง่าย
- Apple Silicon ได้รับการปรับแต่งเป็นพิเศษผ่าน ARM NEON, Accelerate และ Metal frameworks
- รองรับการลดขนาดโมเดลหลายระดับเพื่อประหยัดหน่วยความจำ

## การติดตั้ง

### วิธีที่ 1: ไฟล์ไบนารีสำเร็จรูป (แนะนำสำหรับผู้เริ่มต้น)

#### ดาวน์โหลดจาก GitHub Releases
1. ไปที่ [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. ดาวน์โหลดไฟล์ไบนารีที่เหมาะสมกับระบบของคุณ:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` สำหรับ Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` สำหรับ macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` สำหรับ Linux

3. แตกไฟล์และเพิ่มไดเรกทอรีลงใน PATH ของระบบ

#### การใช้ตัวจัดการแพ็กเกจ

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (การแจกจ่ายต่าง ๆ):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### วิธีที่ 2: แพ็กเกจ Python (llama-cpp-python)

#### การติดตั้งพื้นฐาน
```bash
pip install llama-cpp-python
```

#### พร้อมการเร่งความเร็วฮาร์ดแวร์
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## การสร้างจากซอร์สโค้ด

### ข้อกำหนดเบื้องต้น

**ความต้องการของระบบ:**
- คอมไพเลอร์ C++ (GCC, Clang หรือ MSVC)
- CMake (เวอร์ชัน 3.14 หรือสูงกว่า)
- Git
- เครื่องมือสำหรับการสร้างที่เหมาะสมกับแพลตฟอร์มของคุณ

**การติดตั้งข้อกำหนดเบื้องต้น:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- ติดตั้ง Visual Studio 2022 พร้อมเครื่องมือพัฒนา C++
- ติดตั้ง CMake จากเว็บไซต์ทางการ
- ติดตั้ง Git

### ขั้นตอนการสร้างพื้นฐาน

1. **โคลน repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **ตั้งค่าการสร้าง:**
```bash
cmake -B build
```

3. **สร้างโปรเจกต์:**
```bash
cmake --build build --config Release
```

สำหรับการคอมไพล์ที่เร็วขึ้น ให้ใช้ parallel jobs:
```bash
cmake --build build --config Release -j 8
```

### การสร้างเฉพาะฮาร์ดแวร์

#### รองรับ CUDA (GPU ของ NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### รองรับ Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### รองรับ OpenBLAS (การปรับแต่ง CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### รองรับ Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### ตัวเลือกการสร้างขั้นสูง

#### การสร้างแบบ Debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### พร้อมฟีเจอร์เพิ่มเติม
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## การลดขนาดโมเดล

### การทำความเข้าใจรูปแบบ GGUF

GGUF (Generalized GGML Unified Format) เป็นรูปแบบไฟล์ที่ปรับแต่งเพื่อการประมวลผลโมเดลภาษาขนาดใหญ่ได้อย่างมีประสิทธิภาพ โดยใช้ Llama.cpp และเฟรมเวิร์กอื่น ๆ ซึ่งมีข้อดีดังนี้:

- การจัดเก็บน้ำหนักโมเดลแบบมาตรฐาน
- ความเข้ากันได้ที่ดีขึ้นระหว่างแพลตฟอร์ม
- ประสิทธิภาพที่เพิ่มขึ้น
- การจัดการเมตาดาต้าอย่างมีประสิทธิภาพ

### ประเภทการลดขนาด

Llama.cpp รองรับการลดขนาดหลายระดับ:

| ประเภท | จำนวนบิต | คำอธิบาย | กรณีการใช้งาน |
|--------|----------|-----------|----------------|
| F16 | 16 | ความแม่นยำครึ่งหนึ่ง | คุณภาพสูง, ใช้หน่วยความจำมาก |
| Q8_0 | 8 | การลดขนาด 8-bit | สมดุลที่ดี |
| Q4_0 | 4 | การลดขนาด 4-bit | คุณภาพปานกลาง, ขนาดเล็กลง |
| Q2_K | 2 | การลดขนาด 2-bit | ขนาดเล็กที่สุด, คุณภาพต่ำกว่า |

### การแปลงโมเดล

#### จาก PyTorch เป็น GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### ดาวน์โหลดโดยตรงจาก Hugging Face
มีโมเดลในรูปแบบ GGUF ให้ดาวน์โหลดบน Hugging Face:
- ค้นหาโมเดลที่มี "GGUF" ในชื่อ
- ดาวน์โหลดระดับการลดขนาดที่เหมาะสม
- ใช้งานได้ทันทีกับ Llama.cpp

## การใช้งานพื้นฐาน

### การใช้งานผ่าน Command Line

#### การสร้างข้อความแบบง่าย
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### การใช้โมเดลจาก Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### โหมดเซิร์ฟเวอร์
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### พารามิเตอร์ทั่วไป

| พารามิเตอร์ | คำอธิบาย | ตัวอย่าง |
|-------------|----------|----------|
| `-m` | เส้นทางไฟล์โมเดล | `-m model.gguf` |
| `-p` | ข้อความเริ่มต้น | `-p "Hello world"` |
| `-n` | จำนวนโทเค็นที่ต้องการสร้าง | `-n 100` |
| `-c` | ขนาดบริบท | `-c 4096` |
| `-t` | จำนวนเธรด | `-t 8` |
| `-ngl` | เลเยอร์ GPU | `-ngl 32` |
| `-temp` | อุณหภูมิ | `-temp 0.7` |

### โหมดโต้ตอบ

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## ฟีเจอร์ขั้นสูง

### API เซิร์ฟเวอร์

#### การเริ่มต้นเซิร์ฟเวอร์
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### การใช้งาน API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### การปรับแต่งประสิทธิภาพ

#### การจัดการหน่วยความจำ
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### การใช้งานหลายเธรด
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### การเร่งความเร็วด้วย GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## การใช้งานร่วมกับ Python

### การใช้งานพื้นฐานกับ llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### อินเทอร์เฟซแชท

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### การตอบกลับแบบสตรีม

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### การใช้งานร่วมกับ LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## การแก้ไขปัญหา

### ปัญหาทั่วไปและวิธีแก้ไข

#### ข้อผิดพลาดในการสร้าง

**ปัญหา: ไม่พบ CMake**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**ปัญหา: ไม่พบคอมไพเลอร์**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### ปัญหาระหว่างการใช้งาน

**ปัญหา: การโหลดโมเดลล้มเหลว**
- ตรวจสอบเส้นทางไฟล์โมเดล
- ตรวจสอบสิทธิ์ไฟล์
- ตรวจสอบ RAM ที่เพียงพอ
- ลองใช้ระดับการลดขนาดที่แตกต่างกัน

**ปัญหา: ประสิทธิภาพต่ำ**
- เปิดใช้งานการเร่งความเร็วฮาร์ดแวร์
- เพิ่มจำนวนเธรด
- ใช้ระดับการลดขนาดที่เหมาะสม
- ตรวจสอบการใช้งานหน่วยความจำ GPU

#### ปัญหาหน่วยความจำ

**ปัญหา: หน่วยความจำไม่เพียงพอ**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### ปัญหาเฉพาะแพลตฟอร์ม

#### Windows
- ใช้คอมไพเลอร์ MinGW หรือ Visual Studio
- ตรวจสอบการตั้งค่า PATH
- ตรวจสอบการรบกวนจากโปรแกรมป้องกันไวรัส

#### macOS
- เปิดใช้งาน Metal สำหรับ Apple Silicon
- ใช้ Rosetta 2 เพื่อความเข้ากันได้หากจำเป็น
- ตรวจสอบเครื่องมือคำสั่ง Xcode

#### Linux
- ติดตั้งแพ็กเกจสำหรับการพัฒนา
- ตรวจสอบเวอร์ชันไดรเวอร์ GPU
- ตรวจสอบการติดตั้ง CUDA toolkit

## แนวทางปฏิบัติที่ดีที่สุด

### การเลือกโมเดล
1. **เลือกการลดขนาดที่เหมาะสม** ตามฮาร์ดแวร์ของคุณ
2. **พิจารณาขนาดโมเดล** เทียบกับคุณภาพ
3. **ทดลองโมเดลต่าง ๆ** สำหรับกรณีการใช้งานเฉพาะของคุณ

### การปรับแต่งประสิทธิภาพ
1. **ใช้การเร่งความเร็ว GPU** เมื่อมี
2. **ปรับจำนวนเธรด** ให้เหมาะสมกับ CPU ของคุณ
3. **ตั้งค่าขนาดบริบท** ให้เหมาะสมกับกรณีการใช้งาน
4. **เปิดใช้งานการแมปหน่วยความจำ** สำหรับโมเดลขนาดใหญ่

### การใช้งานในระบบผลิต
1. **ใช้โหมดเซิร์ฟเวอร์** สำหรับการเข้าถึง API
2. **จัดการข้อผิดพลาดอย่างเหมาะสม**
3. **ตรวจสอบการใช้งานทรัพยากร**
4. **ตั้งค่าการบันทึกและการตรวจสอบ**

### เวิร์กโฟลว์การพัฒนา
1. **เริ่มต้นด้วยโมเดลขนาดเล็ก** สำหรับการทดสอบ
2. **ใช้การควบคุมเวอร์ชัน** สำหรับการตั้งค่าโมเดล
3. **บันทึกการตั้งค่าของคุณ**
4. **ทดสอบบนแพลตฟอร์มต่าง ๆ**

### การพิจารณาด้านความปลอดภัย
1. **ตรวจสอบข้อความที่ป้อน**
2. **ตั้งค่าการจำกัดอัตรา**
3. **รักษาความปลอดภัย API**
4. **ตรวจสอบรูปแบบการใช้งานที่ไม่เหมาะสม**

## สรุป

Llama.cpp เป็นเครื่องมือที่ทรงพลังและมีประสิทธิภาพสำหรับการรันโมเดลภาษาขนาดใหญ่ในเครื่องของคุณ โดยรองรับฮาร์ดแวร์หลากหลายประเภท ไม่ว่าคุณจะพัฒนาแอปพลิเคชัน AI ทำการวิจัย หรือทดลองใช้งาน LLMs เฟรมเวิร์กนี้มอบความยืดหยุ่นและประสิทธิภาพที่เหมาะสมสำหรับกรณีการใช้งานที่หลากหลาย

สิ่งสำคัญที่ควรทราบ:
- เลือกวิธีการติดตั้งที่เหมาะสมกับความต้องการของคุณ
- ปรับแต่งให้เหมาะสมกับฮาร์ดแวร์ของคุณ
- เริ่มต้นด้วยการใช้งานพื้นฐานและค่อย ๆ สำรวจฟีเจอร์ขั้นสูง
- พิจารณาใช้ Python bindings เพื่อการผสานรวมที่ง่ายขึ้น
- ปฏิบัติตามแนวทางที่ดีที่สุดสำหรับการใช้งานในระบบผลิต

สำหรับข้อมูลเพิ่มเติมและการอัปเดต โปรดเยี่ยมชม [repository Llama.cpp อย่างเป็นทางการ](https://github.com/ggml-org/llama.cpp) และดูเอกสารและทรัพยากรชุมชนที่มีให้

## ➡️ สิ่งที่ควรทำต่อไป

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้