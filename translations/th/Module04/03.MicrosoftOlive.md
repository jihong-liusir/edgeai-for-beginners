<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T19:05:20+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "th"
}
-->
# ส่วนที่ 3 : Microsoft Olive Optimization Suite

## สารบัญ
1. [บทนำ](../../../Module04)
2. [Microsoft Olive คืออะไร?](../../../Module04)
3. [การติดตั้ง](../../../Module04)
4. [คู่มือเริ่มต้นใช้งาน](../../../Module04)
5. [ตัวอย่าง: การแปลง Qwen3 เป็น ONNX INT4](../../../Module04)
6. [การใช้งานขั้นสูง](../../../Module04)
7. [แนวทางปฏิบัติที่ดีที่สุด](../../../Module04)
8. [การแก้ไขปัญหา](../../../Module04)
9. [แหล่งข้อมูลเพิ่มเติม](../../../Module04)

## บทนำ

Microsoft Olive เป็นเครื่องมือที่ทรงพลังและใช้งานง่ายสำหรับการปรับแต่งโมเดลที่คำนึงถึงฮาร์ดแวร์ ช่วยให้การปรับแต่งโมเดลการเรียนรู้ของเครื่องเพื่อการใช้งานบนแพลตฟอร์มฮาร์ดแวร์ต่างๆ เป็นเรื่องง่าย ไม่ว่าคุณจะกำหนดเป้าหมายไปที่ CPU, GPU หรือ AI accelerators เฉพาะทาง Olive จะช่วยให้คุณได้ประสิทธิภาพสูงสุดโดยยังคงความแม่นยำของโมเดลไว้

## Microsoft Olive คืออะไร?

Olive เป็นเครื่องมือปรับแต่งโมเดลที่คำนึงถึงฮาร์ดแวร์ ซึ่งรวบรวมเทคนิคชั้นนำในอุตสาหกรรมสำหรับการบีบอัดโมเดล การปรับแต่ง และการคอมไพล์ โดยทำงานร่วมกับ ONNX Runtime เพื่อเป็นโซลูชันการปรับแต่งการอนุมานแบบครบวงจร

### คุณสมบัติเด่น

- **การปรับแต่งที่คำนึงถึงฮาร์ดแวร์**: เลือกเทคนิคการปรับแต่งที่ดีที่สุดสำหรับฮาร์ดแวร์เป้าหมายโดยอัตโนมัติ
- **ส่วนประกอบการปรับแต่งในตัวกว่า 40+**: ครอบคลุมการบีบอัดโมเดล การปรับแต่งกราฟ และอื่นๆ
- **อินเทอร์เฟซ CLI ที่ใช้งานง่าย**: คำสั่งง่ายๆ สำหรับงานปรับแต่งทั่วไป
- **รองรับหลายเฟรมเวิร์ก**: ใช้งานได้กับ PyTorch, Hugging Face models และ ONNX
- **รองรับโมเดลยอดนิยม**: Olive สามารถปรับแต่งสถาปัตยกรรมโมเดลยอดนิยม เช่น Llama, Phi, Qwen, Gemma เป็นต้น ได้โดยอัตโนมัติ

### ประโยชน์

- **ลดเวลาในการพัฒนา**: ไม่ต้องทดลองเทคนิคการปรับแต่งต่างๆ ด้วยตัวเอง
- **เพิ่มประสิทธิภาพ**: ปรับปรุงความเร็วได้อย่างมาก (สูงสุดถึง 6 เท่าในบางกรณี)
- **การใช้งานข้ามแพลตฟอร์ม**: โมเดลที่ปรับแต่งแล้วสามารถใช้งานได้บนฮาร์ดแวร์และระบบปฏิบัติการต่างๆ
- **รักษาความแม่นยำ**: การปรับแต่งยังคงคุณภาพของโมเดลไว้ในขณะที่เพิ่มประสิทธิภาพ

## การติดตั้ง

### ข้อกำหนดเบื้องต้น

- Python 3.8 หรือสูงกว่า
- ตัวจัดการแพ็กเกจ pip
- สภาพแวดล้อมเสมือน (แนะนำ)

### การติดตั้งพื้นฐาน

สร้างและเปิดใช้งานสภาพแวดล้อมเสมือน:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ติดตั้ง Olive พร้อมฟีเจอร์การปรับแต่งอัตโนมัติ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### การติดตั้งส่วนเสริม (ไม่บังคับ)

Olive มีส่วนเสริมต่างๆ สำหรับฟีเจอร์เพิ่มเติม:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ตรวจสอบการติดตั้ง

```bash
olive --help
```

หากสำเร็จ คุณควรเห็นข้อความช่วยเหลือของ Olive CLI

## คู่มือเริ่มต้นใช้งาน

### การปรับแต่งครั้งแรกของคุณ

มาลองปรับแต่งโมเดลภาษาขนาดเล็กโดยใช้ฟีเจอร์การปรับแต่งอัตโนมัติของ Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### สิ่งที่คำสั่งนี้ทำ

กระบวนการปรับแต่งประกอบด้วย: การดึงโมเดลจากแคชในเครื่อง การจับ ONNX Graph และจัดเก็บน้ำหนักในไฟล์ข้อมูล ONNX การปรับแต่ง ONNX Graph และการแปลงโมเดลเป็น int4 โดยใช้วิธี RTN

### อธิบายพารามิเตอร์ของคำสั่ง

- `--model_name_or_path`: ตัวระบุโมเดล Hugging Face หรือเส้นทางในเครื่อง
- `--output_path`: ไดเรกทอรีที่โมเดลที่ปรับแต่งแล้วจะถูกบันทึก
- `--device`: อุปกรณ์เป้าหมาย (cpu, gpu)
- `--provider`: ผู้ให้บริการการดำเนินการ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ใช้ ONNX Runtime Generate AI สำหรับการอนุมาน
- `--precision`: ความแม่นยำของการแปลง (int4, int8, fp16)
- `--log_level`: ระดับความละเอียดของการบันทึก (0=น้อยที่สุด, 1=ละเอียด)

## ตัวอย่าง: การแปลง Qwen3 เป็น ONNX INT4

จากตัวอย่างที่ให้ไว้ใน Hugging Face ที่ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) นี่คือวิธีการปรับแต่งโมเดล Qwen3:

### ขั้นตอนที่ 1: ดาวน์โหลดโมเดล (ไม่บังคับ)

เพื่อประหยัดเวลาในการดาวน์โหลด ให้แคชเฉพาะไฟล์ที่จำเป็น:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ขั้นตอนที่ 2: ปรับแต่งโมเดล Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ขั้นตอนที่ 3: ทดสอบโมเดลที่ปรับแต่งแล้ว

สร้างสคริปต์ Python ง่ายๆ เพื่อทดสอบโมเดลที่ปรับแต่งแล้ว:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### โครงสร้างผลลัพธ์

หลังการปรับแต่ง ไดเรกทอรีผลลัพธ์ของคุณจะประกอบด้วย:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## การใช้งานขั้นสูง

### ไฟล์การตั้งค่า

สำหรับเวิร์กโฟลว์การปรับแต่งที่ซับซ้อนยิ่งขึ้น คุณสามารถใช้ไฟล์การตั้งค่า JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

เรียกใช้งานด้วยการตั้งค่า:

```bash
olive run --config config.json
```

### การปรับแต่ง GPU

สำหรับการปรับแต่ง CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

สำหรับ DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### การปรับแต่งโมเดลด้วย Olive

Olive ยังรองรับการปรับแต่งโมเดล:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## แนวทางปฏิบัติที่ดีที่สุด

### 1. การเลือกโมเดล
- เริ่มต้นด้วยโมเดลขนาดเล็กสำหรับการทดสอบ (เช่น 0.5B-7B พารามิเตอร์)
- ตรวจสอบให้แน่ใจว่าสถาปัตยกรรมโมเดลเป้าหมายรองรับโดย Olive

### 2. การพิจารณาฮาร์ดแวร์
- ปรับเป้าหมายการปรับแต่งให้ตรงกับฮาร์ดแวร์ที่คุณจะใช้งาน
- ใช้การปรับแต่ง GPU หากคุณมีฮาร์ดแวร์ที่รองรับ CUDA
- พิจารณา DirectML สำหรับเครื่อง Windows ที่มีกราฟิกในตัว

### 3. การเลือกความแม่นยำ
- **INT4**: การบีบอัดสูงสุด แต่สูญเสียความแม่นยำเล็กน้อย
- **INT8**: สมดุลระหว่างขนาดและความแม่นยำ
- **FP16**: สูญเสียความแม่นยำน้อยที่สุด ลดขนาดปานกลาง

### 4. การทดสอบและการตรวจสอบ
- ทดสอบโมเดลที่ปรับแต่งแล้วกับกรณีการใช้งานเฉพาะของคุณเสมอ
- เปรียบเทียบเมตริกประสิทธิภาพ (latency, throughput, accuracy)
- ใช้ข้อมูลอินพุตที่เป็นตัวแทนสำหรับการประเมินผล

### 5. การปรับแต่งแบบวนซ้ำ
- เริ่มต้นด้วยการปรับแต่งอัตโนมัติเพื่อผลลัพธ์ที่รวดเร็ว
- ใช้ไฟล์การตั้งค่าสำหรับการควบคุมที่ละเอียด
- ทดลองกับการปรับแต่งแบบต่างๆ

## การแก้ไขปัญหา

### ปัญหาทั่วไป

#### 1. ปัญหาการติดตั้ง
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. ปัญหา CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. ปัญหาหน่วยความจำ
- ใช้ขนาด batch ที่เล็กลงระหว่างการปรับแต่ง
- ลองใช้การแปลงที่มีความแม่นยำสูงก่อน (int8 แทน int4)
- ตรวจสอบให้แน่ใจว่ามีพื้นที่ดิสก์เพียงพอสำหรับการแคชโมเดล

#### 4. ข้อผิดพลาดในการโหลดโมเดล
- ตรวจสอบเส้นทางโมเดลและสิทธิ์การเข้าถึง
- ตรวจสอบว่าโมเดลต้องการ `trust_remote_code=True` หรือไม่
- ตรวจสอบให้แน่ใจว่าไฟล์โมเดลที่จำเป็นทั้งหมดถูกดาวน์โหลดแล้ว

### การขอความช่วยเหลือ

- **เอกสารประกอบ**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ตัวอย่าง**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## แหล่งข้อมูลเพิ่มเติม

### ลิงก์ทางการ
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **เอกสาร ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **ตัวอย่าง Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### ตัวอย่างจากชุมชน
- **Jupyter Notebooks**: มีให้ใน Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **ส่วนขยาย VS Code**: ภาพรวม AI Toolkit สำหรับ VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **โพสต์บล็อก**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### เครื่องมือที่เกี่ยวข้อง
- **ONNX Runtime**: เอนจินการอนุมานที่มีประสิทธิภาพสูง — https://onnxruntime.ai/
- **Hugging Face Transformers**: แหล่งที่มาของโมเดลที่เข้ากันได้หลายตัว — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: เวิร์กโฟลว์การปรับแต่งบนคลาวด์ — https://learn.microsoft.com/azure/machine-learning/

## ➡️ สิ่งที่ต้องทำต่อไป

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

