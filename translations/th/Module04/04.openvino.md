<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T07:51:28+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "th"
}
-->
# ส่วนที่ 4 : OpenVINO Toolkit Optimization Suite

## สารบัญ
1. [บทนำ](../../../Module04)
2. [OpenVINO คืออะไร?](../../../Module04)
3. [การติดตั้ง](../../../Module04)
4. [คู่มือเริ่มต้นใช้งาน](../../../Module04)
5. [ตัวอย่าง: การแปลงและปรับแต่งโมเดลด้วย OpenVINO](../../../Module04)
6. [การใช้งานขั้นสูง](../../../Module04)
7. [แนวทางปฏิบัติที่ดีที่สุด](../../../Module04)
8. [การแก้ไขปัญหา](../../../Module04)
9. [แหล่งข้อมูลเพิ่มเติม](../../../Module04)

## บทนำ

OpenVINO (Open Visual Inference and Neural Network Optimization) เป็นเครื่องมือโอเพ่นซอร์สของ Intel สำหรับการนำ AI ที่มีประสิทธิภาพไปใช้งานในระบบคลาวด์, ระบบในองค์กร, และอุปกรณ์ปลายทาง ไม่ว่าคุณจะใช้งานบน CPU, GPU, VPU หรืออุปกรณ์เร่งความเร็ว AI เฉพาะทาง OpenVINO มีความสามารถในการปรับแต่งที่ครอบคลุม พร้อมทั้งรักษาความแม่นยำของโมเดลและรองรับการใช้งานข้ามแพลตฟอร์ม

## OpenVINO คืออะไร?

OpenVINO เป็นเครื่องมือโอเพ่นซอร์สที่ช่วยให้นักพัฒนาสามารถปรับแต่ง, แปลง, และนำโมเดล AI ไปใช้งานได้อย่างมีประสิทธิภาพบนแพลตฟอร์มฮาร์ดแวร์ที่หลากหลาย ประกอบด้วยสามส่วนหลัก: OpenVINO Runtime สำหรับการประมวลผล, Neural Network Compression Framework (NNCF) สำหรับการปรับแต่งโมเดล, และ OpenVINO Model Server สำหรับการใช้งานที่สามารถขยายได้

### คุณสมบัติเด่น

- **การใช้งานข้ามแพลตฟอร์ม**: รองรับ Linux, Windows, และ macOS พร้อม API สำหรับ Python, C++, และ C
- **การเร่งความเร็วฮาร์ดแวร์**: ค้นหาอุปกรณ์อัตโนมัติและปรับแต่งสำหรับ CPU, GPU, VPU และอุปกรณ์เร่งความเร็ว AI
- **กรอบการบีบอัดโมเดล**: เทคนิคการปรับแต่งขั้นสูง เช่น การลดขนาด, การตัดแต่ง และการปรับแต่งผ่าน NNCF
- **ความเข้ากันได้กับเฟรมเวิร์ก**: รองรับโมเดลจาก TensorFlow, ONNX, PaddlePaddle และ PyTorch โดยตรง
- **การสนับสนุน Generative AI**: OpenVINO GenAI สำหรับการใช้งานโมเดลภาษาขนาดใหญ่และแอปพลิเคชัน Generative AI

### ประโยชน์

- **การปรับปรุงประสิทธิภาพ**: เพิ่มความเร็วอย่างมีนัยสำคัญโดยสูญเสียความแม่นยำเพียงเล็กน้อย
- **ลดขนาดการติดตั้ง**: ลดการพึ่งพาภายนอกเพื่อให้ง่ายต่อการติดตั้งและใช้งาน
- **เวลาเริ่มต้นที่เร็วขึ้น**: โหลดโมเดลและแคชที่ปรับแต่งเพื่อการเริ่มต้นแอปพลิเคชันที่เร็วขึ้น
- **การใช้งานที่สามารถขยายได้**: จากอุปกรณ์ปลายทางไปยังโครงสร้างพื้นฐานคลาวด์ด้วย API ที่สม่ำเสมอ
- **พร้อมสำหรับการใช้งานจริง**: ความน่าเชื่อถือระดับองค์กรพร้อมเอกสารประกอบและการสนับสนุนจากชุมชน

## การติดตั้ง

### ข้อกำหนดเบื้องต้น

- Python 3.8 หรือสูงกว่า
- ตัวจัดการแพ็กเกจ pip
- สภาพแวดล้อมเสมือน (แนะนำ)
- ฮาร์ดแวร์ที่รองรับ (แนะนำ Intel CPUs แต่รองรับสถาปัตยกรรมหลากหลาย)

### การติดตั้งพื้นฐาน

สร้างและเปิดใช้งานสภาพแวดล้อมเสมือน:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

ติดตั้ง OpenVINO Runtime:

```bash
pip install openvino
```

ติดตั้ง NNCF สำหรับการปรับแต่งโมเดล:

```bash
pip install nncf
```

### การติดตั้ง OpenVINO GenAI

สำหรับแอปพลิเคชัน Generative AI:

```bash
pip install openvino-genai
```

### การติดตั้งเพิ่มเติม

แพ็กเกจเพิ่มเติมสำหรับการใช้งานเฉพาะ:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### ตรวจสอบการติดตั้ง

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

หากสำเร็จ คุณควรเห็นข้อมูลเวอร์ชันของ OpenVINO

## คู่มือเริ่มต้นใช้งาน

### การปรับแต่งโมเดลครั้งแรกของคุณ

มาลองแปลงและปรับแต่งโมเดล Hugging Face ด้วย OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### สิ่งที่กระบวนการนี้ทำ

กระบวนการปรับแต่งประกอบด้วย: โหลดโมเดลต้นฉบับจาก Hugging Face, แปลงเป็นรูปแบบ Intermediate Representation (IR) ของ OpenVINO, ใช้การปรับแต่งเริ่มต้น, และคอมไพล์สำหรับฮาร์ดแวร์เป้าหมาย

### อธิบายพารามิเตอร์สำคัญ

- `export=True`: แปลงโมเดลเป็นรูปแบบ IR ของ OpenVINO
- `compile=False`: เลื่อนการคอมไพล์ไปจนถึงเวลารันเพื่อความยืดหยุ่น
- `device`: ฮาร์ดแวร์เป้าหมาย ("CPU", "GPU", "AUTO" สำหรับการเลือกอัตโนมัติ)
- `save_pretrained()`: บันทึกโมเดลที่ปรับแต่งเพื่อการใช้งานซ้ำ

## ตัวอย่าง: การแปลงและปรับแต่งโมเดลด้วย OpenVINO

### ขั้นตอนที่ 1: การแปลงโมเดลด้วย NNCF Quantization

นี่คือวิธีการใช้การลดขนาดหลังการฝึกอบรมด้วย NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### ขั้นตอนที่ 2: การปรับแต่งขั้นสูงด้วยการบีบอัดน้ำหนัก

สำหรับโมเดลประเภท transformer ใช้การบีบอัดน้ำหนัก:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### ขั้นตอนที่ 3: การประมวลผลด้วยโมเดลที่ปรับแต่งแล้ว

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### โครงสร้างผลลัพธ์

หลังการปรับแต่ง โฟลเดอร์โมเดลของคุณจะมี:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## การใช้งานขั้นสูง

### การตั้งค่าด้วย NNCF YAML

สำหรับกระบวนการปรับแต่งที่ซับซ้อน ใช้ไฟล์การตั้งค่า NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

ใช้การตั้งค่า:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### การปรับแต่ง GPU

สำหรับการเร่งความเร็ว GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### การปรับแต่งการประมวลผลแบบแบตช์

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### การใช้งาน Model Server

นำโมเดลที่ปรับแต่งไปใช้งานด้วย OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

โค้ดฝั่งไคลเอนต์สำหรับ Model Server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## แนวทางปฏิบัติที่ดีที่สุด

### 1. การเลือกและเตรียมโมเดล
- ใช้โมเดลจากเฟรมเวิร์กที่รองรับ (PyTorch, TensorFlow, ONNX)
- ตรวจสอบให้แน่ใจว่าอินพุตของโมเดลมีรูปร่างคงที่หรือรูปร่างไดนามิกที่ทราบ
- ทดสอบด้วยชุดข้อมูลตัวแทนสำหรับการปรับเทียบ

### 2. การเลือกกลยุทธ์การปรับแต่ง
- **Post-training Quantization**: เริ่มต้นที่นี่สำหรับการปรับแต่งอย่างรวดเร็ว
- **Weight Compression**: เหมาะสำหรับโมเดลภาษาขนาดใหญ่และ transformer
- **Quantization-aware Training**: ใช้เมื่อความแม่นยำมีความสำคัญ

### 3. การปรับแต่งเฉพาะฮาร์ดแวร์
- **CPU**: ใช้ INT8 quantization เพื่อประสิทธิภาพที่สมดุล
- **GPU**: ใช้ความแม่นยำ FP16 และการประมวลผลแบบแบตช์
- **VPU**: เน้นการลดความซับซ้อนของโมเดลและการรวมเลเยอร์

### 4. การปรับแต่งประสิทธิภาพ
- **Throughput Mode**: สำหรับการประมวลผลแบบแบตช์ปริมาณมาก
- **Latency Mode**: สำหรับแอปพลิเคชันแบบโต้ตอบเรียลไทม์
- **AUTO Device**: ให้ OpenVINO เลือกฮาร์ดแวร์ที่เหมาะสมที่สุด

### 5. การจัดการหน่วยความจำ
- ใช้รูปร่างไดนามิกอย่างระมัดระวังเพื่อหลีกเลี่ยงการใช้หน่วยความจำเกิน
- ใช้การแคชโมเดลเพื่อการโหลดที่เร็วขึ้นในครั้งถัดไป
- ตรวจสอบการใช้หน่วยความจำระหว่างการปรับแต่ง

### 6. การตรวจสอบความแม่นยำ
- ตรวจสอบโมเดลที่ปรับแต่งแล้วเทียบกับประสิทธิภาพต้นฉบับเสมอ
- ใช้ชุดข้อมูลทดสอบตัวแทนสำหรับการประเมินผล
- พิจารณาการปรับแต่งแบบค่อยเป็นค่อยไป (เริ่มต้นด้วยการตั้งค่าที่อนุรักษ์นิยม)

## การแก้ไขปัญหา

### ปัญหาทั่วไป

#### 1. ปัญหาการติดตั้ง
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. ข้อผิดพลาดในการแปลงโมเดล
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. ปัญหาด้านประสิทธิภาพ
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. ปัญหาด้านหน่วยความจำ
- ลดขนาดแบตช์ของโมเดลระหว่างการปรับแต่ง
- ใช้การสตรีมสำหรับชุดข้อมูลขนาดใหญ่
- เปิดใช้งานการแคชโมเดล: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. การลดลงของความแม่นยำ
- ใช้ความแม่นยำที่สูงขึ้น (INT8 แทน INT4)
- เพิ่มขนาดชุดข้อมูลการปรับเทียบ
- ใช้การปรับแต่งแบบผสมความแม่นยำ

### การตรวจสอบประสิทธิภาพ

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### การขอความช่วยเหลือ

- **เอกสารประกอบ**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **ปัญหาใน GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **ฟอรัมชุมชน**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## แหล่งข้อมูลเพิ่มเติม

### ลิงก์ทางการ
- **หน้าแรก OpenVINO**: [openvino.ai](https://openvino.ai/)
- **ที่เก็บ GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **ที่เก็บ NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### แหล่งเรียนรู้
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **คู่มือเริ่มต้นใช้งาน**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **คู่มือการปรับแต่ง**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### เครื่องมือการผสานรวม
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### การวัดประสิทธิภาพ
- **การวัดประสิทธิภาพทางการ**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### ตัวอย่างจากชุมชน
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - บทเรียนที่ครอบคลุมในที่เก็บ OpenVINO notebooks
- **แอปพลิเคชันตัวอย่าง**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - ตัวอย่างการใช้งานจริงสำหรับโดเมนต่าง ๆ (การมองเห็นด้วยคอมพิวเตอร์, NLP, เสียง)
- **โพสต์บล็อก**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - บล็อก AI ของ Intel และชุมชนพร้อมกรณีการใช้งานโดยละเอียด

### เครื่องมือที่เกี่ยวข้อง
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - เทคนิคการปรับแต่งเพิ่มเติมสำหรับฮาร์ดแวร์ Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - สำหรับการเปรียบเทียบการใช้งานบนมือถือและอุปกรณ์ปลายทาง
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - ตัวเลือกเครื่องมือประมวลผลข้ามแพลตฟอร์ม

## ➡️ สิ่งที่ควรทำต่อไป

- [05: การเจาะลึก Apple MLX Framework](./05.AppleMLX.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้