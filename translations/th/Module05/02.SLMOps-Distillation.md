<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-18T08:10:48+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "th"
}
-->
# ส่วนที่ 2: การกลั่นแบบจำลอง - จากทฤษฎีสู่การปฏิบัติ

## สารบัญ
1. [แนะนำการกลั่นแบบจำลอง](../../../Module05)
2. [ทำไมการกลั่นจึงสำคัญ](../../../Module05)
3. [กระบวนการกลั่น](../../../Module05)
4. [การนำไปใช้ในทางปฏิบัติ](../../../Module05)
5. [ตัวอย่างการกลั่นใน Azure ML](../../../Module05)
6. [แนวทางปฏิบัติที่ดีที่สุดและการปรับแต่ง](../../../Module05)
7. [การใช้งานในโลกจริง](../../../Module05)
8. [บทสรุป](../../../Module05)

## แนะนำการกลั่นแบบจำลอง {#introduction}

การกลั่นแบบจำลองเป็นเทคนิคที่ทรงพลังที่ช่วยให้เราสร้างแบบจำลองที่เล็กลงและมีประสิทธิภาพมากขึ้น ในขณะที่ยังคงรักษาประสิทธิภาพของแบบจำลองที่ใหญ่และซับซ้อน กระบวนการนี้เกี่ยวข้องกับการฝึกอบรมแบบจำลอง "นักเรียน" ที่กะทัดรัดเพื่อเลียนแบบพฤติกรรมของแบบจำลอง "ครู" ที่ใหญ่กว่า

**ประโยชน์สำคัญ:**
- **ลดความต้องการด้านการประมวลผล** สำหรับการคาดการณ์
- **ลดการใช้หน่วยความจำ** และความต้องการพื้นที่จัดเก็บ
- **เพิ่มความเร็วในการคาดการณ์** ในขณะที่ยังคงรักษาความแม่นยำที่เหมาะสม
- **การใช้งานที่คุ้มค่า** ในสภาพแวดล้อมที่มีทรัพยากรจำกัด

## ทำไมการกลั่นจึงสำคัญ {#why-distillation-matters}

แบบจำลองภาษาขนาดใหญ่ (LLMs) กำลังมีความสามารถเพิ่มขึ้นเรื่อย ๆ แต่ก็ต้องการทรัพยากรมากขึ้นเช่นกัน แม้ว่าแบบจำลองที่มีพารามิเตอร์นับพันล้านอาจให้ผลลัพธ์ที่ยอดเยี่ยม แต่ก็อาจไม่เหมาะสมสำหรับการใช้งานในโลกจริงหลาย ๆ กรณีเนื่องจาก:

### ข้อจำกัดด้านทรัพยากร
- **ภาระการประมวลผล**: แบบจำลองขนาดใหญ่ต้องการหน่วยความจำ GPU และพลังการประมวลผลจำนวนมาก
- **ความล่าช้าในการคาดการณ์**: แบบจำลองที่ซับซ้อนใช้เวลานานในการสร้างคำตอบ
- **การใช้พลังงาน**: แบบจำลองขนาดใหญ่ใช้พลังงานมากขึ้น เพิ่มต้นทุนการดำเนินงาน
- **ต้นทุนโครงสร้างพื้นฐาน**: การโฮสต์แบบจำลองขนาดใหญ่ต้องใช้ฮาร์ดแวร์ราคาแพง

### ข้อจำกัดในทางปฏิบัติ
- **การใช้งานบนมือถือ**: แบบจำลองขนาดใหญ่ไม่สามารถทำงานได้อย่างมีประสิทธิภาพบนอุปกรณ์มือถือ
- **แอปพลิเคชันแบบเรียลไทม์**: แอปพลิเคชันที่ต้องการความล่าช้าต่ำไม่สามารถรองรับการคาดการณ์ที่ช้าได้
- **การประมวลผลที่ปลายทาง**: อุปกรณ์ IoT และปลายทางมีทรัพยากรการประมวลผลจำกัด
- **ข้อจำกัดด้านต้นทุน**: องค์กรหลายแห่งไม่สามารถจ่ายค่าโครงสร้างพื้นฐานสำหรับการใช้งานแบบจำลองขนาดใหญ่ได้

## กระบวนการกลั่น {#the-distillation-process}

การกลั่นแบบจำลองเป็นกระบวนการสองขั้นตอนที่ถ่ายโอนความรู้จากแบบจำลองครูไปยังแบบจำลองนักเรียน:

### ขั้นตอนที่ 1: การสร้างข้อมูลสังเคราะห์

แบบจำลองครูสร้างคำตอบสำหรับชุดข้อมูลการฝึกอบรมของคุณ สร้างข้อมูลสังเคราะห์คุณภาพสูงที่สะท้อนรูปแบบความรู้และการให้เหตุผลของครู

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**ประเด็นสำคัญของขั้นตอนนี้:**
- แบบจำลองครูประมวลผลตัวอย่างการฝึกอบรมแต่ละรายการ
- คำตอบที่สร้างขึ้นกลายเป็น "ความจริงพื้นฐาน" สำหรับการฝึกอบรมแบบจำลองนักเรียน
- กระบวนการนี้จับรูปแบบการตัดสินใจของครู
- คุณภาพของข้อมูลสังเคราะห์ส่งผลโดยตรงต่อประสิทธิภาพของแบบจำลองนักเรียน

### ขั้นตอนที่ 2: การปรับแต่งแบบจำลองนักเรียน

แบบจำลองนักเรียนได้รับการฝึกอบรมบนชุดข้อมูลสังเคราะห์ เรียนรู้ที่จะเลียนแบบพฤติกรรมและคำตอบของครู

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**วัตถุประสงค์ของการฝึกอบรม:**
- ลดความแตกต่างระหว่างผลลัพธ์ของนักเรียนและครู
- รักษาความรู้ของครูในพื้นที่พารามิเตอร์ที่เล็กลง
- รักษาประสิทธิภาพในขณะที่ลดความซับซ้อนของแบบจำลอง

## การนำไปใช้ในทางปฏิบัติ {#practical-implementation}

### การเลือกแบบจำลองครูและนักเรียน

**การเลือกแบบจำลองครู:**
- เลือก LLM ขนาดใหญ่ (100B+ พารามิเตอร์) ที่มีประสิทธิภาพพิสูจน์แล้วในงานเฉพาะของคุณ
- แบบจำลองครูยอดนิยม ได้แก่:
  - **DeepSeek V3** (671B พารามิเตอร์) - ยอดเยี่ยมสำหรับการให้เหตุผลและการสร้างโค้ด
  - **Meta Llama 3.1 405B Instruct** - ความสามารถทั่วไปที่ครอบคลุม
  - **GPT-4** - ประสิทธิภาพที่แข็งแกร่งในงานหลากหลาย
  - **Claude 3.5 Sonnet** - ยอดเยี่ยมสำหรับงานที่ต้องการการให้เหตุผลที่ซับซ้อน
- ตรวจสอบให้แน่ใจว่าแบบจำลองครูมีประสิทธิภาพดีในข้อมูลเฉพาะโดเมนของคุณ

**การเลือกแบบจำลองนักเรียน:**
- สมดุลระหว่างขนาดแบบจำลองและความต้องการด้านประสิทธิภาพ
- มุ่งเน้นไปที่แบบจำลองที่มีประสิทธิภาพและเล็กกว่า เช่น:
  - **Microsoft Phi-4-mini** - แบบจำลองที่มีประสิทธิภาพล่าสุดพร้อมความสามารถในการให้เหตุผลที่แข็งแกร่ง
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (รุ่น 4K และ 128K)
  - Microsoft Phi-3.5 Mini Instruct

### ขั้นตอนการนำไปใช้

1. **การเตรียมข้อมูล**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **การตั้งค่าแบบจำลองครู**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **การสร้างข้อมูลสังเคราะห์**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **การฝึกอบรมแบบจำลองนักเรียน**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## ตัวอย่างการกลั่นใน Azure ML {#azure-ml-example}

Azure Machine Learning มีแพลตฟอร์มที่ครอบคลุมสำหรับการนำการกลั่นแบบจำลองไปใช้ นี่คือวิธีการใช้ Azure ML ในเวิร์กโฟลว์การกลั่นของคุณ:

### ข้อกำหนดเบื้องต้น

1. **พื้นที่ทำงาน Azure ML**: ตั้งค่าพื้นที่ทำงานของคุณในภูมิภาคที่เหมาะสม
   - ตรวจสอบให้แน่ใจว่ามีการเข้าถึงแบบจำลองครูขนาดใหญ่ (DeepSeek V3, Llama 405B)
   - กำหนดค่าภูมิภาคตามความพร้อมใช้งานของแบบจำลอง

2. **ทรัพยากรการประมวลผล**: กำหนดค่าตัวประมวลผลที่เหมาะสมสำหรับการฝึกอบรม
   - อินสแตนซ์ที่มีหน่วยความจำสูงสำหรับการคาดการณ์แบบจำลองครู
   - ตัวประมวลผลที่รองรับ GPU สำหรับการปรับแต่งแบบจำลองนักเรียน

### ประเภทงานที่รองรับ

Azure ML รองรับการกลั่นสำหรับงานต่าง ๆ เช่น:
- **การตีความภาษาธรรมชาติ (NLI)**
- **AI สำหรับการสนทนา**
- **การถามตอบ (QA)**
- **การให้เหตุผลทางคณิตศาสตร์**
- **การสรุปข้อความ**

### ตัวอย่างการนำไปใช้

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### การติดตามและการประเมินผล

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## แนวทางปฏิบัติที่ดีที่สุดและการปรับแต่ง {#best-practices}

### คุณภาพของข้อมูล

**ข้อมูลการฝึกอบรมคุณภาพสูงมีความสำคัญ:**
- ตรวจสอบให้แน่ใจว่ามีตัวอย่างการฝึกอบรมที่หลากหลายและเป็นตัวแทน
- ใช้ข้อมูลเฉพาะโดเมนเมื่อเป็นไปได้
- ตรวจสอบผลลัพธ์ของแบบจำลองครูก่อนใช้สำหรับการฝึกอบรมแบบจำลองนักเรียน
- ปรับสมดุลชุดข้อมูลเพื่อหลีกเลี่ยงอคติในการเรียนรู้ของแบบจำลองนักเรียน

### การปรับแต่งไฮเปอร์พารามิเตอร์

**พารามิเตอร์สำคัญที่ต้องปรับแต่ง:**
- **อัตราการเรียนรู้**: เริ่มต้นด้วยอัตราที่เล็กกว่า (1e-5 ถึง 5e-5) สำหรับการปรับแต่ง
- **ขนาดแบตช์**: สมดุลระหว่างข้อจำกัดด้านหน่วยความจำและความเสถียรของการฝึกอบรม
- **จำนวนรอบการฝึกอบรม**: ตรวจสอบการเกิด overfitting; โดยทั่วไป 2-5 รอบก็เพียงพอ
- **การปรับอุณหภูมิ**: ปรับความนุ่มนวลของผลลัพธ์ครูเพื่อการถ่ายโอนความรู้ที่ดีขึ้น

### การพิจารณาสถาปัตยกรรมแบบจำลอง

**ความเข้ากันได้ระหว่างครูและนักเรียน:**
- ตรวจสอบให้แน่ใจว่าสถาปัตยกรรมของแบบจำลองครูและนักเรียนเข้ากันได้
- พิจารณาการจับคู่เลเยอร์ระดับกลางเพื่อการถ่ายโอนความรู้ที่ดีขึ้น
- ใช้เทคนิคการถ่ายโอนความสนใจเมื่อเหมาะสม

### กลยุทธ์การประเมินผล

**แนวทางการประเมินผลที่ครอบคลุม:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## การใช้งานในโลกจริง {#real-world-applications}

### การใช้งานบนมือถือและปลายทาง

แบบจำลองที่กลั่นช่วยให้มีความสามารถ AI บนอุปกรณ์ที่มีทรัพยากรจำกัด:
- **แอปพลิเคชันสมาร์ทโฟน** ที่ประมวลผลข้อความแบบเรียลไทม์
- **อุปกรณ์ IoT** ที่ทำการคาดการณ์ในพื้นที่
- **ระบบฝังตัว** ที่มีทรัพยากรการประมวลผลจำกัด

### ระบบการผลิตที่คุ้มค่า

องค์กรใช้การกลั่นเพื่อลดต้นทุนการดำเนินงาน:
- **แชทบอทบริการลูกค้า** ที่ตอบสนองเร็วขึ้น
- **ระบบตรวจสอบเนื้อหา** ที่ประมวลผลปริมาณมากได้อย่างมีประสิทธิภาพ
- **บริการแปลภาษาแบบเรียลไทม์** ที่มีความล่าช้าต่ำ

### การใช้งานเฉพาะโดเมน

การกลั่นช่วยสร้างแบบจำลองเฉพาะทาง:
- **การช่วยวินิจฉัยทางการแพทย์** ที่รักษาความเป็นส่วนตัวด้วยการคาดการณ์ในพื้นที่
- **การวิเคราะห์เอกสารทางกฎหมาย** ที่ปรับให้เหมาะสมกับโดเมนทางกฎหมายเฉพาะ
- **การประเมินความเสี่ยงทางการเงิน** ที่มีความสามารถในการตัดสินใจอย่างรวดเร็ว

### กรณีศึกษา: การสนับสนุนลูกค้าด้วย DeepSeek V3 → Phi-4-mini

บริษัทเทคโนโลยีแห่งหนึ่งนำการกลั่นไปใช้ในระบบสนับสนุนลูกค้าของพวกเขา:

**รายละเอียดการนำไปใช้:**
- **แบบจำลองครู**: DeepSeek V3 (671B พารามิเตอร์) - ยอดเยี่ยมสำหรับการให้เหตุผลในคำถามลูกค้าที่ซับซ้อน
- **แบบจำลองนักเรียน**: Phi-4-mini - ปรับให้เหมาะสมสำหรับการคาดการณ์ที่รวดเร็วและการใช้งาน
- **ข้อมูลการฝึกอบรม**: บทสนทนาสนับสนุนลูกค้า 50,000 รายการ
- **งาน**: การสนับสนุนการสนทนาแบบหลายรอบพร้อมการแก้ปัญหาทางเทคนิค

**ผลลัพธ์ที่ได้:**
- **ลดเวลาในการคาดการณ์ลง 85%** (จาก 3.2 วินาทีเป็น 0.48 วินาทีต่อคำตอบ)
- **ลดความต้องการหน่วยความจำลง 95%** (จาก 1.2TB เป็น 60GB)
- **รักษาความแม่นยำของแบบจำลองเดิมไว้ 92%** ในงานสนับสนุน
- **ลดต้นทุนการดำเนินงานลง 60%**
- **เพิ่มความสามารถในการขยายตัว** - สามารถรองรับผู้ใช้พร้อมกันได้มากขึ้น 10 เท่า

**การวิเคราะห์ประสิทธิภาพ:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## บทสรุป {#conclusion}

การกลั่นแบบจำลองเป็นเทคนิคสำคัญที่ช่วยให้การเข้าถึงความสามารถ AI ขั้นสูงเป็นไปได้ โดยการสร้างแบบจำลองที่เล็กลงและมีประสิทธิภาพมากขึ้นที่ยังคงรักษาประสิทธิภาพของแบบจำลองขนาดใหญ่ การกลั่นตอบสนองความต้องการที่เพิ่มขึ้นสำหรับการใช้งาน AI ในทางปฏิบัติ

### ประเด็นสำคัญ

1. **การกลั่นช่วยเชื่อมช่องว่าง** ระหว่างประสิทธิภาพของแบบจำลองและข้อจำกัดในทางปฏิบัติ
2. **กระบวนการสองขั้นตอน** ช่วยให้การถ่ายโอนความรู้จากครูไปยังนักเรียนมีประสิทธิภาพ
3. **Azure ML มีโครงสร้างพื้นฐานที่แข็งแกร่ง** สำหรับการนำเวิร์กโฟลว์การกลั่นไปใช้
4. **การประเมินผลและการปรับแต่งที่เหมาะสม** เป็นสิ่งสำคัญสำหรับการกลั่นที่ประสบความสำเร็จ
5. **การใช้งานในโลกจริง** แสดงให้เห็นถึงประโยชน์ที่สำคัญในด้านต้นทุน ความเร็ว และการเข้าถึง

### ทิศทางในอนาคต

เมื่อสาขานี้พัฒนาไปเรื่อย ๆ เราสามารถคาดหวังได้ว่า:
- **เทคนิคการกลั่นขั้นสูง** ที่มีวิธีการถ่ายโอนความรู้ที่ดียิ่งขึ้น
- **การกลั่นแบบหลายครู** เพื่อเพิ่มความสามารถของแบบจำลองนักเรียน
- **การปรับแต่งกระบวนการกลั่นโดยอัตโนมัติ**
- **การสนับสนุนแบบจำลองที่กว้างขึ้น** ในสถาปัตยกรรมและโดเมนต่าง ๆ

การกลั่นแบบจำลองช่วยให้องค์กรสามารถใช้ความสามารถ AI ขั้นสูงได้ ในขณะที่ยังคงรักษาข้อจำกัดในการใช้งานจริง ทำให้แบบจำลองภาษาขั้นสูงสามารถเข้าถึงได้ในหลากหลายแอปพลิเคชันและสภาพแวดล้อม

## ➡️ สิ่งที่ต้องทำต่อไป

- [03: การปรับแต่ง - การปรับแบบจำลองให้เหมาะสมกับงานเฉพาะ](./03.SLMOps-Finetuing.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้