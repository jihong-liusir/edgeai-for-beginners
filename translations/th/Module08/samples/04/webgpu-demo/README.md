<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7a474b8e201d5316c0095cdbc3bf0555",
  "translation_date": "2025-09-24T22:55:22+00:00",
  "source_file": "Module08/samples/04/webgpu-demo/README.md",
  "language_code": "th"
}
-->
# WebGPU + ONNX Runtime Demo

เดโมนี้แสดงวิธีการรันโมเดล AI โดยตรงในเบราว์เซอร์โดยใช้ WebGPU เพื่อเร่งความเร็วด้วยฮาร์ดแวร์และ ONNX Runtime Web

## สิ่งที่เดโมนี้แสดงให้เห็น

- **AI ในเบราว์เซอร์**: รันโมเดลทั้งหมดในเบราว์เซอร์
- **การเร่งความเร็วด้วย WebGPU**: ใช้ฮาร์ดแวร์ช่วยเร่งการประมวลผลเมื่อมีให้ใช้งาน
- **ความเป็นส่วนตัวสูงสุด**: ไม่มีข้อมูลออกจากอุปกรณ์ของคุณ
- **ไม่ต้องติดตั้ง**: ใช้งานได้ในเบราว์เซอร์ที่รองรับ
- **การทำงานสำรอง**: เปลี่ยนไปใช้ CPU หากไม่มี WebGPU

## ข้อกำหนด

**ความเข้ากันได้ของเบราว์เซอร์:**
- Chrome/Edge 113+ พร้อมเปิดใช้งาน WebGPU
- ตรวจสอบสถานะ WebGPU: `chrome://gpu`
- เปิดใช้งาน WebGPU: `chrome://flags/#enable-unsafe-webgpu`

## การรันเดโม

### วิธีที่ 1: เซิร์ฟเวอร์ในเครื่อง (แนะนำ)

```cmd
# Navigate to the demo directory
cd Module08\samples\04\webgpu-demo

# Start a local server
python -m http.server 5173

# Open browser to http://localhost:5173
```

### วิธีที่ 2: VS Code Live Server

1. ติดตั้งส่วนขยาย "Live Server" ใน VS Code
2. คลิกขวาที่ `index.html` → "Open with Live Server"
3. เดโมจะเปิดในเบราว์เซอร์โดยอัตโนมัติ

## สิ่งที่คุณจะเห็น

1. **การตรวจสอบ WebGPU**: ตรวจสอบความเข้ากันได้ของเบราว์เซอร์
2. **การโหลดโมเดล**: ดาวน์โหลดและเริ่มต้น MNIST classifier
3. **การรันการทำนาย**: รันการทำนายบนข้อมูลตัวอย่าง
4. **การวัดประสิทธิภาพ**: แสดงเวลาโหลดและความเร็วในการทำนาย
5. **การแสดงผลลัพธ์**: ความมั่นใจในการทำนายและผลลัพธ์ดิบ

## ประสิทธิภาพที่คาดหวัง

| Execution Provider | การโหลดโมเดล | การทำนาย | หมายเหตุ |
|-------------------|------------|-----------|-------|
| **WebGPU** | ~2-5s | ~10-50ms | เร่งความเร็วด้วยฮาร์ดแวร์ |
| **CPU (WASM)** | ~2-5s | ~50-200ms | ใช้ซอฟต์แวร์สำรอง |

## การแก้ไขปัญหา

**WebGPU ไม่พร้อมใช้งาน:**
- อัปเดตเป็น Chrome/Edge 113+
- เปิดใช้งาน WebGPU ใน `chrome://flags`
- ตรวจสอบให้แน่ใจว่าไดรเวอร์ GPU เป็นเวอร์ชันล่าสุด
- เดโมจะเปลี่ยนไปใช้ CPU โดยอัตโนมัติ

**ข้อผิดพลาดในการโหลด:**
- ตรวจสอบให้แน่ใจว่าคุณกำลังให้บริการผ่าน HTTP (ไม่ใช่ file://)
- ตรวจสอบการเชื่อมต่อเครือข่ายสำหรับการดาวน์โหลดโมเดล
- ตรวจสอบว่า CORS ไม่ได้บล็อกโมเดล ONNX

**ปัญหาด้านประสิทธิภาพ:**
- WebGPU ให้ความเร็วที่เพิ่มขึ้นอย่างมากเมื่อเทียบกับ CPU
- การรันครั้งแรกอาจช้ากว่าเนื่องจากการดาวน์โหลดโมเดล
- การรันครั้งต่อไปจะใช้แคชของเบราว์เซอร์

## การผสานรวมกับ Foundry Local

เดโม WebGPU นี้ช่วยเสริม Foundry Local โดยแสดง:

- **การทำนายฝั่งไคลเอนต์** เพื่อความเป็นส่วนตัวสูงสุด
- **ความสามารถในการทำงานแบบออฟไลน์** เมื่อไม่มีอินเทอร์เน็ต  
- **การใช้งานที่ขอบเครือข่าย** สำหรับสภาพแวดล้อมที่มีทรัพยากรจำกัด
- **สถาปัตยกรรมแบบไฮบริด** ที่รวมการทำนายฝั่งไคลเอนต์และเซิร์ฟเวอร์

สำหรับการใช้งานในระดับโปรดักชัน ควรพิจารณา:
- ใช้ Foundry Local สำหรับการทำนายฝั่งเซิร์ฟเวอร์
- ใช้ WebGPU สำหรับการประมวลผลก่อน/หลังฝั่งไคลเอนต์
- ใช้การกำหนดเส้นทางอัจฉริยะระหว่างการทำนายแบบโลคอล/รีโมต

## รายละเอียดทางเทคนิค

**โมเดลที่ใช้:**
- MNIST digit classifier (รูปแบบ ONNX)
- อินพุต: ภาพขาวดำขนาด 28x28
- เอาต์พุต: การแจกแจงความน่าจะเป็น 10 คลาส
- ขนาด: ~500KB (ดาวน์โหลดรวดเร็ว)

**ONNX Runtime Web:**
- ผู้ให้บริการการรัน WebGPU สำหรับการเร่งความเร็วด้วย GPU
- ผู้ให้บริการการรัน WASM สำหรับการสำรองด้วย CPU
- การปรับแต่งอัตโนมัติและการปรับกราฟให้เหมาะสม

**API ของเบราว์เซอร์:**
- WebGPU สำหรับการเข้าถึงฮาร์ดแวร์
- Web Workers สำหรับการประมวลผลเบื้องหลัง (การปรับปรุงในอนาคต)
- WebAssembly สำหรับการคำนวณที่มีประสิทธิภาพ

## ขั้นตอนถัดไป

- ทดลองใช้โมเดล ONNX แบบกำหนดเอง
- เพิ่มการอัปโหลดภาพจริงและการจำแนกประเภท
- เพิ่มการทำนายแบบสตรีมสำหรับโมเดลที่ใหญ่ขึ้น
- ผสานรวมกับอินพุตจากกล้อง/ไมโครโฟน

---

