<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:29:43+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "th"
}
-->
# เซสชัน 3: การค้นหาและจัดการโมเดลโอเพ่นซอร์ส

## ภาพรวม

เซสชันนี้เน้นการค้นหาและจัดการโมเดลในเชิงปฏิบัติด้วย Foundry Local คุณจะได้เรียนรู้วิธีแสดงรายการโมเดลที่มีอยู่ ทดสอบตัวเลือกต่าง ๆ และทำความเข้าใจลักษณะการทำงานพื้นฐาน โดยเน้นการสำรวจด้วยมือผ่าน foundry CLI เพื่อช่วยคุณเลือกโมเดลที่เหมาะสมกับการใช้งานของคุณ

## วัตถุประสงค์การเรียนรู้

- เชี่ยวชาญคำสั่ง foundry CLI สำหรับการค้นหาและจัดการโมเดล
- เข้าใจรูปแบบการจัดเก็บโมเดลในแคชและพื้นที่จัดเก็บในเครื่อง
- เรียนรู้วิธีทดสอบและเปรียบเทียบโมเดลต่าง ๆ อย่างรวดเร็ว
- สร้างเวิร์กโฟลว์ที่ใช้งานได้จริงสำหรับการเลือกและวัดประสิทธิภาพโมเดล
- สำรวจระบบนิเวศของโมเดลที่เติบโตขึ้นเรื่อย ๆ ผ่าน Foundry Local

## สิ่งที่ต้องเตรียม

- ผ่านเซสชัน 1: เริ่มต้นใช้งาน Foundry Local
- ติดตั้งและเข้าถึง Foundry Local CLI
- มีพื้นที่จัดเก็บเพียงพอสำหรับดาวน์โหลดโมเดล (ขนาดโมเดลอาจอยู่ระหว่าง 1GB ถึง 20GB+)
- มีความเข้าใจพื้นฐานเกี่ยวกับประเภทโมเดลและการใช้งาน

## ภาพรวม

เซสชันนี้สำรวจวิธีการนำโมเดลโอเพ่นซอร์สมาสู่ Foundry Local

## ส่วนที่ 6: การฝึกปฏิบัติ

### การฝึก: การค้นหาและเปรียบเทียบโมเดล

สร้างสคริปต์ประเมินโมเดลของคุณเองโดยอ้างอิงจากตัวอย่าง 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### งานของคุณ

1. **รันสคริปต์ตัวอย่าง 03**: `samples\03\list_and_bench.cmd`
2. **ลองใช้โมเดลต่าง ๆ**: ทดสอบโมเดลอย่างน้อย 3 ตัว
3. **เปรียบเทียบประสิทธิภาพ**: สังเกตความแตกต่างในความเร็วและคุณภาพการตอบสนอง
4. **บันทึกผลการค้นพบ**: สร้างแผนภูมิเปรียบเทียบแบบง่าย

### รูปแบบการเปรียบเทียบตัวอย่าง

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## ส่วนที่ 7: การแก้ไขปัญหาและแนวทางปฏิบัติที่ดีที่สุด

### ปัญหาทั่วไปและวิธีแก้ไข

**โมเดลไม่เริ่มทำงาน:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**หน่วยความจำไม่เพียงพอ:**
- เริ่มต้นด้วยโมเดลขนาดเล็ก (`phi-4-mini`)
- ปิดแอปพลิเคชันอื่น ๆ
- อัปเกรด RAM หากพบปัญหาบ่อยครั้ง

**ประสิทธิภาพช้า:**
- ตรวจสอบว่าโมเดลโหลดเสร็จสมบูรณ์ (ดูผลลัพธ์แบบ verbose)
- ปิดแอปพลิเคชันพื้นหลังที่ไม่จำเป็น
- พิจารณาใช้พื้นที่จัดเก็บที่เร็วขึ้น (SSD)

### แนวทางปฏิบัติที่ดีที่สุด

1. **เริ่มต้นเล็ก ๆ**: เริ่มด้วย `phi-4-mini` เพื่อยืนยันการตั้งค่า
2. **ทีละโมเดล**: หยุดโมเดลก่อนหน้า ก่อนเริ่มโมเดลใหม่
3. **ตรวจสอบทรัพยากร**: เฝ้าดูการใช้งานหน่วยความจำ
4. **ทดสอบอย่างสม่ำเสมอ**: ใช้คำถามเดียวกันเพื่อการเปรียบเทียบที่ยุติธรรม
5. **บันทึกผลลัพธ์**: จดบันทึกประสิทธิภาพโมเดลสำหรับการใช้งานของคุณ

## ส่วนที่ 8: ขั้นตอนถัดไปและแหล่งข้อมูลอ้างอิง

### เตรียมตัวสำหรับเซสชัน 4

- **หัวข้อเซสชัน 4**: เครื่องมือและเทคนิคการปรับแต่ง
- **สิ่งที่ต้องเตรียม**: ความคุ้นเคยกับการสลับโมเดลและการทดสอบประสิทธิภาพพื้นฐาน
- **แนะนำ**: มีโมเดลที่ชื่นชอบ 2-3 ตัวจากเซสชันนี้

### แหล่งข้อมูลเพิ่มเติม

- **[เอกสาร Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: เอกสารอย่างเป็นทางการ
- **[CLI Reference](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: อ้างอิงคำสั่งทั้งหมด
- **[Model Mondays](https://aka.ms/model-mondays)**: การแนะนำโมเดลรายสัปดาห์
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: ชุมชนและปัญหา
- **[ตัวอย่าง 03: การค้นหาโมเดล](samples/03/README.md)**: สคริปต์ตัวอย่างสำหรับการฝึกปฏิบัติ

### ประเด็นสำคัญที่ควรทราบ

✅ **การค้นหาโมเดล**: ใช้ `foundry model list` เพื่อสำรวจโมเดลที่มีอยู่  
✅ **การทดสอบอย่างรวดเร็ว**: รูปแบบ `list_and_bench.cmd` สำหรับการประเมินอย่างรวดเร็ว  
✅ **การตรวจสอบประสิทธิภาพ**: การใช้งานทรัพยากรพื้นฐานและการวัดเวลาตอบสนอง  
✅ **การเลือกโมเดล**: แนวทางปฏิบัติสำหรับการเลือกโมเดลตามการใช้งาน  
✅ **การจัดการแคช**: เข้าใจการจัดเก็บและขั้นตอนการล้างข้อมูล  

คุณมีทักษะในเชิงปฏิบัติในการค้นหา ทดสอบ และเลือกโมเดลที่เหมาะสมสำหรับแอปพลิเคชัน AI ของคุณโดยใช้ CLI ที่เรียบง่ายของ Foundry Local: การเลือกโมเดลจากชุมชน การรวมเนื้อหา Hugging Face และการนำกลยุทธ์ “นำโมเดลของคุณเอง” (BYOM) มาใช้ คุณยังจะได้ค้นพบซีรีส์ Model Mondays เพื่อการเรียนรู้และการค้นหาโมเดลอย่างต่อเนื่อง

แหล่งข้อมูลอ้างอิง:
- เอกสาร Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- การคอมไพล์โมเดล Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## วัตถุประสงค์การเรียนรู้
- ค้นหาและประเมินโมเดลโอเพ่นซอร์สสำหรับการใช้งานในเครื่อง
- คอมไพล์และรันโมเดล Hugging Face ที่เลือกใน Foundry Local
- ใช้กลยุทธ์การเลือกโมเดลสำหรับความแม่นยำ ความหน่วง และความต้องการทรัพยากร
- จัดการโมเดลในเครื่องด้วยแคชและการจัดการเวอร์ชัน

## ส่วนที่ 1: การค้นหาโมเดลด้วย Foundry CLI

### คำสั่งการจัดการโมเดลพื้นฐาน

Foundry CLI มีคำสั่งที่เรียบง่ายสำหรับการค้นหาและจัดการโมเดล:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### การรันโมเดลแรกของคุณ

เริ่มต้นด้วยโมเดลยอดนิยมที่ผ่านการทดสอบมาอย่างดีเพื่อทำความเข้าใจลักษณะการทำงาน:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**หมายเหตุ:** ตัวเลือก `--verbose` ให้ข้อมูลการเริ่มต้นโดยละเอียด รวมถึง:
- ความคืบหน้าการดาวน์โหลดโมเดล (ในครั้งแรก)
- รายละเอียดการจัดสรรหน่วยความจำ
- ข้อมูลการเชื่อมโยงบริการ
- เมตริกการเริ่มต้นประสิทธิภาพ

### การทำความเข้าใจประเภทโมเดล

**Small Language Models (SLMs):**
- `phi-4-mini`: เร็ว มีประสิทธิภาพ เหมาะสำหรับการสนทนาทั่วไป
- `phi-4`: เวอร์ชันที่มีความสามารถมากขึ้น พร้อมเหตุผลที่ดีกว่า

**โมเดลขนาดกลาง:**
- `qwen2.5-7b`: มีเหตุผลที่ยอดเยี่ยมและบริบทที่ยาวขึ้น
- `deepseek-r1-7b`: ปรับแต่งสำหรับการสร้างโค้ด

**โมเดลขนาดใหญ่:**
- `llama-3.2`: โมเดลโอเพ่นซอร์สล่าสุดของ Meta
- `qwen2.5-14b`: เหตุผลระดับองค์กร

## ส่วนที่ 2: การทดสอบและเปรียบเทียบโมเดลอย่างรวดเร็ว

### รูปแบบตัวอย่าง 03: การแสดงรายการและการวัดผลอย่างง่าย

อ้างอิงจากรูปแบบตัวอย่าง 03 นี่คือเวิร์กโฟลว์ขั้นต่ำ:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### การทดสอบประสิทธิภาพโมเดล

เมื่อโมเดลทำงานแล้ว ให้ทดสอบด้วยคำถามที่สม่ำเสมอ:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### ทางเลือกการทดสอบด้วย PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## ส่วนที่ 3: การจัดการแคชและพื้นที่จัดเก็บโมเดล

### การทำความเข้าใจแคชโมเดล

Foundry Local จัดการการดาวน์โหลดและแคชโมเดลโดยอัตโนมัติ:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### ข้อควรพิจารณาเกี่ยวกับพื้นที่จัดเก็บโมเดล

**ขนาดโมเดลทั่วไป:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**แนวทางปฏิบัติที่ดีที่สุดสำหรับพื้นที่จัดเก็บ:**
- เก็บโมเดล 2-3 ตัวไว้ในแคชเพื่อการสลับที่รวดเร็ว
- ลบโมเดลที่ไม่ได้ใช้งานเพื่อเพิ่มพื้นที่ว่าง: `foundry cache clean`
- เฝ้าดูการใช้งานดิสก์ โดยเฉพาะบน SSD ขนาดเล็ก
- พิจารณาการแลกเปลี่ยนระหว่างขนาดโมเดลและความสามารถ

### การตรวจสอบประสิทธิภาพโมเดล

ขณะโมเดลทำงาน ให้ตรวจสอบทรัพยากรระบบ:

**Windows Task Manager:**
- เฝ้าดูการใช้งานหน่วยความจำ (โมเดลจะอยู่ใน RAM)
- ตรวจสอบการใช้งาน CPU ระหว่างการประมวลผล
- ตรวจสอบ I/O ดิสก์ระหว่างการโหลดโมเดลครั้งแรก

**การตรวจสอบผ่าน Command Line:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## ส่วนที่ 4: แนวทางปฏิบัติในการเลือกโมเดล

### การเลือกโมเดลตามการใช้งาน

**สำหรับการสนทนาและถามตอบทั่วไป:**
- เริ่มต้นด้วย: `phi-4-mini` (เร็ว มีประสิทธิภาพ)
- อัปเกรดเป็น: `phi-4` (เหตุผลที่ดีกว่า)
- ขั้นสูง: `qwen2.5-7b` (บริบทที่ยาวขึ้น)

**สำหรับการสร้างโค้ด:**
- แนะนำ: `deepseek-r1-7b`
- ทางเลือก: `qwen2.5-7b` (ดีสำหรับโค้ดเช่นกัน)

**สำหรับเหตุผลที่ซับซ้อน:**
- ดีที่สุด: `qwen2.5-7b` หรือ `qwen2.5-14b`
- ตัวเลือกประหยัด: `phi-4`

### คู่มือข้อกำหนดฮาร์ดแวร์

**ข้อกำหนดระบบขั้นต่ำ:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**แนะนำสำหรับประสิทธิภาพที่ดีที่สุด:**
- RAM 32GB+ สำหรับการสลับโมเดลหลายตัวอย่างสะดวก
- พื้นที่จัดเก็บ SSD สำหรับการโหลดโมเดลที่เร็วขึ้น
- CPU รุ่นใหม่ที่มีประสิทธิภาพการประมวลผลแบบ single-thread ดี
- รองรับ NPU (Windows 11 Copilot+ PCs) สำหรับการเร่งความเร็ว

### เวิร์กโฟลว์การสลับโมเดล

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## ส่วนที่ 5: การวัดผลโมเดลอย่างง่าย

### การทดสอบประสิทธิภาพพื้นฐาน

นี่คือวิธีการเปรียบเทียบประสิทธิภาพโมเดลอย่างตรงไปตรงมา:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### การประเมินคุณภาพด้วยตนเอง

สำหรับแต่ละโมเดล ทดสอบด้วยคำถามที่สม่ำเสมอและประเมินด้วยตนเอง:

**คำถามทดสอบ:**
1. "อธิบายการคำนวณควอนตัมในแบบง่าย ๆ"
2. "เขียนฟังก์ชัน Python เพื่อเรียงลำดับรายการ"
3. "ข้อดีและข้อเสียของการทำงานระยะไกลคืออะไร?"
4. "สรุปประโยชน์ของ AI ที่ขอบเครือข่าย"

**เกณฑ์การประเมิน:**
- **ความถูกต้อง**: ข้อมูลถูกต้องหรือไม่?
- **ความชัดเจน**: คำอธิบายเข้าใจง่ายหรือไม่?
- **ความครบถ้วน**: ตอบคำถามครบถ้วนหรือไม่?
- **ความเร็ว**: ตอบสนองเร็วแค่ไหน?

### การตรวจสอบการใช้งานทรัพยากร

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## ส่วนที่ 6: ขั้นตอนถัดไป
- สมัครรับข้อมูล Model Mondays สำหรับโมเดลใหม่และเคล็ดลับ: https://aka.ms/model-mondays
- มีส่วนร่วมใน `models.json` ของทีมคุณ
- เตรียมตัวสำหรับเซสชัน 4: เปรียบเทียบ LLMs กับ SLMs, การใช้งานในเครื่องกับคลาวด์, และการสาธิตแบบลงมือทำ

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์ที่มีความเชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้