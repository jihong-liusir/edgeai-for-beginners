<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T19:18:56+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "th"
}
-->
# Session 3: โมเดลโอเพ่นซอร์สกับ Foundry Local

## ภาพรวม

เซสชันนี้จะสำรวจวิธีการนำโมเดลโอเพ่นซอร์สมาสู่ Foundry Local: การเลือกโมเดลจากชุมชน, การผสานเนื้อหาจาก Hugging Face, และการใช้กลยุทธ์ “นำโมเดลของคุณเอง” (BYOM) นอกจากนี้คุณยังจะได้เรียนรู้เกี่ยวกับซีรีส์ Model Mondays เพื่อการเรียนรู้และค้นพบโมเดลอย่างต่อเนื่อง

อ้างอิง:
- เอกสาร Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- การคอมไพล์โมเดล Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## วัตถุประสงค์การเรียนรู้
- ค้นหาและประเมินโมเดลโอเพ่นซอร์สสำหรับการประมวลผลในเครื่อง
- คอมไพล์และรันโมเดล Hugging Face ที่เลือกใน Foundry Local
- ใช้กลยุทธ์การเลือกโมเดลโดยคำนึงถึงความแม่นยำ, ความเร็ว, และทรัพยากรที่ต้องการ
- จัดการโมเดลในเครื่องด้วยแคชและการจัดการเวอร์ชัน

## ส่วนที่ 1: การค้นหาและเลือกโมเดล (ทีละขั้นตอน)

ขั้นตอนที่ 1) แสดงรายการโมเดลที่มีอยู่ในแคตตาล็อกในเครื่อง  
```cmd
foundry model list
```
  
ขั้นตอนที่ 2) ทดลองใช้งานโมเดลสองตัวอย่างรวดเร็ว (ดาวน์โหลดอัตโนมัติเมื่อรันครั้งแรก)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
ขั้นตอนที่ 3) สังเกตเมตริกพื้นฐาน  
- สังเกตความเร็ว (โดยประมาณ) และคุณภาพสำหรับข้อความที่กำหนด  
- ตรวจสอบการใช้หน่วยความจำผ่าน Task Manager ขณะรันแต่ละโมเดล  

## ส่วนที่ 2: การรันโมเดลในแคตตาล็อกผ่าน CLI (ทีละขั้นตอน)

ขั้นตอนที่ 1) เริ่มโมเดล  
```cmd
foundry model run llama-3.2
```
  
ขั้นตอนที่ 2) ส่งข้อความทดสอบผ่าน endpoint ที่เข้ากันได้กับ OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## ส่วนที่ 3: BYOM – การคอมไพล์โมเดล Hugging Face (ทีละขั้นตอน)

ทำตามคำแนะนำอย่างเป็นทางการสำหรับการคอมไพล์โมเดล ด้านล่างเป็นภาพรวมระดับสูง—ดูบทความ Microsoft Learn สำหรับคำสั่งและการตั้งค่าที่รองรับ

ขั้นตอนที่ 1) เตรียมไดเรกทอรีทำงาน  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
ขั้นตอนที่ 2) คอมไพล์โมเดล HF ที่รองรับ  
- ใช้ขั้นตอนจากเอกสาร Learn เพื่อแปลงและวางโมเดล ONNX ที่คอมไพล์แล้วในไดเรกทอรี `models`  
- ยืนยันด้วย:  
```cmd
foundry cache ls
```
  
คุณควรเห็นชื่อโมเดลที่คอมไพล์แล้ว (เช่น `llama-3.2`)  

ขั้นตอนที่ 3) รันโมเดลที่คอมไพล์แล้ว  
```cmd
foundry model run llama-3.2 --verbose
```
  
หมายเหตุ:  
- ตรวจสอบให้แน่ใจว่ามีพื้นที่ดิสก์และ RAM เพียงพอสำหรับการคอมไพล์และรัน  
- เริ่มต้นด้วยโมเดลขนาดเล็กเพื่อยืนยันกระบวนการ จากนั้นจึงขยายขนาด  

## ส่วนที่ 4: การคัดเลือกโมเดลเชิงปฏิบัติ (ทีละขั้นตอน)

ขั้นตอนที่ 1) สร้างรีจิสทรี `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
ขั้นตอนที่ 2) สคริปต์ตัวเลือกขนาดเล็ก  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## ส่วนที่ 5: การทดสอบประสิทธิภาพ (ทีละขั้นตอน)

ขั้นตอนที่ 1) การทดสอบความเร็วอย่างง่าย  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
ขั้นตอนที่ 2) การตรวจสอบคุณภาพ  
- ใช้ชุดข้อความที่กำหนด จับผลลัพธ์ลงใน CSV/JSON  
- ให้คะแนนความลื่นไหล, ความเกี่ยวข้อง, และความถูกต้อง (1–5)  

## ส่วนที่ 6: ขั้นตอนถัดไป
- สมัครรับข้อมูล Model Mondays เพื่อรับโมเดลและเคล็ดลับใหม่ๆ: https://aka.ms/model-mondays  
- ส่งผลการค้นพบไปยัง `models.json` ของทีมคุณ  
- เตรียมตัวสำหรับเซสชันที่ 4: การเปรียบเทียบ LLMs กับ SLMs, การประมวลผลในเครื่องเทียบกับคลาวด์, และการสาธิตแบบลงมือทำ  

---

