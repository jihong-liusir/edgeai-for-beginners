<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "382a763fcea7087e68a94c26216e5e70",
  "translation_date": "2025-09-22T19:17:40+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "th"
}
-->
# Session 5: สร้างตัวแทนที่ขับเคลื่อนด้วย AI อย่างรวดเร็วด้วย Foundry Local

หมายเหตุ: ความสามารถของตัวแทนใน Foundry Local มีการพัฒนาอยู่เสมอ—โปรดตรวจสอบการสนับสนุนในหมายเหตุการเปิดตัวล่าสุดก่อนที่จะนำรูปแบบขั้นสูงไปใช้งาน

## ภาพรวม

ใช้ Foundry Local เพื่อสร้างต้นแบบแอปพลิเคชันที่มีลักษณะเป็นตัวแทนอย่างรวดเร็ว: การตั้งค่าระบบ, การเชื่อมโยงข้อมูล, และรูปแบบการประสานงาน เมื่อมีการสนับสนุนตัวแทน คุณสามารถมาตรฐานการเรียกฟังก์ชันที่เข้ากันได้กับ OpenAI หรือใช้ Azure AI Agents บนระบบคลาวด์ในรูปแบบไฮบริด

แหล่งข้อมูลอ้างอิง:
- เอกสาร Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Azure AI Foundry Agents: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- ตัวอย่างการเรียกฟังก์ชัน (Foundry Local samples): https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## วัตถุประสงค์การเรียนรู้
- ออกแบบการตั้งค่าระบบและกลยุทธ์การเชื่อมโยงข้อมูลเพื่อให้ได้พฤติกรรมที่เชื่อถือได้
- ใช้รูปแบบการเรียกฟังก์ชัน (การใช้เครื่องมือ)
- ประสานงานการทำงานของตัวแทนหลายตัว (ทั้งในระบบท้องถิ่นและไฮบริด)
- วางแผนสำหรับการสังเกตการณ์และความปลอดภัย

## ส่วนที่ 1: การตั้งค่าระบบและการเชื่อมโยงข้อมูล

- กำหนดบทบาท ข้อจำกัด และรูปแบบผลลัพธ์อย่างเข้มงวด
- เชื่อมโยงการตอบกลับด้วยข้อมูลในระบบท้องถิ่นหรือข้อมูลองค์กร
- บังคับให้ผลลัพธ์เป็น JSON เพื่อการทำงานอัตโนมัติในขั้นตอนถัดไป

## ส่วนที่ 2: การเรียกฟังก์ชัน (เข้ากันได้กับ OpenAI)

```python
# tools.py
import json

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

FUNCTIONS = [
    {
        "name": "get_weather",
        "description": "Get current weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "City name"}
            },
            "required": ["city"]
        }
    }
]
```

```python
# agent.py
import requests
import json
from tools import FUNCTIONS, get_weather

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def call_model(messages, functions=None):
    payload = {
        "model": MODEL,
        "messages": messages,
        "functions": functions,
        "function_call": "auto"
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    return r.json()

messages = [{"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": "What's the weather in Paris?"}]

resp = call_model(messages, functions=FUNCTIONS)
choice = resp["choices"][0]["message"]

if "function_call" in choice:
    fc = choice["function_call"]
    if fc["name"] == "get_weather":
        args = json.loads(fc["arguments"])
        result = get_weather(args["city"])
        messages.append(choice)
        messages.append({"role": "function", "name": "get_weather", "content": result})
        final = call_model(messages)
        print(final["choices"][0]["message"]["content"]) 
else:
    print(choice.get("content"))
```

รัน:
```powershell
# Ensure a model is running
foundry model run phi-4-mini
python agent.py
```


## ส่วนที่ 3: การประสานงานตัวแทนหลายตัว (รูปแบบ)

ออกแบบตัวประสานงานที่จัดการงานไปยังตัวแทนเฉพาะทาง (การดึงข้อมูล, การให้เหตุผล, การดำเนินการ) โดยใช้จุดเชื่อมต่อที่เข้ากันได้กับ OpenAI ของ Foundry Local

ขั้นตอนที่ 1) กำหนดตัวแทนเฉพาะทาง
```python
# agents/specialists.py
import requests
BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

headers = {"Content-Type": "application/json", "Authorization": "Bearer local-key"}

def chat(messages, max_tokens=300, temperature=0.4):
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json={
        "model": MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }, headers=headers, timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

class RetrievalAgent:
    SYSTEM = "You retrieve relevant snippets from knowledge sources based on a query."
    def run(self, query: str) -> str:
        # Placeholder: in real use, fetch from local files or vector DB
        messages = [{"role": "system", "content": self.SYSTEM},
                    {"role": "user", "content": f"Retrieve key facts for: {query}"}]
        return chat(messages)

class ReasoningAgent:
    SYSTEM = "You analyze inputs step by step and produce structured conclusions."
    def run(self, context: str, question: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\nThink step-by-step and produce a concise answer."}
        ]
        return chat(messages)

class ExecutionAgent:
    SYSTEM = "You transform decisions into actionable steps (JSON with actions)."
    def run(self, decision: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Turn this decision into 3 executable steps as JSON:\n{decision}"}
        ]
        return chat(messages)
```

ขั้นตอนที่ 2) สร้างตัวประสานงาน
```python
# agents/coordinator.py
from agents.specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent

class Coordinator:
    def __init__(self):
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()

    def handle(self, user_goal: str) -> dict:
        # 1. Retrieve context
        context = self.retrieval.run(user_goal)
        # 2. Reason on context
        decision = self.reasoning.run(context, user_goal)
        # 3. Produce actionable steps
        actions = self.execution.run(decision)
        return {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions
        }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    coord = Coordinator()
    result = coord.handle("Create a plan to onboard 5 new customers this month")
    print(result)
```

ขั้นตอนที่ 3) ตรวจสอบกับ Foundry Local
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator
python -m samples.05.agents.coordinator
```


แนวทาง:
- ใช้การลองใหม่และการตั้งค่าการหมดเวลาในการทำงานระหว่างตัวแทน
- เพิ่มพื้นที่เก็บข้อมูลในหน่วยความจำขนาดเล็ก (dict) สำหรับสถานะการสนทนา/เธรด
- เพิ่มการจำกัดอัตราเมื่อมีการเรียกหลายครั้งต่อเนื่อง

## ส่วนที่ 4: การสังเกตการณ์และความปลอดภัย

ติดตามการตั้งค่าระบบ การตอบกลับ และข้อผิดพลาดในระบบท้องถิ่น พร้อมทั้งบังคับใช้การจัดการข้อมูลในตัวแทนของคุณ

ขั้นตอนที่ 1) การบันทึกคำขอแบบเบา (ตัวเลือก)

หมายเหตุ: ตัวช่วยต่อไปนี้ไม่ได้รวมอยู่ในค่าเริ่มต้น สร้าง `infra/obs.py` หากคุณต้องการบันทึก JSON ในระบบท้องถิ่นสำหรับการทดลอง
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```

รวมการบันทึกเข้ากับตัวแทน (ตัวเลือก):
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```


ขั้นตอนที่ 2) ตรวจสอบความพร้อมใช้งานและสุขภาพพื้นฐานผ่าน CLI
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```


ขั้นตอนที่ 3) การลบข้อมูลและการจัดการ PII
- ก่อนส่งข้อความไปยังโมเดล ให้ลบหรือแฮชฟิลด์ที่มีข้อมูลสำคัญ (อีเมล, หมายเลขโทรศัพท์, รหัส)
- เก็บข้อมูลต้นฉบับไว้ในอุปกรณ์เท่านั้น ส่งเฉพาะข้อความที่จำเป็นสำหรับบริบท

ตัวช่วยการลบข้อมูลตัวอย่าง:
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

ใช้ในตัวแทน:
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```


ขั้นตอนที่ 4) ตัวตัดวงจรและการจัดการข้อผิดพลาด
- ห่อการเรียกตัวแทนแต่ละครั้งด้วย try/except และการถอยหลังแบบทวีคูณ
- หยุดการทำงานของกระบวนการเมื่อเกิดข้อผิดพลาดซ้ำ

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```


ขั้นตอนที่ 5) เส้นทางการตรวจสอบในระบบท้องถิ่นและการส่งออก
- เก็บบันทึก JSON ไว้ใน `./agent_logs`
- บีบอัดและหมุนเวียนบันทึกเป็นระยะ
- ส่งออกสรุปสำหรับการตรวจสอบ (จำนวน, เวลาเฉลี่ย, อัตราข้อผิดพลาด)

ขั้นตอนที่ 6) ตรวจสอบกับเอกสาร Microsoft Learn
- Foundry Local ให้บริการ API ที่เข้ากันได้กับ OpenAI (ตรวจสอบด้วย `curl /v1/models`)
- ใช้ `foundry model run <name>` เพื่อยืนยันความพร้อมใช้งานของโมเดล
- ปฏิบัติตามคำแนะนำอย่างเป็นทางการสำหรับการรวมไคลเอนต์และแอปตัวอย่าง (Open WebUI/how-tos)

แหล่งข้อมูลอ้างอิง:
- Foundry Local (Learn): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- วิธีการใช้งาน Open WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
- ตัวอย่างการเรียกฟังก์ชัน: https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## ขั้นตอนถัดไป
- สำรวจ Azure AI Agents สำหรับการประสานงานที่โฮสต์บนคลาวด์
- เพิ่มตัวเชื่อมต่อองค์กร (Microsoft Graph, Search, ฐานข้อมูล)

---

