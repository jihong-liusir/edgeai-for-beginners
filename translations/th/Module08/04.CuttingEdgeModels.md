<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T19:21:39+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "th"
}
-->
# Session 4: โมเดลล้ำสมัย – LLMs, SLMs และการประมวลผลบนอุปกรณ์

## ภาพรวม

เปรียบเทียบ LLMs และ SLMs, ประเมินข้อดีข้อเสียระหว่างการประมวลผลในเครื่องและบนคลาวด์ และสร้างเดโมที่แสดงสถานการณ์ EdgeAI โดยใช้ Phi และ ONNX Runtime นอกจากนี้เรายังจะพูดถึง Chainlit RAG, ตัวเลือกการประมวลผล WebGPU และการผสานรวม Open WebUI

แหล่งข้อมูลอ้างอิง:
- เอกสาร Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- วิธีใช้ Open WebUI (แอปแชทด้วย Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## วัตถุประสงค์การเรียนรู้
- เข้าใจข้อดีข้อเสียระหว่าง LLM และ SLM ในด้านต้นทุน, ความหน่วงเวลา และความแม่นยำ
- เลือกการประมวลผลในเครื่องหรือบนคลาวด์ให้เหมาะสมกับความต้องการทางธุรกิจ
- สร้างเดโม RAG ขนาดเล็กด้วย Chainlit
- สำรวจ WebGPU เพื่อเร่งการประมวลผลในเบราว์เซอร์
- เชื่อมต่อ Open WebUI กับ Foundry Local

## ส่วนที่ 1: LLM vs SLM – ตารางการตัดสินใจ

พิจารณา:
- ความหน่วงเวลา: SLMs บนอุปกรณ์มักให้การตอบสนองในระดับเสี้ยววินาที
- ต้นทุน: การประมวลผลในเครื่องช่วยลดค่าใช้จ่ายบนคลาวด์
- ความเป็นส่วนตัว: ข้อมูลที่สำคัญจะอยู่ในอุปกรณ์
- ความสามารถ: LLMs อาจมีประสิทธิภาพเหนือกว่า SLMs ในงานที่ซับซ้อน
- ความน่าเชื่อถือ: กลยุทธ์แบบไฮบริดช่วยลดความเสี่ยงของการหยุดทำงาน

## ส่วนที่ 2: การประมวลผลในเครื่อง vs บนคลาวด์ – รูปแบบไฮบริด

- ใช้การประมวลผลในเครื่องเป็นหลักและสำรองด้วยคลาวด์สำหรับคำสั่งที่ใหญ่หรือซับซ้อน
- ใช้คลาวด์เป็นหลักและสำรองด้วยการประมวลผลในเครื่องสำหรับข้อมูลที่ต้องการความเป็นส่วนตัวหรือสถานการณ์ออฟไลน์
- เลือกเส้นทางตามประเภทงาน (เช่น การสร้างโค้ดไปที่ DeepSeek, การแชททั่วไปไปที่ Phi/Qwen)

## ส่วนที่ 3: แอปแชท RAG ด้วย Chainlit (ขั้นพื้นฐาน)

ติดตั้ง dependencies:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

รัน:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

ขยาย: เพิ่มตัวดึงข้อมูลง่ายๆ (ไฟล์ในเครื่อง) และเพิ่มบริบทที่ดึงมาไว้ก่อนคำสั่งของผู้ใช้

## ส่วนที่ 4: การประมวลผลด้วย WebGPU (ข้อมูลเบื้องต้น)

รันโมเดลขนาดเล็กโดยตรงในเบราว์เซอร์ด้วย WebGPU เหมาะสำหรับเดโมที่เน้นความเป็นส่วนตัวและประสบการณ์ที่ไม่ต้องติดตั้ง ด้านล่างเป็นตัวอย่างขั้นตอนพื้นฐานโดยใช้ ONNX Runtime Web กับ WebGPU execution provider

1) ตรวจสอบการรองรับ WebGPU
- เบราว์เซอร์ Chromium: chrome://gpu → ยืนยันว่า “WebGPU” เปิดใช้งาน
- ตรวจสอบด้วยโปรแกรม (เราจะตรวจสอบในโค้ดด้วย): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) สร้างโปรเจกต์พื้นฐาน
สร้างโฟลเดอร์และไฟล์สองไฟล์: `index.html` และ `main.js`

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) ให้บริการในเครื่อง (Windows cmd.exe)
ใช้เซิร์ฟเวอร์แบบ static ง่ายๆ เพื่อให้เบราว์เซอร์ดึงโมเดลได้

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

เปิด http://localhost:5173 ในเบราว์เซอร์ คุณควรเห็นล็อกการเริ่มต้น, การสร้าง session ด้วย WebGPU และการทำนาย argmax

4) การแก้ไขปัญหา
- หาก WebGPU ไม่พร้อมใช้งาน: อัปเดต Chrome/Edge และตรวจสอบให้แน่ใจว่าไดรเวอร์ GPU เป็นเวอร์ชันล่าสุด จากนั้นตรวจสอบ chrome://flags เพื่อเปิดใช้งาน “Enable WebGPU”
- หากเกิดข้อผิดพลาด CORS หรือ fetch: ตรวจสอบให้แน่ใจว่าคุณให้บริการไฟล์ผ่าน http:// (ไม่ใช่ file://) และ URL ของโมเดลอนุญาตคำขอข้ามต้นทาง
- สำรองไปที่ CPU: เปลี่ยน `executionProviders: ['wasm']` เพื่อยืนยันพฤติกรรมพื้นฐาน

5) ขั้นตอนถัดไป
- เปลี่ยนเป็นโมเดล ONNX เฉพาะโดเมน (เช่น การจำแนกภาพหรือโมเดลข้อความขนาดเล็ก)
- เพิ่มตรรกะการประมวลผลก่อน/หลังสำหรับข้อมูลจริง
- สำหรับโมเดลขนาดใหญ่หรือความหน่วงเวลาสำหรับการใช้งานจริง ให้ใช้ Foundry Local หรือ ONNX Runtime Server

## ส่วนที่ 5: Open WebUI + Foundry Local (ขั้นตอน)

เชื่อมต่อ Open WebUI กับ API ที่เข้ากันได้กับ OpenAI ของ Foundry Local เพื่อสร้าง UI แชทในเครื่อง

1) สิ่งที่ต้องเตรียม
- ติดตั้ง Foundry Local และใช้งานได้ (`foundry --version`)
- มีโมเดลพร้อมใช้งานในเครื่อง (เช่น `phi-4-mini`)
- ติดตั้ง Docker Desktop (แนะนำสำหรับ Open WebUI)

2) เริ่มโมเดลด้วย Foundry Local
```powershell
foundry model run phi-4-mini
```
สิ่งนี้จะเปิด API ที่เข้ากันได้กับ OpenAI ที่ `http://localhost:8000`

3) เริ่ม Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
หมายเหตุ:
- บน Windows, `host.docker.internal` ช่วยให้ container เข้าถึง host ที่ `localhost`
- เราตั้งค่า `OPENAI_API_BASE_URL` เป็น endpoint ของ Foundry Local และ `OPENAI_API_KEY` เป็นค่าหลอก

4) ตั้งค่าจาก UI ของ Open WebUI (ทางเลือก)
- เปิด http://localhost:3000
- ทำการตั้งค่าเริ่มต้น (ผู้ดูแลระบบ)
- ไปที่ Settings → Models/Providers
- ตั้งค่า Base URL: `http://host.docker.internal:8000/v1`
- ตั้งค่า API Key: `local-key` (ตัวแทน)
- บันทึก

5) ทดสอบคำสั่ง
- ใน Open WebUI chat, เลือกหรือใส่ชื่อโมเดล `phi-4-mini`
- คำสั่ง: “List five benefits of on-device AI inference.”
- คุณควรเห็นการตอบกลับที่สตรีมจากโมเดลในเครื่องของคุณ

6) การแก้ไขปัญหา
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) ตัวเลือก: บันทึกข้อมูล Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## รายการตรวจสอบการลงมือทำ
- [ ] เปรียบเทียบการตอบกลับ/ความหน่วงเวลาระหว่าง SLM และ LLM ในเครื่อง
- [ ] รันเดโม Chainlit กับโมเดลอย่างน้อยสองตัว
- [ ] เชื่อมต่อ Open WebUI กับ endpoint ในเครื่องของคุณและทดสอบ

## ขั้นตอนถัดไป
- เตรียมตัวสำหรับ workflow ของ agent ใน Session 5
- ระบุสถานการณ์ที่รูปแบบไฮบริดในเครื่อง/คลาวด์ช่วยเพิ่ม ROI

---

