<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-18T08:17:22+00:00",
  "source_file": "Module03/README.md",
  "language_code": "th"
}
-->
# บทที่ 03: การปรับใช้ Small Language Models (SLMs)

บทนี้เป็นการสำรวจวงจรชีวิตทั้งหมดของการปรับใช้ Small Language Models (SLMs) โดยครอบคลุมถึงพื้นฐานทางทฤษฎี กลยุทธ์การใช้งานในทางปฏิบัติ และโซลูชันที่พร้อมใช้งานในระดับการผลิตผ่านการคอนเทนเนอร์ บทนี้ถูกจัดโครงสร้างเป็นสามส่วนที่พัฒนาต่อเนื่องกัน เพื่อพาผู้อ่านจากแนวคิดพื้นฐานไปสู่สถานการณ์การปรับใช้ขั้นสูง

## โครงสร้างบทและเส้นทางการเรียนรู้

### **[ส่วนที่ 1: การเรียนรู้ขั้นสูงของ SLM - พื้นฐานและการปรับแต่ง](./01.SLMAdvancedLearning.md)**
ส่วนเปิดนี้จะวางรากฐานทางทฤษฎีสำหรับการทำความเข้าใจ Small Language Models และความสำคัญเชิงกลยุทธ์ของพวกมันในงานปรับใช้ AI บน Edge โดยเนื้อหาครอบคลุม:

- **กรอบการจำแนกพารามิเตอร์**: การสำรวจรายละเอียดของหมวดหมู่ SLM ตั้งแต่ Micro SLMs (100M-1.4B พารามิเตอร์) ไปจนถึง Medium SLMs (14B-30B พารามิเตอร์) โดยเน้นที่โมเดลอย่าง Phi-4-mini-3.8B, Qwen3 series และ Google Gemma3 รวมถึงการวิเคราะห์ความต้องการฮาร์ดแวร์และการใช้หน่วยความจำในแต่ละระดับ
- **เทคนิคการปรับแต่งขั้นสูง**: การครอบคลุมวิธีการควอนไทเซชันโดยใช้ Llama.cpp, Microsoft Olive และ Apple MLX รวมถึง BitNET 1-bit quantization ที่ล้ำสมัย พร้อมตัวอย่างโค้ดที่แสดงกระบวนการควอนไทเซชันและผลการทดสอบ
- **กลยุทธ์การจัดหาโมเดล**: การวิเคราะห์เชิงลึกของระบบนิเวศ Hugging Face และ Azure AI Foundry Model Catalog สำหรับการปรับใช้ SLM ระดับองค์กร พร้อมตัวอย่างโค้ดสำหรับการดาวน์โหลดโมเดล การตรวจสอบความถูกต้อง และการแปลงรูปแบบ
- **API สำหรับนักพัฒนา**: ตัวอย่างโค้ดใน Python, C++ และ C# ที่แสดงวิธีโหลดโมเดล ทำการอนุมาน และผสานรวมกับเฟรมเวิร์กยอดนิยมอย่าง PyTorch, TensorFlow และ ONNX Runtime

ส่วนพื้นฐานนี้เน้นความสมดุลระหว่างประสิทธิภาพการดำเนินงาน ความยืดหยุ่นในการปรับใช้ และความคุ้มค่า ซึ่งทำให้ SLM เหมาะสำหรับงาน Edge Computing พร้อมตัวอย่างโค้ดที่นักพัฒนาสามารถนำไปใช้ในโครงการของตนได้โดยตรง

### **[ส่วนที่ 2: การปรับใช้ในสภาพแวดล้อมท้องถิ่น - โซลูชันที่ให้ความสำคัญกับความเป็นส่วนตัว](./02.DeployingSLMinLocalEnv.md)**
ส่วนที่สองนี้เปลี่ยนจากทฤษฎีไปสู่การใช้งานจริง โดยมุ่งเน้นไปที่กลยุทธ์การปรับใช้ในสภาพแวดล้อมท้องถิ่นที่ให้ความสำคัญกับอธิปไตยของข้อมูลและความเป็นอิสระในการดำเนินงาน หัวข้อสำคัญได้แก่:

- **แพลตฟอร์ม Ollama Universal**: การสำรวจการปรับใช้ข้ามแพลตฟอร์มอย่างครอบคลุม โดยเน้นที่เวิร์กโฟลว์ที่เป็นมิตรกับนักพัฒนา การจัดการวงจรชีวิตของโมเดล และการปรับแต่งผ่าน Modelfiles รวมถึงตัวอย่างการผสาน REST API และสคริปต์ CLI อัตโนมัติ
- **Microsoft Foundry Local**: โซลูชันการปรับใช้ระดับองค์กรที่ใช้การปรับแต่งด้วย ONNX การผสานรวม Windows ML และคุณสมบัติด้านความปลอดภัยที่ครอบคลุม พร้อมตัวอย่างโค้ด C# และ Python สำหรับการผสานแอปพลิเคชันแบบเนทีฟ
- **การวิเคราะห์เปรียบเทียบ**: การเปรียบเทียบเฟรมเวิร์กอย่างละเอียด ครอบคลุมสถาปัตยกรรมทางเทคนิค ลักษณะการทำงาน และแนวทางการปรับใช้ที่เหมาะสม พร้อมโค้ดสำหรับการทดสอบความเร็วในการอนุมานและการใช้หน่วยความจำบนฮาร์ดแวร์ต่างๆ
- **การผสาน API**: ตัวอย่างแอปพลิเคชันที่แสดงวิธีสร้างเว็บเซอร์วิส แอปพลิเคชันแชท และสายงานประมวลผลข้อมูลโดยใช้การปรับใช้ SLM ในพื้นที่ พร้อมตัวอย่างโค้ดใน Node.js, Python Flask/FastAPI และ ASP.NET Core
- **เฟรมเวิร์กการทดสอบ**: แนวทางการทดสอบอัตโนมัติสำหรับการประกันคุณภาพของโมเดล รวมถึงตัวอย่างการทดสอบหน่วยและการทดสอบการผสานสำหรับการใช้งาน SLM

ส่วนนี้ให้คำแนะนำเชิงปฏิบัติสำหรับองค์กรที่ต้องการใช้โซลูชัน AI ที่รักษาความเป็นส่วนตัว พร้อมตัวอย่างโค้ดที่สามารถปรับใช้ได้ตามความต้องการเฉพาะของนักพัฒนา

### **[ส่วนที่ 3: การปรับใช้ในระบบคลาวด์แบบคอนเทนเนอร์ - โซลูชันระดับการผลิต](./03.DeployingSLMinCloud.md)**
ส่วนสุดท้ายนี้มุ่งเน้นไปที่กลยุทธ์การปรับใช้แบบคอนเทนเนอร์ขั้นสูง โดยใช้ Microsoft’s Phi-4-mini-instruct เป็นกรณีศึกษา หลักสูตรนี้ครอบคลุม:

- **การปรับใช้ vLLM**: การปรับแต่งการอนุมานประสิทธิภาพสูงด้วย OpenAI-compatible APIs การเร่งความเร็วด้วย GPU ขั้นสูง และการกำหนดค่าระดับการผลิต รวมถึง Dockerfiles, Kubernetes manifests และพารามิเตอร์การปรับแต่งประสิทธิภาพ
- **การจัดการคอนเทนเนอร์ Ollama**: เวิร์กโฟลว์การปรับใช้ง่ายๆ ด้วย Docker Compose ตัวเลือกการปรับแต่งโมเดล และการผสาน UI บนเว็บ พร้อมตัวอย่าง CI/CD pipeline สำหรับการปรับใช้อัตโนมัติและการทดสอบ
- **การใช้งาน ONNX Runtime**: การปรับใช้ที่เหมาะสมกับ Edge พร้อมกลยุทธ์การแปลงโมเดล การควอนไทเซชัน และความเข้ากันได้ข้ามแพลตฟอร์ม รวมถึงตัวอย่างโค้ดสำหรับการปรับแต่งและการปรับใช้โมเดล
- **การตรวจสอบและการสังเกตการณ์**: การใช้งานแดชบอร์ด Prometheus/Grafana พร้อมเมตริกที่กำหนดเองสำหรับการตรวจสอบประสิทธิภาพ SLM รวมถึงการกำหนดค่าการแจ้งเตือนและการรวบรวมบันทึก
- **การปรับสมดุลโหลดและการปรับขนาด**: ตัวอย่างเชิงปฏิบัติของกลยุทธ์การปรับขนาดแนวนอนและแนวตั้ง พร้อมการกำหนดค่า autoscaling ตามการใช้งาน CPU/GPU และรูปแบบคำขอ
- **การเพิ่มความปลอดภัย**: แนวทางปฏิบัติที่ดีที่สุดสำหรับความปลอดภัยของคอนเทนเนอร์ รวมถึงการลดสิทธิพิเศษ นโยบายเครือข่าย และการจัดการความลับสำหรับคีย์ API และข้อมูลประจำตัวการเข้าถึงโมเดล

แต่ละแนวทางการปรับใช้มาพร้อมตัวอย่างการกำหนดค่า กระบวนการทดสอบ รายการตรวจสอบความพร้อมสำหรับการผลิต และเทมเพลตโครงสร้างพื้นฐานเป็นโค้ดที่นักพัฒนาสามารถนำไปใช้ในเวิร์กโฟลว์การปรับใช้ของตนได้โดยตรง

## ผลลัพธ์การเรียนรู้ที่สำคัญ

เมื่อจบบทนี้ ผู้อ่านจะเชี่ยวชาญใน:

1. **การเลือกโมเดลเชิงกลยุทธ์**: เข้าใจขอบเขตของพารามิเตอร์และเลือก SLM ที่เหมาะสมตามข้อจำกัดด้านทรัพยากรและความต้องการด้านประสิทธิภาพ
2. **ความเชี่ยวชาญด้านการปรับแต่ง**: การใช้เทคนิคควอนไทเซชันขั้นสูงในเฟรมเวิร์กต่างๆ เพื่อให้ได้สมดุลระหว่างประสิทธิภาพและความคุ้มค่า
3. **ความยืดหยุ่นในการปรับใช้**: การเลือกโซลูชันที่เน้นความเป็นส่วนตัวในพื้นที่หรือการปรับใช้แบบคอนเทนเนอร์ที่ปรับขนาดได้ตามความต้องการขององค์กร
4. **ความพร้อมสำหรับการผลิต**: การกำหนดค่าระบบตรวจสอบ ความปลอดภัย และการปรับขนาดสำหรับการปรับใช้ SLM ระดับองค์กร

## การมุ่งเน้นเชิงปฏิบัติและการประยุกต์ใช้ในโลกจริง

บทนี้รักษาแนวทางเชิงปฏิบัติอย่างเข้มข้น โดยมี:

- **ตัวอย่างการใช้งานจริง**: ไฟล์การกำหนดค่า กระบวนการทดสอบ API และสคริปต์การปรับใช้ที่สมบูรณ์
- **การเปรียบเทียบประสิทธิภาพ**: การเปรียบเทียบความเร็วในการอนุมาน การใช้หน่วยความจำ และความต้องการทรัพยากรอย่างละเอียด
- **ข้อควรพิจารณาด้านความปลอดภัย**: แนวทางปฏิบัติด้านความปลอดภัยระดับองค์กร กรอบการปฏิบัติตามข้อกำหนด และกลยุทธ์การปกป้องข้อมูล
- **แนวทางปฏิบัติที่ดีที่สุด**: แนวทางที่พิสูจน์แล้วสำหรับการตรวจสอบ การปรับขนาด และการบำรุงรักษา

## มุมมองที่พร้อมสำหรับอนาคต

บทนี้สรุปด้วยข้อมูลเชิงลึกเกี่ยวกับแนวโน้มที่กำลังเกิดขึ้น เช่น:

- สถาปัตยกรรมโมเดลขั้นสูงที่มีอัตราส่วนประสิทธิภาพที่ดีขึ้น
- การผสานฮาร์ดแวร์ที่ลึกซึ้งยิ่งขึ้นกับตัวเร่ง AI เฉพาะทาง
- การพัฒนาระบบนิเวศไปสู่มาตรฐานและความสามารถในการทำงานร่วมกัน
- รูปแบบการนำไปใช้ในองค์กรที่ขับเคลื่อนด้วยความเป็นส่วนตัวและข้อกำหนดการปฏิบัติตามกฎระเบียบ

แนวทางที่ครอบคลุมนี้ช่วยให้ผู้อ่านพร้อมรับมือกับความท้าทายในการปรับใช้ SLM ในปัจจุบันและการพัฒนาเทคโนโลยีในอนาคต โดยสามารถตัดสินใจได้อย่างมีข้อมูลที่สอดคล้องกับความต้องการและข้อจำกัดเฉพาะขององค์กร

บทนี้ทำหน้าที่เป็นทั้งคู่มือเชิงปฏิบัติสำหรับการใช้งานทันทีและทรัพยากรเชิงกลยุทธ์สำหรับการวางแผนการปรับใช้ AI ระยะยาว โดยเน้นความสมดุลที่สำคัญระหว่างความสามารถ ประสิทธิภาพ และความเป็นเลิศในการดำเนินงานที่กำหนดความสำเร็จของการปรับใช้ SLM

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้