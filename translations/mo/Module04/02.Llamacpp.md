<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T18:52:43+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "mo"
}
-->
# 第 2 節：Llama.cpp 實作指南

## 目錄
1. [簡介](../../../Module04)
2. [什麼是 Llama.cpp？](../../../Module04)
3. [安裝](../../../Module04)
4. [從原始碼建置](../../../Module04)
5. [模型量化](../../../Module04)
6. [基本使用](../../../Module04)
7. [進階功能](../../../Module04)
8. [Python 整合](../../../Module04)
9. [故障排除](../../../Module04)
10. [最佳實踐](../../../Module04)

## 簡介

這份完整的教學指南將帶領您了解 Llama.cpp 的所有內容，從基本安裝到進階使用情境。Llama.cpp 是一個強大的 C++ 實作框架，能以最少的設定和卓越的效能在各種硬體配置上進行大型語言模型（LLMs）的推論。

## 什麼是 Llama.cpp？

Llama.cpp 是一個以 C/C++ 撰寫的 LLM 推論框架，能夠在本地執行大型語言模型，並提供最少的設定和最先進的效能，適用於多種硬體。其主要特點包括：

### 核心功能
- **純 C/C++ 實作**，無需依賴其他套件
- **跨平台相容性**（Windows、macOS、Linux）
- **硬體優化**，適用於多種架構
- **量化支援**（1.5-bit 至 8-bit 整數量化）
- **CPU 和 GPU 加速**支援
- **記憶體效率**，適合資源有限的環境

### 優勢
- 在 CPU 上高效運行，無需特殊硬體
- 支援多種 GPU 後端（CUDA、Metal、OpenCL、Vulkan）
- 輕量且可攜性高
- Apple Silicon 是一等公民 - 透過 ARM NEON、Accelerate 和 Metal 框架進行優化
- 支援多種量化級別以減少記憶體使用

## 安裝

### 方法 1：預建二進制檔案（建議初學者使用）

#### 從 GitHub Releases 下載
1. 前往 [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. 下載適合您系統的二進制檔案：
   - `llama-<version>-bin-win-<feature>-<arch>.zip` 適用於 Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` 適用於 macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` 適用於 Linux

3. 解壓縮檔案並將目錄加入系統的 PATH

#### 使用套件管理工具

**macOS（Homebrew）：**
```bash
brew install llama.cpp
```

**Linux（各種發行版）：**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### 方法 2：Python 套件（llama-cpp-python）

#### 基本安裝
```bash
pip install llama-cpp-python
```

#### 啟用硬體加速
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## 從原始碼建置

### 先決條件

**系統需求：**
- C++ 編譯器（GCC、Clang 或 MSVC）
- CMake（版本 3.14 或更高）
- Git
- 適合您平台的建置工具

**安裝先決條件：**

**macOS：**
```bash
xcode-select --install
```

**Ubuntu/Debian：**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows：**
- 安裝 Visual Studio 2022 並啟用 C++ 開發工具
- 從官方網站安裝 CMake
- 安裝 Git

### 基本建置流程

1. **克隆存儲庫：**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **配置建置：**
```bash
cmake -B build
```

3. **建置專案：**
```bash
cmake --build build --config Release
```

為了加快編譯速度，可使用平行作業：
```bash
cmake --build build --config Release -j 8
```

### 特定硬體建置

#### CUDA 支援（NVIDIA GPU）
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal 支援（Apple Silicon）
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS 支援（CPU 優化）
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan 支援
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### 進階建置選項

#### 除錯建置
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### 啟用額外功能
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## 模型量化

### 了解 GGUF 格式

GGUF（Generalized GGML Unified Format）是一種優化的檔案格式，專為使用 Llama.cpp 和其他框架高效運行大型語言模型而設計。其提供：

- 標準化的模型權重存儲
- 提升跨平台相容性
- 增強效能
- 高效的元數據處理

### 量化類型

Llama.cpp 支援多種量化級別：

| 類型 | 位元 | 描述 | 使用情境 |
|------|------|------|----------|
| F16 | 16 | 半精度 | 高品質，大記憶體 |
| Q8_0 | 8 | 8-bit 量化 | 良好平衡 |
| Q4_0 | 4 | 4-bit 量化 | 中等品質，較小尺寸 |
| Q2_K | 2 | 2-bit 量化 | 最小尺寸，較低品質 |

### 模型轉換

#### 從 PyTorch 轉換為 GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### 從 Hugging Face 直接下載
許多模型已在 Hugging Face 上提供 GGUF 格式：
- 搜尋名稱中包含 "GGUF" 的模型
- 下載適合的量化級別
- 可直接搭配 Llama.cpp 使用

## 基本使用

### 命令列介面

#### 簡單文字生成
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### 使用 Hugging Face 的模型
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### 伺服器模式
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### 常用參數

| 參數 | 描述 | 範例 |
|------|------|------|
| `-m` | 模型檔案路徑 | `-m model.gguf` |
| `-p` | 提示文字 | `-p "Hello world"` |
| `-n` | 要生成的 token 數量 | `-n 100` |
| `-c` | 上下文大小 | `-c 4096` |
| `-t` | 執行緒數量 | `-t 8` |
| `-ngl` | GPU 層數 | `-ngl 32` |
| `-temp` | 溫度 | `-temp 0.7` |

### 互動模式

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## 進階功能

### 伺服器 API

#### 啟動伺服器
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API 使用
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### 效能優化

#### 記憶體管理
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### 多執行緒
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU 加速
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python 整合

### 搭配 llama-cpp-python 的基本使用

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### 聊天介面

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### 串流回應

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### 與 LangChain 整合

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## 故障排除

### 常見問題與解決方法

#### 建置錯誤

**問題：找不到 CMake**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**問題：找不到編譯器**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### 執行時問題

**問題：模型載入失敗**
- 確認模型檔案路徑
- 檢查檔案權限
- 確保有足夠的 RAM
- 嘗試不同的量化級別

**問題：效能不佳**
- 啟用硬體加速
- 增加執行緒數量
- 使用適合的量化級別
- 檢查 GPU 記憶體使用情況

#### 記憶體問題

**問題：記憶體不足**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### 平台特定問題

#### Windows
- 使用 MinGW 或 Visual Studio 編譯器
- 確保 PATH 配置正確
- 檢查防毒軟體是否干擾

#### macOS
- 為 Apple Silicon 啟用 Metal
- 如有需要，使用 Rosetta 2 提升相容性
- 檢查 Xcode 命令列工具

#### Linux
- 安裝開發套件
- 檢查 GPU 驅動版本
- 確認 CUDA 工具包安裝

## 最佳實踐

### 模型選擇
1. **根據硬體選擇適合的量化級別**
2. **考量模型大小與品質的取捨**
3. **針對特定使用情境測試不同模型**

### 效能優化
1. **有 GPU 時啟用加速**
2. **根據 CPU 優化執行緒數量**
3. **設定適合的上下文大小**
4. **為大型模型啟用記憶體映射**

### 生產部署
1. **使用伺服器模式提供 API 存取**
2. **實作適當的錯誤處理**
3. **監控資源使用情況**
4. **設置日誌記錄與監控**

### 開發工作流程
1. **從較小的模型開始測試**
2. **使用版本控制管理模型配置**
3. **記錄您的配置**
4. **在不同平台上進行測試**

### 安全考量
1. **驗證輸入提示**
2. **實作速率限制**
3. **保護 API 端點**
4. **監控濫用模式**

## 結論

Llama.cpp 提供了一種強大且高效的方式，能在各種硬體配置上本地運行大型語言模型。無論您是在開發 AI 應用、進行研究，或僅僅是探索 LLM，這個框架都能提供所需的靈活性與效能，適用於多種使用情境。

重點摘要：
- 選擇最適合您需求的安裝方法
- 根據硬體配置進行優化
- 從基本使用開始，逐步探索進階功能
- 考慮使用 Python 綁定以簡化整合
- 遵循最佳實踐進行生產部署

欲了解更多資訊與更新，請造訪 [官方 Llama.cpp 存儲庫](https://github.com/ggml-org/llama.cpp)，並參考完整的文件與社群資源。

## ➡️ 下一步

- [03：Microsoft Olive 優化套件](./03.MicrosoftOlive.md)

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。