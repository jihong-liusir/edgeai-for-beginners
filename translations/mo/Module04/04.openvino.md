<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T18:50:23+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "mo"
}
-->
# 第四章：OpenVINO 工具套件優化套件

## 目錄
1. [簡介](../../../Module04)
2. [什麼是 OpenVINO？](../../../Module04)
3. [安裝](../../../Module04)
4. [快速入門指南](../../../Module04)
5. [範例：使用 OpenVINO 進行模型轉換與優化](../../../Module04)
6. [進階使用](../../../Module04)
7. [最佳實踐](../../../Module04)
8. [故障排除](../../../Module04)
9. [其他資源](../../../Module04)

## 簡介

OpenVINO（Open Visual Inference and Neural Network Optimization）是 Intel 的開源工具套件，用於在雲端、本地和邊緣環境中部署高效能 AI 解決方案。無論目標是 CPU、GPU、VPU 或專用 AI 加速器，OpenVINO 都能提供全面的優化功能，同時保持模型準確性並支持跨平台部署。

## 什麼是 OpenVINO？

OpenVINO 是一個開源工具套件，幫助開發者在多種硬體平台上高效地優化、轉換和部署 AI 模型。它包含三個主要組件：用於推理的 OpenVINO Runtime、用於模型優化的神經網路壓縮框架（NNCF），以及用於可擴展部署的 OpenVINO Model Server。

### 主要功能

- **跨平台部署**：支持 Linux、Windows 和 macOS，並提供 Python、C++ 和 C API
- **硬體加速**：自動設備檢測與優化，支持 CPU、GPU、VPU 和 AI 加速器
- **模型壓縮框架**：通過 NNCF 提供先進的量化、剪枝和優化技術
- **框架兼容性**：直接支持 TensorFlow、ONNX、PaddlePaddle 和 PyTorch 模型
- **生成式 AI 支持**：專門的 OpenVINO GenAI 用於部署大型語言模型和生成式 AI 應用

### 優勢

- **性能優化**：顯著提升速度，準確性損失最小
- **減少部署負擔**：外部依賴最少，簡化安裝與部署
- **提升啟動速度**：優化模型加載與緩存，加快應用初始化
- **可擴展部署**：從邊緣設備到雲端基礎設施，提供一致的 API
- **生產就緒**：企業級可靠性，配有全面的文檔與社群支持

## 安裝

### 先決條件

- Python 3.8 或更高版本
- pip 套件管理器
- 建議使用虛擬環境
- 兼容硬體（建議使用 Intel CPU，但支持多種架構）

### 基本安裝

建立並啟用虛擬環境：

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

安裝 OpenVINO Runtime：

```bash
pip install openvino
```

安裝 NNCF 以進行模型優化：

```bash
pip install nncf
```

### OpenVINO GenAI 安裝

用於生成式 AI 應用：

```bash
pip install openvino-genai
```

### 可選依賴項

針對特定使用案例的額外套件：

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### 驗證安裝

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

如果成功，您應該能看到 OpenVINO 的版本信息。

## 快速入門指南

### 您的第一個模型優化

以下是使用 OpenVINO 轉換並優化 Hugging Face 模型的步驟：

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### 此過程的作用

優化工作流程包括：從 Hugging Face 加載原始模型，轉換為 OpenVINO 中間表示（IR）格式，應用預設優化，並為目標硬體編譯。

### 關鍵參數解析

- `export=True`：將模型轉換為 OpenVINO IR 格式
- `compile=False`：延遲編譯至運行時以提高靈活性
- `device`：目標硬體（如 "CPU"、"GPU"、"AUTO" 自動選擇）
- `save_pretrained()`：保存優化後的模型以供重用

## 範例：使用 OpenVINO 進行模型轉換與優化

### 步驟 1：使用 NNCF 量化進行模型轉換

以下是如何使用 NNCF 進行後訓練量化：

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### 步驟 2：使用權重壓縮進行進階優化

針對基於 Transformer 的模型，應用權重壓縮：

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### 步驟 3：使用優化後的模型進行推理

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### 輸出結構

優化後，您的模型目錄將包含：

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## 進階使用

### 使用 NNCF YAML 進行配置

針對複雜的優化工作流程，使用 NNCF 配置文件：

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

應用配置：

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU 優化

針對 GPU 加速：

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### 批量處理優化

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### 模型伺服器部署

使用 OpenVINO Model Server 部署優化後的模型：

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

模型伺服器的客戶端代碼：

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## 最佳實踐

### 1. 模型選擇與準備
- 使用支持的框架（PyTorch、TensorFlow、ONNX）的模型
- 確保模型輸入具有固定或已知的動態形狀
- 使用代表性數據集進行校準

### 2. 優化策略選擇
- **後訓練量化**：快速優化的首選
- **權重壓縮**：適合大型語言模型和 Transformer
- **量化感知訓練**：當準確性至關重要時使用

### 3. 硬體特定優化
- **CPU**：使用 INT8 量化以平衡性能
- **GPU**：利用 FP16 精度和批量處理
- **VPU**：專注於模型簡化和層融合

### 4. 性能調整
- **吞吐模式**：適用於高容量批量處理
- **延遲模式**：適用於實時交互應用
- **AUTO 設備**：讓 OpenVINO 自動選擇最佳硬體

### 5. 記憶體管理
- 謹慎使用動態形狀以避免記憶體負擔
- 實施模型緩存以加快後續加載
- 在優化過程中監控記憶體使用情況

### 6. 準確性驗證
- 始終驗證優化後的模型是否符合原始性能
- 使用代表性測試數據集進行評估
- 考慮逐步優化（從保守設置開始）

## 故障排除

### 常見問題

#### 1. 安裝問題
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. 模型轉換錯誤
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. 性能問題
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. 記憶體問題
- 在優化過程中減少模型批量大小
- 對於大型數據集使用流式處理
- 啟用模型緩存：`core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. 準確性下降
- 使用更高精度（如 INT8 而非 INT4）
- 增加校準數據集大小
- 應用混合精度優化

### 性能監控

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### 尋求幫助

- **文檔**：[docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub 問題**：[github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **社群論壇**：[community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## 其他資源

### 官方連結
- **OpenVINO 主頁**：[openvino.ai](https://openvino.ai/)
- **GitHub 儲存庫**：[github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF 儲存庫**：[github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **模型動物園**：[github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### 學習資源
- **OpenVINO 筆記本**：[github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **快速入門指南**：[docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **優化指南**：[docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### 整合工具
- **Hugging Face Optimum Intel**：[huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**：[docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**：[docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### 性能基準
- **官方基準**：[docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF 模型動物園**：[github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### 社群範例
- **Jupyter 筆記本**：[OpenVINO 筆記本儲存庫](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO 筆記本儲存庫中提供的全面教程
- **範例應用**：[OpenVINO 模型動物園](https://github.com/openvinotoolkit/open_model_zoo) - 各種領域的實際案例（計算機視覺、NLP、音頻）
- **部落格文章**：[Intel AI 部落格](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI 和社群部落格文章，提供詳細的使用案例

### 相關工具
- **Intel Neural Compressor**：[github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intel 硬體的額外優化技術
- **TensorFlow Lite**：[tensorflow.org/lite](https://www.tensorflow.org/lite) - 用於移動和邊緣部署的比較
- **ONNX Runtime**：[onnxruntime.ai](https://onnxruntime.ai/) - 跨平台推理引擎替代方案

## ➡️ 下一步

- [05：深入探討 Apple MLX 框架](./05.AppleMLX.md)

---

**免責聲明**：  
本文件使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。