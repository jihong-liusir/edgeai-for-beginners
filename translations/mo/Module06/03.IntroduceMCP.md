<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T18:46:34+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "mo"
}
-->
# 第三章 - 模型上下文協議 (MCP) 整合

## MCP (模型上下文協議) 簡介

模型上下文協議 (MCP) 是一個革命性的框架，能夠讓語言模型以標準化的方式與外部工具和系統互動。與傳統方法中模型被孤立不同，MCP 通過明確定義的協議，為 AI 模型與現實世界之間建立了一座橋樑。

### MCP 是什麼？

MCP 是一種通信協議，允許語言模型：
- 連接外部數據來源
- 執行工具和函數
- 與 API 和服務互動
- 獲取即時信息
- 執行複雜的多步操作

此協議將靜態語言模型轉變為能夠執行實際任務的動態代理，超越僅僅生成文本的功能。

## MCP 中的小型語言模型 (SLMs)

小型語言模型代表了一種高效的 AI 部署方式，具有以下幾個優勢：

### SLMs 的優勢
- **資源效率**：較低的計算需求
- **快速響應時間**：適用於即時應用的低延遲  
- **成本效益**：基礎設施需求最小化
- **隱私性**：可在本地運行，無需數據傳輸
- **定制化**：更容易針對特定領域進行微調

### 為什麼 SLMs 與 MCP 配合良好

SLMs 與 MCP 的結合是一種強大的組合，模型的推理能力通過外部工具得到增強，彌補了其較少的參數數量，並提供了更強大的功能。

## Python MCP SDK 概述

Python MCP SDK 提供了構建 MCP 應用的基礎。該 SDK 包括：

- **客戶端庫**：用於連接 MCP 伺服器
- **伺服器框架**：用於創建自定義 MCP 伺服器
- **協議處理器**：用於管理通信
- **工具整合**：用於執行外部函數

## 實際應用：Phi-4 MCP 客戶端

以下是使用 Microsoft 的 Phi-4 小型模型整合 MCP 功能的實際應用示例。

### 系統架構

該實現遵循分層架構：

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### 核心組件

#### 1. MCP 客戶端類

**BaseMCPClient**：提供通用功能的抽象基礎
- 非同步上下文管理協議
- 標準介面定義
- 資源管理

**Phi4MiniMCPClient**：基於 STDIO 的實現
- 本地進程通信
- 標準輸入/輸出處理
- 子進程管理

**Phi4MiniSSEMCPClient**：基於伺服器推送事件的實現
- HTTP 流式通信
- 即時事件處理
- 基於網絡的伺服器連接

#### 2. LLM 整合

**OllamaClient**：本地模型託管
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**：高性能服務
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. 工具處理管道

工具處理管道將 MCP 工具轉換為語言模型兼容的格式：

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## 入門指南：逐步操作

### 第一步：環境設置

安裝所需的依賴項：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 第二步：基本配置

設置環境變數：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 第三步：運行您的第一個 MCP 客戶端

**基本 Ollama 設置：**
```bash
python ghmodel_mcp_demo.py
```

**使用 vLLM 後端：**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**伺服器推送事件連接：**
```bash
python ghmodel_mcp_demo.py --run sse
```

**自定義 MCP 伺服器：**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 第四步：程式化使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高級功能

### 多後端支持

該實現支持 Ollama 和 vLLM 後端，您可以根據需求進行選擇：

- **Ollama**：更適合本地開發和測試
- **vLLM**：針對生產和高吞吐量場景進行優化

### 靈活的連接協議

支持兩種連接模式：

**STDIO 模式**：直接進程通信
- 低延遲
- 適合本地工具
- 簡單設置

**SSE 模式**：基於 HTTP 的流式通信
- 支持網絡
- 更適合分佈式系統
- 即時更新

### 工具整合能力

系統可以整合多種工具：
- 網絡自動化 (Playwright)
- 文件操作
- API 交互
- 系統命令
- 自定義函數

## 錯誤處理與最佳實踐

### 全面錯誤管理

該實現包含健全的錯誤處理機制：

**連接錯誤：**
- MCP 伺服器故障
- 網絡超時
- 連接問題

**工具執行錯誤：**
- 缺少工具
- 參數驗證
- 執行失敗

**響應處理錯誤：**
- JSON 解析問題
- 格式不一致
- LLM 響應異常

### 最佳實踐

1. **資源管理**：使用非同步上下文管理器
2. **錯誤處理**：實施全面的 try-catch 塊
3. **日誌記錄**：啟用適當的日誌級別
4. **安全性**：驗證輸入並清理輸出
5. **性能**：使用連接池和緩存

## 實際應用

### 網絡自動化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 數據處理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 整合
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 性能優化

### 記憶體管理
- 高效的消息歷史處理
- 適當的資源清理
- 連接池管理

### 網絡優化
- 非同步 HTTP 操作
- 可配置的超時設置
- 優雅的錯誤恢復

### 並行處理
- 非阻塞 I/O
- 工具的並行執行
- 高效的非同步模式

## 安全考量

### 數據保護
- 安全的 API 密鑰管理
- 輸入驗證
- 輸出清理

### 網絡安全
- 支持 HTTPS
- 本地端點默認設置
- 安全令牌管理

### 執行安全
- 工具過濾
- 沙盒環境
- 審計日誌

## 結論

整合 MCP 的 SLMs 代表了 AI 應用開發的一個新範式。通過結合小型模型的效率與外部工具的能力，開發者可以創建既資源高效又功能強大的智能系統。

Phi-4 MCP 客戶端的實現展示了如何在實際中完成這種整合，為構建複雜的 AI 驅動應用提供了堅實的基礎。

關鍵要點：
- MCP 彌合了語言模型與外部系統之間的差距
- SLMs 在工具增強下提供了高效且強大的能力
- 模塊化架構使擴展和定制變得簡單
- 健全的錯誤處理和安全措施對生產環境至關重要

本教程為構建您自己的 SLM 驅動 MCP 應用提供了基礎，開啟了自動化、數據處理和智能系統整合的可能性。

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。