<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T18:58:06+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "mo"
}
-->
# 容器化雲端部署 - 生產級解決方案

本教程全面介紹了三種主要方法，用於在容器化環境中部署 Microsoft 的 Phi-4-mini-instruct 模型：vLLM、Ollama 和使用 ONNX Runtime 的 SLM Engine。這款擁有 3.8B 參數的模型在推理任務中表現出色，同時保持了邊緣部署的效率。

## 目錄

1. [Phi-4-mini 容器部署介紹](../../../Module03)
2. [學習目標](../../../Module03)
3. [Phi-4-mini 分類解析](../../../Module03)
4. [vLLM 容器部署](../../../Module03)
5. [Ollama 容器部署](../../../Module03)
6. [使用 ONNX Runtime 的 SLM Engine](../../../Module03)
7. [框架比較](../../../Module03)
8. [最佳實踐](../../../Module03)

## Phi-4-mini 容器部署介紹

小型語言模型（SLMs）是 EdgeAI 的重要進展，能夠在資源有限的設備上實現高級自然語言處理功能。本教程聚焦於 Microsoft 的 Phi-4-mini-instruct 的容器化部署策略，這是一款平衡能力與效率的最先進推理模型。

### 特色模型：Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B 參數)**：Microsoft 最新的輕量化指令調整模型，專為記憶體/計算資源有限的環境設計，具備以下卓越能力：
- **數學推理與複雜計算**
- **代碼生成、調試與分析**
- **邏輯問題解決與逐步推理**
- **需要詳細解釋的教育應用**
- **函數調用與工具整合**

作為「小型 SLMs」類別的一部分（1.5B - 13.9B 參數），Phi-4-mini 在推理能力與資源效率之間達到了最佳平衡。

### Phi-4-mini 容器化部署的優勢

- **運行效率**：推理任務的快速推斷，計算需求較低
- **部署靈活性**：設備端 AI 功能，通過本地處理增強隱私
- **成本效益**：相比大型模型，運行成本更低，質量保持不變
- **隔離性**：模型實例之間的清晰分離與安全執行環境
- **可擴展性**：輕鬆水平擴展以提高推理吞吐量

## 學習目標

完成本教程後，您將能夠：

- 在多種容器化環境中部署並優化 Phi-4-mini-instruct
- 為不同部署場景實施高級量化與壓縮策略
- 配置生產級容器編排以支持推理工作負載
- 根據特定用例需求評估並選擇合適的部署框架
- 為容器化 SLM 部署應用安全性、監控與擴展的最佳實踐

## Phi-4-mini 分類解析

### 模型規格

**技術細節：**
- **參數**：3.8 億（小型 SLM 類別）
- **架構**：密集型僅解碼 Transformer，具備分組查詢注意力
- **上下文長度**：128K tokens（建議 32K 以獲得最佳性能）
- **詞彙表**：200K tokens，支持多語言
- **訓練數據**：5T 高質量推理密集型內容

### 資源需求

| 部署類型 | 最低 RAM | 建議 RAM | VRAM (GPU) | 存儲 | 典型用例 |
|----------|----------|----------|------------|------|----------|
| **開發** | 6GB | 8GB | - | 8GB | 本地測試、原型設計 |
| **生產 CPU** | 8GB | 12GB | - | 10GB | 邊緣服務器、成本優化部署 |
| **生產 GPU** | 6GB | 8GB | 4-6GB | 8GB | 高吞吐量推理服務 |
| **邊緣優化** | 4GB | 6GB | - | 6GB | 量化部署、IoT 閘道 |

### Phi-4-mini 能力

- **數學卓越**：高級算術、代數與微積分問題解決
- **代碼智能**：Python、JavaScript 及多語言代碼生成與調試
- **邏輯推理**：逐步分解問題並構建解決方案
- **教育支持**：適合學習與教學場景的詳細解釋
- **函數調用**：原生支持工具整合與 API 交互

## vLLM 容器部署

vLLM 為 Phi-4-mini-instruct 提供了卓越支持，具備優化的推理性能與 OpenAI 兼容的 API，非常適合生產推理服務。

### 快速入門示例

#### 基本 CPU 部署（開發）
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU 加速生產部署
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### 生產配置

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### 測試 Phi-4-mini 推理能力

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama 容器部署

Ollama 為 Phi-4-mini-instruct 提供了簡化的部署與管理，非常適合開發與平衡的生產部署。

### 快速設置

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### 生產配置

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### 模型優化與變體

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API 使用示例

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## 使用 ONNX Runtime 的 SLM Engine

ONNX Runtime 為 Phi-4-mini-instruct 的邊緣部署提供了最佳性能，具備高級優化與跨平台兼容性。

### 基本設置

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### 簡化的服務器實現

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### 模型轉換腳本

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### 生產配置

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### 測試 ONNX 部署

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## 框架比較

### Phi-4-mini 的框架比較

| 特性 | vLLM | Ollama | ONNX Runtime |
|------|------|--------|--------------|
| **設置複雜性** | 中等 | 簡單 | 複雜 |
| **性能（GPU）** | 優秀 (~25 tok/s) | 非常好 (~20 tok/s) | 良好 (~15 tok/s) |
| **性能（CPU）** | 良好 (~8 tok/s) | 非常好 (~12 tok/s) | 優秀 (~15 tok/s) |
| **內存使用** | 8-12GB | 6-10GB | 4-8GB |
| **API 兼容性** | OpenAI 兼容 | 自定義 REST | 自定義 FastAPI |
| **函數調用** | ✅ 原生支持 | ✅ 支持 | ⚠️ 自定義實現 |
| **量化支持** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX 量化 |
| **生產就緒** | ✅ 優秀 | ✅ 非常好 | ✅ 良好 |
| **邊緣部署** | 良好 | 優秀 | 出色 |

## 附加資源

### 官方文檔
- **Microsoft Phi-4 模型卡**：詳細規格與使用指南
- **vLLM 文檔**：高級配置與優化選項
- **Ollama 模型庫**：社群模型與自定義示例
- **ONNX Runtime 指南**：性能優化與部署策略

### 開發工具
- **Hugging Face Transformers**：用於模型交互與自定義
- **OpenAI API 規範**：用於 vLLM 兼容性測試
- **Docker 最佳實踐**：容器安全與優化指南
- **Kubernetes 部署**：生產擴展的編排模式

### 學習資源
- **SLM 性能基準測試**：比較分析方法
- **邊緣 AI 部署**：資源有限環境的最佳實踐
- **推理任務優化**：數學與邏輯問題的提示策略
- **容器安全**：AI 模型部署的加固實踐

## 學習成果

完成本模組後，您將能夠：

1. 使用多種框架在容器化環境中部署 Phi-4-mini-instruct 模型
2. 為不同硬件環境配置並優化 SLM 部署
3. 為容器化 AI 部署實施安全性最佳實踐
4. 根據特定用例需求比較並選擇合適的部署框架
5. 為生產級 SLM 服務應用監控與擴展策略

## 下一步

- 返回 [模組 1](../Module01/README.md)
- 返回 [模組 2](../Module02/README.md)

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。