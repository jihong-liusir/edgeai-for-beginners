<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-17T18:56:07+00:00",
  "source_file": "Module03/README.md",
  "language_code": "mo"
}
-->
# 第三章：部署小型語言模型 (SLMs)

本章全面探討小型語言模型 (SLMs) 部署的完整生命周期，涵蓋理論基礎、實際實施策略以及適用於生產環境的容器化解決方案。章節分為三個循序漸進的部分，帶領讀者從基本概念到高級部署場景。

## 章節結構與學習旅程

### **[第一部分：SLM 高級學習 - 基礎與優化](./01.SLMAdvancedLearning.md)**
開篇部分建立了理解小型語言模型的理論基礎，並闡述其在邊緣 AI 部署中的策略性重要性。本部分涵蓋：

- **參數分類框架**：詳細探討 SLM 類別，從微型 SLMs (100M-1.4B 參數) 到中型 SLMs (14B-30B 參數)，重點介紹 Phi-4-mini-3.8B、Qwen3 系列和 Google Gemma3 等模型，包括各模型層級的硬體需求和記憶體佔用分析
- **高級優化技術**：全面介紹使用 Llama.cpp、Microsoft Olive 和 Apple MLX 框架的量化方法，包括最前沿的 BitNET 1-bit 量化技術，並提供量化管道和基準測試結果的實際代碼範例
- **模型獲取策略**：深入分析 Hugging Face 生態系統和 Azure AI Foundry Model Catalog，用於企業級 SLM 部署，並提供程式化模型下載、驗證和格式轉換的代碼範例
- **開發者 API**：提供 Python、C++ 和 C# 的代碼範例，展示如何載入模型、執行推理以及與 PyTorch、TensorFlow 和 ONNX Runtime 等流行框架整合

本基礎部分強調操作效率、部署靈活性和成本效益之間的平衡，使 SLM 成為邊緣計算場景的理想選擇，並提供開發者可直接在項目中實施的實際代碼範例。

### **[第二部分：本地環境部署 - 隱私優先解決方案](./02.DeployingSLMinLocalEnv.md)**
第二部分從理論過渡到實際實施，專注於優先考慮數據主權和操作獨立性的本地部署策略。主要內容包括：

- **Ollama 通用平台**：全面探討跨平台部署，重點介紹開發者友好的工作流程、模型生命周期管理以及通過 Modelfiles 進行定制化，包括完整的 REST API 整合範例和 CLI 自動化腳本
- **Microsoft Foundry Local**：企業級部署解決方案，結合基於 ONNX 的優化、Windows ML 整合以及全面的安全功能，並提供 C# 和 Python 的代碼範例，用於原生應用整合
- **比較分析**：詳細框架比較，涵蓋技術架構、性能特徵和用例優化指南，並提供基準代碼以評估不同硬體上的推理速度和記憶體使用情況
- **API 整合**：示例應用展示如何使用本地 SLM 部署構建網頁服務、聊天應用和數據處理管道，並提供 Node.js、Python Flask/FastAPI 和 ASP.NET Core 的代碼範例
- **測試框架**：模型質量保證的自動化測試方法，包括 SLM 實施的單元測試和整合測試範例

本部分為希望實施隱私保護 AI 解決方案的組織提供實用指導，同時保持對部署環境的完全控制，並提供開發者可根據具體需求調整的即用型代碼範例。

### **[第三部分：容器化雲端部署 - 生產級解決方案](./03.DeployingSLMinCloud.md)**
最後一部分以高級容器化部署策略為主，並以 Microsoft 的 Phi-4-mini-instruct 作為主要案例研究。本部分涵蓋：

- **vLLM 部署**：高性能推理優化，兼容 OpenAI API、高級 GPU 加速和生產級配置，包括完整的 Dockerfiles、Kubernetes 清單和性能調整參數
- **Ollama 容器編排**：簡化的部署工作流程，結合 Docker Compose、模型優化變體和網頁 UI 整合，並提供 CI/CD 管道範例，用於自動化部署和測試
- **ONNX Runtime 實施**：邊緣優化部署，結合全面的模型轉換、量化策略和跨平台兼容性，並提供詳細的模型優化和部署代碼範例
- **監控與可觀察性**：實施 Prometheus/Grafana 儀表板，用於 SLM 性能監控，包括警報配置和日誌聚合
- **負載平衡與擴展**：水平和垂直擴展策略的實際範例，包括基於 CPU/GPU 使用率和請求模式的自動擴展配置
- **安全加固**：容器安全最佳實踐，包括權限減少、網絡策略以及 API 密鑰和模型訪問憑證的秘密管理

每種部署方法均提供完整的配置範例、測試程序、生產準備檢查清單以及基於基礎設施即代碼的模板，開發者可直接應用於其部署工作流程。

## 主要學習成果

完成本章後，讀者將掌握：

1. **策略性模型選擇**：理解參數範圍並根據資源限制和性能需求選擇合適的 SLM
2. **優化技術精通**：在不同框架中實施高級量化技術，以實現最佳性能與效率平衡
3. **部署靈活性**：根據組織需求選擇本地隱私優先解決方案或可擴展的容器化部署
4. **生產準備**：配置監控、安全和擴展系統，用於企業級 SLM 部署

## 實用重點與實際應用

本章始終保持強烈的實用導向，特色包括：

- **動手範例**：完整的配置文件、API 測試程序和部署腳本
- **性能基準測試**：詳細比較推理速度、記憶體使用和資源需求
- **安全考量**：企業級安全實踐、合規框架和數據保護策略
- **最佳實踐**：經過生產驗證的監控、擴展和維護指南

## 面向未來的視角

本章以前瞻性洞察作結，探討新興趨勢，包括：

- 效率比率更高的高級模型架構
- 與專用 AI 加速器的更深層硬體整合
- 生態系統向標準化和互操作性演進
- 由隱私和合規需求驅動的企業採用模式

這種全面的方法確保讀者能夠熟練應對當前 SLM 部署挑戰和未來技術發展，並做出符合其特定組織需求和限制的明智決策。

本章既是立即實施的實用指南，也是長期 AI 部署規劃的策略資源，強調成功 SLM 部署所需的能力、效率和操作卓越性之間的關鍵平衡。

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。