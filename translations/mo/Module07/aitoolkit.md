<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-17T19:00:03+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "mo"
}
-->
# AI Toolkit for Visual Studio Code - 邊緣 AI 開發指南

## 簡介

歡迎使用 AI Toolkit for Visual Studio Code 的完整指南，專為邊緣 AI 開發而設計。隨著人工智慧從集中式雲端計算轉向分散式邊緣設備，開發者需要強大的整合工具來應對邊緣部署的獨特挑戰，例如資源限制和離線操作需求。

AI Toolkit for Visual Studio Code 彌補了這一差距，提供了一個完整的開發環境，專門用於構建、測試和優化能在邊緣設備上高效運行的 AI 應用。不論您是為 IoT 感測器、移動設備、嵌入式系統或邊緣伺服器開發，這款工具包都能在熟悉的 VS Code 環境中簡化您的整個開發工作流程。

本指南將帶您了解如何在邊緣 AI 專案中使用 AI Toolkit 的基本概念、工具和最佳實踐，從初始模型選擇到生產部署。

## 概述

AI Toolkit 提供了一個整合的開發環境，涵蓋完整的邊緣 AI 應用生命周期，並與 VS Code 無縫整合。它支持來自 OpenAI、Anthropic、Google 和 GitHub 等提供者的流行 AI 模型，同時支持通過 ONNX 和 Ollama 進行本地模型部署——這對於需要設備端推理的邊緣 AI 應用至關重要。

AI Toolkit 在邊緣 AI 開發中的獨特之處在於其專注於整個邊緣部署管道。與主要針對雲端部署的傳統 AI 開發工具不同，AI Toolkit 包含專門的功能，用於模型優化、資源受限測試和邊緣特定性能評估。該工具包深知邊緣 AI 開發需要不同的考量——更小的模型尺寸、更快的推理速度、離線能力以及硬體特定的優化。

該平台支持多種部署場景，從簡單的設備端推理到複雜的多模型邊緣架構。它提供了模型轉換、量化和優化工具，這些工具對成功的邊緣部署至關重要，同時保持 VS Code 所知名的開發者生產力。

## 學習目標

完成本指南後，您將能夠：

### 核心能力
- **安裝和配置** AI Toolkit for Visual Studio Code，用於邊緣 AI 開發工作流程
- **導航和使用** AI Toolkit 界面，包括模型目錄、遊樂場和代理構建器
- **選擇和評估** 適合邊緣部署的 AI 模型，基於性能和資源限制
- **轉換和優化** 使用 ONNX 格式和量化技術的模型，以適應邊緣設備

### 邊緣 AI 開發技能
- **設計和實現** 使用整合開發環境的邊緣 AI 應用
- **在邊緣條件下進行模型測試**，使用本地推理和資源監控
- **創建和定制** 為邊緣部署場景優化的 AI 代理
- **使用與邊緣計算相關的指標** 評估模型性能（延遲、記憶體使用率、準確性）

### 優化和部署
- **應用量化和剪枝技術**，在保持可接受性能的同時減少模型尺寸
- **優化模型**，以適應特定的邊緣硬體平台，包括 CPU、GPU 和 NPU 加速
- **實施最佳實踐**，用於邊緣 AI 開發，包括資源管理和備援策略
- **準備模型和應用**，以便在邊緣設備上進行生產部署

### 高級邊緣 AI 概念
- **整合邊緣 AI 框架**，包括 ONNX Runtime、Windows ML 和 TensorFlow Lite
- **實現多模型架構** 和聯邦學習場景，用於邊緣環境
- **排除常見邊緣 AI 問題**，包括記憶體限制、推理速度和硬體兼容性
- **設計監控和日誌策略**，用於生產中的邊緣 AI 應用

### 實際應用
- **構建端到端邊緣 AI 解決方案**，從模型選擇到部署
- **展示熟練度**，在邊緣特定的開發工作流程和優化技術中
- **將所學概念應用於** 真實世界的邊緣 AI 用例，包括 IoT、移動和嵌入式應用
- **評估和比較** 不同的邊緣 AI 部署策略及其權衡

## 邊緣 AI 開發的主要功能

### 1. 模型目錄和探索
- **本地模型支持**：探索和訪問專為邊緣部署優化的 AI 模型
- **ONNX 整合**：訪問 ONNX 格式的模型，以進行高效的邊緣推理
- **Ollama 支持**：通過 Ollama 使用本地運行的模型，實現隱私和離線操作
- **模型比較**：並排比較模型，找到性能和資源消耗的最佳平衡，適合邊緣設備

### 2. 互動式遊樂場
- **本地測試環境**：在邊緣部署之前本地測試模型
- **多模態實驗**：使用圖像、文本和其他典型邊緣場景的輸入進行測試
- **參數調整**：嘗試不同的模型參數，以適應邊緣限制
- **即時性能監控**：在開發過程中觀察推理速度和資源使用情況

### 3. 用於邊緣應用的代理構建器
- **提示工程**：創建能高效運行於較小邊緣模型的優化提示
- **MCP 工具整合**：整合模型上下文協議工具，以增強邊緣代理功能
- **代碼生成**：生成適合邊緣部署場景的生產就緒代碼
- **結構化輸出**：設計提供一致、結構化響應的代理，適合邊緣應用

### 4. 模型評估和測試
- **性能指標**：使用與邊緣部署相關的指標（延遲、記憶體使用率、準確性）評估模型
- **批量測試**：同時測試多個模型配置，以找到最佳邊緣設置
- **自定義評估**：創建特定於邊緣 AI 用例的自定義評估標準
- **資源分析**：分析記憶體和計算需求，以進行邊緣部署規劃

### 5. 模型轉換和優化
- **ONNX 轉換**：將模型從多種格式轉換為 ONNX，以適應邊緣兼容性
- **量化**：通過量化技術減少模型尺寸並提高推理速度
- **硬體優化**：針對特定邊緣硬體（CPU、GPU、NPU）進行模型優化
- **格式轉換**：將 Hugging Face 和其他來源的模型轉換為邊緣部署格式

### 6. 邊緣場景的微調
- **領域適配**：根據特定邊緣用例和環境定制模型
- **本地訓練**：使用 GPU 支持進行本地訓練，以滿足邊緣特定需求
- **Azure 整合**：在邊緣部署之前利用 Azure 容器應用進行基於雲的微調
- **遷移學習**：為邊緣特定任務和限制改編預訓練模型

### 7. 性能監控和追蹤
- **邊緣性能分析**：在邊緣條件下監控模型性能
- **追蹤收集**：收集詳細的性能數據以進行優化
- **瓶頸識別**：在部署到邊緣設備之前識別性能問題
- **資源使用追蹤**：監控記憶體、CPU 和推理時間，以進行邊緣優化

## 邊緣 AI 開發工作流程

### 第一階段：模型探索和選擇
1. **探索模型目錄**：使用模型目錄尋找適合邊緣部署的模型
2. **比較性能**：根據尺寸、準確性和推理速度評估模型
3. **本地測試**：使用 Ollama 或 ONNX 模型進行本地測試
4. **評估資源需求**：確定目標邊緣設備的記憶體和計算需求

### 第二階段：模型優化
1. **轉換為 ONNX**：將選定模型轉換為 ONNX 格式，以適應邊緣兼容性
2. **應用量化**：通過 INT8 或 INT4 量化減少模型尺寸
3. **硬體優化**：針對目標邊緣硬體（ARM、x86、專用加速器）進行優化
4. **性能驗證**：驗證優化後的模型是否保持可接受的準確性

### 第三階段：應用開發
1. **代理設計**：使用代理構建器創建邊緣優化的 AI 代理
2. **提示工程**：開發能有效運行於較小模型的提示
3. **整合測試**：在模擬邊緣條件下測試代理
4. **代碼生成**：生成適合邊緣部署的生產代碼

### 第四階段：評估和測試
1. **批量評估**：測試多個配置以找到最佳邊緣設置
2. **性能分析**：分析推理速度、記憶體使用率和準確性
3. **邊緣模擬**：在類似目標邊緣部署環境的條件下進行測試
4. **壓力測試**：在各種負載條件下評估性能

### 第五階段：部署準備
1. **最終優化**：根據測試結果進行最終優化
2. **部署打包**：打包模型和代碼以進行邊緣部署
3. **文檔編寫**：記錄部署需求和配置
4. **監控設置**：為邊緣部署準備監控和日誌

## 邊緣 AI 開發的目標受眾

### 邊緣 AI 開發者
- 構建 AI 驅動的邊緣設備和 IoT 解決方案的應用開發者
- 將 AI 功能整合到資源受限設備中的嵌入式系統開發者
- 為智能手機和平板電腦創建設備端 AI 應用的移動開發者

### 邊緣 AI 工程師
- 優化模型以進行邊緣部署並管理推理管道的 AI 工程師
- 部署和管理分散式邊緣基礎設施中的 AI 模型的 DevOps 工程師
- 優化 AI 工作負載以適應邊緣硬體限制的性能工程師

### 研究人員和教育者
- 開發高效模型和算法以適應邊緣計算的 AI 研究人員
- 教授邊緣 AI 概念並演示優化技術的教育者
- 學習邊緣 AI 部署中的挑戰和解決方案的學生

## 邊緣 AI 用例

### 智能 IoT 設備
- **即時圖像識別**：在 IoT 攝像頭和感測器上部署計算機視覺模型
- **語音處理**：在智能音箱上實現語音識別和自然語言處理
- **預測性維護**：在工業邊緣設備上運行異常檢測模型
- **環境監測**：部署感測器數據分析模型，用於環境應用

### 移動和嵌入式應用
- **設備端翻譯**：實現能離線工作的語言翻譯模型
- **擴增實境**：部署即時物體識別和追蹤，用於 AR 應用
- **健康監測**：在穿戴設備和醫療設備上運行健康分析模型
- **自主系統**：為無人機、機器人和車輛實現決策模型

### 邊緣計算基礎設施
- **邊緣數據中心**：在邊緣數據中心部署 AI 模型，用於低延遲應用
- **CDN 整合**：將 AI 處理能力整合到內容分發網絡中
- **5G 邊緣**：利用 5G 邊緣計算進行 AI 驅動的應用
- **霧計算**：在霧計算環境中實現 AI 處理

## 安裝和設置

### 快速安裝
直接從 Visual Studio Code Marketplace 安裝 AI Toolkit 擴展：

```
Install: AI Toolkit for Visual Studio Code (ms-windows-ai-studio.windows-ai-studio)
```

### 邊緣 AI 開發的先決條件
- **ONNX Runtime**：安裝 ONNX Runtime 以進行模型推理
- **Ollama**（可選）：安裝 Ollama 以進行本地模型服務
- **Python 環境**：設置 Python 並安裝所需的 AI 庫
- **邊緣硬體工具**：安裝硬體特定的開發工具（CUDA、OpenVINO 等）

### 初始配置
1. 打開 VS Code 並安裝 AI Toolkit 擴展
2. 配置模型來源（ONNX、Ollama、雲端提供者）
3. 設置本地開發環境以進行邊緣測試
4. 配置開發機器的硬體加速選項

## 邊緣 AI 開發入門

### 第一步：模型選擇
1. 在活動欄中打開 AI Toolkit 視圖
2. 瀏覽模型目錄以尋找邊緣兼容模型
3. 按模型尺寸、格式（ONNX）和性能特徵篩選
4. 使用內建比較工具比較模型

### 第二步：本地測試
1. 使用遊樂場本地測試選定模型
2. 嘗試不同的提示和參數
3. 在測試期間監控性能指標
4. 評估模型響應是否符合邊緣用例需求

### 第三步：模型優化
1. 使用模型轉換工具進行邊緣部署優化
2. 應用量化以減少模型尺寸
3. 測試優化後的模型以確保性能可接受
4. 記錄優化設置和性能權衡

### 第四步：代理開發
1. 使用代理構建器創建邊緣優化的 AI 代理
2. 開發能有效運行於較小模型的提示
3. 整合必要的工具和 API，用於邊緣場景
4. 在模擬邊緣條件下測試代理

### 第五步：評估和部署
1. 使用批量評估測試多個配置
2. 在各種條件下分析性能
3. 為目標邊緣設備準備部署包
4. 設置生產部署的監控和日誌

## 邊緣 AI 開發的最佳實踐

### 模型選擇
- **尺寸限制**：選擇適合目標設備記憶體限制的模型
- **推理速度**：優先選擇推理速度快的模型，用於即時應用
- **準確性權衡**：在模型準確性和資源限制之間取得平衡
- **格式兼容性**：優先選擇 ONNX 或硬體優化格式，用於邊緣部署

### 優化技術
- **量化**：使用 INT8 或 INT4 量化減少模型尺寸並提高速度
- **剪枝**：移除不必要的模型參數以減少計算需求
- **知識蒸餾**：創建能保持大模型性能的小模型
- **硬體加速**：在可用時利用 NPU、GPU 或專用加速器

### 開發工作流程
- **迭代測試**：在開發過程中頻繁在
- **安全性**：為邊緣 AI 應用實施適當的安全措施

## 與邊緣 AI 框架的整合

### ONNX Runtime
- **跨平台部署**：在不同的邊緣平台上部署 ONNX 模型
- **硬體優化**：利用 ONNX Runtime 的硬體特定優化功能
- **行動支援**：使用 ONNX Runtime Mobile 於智慧型手機和平板應用程式
- **物聯網整合**：使用 ONNX Runtime 的輕量化版本部署於物聯網設備

### Windows ML
- **Windows 設備**：針對基於 Windows 的邊緣設備和 PC 進行優化
- **NPU 加速**：利用 Windows 設備上的神經處理單元進行加速
- **DirectML**：在 Windows 平台上使用 DirectML 進行 GPU 加速
- **UWP 整合**：與通用 Windows 平台應用程式整合

### TensorFlow Lite
- **行動優化**：在行動和嵌入式設備上部署 TensorFlow Lite 模型
- **硬體代理**：使用專門的硬體代理進行加速
- **微控制器**：使用 TensorFlow Lite Micro 部署於微控制器
- **跨平台支援**：在 Android、iOS 和嵌入式 Linux 系統上部署

### Azure IoT Edge
- **雲端與邊緣混合**：結合雲端訓練與邊緣推論
- **模組部署**：將 AI 模型部署為 IoT Edge 模組
- **設備管理**：遠端管理邊緣設備和模型更新
- **遙測**：收集邊緣部署的性能數據和模型指標

## 高級邊緣 AI 場景

### 多模型部署
- **模型集成**：部署多個模型以提高準確性或冗餘性
- **A/B 測試**：在邊緣設備上同時測試不同模型
- **動態選擇**：根據當前設備狀況選擇模型
- **資源共享**：在多個部署模型之間優化資源使用

### 聯邦學習
- **分散式訓練**：在多個邊緣設備上訓練模型
- **隱私保護**：保持訓練數據本地化，同時共享模型改進
- **協作學習**：使設備能從集體經驗中學習
- **邊緣與雲端協調**：協調邊緣設備與雲端基礎設施之間的學習

### 即時處理
- **流處理**：在邊緣設備上處理連續數據流
- **低延遲推論**：優化以實現最小的推論延遲
- **批量處理**：在邊緣設備上高效處理批量數據
- **自適應處理**：根據當前設備能力調整處理方式

## 邊緣 AI 開發故障排除

### 常見問題
- **記憶體限制**：模型過大，超出目標設備的記憶體容量
- **推論速度**：模型推論速度不足以滿足即時需求
- **準確性下降**：優化導致模型準確性無法接受的下降
- **硬體相容性**：模型與目標硬體不相容

### 除錯策略
- **性能分析**：使用 AI 工具包的追蹤功能識別瓶頸
- **資源監控**：在開發過程中監控記憶體和 CPU 使用情況
- **漸進測試**：逐步測試優化以隔離問題
- **硬體模擬**：使用開發工具模擬目標硬體

### 優化解決方案
- **進一步量化**：採用更激進的量化技術
- **模型架構**：考慮針對邊緣優化的不同模型架構
- **預處理優化**：針對邊緣限制優化數據預處理
- **推論優化**：使用硬體特定的推論優化技術

## 資源與下一步

### 文件
- [AI 工具包模型指南](https://code.visualstudio.com/docs/intelligentapps/models)
- [模型遊樂場文件](https://code.visualstudio.com/docs/intelligentapps/playground)
- [ONNX Runtime 文件](https://onnxruntime.ai/)
- [Windows ML 文件](https://docs.microsoft.com/en-us/windows/ai/)

### 社群與支援
- [VS Code AI 工具包 GitHub](https://github.com/microsoft/vscode-ai-toolkit)
- [ONNX 社群](https://github.com/onnx/onnx)
- [邊緣 AI 開發者社群](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [VS Code 擴展市場](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 學習資源
- [邊緣 AI 基礎課程](./Module01/README.md)
- [小型語言模型指南](./Module02/README.md)
- [邊緣部署策略](./Module03/README.md)
- [Windows 邊緣 AI 開發](./windowdeveloper.md)

## 結論

Visual Studio Code 的 AI 工具包提供了一個全面的平台，用於邊緣 AI 的開發，涵蓋了模型發現、優化到部署和監控的全過程。透過其整合的工具和工作流程，開發者能高效地創建、測試並部署能在資源受限的邊緣設備上有效運行的 AI 應用程式。

該工具包對 ONNX、Ollama 和各種雲端提供商的支援，加上其優化和評估能力，使其成為邊緣 AI 開發的理想選擇。無論您是在構建物聯網應用程式、行動 AI 功能，還是嵌入式智能系統，AI 工具包都提供了成功部署邊緣 AI 所需的工具和工作流程。

隨著邊緣 AI 的持續發展，VS Code 的 AI 工具包始終走在前沿，為開發者提供尖端工具和能力，助力打造下一代智能邊緣應用程式。

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。