<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T16:56:33+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "mo"
}
-->
# 第四節：前沿模型——LLM、SLM 和設備端推理

## 概述

比較 LLM 和 SLM，評估本地與雲端推理的取捨，並實現展示 EdgeAI 場景的示範，使用 Phi 和 ONNX Runtime。我們還將重點介紹 Chainlit RAG、WebGPU 推理選項以及 Open WebUI 的整合。

參考資料：
- Foundry Local 文件：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI：https://onnxruntime.ai/
- Open WebUI 使用指南（使用 Open WebUI 的聊天應用）：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## 學習目標
- 理解 LLM 與 SLM 在成本、延遲和準確性上的取捨
- 根據特定業務需求選擇本地或雲端推理
- 使用 Chainlit 實現一個簡單的 RAG 示範
- 探索 WebGPU 用於瀏覽器端加速
- 將 Open WebUI 連接到 Foundry Local

## 第一部分：LLM vs SLM——決策矩陣

考慮因素：
- 延遲：SLM 在設備端通常能提供毫秒級的響應
- 成本：本地推理降低雲端成本
- 隱私：敏感數據保留在設備端
- 能力：LLM 在處理複雜任務時可能優於 SLM
- 可靠性：混合策略降低停機風險

## 第二部分：本地 vs 雲端——混合模式

- 本地優先，雲端作為備援，用於大型或複雜的提示
- 雲端優先，本地用於隱私敏感或離線場景
- 根據任務類型進行路由（例如，代碼生成使用 DeepSeek，普通聊天使用 Phi/Qwen）

## 第三部分：使用 Chainlit 的 RAG 聊天應用（簡易版）

安裝依賴項：
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`：
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

運行：
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

擴展：添加一個簡單的檢索器（本地文件），並將檢索到的上下文添加到用戶提示之前。

## 第四部分：WebGPU 推理（注意事項）

使用 WebGPU 在瀏覽器中直接運行小型模型。這非常適合隱私優先的示範和零安裝體驗。以下是使用 ONNX Runtime Web 和 WebGPU 執行提供者的逐步示例。

1) 檢查 WebGPU 支援
- Chromium 瀏覽器：chrome://gpu → 確認“WebGPU”已啟用
- 程式檢查（我們也會在代碼中檢查）：`if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) 創建一個簡易項目
創建一個文件夾和兩個文件：`index.html` 和 `main.js`。

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) 本地服務（Windows cmd.exe）
使用簡單的靜態服務器，讓瀏覽器能夠獲取模型。

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

打開瀏覽器中的 http://localhost:5173。你應該能看到初始化日誌、使用 WebGPU 創建會話以及 argmax 預測。

4) 故障排除
- 如果 WebGPU 不可用：更新 Chrome/Edge 並確保 GPU 驅動程式是最新的，然後檢查 chrome://flags 中的“啟用 WebGPU”。
- 如果出現 CORS 或 fetch 錯誤：確保文件通過 http://（而不是 file://）提供服務，並且模型 URL 允許跨域請求。
- 回退到 CPU：將 `executionProviders: ['wasm']` 更改以驗證基線行為。

5) 下一步
- 替換為領域特定的 ONNX 模型（例如，圖像分類或小型文本模型）。
- 添加預處理/後處理邏輯以處理真實輸入。
- 對於更大的模型或生產延遲，優先使用 Foundry Local 或 ONNX Runtime Server。

## 第五部分：Open WebUI + Foundry Local（逐步指南）

這將 Open WebUI 連接到 Foundry Local 的 OpenAI 兼容端點，用於本地聊天界面。

1) 前置條件
- 已安裝並運行 Foundry Local（`foundry --version`）
- 一個準備好本地運行的模型（例如，`phi-4-mini`）
- 已安裝 Docker Desktop（推薦用於 Open WebUI）

2) 使用 Foundry Local 啟動模型
```powershell
foundry model run phi-4-mini
```
這會在 `http://localhost:8000` 暴露一個 OpenAI 兼容的 API。

3) 啟動 Open WebUI（Docker）
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
注意：
- 在 Windows 上，`host.docker.internal` 允許容器訪問主機的 `localhost`。
- 我們將 `OPENAI_API_BASE_URL` 設置為 Foundry Local 的端點，並使用一個虛擬的 `OPENAI_API_KEY`。

4) 從 Open WebUI 的界面進行配置（替代方法）
- 瀏覽到 http://localhost:3000
- 完成初始設置（管理員用戶）
- 前往設置 → 模型/提供者
- 設置基礎 URL：`http://host.docker.internal:8000/v1`
- 設置 API 密鑰：`local-key`（佔位符）
- 保存

5) 運行測試提示
- 在 Open WebUI 聊天中，選擇或輸入模型名稱 `phi-4-mini`
- 提示：“列出設備端 AI 推理的五個優勢。”
- 你應該能看到來自本地模型的流式響應

6) 故障排除
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) 可選：持久化 Open WebUI 數據
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```


## 實踐清單
- [ ] 比較 SLM 和 LLM 在本地的響應/延遲
- [ ] 使用至少兩個模型運行 Chainlit 示範
- [ ] 將 Open WebUI 連接到本地端點並進行測試

## 下一步
- 為第五節的代理工作流程做好準備
- 確定混合本地/雲端能提高 ROI 的場景

---

