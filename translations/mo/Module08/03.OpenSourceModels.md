<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T14:25:41+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "mo"
}
-->
# 第三節：開源模型的探索與管理

## 概述

本節課程專注於使用 Foundry Local 進行實際的模型探索與管理。您將學習如何列出可用模型、測試不同選項，以及了解基本性能特徵。課程重點在於使用 Foundry CLI 進行實際操作，幫助您選擇適合自己使用案例的模型。

## 學習目標

- 掌握使用 Foundry CLI 進行模型探索與管理的指令
- 理解模型緩存與本地存儲模式
- 學習快速測試與比較不同模型
- 建立模型選擇與基準測試的實用工作流程
- 探索 Foundry Local 提供的日益增長的模型生態系統

## 先決條件

- 完成第一節：Foundry Local 入門
- 已安裝並可使用 Foundry Local CLI
- 擁有足夠的存儲空間以下載模型（模型大小範圍從 1GB 到 20GB+）
- 基本了解模型類型與使用案例

## 概述

本節課程探討如何將開源模型引入 Foundry Local，選擇社群模型、整合 Hugging Face 的內容，以及採用“自帶模型”（BYOM）策略。您還將了解 Model Mondays 系列，持續學習並探索模型。

## 第六部分：實作練習

### 練習：模型探索與比較

基於範例 03，創建自己的模型評估腳本：

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### 您的任務

1. **執行範例 03 腳本**：`samples\03\list_and_bench.cmd`
2. **嘗試不同模型**：至少測試 3 種不同模型
3. **比較性能**：記錄速度與回應品質的差異
4. **記錄結果**：創建簡單的比較圖表

### 比較格式範例

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## 第七部分：故障排除與最佳實踐

### 常見問題與解決方案

**模型無法啟動：**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**內存不足：**
- 從較小的模型開始（如 `phi-4-mini`）
- 關閉其他應用程式
- 如果經常遇到限制，考慮升級 RAM

**性能緩慢：**
- 確保模型已完全加載（檢查詳細輸出）
- 關閉不必要的背景應用程式
- 考慮使用更快的存儲設備（如 SSD）

### 最佳實踐

1. **從小開始**：使用 `phi-4-mini` 驗證設置
2. **一次僅運行一個模型**：啟動新模型前停止之前的模型
3. **監控資源**：密切關注內存使用情況
4. **一致測試**：使用相同的提示進行公平比較
5. **記錄結果**：記錄模型性能以便於使用案例分析

## 第八部分：後續步驟與參考資料

### 為第四節課程做準備

- **第四節課程重點**：優化工具與技術
- **先決條件**：熟悉模型切換與基本性能測試
- **建議**：從本節課程中選出 2-3 個最喜歡的模型

### 附加資源

- **[Foundry Local 文件](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**：官方文件
- **[CLI 參考](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**：完整指令參考
- **[Model Mondays](https://aka.ms/model-mondays)**：每週模型亮點
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**：社群與問題追蹤
- **[範例 03：模型探索](samples/03/README.md)**：實作範例腳本

### 關鍵要點

✅ **模型探索**：使用 `foundry model list` 探索可用模型  
✅ **快速測試**：使用 `list_and_bench.cmd` 模式進行快速評估  
✅ **性能監控**：基本資源使用與回應時間測量  
✅ **模型選擇**：根據使用案例選擇模型的實用指南  
✅ **緩存管理**：理解存儲與清理程序  

您現在擁有使用 Foundry Local 的 CLI 方法探索、測試與選擇適合 AI 應用的模型的實用技能。

## 學習目標

- 探索並評估本地推理的開源模型
- 編譯並運行選定的 Hugging Face 模型於 Foundry Local
- 根據準確性、延遲與資源需求應用模型選擇策略
- 使用緩存與版本管理本地模型

## 第一部分：使用 Foundry CLI 進行模型探索

### 基本模型管理指令

Foundry CLI 提供了簡單的指令進行模型探索與管理：

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### 運行您的第一個模型

從流行且經過測試的模型開始，了解性能特徵：

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**注意：** `--verbose` 標誌提供詳細的啟動信息，包括：
- 模型下載進度（首次運行時）
- 內存分配細節
- 服務綁定信息
- 性能初始化指標

### 理解模型類別

**小型語言模型（SLMs）：**
- `phi-4-mini`：快速、高效，適合一般聊天
- `phi-4`：更強大的版本，具備更好的推理能力

**中型模型：**
- `qwen2.5-7b-instruct`：出色的推理能力與更長的上下文
- `deepseek-r1-distill-qwen-7b`：針對代碼生成進行優化

**大型模型：**
- `llama-3.2`：Meta 最新的開源模型
- `qwen2.5-14b-instruct`：企業級推理能力

## 第二部分：快速模型測試與比較

### 範例 03 方法：簡單的列出與基準測試

基於範例 03 模式，以下是最小化的工作流程：

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### 測試模型性能

模型運行後，使用一致的提示進行測試：

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell 測試替代方案

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## 第三部分：模型緩存與存儲管理

### 理解模型緩存

Foundry Local 自動管理模型下載與緩存：

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### 模型存儲考量

**典型模型大小：**
- `phi-4-mini`：約 2.5 GB
- `qwen2.5-7b-instruct`：約 4.1 GB  
- `deepseek-r1-distill-qwen-7b`：約 4.3 GB
- `llama-3.2`：約 4.9 GB
- `qwen2.5-14b-instruct`：約 8.2 GB

**存儲最佳實踐：**
- 保留 2-3 個模型緩存以便快速切換
- 移除未使用的模型以釋放空間：`foundry cache clean`
- 監控磁碟使用情況，尤其是較小的 SSD
- 考慮模型大小與能力的權衡

### 模型性能監控

模型運行時，監控系統資源：

**Windows 任務管理器：**
- 觀察內存使用情況（模型保持加載於 RAM）
- 監控推理過程中的 CPU 使用率
- 檢查初始模型加載期間的磁碟 I/O

**命令行監控：**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## 第四部分：實用模型選擇指南

### 根據使用案例選擇模型

**一般聊天與問答：**
- 起步：`phi-4-mini`（快速、高效）
- 升級：`phi-4`（更好的推理能力）
- 高級：`qwen2.5-7b-instruct`（更長的上下文）

**代碼生成：**
- 推薦：`deepseek-r1-distill-qwen-7b`
- 替代：`qwen2.5-7b-instruct`（同樣適合代碼）

**複雜推理：**
- 最佳：`qwen2.5-7b-instruct` 或 `qwen2.5-14b-instruct`
- 經濟選擇：`phi-4`

### 硬體需求指南

**最低系統需求：**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**最佳性能推薦：**
- 32GB+ RAM，方便多模型切換
- SSD 存儲，提升模型加載速度
- 現代 CPU，具備良好的單線程性能
- 支援 NPU（Windows 11 Copilot+ PC）加速

### 模型切換工作流程

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## 第五部分：簡單模型基準測試

### 基本性能測試

以下是比較模型性能的簡單方法：

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### 手動品質評估

對每個模型進行一致的提示測試並手動評估：

**測試提示：**
1. "用簡單的方式解釋量子計算。"
2. "寫一個 Python 函數來排序列表。"
3. "遠程工作的優缺點是什麼？"
4. "總結邊緣 AI 的好處。"

**評估標準：**
- **準確性**：信息是否正確？
- **清晰度**：解釋是否容易理解？
- **完整性**：是否全面回答問題？
- **速度**：回應速度如何？

### 資源使用監控

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## 第六部分：後續步驟

- 訂閱 Model Mondays，獲取新模型與技巧：https://aka.ms/model-mondays
- 將發現結果貢獻到您的團隊 `models.json`
- 為第四節課程做準備：比較 LLMs 與 SLMs、本地與雲端推理，以及實作演示

---

