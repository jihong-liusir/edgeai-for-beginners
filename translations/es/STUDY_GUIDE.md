<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T12:40:24+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "es"
}
-->
# EdgeAI para Principiantes: Rutas de Aprendizaje y Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 2 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 3 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 4-5 | Módulo 4: Optimización de Modelos (6 frameworks) | 4 horas |
| Día 6 | Módulo 5: SLMOps | 3 horas |
| Día 7 | Módulo 6-7: Agentes de IA y Herramientas de Desarrollo | 5 horas |

### Ruta de Aprendizaje Concentrada (2 semanas)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Agentes de IA | 2 horas |
| Día 13-14 | Módulo 7: Herramientas de Desarrollo | 3 horas |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Sistemas Agentes de SLM | 2 horas |
| Día 13-14 | Módulo 7: Ejemplos de Implementación de EdgeAI | 2 horas |

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave |
|--------|----------------|-------------|--------------|
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos (6 frameworks) | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Sistemas Agentes de SLM | | | |
| Módulo 7: Ejemplos de Implementación de EdgeAI | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

## Introducción

¡Bienvenido a la guía de estudio de EdgeAI para Principiantes! Este documento está diseñado para ayudarte a navegar por los materiales del curso de manera efectiva y maximizar tu experiencia de aprendizaje. Proporciona rutas de aprendizaje estructuradas, horarios de estudio sugeridos, resúmenes de conceptos clave y recursos complementarios para profundizar en tu comprensión de las tecnologías EdgeAI.

Este es un curso conciso de 20 horas que ofrece conocimientos esenciales sobre EdgeAI en un formato eficiente en tiempo, ideal para profesionales ocupados y estudiantes que desean adquirir rápidamente habilidades prácticas en este campo emergente.

## Resumen del Curso

Este curso está organizado en siete módulos completos:

1. **Fundamentos y Transformación de EdgeAI** - Comprender los conceptos básicos y el cambio tecnológico
2. **Fundamentos de Modelos de Lenguaje Pequeños (SLM)** - Exploración de diversas familias de SLM y sus arquitecturas
3. **Despliegue de Modelos de Lenguaje Pequeños (SLM)** - Implementación de estrategias prácticas de despliegue
4. **Conversión de Formatos de Modelos y Cuantización** - Optimización avanzada con 6 frameworks, incluyendo OpenVINO
5. **SLMOps - Operaciones de Modelos de Lenguaje Pequeños** - Gestión del ciclo de vida de producción y despliegue
6. **Sistemas Agentes de SLM** - Agentes de IA, llamadas de funciones y Protocolo de Contexto de Modelos
7. **Ejemplos de Implementación de EdgeAI** - Herramientas de IA, desarrollo en Windows e implementaciones específicas de plataformas
8. **Microsoft Foundry Local – Kit de Herramientas Completo para Desarrolladores** - Desarrollo local con integración híbrida de Azure (Módulo 08)

## Cómo Usar Esta Guía de Estudio

- **Aprendizaje Progresivo**: Sigue los módulos en orden para una experiencia de aprendizaje más coherente.
- **Puntos de Control de Conocimiento**: Utiliza las preguntas de autoevaluación después de cada sección.
- **Práctica Práctica**: Completa los ejercicios sugeridos para reforzar los conceptos teóricos.
- **Recursos Complementarios**: Explora materiales adicionales sobre los temas que más te interesen.

## Recomendaciones de Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|-----------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 6 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 8 horas |
| Día 5 | Módulo 3: Despliegue de SLM | 3 horas |
| Día 6 | Módulo 8: Kit de Herramientas Foundry Local | 3 horas |

### Estudio a Tiempo Parcial (3 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|-----------------|
| Semana 1 | Módulo 1: Fundamentos de EdgeAI | 6-7 horas |
| Semana 2 | Módulo 2: Fundamentos de SLM | 7-8 horas |
| Semana 3 | Módulo 3: Despliegue de SLM (3h) + Módulo 8: Kit de Herramientas Foundry Local (2-3h) | 5-6 horas |

## Módulo 1: Fundamentos y Transformación de EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender las diferencias entre la IA basada en la nube y la IA basada en el borde.
- Dominar técnicas de optimización clave para entornos con recursos limitados.
- Analizar aplicaciones reales de tecnologías EdgeAI.
- Configurar un entorno de desarrollo para proyectos de EdgeAI.

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de EdgeAI
- **Conceptos Prioritarios**: 
  - Paradigmas de computación en el borde vs. en la nube.
  - Técnicas de cuantización de modelos.
  - Opciones de aceleración de hardware (NPUs, GPUs, CPUs).
  - Ventajas de privacidad y seguridad.

- **Materiales Complementarios**:
  - [Documentación de TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub de ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentación de Edge Impulse](https://docs.edgeimpulse.com)

#### Sección 2: Estudios de Caso Reales
- **Conceptos Prioritarios**: 
  - Ecosistema de modelos Phi & Mu de Microsoft.
  - Implementaciones prácticas en diversas industrias.
  - Consideraciones de despliegue.

#### Sección 3: Guía de Implementación Práctica
- **Conceptos Prioritarios**: 
  - Configuración del entorno de desarrollo.
  - Herramientas de cuantización y optimización.
  - Métodos de evaluación para implementaciones de EdgeAI.

#### Sección 4: Hardware de Despliegue en el Borde
- **Conceptos Prioritarios**: 
  - Comparaciones de plataformas de hardware.
  - Estrategias de optimización para hardware específico.
  - Consideraciones de despliegue.

### Preguntas de Autoevaluación

1. Compara y contrasta la implementación de IA basada en la nube con la basada en el borde.
2. Explica tres técnicas clave para optimizar modelos para despliegue en el borde.
3. ¿Cuáles son las principales ventajas de ejecutar modelos de IA en el borde?
4. Describe el proceso de cuantización de un modelo y cómo afecta al rendimiento.
5. Explica cómo los diferentes aceleradores de hardware (NPUs, GPUs, CPUs) influyen en el despliegue de EdgeAI.

### Ejercicios Prácticos

1. **Configuración Rápida del Entorno**: Configura un entorno de desarrollo mínimo con los paquetes esenciales (30 minutos).
2. **Exploración de Modelos**: Descarga y examina un modelo de lenguaje pequeño preentrenado (1 hora).
3. **Cuantización Básica**: Prueba una cuantización simple en un modelo pequeño (1 hora).

## Módulo 2: Fundamentos de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Comprender los principios arquitectónicos de diferentes familias de SLM.
- Comparar las capacidades de los modelos en diferentes escalas de parámetros.
- Evaluar modelos según eficiencia, capacidad y requisitos de despliegue.
- Reconocer casos de uso apropiados para diferentes familias de modelos.

### Áreas de Enfoque de Estudio

#### Sección 1: Familia de Modelos Phi de Microsoft
- **Conceptos Prioritarios**: 
  - Evolución de la filosofía de diseño.
  - Arquitectura orientada a la eficiencia.
  - Capacidades especializadas.

#### Sección 2: Familia Qwen
- **Conceptos Prioritarios**: 
  - Contribuciones de código abierto.
  - Opciones de despliegue escalables.
  - Arquitectura avanzada de razonamiento.

#### Sección 3: Familia Gemma
- **Conceptos Prioritarios**: 
  - Innovación impulsada por la investigación.
  - Capacidades multimodales.
  - Optimización para dispositivos móviles.

#### Sección 4: Familia BitNET
- **Conceptos Prioritarios**: 
  - Tecnología de cuantización de 1 bit.
  - Marco de optimización de inferencia.
  - Consideraciones de sostenibilidad.

#### Sección 5: Modelo Mu de Microsoft
- **Conceptos Prioritarios**: 
  - Arquitectura orientada a dispositivos.
  - Integración del sistema con Windows.
  - Operación con preservación de privacidad.

#### Sección 6: Phi-Silica
- **Conceptos Prioritarios**: 
  - Arquitectura optimizada para NPU.
  - Métricas de rendimiento.
  - Integración para desarrolladores.

### Preguntas de Autoevaluación

1. Compara los enfoques arquitectónicos de las familias de modelos Phi y Qwen.
2. Explica cómo la tecnología de cuantización de BitNET difiere de la cuantización tradicional.
3. ¿Cuáles son las ventajas únicas del modelo Mu para la integración con Windows?
4. Describe cómo Phi-Silica aprovecha el hardware NPU para optimizar el rendimiento.
5. Para una aplicación móvil con conectividad limitada, ¿qué familia de modelos sería más adecuada y por qué?

### Ejercicios Prácticos

1. **Comparación de Modelos**: Benchmark rápido de dos modelos SLM diferentes (1 hora).
2. **Generación de Texto Simple**: Implementación básica de generación de texto con un modelo pequeño (1 hora).
3. **Optimización Rápida**: Aplica una técnica de optimización para mejorar la velocidad de inferencia (1 hora).

## Módulo 3: Despliegue de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Seleccionar modelos apropiados según las restricciones de despliegue.
- Dominar técnicas de optimización para diversos escenarios de despliegue.
- Implementar SLMs en entornos locales y en la nube.
- Diseñar configuraciones listas para producción en aplicaciones EdgeAI.

### Áreas de Enfoque de Estudio

#### Sección 1: Aprendizaje Avanzado de SLM
- **Conceptos Prioritarios**: 
  - Marco de clasificación de parámetros.
  - Técnicas avanzadas de optimización.
  - Estrategias de adquisición de modelos.

#### Sección 2: Despliegue en Entornos Locales
- **Conceptos Prioritarios**: 
  - Despliegue en la plataforma Ollama.
  - Soluciones locales de Microsoft Foundry.
  - Análisis comparativo de frameworks.

#### Sección 3: Despliegue en la Nube con Contenedores
- **Conceptos Prioritarios**: 
  - Inferencia de alto rendimiento con vLLM.
  - Orquestación de contenedores.
  - Implementación con ONNX Runtime.

### Preguntas de Autoevaluación

1. ¿Qué factores deben considerarse al elegir entre despliegue local y despliegue en la nube?
2. Compara Ollama y Microsoft Foundry Local como opciones de despliegue.
3. Explica los beneficios de la contenedorización para el despliegue de SLM.
4. ¿Cuáles son las métricas clave de rendimiento que se deben monitorear para un SLM desplegado en el borde?
5. Describe un flujo completo de despliegue desde la selección del modelo hasta la implementación en producción.

### Ejercicios Prácticos

1. **Despliegue Local Básico**: Despliega un SLM simple usando Ollama (1 hora).
2. **Verificación de Rendimiento**: Ejecuta un benchmark rápido en tu modelo desplegado (30 minutos).
3. **Integración Simple**: Crea una aplicación mínima que utilice tu modelo desplegado (1 hora).

## Módulo 4: Conversión de Formatos de Modelos y Cuantización

### Objetivos Clave de Aprendizaje

- Dominar técnicas avanzadas de cuantización desde precisión de 1 bit hasta 8 bits.
- Comprender estrategias de conversión de formatos (GGUF, ONNX).
- Implementar optimización en seis frameworks (Llama.cpp, Olive, OpenVINO, MLX, síntesis de flujos de trabajo).
- Desplegar modelos optimizados para entornos de producción en el borde en hardware Intel, Apple y multiplataforma.

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de Cuantización
- **Conceptos Prioritarios**: 
  - Marco de clasificación de precisión.
  - Equilibrio entre rendimiento y precisión.
  - Optimización de huella de memoria.

#### Sección 2: Implementación con Llama.cpp
- **Conceptos Prioritarios**: 
  - Despliegue multiplataforma.
  - Optimización de formato GGUF.
  - Técnicas de aceleración de hardware.

#### Sección 3: Suite Microsoft Olive
- **Conceptos Prioritarios**: 
  - Optimización consciente del hardware.
  - Despliegue a nivel empresarial.
  - Flujos de trabajo de optimización automatizados.

#### Sección 4: Toolkit OpenVINO
- **Conceptos Prioritarios**: 
  - Optimización para hardware Intel.
  - Marco de Compresión de Redes Neuronales (NNCF).
  - Despliegue de inferencia multiplataforma.
  - OpenVINO GenAI para despliegue de LLM.

#### Sección 5: Framework Apple MLX
- **Conceptos Prioritarios**: 
  - Optimización para Apple Silicon
  - Arquitectura de memoria unificada
  - Capacidades de ajuste fino con LoRA

#### Sección 6: Síntesis del Flujo de Trabajo para el Desarrollo de Edge AI
- **Conceptos Prioritarios**: 
  - Arquitectura de flujo de trabajo unificada
  - Árboles de decisión para selección de frameworks
  - Validación de preparación para producción
  - Estrategias para garantizar la sostenibilidad futura

### Preguntas de Autoevaluación

1. Compara las estrategias de cuantización en diferentes niveles de precisión (de 1 bit a 8 bits).
2. Explica las ventajas del formato GGUF para el despliegue en el edge.
3. ¿Cómo mejora la eficiencia de despliegue la optimización consciente del hardware en Microsoft Olive?
4. ¿Cuáles son los beneficios clave de NNCF de OpenVINO para la compresión de modelos?
5. Describe cómo Apple MLX aprovecha la arquitectura de memoria unificada para la optimización.
6. ¿Cómo ayuda la síntesis del flujo de trabajo en la selección de frameworks de optimización óptimos?

### Ejercicios Prácticos

1. **Cuantización de Modelos**: Aplica diferentes niveles de cuantización a un modelo y compara los resultados (1 hora).
2. **Optimización con OpenVINO**: Utiliza NNCF para comprimir un modelo para hardware de Intel (1 hora).
3. **Comparación de Frameworks**: Prueba el mismo modelo en tres frameworks de optimización diferentes (1 hora).
4. **Evaluación de Rendimiento**: Mide el impacto de la optimización en la velocidad de inferencia y el uso de memoria (1 hora).

## Módulo 5: SLMOps - Operaciones con Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Comprender los principios de gestión del ciclo de vida de SLMOps
- Dominar técnicas de destilación y ajuste fino para despliegues en el edge
- Implementar estrategias de despliegue en producción con monitoreo
- Construir flujos de trabajo empresariales para operaciones y mantenimiento de SLM

### Áreas de Enfoque de Estudio

#### Sección 1: Introducción a SLMOps
- **Conceptos Prioritarios**: 
  - Cambio de paradigma de SLMOps en operaciones de IA
  - Arquitectura eficiente en costos y centrada en la privacidad
  - Impacto estratégico en los negocios y ventajas competitivas

#### Sección 2: Destilación de Modelos
- **Conceptos Prioritarios**: 
  - Técnicas de transferencia de conocimiento
  - Implementación del proceso de destilación en dos etapas
  - Flujos de trabajo de destilación en Azure ML

#### Sección 3: Estrategias de Ajuste Fino
- **Conceptos Prioritarios**: 
  - Ajuste fino eficiente en parámetros (PEFT)
  - Métodos avanzados como LoRA y QLoRA
  - Entrenamiento multi-adaptador y optimización de hiperparámetros

#### Sección 4: Despliegue en Producción
- **Conceptos Prioritarios**: 
  - Conversión y cuantización de modelos para producción
  - Configuración de despliegue con Foundry Local
  - Evaluación de rendimiento y validación de calidad

### Preguntas de Autoevaluación

1. ¿En qué se diferencia SLMOps de MLOps tradicional?
2. Explica los beneficios de la destilación de modelos para despliegues en el edge.
3. ¿Cuáles son las consideraciones clave para ajustar modelos de lenguaje pequeños en entornos con recursos limitados?
4. Describe un flujo de trabajo completo de despliegue en producción para aplicaciones de IA en el edge.

### Ejercicios Prácticos

1. **Destilación Básica**: Crea un modelo más pequeño a partir de un modelo maestro más grande (1 hora).
2. **Experimento de Ajuste Fino**: Ajusta un modelo para un dominio específico (1 hora).
3. **Pipeline de Despliegue**: Configura un pipeline básico de CI/CD para el despliegue de modelos (1 hora).

## Módulo 6: Sistemas Agénticos con SLM - Agentes de IA y Llamadas a Funciones

### Objetivos Clave de Aprendizaje

- Construir agentes de IA inteligentes para entornos edge utilizando Modelos de Lenguaje Pequeños
- Implementar capacidades de llamadas a funciones con flujos de trabajo sistemáticos
- Dominar la integración del Protocolo de Contexto del Modelo (MCP) para interacción estandarizada con herramientas
- Crear sistemas agénticos sofisticados con mínima intervención humana

### Áreas de Enfoque de Estudio

#### Sección 1: Agentes de IA y Fundamentos de SLM
- **Conceptos Prioritarios**: 
  - Marco de clasificación de agentes (reflejo, basado en modelos, basado en objetivos, agentes de aprendizaje)
  - Análisis de compensaciones entre SLM y LLM
  - Patrones de diseño de agentes específicos para el edge
  - Optimización de recursos para agentes

#### Sección 2: Llamadas a Funciones en Modelos de Lenguaje Pequeños
- **Conceptos Prioritarios**: 
  - Implementación de flujos de trabajo sistemáticos (detección de intención, salida en JSON, ejecución externa)
  - Implementaciones específicas de plataformas (Phi-4-mini, modelos Qwen seleccionados, Microsoft Foundry Local)
  - Ejemplos avanzados (colaboración multi-agente, selección dinámica de herramientas)
  - Consideraciones para producción (limitación de tasas, registro de auditorías, medidas de seguridad)

#### Sección 3: Integración del Protocolo de Contexto del Modelo (MCP)
- **Conceptos Prioritarios**: 
  - Arquitectura del protocolo y diseño de sistemas en capas
  - Soporte multi-backend (Ollama para desarrollo, vLLM para producción)
  - Protocolos de conexión (modos STDIO y SSE)
  - Aplicaciones en el mundo real (automatización web, procesamiento de datos, integración de APIs)

### Preguntas de Autoevaluación

1. ¿Cuáles son las consideraciones arquitectónicas clave para agentes de IA en el edge?
2. ¿Cómo mejoran las llamadas a funciones las capacidades de los agentes?
3. Explica el papel del Protocolo de Contexto del Modelo en la comunicación de agentes.

### Ejercicios Prácticos

1. **Agente Simple**: Construye un agente de IA básico con llamadas a funciones (1 hora).
2. **Integración de MCP**: Implementa MCP en una aplicación de agente (30 minutos).

## Módulo 7: Ejemplos de Implementación de EdgeAI

### Objetivos Clave de Aprendizaje

- Dominar el AI Toolkit para Visual Studio Code para flujos de trabajo completos de desarrollo de EdgeAI
- Adquirir experiencia en la plataforma Windows AI Foundry y estrategias de optimización para NPU
- Implementar EdgeAI en múltiples plataformas de hardware y escenarios de despliegue
- Construir aplicaciones de EdgeAI listas para producción con optimizaciones específicas de la plataforma

### Áreas de Enfoque de Estudio

#### Sección 1: AI Toolkit para Visual Studio Code
- **Conceptos Prioritarios**: 
  - Entorno de desarrollo completo de Edge AI dentro de VS Code
  - Catálogo de modelos y descubrimiento para despliegue en el edge
  - Flujos de trabajo de prueba local, optimización y desarrollo de agentes
  - Monitoreo y evaluación de rendimiento para escenarios en el edge

#### Sección 2: Guía de Desarrollo de Windows EdgeAI
- **Conceptos Prioritarios**: 
  - Descripción general de la plataforma Windows AI Foundry
  - API Phi Silica para inferencia eficiente en NPU
  - APIs de Visión por Computadora para procesamiento de imágenes y OCR
  - CLI de Foundry Local para desarrollo y pruebas locales

#### Sección 3: Implementaciones Específicas de Plataformas
- **Conceptos Prioritarios**: 
  - Despliegue en NVIDIA Jetson Orin Nano (67 TOPS de rendimiento en IA)
  - Aplicaciones móviles con .NET MAUI y ONNX Runtime GenAI
  - Soluciones Azure EdgeAI con arquitectura híbrida nube-edge
  - Optimización de Windows ML con soporte universal de hardware
  - Aplicaciones Foundry Local con implementación RAG centrada en la privacidad

### Preguntas de Autoevaluación

1. ¿Cómo simplifica el AI Toolkit el flujo de trabajo de desarrollo de EdgeAI?
2. Compara las estrategias de despliegue en diferentes plataformas de hardware.
3. ¿Cuáles son las ventajas de Windows AI Foundry para el desarrollo en el edge?
4. Explica el papel de la optimización de NPU en las aplicaciones modernas de EdgeAI.
5. ¿Cómo aprovecha la API Phi Silica el hardware NPU para la optimización del rendimiento?
6. Compara los beneficios del despliegue local frente al despliegue en la nube para aplicaciones sensibles a la privacidad.

### Ejercicios Prácticos

1. **Configuración del AI Toolkit**: Configura el AI Toolkit y optimiza un modelo (1 hora).
2. **Windows AI Foundry**: Construye una aplicación simple de Windows AI utilizando la API Phi Silica (1 hora).
3. **Despliegue Multiplataforma**: Despliega el mismo modelo en dos plataformas diferentes (1 hora).
4. **Optimización de NPU**: Prueba el rendimiento de NPU con herramientas de Windows AI Foundry (30 minutos).

## Módulo 8: Microsoft Foundry Local – Kit de Herramientas Completo para Desarrolladores

### Objetivos Clave de Aprendizaje

- Instalar y configurar Foundry Local en Windows
- Ejecutar, descubrir y gestionar modelos localmente a través del CLI de Foundry
- Integrar con clientes REST y SDK compatibles con OpenAI
- Construir ejemplos prácticos: chat con Chainlit, agentes y enrutador de modelos
- Comprender patrones híbridos con Azure AI Foundry

### Áreas de Enfoque de Estudio

- Instalación y fundamentos del CLI (modelo, servicio, caché)
- Integración de SDK (clientes compatibles con OpenAI y Azure OpenAI)
- Validación rápida con Open WebUI
- Patrones de agentes y llamadas a funciones
- Modelos como herramientas (diseño de enrutador y registro)

### Preguntas de Autoevaluación

1. ¿Cómo descubres el endpoint local y listas los modelos disponibles?
2. ¿Cuáles son las diferencias entre el uso de REST de Foundry Local y Azure OpenAI?
3. ¿Cómo diseñarías un enrutador simple para seleccionar modelos como herramientas?
4. ¿Qué categorías del CLI son más relevantes para el desarrollo diario?
5. ¿Cómo validas la preparación de Foundry Local antes de ejecutar aplicaciones?

### Ejercicios Prácticos

1. Instala/actualiza Foundry Local y ejecuta `phi-4-mini` localmente (30 minutos).
2. Llama a `/v1/models` y ejecuta un chat simple a través de REST (30 minutos).
3. Lanza el ejemplo de la aplicación Chainlit y chatea localmente (30 minutos).
4. Ejecuta el coordinador multi-agente e inspecciona los resultados (30 minutos).
5. Prueba el enrutador de modelos como herramientas con sobrescrituras basadas en el entorno (30 minutos).

## Guía de Asignación de Tiempo

Para ayudarte a aprovechar al máximo las 20 horas del curso, aquí tienes una sugerencia de cómo distribuir tu tiempo:

| Actividad | Asignación de Tiempo | Descripción |
|-----------|----------------------|-------------|
| Lectura de Materiales Básicos | 9 horas | Enfocándote en los conceptos esenciales de cada módulo |
| Ejercicios Prácticos | 6 horas | Implementación práctica de técnicas clave |
| Autoevaluación | 2 horas | Evaluar tu comprensión a través de preguntas y reflexión |
| Mini-Proyecto | 3 horas | Aplicar el conocimiento en una implementación práctica pequeña |

### Áreas Clave de Enfoque según la Disponibilidad de Tiempo

**Si solo tienes 10 horas:**
- Completa los Módulos 1, 2 y 3 (conceptos básicos de EdgeAI).
- Realiza al menos un ejercicio práctico por módulo.
- Enfócate en comprender los conceptos clave en lugar de los detalles de implementación.

**Si puedes dedicar las 20 horas completas:**
- Completa los siete módulos.
- Realiza ejercicios prácticos clave de cada módulo.
- Completa un mini-proyecto del Módulo 7.
- Explora al menos 2-3 recursos complementarios.

**Si tienes más de 20 horas:**
- Completa todos los módulos con ejercicios detallados.
- Construye múltiples mini-proyectos.
- Explora técnicas avanzadas de optimización en el Módulo 4.
- Implementa despliegues en producción del Módulo 5.

## Recursos Esenciales

Estos recursos cuidadosamente seleccionados te proporcionarán el mayor valor para tu tiempo de estudio limitado:

### Documentación Imprescindible
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - La herramienta más eficiente para la optimización de modelos.
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La forma más rápida de desplegar SLMs localmente.
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referencia para un modelo líder optimizado para el edge.
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kit de herramientas de optimización integral de Intel.
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Entorno integrado de desarrollo de EdgeAI.
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plataforma de desarrollo de EdgeAI específica para Windows.

### Herramientas que Ahorran Tiempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Acceso rápido a modelos y despliegue.
- [Gradio](https://www.gradio.app/docs/interface) - Desarrollo rápido de interfaces para demostraciones de IA.
- [Microsoft Olive](https://github.com/microsoft/Olive) - Optimización de modelos simplificada.
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferencia eficiente en CPU.
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Marco de compresión de redes neuronales.
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Kit de herramientas para el despliegue de modelos de lenguaje grandes.

## Plantilla de Seguimiento de Progreso

Utiliza esta plantilla simplificada para hacer un seguimiento de tu progreso en el curso de 20 horas:

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave Aprendidos |
|--------|-----------------------|-----------------|-------------------------|
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Agentes de IA | | | |
| Módulo 7: Herramientas de Desarrollo | | | |
| Módulo 8: Kit de Herramientas Foundry Local | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

## Ideas para Mini-Proyectos

Considera completar uno de estos proyectos para practicar conceptos de EdgeAI (cada uno diseñado para tomar entre 2 y 4 horas):

### Proyectos para Principiantes (2-3 horas cada uno)
1. **Asistente de Texto en el Edge**: Crea una herramienta simple de autocompletado de texto offline utilizando un modelo de lenguaje pequeño.
2. **Panel de Comparación de Modelos**: Construye una visualización básica de métricas de rendimiento entre diferentes SLMs.
3. **Experimento de Optimización**: Mide el impacto de diferentes niveles de cuantización en el mismo modelo base.

### Proyectos Intermedios (3-4 horas cada uno)
4. **Flujo de Trabajo con AI Toolkit**: Utiliza el AI Toolkit de VS Code para optimizar y desplegar un modelo de principio a fin.
5. **Aplicación con Windows AI Foundry**: Crea una aplicación de Windows utilizando la API Phi Silica y optimización para NPU.
6. **Despliegue Multiplataforma**: Despliega el mismo modelo optimizado en Windows (OpenVINO) y móvil (.NET MAUI).
7. **Agente con Llamadas a Funciones**: Construye un agente de IA con capacidades de llamadas a funciones para escenarios en el edge.

### Proyectos de Integración Avanzada (4-5 horas cada uno)
8. **Pipeline de Optimización OpenVINO**: Implementar la optimización completa del modelo utilizando NNCF y el toolkit GenAI  
9. **Pipeline SLMOps**: Implementar un ciclo de vida completo del modelo desde el entrenamiento hasta el despliegue en el edge  
10. **Sistema Edge Multi-Modelo**: Desplegar múltiples modelos especializados trabajando juntos en hardware de edge  
11. **Sistema de Integración MCP**: Construir un sistema agente utilizando Model Context Protocol para la interacción con herramientas  

## Referencias

- Microsoft Learn (Foundry Local)  
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Primeros pasos: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Referencia CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integración con SDKs de inferencia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Cómo usar Open WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Compilar modelos de Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agentes (descripción general): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Herramientas de Optimización e Inferencia  
  - Microsoft Olive (documentación): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (primeros pasos): https://onnxruntime.ai/docs/get-started/with-python.html  
  - Integración de ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (documentación): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (documentación): https://ml-explore.github.io/mlx/build/html/index.html  
- Frameworks de Despliegue y Modelos  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (documentación): https://docs.vllm.ai/  
  - Ollama (primeros pasos): https://github.com/ollama/ollama#get-started  
- Herramientas para Desarrolladores (Windows y VS Code)  
  - AI Toolkit para VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (descripción general): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Comunidad de Aprendizaje

Únete a la discusión y conecta con otros estudiantes:  
- Discusiones en GitHub en el [repositorio EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Conclusión

EdgeAI representa la vanguardia de la implementación de inteligencia artificial, llevando capacidades poderosas directamente a los dispositivos mientras aborda preocupaciones críticas sobre privacidad, latencia y conectividad. Este curso de 20 horas te proporciona el conocimiento esencial y las habilidades prácticas para comenzar a trabajar con tecnologías EdgeAI de inmediato.

El curso está diseñado para ser conciso y centrado en los conceptos más importantes, permitiéndote adquirir experiencia valiosa rápidamente sin una gran inversión de tiempo. Recuerda que la práctica práctica, incluso con ejemplos simples, es clave para reforzar lo que has aprendido.

¡Feliz aprendizaje!

---

