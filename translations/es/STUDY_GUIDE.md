<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-10-08T20:31:49+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "es"
}
-->
# EdgeAI para Principiantes: Rutas de Aprendizaje y Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 2 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 3 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 4-5 | Módulo 4: Optimización de Modelos (6 frameworks) | 4 horas |
| Día 6 | Módulo 5: SLMOps | 3 horas |
| Día 7 | Módulo 6-7: Agentes de IA y Herramientas de Desarrollo | 4 horas |
| Día 8 | Módulo 8: Toolkit Local Foundry (Implementación Moderna) | 1 hora |

### Ruta de Aprendizaje Concentrada (2 semanas)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Agentes de IA | 2 horas |
| Día 13-14 | Módulo 7: Herramientas de Desarrollo | 3 horas |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Sistemas Agénticos de SLM | 2 horas |
| Día 13-14 | Módulo 7: Ejemplos de Implementación de EdgeAI | 2 horas |

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave |
|--------|----------------|-------------|--------------|
| Módulo 0: Introducción a EdgeAI | | | |
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos (6 frameworks) | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Sistemas Agénticos de SLM | | | |
| Módulo 7: Ejemplos de Implementación de EdgeAI | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

## Introducción

¡Bienvenido a la guía de estudio de EdgeAI para Principiantes! Este documento está diseñado para ayudarte a navegar por los materiales del curso de manera efectiva y maximizar tu experiencia de aprendizaje. Proporciona rutas de aprendizaje estructuradas, horarios de estudio sugeridos, resúmenes de conceptos clave y recursos complementarios para profundizar en tu comprensión de las tecnologías Edge AI.

Este es un curso conciso de 20 horas que ofrece conocimientos esenciales sobre EdgeAI en un formato eficiente en tiempo, ideal para profesionales ocupados y estudiantes que desean adquirir rápidamente habilidades prácticas en este campo emergente.

## Resumen del Curso

Este curso está organizado en ocho módulos completos:

0. **Introducción a EdgeAI** - Fundamentos y contexto con aplicaciones industriales y objetivos de aprendizaje  
1. **Fundamentos y Transformación de EdgeAI** - Comprender los conceptos básicos y el cambio tecnológico  
2. **Fundamentos de Modelos de Lenguaje Pequeños (SLM)** - Exploración de diversas familias de SLM y sus arquitecturas  
3. **Despliegue de Modelos de Lenguaje Pequeños** - Estrategias prácticas de implementación  
4. **Conversión de Formato de Modelos y Cuantización** - Optimización avanzada con 6 frameworks, incluido OpenVINO  
5. **SLMOps - Operaciones de Modelos de Lenguaje Pequeños** - Gestión del ciclo de vida de producción y despliegue  
6. **Sistemas Agénticos de SLM** - Agentes de IA, llamadas a funciones y Protocolo de Contexto de Modelos  
7. **Ejemplos de Implementación de EdgeAI** - Toolkit de IA, desarrollo en Windows e implementaciones específicas de plataformas  
8. **Microsoft Foundry Local – Toolkit Completo para Desarrolladores** - Desarrollo local con integración híbrida de Azure (Módulo 08)

## Cómo Usar Esta Guía de Estudio

- **Aprendizaje Progresivo**: Sigue los módulos en orden para una experiencia de aprendizaje más coherente  
- **Puntos de Control de Conocimiento**: Utiliza las preguntas de autoevaluación después de cada sección  
- **Práctica Práctica**: Completa los ejercicios sugeridos para reforzar los conceptos teóricos  
- **Recursos Complementarios**: Explora materiales adicionales sobre los temas que más te interesen  

## Recomendaciones de Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 6 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 8 horas |
| Día 5 | Módulo 3: Despliegue de SLM | 3 horas |
| Día 6 | Módulo 8: Toolkit Local Foundry | 3 horas |

### Estudio a Tiempo Parcial (3 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 0: Introducción + Módulo 1: Fundamentos de EdgeAI | 7-9 horas |
| Semana 2 | Módulo 2: Fundamentos de SLM | 7-8 horas |
| Semana 3 | Módulo 3: Despliegue de SLM (3h) + Módulo 8: Toolkit Local Foundry (2-3h) | 5-6 horas |

## Módulo 0: Introducción a EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender qué es Edge AI y por qué es importante en el panorama tecnológico actual  
- Identificar las principales industrias transformadas por Edge AI y sus casos de uso específicos  
- Comprender las ventajas de los Modelos de Lenguaje Pequeños (SLMs) para el despliegue en el borde  
- Establecer expectativas claras de aprendizaje y resultados para el curso completo  
- Reconocer oportunidades de carrera y requisitos de habilidades en el campo de Edge AI  

### Áreas de Enfoque de Estudio

#### Sección 1: Paradigma y Definición de Edge AI  
- **Conceptos Prioritarios**:  
  - Edge AI vs. procesamiento tradicional en la nube  
  - La convergencia de hardware, optimización de modelos y demandas empresariales  
  - Despliegue de IA en tiempo real, preservación de la privacidad y eficiencia de costos  

#### Sección 2: Aplicaciones Industriales  
- **Conceptos Prioritarios**:  
  - Manufactura e Industria 4.0: Mantenimiento predictivo y control de calidad  
  - Salud: Imágenes diagnósticas y monitoreo de pacientes  
  - Sistemas Autónomos: Vehículos autónomos y transporte  
  - Ciudades Inteligentes: Gestión del tráfico y seguridad pública  
  - Tecnología de Consumo: Smartphones, wearables y hogares inteligentes  

#### Sección 3: Fundamentos de Modelos de Lenguaje Pequeños  
- **Conceptos Prioritarios**:  
  - Características y comparaciones de rendimiento de SLM  
  - Eficiencia de parámetros vs. compensaciones de capacidad  
  - Restricciones de despliegue en el borde y estrategias de optimización  

#### Sección 4: Marco de Aprendizaje y Camino Profesional  
- **Conceptos Prioritarios**:  
  - Arquitectura del curso y enfoque de dominio progresivo  
  - Habilidades técnicas y objetivos de implementación práctica  
  - Oportunidades de avance profesional y aplicaciones industriales  

### Preguntas de Autoevaluación

1. ¿Cuáles son las tres principales tendencias tecnológicas que han permitido Edge AI?  
2. Compara las ventajas y desafíos de Edge AI frente a la IA basada en la nube.  
3. Nombra tres industrias donde Edge AI aporta valor crítico y explica por qué.  
4. ¿Cómo hacen los Modelos de Lenguaje Pequeños que Edge AI sea práctico para el despliegue en el mundo real?  
5. ¿Cuáles son las habilidades técnicas clave que desarrollarás a lo largo de este curso?  
6. Describe el enfoque de aprendizaje en cuatro fases utilizado en este curso.  

### Ejercicios Prácticos

1. **Investigación Industrial**: Elige una aplicación industrial e investiga una implementación real de Edge AI (30 minutos)  
2. **Exploración de Modelos**: Navega por los Modelos de Lenguaje Pequeños disponibles en Hugging Face y compara sus conteos de parámetros y capacidades (30 minutos)  
3. **Planificación de Aprendizaje**: Revisa la estructura completa del curso y crea tu horario de estudio personal (15 minutos)  

### Materiales Complementarios

- [Resumen del Mercado de Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Resumen de Modelos de Lenguaje Pequeños - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Fundamentos de Computación en el Borde](https://www.edgecomputing.org/)  

## Módulo 1: Fundamentos y Transformación de EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender las diferencias entre IA basada en la nube y basada en el borde  
- Dominar técnicas de optimización clave para entornos con recursos limitados  
- Analizar aplicaciones reales de tecnologías EdgeAI  
- Configurar un entorno de desarrollo para proyectos de EdgeAI  

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de EdgeAI  
- **Conceptos Prioritarios**:  
  - Paradigmas de computación en el borde vs. en la nube  
  - Técnicas de cuantización de modelos  
  - Opciones de aceleración de hardware (NPUs, GPUs, CPUs)  
  - Ventajas de privacidad y seguridad  

- **Materiales Complementarios**:  
  - [Documentación de TensorFlow Lite](https://www.tensorflow.org/lite)  
  - [GitHub de ONNX Runtime](https://github.com/microsoft/onnxruntime)  
  - [Documentación de Edge Impulse](https://docs.edgeimpulse.com)  

#### Sección 2: Estudios de Caso Reales  
- **Conceptos Prioritarios**:  
  - Ecosistema de modelos Microsoft Phi & Mu  
  - Implementaciones prácticas en diversas industrias  
  - Consideraciones de despliegue  

#### Sección 3: Guía Práctica de Implementación  
- **Conceptos Prioritarios**:  
  - Configuración del entorno de desarrollo  
  - Herramientas de cuantización y optimización  
  - Métodos de evaluación para implementaciones de EdgeAI  

#### Sección 4: Hardware de Despliegue en el Borde  
- **Conceptos Prioritarios**:  
  - Comparaciones de plataformas de hardware  
  - Estrategias de optimización para hardware específico  
  - Consideraciones de despliegue  

### Preguntas de Autoevaluación

1. Compara y contrasta la IA basada en la nube con las implementaciones de IA en el borde.  
2. Explica tres técnicas clave para optimizar modelos para el despliegue en el borde.  
3. ¿Cuáles son las principales ventajas de ejecutar modelos de IA en el borde?  
4. Describe el proceso de cuantización de un modelo y cómo afecta su rendimiento.  
5. Explica cómo los diferentes aceleradores de hardware (NPUs, GPUs, CPUs) influyen en el despliegue de EdgeAI.  

### Ejercicios Prácticos

1. **Configuración Rápida del Entorno**: Configura un entorno de desarrollo mínimo con los paquetes esenciales (30 minutos)  
2. **Exploración de Modelos**: Descarga y examina un modelo de lenguaje pequeño preentrenado (1 hora)  
3. **Cuantización Básica**: Prueba una cuantización simple en un modelo pequeño (1 hora)  

## Módulo 2: Fundamentos de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Comprender los principios arquitectónicos de diferentes familias de SLM  
- Comparar las capacidades de los modelos en diferentes escalas de parámetros  
- Evaluar modelos según eficiencia, capacidad y requisitos de despliegue  
- Reconocer casos de uso apropiados para diferentes familias de modelos  

### Áreas de Enfoque de Estudio

#### Sección 1: Familia de Modelos Phi de Microsoft  
- **Conceptos Prioritarios**:  
  - Evolución de la filosofía de diseño  
  - Arquitectura centrada en la eficiencia  
  - Capacidades especializadas  

#### Sección 2: Familia Qwen  
- **Conceptos Prioritarios**:  
  - Contribuciones de código abierto  
  - Opciones de despliegue escalables  
  - Arquitectura avanzada de razonamiento  

#### Sección 3: Familia Gemma  
- **Conceptos Prioritarios**:  
  - Innovación impulsada por la investigación  
  - Capacidades multimodales  
  - Optimización para dispositivos móviles  

#### Sección 4: Familia BitNET  
- **Conceptos Prioritarios**:  
  - Tecnología de cuantización de 1 bit  
  - Marco de optimización de inferencia  
  - Consideraciones de sostenibilidad  

#### Sección 5: Modelo Mu de Microsoft  
- **Conceptos Prioritarios**:  
  - Arquitectura centrada en dispositivos  
  - Integración del sistema con Windows  
  - Operación que preserva la privacidad  

#### Sección 6: Phi-Silica  
- **Conceptos Prioritarios**:  
  - Arquitectura optimizada para NPU  
  - Métricas de rendimiento  
  - Integración para desarrolladores  

### Preguntas de Autoevaluación

1. Compara los enfoques arquitectónicos de las familias de modelos Phi y Qwen.  
2. Explica cómo la tecnología de cuantización de BitNET difiere de la cuantización tradicional.  
3. ¿Cuáles son las ventajas únicas del modelo Mu para la integración con Windows?  
4. Describe cómo Phi-Silica aprovecha el hardware NPU para la optimización del rendimiento.  
5. Para una aplicación móvil con conectividad limitada, ¿qué familia de modelos sería más adecuada y por qué?  

### Ejercicios Prácticos  

1. **Comparación de Modelos**: Benchmark rápido de dos modelos SLM diferentes (1 hora)  
2. **Generación de Texto Simple**: Implementación básica de generación de texto con un modelo pequeño (1 hora)  
3. **Optimización Rápida**: Aplicar una técnica de optimización para mejorar la velocidad de inferencia (1 hora)  

## Módulo 3: Despliegue de Modelos de Lenguaje Pequeños  

### Objetivos Clave de Aprendizaje  

- Seleccionar modelos adecuados según las restricciones de despliegue  
- Dominar técnicas de optimización para diversos escenarios de despliegue  
- Implementar SLMs en entornos locales y en la nube  
- Diseñar configuraciones listas para producción en aplicaciones EdgeAI  

### Áreas de Estudio  

#### Sección 1: Aprendizaje Avanzado de SLM  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de parámetros  
  - Técnicas avanzadas de optimización  
  - Estrategias de adquisición de modelos  

#### Sección 2: Despliegue en Entornos Locales  
- **Conceptos Prioritarios**:  
  - Despliegue en la plataforma Ollama  
  - Soluciones locales de Microsoft Foundry  
  - Análisis comparativo de frameworks  

#### Sección 3: Despliegue en la Nube con Contenedores  
- **Conceptos Prioritarios**:  
  - Inferencia de alto rendimiento con vLLM  
  - Orquestación de contenedores  
  - Implementación de ONNX Runtime  

### Preguntas de Autoevaluación  

1. ¿Qué factores deben considerarse al elegir entre despliegue local y en la nube?  
2. Compara Ollama y Microsoft Foundry Local como opciones de despliegue.  
3. Explica los beneficios de la contenedorización para el despliegue de SLM.  
4. ¿Cuáles son las métricas clave de rendimiento que se deben monitorear para un SLM desplegado en el borde?  
5. Describe un flujo completo de despliegue desde la selección del modelo hasta la implementación en producción.  

### Ejercicios Prácticos  

1. **Despliegue Local Básico**: Desplegar un SLM simple usando Ollama (1 hora)  
2. **Verificación de Rendimiento**: Ejecutar un benchmark rápido en tu modelo desplegado (30 minutos)  
3. **Integración Simple**: Crear una aplicación mínima que utilice tu modelo desplegado (1 hora)  

## Módulo 4: Conversión de Formato de Modelos y Cuantización  

### Objetivos Clave de Aprendizaje  

- Dominar técnicas avanzadas de cuantización desde 1-bit hasta 8-bit de precisión  
- Comprender estrategias de conversión de formatos (GGUF, ONNX)  
- Implementar optimización en seis frameworks (Llama.cpp, Olive, OpenVINO, MLX, síntesis de flujos de trabajo)  
- Desplegar modelos optimizados para entornos de producción en el borde en hardware Intel, Apple y multiplataforma  

### Áreas de Estudio  

#### Sección 1: Fundamentos de Cuantización  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de precisión  
  - Equilibrio entre rendimiento y precisión  
  - Optimización de huella de memoria  

#### Sección 2: Implementación en Llama.cpp  
- **Conceptos Prioritarios**:  
  - Despliegue multiplataforma  
  - Optimización del formato GGUF  
  - Técnicas de aceleración de hardware  

#### Sección 3: Suite Microsoft Olive  
- **Conceptos Prioritarios**:  
  - Optimización consciente del hardware  
  - Despliegue de nivel empresarial  
  - Flujos de trabajo de optimización automatizados  

#### Sección 4: Toolkit OpenVINO  
- **Conceptos Prioritarios**:  
  - Optimización para hardware Intel  
  - Framework de Compresión de Redes Neuronales (NNCF)  
  - Despliegue de inferencia multiplataforma  
  - OpenVINO GenAI para despliegue de LLM  

#### Sección 5: Framework Apple MLX  
- **Conceptos Prioritarios**:  
  - Optimización para Apple Silicon  
  - Arquitectura de memoria unificada  
  - Capacidades de ajuste fino con LoRA  

#### Sección 6: Síntesis de Flujos de Trabajo para Edge AI  
- **Conceptos Prioritarios**:  
  - Arquitectura de flujo de trabajo unificada  
  - Árboles de decisión para selección de frameworks  
  - Validación de preparación para producción  
  - Estrategias para futuro asegurado  

### Preguntas de Autoevaluación  

1. Compara estrategias de cuantización en diferentes niveles de precisión (1-bit a 8-bit).  
2. Explica las ventajas del formato GGUF para despliegue en el borde.  
3. ¿Cómo mejora la eficiencia de despliegue la optimización consciente del hardware en Microsoft Olive?  
4. ¿Cuáles son los beneficios clave del NNCF de OpenVINO para la compresión de modelos?  
5. Describe cómo Apple MLX aprovecha la arquitectura de memoria unificada para la optimización.  
6. ¿Cómo ayuda la síntesis de flujos de trabajo en la selección de frameworks óptimos de optimización?  

### Ejercicios Prácticos  

1. **Cuantización de Modelos**: Aplicar diferentes niveles de cuantización a un modelo y comparar resultados (1 hora)  
2. **Optimización con OpenVINO**: Usar NNCF para comprimir un modelo para hardware Intel (1 hora)  
3. **Comparación de Frameworks**: Probar el mismo modelo en tres frameworks de optimización diferentes (1 hora)  
4. **Benchmarking de Rendimiento**: Medir el impacto de la optimización en velocidad de inferencia y uso de memoria (1 hora)  

## Módulo 5: SLMOps - Operaciones de Modelos de Lenguaje Pequeños  

### Objetivos Clave de Aprendizaje  

- Comprender los principios de gestión del ciclo de vida de SLMOps  
- Dominar técnicas de destilación y ajuste fino para despliegue en el borde  
- Implementar estrategias de despliegue en producción con monitoreo  
- Construir flujos de trabajo de operaciones y mantenimiento de SLM de nivel empresarial  

### Áreas de Estudio  

#### Sección 1: Introducción a SLMOps  
- **Conceptos Prioritarios**:  
  - Cambio de paradigma de SLMOps en operaciones de IA  
  - Arquitectura eficiente en costos y centrada en la privacidad  
  - Impacto estratégico en negocios y ventajas competitivas  

#### Sección 2: Destilación de Modelos  
- **Conceptos Prioritarios**:  
  - Técnicas de transferencia de conocimiento  
  - Implementación del proceso de destilación en dos etapas  
  - Flujos de trabajo de destilación en Azure ML  

#### Sección 3: Estrategias de Ajuste Fino  
- **Conceptos Prioritarios**:  
  - Ajuste fino eficiente en parámetros (PEFT)  
  - Métodos avanzados LoRA y QLoRA  
  - Entrenamiento multi-adaptador y optimización de hiperparámetros  

#### Sección 4: Despliegue en Producción  
- **Conceptos Prioritarios**:  
  - Conversión y cuantización de modelos para producción  
  - Configuración de despliegue en Foundry Local  
  - Benchmarking de rendimiento y validación de calidad  

### Preguntas de Autoevaluación  

1. ¿Cómo difiere SLMOps de MLOps tradicional?  
2. Explica los beneficios de la destilación de modelos para despliegue en el borde.  
3. ¿Cuáles son las consideraciones clave para ajustar SLMs en entornos con recursos limitados?  
4. Describe un flujo completo de despliegue en producción para aplicaciones de Edge AI.  

### Ejercicios Prácticos  

1. **Destilación Básica**: Crear un modelo más pequeño a partir de un modelo maestro más grande (1 hora)  
2. **Experimento de Ajuste Fino**: Ajustar un modelo para un dominio específico (1 hora)  
3. **Pipeline de Despliegue**: Configurar un pipeline básico de CI/CD para despliegue de modelos (1 hora)  

## Módulo 6: Sistemas Agénticos SLM - Agentes de IA y Llamadas a Funciones  

### Objetivos Clave de Aprendizaje  

- Construir agentes inteligentes de IA para entornos Edge utilizando Modelos de Lenguaje Pequeños  
- Implementar capacidades de llamadas a funciones con flujos de trabajo sistemáticos  
- Dominar la integración del Protocolo de Contexto de Modelos (MCP) para interacción estandarizada con herramientas  
- Crear sistemas agénticos sofisticados con mínima intervención humana  

### Áreas de Estudio  

#### Sección 1: Agentes de IA y Fundamentos de SLM  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de agentes (reflejo, basado en modelos, basado en objetivos, agentes de aprendizaje)  
  - Análisis de compensaciones entre SLM y LLM  
  - Patrones de diseño específicos para agentes en el borde  
  - Optimización de recursos para agentes  

#### Sección 2: Llamadas a Funciones en Modelos de Lenguaje Pequeños  
- **Conceptos Prioritarios**:  
  - Implementación de flujos de trabajo sistemáticos (detección de intención, salida JSON, ejecución externa)  
  - Implementaciones específicas de plataformas (Phi-4-mini, modelos Qwen seleccionados, Microsoft Foundry Local)  
  - Ejemplos avanzados (colaboración multi-agente, selección dinámica de herramientas)  
  - Consideraciones para producción (limitación de tasas, registro de auditoría, medidas de seguridad)  

#### Sección 3: Integración del Protocolo de Contexto de Modelos (MCP)  
- **Conceptos Prioritarios**:  
  - Arquitectura del protocolo y diseño de sistemas en capas  
  - Soporte multi-backend (Ollama para desarrollo, vLLM para producción)  
  - Protocolos de conexión (modos STDIO y SSE)  
  - Aplicaciones en el mundo real (automatización web, procesamiento de datos, integración de API)  

### Preguntas de Autoevaluación  

1. ¿Cuáles son las consideraciones arquitectónicas clave para agentes de IA en el borde?  
2. ¿Cómo mejoran las llamadas a funciones las capacidades de los agentes?  
3. Explica el papel del Protocolo de Contexto de Modelos en la comunicación de agentes.  

### Ejercicios Prácticos  

1. **Agente Simple**: Construir un agente de IA básico con llamadas a funciones (1 hora)  
2. **Integración MCP**: Implementar MCP en una aplicación de agentes (30 minutos)  

## Taller: Ruta de Aprendizaje Práctico  

### Objetivos Clave de Aprendizaje  

- Construir aplicaciones de IA listas para producción utilizando Foundry Local SDK y mejores prácticas  
- Implementar patrones completos de manejo de errores y retroalimentación de usuarios  
- Crear pipelines RAG con evaluación de calidad y monitoreo de rendimiento  
- Desarrollar sistemas multi-agente con patrones de coordinación  
- Dominar el enrutamiento inteligente de modelos para selección basada en tareas  
- Desplegar soluciones de IA locales con arquitecturas que preserven la privacidad  

### Áreas de Estudio  

#### Sesión 01: Introducción a Foundry Local  
- **Conceptos Prioritarios**:  
  - Integración del SDK FoundryLocalManager y descubrimiento automático de servicios  
  - Implementaciones básicas y de chat en streaming  
  - Patrones de manejo de errores y retroalimentación de usuarios  
  - Configuración basada en el entorno  

#### Sesión 02: Construcción de Soluciones de IA con RAG  
- **Conceptos Prioritarios**:  
  - Embeddings vectoriales en memoria con sentence-transformers  
  - Implementación de pipelines RAG (recuperar → generar)  
  - Evaluación de calidad con métricas RAGAS  
  - Seguridad en importaciones para dependencias opcionales  

#### Sesión 03: Modelos Open Source  
- **Conceptos Prioritarios**:  
  - Estrategias de benchmarking multi-modelo  
  - Mediciones de latencia y rendimiento  
  - Degradación gradual y recuperación de errores  
  - Comparación de rendimiento entre familias de modelos  

#### Sesión 04: Modelos de Última Generación  
- **Conceptos Prioritarios**:  
  - Metodología de comparación entre SLM y LLM  
  - Pistas de tipo y formato de salida completo  
  - Manejo de errores por modelo  
  - Resultados estructurados para análisis  

#### Sesión 05: Agentes Impulsados por IA  
- **Conceptos Prioritarios**:  
  - Orquestación multi-agente con patrón de coordinación  
  - Gestión de memoria de agentes y seguimiento de estado  
  - Manejo de errores en pipelines y registro de etapas  
  - Monitoreo de rendimiento y estadísticas  

#### Sesión 06: Modelos como Herramientas  
- **Conceptos Prioritarios**:  
  - Detección de intención y coincidencia de patrones  
  - Algoritmos de enrutamiento de modelos basados en palabras clave  
  - Pipelines de múltiples pasos (planificar → ejecutar → refinar)  
  - Documentación completa de funciones  

### Preguntas de Autoevaluación  

1. ¿Cómo simplifica FoundryLocalManager la gestión de servicios en comparación con llamadas REST manuales?  
2. Explica la importancia de las protecciones de importación para dependencias opcionales como sentence-transformers.  
3. ¿Qué estrategias aseguran una degradación gradual en el benchmarking multi-modelo?  
4. ¿Cómo orquesta el patrón de coordinación múltiples agentes especializados?  
5. Describe los componentes de un enrutador de modelos inteligente.  
6. ¿Cuáles son los elementos clave para el manejo de errores listo para producción?  

### Ejercicios Prácticos  

1. **Aplicación de Chat**: Implementar chat en streaming con manejo de errores (45 minutos)  
2. **Pipeline RAG**: Construir un RAG mínimo con evaluación de calidad (1 hora)  
3. **Benchmarking de Modelos**: Comparar 3+ modelos en rendimiento (1 hora)  
4. **Sistema Multi-Agente**: Crear un coordinador con 2 agentes especializados (1.5 horas)  
5. **Enrutador Inteligente**: Construir selección de modelos basada en tareas (1 hora)  
6. **Despliegue en Producción**: Añadir monitoreo y manejo de errores completo (45 minutos)  

### Asignación de Tiempo  

**Aprendizaje Concentrado (1 semana)**:  
- Día 1: Sesión 01-02 (Chat + RAG) - 3 horas  
- Día 2: Sesión 03-04 (Benchmarking + Comparación) - 3 horas  
- Día 3: Sesión 05-06 (Agentes + Enrutamiento) - 3 horas  
- Día 4: Ejercicios prácticos y validación - 2 horas  

**Estudio a Tiempo Parcial (2 semanas)**:  
- Semana 1: Sesiones 01-03 (6 horas en total)  
- Semana 2: Sesiones 04-06 + ejercicios (5 horas en total)  

## Módulo 7: Ejemplos de Implementación de EdgeAI  

### Objetivos Clave de Aprendizaje  

- Dominar AI Toolkit para Visual Studio Code para flujos de trabajo completos de desarrollo EdgeAI  
- Obtener experiencia en la plataforma Windows AI Foundry y estrategias de optimización NPU  
- Implementar EdgeAI en múltiples plataformas de hardware y escenarios de despliegue  
- Construir aplicaciones EdgeAI listas para producción con optimizaciones específicas de plataforma  

### Áreas de Estudio  

#### Sección 1: AI Toolkit para Visual Studio Code  
- **Conceptos Prioritarios**:  
  - Entorno de desarrollo completo de Edge AI dentro de VS Code  
  - Catálogo de modelos y descubrimiento para despliegue en el borde  
  - Pruebas locales, optimización y flujos de trabajo de desarrollo de agentes  
  - Monitoreo de rendimiento y evaluación para escenarios en el borde  

#### Sección 2: Guía de Desarrollo EdgeAI en Windows  
- **Conceptos Prioritarios**:  
  - Descripción general completa de la plataforma Windows AI Foundry  
  - API Phi Silica para inferencia eficiente en NPU  
  - APIs de Visión Computacional para procesamiento de imágenes y OCR  
  - CLI de Foundry Local para desarrollo y pruebas locales  

#### Sección 3: Implementaciones Específicas de Plataforma  
- **Conceptos Prioritarios**:  
  - Despliegue en NVIDIA Jetson Orin Nano (67 TOPS de rendimiento AI)  
  - Aplicaciones móviles con .NET MAUI y ONNX Runtime GenAI  
  - Soluciones Azure EdgeAI con arquitectura híbrida nube-borde  
  - Optimización Windows ML con soporte universal de hardware  
  - Aplicaciones Foundry Local con implementación RAG centrada en la privacidad  

### Preguntas de Autoevaluación  

1. ¿Cómo simplifica AI Toolkit el flujo de trabajo de desarrollo EdgeAI?  
2. Compara estrategias de despliegue en diferentes plataformas de hardware.  
3. ¿Cuáles son las ventajas de Windows AI Foundry para el desarrollo en el borde?  
4. Explica el papel de la optimización de NPU en las aplicaciones modernas de IA en el borde.
5. ¿Cómo aprovecha la API Phi Silica el hardware de NPU para la optimización del rendimiento?
6. Compara los beneficios del despliegue local frente al despliegue en la nube para aplicaciones sensibles a la privacidad.

### Ejercicios Prácticos

1. **Configuración del Toolkit de IA**: Configura el Toolkit de IA y optimiza un modelo (1 hora)
2. **Windows AI Foundry**: Crea una aplicación sencilla de IA para Windows utilizando la API Phi Silica (1 hora)
3. **Despliegue Multiplataforma**: Despliega el mismo modelo en dos plataformas diferentes (1 hora)
4. **Optimización de NPU**: Prueba el rendimiento de la NPU con las herramientas de Windows AI Foundry (30 minutos)

## Módulo 8: Microsoft Foundry Local – Toolkit Completo para Desarrolladores (Modernizado)

### Objetivos Clave de Aprendizaje

- Instalar y configurar Foundry Local con integración moderna de SDK
- Implementar sistemas avanzados de múltiples agentes con patrones de coordinador
- Crear enrutadores inteligentes de modelos con selección automática basada en tareas
- Desplegar soluciones de IA listas para producción con monitoreo integral
- Integrar con Azure AI Foundry para escenarios de despliegue híbrido
- Dominar patrones modernos de SDK con FoundryLocalManager y cliente OpenAI

### Áreas de Estudio Prioritarias

#### Sección 1: Instalación y Configuración Moderna
- **Conceptos Prioritarios**: 
  - Integración del SDK FoundryLocalManager
  - Descubrimiento automático de servicios y monitoreo de salud
  - Patrones de configuración basados en el entorno
  - Consideraciones para despliegues en producción

#### Sección 2: Sistemas Avanzados de Múltiples Agentes
- **Conceptos Prioritarios**: 
  - Patrón de coordinador con agentes especializados
  - Especialización de agentes en recuperación, razonamiento y ejecución
  - Mecanismos de bucle de retroalimentación para refinamiento
  - Monitoreo de rendimiento y seguimiento de estadísticas

#### Sección 3: Enrutamiento Inteligente de Modelos
- **Conceptos Prioritarios**: 
  - Algoritmos de selección de modelos basados en palabras clave
  - Soporte para múltiples modelos (general, razonamiento, código, creativo)
  - Configuración de variables de entorno para flexibilidad
  - Verificación de salud del servicio y manejo de errores

#### Sección 4: Implementación Lista para Producción
- **Conceptos Prioritarios**: 
  - Manejo integral de errores y mecanismos de respaldo
  - Monitoreo de solicitudes y seguimiento de rendimiento
  - Ejemplos interactivos en Jupyter notebooks con benchmarks
  - Patrones de integración con aplicaciones existentes

### Preguntas de Autoevaluación

1. ¿Cómo se diferencia el enfoque moderno de FoundryLocalManager de las llamadas REST manuales?
2. Explica el patrón de coordinador y cómo orquesta agentes especializados.
3. ¿Cómo selecciona el enrutador inteligente los modelos apropiados según el contenido de la consulta?
4. ¿Cuáles son los componentes clave de un sistema de agentes de IA listo para producción?
5. ¿Cómo implementas un monitoreo integral de salud para los servicios de Foundry Local?
6. Compara los beneficios del enfoque modernizado frente a los patrones de implementación tradicionales.

### Ejercicios Prácticos

1. **Configuración Moderna del SDK**: Configura FoundryLocalManager con descubrimiento automático de servicios (30 minutos)
2. **Sistema de Múltiples Agentes**: Ejecuta el coordinador avanzado con agentes especializados (30 minutos)
3. **Enrutamiento Inteligente**: Prueba el enrutador de modelos con diferentes tipos de consultas (30 minutos)
4. **Exploración Interactiva**: Utiliza los Jupyter notebooks para explorar funciones avanzadas (45 minutos)
5. **Despliegue en Producción**: Implementa patrones de monitoreo y manejo de errores (30 minutos)
6. **Integración Híbrida**: Configura escenarios de respaldo con Azure AI Foundry (30 minutos)

## Guía de Asignación de Tiempo

Para ayudarte a aprovechar al máximo el curso extendido de 30 horas (incluyendo el taller), aquí tienes una sugerencia de cómo distribuir tu tiempo:

| Actividad | Asignación de Tiempo | Descripción |
|-----------|----------------------|-------------|
| Lectura de Materiales Básicos | 12 horas | Enfocándote en los conceptos esenciales de cada módulo |
| Ejercicios Prácticos | 10 horas | Implementación práctica de técnicas clave (incluyendo el taller) |
| Autoevaluación | 3 horas | Evaluar tu comprensión mediante preguntas y reflexión |
| Mini-Proyecto | 5 horas | Aplicar conocimientos en una implementación práctica pequeña |

### Áreas Clave de Enfoque según Restricciones de Tiempo

**Si solo tienes 10 horas:**
- Completa el Módulo 0 (Introducción) y los Módulos 1, 2 y 3 (conceptos básicos de EdgeAI)
- Realiza al menos un ejercicio práctico por módulo
- Enfócate en comprender los conceptos clave en lugar de los detalles de implementación

**Si puedes dedicar las 20 horas completas:**
- Completa los ocho módulos (incluyendo la Introducción)
- Realiza ejercicios prácticos clave de cada módulo
- Completa un mini-proyecto del Módulo 7
- Explora al menos 2-3 recursos complementarios

**Si tienes más de 20 horas:**
- Completa todos los módulos (incluyendo la Introducción) con ejercicios detallados
- Crea múltiples mini-proyectos
- Explora técnicas avanzadas de optimización en el Módulo 4
- Implementa despliegues en producción del Módulo 5

## Recursos Esenciales

Estos recursos cuidadosamente seleccionados ofrecen el mayor valor para tu tiempo de estudio limitado:

### Documentación Imprescindible
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - La herramienta más eficiente para optimización de modelos
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La forma más rápida de desplegar SLMs localmente
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referencia para un modelo líder optimizado para el borde
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kit de herramientas integral de optimización de Intel
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Entorno de desarrollo integrado para EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plataforma de desarrollo de EdgeAI específica para Windows

### Herramientas que Ahorran Tiempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Acceso rápido y despliegue de modelos
- [Gradio](https://www.gradio.app/docs/interface) - Desarrollo rápido de interfaces para demostraciones de IA
- [Microsoft Olive](https://github.com/microsoft/Olive) - Optimización simplificada de modelos
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferencia eficiente en CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Marco de compresión de redes neuronales
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Kit de herramientas para despliegue de modelos de lenguaje grande

## Plantilla de Seguimiento de Progreso

Utiliza esta plantilla simplificada para seguir tu progreso de aprendizaje a lo largo del curso de 20 horas:

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave Aprendidos |
|--------|-----------------------|-----------------|-------------------------|
| Módulo 0: Introducción a EdgeAI | | | |
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Agentes de IA | | | |
| Módulo 7: Herramientas de Desarrollo | | | |
| Taller: Aprendizaje Práctico | | | |
| Módulo 8: Toolkit Foundry Local | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

## Ideas de Mini-Proyectos

Considera completar uno de estos proyectos para practicar conceptos de EdgeAI (cada uno diseñado para tomar de 2 a 4 horas):

### Proyectos para Principiantes (2-3 horas cada uno)
1. **Asistente de Texto en el Borde**: Crea una herramienta sencilla de completado de texto offline utilizando un modelo de lenguaje pequeño
2. **Panel de Comparación de Modelos**: Construye una visualización básica de métricas de rendimiento entre diferentes SLMs
3. **Experimento de Optimización**: Mide el impacto de diferentes niveles de cuantización en el mismo modelo base

### Proyectos Intermedios (3-4 horas cada uno)
4. **Flujo de Trabajo del Toolkit de IA**: Utiliza el Toolkit de IA de VS Code para optimizar y desplegar un modelo de principio a fin
5. **Aplicación de Windows AI Foundry**: Crea una aplicación de Windows utilizando la API Phi Silica y optimización de NPU
6. **Despliegue Multiplataforma**: Despliega el mismo modelo optimizado en Windows (OpenVINO) y móvil (.NET MAUI)
7. **Agente de Llamadas a Funciones**: Construye un agente de IA con capacidades de llamadas a funciones para escenarios en el borde

### Proyectos de Integración Avanzada (4-5 horas cada uno)
8. **Pipeline de Optimización OpenVINO**: Implementa una optimización completa de modelos utilizando NNCF y el kit de herramientas GenAI
9. **Pipeline de SLMOps**: Implementa un ciclo de vida completo de modelos desde el entrenamiento hasta el despliegue en el borde
10. **Sistema de Múltiples Modelos en el Borde**: Despliega múltiples modelos especializados trabajando juntos en hardware de borde
11. **Sistema de Integración MCP**: Construye un sistema basado en agentes utilizando el Protocolo de Contexto de Modelos para interacción con herramientas

## Referencias

- Microsoft Learn (Foundry Local)
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Primeros pasos: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Referencia CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integración con SDKs de inferencia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Cómo abrir WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilar modelos de Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agentes (descripción general): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Herramientas de Optimización e Inferencia
  - Microsoft Olive (docs): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (primeros pasos): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integración de ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (docs): https://docs.openvino.ai/2025/index.html
  - Apple MLX (docs): https://ml-explore.github.io/mlx/build/html/index.html
- Frameworks de Despliegue y Modelos
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (docs): https://docs.vllm.ai/
  - Ollama (primeros pasos): https://github.com/ollama/ollama#get-started
- Herramientas para Desarrolladores (Windows y VS Code)
  - AI Toolkit para VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (descripción general): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Comunidad de Aprendizaje

Únete a la discusión y conecta con otros estudiantes:
- Discusiones en GitHub en el [repositorio EdgeAI para principiantes](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusión

EdgeAI representa la vanguardia de la implementación de inteligencia artificial, llevando capacidades poderosas directamente a los dispositivos mientras aborda preocupaciones críticas sobre privacidad, latencia y conectividad. Este curso de 20 horas te proporciona el conocimiento esencial y las habilidades prácticas para comenzar a trabajar con tecnologías EdgeAI de inmediato.

El curso está diseñado para ser conciso y enfocado en los conceptos más importantes, permitiéndote adquirir experiencia valiosa rápidamente sin una inversión de tiempo abrumadora. Recuerda que la práctica práctica, incluso con ejemplos simples, es clave para reforzar lo que has aprendido.

¡Feliz aprendizaje!

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.