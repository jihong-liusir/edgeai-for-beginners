<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef04a48f3f1428fa008738033017e0e8",
  "translation_date": "2025-09-24T11:30:53+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "es"
}
-->
# EdgeAI para Principiantes: Rutas de Aprendizaje y Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 2 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 3 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 4-5 | Módulo 4: Optimización de Modelos (6 frameworks) | 4 horas |
| Día 6 | Módulo 5: SLMOps | 3 horas |
| Día 7 | Módulo 6-7: Agentes de IA y Herramientas de Desarrollo | 4 horas |
| Día 8 | Módulo 8: Toolkit Local de Foundry (Implementación Moderna) | 1 hora |

### Ruta de Aprendizaje Concentrada (2 semanas)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Agentes de IA | 2 horas |
| Día 13-14 | Módulo 7: Herramientas de Desarrollo | 3 horas |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Sistemas Agénticos de SLM | 2 horas |
| Día 13-14 | Módulo 7: Ejemplos de Implementación de EdgeAI | 2 horas |

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave |
|--------|----------------|-------------|--------------|
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos (6 frameworks) | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Sistemas Agénticos de SLM | | | |
| Módulo 7: Ejemplos de Implementación de EdgeAI | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

## Introducción

¡Bienvenido a la guía de estudio de EdgeAI para Principiantes! Este documento está diseñado para ayudarte a navegar por los materiales del curso de manera efectiva y maximizar tu experiencia de aprendizaje. Proporciona rutas de aprendizaje estructuradas, horarios de estudio sugeridos, resúmenes de conceptos clave y recursos complementarios para profundizar en tu comprensión de las tecnologías EdgeAI.

Este es un curso conciso de 20 horas que ofrece conocimientos esenciales sobre EdgeAI en un formato eficiente en tiempo, ideal para profesionales ocupados y estudiantes que desean adquirir rápidamente habilidades prácticas en este campo emergente.

## Resumen del Curso

Este curso está organizado en siete módulos completos:

1. **Fundamentos y Transformación de EdgeAI** - Comprender los conceptos básicos y el cambio tecnológico
2. **Fundamentos de Modelos de Lenguaje Pequeños (SLM)** - Exploración de diversas familias de SLM y sus arquitecturas
3. **Despliegue de Modelos de Lenguaje Pequeños (SLM)** - Implementación de estrategias prácticas de despliegue
4. **Conversión de Formatos de Modelos y Cuantización** - Optimización avanzada con 6 frameworks, incluyendo OpenVINO
5. **SLMOps - Operaciones de Modelos de Lenguaje Pequeños** - Gestión del ciclo de vida de producción y despliegue
6. **Sistemas Agénticos de SLM** - Agentes de IA, llamadas de funciones y Protocolo de Contexto de Modelos
7. **Ejemplos de Implementación de EdgeAI** - Toolkit de IA, desarrollo en Windows e implementaciones específicas de plataformas
8. **Microsoft Foundry Local – Toolkit Completo para Desarrolladores** - Desarrollo local con integración híbrida de Azure (Módulo 08)

## Cómo Usar Esta Guía de Estudio

- **Aprendizaje Progresivo**: Sigue los módulos en orden para una experiencia de aprendizaje más coherente
- **Puntos de Control de Conocimiento**: Utiliza las preguntas de autoevaluación después de cada sección
- **Práctica Práctica**: Completa los ejercicios sugeridos para reforzar los conceptos teóricos
- **Recursos Complementarios**: Explora materiales adicionales sobre los temas que más te interesen

## Recomendaciones de Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|-----------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 6 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 8 horas |
| Día 5 | Módulo 3: Despliegue de SLM | 3 horas |
| Día 6 | Módulo 8: Toolkit Local de Foundry | 3 horas |

### Estudio a Tiempo Parcial (3 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|-----------------|
| Semana 1 | Módulo 1: Fundamentos de EdgeAI | 6-7 horas |
| Semana 2 | Módulo 2: Fundamentos de SLM | 7-8 horas |
| Semana 3 | Módulo 3: Despliegue de SLM (3h) + Módulo 8: Toolkit Local de Foundry (2-3h) | 5-6 horas |

## Módulo 1: Fundamentos y Transformación de EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender las diferencias entre la IA basada en la nube y la IA basada en el edge
- Dominar técnicas de optimización clave para entornos con recursos limitados
- Analizar aplicaciones reales de tecnologías EdgeAI
- Configurar un entorno de desarrollo para proyectos de EdgeAI

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de EdgeAI
- **Conceptos Prioritarios**: 
  - Paradigmas de computación en el edge vs. en la nube
  - Técnicas de cuantización de modelos
  - Opciones de aceleración de hardware (NPUs, GPUs, CPUs)
  - Ventajas en privacidad y seguridad

- **Materiales Complementarios**:
  - [Documentación de TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub de ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentación de Edge Impulse](https://docs.edgeimpulse.com)

#### Sección 2: Estudios de Caso Reales
- **Conceptos Prioritarios**: 
  - Ecosistema de modelos Phi & Mu de Microsoft
  - Implementaciones prácticas en diversas industrias
  - Consideraciones de despliegue

#### Sección 3: Guía de Implementación Práctica
- **Conceptos Prioritarios**: 
  - Configuración del entorno de desarrollo
  - Herramientas de cuantización y optimización
  - Métodos de evaluación para implementaciones de EdgeAI

#### Sección 4: Hardware de Despliegue en el Edge
- **Conceptos Prioritarios**: 
  - Comparaciones de plataformas de hardware
  - Estrategias de optimización para hardware específico
  - Consideraciones de despliegue

### Preguntas de Autoevaluación

1. Compara y contrasta la IA basada en la nube con las implementaciones de IA en el edge.
2. Explica tres técnicas clave para optimizar modelos para despliegue en el edge.
3. ¿Cuáles son las principales ventajas de ejecutar modelos de IA en el edge?
4. Describe el proceso de cuantización de un modelo y cómo afecta al rendimiento.
5. Explica cómo diferentes aceleradores de hardware (NPUs, GPUs, CPUs) influyen en el despliegue de EdgeAI.

### Ejercicios Prácticos

1. **Configuración Rápida del Entorno**: Configura un entorno de desarrollo mínimo con los paquetes esenciales (30 minutos)
2. **Exploración de Modelos**: Descarga y examina un modelo de lenguaje pequeño preentrenado (1 hora)
3. **Cuantización Básica**: Prueba una cuantización simple en un modelo pequeño (1 hora)

## Módulo 2: Fundamentos de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Comprender los principios arquitectónicos de diferentes familias de SLM
- Comparar las capacidades de los modelos en diferentes escalas de parámetros
- Evaluar modelos según eficiencia, capacidad y requisitos de despliegue
- Reconocer casos de uso apropiados para diferentes familias de modelos

### Áreas de Enfoque de Estudio

#### Sección 1: Familia de Modelos Phi de Microsoft
- **Conceptos Prioritarios**: 
  - Evolución de la filosofía de diseño
  - Arquitectura orientada a la eficiencia
  - Capacidades especializadas

#### Sección 2: Familia Qwen
- **Conceptos Prioritarios**: 
  - Contribuciones de código abierto
  - Opciones de despliegue escalables
  - Arquitectura avanzada de razonamiento

#### Sección 3: Familia Gemma
- **Conceptos Prioritarios**: 
  - Innovación impulsada por la investigación
  - Capacidades multimodales
  - Optimización para dispositivos móviles

#### Sección 4: Familia BitNET
- **Conceptos Prioritarios**: 
  - Tecnología de cuantización de 1-bit
  - Framework de optimización de inferencia
  - Consideraciones de sostenibilidad

#### Sección 5: Modelo Mu de Microsoft
- **Conceptos Prioritarios**: 
  - Arquitectura orientada a dispositivos
  - Integración con sistemas Windows
  - Operación con preservación de privacidad

#### Sección 6: Phi-Silica
- **Conceptos Prioritarios**: 
  - Arquitectura optimizada para NPU
  - Métricas de rendimiento
  - Integración para desarrolladores

### Preguntas de Autoevaluación

1. Compara los enfoques arquitectónicos de las familias de modelos Phi y Qwen.
2. Explica cómo la tecnología de cuantización de BitNET difiere de la cuantización tradicional.
3. ¿Cuáles son las ventajas únicas del modelo Mu para la integración con Windows?
4. Describe cómo Phi-Silica aprovecha el hardware NPU para optimizar el rendimiento.
5. Para una aplicación móvil con conectividad limitada, ¿qué familia de modelos sería más adecuada y por qué?

### Ejercicios Prácticos

1. **Comparación de Modelos**: Benchmark rápido de dos modelos SLM diferentes (1 hora)
2. **Generación de Texto Simple**: Implementación básica de generación de texto con un modelo pequeño (1 hora)
3. **Optimización Rápida**: Aplica una técnica de optimización para mejorar la velocidad de inferencia (1 hora)

## Módulo 3: Despliegue de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Seleccionar modelos apropiados según las restricciones de despliegue
- Dominar técnicas de optimización para diversos escenarios de despliegue
- Implementar SLMs en entornos locales y en la nube
- Diseñar configuraciones listas para producción en aplicaciones EdgeAI

### Áreas de Enfoque de Estudio

#### Sección 1: Aprendizaje Avanzado de SLM
- **Conceptos Prioritarios**: 
  - Framework de clasificación de parámetros
  - Técnicas avanzadas de optimización
  - Estrategias de adquisición de modelos

#### Sección 2: Despliegue en Entornos Locales
- **Conceptos Prioritarios**: 
  - Despliegue en la plataforma Ollama
  - Soluciones locales de Microsoft Foundry
  - Análisis comparativo de frameworks

#### Sección 3: Despliegue en la Nube con Contenedores
- **Conceptos Prioritarios**: 
  - Inferencia de alto rendimiento con vLLM
  - Orquestación de contenedores
  - Implementación con ONNX Runtime

### Preguntas de Autoevaluación

1. ¿Qué factores deben considerarse al elegir entre despliegue local y despliegue en la nube?
2. Compara Ollama y Microsoft Foundry Local como opciones de despliegue.
3. Explica los beneficios de la contenedorización para el despliegue de SLM.
4. ¿Cuáles son las métricas clave de rendimiento que se deben monitorear para un SLM desplegado en el edge?
5. Describe un flujo completo de despliegue desde la selección del modelo hasta la implementación en producción.

### Ejercicios Prácticos

1. **Despliegue Local Básico**: Despliega un SLM simple usando Ollama (1 hora)
2. **Verificación de Rendimiento**: Ejecuta un benchmark rápido en tu modelo desplegado (30 minutos)
3. **Integración Simple**: Crea una aplicación mínima que utilice tu modelo desplegado (1 hora)

## Módulo 4: Conversión de Formatos de Modelos y Cuantización

### Objetivos Clave de Aprendizaje

- Dominar técnicas avanzadas de cuantización desde precisión de 1-bit hasta 8-bit
- Comprender estrategias de conversión de formatos (GGUF, ONNX)
- Implementar optimización en seis frameworks (Llama.cpp, Olive, OpenVINO, MLX, síntesis de workflows)
- Desplegar modelos optimizados para entornos de producción en el edge en hardware Intel, Apple y multiplataforma

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de Cuantización
- **Conceptos Prioritarios**: 
  - Framework de clasificación de precisión
  - Equilibrio entre rendimiento y precisión
  - Optimización de huella de memoria

#### Sección 2: Implementación con Llama.cpp
- **Conceptos Prioritarios**: 
  - Despliegue multiplataforma
  - Optimización de formato GGUF
  - Técnicas de aceleración de hardware

#### Sección 3: Suite Microsoft Olive
- **Conceptos Prioritarios**: 
  - Optimización consciente del hardware
  - Despliegue a nivel empresarial
  - Workflows automatizados de optimización

#### Sección 4: Toolkit OpenVINO
- **Conceptos Prioritarios**: 
  - Optimización para hardware Intel
  - Framework de Compresión de Redes Neuronales (NNCF)
  - Despliegue de inferencia multiplataforma
  - OpenVINO GenAI para el despliegue de LLM

#### Sección 5: Framework Apple MLX
- **Conceptos prioritarios**: 
  - Optimización para Apple Silicon
  - Arquitectura de memoria unificada
  - Capacidades de ajuste fino con LoRA

#### Sección 6: Síntesis del flujo de trabajo de desarrollo de Edge AI
- **Conceptos prioritarios**: 
  - Arquitectura de flujo de trabajo unificada
  - Árboles de decisión para selección de frameworks
  - Validación de preparación para producción
  - Estrategias para garantizar compatibilidad futura

### Preguntas de autoevaluación

1. Compara las estrategias de cuantización en diferentes niveles de precisión (1-bit a 8-bit).
2. Explica las ventajas del formato GGUF para el despliegue en el borde.
3. ¿Cómo mejora la optimización consciente del hardware en Microsoft Olive la eficiencia del despliegue?
4. ¿Cuáles son los beneficios clave de NNCF de OpenVINO para la compresión de modelos?
5. Describe cómo Apple MLX aprovecha la arquitectura de memoria unificada para la optimización.
6. ¿Cómo ayuda la síntesis del flujo de trabajo en la selección de frameworks de optimización óptimos?

### Ejercicios prácticos

1. **Cuantización de modelos**: Aplica diferentes niveles de cuantización a un modelo y compara los resultados (1 hora).
2. **Optimización con OpenVINO**: Utiliza NNCF para comprimir un modelo para hardware Intel (1 hora).
3. **Comparación de frameworks**: Prueba el mismo modelo en tres frameworks de optimización diferentes (1 hora).
4. **Evaluación de rendimiento**: Mide el impacto de la optimización en la velocidad de inferencia y el uso de memoria (1 hora).

## Módulo 5: SLMOps - Operaciones con Modelos de Lenguaje Pequeños

### Objetivos clave de aprendizaje

- Comprender los principios de gestión del ciclo de vida de SLMOps
- Dominar técnicas de destilación y ajuste fino para despliegue en el borde
- Implementar estrategias de despliegue en producción con monitoreo
- Construir flujos de trabajo de operaciones y mantenimiento de SLM de nivel empresarial

### Áreas de estudio prioritarias

#### Sección 1: Introducción a SLMOps
- **Conceptos prioritarios**: 
  - Cambio de paradigma de SLMOps en operaciones de IA
  - Arquitectura eficiente en costos y centrada en la privacidad
  - Impacto estratégico en negocios y ventajas competitivas

#### Sección 2: Destilación de modelos
- **Conceptos prioritarios**: 
  - Técnicas de transferencia de conocimiento
  - Implementación del proceso de destilación en dos etapas
  - Flujos de trabajo de destilación en Azure ML

#### Sección 3: Estrategias de ajuste fino
- **Conceptos prioritarios**: 
  - Ajuste fino eficiente en parámetros (PEFT)
  - Métodos avanzados como LoRA y QLoRA
  - Entrenamiento multi-adaptador y optimización de hiperparámetros

#### Sección 4: Despliegue en producción
- **Conceptos prioritarios**: 
  - Conversión y cuantización de modelos para producción
  - Configuración de despliegue local con Foundry Local
  - Evaluación de rendimiento y validación de calidad

### Preguntas de autoevaluación

1. ¿En qué se diferencia SLMOps de MLOps tradicional?
2. Explica los beneficios de la destilación de modelos para el despliegue en el borde.
3. ¿Cuáles son las consideraciones clave para ajustar modelos de lenguaje pequeños en entornos con recursos limitados?
4. Describe un pipeline completo de despliegue en producción para aplicaciones de IA en el borde.

### Ejercicios prácticos

1. **Destilación básica**: Crea un modelo más pequeño a partir de un modelo maestro más grande (1 hora).
2. **Experimento de ajuste fino**: Ajusta un modelo para un dominio específico (1 hora).
3. **Pipeline de despliegue**: Configura un pipeline básico de CI/CD para el despliegue de modelos (1 hora).

## Módulo 6: Sistemas Agénticos SLM - Agentes de IA y Llamadas a Funciones

### Objetivos clave de aprendizaje

- Construir agentes inteligentes de IA para entornos en el borde utilizando Modelos de Lenguaje Pequeños
- Implementar capacidades de llamadas a funciones con flujos de trabajo sistemáticos
- Dominar la integración del Protocolo de Contexto de Modelo (MCP) para interacción estandarizada con herramientas
- Crear sistemas agénticos sofisticados con mínima intervención humana

### Áreas de estudio prioritarias

#### Sección 1: Agentes de IA y fundamentos de SLM
- **Conceptos prioritarios**: 
  - Marco de clasificación de agentes (reflejo, basado en modelos, basado en objetivos, agentes de aprendizaje)
  - Análisis de compensaciones entre SLM y LLM
  - Patrones de diseño de agentes específicos para el borde
  - Optimización de recursos para agentes

#### Sección 2: Llamadas a funciones en Modelos de Lenguaje Pequeños
- **Conceptos prioritarios**: 
  - Implementación de flujos de trabajo sistemáticos (detección de intención, salida en JSON, ejecución externa)
  - Implementaciones específicas de plataformas (Phi-4-mini, modelos seleccionados de Qwen, Microsoft Foundry Local)
  - Ejemplos avanzados (colaboración multi-agente, selección dinámica de herramientas)
  - Consideraciones para producción (limitación de tasas, registro de auditoría, medidas de seguridad)

#### Sección 3: Integración del Protocolo de Contexto de Modelo (MCP)
- **Conceptos prioritarios**: 
  - Arquitectura del protocolo y diseño de sistemas por capas
  - Soporte multi-backend (Ollama para desarrollo, vLLM para producción)
  - Protocolos de conexión (modos STDIO y SSE)
  - Aplicaciones en el mundo real (automatización web, procesamiento de datos, integración de API)

### Preguntas de autoevaluación

1. ¿Cuáles son las consideraciones arquitectónicas clave para agentes de IA en el borde?
2. ¿Cómo mejoran las llamadas a funciones las capacidades de los agentes?
3. Explica el papel del Protocolo de Contexto de Modelo en la comunicación de agentes.

### Ejercicios prácticos

1. **Agente simple**: Construye un agente de IA básico con llamadas a funciones (1 hora).
2. **Integración MCP**: Implementa MCP en una aplicación de agente (30 minutos).

## Módulo 7: Ejemplos de Implementación de EdgeAI

### Objetivos clave de aprendizaje

- Dominar AI Toolkit para Visual Studio Code para flujos de trabajo completos de desarrollo de EdgeAI
- Obtener experiencia en la plataforma Windows AI Foundry y estrategias de optimización para NPU
- Implementar EdgeAI en múltiples plataformas de hardware y escenarios de despliegue
- Construir aplicaciones EdgeAI listas para producción con optimizaciones específicas de plataforma

### Áreas de estudio prioritarias

#### Sección 1: AI Toolkit para Visual Studio Code
- **Conceptos prioritarios**: 
  - Entorno completo de desarrollo de Edge AI dentro de VS Code
  - Catálogo de modelos y descubrimiento para despliegue en el borde
  - Flujos de trabajo de prueba local, optimización y desarrollo de agentes
  - Monitoreo de rendimiento y evaluación para escenarios en el borde

#### Sección 2: Guía de desarrollo de Windows EdgeAI
- **Conceptos prioritarios**: 
  - Descripción general completa de la plataforma Windows AI Foundry
  - API Phi Silica para inferencia eficiente en NPU
  - APIs de visión por computadora para procesamiento de imágenes y OCR
  - CLI de Foundry Local para desarrollo y pruebas locales

#### Sección 3: Implementaciones específicas de plataforma
- **Conceptos prioritarios**: 
  - Despliegue en NVIDIA Jetson Orin Nano (67 TOPS de rendimiento en IA)
  - Aplicaciones móviles con .NET MAUI y ONNX Runtime GenAI
  - Soluciones Azure EdgeAI con arquitectura híbrida nube-borde
  - Optimización de Windows ML con soporte universal de hardware
  - Aplicaciones Foundry Local con implementación RAG centrada en la privacidad

### Preguntas de autoevaluación

1. ¿Cómo simplifica AI Toolkit el flujo de trabajo de desarrollo de EdgeAI?
2. Compara estrategias de despliegue en diferentes plataformas de hardware.
3. ¿Cuáles son las ventajas de Windows AI Foundry para el desarrollo en el borde?
4. Explica el papel de la optimización de NPU en aplicaciones modernas de Edge AI.
5. ¿Cómo aprovecha la API Phi Silica el hardware NPU para la optimización del rendimiento?
6. Compara los beneficios del despliegue local frente al despliegue en la nube para aplicaciones sensibles a la privacidad.

### Ejercicios prácticos

1. **Configuración de AI Toolkit**: Configura AI Toolkit y optimiza un modelo (1 hora).
2. **Windows AI Foundry**: Construye una aplicación simple de Windows AI utilizando la API Phi Silica (1 hora).
3. **Despliegue multiplataforma**: Despliega el mismo modelo en dos plataformas diferentes (1 hora).
4. **Optimización de NPU**: Prueba el rendimiento de NPU con herramientas de Windows AI Foundry (30 minutos).

## Módulo 8: Microsoft Foundry Local – Kit de herramientas completo para desarrolladores (modernizado)

### Objetivos clave de aprendizaje

- Instalar y configurar Foundry Local con integración moderna de SDK
- Implementar sistemas avanzados multi-agente con patrones de coordinador
- Construir enrutadores inteligentes de modelos con selección automática basada en tareas
- Desplegar soluciones de IA listas para producción con monitoreo integral
- Integrar con Azure AI Foundry para escenarios de despliegue híbrido
- Dominar patrones modernos de SDK con FoundryLocalManager y cliente OpenAI

### Áreas de estudio prioritarias

#### Sección 1: Instalación y configuración moderna
- **Conceptos prioritarios**: 
  - Integración de SDK FoundryLocalManager
  - Descubrimiento automático de servicios y monitoreo de salud
  - Patrones de configuración basados en entorno
  - Consideraciones para despliegue en producción

#### Sección 2: Sistemas avanzados multi-agente
- **Conceptos prioritarios**: 
  - Patrón de coordinador con agentes especialistas
  - Especialización de agentes en recuperación, razonamiento y ejecución
  - Mecanismos de bucle de retroalimentación para refinamiento
  - Monitoreo de rendimiento y seguimiento de estadísticas

#### Sección 3: Enrutamiento inteligente de modelos
- **Conceptos prioritarios**: 
  - Algoritmos de selección de modelos basados en palabras clave
  - Soporte para múltiples modelos (general, razonamiento, código, creativo)
  - Configuración de variables de entorno para flexibilidad
  - Verificación de salud del servicio y manejo de errores

#### Sección 4: Implementación lista para producción
- **Conceptos prioritarios**: 
  - Manejo de errores integral y mecanismos de respaldo
  - Monitoreo de solicitudes y seguimiento de rendimiento
  - Ejemplos interactivos en Jupyter notebooks con benchmarks
  - Patrones de integración con aplicaciones existentes

### Preguntas de autoevaluación

1. ¿En qué se diferencia el enfoque moderno de FoundryLocalManager de las llamadas REST manuales?
2. Explica el patrón de coordinador y cómo orquesta agentes especialistas.
3. ¿Cómo selecciona el enrutador inteligente los modelos apropiados según el contenido de la consulta?
4. ¿Cuáles son los componentes clave de un sistema de agentes de IA listo para producción?
5. ¿Cómo implementas monitoreo integral de salud para los servicios de Foundry Local?
6. Compara los beneficios del enfoque modernizado frente a los patrones de implementación tradicionales.

### Ejercicios prácticos

1. **Configuración del SDK moderno**: Configura FoundryLocalManager con descubrimiento automático de servicios (30 minutos).
2. **Sistema multi-agente**: Ejecuta el coordinador avanzado con agentes especialistas (30 minutos).
3. **Enrutamiento inteligente**: Prueba el enrutador de modelos con diferentes tipos de consultas (30 minutos).
4. **Exploración interactiva**: Utiliza los Jupyter notebooks para explorar características avanzadas (45 minutos).
5. **Despliegue en producción**: Implementa patrones de monitoreo y manejo de errores (30 minutos).
6. **Integración híbrida**: Configura escenarios de respaldo con Azure AI Foundry (30 minutos).

## Guía de asignación de tiempo

Para ayudarte a aprovechar al máximo las 20 horas del curso, aquí tienes una sugerencia de cómo distribuir tu tiempo:

| Actividad | Asignación de tiempo | Descripción |
|-----------|----------------------|-------------|
| Lectura de materiales principales | 9 horas | Enfocarse en los conceptos esenciales de cada módulo |
| Ejercicios prácticos | 6 horas | Implementación práctica de técnicas clave |
| Autoevaluación | 2 horas | Evaluar tu comprensión mediante preguntas y reflexión |
| Mini-proyecto | 3 horas | Aplicar conocimientos en una implementación práctica pequeña |

### Áreas clave de enfoque según la restricción de tiempo

**Si solo tienes 10 horas:**
- Completa los módulos 1, 2 y 3 (conceptos básicos de EdgeAI).
- Realiza al menos un ejercicio práctico por módulo.
- Enfócate en comprender los conceptos principales en lugar de los detalles de implementación.

**Si puedes dedicar las 20 horas completas:**
- Completa los siete módulos.
- Realiza ejercicios prácticos clave de cada módulo.
- Completa un mini-proyecto del módulo 7.
- Explora al menos 2-3 recursos complementarios.

**Si tienes más de 20 horas:**
- Completa todos los módulos con ejercicios detallados.
- Construye múltiples mini-proyectos.
- Explora técnicas avanzadas de optimización en el módulo 4.
- Implementa despliegue en producción desde el módulo 5.

## Recursos esenciales

Estos recursos cuidadosamente seleccionados ofrecen el mayor valor para tu tiempo de estudio limitado:

### Documentación imprescindible
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - La herramienta de optimización de modelos más eficiente.
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La forma más rápida de desplegar SLMs localmente.
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referencia para un modelo líder optimizado para el borde.
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kit de herramientas de optimización integral de Intel.
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Entorno de desarrollo integrado para EdgeAI.
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plataforma de desarrollo EdgeAI específica para Windows.

### Herramientas que ahorran tiempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Acceso rápido a modelos y despliegue.
- [Gradio](https://www.gradio.app/docs/interface) - Desarrollo rápido de interfaces para demostraciones de IA.
- [Microsoft Olive](https://github.com/microsoft/Olive) - Optimización simplificada de modelos.
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferencia eficiente en CPU.
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework de compresión de redes neuronales.
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Kit de herramientas para el despliegue de modelos de lenguaje grandes.

## Plantilla de seguimiento de progreso

Utiliza esta plantilla simplificada para rastrear tu progreso de aprendizaje a lo largo del curso de 20 horas:

| Módulo | Fecha de finalización | Horas dedicadas | Puntos clave aprendidos |
|--------|-----------------------|-----------------|--------------------------|
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de modelos | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Agentes de IA | | | |
| Módulo 7: Herramientas de desarrollo | | | |
| Módulo 8: Kit de herramientas Foundry Local | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

## Ideas para Mini-Proyectos

Considera realizar uno de estos proyectos para practicar conceptos de EdgeAI (cada uno diseñado para tomar entre 2 y 4 horas):

### Proyectos para Principiantes (2-3 horas cada uno)
1. **Asistente de Texto en el Borde**: Crea una herramienta simple de autocompletado de texto offline utilizando un modelo de lenguaje pequeño.
2. **Panel de Comparación de Modelos**: Construye una visualización básica de métricas de rendimiento entre diferentes SLMs.
3. **Experimento de Optimización**: Mide el impacto de diferentes niveles de cuantización en el mismo modelo base.

### Proyectos Intermedios (3-4 horas cada uno)
4. **Flujo de Trabajo con AI Toolkit**: Utiliza VS Code AI Toolkit para optimizar y desplegar un modelo de principio a fin.
5. **Aplicación Windows AI Foundry**: Crea una aplicación para Windows utilizando la API Phi Silica y optimización NPU.
6. **Despliegue Multiplataforma**: Despliega el mismo modelo optimizado en Windows (OpenVINO) y móvil (.NET MAUI).
7. **Agente con Llamada a Funciones**: Construye un agente de IA con capacidades de llamada a funciones para escenarios en el borde.

### Proyectos de Integración Avanzada (4-5 horas cada uno)
8. **Pipeline de Optimización OpenVINO**: Implementa una optimización completa de modelos utilizando NNCF y GenAI toolkit.
9. **Pipeline SLMOps**: Implementa un ciclo de vida completo de modelos desde el entrenamiento hasta el despliegue en el borde.
10. **Sistema de Múltiples Modelos en el Borde**: Despliega múltiples modelos especializados trabajando juntos en hardware de borde.
11. **Sistema de Integración MCP**: Construye un sistema agente utilizando el Protocolo de Contexto de Modelos para interacción con herramientas.

## Referencias

- Microsoft Learn (Foundry Local)
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Primeros pasos: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Referencia CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integración con SDKs de inferencia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Cómo usar Open WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilar modelos de Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agentes (descripción general): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Herramientas de Optimización e Inferencia
  - Microsoft Olive (documentación): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (primeros pasos): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integración Olive con ONNX Runtime: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (documentación): https://docs.openvino.ai/2025/index.html
  - Apple MLX (documentación): https://ml-explore.github.io/mlx/build/html/index.html
- Frameworks de Despliegue y Modelos
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (documentación): https://docs.vllm.ai/
  - Ollama (primeros pasos): https://github.com/ollama/ollama#get-started
- Herramientas para Desarrolladores (Windows y VS Code)
  - AI Toolkit para VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (descripción general): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Comunidad de Aprendizaje

Únete a la discusión y conecta con otros estudiantes:
- Discusiones en GitHub sobre el [repositorio EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusión

EdgeAI representa la vanguardia de la implementación de inteligencia artificial, llevando capacidades poderosas directamente a los dispositivos mientras aborda preocupaciones críticas sobre privacidad, latencia y conectividad. Este curso de 20 horas te proporciona el conocimiento esencial y las habilidades prácticas para comenzar a trabajar con tecnologías EdgeAI de inmediato.

El curso está diseñado para ser conciso y centrado en los conceptos más importantes, permitiéndote adquirir experiencia valiosa rápidamente sin una inversión de tiempo abrumadora. Recuerda que la práctica práctica, incluso con ejemplos simples, es clave para reforzar lo que has aprendido.

¡Feliz aprendizaje!

---

