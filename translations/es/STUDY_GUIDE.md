<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T07:24:38+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "es"
}
-->
# EdgeAI para Principiantes: Rutas de Aprendizaje y Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 2 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 3 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 4-5 | Módulo 4: Optimización de Modelos (6 frameworks) | 4 horas |
| Día 6 | Módulo 5: SLMOps | 3 horas |
| Día 7 | Módulo 6-7: Agentes de IA y Herramientas de Desarrollo | 4 horas |
| Día 8 | Módulo 8: Toolkit Local Foundry (Implementación Moderna) | 1 hora |

### Ruta de Aprendizaje Concentrada (2 semanas)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Agentes de IA | 2 horas |
| Día 13-14 | Módulo 7: Herramientas de Desarrollo | 3 horas |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 3 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 3 horas |
| Día 5-6 | Módulo 3: Despliegue de SLM | 2 horas |
| Día 7-8 | Módulo 4: Optimización de Modelos | 4 horas |
| Día 9-10 | Módulo 5: SLMOps | 3 horas |
| Día 11-12 | Módulo 6: Sistemas Agénticos de SLM | 2 horas |
| Día 13-14 | Módulo 7: Ejemplos de Implementación de EdgeAI | 2 horas |

| Módulo | Fecha de Finalización | Horas Dedicadas | Puntos Clave |
|--------|----------------|-------------|--------------|
| Módulo 0: Introducción a EdgeAI | | | |
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de Modelos (6 frameworks) | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Sistemas Agénticos de SLM | | | |
| Módulo 7: Ejemplos de Implementación de EdgeAI | | | |
| Ejercicios Prácticos | | | |
| Mini-Proyecto | | | |

### Estudio a Tiempo Parcial (4 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 1-2: Fundamentos y Fundamentos de SLM | 6 horas |
| Semana 2 | Módulo 3-4: Despliegue y Optimización | 6 horas |
| Semana 3 | Módulo 5-6: SLMOps y Agentes de IA | 5 horas |
| Semana 4 | Módulo 7: Herramientas de Desarrollo e Integración | 3 horas |

## Introducción

¡Bienvenido a la guía de estudio de EdgeAI para Principiantes! Este documento está diseñado para ayudarte a navegar por los materiales del curso de manera efectiva y maximizar tu experiencia de aprendizaje. Proporciona rutas de aprendizaje estructuradas, horarios de estudio sugeridos, resúmenes de conceptos clave y recursos complementarios para profundizar en tu comprensión de las tecnologías Edge AI.

Este es un curso conciso de 20 horas que ofrece conocimientos esenciales sobre EdgeAI en un formato eficiente en tiempo, ideal para profesionales ocupados y estudiantes que desean adquirir rápidamente habilidades prácticas en este campo emergente.

## Resumen del Curso

Este curso está organizado en ocho módulos completos:

0. **Introducción a EdgeAI** - Fundamentos y contexto con aplicaciones industriales y objetivos de aprendizaje  
1. **Fundamentos y Transformación de EdgeAI** - Comprender los conceptos básicos y el cambio tecnológico  
2. **Fundamentos de Modelos de Lenguaje Pequeños (SLM)** - Exploración de diversas familias de SLM y sus arquitecturas  
3. **Despliegue de Modelos de Lenguaje Pequeños (SLM)** - Estrategias prácticas de implementación  
4. **Conversión de Formato de Modelos y Cuantización** - Optimización avanzada con 6 frameworks, incluido OpenVINO  
5. **SLMOps - Operaciones de Modelos de Lenguaje Pequeños** - Gestión del ciclo de vida de producción y despliegue  
6. **Sistemas Agénticos de SLM** - Agentes de IA, llamadas a funciones y Protocolo de Contexto de Modelos  
7. **Ejemplos de Implementación de EdgeAI** - Toolkit de IA, desarrollo en Windows e implementaciones específicas de plataformas  
8. **Microsoft Foundry Local – Toolkit Completo para Desarrolladores** - Desarrollo local con integración híbrida de Azure (Módulo 08)

## Cómo Usar Esta Guía de Estudio

- **Aprendizaje Progresivo**: Sigue los módulos en orden para una experiencia de aprendizaje más coherente  
- **Puntos de Control de Conocimiento**: Utiliza las preguntas de autoevaluación después de cada sección  
- **Práctica Práctica**: Completa los ejercicios sugeridos para reforzar los conceptos teóricos  
- **Recursos Complementarios**: Explora materiales adicionales para los temas que más te interesen  

## Recomendaciones de Horario de Estudio

### Ruta de Aprendizaje Concentrada (1 semana)

| Día | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Día 0 | Módulo 0: Introducción a EdgeAI | 1-2 horas |
| Día 1-2 | Módulo 1: Fundamentos de EdgeAI | 6 horas |
| Día 3-4 | Módulo 2: Fundamentos de SLM | 8 horas |
| Día 5 | Módulo 3: Despliegue de SLM | 3 horas |
| Día 6 | Módulo 8: Toolkit Local Foundry | 3 horas |

### Estudio a Tiempo Parcial (3 semanas)

| Semana | Enfoque | Horas Estimadas |
|------|-------|------------------|
| Semana 1 | Módulo 0: Introducción + Módulo 1: Fundamentos de EdgeAI | 7-9 horas |
| Semana 2 | Módulo 2: Fundamentos de SLM | 7-8 horas |
| Semana 3 | Módulo 3: Despliegue de SLM (3h) + Módulo 8: Toolkit Local Foundry (2-3h) | 5-6 horas |

## Módulo 0: Introducción a EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender qué es Edge AI y por qué es relevante en el panorama tecnológico actual  
- Identificar las principales industrias transformadas por Edge AI y sus casos de uso específicos  
- Comprender las ventajas de los Modelos de Lenguaje Pequeños (SLM) para el despliegue en el borde  
- Establecer expectativas claras de aprendizaje y resultados para el curso completo  
- Reconocer oportunidades de carrera y requisitos de habilidades en el campo de Edge AI  

### Áreas de Enfoque de Estudio

#### Sección 1: Paradigma y Definición de Edge AI  
- **Conceptos Prioritarios**:  
  - Edge AI vs. procesamiento tradicional en la nube  
  - La convergencia de hardware, optimización de modelos y demandas empresariales  
  - Despliegue de IA en tiempo real, preservación de la privacidad y eficiencia de costos  

#### Sección 2: Aplicaciones Industriales  
- **Conceptos Prioritarios**:  
  - Manufactura e Industria 4.0: Mantenimiento predictivo y control de calidad  
  - Salud: Imagen diagnóstica y monitoreo de pacientes  
  - Sistemas Autónomos: Vehículos autónomos y transporte  
  - Ciudades Inteligentes: Gestión del tráfico y seguridad pública  
  - Tecnología de Consumo: Smartphones, wearables y hogares inteligentes  

#### Sección 3: Fundamentos de Modelos de Lenguaje Pequeños  
- **Conceptos Prioritarios**:  
  - Características y comparaciones de rendimiento de SLM  
  - Eficiencia de parámetros vs. compensaciones de capacidad  
  - Restricciones de despliegue en el borde y estrategias de optimización  

#### Sección 4: Marco de Aprendizaje y Camino Profesional  
- **Conceptos Prioritarios**:  
  - Arquitectura del curso y enfoque de dominio progresivo  
  - Habilidades técnicas y objetivos de implementación práctica  
  - Oportunidades de avance profesional y aplicaciones industriales  

### Preguntas de Autoevaluación

1. ¿Cuáles son las tres principales tendencias tecnológicas que han permitido Edge AI?  
2. Compara las ventajas y desafíos de Edge AI frente a la IA basada en la nube.  
3. Nombra tres industrias donde Edge AI aporta valor crítico y explica por qué.  
4. ¿Cómo hacen los Modelos de Lenguaje Pequeños que Edge AI sea práctico para el despliegue en el mundo real?  
5. ¿Cuáles son las habilidades técnicas clave que desarrollarás a lo largo de este curso?  
6. Describe el enfoque de aprendizaje en cuatro fases utilizado en este curso.  

### Ejercicios Prácticos

1. **Investigación Industrial**: Elige una aplicación industrial e investiga una implementación real de Edge AI (30 minutos)  
2. **Exploración de Modelos**: Navega por los Modelos de Lenguaje Pequeños disponibles en Hugging Face y compara sus conteos de parámetros y capacidades (30 minutos)  
3. **Planificación de Aprendizaje**: Revisa la estructura completa del curso y crea tu horario de estudio personal (15 minutos)  

### Materiales Complementarios

- [Edge AI Market Overview - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Small Language Models Overview - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Edge Computing Foundation](https://www.edgecomputing.org/)  

## Módulo 1: Fundamentos y Transformación de EdgeAI

### Objetivos Clave de Aprendizaje

- Comprender las diferencias entre IA basada en la nube y IA basada en el borde  
- Dominar técnicas de optimización clave para entornos con recursos limitados  
- Analizar aplicaciones reales de tecnologías EdgeAI  
- Configurar un entorno de desarrollo para proyectos de EdgeAI  

### Áreas de Enfoque de Estudio

#### Sección 1: Fundamentos de EdgeAI  
- **Conceptos Prioritarios**:  
  - Paradigmas de computación en el borde vs. en la nube  
  - Técnicas de cuantización de modelos  
  - Opciones de aceleración de hardware (NPUs, GPUs, CPUs)  
  - Ventajas de privacidad y seguridad  

- **Materiales Complementarios**:  
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)  
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)  
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)  

#### Sección 2: Estudios de Caso Reales  
- **Conceptos Prioritarios**:  
  - Ecosistema de modelos Microsoft Phi & Mu  
  - Implementaciones prácticas en diversas industrias  
  - Consideraciones de despliegue  

#### Sección 3: Guía de Implementación Práctica  
- **Conceptos Prioritarios**:  
  - Configuración del entorno de desarrollo  
  - Herramientas de cuantización y optimización  
  - Métodos de evaluación para implementaciones de EdgeAI  

#### Sección 4: Hardware de Despliegue en el Borde  
- **Conceptos Prioritarios**:  
  - Comparaciones de plataformas de hardware  
  - Estrategias de optimización para hardware específico  
  - Consideraciones de despliegue  

### Preguntas de Autoevaluación

1. Compara y contrasta la IA basada en la nube con las implementaciones de IA en el borde.  
2. Explica tres técnicas clave para optimizar modelos para el despliegue en el borde.  
3. ¿Cuáles son las principales ventajas de ejecutar modelos de IA en el borde?  
4. Describe el proceso de cuantización de un modelo y cómo afecta su rendimiento.  
5. Explica cómo los diferentes aceleradores de hardware (NPUs, GPUs, CPUs) influyen en el despliegue de EdgeAI.  

### Ejercicios Prácticos

1. **Configuración Rápida del Entorno**: Configura un entorno de desarrollo mínimo con los paquetes esenciales (30 minutos)  
2. **Exploración de Modelos**: Descarga y examina un modelo de lenguaje pequeño preentrenado (1 hora)  
3. **Cuantización Básica**: Prueba una cuantización simple en un modelo pequeño (1 hora)  

## Módulo 2: Fundamentos de Modelos de Lenguaje Pequeños

### Objetivos Clave de Aprendizaje

- Comprender los principios arquitectónicos de diferentes familias de SLM  
- Comparar las capacidades de los modelos en diferentes escalas de parámetros  
- Evaluar modelos según eficiencia, capacidad y requisitos de despliegue  
- Reconocer casos de uso apropiados para diferentes familias de modelos  

### Áreas de Enfoque de Estudio

#### Sección 1: Familia de Modelos Phi de Microsoft  
- **Conceptos Prioritarios**:  
  - Evolución de la filosofía de diseño  
  - Arquitectura centrada en la eficiencia  
  - Capacidades especializadas  

#### Sección 2: Familia Qwen  
- **Conceptos Prioritarios**:  
  - Contribuciones de código abierto  
  - Opciones de despliegue escalables  
  - Arquitectura avanzada de razonamiento  

#### Sección 3: Familia Gemma  
- **Conceptos Prioritarios**:  
  - Innovación impulsada por la investigación  
  - Capacidades multimodales  
  - Optimización para dispositivos móviles  

#### Sección 4: Familia BitNET  
- **Conceptos Prioritarios**:  
  - Tecnología de cuantización de 1-bit  
  - Framework de optimización de inferencia  
  - Consideraciones de sostenibilidad  

#### Sección 5: Modelo Mu de Microsoft  
- **Conceptos Prioritarios**:  
  - Arquitectura centrada en dispositivos  
  - Integración del sistema con Windows  
  - Operación que preserva la privacidad  

#### Sección 6: Phi-Silica  
- **Conceptos Prioritarios**:  
  - Arquitectura optimizada para NPU  
  - Métricas de rendimiento  
  - Integración para desarrolladores  

### Preguntas de Autoevaluación

1. Compara los enfoques arquitectónicos de las familias de modelos Phi y Qwen.  
2. Explica cómo la tecnología de cuantización de BitNET difiere de la cuantización tradicional.  
3. ¿Cuáles son las ventajas únicas del modelo Mu para la integración con Windows?  
4. Describe cómo Phi-Silica aprovecha el hardware NPU para la optimización del rendimiento.  
5. Para una aplicación móvil con conectividad limitada, ¿qué familia de modelos sería más adecuada y por qué?  

### Ejercicios Prácticos  

1. **Comparación de Modelos**: Benchmark rápido de dos modelos SLM diferentes (1 hora)  
2. **Generación de Texto Simple**: Implementación básica de generación de texto con un modelo pequeño (1 hora)  
3. **Optimización Rápida**: Aplicar una técnica de optimización para mejorar la velocidad de inferencia (1 hora)  

## Módulo 3: Despliegue de Modelos de Lenguaje Pequeños  

### Objetivos Clave de Aprendizaje  

- Seleccionar modelos apropiados según las restricciones de despliegue  
- Dominar técnicas de optimización para diversos escenarios de despliegue  
- Implementar SLMs en entornos locales y en la nube  
- Diseñar configuraciones listas para producción en aplicaciones EdgeAI  

### Áreas de Estudio  

#### Sección 1: Aprendizaje Avanzado de SLM  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de parámetros  
  - Técnicas avanzadas de optimización  
  - Estrategias de adquisición de modelos  

#### Sección 2: Despliegue en Entornos Locales  
- **Conceptos Prioritarios**:  
  - Despliegue en la plataforma Ollama  
  - Soluciones locales de Microsoft Foundry  
  - Análisis comparativo de frameworks  

#### Sección 3: Despliegue en la Nube con Contenedores  
- **Conceptos Prioritarios**:  
  - Inferencia de alto rendimiento con vLLM  
  - Orquestación de contenedores  
  - Implementación de ONNX Runtime  

### Preguntas de Autoevaluación  

1. ¿Qué factores deben considerarse al elegir entre despliegue local y en la nube?  
2. Compara Ollama y Microsoft Foundry Local como opciones de despliegue.  
3. Explica los beneficios de la contenedorización para el despliegue de SLM.  
4. ¿Cuáles son las métricas clave de rendimiento que se deben monitorear para un SLM desplegado en el borde?  
5. Describe un flujo completo de despliegue desde la selección del modelo hasta la implementación en producción.  

### Ejercicios Prácticos  

1. **Despliegue Local Básico**: Desplegar un SLM simple usando Ollama (1 hora)  
2. **Verificación de Rendimiento**: Ejecutar un benchmark rápido en tu modelo desplegado (30 minutos)  
3. **Integración Simple**: Crear una aplicación mínima que utilice tu modelo desplegado (1 hora)  

## Módulo 4: Conversión de Formatos de Modelos y Cuantización  

### Objetivos Clave de Aprendizaje  

- Dominar técnicas avanzadas de cuantización desde 1-bit hasta 8-bit de precisión  
- Comprender estrategias de conversión de formatos (GGUF, ONNX)  
- Implementar optimización en seis frameworks (Llama.cpp, Olive, OpenVINO, MLX, síntesis de flujos de trabajo)  
- Desplegar modelos optimizados para entornos de producción en el borde en hardware Intel, Apple y multiplataforma  

### Áreas de Estudio  

#### Sección 1: Fundamentos de Cuantización  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de precisión  
  - Equilibrio entre rendimiento y precisión  
  - Optimización de huella de memoria  

#### Sección 2: Implementación en Llama.cpp  
- **Conceptos Prioritarios**:  
  - Despliegue multiplataforma  
  - Optimización del formato GGUF  
  - Técnicas de aceleración de hardware  

#### Sección 3: Suite Microsoft Olive  
- **Conceptos Prioritarios**:  
  - Optimización consciente del hardware  
  - Despliegue de nivel empresarial  
  - Flujos de trabajo de optimización automatizados  

#### Sección 4: Toolkit OpenVINO  
- **Conceptos Prioritarios**:  
  - Optimización para hardware Intel  
  - Marco de Compresión de Redes Neuronales (NNCF)  
  - Despliegue de inferencia multiplataforma  
  - OpenVINO GenAI para despliegue de LLM  

#### Sección 5: Framework Apple MLX  
- **Conceptos Prioritarios**:  
  - Optimización para Apple Silicon  
  - Arquitectura de memoria unificada  
  - Capacidades de ajuste fino con LoRA  

#### Sección 6: Síntesis de Flujos de Trabajo para Edge AI  
- **Conceptos Prioritarios**:  
  - Arquitectura de flujo de trabajo unificada  
  - Árboles de decisión para selección de frameworks  
  - Validación de preparación para producción  
  - Estrategias para garantizar compatibilidad futura  

### Preguntas de Autoevaluación  

1. Compara estrategias de cuantización en diferentes niveles de precisión (1-bit a 8-bit).  
2. Explica las ventajas del formato GGUF para despliegues en el borde.  
3. ¿Cómo mejora la eficiencia de despliegue la optimización consciente del hardware en Microsoft Olive?  
4. ¿Cuáles son los beneficios clave del NNCF de OpenVINO para la compresión de modelos?  
5. Describe cómo Apple MLX aprovecha la arquitectura de memoria unificada para la optimización.  
6. ¿Cómo ayuda la síntesis de flujos de trabajo en la selección de frameworks óptimos de optimización?  

### Ejercicios Prácticos  

1. **Cuantización de Modelos**: Aplicar diferentes niveles de cuantización a un modelo y comparar resultados (1 hora)  
2. **Optimización con OpenVINO**: Usar NNCF para comprimir un modelo para hardware Intel (1 hora)  
3. **Comparación de Frameworks**: Probar el mismo modelo en tres frameworks de optimización diferentes (1 hora)  
4. **Benchmarking de Rendimiento**: Medir el impacto de la optimización en la velocidad de inferencia y uso de memoria (1 hora)  

## Módulo 5: SLMOps - Operaciones de Modelos de Lenguaje Pequeños  

### Objetivos Clave de Aprendizaje  

- Comprender los principios de gestión del ciclo de vida de SLMOps  
- Dominar técnicas de destilación y ajuste fino para despliegue en el borde  
- Implementar estrategias de despliegue en producción con monitoreo  
- Construir flujos de trabajo de operaciones y mantenimiento de SLM de nivel empresarial  

### Áreas de Estudio  

#### Sección 1: Introducción a SLMOps  
- **Conceptos Prioritarios**:  
  - Cambio de paradigma de SLMOps en operaciones de IA  
  - Arquitectura eficiente en costos y centrada en la privacidad  
  - Impacto estratégico en negocios y ventajas competitivas  

#### Sección 2: Destilación de Modelos  
- **Conceptos Prioritarios**:  
  - Técnicas de transferencia de conocimiento  
  - Implementación del proceso de destilación en dos etapas  
  - Flujos de trabajo de destilación en Azure ML  

#### Sección 3: Estrategias de Ajuste Fino  
- **Conceptos Prioritarios**:  
  - Ajuste fino eficiente en parámetros (PEFT)  
  - Métodos avanzados LoRA y QLoRA  
  - Entrenamiento multi-adaptador y optimización de hiperparámetros  

#### Sección 4: Despliegue en Producción  
- **Conceptos Prioritarios**:  
  - Conversión y cuantización de modelos para producción  
  - Configuración de despliegue en Foundry Local  
  - Benchmarking de rendimiento y validación de calidad  

### Preguntas de Autoevaluación  

1. ¿Cómo difiere SLMOps de MLOps tradicional?  
2. Explica los beneficios de la destilación de modelos para despliegue en el borde.  
3. ¿Cuáles son las consideraciones clave para ajustar SLMs en entornos con recursos limitados?  
4. Describe un flujo completo de despliegue en producción para aplicaciones de Edge AI.  

### Ejercicios Prácticos  

1. **Destilación Básica**: Crear un modelo más pequeño a partir de un modelo maestro más grande (1 hora)  
2. **Experimento de Ajuste Fino**: Ajustar un modelo para un dominio específico (1 hora)  
3. **Pipeline de Despliegue**: Configurar un pipeline básico de CI/CD para despliegue de modelos (1 hora)  

## Módulo 6: Sistemas Agénticos SLM - Agentes de IA y Llamadas a Funciones  

### Objetivos Clave de Aprendizaje  

- Construir agentes inteligentes de IA para entornos Edge utilizando Modelos de Lenguaje Pequeños  
- Implementar capacidades de llamadas a funciones con flujos de trabajo sistemáticos  
- Dominar la integración del Protocolo de Contexto de Modelos (MCP) para interacción estandarizada con herramientas  
- Crear sistemas agénticos sofisticados con mínima intervención humana  

### Áreas de Estudio  

#### Sección 1: Agentes de IA y Fundamentos de SLM  
- **Conceptos Prioritarios**:  
  - Marco de clasificación de agentes (reflejo, basado en modelos, basado en objetivos, agentes de aprendizaje)  
  - Análisis de compensaciones entre SLM y LLM  
  - Patrones de diseño específicos para agentes en el borde  
  - Optimización de recursos para agentes  

#### Sección 2: Llamadas a Funciones en Modelos de Lenguaje Pequeños  
- **Conceptos Prioritarios**:  
  - Implementación de flujos de trabajo sistemáticos (detección de intención, salida JSON, ejecución externa)  
  - Implementaciones específicas de plataformas (Phi-4-mini, modelos Qwen seleccionados, Microsoft Foundry Local)  
  - Ejemplos avanzados (colaboración multi-agente, selección dinámica de herramientas)  
  - Consideraciones para producción (limitación de tasas, registro de auditoría, medidas de seguridad)  

#### Sección 3: Integración del Protocolo de Contexto de Modelos (MCP)  
- **Conceptos Prioritarios**:  
  - Arquitectura del protocolo y diseño de sistemas en capas  
  - Soporte multi-backend (Ollama para desarrollo, vLLM para producción)  
  - Protocolos de conexión (modos STDIO y SSE)  
  - Aplicaciones en el mundo real (automatización web, procesamiento de datos, integración de API)  

### Preguntas de Autoevaluación  

1. ¿Cuáles son las consideraciones arquitectónicas clave para agentes de IA en el borde?  
2. ¿Cómo mejoran las llamadas a funciones las capacidades de los agentes?  
3. Explica el papel del Protocolo de Contexto de Modelos en la comunicación de agentes.  

### Ejercicios Prácticos  

1. **Agente Simple**: Construir un agente de IA básico con llamadas a funciones (1 hora)  
2. **Integración MCP**: Implementar MCP en una aplicación de agentes (30 minutos)  

## Módulo 7: Ejemplos de Implementación de EdgeAI  

### Objetivos Clave de Aprendizaje  

- Dominar el AI Toolkit para Visual Studio Code para flujos de trabajo completos de desarrollo EdgeAI  
- Obtener experiencia en la plataforma Windows AI Foundry y estrategias de optimización NPU  
- Implementar EdgeAI en múltiples plataformas de hardware y escenarios de despliegue  
- Construir aplicaciones EdgeAI listas para producción con optimizaciones específicas de plataforma  

### Áreas de Estudio  

#### Sección 1: AI Toolkit para Visual Studio Code  
- **Conceptos Prioritarios**:  
  - Entorno de desarrollo completo de Edge AI dentro de VS Code  
  - Catálogo de modelos y descubrimiento para despliegue en el borde  
  - Pruebas locales, optimización y flujos de trabajo de desarrollo de agentes  
  - Monitoreo de rendimiento y evaluación para escenarios en el borde  

#### Sección 2: Guía de Desarrollo EdgeAI en Windows  
- **Conceptos Prioritarios**:  
  - Descripción completa de la plataforma Windows AI Foundry  
  - API Phi Silica para inferencia eficiente en NPU  
  - APIs de Visión Computacional para procesamiento de imágenes y OCR  
  - CLI de Foundry Local para desarrollo y pruebas locales  

#### Sección 3: Implementaciones Específicas de Plataforma  
- **Conceptos Prioritarios**:  
  - Despliegue en NVIDIA Jetson Orin Nano (67 TOPS de rendimiento AI)  
  - Aplicaciones móviles con .NET MAUI y ONNX Runtime GenAI  
  - Soluciones Azure EdgeAI con arquitectura híbrida nube-borde  
  - Optimización Windows ML con soporte universal de hardware  
  - Aplicaciones Foundry Local con implementación RAG centrada en la privacidad  

### Preguntas de Autoevaluación  

1. ¿Cómo simplifica el AI Toolkit el flujo de trabajo de desarrollo EdgeAI?  
2. Compara estrategias de despliegue en diferentes plataformas de hardware.  
3. ¿Cuáles son las ventajas de Windows AI Foundry para el desarrollo en el borde?  
4. Explica el papel de la optimización NPU en aplicaciones modernas de Edge AI.  
5. ¿Cómo aprovecha la API Phi Silica el hardware NPU para la optimización del rendimiento?  
6. Compara los beneficios del despliegue local frente al despliegue en la nube para aplicaciones sensibles a la privacidad.  

### Ejercicios Prácticos  

1. **Configuración del AI Toolkit**: Configurar AI Toolkit y optimizar un modelo (1 hora)  
2. **Windows AI Foundry**: Construir una aplicación simple de Windows AI usando la API Phi Silica (1 hora)  
3. **Despliegue Multiplataforma**: Desplegar el mismo modelo en dos plataformas diferentes (1 hora)  
4. **Optimización NPU**: Probar el rendimiento de NPU con herramientas de Windows AI Foundry (30 minutos)  

## Módulo 8: Microsoft Foundry Local – Kit de Herramientas Completo para Desarrolladores (Modernizado)  

### Objetivos Clave de Aprendizaje  

- Instalar y configurar Foundry Local con integración moderna de SDK  
- Implementar sistemas avanzados multi-agente con patrones de coordinación  
- Construir enrutadores inteligentes de modelos con selección automática basada en tareas  
- Desplegar soluciones de IA listas para producción con monitoreo completo  
- Integrar con Azure AI Foundry para escenarios de despliegue híbrido  
- Dominar patrones modernos de SDK con FoundryLocalManager y cliente OpenAI  

### Áreas de Estudio  

#### Sección 1: Instalación y Configuración Moderna  
- **Conceptos Prioritarios**:  
  - Integración del SDK FoundryLocalManager  
  - Descubrimiento automático de servicios y monitoreo de salud  
  - Patrones de configuración basados en entorno  
  - Consideraciones para despliegue en producción  

#### Sección 2: Sistemas Multi-Agente Avanzados  
- **Conceptos Prioritarios**:  
  - Patrón de coordinación con agentes especialistas  
  - Especialización de agentes en recuperación, razonamiento y ejecución  
  - Mecanismos de bucle de retroalimentación para refinamiento  
  - Monitoreo de rendimiento y seguimiento de estadísticas  

#### Sección 3: Enrutamiento Inteligente de Modelos  
- **Conceptos Prioritarios**:  
  - Algoritmos de selección de modelos basados en palabras clave  
  - Soporte para múltiples modelos (general, razonamiento, código, creativo)  
  - Configuración de variables de entorno para flexibilidad  
  - Verificación de salud del servicio y manejo de errores  

#### Sección 4: Implementación Lista para Producción  
- **Conceptos Prioritarios**:  
  - Manejo completo de errores y mecanismos de respaldo  
  - Monitoreo de solicitudes y seguimiento de rendimiento  
  - Ejemplos interactivos en Jupyter notebooks con benchmarks  
  - Patrones de integración con aplicaciones existentes  

### Preguntas de Autoevaluación  

1. ¿Cómo difiere el enfoque moderno de FoundryLocalManager de las llamadas REST manuales?  
2. Explica el patrón de coordinación y cómo orquesta agentes especialistas.  
3. ¿Cómo selecciona el enrutador inteligente los modelos apropiados según el contenido de la consulta?  
4. ¿Cuáles son los componentes clave de un sistema de agentes de IA listo para producción?  
5. ¿Cómo implementas monitoreo completo de salud para los servicios de Foundry Local?  
6. Compara los beneficios del enfoque modernizado frente a los patrones de implementación tradicionales.  

### Ejercicios Prácticos  

1. **Configuración del SDK Moderno**: Configurar FoundryLocalManager con descubrimiento automático de servicios (30 minutos)  
2. **Sistema Multi-Agente**: Ejecutar el coordinador avanzado con agentes especialistas (30 minutos)  
3. **Enrutamiento Inteligente**: Probar el enrutador de modelos con diferentes tipos de consultas (30 minutos)  
4. **Exploración Interactiva**: Usar los Jupyter notebooks para explorar características avanzadas (45 minutos)  
5. **Despliegue en Producción**: Implementar patrones de monitoreo y manejo de errores (30 minutos)  
6. **Integración Híbrida**: Configurar escenarios de respaldo con Azure AI Foundry (30 minutos)  

## Guía de Asignación de Tiempo  

Para ayudarte a aprovechar al máximo las 20 horas del curso, aquí tienes una sugerencia de cómo distribuir tu tiempo:  

| Actividad | Asignación de Tiempo | Descripción |  
|----------|----------------|-------------|  
| Lectura de Materiales Básicos | 9 horas | Enfocarse en los conceptos esenciales de cada módulo |  
| Ejercicios prácticos | 6 horas | Implementación práctica de técnicas clave |
| Autoevaluación | 2 horas | Evaluar tu comprensión a través de preguntas y reflexión |
| Mini-proyecto | 3 horas | Aplicar conocimientos en una pequeña implementación práctica |

### Áreas clave según la disponibilidad de tiempo

**Si solo tienes 10 horas:**
- Completa el Módulo 0 (Introducción) y los Módulos 1, 2 y 3 (conceptos básicos de EdgeAI)
- Realiza al menos un ejercicio práctico por módulo
- Enfócate en comprender los conceptos principales en lugar de los detalles de implementación

**Si puedes dedicar las 20 horas completas:**
- Completa los ocho módulos (incluyendo la Introducción)
- Realiza los ejercicios prácticos clave de cada módulo
- Completa un mini-proyecto del Módulo 7
- Explora al menos 2-3 recursos complementarios

**Si tienes más de 20 horas:**
- Completa todos los módulos (incluyendo la Introducción) con ejercicios detallados
- Desarrolla múltiples mini-proyectos
- Explora técnicas avanzadas de optimización en el Módulo 4
- Implementa el despliegue en producción desde el Módulo 5

## Recursos esenciales

Estos recursos cuidadosamente seleccionados te proporcionarán el mayor valor para tu tiempo de estudio limitado:

### Documentación imprescindible
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - La herramienta más eficiente para la optimización de modelos
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La forma más rápida de desplegar SLMs localmente
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referencia de un modelo optimizado para Edge
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kit de herramientas de optimización integral de Intel
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Entorno integrado de desarrollo EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plataforma de desarrollo EdgeAI específica para Windows

### Herramientas que ahorran tiempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Acceso rápido a modelos y despliegue
- [Gradio](https://www.gradio.app/docs/interface) - Desarrollo rápido de interfaces para demostraciones de IA
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplificación de la optimización de modelos
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferencia eficiente en CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Marco para la compresión de redes neuronales
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Kit de herramientas para el despliegue de modelos de lenguaje grande

## Plantilla de seguimiento de progreso

Utiliza esta plantilla simplificada para hacer un seguimiento de tu progreso en el curso de 20 horas:

| Módulo | Fecha de finalización | Horas dedicadas | Principales aprendizajes |
|--------|-----------------------|-----------------|--------------------------|
| Módulo 0: Introducción a EdgeAI | | | |
| Módulo 1: Fundamentos de EdgeAI | | | |
| Módulo 2: Fundamentos de SLM | | | |
| Módulo 3: Despliegue de SLM | | | |
| Módulo 4: Optimización de modelos | | | |
| Módulo 5: SLMOps | | | |
| Módulo 6: Agentes de IA | | | |
| Módulo 7: Herramientas de desarrollo | | | |
| Módulo 8: Herramientas locales de Foundry | | | |
| Ejercicios prácticos | | | |
| Mini-proyecto | | | |

## Ideas para mini-proyectos

Considera completar uno de estos proyectos para practicar los conceptos de EdgeAI (cada uno diseñado para tomar entre 2 y 4 horas):

### Proyectos para principiantes (2-3 horas cada uno)
1. **Asistente de texto en Edge**: Crea una herramienta simple de autocompletado de texto offline usando un modelo de lenguaje pequeño
2. **Panel de comparación de modelos**: Construye una visualización básica de métricas de rendimiento entre diferentes SLMs
3. **Experimento de optimización**: Mide el impacto de diferentes niveles de cuantización en el mismo modelo base

### Proyectos intermedios (3-4 horas cada uno)
4. **Flujo de trabajo con AI Toolkit**: Usa el AI Toolkit de VS Code para optimizar y desplegar un modelo de principio a fin
5. **Aplicación con Windows AI Foundry**: Crea una aplicación para Windows usando la API Phi Silica y optimización NPU
6. **Despliegue multiplataforma**: Despliega el mismo modelo optimizado en Windows (OpenVINO) y móvil (.NET MAUI)
7. **Agente con llamadas a funciones**: Construye un agente de IA con capacidades de llamadas a funciones para escenarios en Edge

### Proyectos avanzados de integración (4-5 horas cada uno)
8. **Pipeline de optimización con OpenVINO**: Implementa una optimización completa de modelos usando NNCF y el kit de herramientas GenAI
9. **Pipeline de SLMOps**: Implementa un ciclo de vida completo de modelos desde el entrenamiento hasta el despliegue en Edge
10. **Sistema Edge con múltiples modelos**: Despliega múltiples modelos especializados trabajando juntos en hardware Edge
11. **Sistema de integración MCP**: Construye un sistema agente utilizando el Protocolo de Contexto de Modelos para la interacción con herramientas

## Referencias

- Microsoft Learn (Foundry Local)
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Primeros pasos: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Referencia CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integración con SDKs de inferencia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Cómo abrir WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilar modelos de Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Descripción general: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agentes (descripción general): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Herramientas de optimización e inferencia
  - Microsoft Olive (documentación): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (primeros pasos): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integración de ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (documentación): https://docs.openvino.ai/2025/index.html
  - Apple MLX (documentación): https://ml-explore.github.io/mlx/build/html/index.html
- Marcos de despliegue y modelos
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (documentación): https://docs.vllm.ai/
  - Ollama (primeros pasos): https://github.com/ollama/ollama#get-started
- Herramientas para desarrolladores (Windows y VS Code)
  - AI Toolkit para VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (descripción general): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Comunidad de aprendizaje

Únete a la discusión y conecta con otros estudiantes:
- Discusiones en GitHub en el [repositorio EdgeAI para principiantes](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusión

EdgeAI representa la vanguardia de la implementación de inteligencia artificial, llevando capacidades poderosas directamente a los dispositivos mientras aborda preocupaciones críticas sobre privacidad, latencia y conectividad. Este curso de 20 horas te proporciona el conocimiento esencial y las habilidades prácticas para comenzar a trabajar con tecnologías EdgeAI de inmediato.

El curso está diseñado para ser conciso y enfocado en los conceptos más importantes, permitiéndote adquirir experiencia valiosa rápidamente sin una inversión de tiempo abrumadora. Recuerda que la práctica práctica, incluso con ejemplos simples, es clave para reforzar lo que has aprendido.

¡Feliz aprendizaje!

---

