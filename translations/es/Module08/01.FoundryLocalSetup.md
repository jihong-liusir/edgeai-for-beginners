<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T12:54:46+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "es"
}
-->
# Sesión 1: Introducción a Foundry Local

## Descripción General

Microsoft Foundry Local lleva las capacidades de Azure AI Foundry directamente a tu entorno de desarrollo en Windows 11, permitiendo un desarrollo de IA con herramientas de nivel empresarial que preservan la privacidad y ofrecen baja latencia. Esta sesión cubre la instalación completa, configuración y despliegue práctico de modelos populares como phi, qwen, deepseek y GPT-OSS-20B.

## Objetivos de Aprendizaje

Al finalizar esta sesión, podrás:
- Instalar y configurar Foundry Local en Windows 11
- Dominar comandos CLI y opciones de configuración
- Comprender estrategias de almacenamiento en caché de modelos para un rendimiento óptimo
- Ejecutar con éxito los modelos phi, qwen, deepseek y GPT-OSS-20B
- Crear tu primera aplicación de IA utilizando Foundry Local

## Requisitos Previos

### Requisitos del Sistema
- **Windows 11**: Versión 22H2 o posterior
- **RAM**: Mínimo 16GB, recomendado 32GB
- **Almacenamiento**: 50GB de espacio libre para modelos y caché
- **Hardware**: Dispositivo con NPU o GPU preferido (PC Copilot+ o GPU NVIDIA)
- **Red**: Internet de alta velocidad para descargar modelos

### Entorno de Desarrollo
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## Parte 1: Instalación y Configuración

### Paso 1: Instalar Foundry Local

Instala Foundry Local utilizando Winget o descarga el instalador desde GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Paso 2: Verificar la Instalación

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Parte 2: Comprender el CLI

### Estructura de Comandos Principales

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Parte 3: Gestión y Almacenamiento en Caché de Modelos

Foundry Local implementa un almacenamiento en caché inteligente de modelos para optimizar el rendimiento y el uso de almacenamiento:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Parte 4: Despliegue Práctico de Modelos

### Ejecutar Modelos Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Trabajar con Modelos Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### Ejecutar Modelos DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### Ejecutar GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Parte 5: Crear Tu Primera Aplicación

### Interfaz de Chat Simple (API compatible con OpenAI)

Crea una aplicación básica de chat utilizando la API REST compatible con OpenAI de Foundry Local. Asegúrate de que un modelo esté ejecutándose en otra terminal.

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### Ejecutar la Aplicación de Chat

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## Parte 6: Solución de Problemas y Mejores Prácticas

### Problemas Comunes y Soluciones

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### Monitoreo de Recursos del Sistema (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Mejores Prácticas

- Prefiere los comandos `foundry model ...`, `foundry cache ...` y `foundry service ...` (consulta la referencia CLI)
- Actualiza regularmente para acceder a nuevos modelos y correcciones
- Comienza con modelos más pequeños (Phi mini, Qwen 7B) y escala gradualmente
- Monitorea CPU/GPU/memoria mientras ajustas los prompts y configuraciones

## Parte 7: Ejercicios Prácticos

### Ejercicio 1: Ejecuciones Rápidas de Múltiples Modelos

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Ejercicio 2: Benchmark Básico de Latencia

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## Referencias

- Introducción a Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- Referencia CLI y visión general de comandos: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- Compilar modelos de Hugging Face para Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- GitHub de Microsoft Foundry Local: https://github.com/microsoft/Foundry-Local

---

