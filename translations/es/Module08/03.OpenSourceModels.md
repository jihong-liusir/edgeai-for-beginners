<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T22:56:54+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "es"
}
-->
# Sesión 3: Descubrimiento y Gestión de Modelos Open-Source

## Resumen

Esta sesión se centra en el descubrimiento y la gestión práctica de modelos con Foundry Local. Aprenderás a listar modelos disponibles, probar diferentes opciones y comprender características básicas de rendimiento. El enfoque destaca la exploración práctica con la CLI de Foundry para ayudarte a seleccionar los modelos adecuados para tus casos de uso.

## Objetivos de Aprendizaje

- Dominar los comandos de la CLI de Foundry para el descubrimiento y la gestión de modelos
- Comprender los patrones de caché de modelos y almacenamiento local
- Aprender a probar y comparar rápidamente diferentes modelos
- Establecer flujos de trabajo prácticos para la selección y evaluación de modelos
- Explorar el ecosistema creciente de modelos disponibles a través de Foundry Local

## Prerrequisitos

- Haber completado la Sesión 1: Introducción a Foundry Local
- Tener instalada y accesible la CLI de Foundry Local
- Espacio de almacenamiento suficiente para descargar modelos (los modelos pueden variar entre 1GB y más de 20GB)
- Comprensión básica de los tipos de modelos y casos de uso

## Parte 6: Ejercicio Práctico

### Ejercicio: Descubrimiento y Comparación de Modelos

Crea tu propio script de evaluación de modelos basado en el Ejemplo 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Tu Tarea

1. **Ejecuta el script del Ejemplo 03**: `samples\03\list_and_bench.cmd`
2. **Prueba diferentes modelos**: Evalúa al menos 3 modelos distintos
3. **Compara el rendimiento**: Observa las diferencias en velocidad y calidad de respuesta
4. **Documenta tus hallazgos**: Crea un gráfico de comparación sencillo

### Formato de Comparación Ejemplo

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Parte 7: Solución de Problemas y Mejores Prácticas

### Problemas Comunes y Soluciones

**El modelo no inicia:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Memoria insuficiente:**
- Comienza con modelos más pequeños (`phi-4-mini`)
- Cierra otras aplicaciones
- Actualiza la RAM si frecuentemente alcanzas los límites

**Rendimiento lento:**
- Asegúrate de que el modelo esté completamente cargado (verifica la salida detallada)
- Cierra aplicaciones en segundo plano innecesarias
- Considera almacenamiento más rápido (SSD)

### Mejores Prácticas

1. **Comienza pequeño**: Usa `phi-4-mini` para validar la configuración
2. **Un modelo a la vez**: Detén modelos anteriores antes de iniciar nuevos
3. **Monitorea recursos**: Observa el uso de memoria
4. **Prueba de manera consistente**: Usa los mismos prompts para comparaciones justas
5. **Documenta resultados**: Toma notas sobre el rendimiento de los modelos para tus casos de uso

## Parte 8: Próximos Pasos y Referencias

### Preparación para la Sesión 4

- **Enfoque de la Sesión 4**: Herramientas y técnicas de optimización
- **Prerrequisitos**: Familiaridad con el cambio de modelos y pruebas básicas de rendimiento
- **Recomendado**: Identificar 2-3 modelos favoritos de esta sesión

### Recursos Adicionales

- **[Documentación de Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Documentación oficial
- **[Referencia CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Referencia completa de comandos
- **[Model Mondays](https://aka.ms/model-mondays)**: Destacados semanales de modelos
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Comunidad y problemas
- **[Ejemplo 03: Descubrimiento de Modelos](samples/03/README.md)**: Script práctico de ejemplo

### Puntos Clave

✅ **Descubrimiento de Modelos**: Usa `foundry model list` para explorar modelos disponibles  
✅ **Pruebas Rápidas**: El patrón `list_and_bench.cmd` para evaluaciones rápidas  
✅ **Monitoreo de Rendimiento**: Uso básico de recursos y medición de tiempos de respuesta  
✅ **Selección de Modelos**: Guías prácticas para elegir modelos según el caso de uso  
✅ **Gestión de Caché**: Comprender procedimientos de almacenamiento y limpieza  

Ahora tienes las habilidades prácticas para descubrir, probar y seleccionar modelos apropiados para tus aplicaciones de IA utilizando el enfoque sencillo de la CLI de Foundry Local: seleccionando modelos de la comunidad, integrando contenido de Hugging Face y adoptando estrategias de “trae tu propio modelo” (BYOM). También descubrirás la serie Model Mondays para aprendizaje continuo y descubrimiento de modelos.

Referencias:
- Documentación de Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compilar modelos de Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Objetivos de Aprendizaje
- Descubrir y evaluar modelos open-source para inferencia local
- Compilar y ejecutar modelos seleccionados de Hugging Face dentro de Foundry Local
- Aplicar estrategias de selección de modelos para precisión, latencia y necesidades de recursos
- Gestionar modelos localmente con caché y versionado

## Parte 1: Descubrimiento de Modelos con Foundry CLI

### Comandos Básicos de Gestión de Modelos

La CLI de Foundry proporciona comandos sencillos para el descubrimiento y la gestión de modelos:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Ejecutando tus Primeros Modelos

Comienza con modelos populares y bien probados para comprender las características de rendimiento:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Nota:** La opción `--verbose` proporciona información detallada de inicio, incluyendo:
- Progreso de descarga del modelo (en la primera ejecución)
- Detalles de asignación de memoria
- Información de vinculación del servicio
- Métricas de inicialización de rendimiento

### Comprendiendo las Categorías de Modelos

**Modelos de Lenguaje Pequeños (SLMs):**
- `phi-4-mini`: Rápido, eficiente, ideal para chat general
- `phi-4`: Versión más capaz con mejor razonamiento

**Modelos Medianos:**
- `qwen2.5-7b`: Excelente razonamiento y contexto más largo
- `deepseek-r1-7b`: Optimizado para generación de código

**Modelos Grandes:**
- `llama-3.2`: El último modelo open-source de Meta
- `qwen2.5-14b`: Razonamiento de nivel empresarial

## Parte 2: Pruebas Rápidas y Comparación de Modelos

### Enfoque del Ejemplo 03: Lista y Evaluación Simple

Basado en nuestro patrón del Ejemplo 03, aquí está el flujo de trabajo mínimo:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Probando el Rendimiento de los Modelos

Una vez que un modelo esté en ejecución, pruébalo con prompts consistentes:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternativa de Pruebas con PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Parte 3: Gestión de Caché y Almacenamiento de Modelos

### Comprendiendo el Caché de Modelos

Foundry Local gestiona automáticamente las descargas y el caché de modelos:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Consideraciones de Almacenamiento de Modelos

**Tamaños Típicos de Modelos:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Mejores Prácticas de Almacenamiento:**
- Mantén 2-3 modelos en caché para cambios rápidos
- Elimina modelos no utilizados para liberar espacio: `foundry cache clean`
- Monitorea el uso del disco, especialmente en SSDs más pequeños
- Considera el tamaño del modelo frente a las capacidades

### Monitoreo del Rendimiento de Modelos

Mientras los modelos están en ejecución, monitorea los recursos del sistema:

**Administrador de Tareas de Windows:**
- Observa el uso de memoria (los modelos permanecen cargados en RAM)
- Monitorea la utilización de CPU durante la inferencia
- Revisa el I/O del disco durante la carga inicial del modelo

**Monitoreo desde la Línea de Comandos:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Parte 4: Guías Prácticas para la Selección de Modelos

### Elegir Modelos según el Caso de Uso

**Para Chat General y Preguntas y Respuestas:**
- Comienza con: `phi-4-mini` (rápido, eficiente)
- Mejora a: `phi-4` (mejor razonamiento)
- Avanzado: `qwen2.5-7b` (contexto más largo)

**Para Generación de Código:**
- Recomendado: `deepseek-r1-7b`
- Alternativa: `qwen2.5-7b` (también bueno para código)

**Para Razonamiento Complejo:**
- Mejor opción: `qwen2.5-7b` o `qwen2.5-14b`
- Opción económica: `phi-4`

### Guía de Requisitos de Hardware

**Requisitos Mínimos del Sistema:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Recomendado para Mejor Rendimiento:**
- 32GB+ de RAM para cambios cómodos entre modelos
- Almacenamiento SSD para carga más rápida de modelos
- CPU moderna con buen rendimiento de un solo hilo
- Soporte NPU (PCs con Windows 11 Copilot+) para aceleración

### Flujo de Trabajo para Cambiar Modelos

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Parte 5: Evaluación Simple de Modelos

### Pruebas Básicas de Rendimiento

Aquí tienes un enfoque sencillo para comparar el rendimiento de modelos:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Evaluación Manual de Calidad

Para cada modelo, pruébalo con prompts consistentes y evalúa manualmente:

**Prompts de Prueba:**
1. "Explica la computación cuántica en términos simples."
2. "Escribe una función en Python para ordenar una lista."
3. "¿Cuáles son las ventajas y desventajas del trabajo remoto?"
4. "Resume los beneficios de la IA en el borde."

**Criterios de Evaluación:**
- **Precisión**: ¿La información es correcta?
- **Claridad**: ¿La explicación es fácil de entender?
- **Completitud**: ¿Responde completamente a la pregunta?
- **Velocidad**: ¿Qué tan rápido responde?

### Monitoreo del Uso de Recursos

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Parte 6: Próximos Pasos
- Suscríbete a Model Mondays para nuevos modelos y consejos: https://aka.ms/model-mondays
- Contribuye con tus hallazgos al `models.json` de tu equipo
- Prepárate para la Sesión 4: comparando LLMs vs SLMs, inferencia local vs en la nube, y demostraciones prácticas

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.