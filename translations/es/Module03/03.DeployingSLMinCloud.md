<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T13:50:17+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "es"
}
-->
# Despliegue en la Nube con Contenedores - Soluciones a Escala de Producción

Este tutorial completo cubre tres enfoques principales para desplegar el modelo Phi-4-mini-instruct de Microsoft en entornos con contenedores: vLLM, Ollama y SLM Engine con ONNX Runtime. Este modelo de 3.8B parámetros representa una elección óptima para tareas de razonamiento mientras mantiene eficiencia para despliegues en el borde.

## Tabla de Contenidos

1. [Introducción al Despliegue en Contenedores de Phi-4-mini](../../../Module03)
2. [Objetivos de Aprendizaje](../../../Module03)
3. [Comprensión de la Clasificación de Phi-4-mini](../../../Module03)
4. [Despliegue en Contenedores con vLLM](../../../Module03)
5. [Despliegue en Contenedores con Ollama](../../../Module03)
6. [SLM Engine con ONNX Runtime](../../../Module03)
7. [Marco de Comparación](../../../Module03)
8. [Mejores Prácticas](../../../Module03)

## Introducción al Despliegue en Contenedores de Phi-4-mini

Los Modelos de Lenguaje Pequeños (SLMs) representan un avance crucial en EdgeAI, permitiendo capacidades sofisticadas de procesamiento de lenguaje natural en dispositivos con recursos limitados. Este tutorial se centra en estrategias de despliegue en contenedores para Phi-4-mini-instruct de Microsoft, un modelo de razonamiento de última generación que equilibra capacidad y eficiencia.

### Modelo Destacado: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B parámetros)**: El último modelo ligero ajustado por instrucciones de Microsoft diseñado para entornos con restricciones de memoria/cómputo, con capacidades excepcionales en:
- **Razonamiento matemático y cálculos complejos**
- **Generación, depuración y análisis de código**
- **Resolución de problemas lógicos y razonamiento paso a paso**
- **Aplicaciones educativas que requieren explicaciones detalladas**
- **Llamadas a funciones e integración de herramientas**

Parte de la categoría "Pequeños SLMs" (1.5B - 13.9B parámetros), Phi-4-mini logra un equilibrio óptimo entre capacidad de razonamiento y eficiencia de recursos.

### Beneficios del Despliegue en Contenedores de Phi-4-mini

- **Eficiencia Operativa**: Inferencia rápida para tareas de razonamiento con menores requisitos computacionales
- **Flexibilidad de Despliegue**: Capacidades de IA en el dispositivo con mayor privacidad mediante procesamiento local
- **Rentabilidad**: Costos operativos reducidos en comparación con modelos más grandes mientras se mantiene la calidad
- **Aislamiento**: Separación limpia entre instancias del modelo y entornos de ejecución seguros
- **Escalabilidad**: Escalado horizontal sencillo para aumentar el rendimiento de razonamiento

## Objetivos de Aprendizaje

Al final de este tutorial, podrás:

- Desplegar y optimizar Phi-4-mini-instruct en diversos entornos con contenedores
- Implementar estrategias avanzadas de cuantización y compresión para diferentes escenarios de despliegue
- Configurar orquestación de contenedores lista para producción para cargas de trabajo de razonamiento
- Evaluar y seleccionar marcos de despliegue apropiados según los requisitos de casos de uso específicos
- Aplicar mejores prácticas de seguridad, monitoreo y escalado para despliegues de SLM en contenedores

## Comprensión de la Clasificación de Phi-4-mini

### Especificaciones del Modelo

**Detalles Técnicos:**
- **Parámetros**: 3.8 mil millones (categoría de Pequeños SLM)
- **Arquitectura**: Transformer denso solo decodificador con atención de consulta agrupada
- **Longitud de Contexto**: 128K tokens (32K recomendados para un rendimiento óptimo)
- **Vocabulario**: 200K tokens con soporte multilingüe
- **Datos de Entrenamiento**: 5T tokens de contenido de alta calidad y denso en razonamiento

### Requisitos de Recursos

| Tipo de Despliegue | RAM Mínima | RAM Recomendada | VRAM (GPU) | Almacenamiento | Casos de Uso Típicos |
|--------------------|------------|-----------------|------------|----------------|-----------------------|
| **Desarrollo** | 6GB | 8GB | - | 8GB | Pruebas locales, prototipos |
| **Producción CPU** | 8GB | 12GB | - | 10GB | Servidores en el borde, despliegue optimizado en costos |
| **Producción GPU** | 6GB | 8GB | 4-6GB | 8GB | Servicios de razonamiento de alto rendimiento |
| **Optimizado para el Borde** | 4GB | 6GB | - | 6GB | Despliegue cuantizado, puertas de enlace IoT |

### Capacidades de Phi-4-mini

- **Excelencia Matemática**: Resolución avanzada de problemas de aritmética, álgebra y cálculo
- **Inteligencia de Código**: Generación de código en Python, JavaScript y múltiples lenguajes con depuración
- **Razonamiento Lógico**: Descomposición paso a paso de problemas y construcción de soluciones
- **Apoyo Educativo**: Explicaciones detalladas adecuadas para escenarios de aprendizaje y enseñanza
- **Llamadas a Funciones**: Soporte nativo para integración de herramientas e interacciones con APIs

## Despliegue en Contenedores con vLLM

vLLM ofrece un excelente soporte para Phi-4-mini-instruct con un rendimiento de inferencia optimizado y APIs compatibles con OpenAI, lo que lo hace ideal para servicios de razonamiento en producción.

### Ejemplos de Inicio Rápido

#### Despliegue Básico en CPU (Desarrollo)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Despliegue en Producción Acelerado por GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Configuración de Producción

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pruebas de Capacidades de Razonamiento de Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Despliegue en Contenedores con Ollama

Ollama ofrece un excelente soporte para Phi-4-mini-instruct con un despliegue y gestión simplificados, lo que lo hace ideal para desarrollo y despliegues equilibrados en producción.

### Configuración Rápida

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Configuración de Producción

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimización del Modelo y Variantes

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Ejemplos de Uso de API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine con ONNX Runtime

ONNX Runtime proporciona un rendimiento óptimo para el despliegue en el borde de Phi-4-mini-instruct con optimización avanzada y compatibilidad multiplataforma.

### Configuración Básica

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Implementación Simplificada de Servidor

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script de Conversión de Modelo

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Configuración de Producción

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pruebas de Despliegue con ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Marco de Comparación

### Comparación de Marcos para Phi-4-mini

| Característica | vLLM | Ollama | ONNX Runtime |
|----------------|------|--------|--------------|
| **Complejidad de Configuración** | Moderada | Fácil | Compleja |
| **Rendimiento (GPU)** | Excelente (~25 tok/s) | Muy Bueno (~20 tok/s) | Bueno (~15 tok/s) |
| **Rendimiento (CPU)** | Bueno (~8 tok/s) | Muy Bueno (~12 tok/s) | Excelente (~15 tok/s) |
| **Uso de Memoria** | 8-12GB | 6-10GB | 4-8GB |
| **Compatibilidad con API** | Compatible con OpenAI | REST Personalizado | FastAPI Personalizado |
| **Llamadas a Funciones** | ✅ Nativo | ✅ Soportado | ⚠️ Implementación Personalizada |
| **Soporte de Cuantización** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Cuantización ONNX |
| **Listo para Producción** | ✅ Excelente | ✅ Muy Bueno | ✅ Bueno |
| **Despliegue en el Borde** | Bueno | Excelente | Sobresaliente |

## Recursos Adicionales

### Documentación Oficial
- **Microsoft Phi-4 Model Card**: Especificaciones detalladas y guías de uso
- **Documentación de vLLM**: Opciones avanzadas de configuración y optimización
- **Biblioteca de Modelos de Ollama**: Modelos comunitarios y ejemplos de personalización
- **Guías de ONNX Runtime**: Estrategias de optimización de rendimiento y despliegue

### Herramientas de Desarrollo
- **Transformers de Hugging Face**: Para interacción y personalización del modelo
- **Especificación de API de OpenAI**: Para pruebas de compatibilidad con vLLM
- **Mejores Prácticas de Docker**: Seguridad y optimización de contenedores
- **Despliegue con Kubernetes**: Patrones de orquestación para escalado en producción

### Recursos de Aprendizaje
- **Benchmarking de Rendimiento de SLM**: Metodologías de análisis comparativo
- **Despliegue de Edge AI**: Mejores prácticas para entornos con recursos limitados
- **Optimización de Tareas de Razonamiento**: Estrategias de prompting para problemas matemáticos y lógicos
- **Seguridad en Contenedores**: Prácticas de endurecimiento para despliegues de modelos de IA

## Resultados de Aprendizaje

Después de completar este módulo, podrás:

1. Desplegar el modelo Phi-4-mini-instruct en entornos con contenedores utilizando múltiples marcos
2. Configurar y optimizar despliegues de SLM para diferentes entornos de hardware
3. Implementar mejores prácticas de seguridad para despliegues de IA en contenedores
4. Comparar y seleccionar marcos de despliegue apropiados según los requisitos de casos de uso específicos
5. Aplicar estrategias de monitoreo y escalado para servicios SLM de grado de producción

## ¿Qué sigue?

- Regresar a [Módulo 1](../Module01/README.md)
- Regresar a [Módulo 2](../Module02/README.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.