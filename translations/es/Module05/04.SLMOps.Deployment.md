<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T13:42:44+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "es"
}
-->
# Secci√≥n 4: Despliegue - Implementaci√≥n de un Modelo Listo para Producci√≥n

## Descripci√≥n General

Este tutorial completo te guiar√° a trav√©s del proceso completo de desplegar modelos cuantizados ajustados utilizando Foundry Local. Cubriremos la conversi√≥n del modelo, la optimizaci√≥n de cuantizaci√≥n y la configuraci√≥n de despliegue de principio a fin.

## Requisitos Previos

Antes de comenzar, aseg√∫rate de tener lo siguiente:

- ‚úÖ Un modelo onnx ajustado listo para el despliegue
- ‚úÖ Computadora con Windows o Mac
- ‚úÖ Python 3.10 o superior
- ‚úÖ Al menos 8GB de RAM disponible
- ‚úÖ Foundry Local instalado en tu sistema

## Parte 1: Configuraci√≥n del Entorno

### Instalaci√≥n de Herramientas Requeridas

Abre tu terminal (Command Prompt en Windows, Terminal en Mac) y ejecuta los siguientes comandos en secuencia:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Nota Importante**: Tambi√©n necesitar√°s la versi√≥n 3.31 o m√°s reciente de CMake, que puede descargarse desde [cmake.org](https://cmake.org/download/).

## Parte 2: Conversi√≥n y Cuantizaci√≥n del Modelo

### Elegir el Formato Correcto

Para modelos peque√±os de lenguaje ajustados, recomendamos usar el formato **ONNX** porque ofrece:

- üöÄ Mejor optimizaci√≥n de rendimiento
- üîß Despliegue independiente del hardware
- üè≠ Capacidades listas para producci√≥n
- üì± Compatibilidad multiplataforma

### M√©todo 1: Conversi√≥n con un Solo Comando (Recomendado)

Utiliza el siguiente comando para convertir directamente tu modelo ajustado:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Explicaci√≥n de Par√°metros:**
- `--model_name_or_path`: Ruta de tu modelo ajustado
- `--device cpu`: Usa CPU para la optimizaci√≥n
- `--precision int4`: Usa cuantizaci√≥n INT4 (reducci√≥n de tama√±o aproximadamente del 75%)
- `--output_path`: Ruta de salida para el modelo convertido

### M√©todo 2: Enfoque con Archivo de Configuraci√≥n (Usuarios Avanzados)

Crea un archivo de configuraci√≥n llamado `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Luego ejecuta:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Comparaci√≥n de Opciones de Cuantizaci√≥n

| Precisi√≥n | Tama√±o del Archivo | Velocidad de Inferencia | Calidad del Modelo | Uso Recomendado |
|-----------|--------------------|-------------------------|--------------------|-----------------|
| FP16      | L√≠nea base √ó 0.5   | R√°pida                 | Mejor              | Hardware de gama alta |
| INT8      | L√≠nea base √ó 0.25  | Muy r√°pida             | Buena              | Elecci√≥n equilibrada |
| INT4      | L√≠nea base √ó 0.125 | M√°s r√°pida             | Aceptable          | Recursos limitados |

üí° **Recomendaci√≥n**: Comienza con la cuantizaci√≥n INT4 para tu primer despliegue. Si la calidad no es satisfactoria, prueba con INT8 o FP16.

## Parte 3: Configuraci√≥n de Despliegue en Foundry Local

### Creaci√≥n de la Configuraci√≥n del Modelo

Navega al directorio de modelos de Foundry Local:

```bash
foundry cache cd ./models/
```

Crea la estructura del directorio de tu modelo:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Crea el archivo de configuraci√≥n `inference_model.json` en el directorio de tu modelo:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configuraciones de Plantilla Espec√≠ficas del Modelo

#### Para Modelos de la Serie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Parte 4: Pruebas y Optimizaci√≥n del Modelo

### Verificaci√≥n de la Instalaci√≥n del Modelo

Comprueba si Foundry Local puede reconocer tu modelo:

```bash
foundry cache ls
```

Deber√≠as ver `your-finetuned-model-int4` en la lista.

### Inicio de las Pruebas del Modelo

```bash
foundry model run your-finetuned-model-int4
```

### Evaluaci√≥n de Rendimiento

Monitorea m√©tricas clave durante las pruebas:

1. **Tiempo de Respuesta**: Mide el tiempo promedio por respuesta
2. **Uso de Memoria**: Monitorea el consumo de RAM
3. **Utilizaci√≥n de CPU**: Revisa la carga del procesador
4. **Calidad de Salida**: Eval√∫a la relevancia y coherencia de las respuestas

### Lista de Verificaci√≥n para Validaci√≥n de Calidad

- ‚úÖ El modelo responde adecuadamente a consultas del dominio ajustado
- ‚úÖ El formato de respuesta coincide con la estructura de salida esperada
- ‚úÖ No hay fugas de memoria durante un uso prolongado
- ‚úÖ Rendimiento consistente en diferentes longitudes de entrada
- ‚úÖ Manejo adecuado de casos l√≠mite y entradas inv√°lidas

## Resumen

¬°Felicidades! Has completado con √©xito:

- ‚úÖ Conversi√≥n de formato del modelo ajustado
- ‚úÖ Optimizaci√≥n de cuantizaci√≥n del modelo
- ‚úÖ Configuraci√≥n de despliegue en Foundry Local
- ‚úÖ Ajuste de rendimiento y resoluci√≥n de problemas

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.