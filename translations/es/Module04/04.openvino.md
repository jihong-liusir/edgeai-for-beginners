<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T13:24:12+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "es"
}
-->
# Sección 4: Suite de Optimización OpenVINO Toolkit

## Tabla de Contenidos
1. [Introducción](../../../Module04)
2. [¿Qué es OpenVINO?](../../../Module04)
3. [Instalación](../../../Module04)
4. [Guía Rápida](../../../Module04)
5. [Ejemplo: Convertir y Optimizar Modelos con OpenVINO](../../../Module04)
6. [Uso Avanzado](../../../Module04)
7. [Mejores Prácticas](../../../Module04)
8. [Solución de Problemas](../../../Module04)
9. [Recursos Adicionales](../../../Module04)

## Introducción

OpenVINO (Optimización de Redes Neuronales e Inferencia Visual Abierta) es el toolkit de código abierto de Intel para implementar soluciones de IA de alto rendimiento en la nube, entornos locales y dispositivos de borde. Ya sea que estés trabajando con CPUs, GPUs, VPUs o aceleradores de IA especializados, OpenVINO ofrece capacidades de optimización completas mientras mantiene la precisión del modelo y permite la implementación multiplataforma.

## ¿Qué es OpenVINO?

OpenVINO es un toolkit de código abierto que permite a los desarrolladores optimizar, convertir e implementar modelos de IA de manera eficiente en diversas plataformas de hardware. Consta de tres componentes principales: OpenVINO Runtime para inferencia, Neural Network Compression Framework (NNCF) para optimización de modelos y OpenVINO Model Server para implementación escalable.

### Características Clave

- **Implementación Multiplataforma**: Compatible con Linux, Windows y macOS, con APIs en Python, C++ y C.
- **Aceleración de Hardware**: Descubrimiento automático de dispositivos y optimización para CPU, GPU, VPU y aceleradores de IA.
- **Framework de Compresión de Modelos**: Técnicas avanzadas de cuantización, poda y optimización a través de NNCF.
- **Compatibilidad con Frameworks**: Soporte directo para modelos de TensorFlow, ONNX, PaddlePaddle y PyTorch.
- **Soporte para IA Generativa**: OpenVINO GenAI especializado para implementar modelos de lenguaje y aplicaciones de IA generativa.

### Beneficios

- **Optimización de Rendimiento**: Mejoras significativas en velocidad con mínima pérdida de precisión.
- **Huella de Implementación Reducida**: Dependencias externas mínimas simplifican la instalación y el despliegue.
- **Tiempo de Inicio Mejorado**: Carga y almacenamiento en caché de modelos optimizados para una inicialización más rápida.
- **Implementación Escalable**: Desde dispositivos de borde hasta infraestructura en la nube con APIs consistentes.
- **Listo para Producción**: Fiabilidad de nivel empresarial con documentación completa y soporte comunitario.

## Instalación

### Requisitos Previos

- Python 3.8 o superior
- Administrador de paquetes pip
- Entorno virtual (recomendado)
- Hardware compatible (se recomiendan CPUs Intel, pero admite varias arquitecturas)

### Instalación Básica

Crea y activa un entorno virtual:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Instala OpenVINO Runtime:

```bash
pip install openvino
```

Instala NNCF para optimización de modelos:

```bash
pip install nncf
```

### Instalación de OpenVINO GenAI

Para aplicaciones de IA generativa:

```bash
pip install openvino-genai
```

### Dependencias Opcionales

Paquetes adicionales para casos de uso específicos:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verificar Instalación

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Si la instalación es exitosa, deberías ver la información de la versión de OpenVINO.

## Guía Rápida

### Tu Primera Optimización de Modelo

Vamos a convertir y optimizar un modelo de Hugging Face usando OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Qué Hace Este Proceso

El flujo de trabajo de optimización incluye: cargar el modelo original desde Hugging Face, convertirlo al formato de Representación Intermedia (IR) de OpenVINO, aplicar optimizaciones predeterminadas y compilarlo para el hardware objetivo.

### Parámetros Clave Explicados

- `export=True`: Convierte el modelo al formato IR de OpenVINO.
- `compile=False`: Retrasa la compilación hasta el tiempo de ejecución para mayor flexibilidad.
- `device`: Hardware objetivo ("CPU", "GPU", "AUTO" para selección automática).
- `save_pretrained()`: Guarda el modelo optimizado para reutilización.

## Ejemplo: Convertir y Optimizar Modelos con OpenVINO

### Paso 1: Conversión de Modelo con Cuantización NNCF

Aquí se muestra cómo aplicar cuantización posterior al entrenamiento usando NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Paso 2: Optimización Avanzada con Compresión de Pesos

Para modelos basados en transformadores, aplica compresión de pesos:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Paso 3: Inferencia con el Modelo Optimizado

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Estructura de Salida

Después de la optimización, el directorio de tu modelo contendrá:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Uso Avanzado

### Configuración con NNCF YAML

Para flujos de trabajo de optimización complejos, utiliza archivos de configuración NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Aplica la configuración:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimización para GPU

Para aceleración en GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimización de Procesamiento por Lotes

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Implementación con Model Server

Implementa modelos optimizados con OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Código cliente para el servidor de modelos:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Mejores Prácticas

### 1. Selección y Preparación de Modelos
- Utiliza modelos de frameworks compatibles (PyTorch, TensorFlow, ONNX).
- Asegúrate de que las entradas del modelo tengan formas fijas o dinámicas conocidas.
- Prueba con conjuntos de datos representativos para calibración.

### 2. Selección de Estrategia de Optimización
- **Cuantización Posterior al Entrenamiento**: Comienza aquí para una optimización rápida.
- **Compresión de Pesos**: Ideal para modelos de lenguaje y transformadores grandes.
- **Entrenamiento Consciente de Cuantización**: Úsalo cuando la precisión sea crítica.

### 3. Optimización Específica de Hardware
- **CPU**: Usa cuantización INT8 para un rendimiento equilibrado.
- **GPU**: Aprovecha la precisión FP16 y el procesamiento por lotes.
- **VPU**: Enfócate en la simplificación del modelo y la fusión de capas.

### 4. Ajuste de Rendimiento
- **Modo de Rendimiento**: Para procesamiento por lotes de alto volumen.
- **Modo de Latencia**: Para aplicaciones interactivas en tiempo real.
- **Dispositivo AUTO**: Permite que OpenVINO seleccione el hardware óptimo.

### 5. Gestión de Memoria
- Usa formas dinámicas con cuidado para evitar sobrecarga de memoria.
- Implementa almacenamiento en caché de modelos para cargas más rápidas.
- Monitorea el uso de memoria durante la optimización.

### 6. Validación de Precisión
- Siempre valida los modelos optimizados frente al rendimiento original.
- Usa conjuntos de prueba representativos para evaluación.
- Considera una optimización gradual (comienza con configuraciones conservadoras).

## Solución de Problemas

### Problemas Comunes

#### 1. Problemas de Instalación
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Errores de Conversión de Modelos
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Problemas de Rendimiento
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Problemas de Memoria
- Reduce el tamaño de lote del modelo durante la optimización.
- Usa transmisión para conjuntos de datos grandes.
- Habilita almacenamiento en caché del modelo: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`.

#### 5. Degradación de Precisión
- Usa mayor precisión (INT8 en lugar de INT4).
- Aumenta el tamaño del conjunto de datos de calibración.
- Aplica optimización de precisión mixta.

### Monitoreo de Rendimiento

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Obtener Ayuda

- **Documentación**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Problemas en GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Foro Comunitario**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Recursos Adicionales

### Enlaces Oficiales
- **Página Principal de OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Repositorio GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Repositorio NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Recursos de Aprendizaje
- **Notebooks de OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Guía Rápida**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Guía de Optimización**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Herramientas de Integración
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmarks de Rendimiento
- **Benchmarks Oficiales**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Ejemplos Comunitarios
- **Notebooks Jupyter**: [Repositorio de Notebooks de OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Tutoriales completos disponibles en el repositorio de notebooks de OpenVINO.
- **Aplicaciones de Ejemplo**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Ejemplos del mundo real para varios dominios (visión por computadora, NLP, audio).
- **Publicaciones de Blog**: [Blog de Intel AI](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Publicaciones de blog de Intel AI y la comunidad con casos de uso detallados.

### Herramientas Relacionadas
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Técnicas adicionales de optimización para hardware Intel.
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Para comparaciones de implementación en dispositivos móviles y de borde.
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativas de motor de inferencia multiplataforma.

## ➡️ ¿Qué sigue?

- [05: Análisis Detallado del Framework Apple MLX](./05.AppleMLX.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.