<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T13:28:57+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "es"
}
-->
# Sección 3: Microsoft Olive Optimization Suite

## Tabla de Contenidos
1. [Introducción](../../../Module04)
2. [¿Qué es Microsoft Olive?](../../../Module04)
3. [Instalación](../../../Module04)
4. [Guía Rápida](../../../Module04)
5. [Ejemplo: Convertir Qwen3 a ONNX INT4](../../../Module04)
6. [Uso Avanzado](../../../Module04)
7. [Mejores Prácticas](../../../Module04)
8. [Solución de Problemas](../../../Module04)
9. [Recursos Adicionales](../../../Module04)

## Introducción

Microsoft Olive es una herramienta de optimización de modelos consciente del hardware, potente y fácil de usar, que simplifica el proceso de optimización de modelos de aprendizaje automático para su implementación en diferentes plataformas de hardware. Ya sea que estés trabajando con CPUs, GPUs o aceleradores de IA especializados, Olive te ayuda a lograr un rendimiento óptimo mientras mantiene la precisión del modelo.

## ¿Qué es Microsoft Olive?

Olive es una herramienta de optimización de modelos consciente del hardware que integra técnicas líderes en la industria para compresión, optimización y compilación de modelos. Funciona con ONNX Runtime como una solución de optimización de inferencia de extremo a extremo.

### Características Principales

- **Optimización Consciente del Hardware**: Selecciona automáticamente las mejores técnicas de optimización para tu hardware objetivo.
- **Más de 40 Componentes de Optimización Integrados**: Incluye compresión de modelos, cuantización, optimización de gráficos y más.
- **Interfaz CLI Fácil de Usar**: Comandos simples para tareas comunes de optimización.
- **Soporte Multiplataforma**: Compatible con PyTorch, modelos de Hugging Face y ONNX.
- **Soporte para Modelos Populares**: Olive puede optimizar automáticamente arquitecturas de modelos populares como Llama, Phi, Qwen, Gemma, etc., sin configuración adicional.

### Beneficios

- **Reducción del Tiempo de Desarrollo**: No es necesario experimentar manualmente con diferentes técnicas de optimización.
- **Mejoras en el Rendimiento**: Incrementos significativos en velocidad (hasta 6x en algunos casos).
- **Implementación Multiplataforma**: Los modelos optimizados funcionan en diferentes hardware y sistemas operativos.
- **Precisión Mantenida**: Las optimizaciones preservan la calidad del modelo mientras mejoran el rendimiento.

## Instalación

### Requisitos Previos

- Python 3.8 o superior
- Administrador de paquetes pip
- Entorno virtual (recomendado)

### Instalación Básica

Crea y activa un entorno virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instala Olive con funciones de auto-optimización:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependencias Opcionales

Olive ofrece varias dependencias opcionales para funciones adicionales:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificar Instalación

```bash
olive --help
```

Si la instalación es exitosa, deberías ver el mensaje de ayuda de Olive CLI.

## Guía Rápida

### Tu Primera Optimización

Vamos a optimizar un modelo de lenguaje pequeño utilizando la función de auto-optimización de Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Qué Hace Este Comando

El proceso de optimización incluye: adquirir el modelo desde la caché local, capturar el gráfico ONNX y almacenar los pesos en un archivo de datos ONNX, optimizar el gráfico ONNX y cuantizar el modelo a int4 utilizando el método RTN.

### Parámetros del Comando Explicados

- `--model_name_or_path`: Identificador del modelo de Hugging Face o ruta local.
- `--output_path`: Directorio donde se guardará el modelo optimizado.
- `--device`: Dispositivo objetivo (cpu, gpu).
- `--provider`: Proveedor de ejecución (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider).
- `--use_ort_genai`: Usar ONNX Runtime Generate AI para inferencia.
- `--precision`: Precisión de cuantización (int4, int8, fp16).
- `--log_level`: Nivel de detalle en los registros (0=mínimo, 1=detallado).

## Ejemplo: Convertir Qwen3 a ONNX INT4

Basado en el ejemplo proporcionado por Hugging Face en [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), aquí se muestra cómo optimizar un modelo Qwen3:

### Paso 1: Descargar el Modelo (Opcional)

Para minimizar el tiempo de descarga, almacena en caché solo los archivos esenciales:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Paso 2: Optimizar el Modelo Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Paso 3: Probar el Modelo Optimizado

Crea un script simple en Python para probar tu modelo optimizado:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Estructura de Salida

Después de la optimización, tu directorio de salida contendrá:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Uso Avanzado

### Archivos de Configuración

Para flujos de trabajo de optimización más complejos, puedes usar archivos de configuración JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Ejecuta con configuración:

```bash
olive run --config config.json
```

### Optimización para GPU

Para optimización con GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ajuste Fino con Olive

Olive también admite el ajuste fino de modelos:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mejores Prácticas

### 1. Selección de Modelos
- Comienza con modelos más pequeños para pruebas (por ejemplo, 0.5B-7B parámetros).
- Asegúrate de que la arquitectura de tu modelo objetivo sea compatible con Olive.

### 2. Consideraciones de Hardware
- Ajusta tu objetivo de optimización al hardware de implementación.
- Usa optimización para GPU si tienes hardware compatible con CUDA.
- Considera DirectML para máquinas Windows con gráficos integrados.

### 3. Selección de Precisión
- **INT4**: Máxima compresión, ligera pérdida de precisión.
- **INT8**: Buen equilibrio entre tamaño y precisión.
- **FP16**: Pérdida mínima de precisión, reducción moderada de tamaño.

### 4. Pruebas y Validación
- Siempre prueba los modelos optimizados con tus casos de uso específicos.
- Compara métricas de rendimiento (latencia, rendimiento, precisión).
- Usa datos de entrada representativos para la evaluación.

### 5. Optimización Iterativa
- Comienza con auto-optimización para resultados rápidos.
- Usa archivos de configuración para un control más detallado.
- Experimenta con diferentes pasos de optimización.

## Solución de Problemas

### Problemas Comunes

#### 1. Problemas de Instalación
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemas con CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemas de Memoria
- Usa tamaños de lote más pequeños durante la optimización.
- Prueba la cuantización con mayor precisión primero (int8 en lugar de int4).
- Asegúrate de tener suficiente espacio en disco para la caché del modelo.

#### 4. Errores al Cargar el Modelo
- Verifica la ruta del modelo y los permisos de acceso.
- Comprueba si el modelo requiere `trust_remote_code=True`.
- Asegúrate de que todos los archivos necesarios del modelo estén descargados.

### Obtener Ayuda

- **Documentación**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemas en GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Ejemplos**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Recursos Adicionales

### Enlaces Oficiales
- **Repositorio GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Documentación de ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Ejemplo de Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Ejemplos de la Comunidad
- **Jupyter Notebooks**: Disponibles en el repositorio de GitHub de Olive.
- **Extensión de VS Code**: La extensión AI Toolkit utiliza Olive para la optimización de modelos.
- **Publicaciones en Blogs**: El blog de Microsoft Open Source tiene tutoriales detallados sobre Olive.

### Herramientas Relacionadas
- **ONNX Runtime**: Motor de inferencia de alto rendimiento.
- **Hugging Face Transformers**: Fuente de muchos modelos compatibles.
- **Azure Machine Learning**: Flujos de trabajo de optimización basados en la nube.

## ➡️ ¿Qué sigue?

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.