<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-19T01:10:25+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "sl"
}
-->
# Poglavje 2: Destilacija modelov - od teorije do prakse

## Kazalo
1. [Uvod v destilacijo modelov](../../../Module05)
2. [Zakaj je destilacija pomembna](../../../Module05)
3. [Postopek destilacije](../../../Module05)
4. [Praktična izvedba](../../../Module05)
5. [Primer destilacije v Azure ML](../../../Module05)
6. [Najboljše prakse in optimizacija](../../../Module05)
7. [Uporaba v resničnem svetu](../../../Module05)
8. [Zaključek](../../../Module05)

## Uvod v destilacijo modelov {#introduction}

Destilacija modelov je zmogljena tehnika, ki nam omogoča ustvarjanje manjših in bolj učinkovitih modelov, pri čemer ohranimo večino zmogljivosti večjih in kompleksnejših modelov. Ta proces vključuje učenje kompaktnega "študentskega" modela, da posnema vedenje večjega "učiteljskega" modela.

**Ključne prednosti:**
- **Zmanjšane zahteve po računalniški moči** za sklepanje
- **Manjša poraba pomnilnika** in potrebe po shranjevanju
- **Hitrejši časi sklepanja** ob ohranjanju sprejemljive natančnosti
- **Stroškovno učinkovita uporaba** v okolju z omejenimi viri

## Zakaj je destilacija pomembna {#why-distillation-matters}

Veliki jezikovni modeli (LLM) postajajo vse bolj zmogljivi, a hkrati tudi vse bolj zahtevni glede virov. Čeprav model z milijardami parametrov lahko zagotavlja odlične rezultate, pogosto ni praktičen za številne resnične aplikacije zaradi:

### Omejitve virov
- **Računalniška obremenitev**: Veliki modeli zahtevajo veliko pomnilnika GPU in procesorske moči
- **Latenca pri sklepanju**: Kompleksni modeli potrebujejo več časa za generiranje odgovorov
- **Poraba energije**: Večji modeli porabijo več energije, kar povečuje operativne stroške
- **Stroški infrastrukture**: Gostovanje velikih modelov zahteva drago strojno opremo

### Praktične omejitve
- **Mobilna uporaba**: Veliki modeli ne delujejo učinkovito na mobilnih napravah
- **Aplikacije v realnem času**: Aplikacije, ki zahtevajo nizko latenco, ne morejo sprejeti počasnega sklepanja
- **Robno računalništvo**: IoT in robne naprave imajo omejene računalniške vire
- **Stroškovni vidik**: Številne organizacije si ne morejo privoščiti infrastrukture za uporabo velikih modelov

## Postopek destilacije {#the-distillation-process}

Destilacija modelov sledi dvostopenjskemu procesu, ki prenaša znanje iz učiteljskega modela na študentski model:

### 1. stopnja: Generiranje sintetičnih podatkov

Učiteljski model generira odgovore za vaš učni nabor, s čimer ustvari visokokakovostne sintetične podatke, ki zajemajo znanje in vzorce sklepanja učitelja.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Ključni vidiki te stopnje:**
- Učiteljski model obdela vsak učni primer
- Generirani odgovori postanejo "resnica" za učenje študenta
- Ta proces zajame vzorce odločanja učitelja
- Kakovost sintetičnih podatkov neposredno vpliva na zmogljivost študentskega modela

### 2. stopnja: Fino prilagajanje študentskega modela

Študentski model se uči na sintetičnem naboru podatkov, da posnema vedenje in odgovore učitelja.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Cilji učenja:**
- Zmanjšanje razlike med izhodi študenta in učitelja
- Ohranjanje znanja učitelja v manjšem prostoru parametrov
- Ohranitev zmogljivosti ob zmanjšanju kompleksnosti modela

## Praktična izvedba {#practical-implementation}

### Izbira učiteljskega in študentskega modela

**Izbira učiteljskega modela:**
- Izberite velike LLM modele (100B+ parametrov) z dokazano zmogljivostjo za vašo specifično nalogo
- Priljubljeni učiteljski modeli vključujejo:
  - **DeepSeek V3** (671B parametrov) - odličen za sklepanje in generiranje kode
  - **Meta Llama 3.1 405B Instruct** - celovite splošne zmogljivosti
  - **GPT-4** - močna zmogljivost pri raznolikih nalogah
  - **Claude 3.5 Sonnet** - odličen za kompleksne naloge sklepanja
- Poskrbite, da učiteljski model dobro deluje na vaših podatkih specifičnega področja

**Izbira študentskega modela:**
- Uravnotežite velikost modela in zahteve glede zmogljivosti
- Osredotočite se na učinkovite, manjše modele, kot so:
  - **Microsoft Phi-4-mini** - najnovejši učinkovit model z močnimi sposobnostmi sklepanja
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K in 128K različice)
  - Microsoft Phi-3.5 Mini Instruct

### Koraki izvedbe

1. **Priprava podatkov**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Nastavitev učiteljskega modela**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Generiranje sintetičnih podatkov**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Učenje študentskega modela**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Primer destilacije v Azure ML {#azure-ml-example}

Azure Machine Learning ponuja celovito platformo za izvedbo destilacije modelov. Tukaj je opis, kako izkoristiti Azure ML za vaš delovni proces destilacije:

### Predpogoji

1. **Azure ML Workspace**: Nastavite delovni prostor v ustrezni regiji
   - Poskrbite za dostop do velikih učiteljskih modelov (DeepSeek V3, Llama 405B)
   - Konfigurirajte regije glede na razpoložljivost modelov

2. **Računalniški viri**: Konfigurirajte ustrezne računalniške instance za učenje
   - Instance z veliko pomnilnika za sklepanje učiteljskega modela
   - GPU-omogočene instance za fino prilagajanje študentskega modela

### Podprte vrste nalog

Azure ML podpira destilacijo za različne naloge:

- **Interpretacija naravnega jezika (NLI)**
- **Pogovorni AI**
- **Vprašanja in odgovori (QA)**
- **Matematično sklepanje**
- **Povzemanje besedila**

### Primer izvedbe

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Spremljanje in ocenjevanje

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Najboljše prakse in optimizacija {#best-practices}

### Kakovost podatkov

**Visokokakovostni učni podatki so ključni:**
- Poskrbite za raznolike in reprezentativne učne primere
- Uporabljajte podatke specifičnega področja, kadar je to mogoče
- Validirajte izhode učiteljskega modela, preden jih uporabite za učenje študenta
- Uravnotežite nabor podatkov, da se izognete pristranskosti pri učenju študentskega modela

### Prilagajanje hiperparametrov

**Ključni parametri za optimizacijo:**
- **Hitrost učenja**: Začnite z manjšimi hitrostmi (1e-5 do 5e-5) za fino prilagajanje
- **Velikost serije**: Uravnotežite med omejitvami pomnilnika in stabilnostjo učenja
- **Število epoh**: Spremljajte prekomerno prilagajanje; običajno zadostuje 2-5 epoh
- **Temperaturno skaliranje**: Prilagodite mehkobo izhodov učitelja za boljši prenos znanja

### Premisleki o arhitekturi modela

**Združljivost učitelja in študenta:**
- Poskrbite za arhitekturno združljivost med učiteljskim in študentskim modelom
- Razmislite o ujemanju vmesnih slojev za boljši prenos znanja
- Uporabljajte tehnike prenosa pozornosti, kadar je to primerno

### Strategije ocenjevanja

**Celovit pristop k ocenjevanju:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Uporaba v resničnem svetu {#real-world-applications}

### Mobilna in robna uporaba

Destilirani modeli omogočajo AI zmogljivosti na napravah z omejenimi viri:
- **Aplikacije za pametne telefone** z obdelavo besedila v realnem času
- **IoT naprave** z lokalnim sklepanjem
- **Vgrajeni sistemi** z omejenimi računalniškimi viri

### Stroškovno učinkoviti produkcijski sistemi

Organizacije uporabljajo destilacijo za zmanjšanje operativnih stroškov:
- **Klepetalni roboti za podporo strankam** z hitrejšimi odzivnimi časi
- **Sistemi za moderiranje vsebine** z učinkovito obdelavo velikih količin podatkov
- **Storitve za prevajanje v realnem času** z nižjo latenco

### Aplikacije specifičnega področja

Destilacija pomaga pri ustvarjanju specializiranih modelov:
- **Pomoč pri medicinski diagnostiki** z lokalnim sklepanjem, ki ohranja zasebnost
- **Analiza pravnih dokumentov** optimizirana za specifična pravna področja
- **Ocena finančnih tveganj** z hitrim sprejemanjem odločitev

### Študija primera: Podpora strankam z DeepSeek V3 → Phi-4-mini

Tehnološko podjetje je izvedlo destilacijo za svoj sistem podpore strankam:

**Podrobnosti izvedbe:**
- **Učiteljski model**: DeepSeek V3 (671B parametrov) - odlično sklepanje za kompleksna vprašanja strank
- **Študentski model**: Phi-4-mini - optimiziran za hitro sklepanje in uporabo
- **Učni podatki**: 50.000 pogovorov s podporo strankam
- **Naloga**: Večkratni pogovori s tehničnim reševanjem težav

**Doseženi rezultati:**
- **85% zmanjšanje** časa sklepanja (z 3,2s na 0,48s na odgovor)
- **95% zmanjšanje** potreb po pomnilniku (z 1,2TB na 60GB)
- **92% ohranitev** natančnosti izvirnega modela pri nalogah podpore
- **60% zmanjšanje** operativnih stroškov
- **Izboljšana skalabilnost** - zdaj lahko obravnavajo 10x več sočasnih uporabnikov

**Razčlenitev zmogljivosti:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Zaključek {#conclusion}

Destilacija modelov predstavlja ključno tehniko za demokratizacijo dostopa do naprednih AI zmogljivosti. Z omogočanjem ustvarjanja manjših, bolj učinkovitih modelov, ki ohranijo večino zmogljivosti svojih večjih različic, destilacija odgovarja na naraščajoče potrebe po praktični uporabi AI.

### Ključne ugotovitve

1. **Destilacija premošča vrzel** med zmogljivostjo modela in praktičnimi omejitvami
2. **Dvostopenjski proces** zagotavlja učinkovit prenos znanja od učitelja do študenta
3. **Azure ML ponuja robustno infrastrukturo** za izvedbo delovnih procesov destilacije
4. **Pravilno ocenjevanje in optimizacija** sta ključna za uspešno destilacijo
5. **Uporaba v resničnem svetu** kaže na pomembne koristi glede stroškov, hitrosti in dostopnosti

### Prihodnje smeri

Ker se področje še naprej razvija, lahko pričakujemo:
- **Napredne tehnike destilacije** z boljšimi metodami prenosa znanja
- **Destilacijo z več učitelji** za izboljšane zmogljivosti študentskega modela
- **Avtomatizirano optimizacijo** procesa destilacije
- **Širšo podporo modelom** v različnih arhitekturah in domenah

Destilacija modelov omogoča organizacijam, da izkoristijo najsodobnejše AI zmogljivosti ob ohranjanju praktičnih omejitev uporabe, kar omogoča dostop do naprednih jezikovnih modelov v širokem spektru aplikacij in okolij.

## ➡️ Kaj sledi

- [03: Fino prilagajanje - prilagoditev modelov za specifične naloge](./03.SLMOps-Finetuing.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna napačna razumevanja ali napačne interpretacije, ki bi nastale zaradi uporabe tega prevoda.