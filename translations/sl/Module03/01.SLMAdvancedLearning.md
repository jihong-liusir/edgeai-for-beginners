<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T20:55:00+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "sl"
}
-->
# Poglavje 1: Napredno učenje SLM - Osnove in optimizacija

Majhni jezikovni modeli (SLM) predstavljajo ključen napredek v EdgeAI, saj omogočajo napredne zmogljivosti obdelave naravnega jezika na napravah z omejenimi viri. Razumevanje učinkovite uporabe, optimizacije in implementacije SLM-jev je bistveno za gradnjo praktičnih rešitev umetne inteligence na robu.

## Uvod

V tej lekciji bomo raziskali majhne jezikovne modele (SLM) in njihove napredne strategije implementacije. Pokrili bomo temeljne koncepte SLM-jev, njihove meje parametrov in klasifikacije, tehnike optimizacije ter praktične strategije implementacije v okolju robnega računalništva.

## Cilji učenja

Do konca te lekcije boste sposobni:

- 🔢 Razumeti meje parametrov in klasifikacije majhnih jezikovnih modelov.
- 🛠️ Prepoznati ključne tehnike optimizacije za implementacijo SLM-jev na robnih napravah.
- 🚀 Naučiti se izvajati napredne strategije kvantizacije in kompresije za SLM-je.

## Razumevanje meja parametrov in klasifikacije SLM-jev

Majhni jezikovni modeli (SLM) so AI modeli, zasnovani za obdelavo, razumevanje in generiranje vsebine naravnega jezika z bistveno manj parametri kot njihovi veliki kolegi. Medtem ko veliki jezikovni modeli (LLM) vsebujejo stotine milijard do trilijonov parametrov, so SLM-ji posebej zasnovani za učinkovitost in implementacijo na robu.

Okvir klasifikacije parametrov nam pomaga razumeti različne kategorije SLM-jev in njihove ustrezne primere uporabe. Ta klasifikacija je ključna za izbiro pravega modela za specifične scenarije robnega računalništva.

### Okvir klasifikacije parametrov

Razumevanje meja parametrov pomaga pri izbiri ustreznih modelov za različne scenarije robnega računalništva:

- **🔬 Mikro SLM-ji**: 100M - 1,4B parametrov (izjemno lahki za mobilne naprave)
- **📱 Majhni SLM-ji**: 1,5B - 13,9B parametrov (uravnotežena zmogljivost in učinkovitost)
- **⚖️ Srednji SLM-ji**: 14B - 30B parametrov (približevanje zmogljivostim LLM-jev ob ohranjanju učinkovitosti)

Natančna meja ostaja fluidna v raziskovalni skupnosti, vendar večina praktikov modele z manj kot 30 milijardami parametrov šteje za "majhne," pri čemer nekateri viri postavljajo prag še nižje, na 10 milijard parametrov.

### Ključne prednosti SLM-jev

SLM-ji ponujajo več temeljnih prednosti, zaradi katerih so idealni za aplikacije robnega računalništva:

**Operativna učinkovitost**: SLM-ji omogočajo hitrejše čase sklepanja zaradi manj parametrov za obdelavo, kar jih naredi idealne za aplikacije v realnem času. Zahtevajo manj računalniških virov, kar omogoča implementacijo na napravah z omejenimi viri, hkrati pa porabijo manj energije in ohranjajo zmanjšan ogljični odtis.

**Fleksibilnost implementacije**: Ti modeli omogočajo zmogljivosti AI na napravi brez potrebe po internetni povezavi, izboljšujejo zasebnost in varnost z lokalno obdelavo, jih je mogoče prilagoditi za aplikacije specifične za določeno področje in so primerni za različna okolja robnega računalništva.

**Stroškovna učinkovitost**: SLM-ji ponujajo stroškovno učinkovito usposabljanje in implementacijo v primerjavi z LLM-ji, z zmanjšanimi operativnimi stroški in nižjimi zahtevami po pasovni širini za aplikacije na robu.

## Napredne strategije pridobivanja modelov

### Ekosistem Hugging Face

Hugging Face služi kot primarno središče za odkrivanje in dostop do najsodobnejših SLM-jev. Platforma ponuja obsežne vire za odkrivanje in implementacijo modelov:

**Funkcije odkrivanja modelov**: Platforma omogoča napredno filtriranje po številu parametrov, vrsti licence in metrikah zmogljivosti. Uporabniki lahko dostopajo do orodij za primerjavo modelov, rezultatov ocenjevanja zmogljivosti v realnem času in WebGPU demo različic za takojšnje testiranje.

**Kurirane zbirke SLM-jev**: Priljubljeni modeli vključujejo Phi-4-mini-3.8B za napredne naloge sklepanja, serijo Qwen3 (0.6B/1.7B/4B) za večjezične aplikacije, Google Gemma3 za učinkovite splošne naloge in eksperimentalne modele, kot je BitNET za ultra-nizko precizno implementacijo. Platforma vključuje tudi zbirke, ki jih vodi skupnost, s specializiranimi modeli za specifična področja ter predhodno usposobljene in na navodila optimizirane različice za različne primere uporabe.

### Katalog modelov Azure AI Foundry

Katalog modelov Azure AI Foundry omogoča dostop do SLM-jev na ravni podjetja z izboljšanimi integracijskimi zmogljivostmi:

**Integracija na ravni podjetja**: Katalog vključuje modele, ki jih neposredno prodaja Azure z podporo na ravni podjetja in SLA-ji, vključno s Phi-4-mini-3.8B za napredne zmogljivosti sklepanja in Llama 3-8B za produkcijsko implementacijo. Vključuje tudi modele, kot je Qwen3 8B, ki jih ponujajo zaupanja vredni tretji odprtokodni viri.

**Prednosti za podjetja**: Vgrajena orodja za prilagajanje, opazovanje in odgovorno AI so integrirana z zamenljivim Provisioned Throughput med družinami modelov. Neposredna podpora Microsofta z SLA-ji na ravni podjetja, integrirane funkcije varnosti in skladnosti ter celoviti delovni tokovi implementacije izboljšujejo izkušnjo na ravni podjetja.

## Napredne tehnike kvantizacije in optimizacije

### Okvir optimizacije Llama.cpp

Llama.cpp ponuja najsodobnejše tehnike kvantizacije za maksimalno učinkovitost pri implementaciji na robu:

**Metode kvantizacije**: Okvir podpira različne ravni kvantizacije, vključno z Q4_0 (4-bitna kvantizacija z odlično redukcijo velikosti - idealna za mobilno implementacijo Qwen3-0.6B), Q5_1 (5-bitna kvantizacija, ki uravnoteži kakovost in kompresijo - primerna za robno sklepanje Phi-4-mini-3.8B) in Q8_0 (8-bitna kvantizacija za skoraj originalno kakovost - priporočljiva za produkcijsko uporabo Google Gemma3). BitNET predstavlja vrhunec z 1-bitno kvantizacijo za ekstremne scenarije kompresije.

**Prednosti implementacije**: Sklepanje, optimizirano za CPU, s pospeševanjem SIMD omogoča učinkovito nalaganje in izvajanje modelov. Združljivost med platformami na arhitekturah x86, ARM in Apple Silicon omogoča strojno neodvisne zmogljivosti implementacije.

**Praktičen primer implementacije**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Primerjava pomnilniškega odtisa**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Optimizacijski paket Microsoft Olive

Microsoft Olive ponuja celovite delovne tokove optimizacije modelov, zasnovane za produkcijska okolja:

**Tehnike optimizacije**: Paket vključuje dinamično kvantizacijo za samodejno izbiro natančnosti (še posebej učinkovito pri modelih serije Qwen3), optimizacijo grafov in združevanje operaterjev (optimizirano za arhitekturo Google Gemma3), optimizacije, specifične za strojno opremo, za CPU, GPU in NPU (s posebno podporo za Phi-4-mini-3.8B na ARM napravah) ter večstopenjske optimizacijske tokove. Modeli BitNET zahtevajo specializirane delovne tokove za 1-bitno kvantizacijo znotraj okvira Olive.

**Avtomatizacija delovnih tokov**: Avtomatizirano primerjanje med različicami optimizacije zagotavlja ohranjanje kakovostnih metrik med optimizacijo. Integracija s priljubljenimi okviri ML, kot sta PyTorch in ONNX, omogoča optimizacijo za oblak in robno implementacijo.

**Praktičen primer implementacije**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Okvir Apple MLX

Apple MLX ponuja nativno optimizacijo, posebej zasnovano za naprave Apple Silicon:

**Optimizacija za Apple Silicon**: Okvir uporablja arhitekturo enotnega pomnilnika z integracijo Metal Performance Shaders, samodejno mešano natančnost sklepanja (še posebej učinkovito pri Google Gemma3) in optimizirano uporabo pasovne širine pomnilnika. Phi-4-mini-3.8B kaže izjemno zmogljivost na čipih serije M, medtem ko Qwen3-1.7B zagotavlja optimalno ravnovesje za implementacijo na MacBook Air.

**Razvojne funkcije**: Podpora za Python in Swift API z operacijami, združljivimi z NumPy, zmogljivostmi samodejne diferenciacije in brezhibno integracijo z razvojnimi orodji Apple zagotavljajo celovito razvojno okolje.

**Praktičen primer implementacije**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Produkcijska implementacija in strategije sklepanja

### Ollama: Poenostavljena lokalna implementacija

Ollama poenostavi implementacijo SLM-jev z značilnostmi, pripravljenimi za podjetja, za lokalna in robna okolja:

**Zmogljivosti implementacije**: Namestitev in izvajanje modela z enim ukazom z avtomatskim pridobivanjem in predpomnjenjem modela. Podpora za Phi-4-mini-3.8B, celotno serijo Qwen3 (0.6B/1.7B/4B) in Google Gemma3 z REST API za integracijo aplikacij ter zmogljivosti upravljanja in preklapljanja med modeli. Modeli BitNET zahtevajo eksperimentalne konfiguracije gradnje za podporo 1-bitni kvantizaciji.

**Napredne funkcije**: Podpora za prilagajanje modelov, generiranje Dockerfile za implementacijo v kontejnerjih, pospeševanje GPU z avtomatskim zaznavanjem ter možnosti kvantizacije in optimizacije modelov zagotavljajo celovito fleksibilnost implementacije.

### VLLM: Sklepanje z visoko zmogljivostjo

VLLM omogoča optimizacijo sklepanja na ravni produkcije za scenarije z visokim pretokom:

**Optimizacije zmogljivosti**: PagedAttention za učinkovito računalniško obdelavo pozornosti (še posebej koristno za transformacijsko arhitekturo Phi-4-mini-3.8B), dinamično združevanje za optimizacijo pretoka (optimizirano za paralelno obdelavo serije Qwen3), paralelizem tenzorjev za skaliranje na več GPU-jih (podpora za Google Gemma3) in spekulativno dekodiranje za zmanjšanje zakasnitve. Modeli BitNET zahtevajo specializirane jedrne funkcije sklepanja za 1-bitne operacije.

**Integracija na ravni podjetja**: API končne točke, združljive z OpenAI, podpora za implementacijo Kubernetes, integracija opazovanja in spremljanja ter zmogljivosti samodejnega skaliranja zagotavljajo rešitve implementacije na ravni podjetja.

### Foundry Local: Microsoftova rešitev za rob

Foundry Local zagotavlja celovite zmogljivosti implementacije na robu za okolja na ravni podjetja:

**Značilnosti robnega računalništva**: Zasnova arhitekture "offline-first" z optimizacijo za omejene vire, upravljanje lokalnega registra modelov in zmogljivosti sinhronizacije med robom in oblakom zagotavljajo zanesljivo implementacijo na robu.

**Varnost in skladnost**: Lokalna obdelava podatkov za ohranjanje zasebnosti, varnostni nadzor na ravni podjetja, beleženje revizij in poročanje o skladnosti ter upravljanje dostopa na podlagi vlog zagotavljajo celovito varnost za implementacije na robu.

## Najboljše prakse za implementacijo SLM-jev

### Smernice za izbiro modelov

Pri izbiri SLM-jev za implementacijo na robu upoštevajte naslednje dejavnike:

**Premisleki o številu parametrov**: Izberite mikro SLM-je, kot je Qwen3-0.6B, za izjemno lahke mobilne aplikacije, majhne SLM-je, kot sta Qwen3-1.7B ali Google Gemma3, za uravnotežene scenarije zmogljivosti, in srednje SLM-je, kot sta Phi-4-mini-3.8B ali Qwen3-4B, ko se približujete zmogljivostim LLM-jev ob ohranjanju učinkovitosti. Modeli BitNET ponujajo eksperimentalno ultra-kompresijo za specifične raziskovalne aplikacije.

**Usklajenost s primerom uporabe**: Ujemajte zmogljivosti modela s specifičnimi zahtevami aplikacije, pri čemer upoštevajte dejavnike, kot so kakovost odziva, hitrost sklepanja, omejitve pomnilnika in zahteve za delovanje brez povezave.

### Izbira strategije optimizacije

**Pristop kvantizacije**: Izberite ustrezne ravni kvantizacije glede na zahteve kakovosti in omejitve strojne opreme. Upoštevajte Q4_0 za maksimalno kompresijo (idealno za mobilno implementacijo Qwen3-0.6B), Q5_1 za uravnoteženo razmerje med kakovostjo in kompresijo (primerno za Phi-4-mini-3.8B in Google Gemma3) ter Q8_0 za ohranjanje skoraj originalne kakovosti (priporočljivo za produkcijska okolja Qwen3-4B). BitNET-ova 1-bitna kvantizacija predstavlja skrajno mejo kompresije za specializirane aplikacije.

**Izbira okvira**: Izberite optimizacijske okvire glede na ciljno strojno opremo in zahteve implementacije. Uporabite Llama.cpp za implementacijo, optimizirano za CPU, Microsoft Olive za celovite delovne tokove optimizacije in Apple MLX za naprave Apple Silicon.

## Praktični primeri modelov in primeri uporabe

### Scenariji implementacije v resničnem svetu

**Mobilne aplikacije**: Qwen3-0.6B se odlično obnese v aplikacijah za pametne telefone z minimalnim pomnilniškim odtisom, medtem ko Google Gemma3 zagotavlja uravnoteženo zmogljivost za izobraževalna orodja na tablicah. Phi-4-mini-3.8B ponuja vrhunske zmogljivosti sklepanja za aplikacije mobilne produktivnosti.

**Namizno in robno računalništvo**: Qwen3-1.7B zagotavlja optimalno zmogljivost za aplikacije namiznih pomočnikov, Phi-4-mini-3.8B ponuja napredne zmogljivosti generiranja kode za razvojna orodja, medtem ko Qwen3-4B omogoča sofisticirano analizo dokumentov v delovnih okoljih.

**Raziskave in eksperimentalno**: Modeli BitNET omogočajo raziskovanje ultra-nizko preciznega sklepanja za akademske raziskave in aplikacije dokazovanja koncepta, ki zahtevajo ekstremne omejitve virov.

### Primerjalne zmogljivosti

**Hitrost sklepanja**: Qwen3-0.6B dosega najhitrejše čase sklepanja na mobilnih CPU-jih, Google Gemma3 zagotavlja uravnoteženo razmerje med hitrostjo in kakovostjo za splošne aplikacije, Phi-4-mini-3.8B ponuja vrhunsko hitrost sklepanja za kompleksne naloge, medtem ko BitNET zagotavlja teoretično maksimalen pretok s specializirano strojno opremo.

**Zahteve po pomnilniku**: Pomnilniški odtisi modelov segajo od Qwen3-0.6B (manj kot 1GB kvantiziran) do Phi-4-mini-3.8B (približno 3-4GB kvantiziran), medtem ko BitNET dosega odtise pod 500MB v eksperimentalnih konfiguracijah.

## Izzivi in premisleki

### Kompromisi zmogljivosti

Implementacija SLM-jev zahteva skrbno razmislek o kompromisih med velikostjo modela, hitrostjo

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne odgovarjamo za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.