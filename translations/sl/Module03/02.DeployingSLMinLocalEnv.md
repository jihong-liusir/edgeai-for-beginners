<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-19T01:31:17+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "sl"
}
-->
# Poglavje 2: Namestitev v lokalnem okolju - Rešitve, ki dajejo prednost zasebnosti

Lokalna namestitev malih jezikovnih modelov (SLM) predstavlja premik paradigme k rešitvam umetne inteligence, ki ohranjajo zasebnost in so stroškovno učinkovite. Ta celovit vodnik raziskuje dva zmogljiva okvira—Ollama in Microsoft Foundry Local—ki razvijalcem omogočata, da izkoristijo polni potencial SLM-jev, hkrati pa ohranijo popoln nadzor nad svojim okoljem namestitve.

## Uvod

V tej lekciji bomo raziskali napredne strategije za namestitev malih jezikovnih modelov v lokalnih okoljih. Pokrili bomo temeljne koncepte lokalne namestitve umetne inteligence, preučili dve vodilni platformi (Ollama in Microsoft Foundry Local) ter zagotovili praktične smernice za implementacijo rešitev, pripravljenih za produkcijo.

## Cilji učenja

Do konca te lekcije boste sposobni:

- Razumeti arhitekturo in prednosti lokalnih okvirov za namestitev SLM-jev.
- Izvesti namestitve, pripravljene za produkcijo, z uporabo Ollama in Microsoft Foundry Local.
- Primerjati in izbrati ustrezno platformo glede na specifične zahteve in omejitve.
- Optimizirati lokalne namestitve za zmogljivost, varnost in skalabilnost.

## Razumevanje arhitektur lokalne namestitve SLM-jev

Lokalna namestitev SLM-jev predstavlja temeljni premik od storitev umetne inteligence, ki so odvisne od oblaka, k rešitvam na lokaciji, ki ohranjajo zasebnost. Ta pristop organizacijam omogoča popoln nadzor nad njihovo infrastrukturo umetne inteligence, hkrati pa zagotavlja suverenost podatkov in operativno neodvisnost.

### Razvrstitev okvirov za namestitev

Razumevanje različnih pristopov k namestitvi pomaga pri izbiri prave strategije za specifične primere uporabe:

- **Osredotočeno na razvoj**: Poenostavljena nastavitev za eksperimentiranje in prototipiranje
- **Na ravni podjetja**: Rešitve, pripravljene za produkcijo, z zmogljivostmi za integracijo v podjetje  
- **Medplatformsko**: Univerzalna združljivost med različnimi operacijskimi sistemi in strojno opremo

### Ključne prednosti lokalne namestitve SLM-jev

Lokalna namestitev SLM-jev ponuja več temeljnih prednosti, zaradi katerih je idealna za aplikacije v podjetjih in občutljive na zasebnost:

**Zasebnost in varnost**: Lokalna obdelava zagotavlja, da občutljivi podatki nikoli ne zapustijo infrastrukture organizacije, kar omogoča skladnost z GDPR, HIPAA in drugimi regulativnimi zahtevami. Možna so okolja brez povezave z zunanjim omrežjem za zaupne aplikacije, medtem ko popolne revizijske sledi ohranjajo nadzor nad varnostjo.

**Stroškovna učinkovitost**: Odprava cenovnih modelov na podlagi števila obdelanih tokenov bistveno zmanjša operativne stroške. Nižje zahteve po pasovni širini in zmanjšana odvisnost od oblaka zagotavljajo predvidljive stroškovne strukture za proračun podjetja.

**Zmogljivost in zanesljivost**: Hitrejši časi sklepanja brez omrežne zakasnitve omogočajo aplikacije v realnem času. Funkcionalnost brez povezave zagotavlja neprekinjeno delovanje ne glede na internetno povezljivost, medtem ko lokalna optimizacija virov zagotavlja dosledno zmogljivost.

## Ollama: Univerzalna platforma za lokalno namestitev

### Osnovna arhitektura in filozofija

Ollama je zasnovana kot univerzalna, razvijalcem prijazna platforma, ki demokratizira lokalno namestitev LLM-jev na različnih strojnih konfiguracijah in operacijskih sistemih.

**Tehnična osnova**: Zgrajena na robustnem okviru llama.cpp, Ollama uporablja učinkovit modelni format GGUF za optimalno zmogljivost. Združljivost med platformami zagotavlja dosledno delovanje v okolju Windows, macOS in Linux, medtem ko inteligentno upravljanje virov optimizira uporabo CPU, GPU in pomnilnika.

**Filozofija oblikovanja**: Ollama daje prednost preprostosti brez žrtvovanja funkcionalnosti, saj ponuja namestitev brez konfiguracije za takojšnjo produktivnost. Platforma ohranja široko združljivost modelov, hkrati pa zagotavlja dosledne API-je med različnimi arhitekturami modelov.

### Napredne funkcije in zmogljivosti

**Odličnost upravljanja modelov**: Ollama zagotavlja celovito upravljanje življenjskega cikla modelov z avtomatskim prenosom, predpomnjenjem in različicami. Platforma podpira obsežen ekosistem modelov, vključno z Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral in specializiranimi modeli za vdelavo.

**Prilagoditev prek datotek modelov**: Napredni uporabniki lahko ustvarijo prilagojene konfiguracije modelov s specifičnimi parametri, sistemskimi pozivi in spremembami vedenja. To omogoča optimizacije za specifične domene in posebne zahteve aplikacij.

**Optimizacija zmogljivosti**: Ollama samodejno zazna in uporablja razpoložljivo strojno pospeševanje, vključno z NVIDIA CUDA, Apple Metal in OpenCL. Inteligentno upravljanje pomnilnika zagotavlja optimalno uporabo virov na različnih strojnih konfiguracijah.

### Strategije implementacije v produkciji

**Namestitev in nastavitev**: Ollama omogoča poenostavljeno namestitev na različnih platformah prek domačih namestitvenih programov, upravljalnikov paketov (WinGet, Homebrew, APT) in Docker kontejnerjev za kontejnerske namestitve.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Osnovni ukazi in operacije**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Napredna konfiguracija**: Datoteke modelov omogočajo sofisticirano prilagoditev za zahteve podjetij:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Primeri integracije za razvijalce

**Integracija Python API-ja**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Integracija JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Uporaba RESTful API-ja s cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Nastavitve zmogljivosti in optimizacija

**Konfiguracija pomnilnika in niti**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Izbira kvantizacije za različne strojne opreme**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Platforma za robno umetno inteligenco na ravni podjetja

### Arhitektura na ravni podjetja

Microsoft Foundry Local predstavlja celovito rešitev za podjetja, zasnovano posebej za produkcijske namestitve robne umetne inteligence z globoko integracijo v ekosistem Microsoft.

**Osnova na ONNX**: Zgrajena na industrijskem standardu ONNX Runtime, Foundry Local zagotavlja optimizirano zmogljivost na različnih strojnih arhitekturah. Platforma izkorišča integracijo Windows ML za optimizacijo v okolju Windows, hkrati pa ohranja združljivost med platformami.

**Odličnost strojnega pospeševanja**: Foundry Local vključuje inteligentno zaznavanje strojne opreme in optimizacijo med CPU-ji, GPU-ji in NPU-ji. Globoko sodelovanje z dobavitelji strojne opreme (AMD, Intel, NVIDIA, Qualcomm) zagotavlja optimalno zmogljivost na strojnih konfiguracijah podjetij.

### Napredna izkušnja za razvijalce

**Dostop prek več vmesnikov**: Foundry Local ponuja celovite razvojne vmesnike, vključno z zmogljivim CLI za upravljanje modelov in namestitev, večjezičnimi SDK-ji (Python, NodeJS) za nativno integracijo ter RESTful API-ji z združljivostjo OpenAI za enostavno migracijo.

**Integracija z Visual Studio**: Platforma se brezhibno integrira z AI Toolkit za VS Code, ki zagotavlja orodja za pretvorbo modelov, kvantizacijo in optimizacijo znotraj razvojnega okolja. Ta integracija pospeši razvojne procese in zmanjša kompleksnost namestitve.

**Cevovod za optimizacijo modelov**: Integracija Microsoft Olive omogoča sofisticirane delovne procese optimizacije modelov, vključno z dinamično kvantizacijo, optimizacijo grafov in prilagoditvijo za specifično strojno opremo. Zmogljivosti pretvorbe v oblaku prek Azure ML zagotavljajo skalabilno optimizacijo za velike modele.

### Strategije implementacije v produkciji

**Namestitev in konfiguracija**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operacije upravljanja modelov**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Napredna konfiguracija namestitve**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integracija v ekosistem podjetja

**Varnost in skladnost**: Foundry Local zagotavlja varnostne funkcije na ravni podjetja, vključno z nadzorom dostopa na podlagi vlog, revizijskimi dnevniki, poročanjem o skladnosti in šifriranim shranjevanjem modelov. Integracija z varnostno infrastrukturo Microsoft zagotavlja skladnost z varnostnimi politikami podjetja.

**Vgrajene storitve umetne inteligence**: Platforma ponuja pripravljene zmogljivosti umetne inteligence, vključno s Phi Silica za lokalno obdelavo jezika, AI Imaging za izboljšanje in analizo slik ter specializirane API-je za pogoste naloge umetne inteligence v podjetjih.

## Primerjalna analiza: Ollama vs Foundry Local

### Primerjava tehnične arhitekture

| **Vidik** | **Ollama** | **Foundry Local** |
|-----------|------------|-------------------|
| **Format modela** | GGUF (prek llama.cpp) | ONNX (prek ONNX Runtime) |
| **Osredotočenost platforme** | Univerzalna medplatformska | Optimizacija za Windows/podjetja |
| **Integracija strojne opreme** | Splošna podpora GPU/CPU | Globoka podpora Windows ML, NPU |
| **Optimizacija** | Kvantizacija llama.cpp | Microsoft Olive + ONNX Runtime |
| **Funkcije za podjetja** | Skupnostno usmerjeno | Na ravni podjetja z SLA-ji |

### Značilnosti zmogljivosti

**Prednosti zmogljivosti Ollama**:
- Izjemna zmogljivost CPU prek optimizacije llama.cpp
- Dosledno delovanje na različnih platformah in strojni opremi
- Učinkovita uporaba pomnilnika z inteligentnim nalaganjem modelov
- Hitri časi zagona za razvojne in testne scenarije

**Prednosti zmogljivosti Foundry Local**:
- Superiorna uporaba NPU na sodobni strojni opremi Windows
- Optimizirano pospeševanje GPU prek partnerstev z dobavitelji
- Spremljanje zmogljivosti na ravni podjetja in optimizacija
- Zmogljivosti za skalabilno namestitev v produkciji

### Analiza izkušnje za razvijalce

**Izkušnja razvijalcev z Ollama**:
- Minimalne zahteve za nastavitev z takojšnjo produktivnostjo
- Intuitiven vmesnik ukazne vrstice za vse operacije
- Obsežna podpora skupnosti in dokumentacija
- Prilagodljiva prilagoditev prek datotek modelov

**Izkušnja razvijalcev z Foundry Local**:
- Celovita integracija IDE z ekosistemom Visual Studio
- Razvojni procesi na ravni podjetja z funkcijami za sodelovanje ekip
- Profesionalni kanali podpore z Microsoftovo podporo
- Napredna orodja za odpravljanje napak in optimizacijo

### Optimizacija primerov uporabe

**Izberite Ollama, ko**:
- Razvijate medplatformske aplikacije, ki zahtevajo dosledno delovanje
- Dajete prednost preglednosti odprte kode in prispevkom skupnosti
- Delate z omejenimi viri ali proračunskimi omejitvami
- Gradite eksperimentalne ali raziskovalno usmerjene aplikacije
- Potrebujete široko združljivost modelov med različnimi arhitekturami

**Izberite Foundry Local, ko**:
- Nameščate aplikacije na ravni podjetja z strogimi zahtevami glede zmogljivosti
- Izkoristite optimizacije strojne opreme, specifične za Windows (NPU, Windows ML)
- Potrebujete podporo na ravni podjetja, SLA-je in funkcije skladnosti
- Gradite produkcijske aplikacije z integracijo v ekosistem Microsoft
- Potrebujete napredna orodja za optimizacijo in profesionalne razvojne procese

## Napredne strategije namestitve

### Vzorci kontejnerske namestitve

**Kontejnerizacija Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Namestitev Foundry Local na ravni podjetja**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Tehnike optimizacije zmogljivosti

**Strategije optimizacije Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Optimizacija Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Varnostne in skladnostne zahteve

### Implementacija varnosti na ravni podjetja

**Najboljše prakse varnosti Ollama**:
- Izolacija omrežja z nastavitvami požarnega zidu in dostopom prek VPN
- Avtentikacija prek integracije povratnega proxyja
- Preverjanje integritete modelov in varna distribucija modelov
- Revizijski dnevniki za dostop do API-jev in operacije modelov

**Varnost na ravni podjetja Foundry Local**:
- Vgrajen nadzor dostopa na podlagi vlog z integracijo Active Directory
- Celovite revizijske sledi z poročanjem o skladnosti
- Šifrirano shranjevanje modelov in varna namestitev modelov
- Integracija z varnostno infrastrukturo Microsoft

### Zahteve skladnosti in regulative

Obe platformi podpirata skladnost z regulativnimi zahtevami prek:
- Nadzora nad lokacijo podatkov, ki zagotavlja lokalno obdelavo
- Revizijskih dnevnikov za zahteve poročanja o skladnosti
- Nadzora dostopa za obravnavo občutljivih podatkov
- Šifriranja podatkov med shranjevanjem in prenosom za zaščito podatkov

## Najboljše prakse za produkcijsko namestitev

### Spremljanje in opazovanje

**Ključne metrike za spremljanje**:
- Zakasnitev in prepustnost sklepanja modelov
- Uporaba virov (CPU, GPU, pomnilnik)
- Časi odziva API-jev in stopnje napak
- Natančnost modelov in odstopanje zmogljivosti

**Implementacija spremljanja**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Neprekinjena integracija in namestitev

**Integracija CI/CD cevovoda**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Prihodnji trendi in razmisleki

### Nastajajoče tehnologije

Pokrajina lokalne namestitve SLM-jev se še naprej razvija z več ključnimi trendi:

**Napredne arhitekture modelov**: Pojavljajo se modeli naslednje generacije z izboljšano učinkovitostjo in razmerjem zmogljivosti, vključno z modeli "mešanice strokovnjakov" za dinamično skaliranje in specializiranimi arhitekturami za namestitev na robu.

**Integracija strojne opreme**: Globlja integracija s specializirano strojno opremo za umetno inteligenco, vključno z NPU-ji, prilagojenimi silicijskimi čipi in pospeševalniki za računalništvo na robu, bo zagotovila izboljšane zmogljivosti.

**Evolucija ekosistema**: Prizadevanja za standardizacijo med platformami za namestitev in izboljšana interoperabilnost med različnimi okviri bodo poenostavila namestitve na več platformah.

### Vzorci sprejemanja v industriji

**Sprejemanje v podjetjih**: Naraščajoče sprejemanje v podjetjih, ki ga spodbujajo zahteve po zasebnosti, optimizaciji stroškov in skladnosti z regulativnimi zahtevami. Vladni in obrambni sektorji se še posebej osredotočajo na namestitve brez povezave z zunanjim omrežjem.

**Globalni razmisleki**: Mednarodne zahteve po suverenosti podatkov spodbujajo sprejemanje lokalnih namestitev, zlasti v regijah s strogimi predpisi o zaščiti podatkov.

## Izzivi in razmisleki

### Tehnični izzivi

**Zahteve infrastrukture**: Lokal

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.