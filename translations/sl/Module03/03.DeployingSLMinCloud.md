<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-19T01:45:00+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "sl"
}
-->
# Kontejnerska oblačna uvedba - rešitve za produkcijsko uporabo

Ta obsežen vodič zajema tri glavne pristope za uvedbo Microsoftovega modela Phi-4-mini-instruct v kontejnerskih okoljih: vLLM, Ollama in SLM Engine z ONNX Runtime. Ta model s 3,8 milijardami parametrov predstavlja optimalno izbiro za naloge sklepanja, hkrati pa ohranja učinkovitost za uvedbo na robnih napravah.

## Kazalo vsebine

1. [Uvod v kontejnersko uvedbo Phi-4-mini](../../../Module03)
2. [Cilji učenja](../../../Module03)
3. [Razumevanje razvrstitve Phi-4-mini](../../../Module03)
4. [Kontejnerska uvedba vLLM](../../../Module03)
5. [Kontejnerska uvedba Ollama](../../../Module03)
6. [SLM Engine z ONNX Runtime](../../../Module03)
7. [Primerjalni okvir](../../../Module03)
8. [Najboljše prakse](../../../Module03)

## Uvod v kontejnersko uvedbo Phi-4-mini

Majhni jezikovni modeli (SLM) predstavljajo ključen napredek v EdgeAI, saj omogočajo napredne zmogljivosti obdelave naravnega jezika na napravah z omejenimi viri. Ta vodič se osredotoča na strategije kontejnerske uvedbe za Microsoftov model Phi-4-mini-instruct, ki je vrhunski model za sklepanja, ki združuje zmogljivost in učinkovitost.

### Predstavljeni model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 milijarde parametrov)**: Najnovejši Microsoftov lahek model, prilagojen za navodila, zasnovan za okolja z omejenim pomnilnikom/računsko močjo, z izjemnimi zmogljivostmi na področjih:
- **Matematično sklepanje in kompleksni izračuni**
- **Generiranje kode, odpravljanje napak in analiza**
- **Reševanje logičnih problemov in sklepanje po korakih**
- **Izobraževalne aplikacije, ki zahtevajo podrobne razlage**
- **Klicanje funkcij in integracija orodij**

Kot del kategorije "Majhni SLM-ji" (1,5B - 13,9B parametrov) Phi-4-mini dosega optimalno ravnovesje med zmogljivostjo sklepanja in učinkovitostjo virov.

### Prednosti kontejnerske uvedbe Phi-4-mini

- **Operativna učinkovitost**: Hitro sklepanje za naloge z nižjimi računalniškimi zahtevami
- **Fleksibilnost uvedbe**: AI na napravi z izboljšano zasebnostjo prek lokalne obdelave
- **Stroškovna učinkovitost**: Zmanjšani operativni stroški v primerjavi z večjimi modeli ob ohranjanju kakovosti
- **Izolacija**: Čista ločitev med instancami modela in varnimi okolji izvajanja
- **Razširljivost**: Enostavno horizontalno skaliranje za povečanje zmogljivosti sklepanja

## Cilji učenja

Do konca tega vodiča boste sposobni:

- Uvesti in optimizirati Phi-4-mini-instruct v različnih kontejnerskih okoljih
- Izvesti napredne strategije kvantizacije in stiskanja za različne scenarije uvedbe
- Konfigurirati kontejnersko orkestracijo, pripravljeno za produkcijo, za naloge sklepanja
- Oceniti in izbrati ustrezne okvirje za uvedbo glede na specifične zahteve uporabe
- Uporabiti varnostne, nadzorne in skalirne najboljše prakse za kontejnerske uvedbe SLM

## Razumevanje razvrstitve Phi-4-mini

### Specifikacije modela

**Tehnične podrobnosti:**
- **Parametri**: 3,8 milijarde (kategorija Majhni SLM)
- **Arhitektura**: Gost dekoder-only Transformer z grupiranim poizvedovalnim pozornostnim mehanizmom
- **Dolžina konteksta**: 128K tokenov (32K priporočeno za optimalno delovanje)
- **Besedišče**: 200K tokenov z večjezično podporo
- **Učni podatki**: 5T tokenov visokokakovostne vsebine, bogate z sklepanjem

### Zahteve glede virov

| Tip uvedbe | Min RAM | Priporočeni RAM | VRAM (GPU) | Shranjevanje | Tipične uporabe |
|------------|---------|-----------------|------------|--------------|-----------------|
| **Razvoj** | 6GB | 8GB | - | 8GB | Lokalno testiranje, prototipiranje |
| **Produkcija CPU** | 8GB | 12GB | - | 10GB | Robni strežniki, stroškovno optimizirana uvedba |
| **Produkcija GPU** | 6GB | 8GB | 4-6GB | 8GB | Storitev sklepanja z visokim pretokom |
| **Optimizirano za rob** | 4GB | 6GB | - | 6GB | Kvantizirana uvedba, IoT prehodi |

### Zmogljivosti Phi-4-mini

- **Matematična odličnost**: Napredno reševanje problemov iz aritmetike, algebre in kalkulusa
- **Inteligenca kode**: Generiranje kode v Pythonu, JavaScriptu in več jezikih z odpravljanjem napak
- **Logično sklepanje**: Razčlenjevanje problemov po korakih in konstrukcija rešitev
- **Izobraževalna podpora**: Podrobne razlage, primerne za učenje in poučevanje
- **Klicanje funkcij**: Naravna podpora za integracijo orodij in interakcije z API-ji

## Kontejnerska uvedba vLLM

vLLM zagotavlja odlično podporo za Phi-4-mini-instruct z optimizirano zmogljivostjo sklepanja in OpenAI-kompatibilnimi API-ji, kar ga naredi idealnega za produkcijske storitve sklepanja.

### Hitri primeri začetka

#### Osnovna uvedba na CPU (Razvoj)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Produkcijska uvedba z GPU pospeševanjem
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Konfiguracija za produkcijo

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testiranje zmogljivosti sklepanja Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Kontejnerska uvedba Ollama

Ollama zagotavlja odlično podporo za Phi-4-mini-instruct z enostavno uvedbo in upravljanjem, kar ga naredi idealnega za razvoj in uravnotežene produkcijske uvedbe.

### Hitra nastavitev

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Konfiguracija za produkcijo

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimizacija modela in različice

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Primeri uporabe API-ja

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine z ONNX Runtime

ONNX Runtime zagotavlja optimalno zmogljivost za robno uvedbo Phi-4-mini-instruct z napredno optimizacijo in združljivostjo med platformami.

### Osnovna nastavitev

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Poenostavljena implementacija strežnika

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Skripta za pretvorbo modela

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Konfiguracija za produkcijo

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testiranje uvedbe ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Primerjalni okvir

### Primerjava okvirjev za Phi-4-mini

| Značilnost | vLLM | Ollama | ONNX Runtime |
|------------|------|--------|--------------|
| **Kompleksnost nastavitve** | Zmerna | Enostavna | Kompleksna |
| **Zmogljivost (GPU)** | Odlična (~25 tok/s) | Zelo dobra (~20 tok/s) | Dobra (~15 tok/s) |
| **Zmogljivost (CPU)** | Dobra (~8 tok/s) | Zelo dobra (~12 tok/s) | Odlična (~15 tok/s) |
| **Poraba pomnilnika** | 8-12GB | 6-10GB | 4-8GB |
| **Združljivost API-ja** | OpenAI kompatibilen | Custom REST | Custom FastAPI |
| **Klicanje funkcij** | ✅ Naravno | ✅ Podprto | ⚠️ Po meri |
| **Podpora za kvantizacijo** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX kvantizacija |
| **Pripravljeno za produkcijo** | ✅ Odlično | ✅ Zelo dobro | ✅ Dobro |
| **Uvedba na robu** | Dobra | Odlična | Izjemna |

## Dodatni viri

### Uradna dokumentacija
- **Microsoft Phi-4 Model Card**: Podrobne specifikacije in smernice za uporabo
- **vLLM Dokumentacija**: Napredne možnosti konfiguracije in optimizacije
- **Ollama Model Library**: Skupnostni modeli in primeri prilagoditev
- **ONNX Runtime Guides**: Strategije optimizacije zmogljivosti in uvedbe

### Razvojna orodja
- **Hugging Face Transformers**: Za interakcijo in prilagoditev modela
- **OpenAI API Specifikacija**: Za testiranje združljivosti z vLLM
- **Docker Najboljše prakse**: Varnost kontejnerjev in smernice za optimizacijo
- **Kubernetes Uvedba**: Vzorci orkestracije za skaliranje v produkciji

### Učni viri
- **SLM Benchmarking zmogljivosti**: Metodologije primerjalne analize
- **Uvedba Edge AI**: Najboljše prakse za okolja z omejenimi viri
- **Optimizacija nalog sklepanja**: Strategije za oblikovanje pozivov za matematične in logične probleme
- **Varnost kontejnerjev**: Prakse za utrjevanje uvedb AI modelov

## Rezultati učenja

Po zaključku tega modula boste sposobni:

1. Uvesti model Phi-4-mini-instruct v kontejnerska okolja z uporabo več okvirjev
2. Konfigurirati in optimizirati uvedbe SLM za različna strojna okolja
3. Izvesti najboljše prakse za varnost kontejnerskih uvedb AI
4. Primerjati in izbrati ustrezne okvirje za uvedbo glede na specifične zahteve uporabe
5. Uporabiti strategije za nadzor in skaliranje za storitve SLM, pripravljene za produkcijo

## Kaj sledi

- Vrnite se na [Modul 1](../Module01/README.md)
- Vrnite se na [Modul 2](../Module02/README.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitne nesporazume ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.