<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:55:58+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "sl"
}
-->
# Poglavje 2: Osnove družine Qwen

Modelna družina Qwen predstavlja celovit pristop Alibaba Clouda k velikim jezikovnim modelom in multimodalni umetni inteligenci, kar dokazuje, da lahko odprtokodni modeli dosežejo izjemno zmogljivost in so hkrati dostopni v različnih scenarijih uporabe. Pomembno je razumeti, kako družina Qwen omogoča zmogljive AI sposobnosti z možnostmi prilagodljivega uvajanja, hkrati pa ohranja konkurenčno zmogljivost pri raznolikih nalogah.

## Viri za razvijalce

### Repozitorij modelov na Hugging Face
Izbrani modeli družine Qwen so na voljo prek [Hugging Face](https://huggingface.co/models?search=qwen), kar omogoča dostop do nekaterih različic teh modelov. Raziskujete lahko razpoložljive različice, jih prilagodite za svoje specifične potrebe in jih uvedete prek različnih ogrodij.

### Orodja za lokalni razvoj
Za lokalni razvoj in testiranje lahko uporabite [Microsoft Foundry Local](https://github.com/microsoft/foundry-local), da zaženete razpoložljive modele Qwen na svojem razvojnem računalniku z optimizirano zmogljivostjo.

### Dokumentacija
- [Dokumentacija modelov Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizacija modelov Qwen za robno uvajanje](https://github.com/microsoft/olive)

## Uvod

V tem priročniku bomo raziskali modelno družino Qwen podjetja Alibaba in njene osnovne koncepte. Pokrili bomo razvoj družine Qwen, inovativne metodologije učenja, ki naredijo modele Qwen učinkovite, ključne različice v družini ter praktične aplikacije v različnih scenarijih.

## Cilji učenja

Do konca tega priročnika boste lahko:

- Razumeli filozofijo oblikovanja in razvoj modelne družine Qwen podjetja Alibaba
- Prepoznali ključne inovacije, ki omogočajo modelom Qwen doseganje visoke zmogljivosti pri različnih velikostih parametrov
- Prepoznali prednosti in omejitve različnih različic modelov Qwen
- Uporabili znanje o modelih Qwen za izbiro ustreznih različic za resnične scenarije

## Razumevanje sodobne pokrajine AI modelov

Pokrajina umetne inteligence se je močno razvila, pri čemer različne organizacije sledijo različnim pristopom k razvoju jezikovnih modelov. Medtem ko se nekateri osredotočajo na lastniške zaprte modele, drugi poudarjajo odprtokodno dostopnost in transparentnost. Tradicionalni pristop vključuje bodisi ogromne lastniške modele, dostopne le prek API-jev, bodisi odprtokodne modele, ki lahko zaostajajo v zmogljivostih.

Ta paradigma ustvarja izzive za organizacije, ki iščejo zmogljive AI sposobnosti, hkrati pa želijo ohraniti nadzor nad svojimi podatki, stroški in prilagodljivostjo uvajanja. Tradicionalni pristop pogosto zahteva izbiro med vrhunsko zmogljivostjo in praktičnimi vidiki uvajanja.

## Izziv dostopne AI odličnosti

Potreba po visokokakovostni, dostopni umetni inteligenci postaja vse bolj pomembna v različnih scenarijih. Upoštevajte aplikacije, ki zahtevajo prilagodljive možnosti uvajanja za različne organizacijske potrebe, stroškovno učinkovite implementacije, kjer lahko stroški API-jev postanejo pomembni, večjezične sposobnosti za globalne aplikacije ali specializirano strokovno znanje na področjih, kot sta programiranje in matematika.

### Ključne zahteve za uvajanje

Sodobna uvajanja AI se soočajo z več temeljnimi zahtevami, ki omejujejo praktično uporabnost:

- **Dostopnost**: Odprtokodna razpoložljivost za transparentnost in prilagoditev
- **Stroškovna učinkovitost**: Razumne zahteve po računalniških virih za različne proračune
- **Prilagodljivost**: Več velikosti modelov za različne scenarije uvajanja
- **Globalni doseg**: Močne večjezične in medkulturne sposobnosti
- **Specializacija**: Različice, prilagojene specifičnim področjem uporabe

## Filozofija modelov Qwen

Modelna družina Qwen predstavlja celovit pristop k razvoju AI modelov, ki daje prednost odprtokodni dostopnosti, večjezičnim sposobnostim in praktičnemu uvajanju, hkrati pa ohranja konkurenčne zmogljivostne značilnosti. Modeli Qwen to dosežejo z raznolikimi velikostmi modelov, visokokakovostnimi metodologijami učenja in specializiranimi različicami za različna področja.

Družina Qwen vključuje različne pristope, zasnovane za zagotavljanje možnosti po spektru zmogljivosti in učinkovitosti, kar omogoča uvajanje od mobilnih naprav do strežnikov v podjetjih, hkrati pa zagotavlja smiselne AI sposobnosti. Cilj je demokratizirati dostop do visokokakovostne AI, hkrati pa omogočiti prilagodljivost pri izbiri uvajanja.

### Temeljna načela oblikovanja Qwen

Modeli Qwen temeljijo na več osnovnih načelih, ki jih ločujejo od drugih družin jezikovnih modelov:

- **Najprej odprtokodnost**: Popolna transparentnost in dostopnost za raziskovalne in komercialne namene
- **Celovito učenje**: Učenje na ogromnih, raznolikih podatkovnih nizih, ki pokrivajo več jezikov in področij
- **Prilagodljiva arhitektura**: Več velikosti modelov za prilagoditev različnim računalniškim zahtevam
- **Specializirana odličnost**: Različice, optimizirane za specifične naloge

## Ključne tehnologije, ki omogočajo družino Qwen

### Učenje na ogromnem obsegu

Ena od značilnosti družine Qwen je obsežnost podatkov za učenje in računalniških virov, vloženih v razvoj modelov. Modeli Qwen izkoriščajo skrbno izbrane, večjezične podatkovne nize, ki obsegajo bilijone tokenov, zasnovane za zagotavljanje celovitega svetovnega znanja in sposobnosti sklepanja.

Ta pristop združuje visokokakovostno spletno vsebino, akademsko literaturo, repozitorije kode in večjezične vire. Metodologija učenja poudarja tako širino znanja kot globino razumevanja različnih področij in jezikov.

### Napredno sklepanje in razmišljanje

Novejši modeli Qwen vključujejo sofisticirane sposobnosti sklepanja, ki omogočajo kompleksno večstopenjsko reševanje problemov:

**Način razmišljanja (Qwen3)**: Modeli se lahko vključijo v podrobno večstopenjsko sklepanje, preden podajo končne odgovore, podobno človeškim pristopom k reševanju problemov.

**Dvojni način delovanja**: Sposobnost preklopa med hitrim odzivnim načinom za enostavna vprašanja in globljim načinom razmišljanja za kompleksne probleme.

**Integracija verige misli**: Naravna vključitev korakov sklepanja, ki izboljšujejo transparentnost in natančnost pri kompleksnih nalogah.

### Arhitekturne inovacije

Družina Qwen vključuje več arhitekturnih optimizacij, zasnovanih za zmogljivost in učinkovitost:

**Prilagodljiva zasnova**: Dosledna arhitektura med velikostmi modelov, ki omogoča enostavno skaliranje in primerjavo.

**Multimodalna integracija**: Brezhibna integracija obdelave besedila, vizualnih in zvočnih podatkov v enotne arhitekture.

**Optimizacija uvajanja**: Več možnosti kvantizacije in formatov uvajanja za različne strojne konfiguracije.

## Velikost modelov in možnosti uvajanja

Sodobna okolja uvajanja koristijo prilagodljivosti modelov Qwen glede na različne računalniške zahteve:

### Majhni modeli (0,5B-3B)

Qwen ponuja učinkovite majhne modele, primerne za robno uvajanje, mobilne aplikacije in okolja z omejenimi viri, hkrati pa ohranja impresivne sposobnosti.

### Srednji modeli (7B-32B)

Srednje veliki modeli ponujajo izboljšane sposobnosti za profesionalne aplikacije, kar zagotavlja odlično ravnovesje med zmogljivostjo in računalniškimi zahtevami.

### Veliki modeli (72B+)

Modeli polnega obsega zagotavljajo vrhunsko zmogljivost za zahtevne aplikacije, raziskave in uvajanja v podjetjih, ki zahtevajo največje sposobnosti.

## Prednosti družine modelov Qwen

### Dostopnost odprte kode

Modeli Qwen zagotavljajo popolno transparentnost in možnosti prilagoditve, kar organizacijam omogoča razumevanje, spreminjanje in prilagajanje modelov njihovim specifičnim potrebam brez odvisnosti od ponudnika.

### Prilagodljivost uvajanja

Razpon velikosti modelov omogoča uvajanje na raznolikih strojnih konfiguracijah, od mobilnih naprav do zmogljivih strežnikov, kar organizacijam omogoča prilagodljivost pri izbiri AI infrastrukture.

### Večjezična odličnost

Modeli Qwen se odlikujejo v razumevanju in generiranju več jezikov, podpirajo desetine jezikov, zlasti pa so močni v angleščini in kitajščini, kar jih naredi primerne za globalne aplikacije.

### Konkurenčna zmogljivost

Modeli Qwen dosledno dosegajo konkurenčne rezultate na merilih, hkrati pa zagotavljajo odprtokodno dostopnost, kar dokazuje, da lahko odprti modeli konkurirajo lastniškim alternativam.

### Specializirane sposobnosti

Različice, kot sta Qwen-Coder in Qwen-Math, zagotavljajo specializirano strokovno znanje, hkrati pa ohranjajo splošne sposobnosti razumevanja jezika.

## Praktični primeri in uporabe

Preden se poglobimo v tehnične podrobnosti, si oglejmo nekaj konkretnih primerov, kaj modeli Qwen lahko dosežejo:

### Primer matematičnega sklepanja

Qwen-Math se odlikuje pri večstopenjskem reševanju matematičnih problemov. Na primer, ko je vprašanje rešiti kompleksno nalogo iz kalkulusa:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Primer večjezične podpore

Modeli Qwen kažejo močne večjezične sposobnosti v različnih jezikih:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Primer multimodalnih sposobnosti

Qwen-VL lahko hkrati obdeluje besedilo in slike:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Primer generiranja kode

Qwen-Coder se odlikuje pri generiranju in razlagi kode v različnih programskih jezikih:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

Ta implementacija sledi najboljšim praksam z jasnimi imeni spremenljivk, celovito dokumentacijo in učinkovito logiko.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Primer uvajanja na mobilni napravi s kvantizacijo
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Naloži kvantiziran model za mobilno uvajanje

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Razvoj družine Qwen

### Qwen 1.0 in 1.5: Temeljni modeli

Zgodnji modeli Qwen so vzpostavili temeljna načela celovitega učenja in odprtokodne dostopnosti:

- **Qwen-7B (7B parametrov)**: Prva izdaja, osredotočena na razumevanje kitajskega in angleškega jezika
- **Qwen-14B (14B parametrov)**: Izboljšane sposobnosti z izboljšanim sklepanjem in znanjem
- **Qwen-72B (72B parametrov)**: Model velikega obsega, ki zagotavlja vrhunsko zmogljivost
- **Serija Qwen1.5**: Razširjena na več velikosti (0,5B do 110B) z izboljšanim obravnavanjem dolgega konteksta

### Družina Qwen2: Multimodalna razširitev

Serija Qwen2 je prinesla pomemben napredek tako v jezikovnih kot multimodalnih sposobnostih:

- **Qwen2-0.5B do 72B**: Celovit razpon jezikovnih modelov za različne potrebe uvajanja
- **Qwen2-57B-A14B (MoE)**: Arhitektura mešanice strokovnjakov za učinkovito uporabo parametrov
- **Qwen2-VL**: Napredne sposobnosti vizualnega jezika za razumevanje slik
- **Qwen2-Audio**: Sposobnosti obdelave in razumevanja zvoka
- **Qwen2-Math**: Specializirano matematično sklepanje in reševanje problemov

### Družina Qwen2.5: Izboljšana zmogljivost

Serija Qwen2.5 je prinesla pomembne izboljšave na vseh področjih:

- **Razširjeno učenje**: 18 bilijonov tokenov podatkov za učenje za izboljšane sposobnosti
- **Razširjen kontekst**: Do dolžine konteksta 128K tokenov, s Turbo različico, ki podpira 1M tokenov
- **Izboljšana specializacija**: Izboljšane različice Qwen2.5-Coder in Qwen2.5-Math
- **Boljša večjezična podpora**: Izboljšana zmogljivost v več kot 27 jezikih

### Družina Qwen3: Napredno sklepanje

Najnovejša generacija premika meje sposobnosti sklepanja in razmišljanja:

- **Qwen3-235B-A22B**: Vodilni model mešanice strokovnjakov s skupno 235B parametri
- **Qwen3-30B-A3B**: Učinkovit MoE model z močno zmogljivostjo na aktivni parameter
- **Gostejši modeli**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B za različne scenarije uvajanja
- **Način razmišljanja**: Hibridni pristop k sklepanju, ki podpira tako hitre odzive kot globoko razmišljanje
- **Večjezična odličnost**: Podpora za 119 jezikov in dialektov
- **Izboljšano učenje**: 36 bilijonov tokenov raznolikih, visokokakovostnih podatkov za učenje

## Uporabe modelov Qwen

### Aplikacije v podjetjih

Organizacije uporabljajo modele Qwen za analizo dokumentov, avtomatizacijo storitev za stranke, pomoč pri generiranju kode in aplikacije poslovne inteligence. Odprtokodna narava omogoča prilagoditev specifičnim poslovnim potrebam, hkrati pa ohranja zasebnost podatkov in nadzor.

### Mobilno in robno računalništvo

Mobilne aplikacije izkoriščajo modele Qwen za prevajanje v realnem času, inteligentne asistente, generiranje vsebine in personalizirana priporočila. Razpon velikosti modelov omogoča uvajanje od mobilnih naprav do robnih strežnikov.

### Izobraževalna tehnologija

Izobraževalne platforme uporabljajo modele Qwen za personalizirano tutorstvo, avtomatizirano generiranje vsebine, pomoč pri učenju jezikov in interaktivne izobraževalne izkušnje. Specializirani modeli, kot je Qwen-Math, zagotavljajo strokovno znanje na specifičnih področjih.

### Globalne aplikacije

Mednarodne aplikacije koristijo močne večjezične sposobnosti modelov Qwen, kar omogoča dosledne AI izkušnje v različnih jezikih in kulturnih kontekstih.

## Izzivi in omejitve

### Računalniške zahteve

Čeprav Qwen ponuja modele različnih velikosti, večje različice še vedno zahtevajo znatne računalniške vire za optimalno zmogljivost, kar lahko omeji možnosti uvajanja za nekatere organizacije.

### Zmogljivost na specializiranih področjih

Čeprav modeli Qwen dobro delujejo na splošnih področjih, lahko zelo specializirane aplikacije koristijo od finega prilagajanja ali specializiranih modelov.

### Kompleksnost izbire modela

Širok razpon razpoložljivih modelov in različic lahko oteži izbiro za uporabnike, ki so novi v ekosistemu.

### Neuravnoteženost jezikov

Čeprav podpirajo številne jezike, se zmogljivost lahko razlikuje med različnimi jeziki, pri čemer so najmočnejše sposobnosti v angleščini in kitajščini.

## Prihodnost družine modelov Qwen

Družina modelov Qwen predstavlja nenehen razvoj k demokratizirani, visokokakovostni AI. Prihodnji razvoj
- Qwen3-235B-A22B dosega konkurenčne rezultate pri ocenjevanju zmogljivosti kodiranja, matematike in splošnih sposobnosti v primerjavi z drugimi vrhunskimi modeli, kot so DeepSeek-R1, o1, o3-mini, Grok-3 in Gemini-2.5-Pro.
- Qwen3-30B-A3B presega QwQ-32B z 10-krat več aktiviranimi parametri.
- Qwen3-4B se lahko kosa z zmogljivostjo Qwen2.5-72B-Instruct.

**Dosežki učinkovitosti:**
- Osnovni modeli Qwen3-MoE dosežejo podobno zmogljivost kot osnovni modeli Qwen2.5, pri čemer uporabljajo le 10 % aktivnih parametrov.
- Pomembni prihranki pri stroških tako pri usposabljanju kot pri sklepanju v primerjavi z gostimi modeli.

**Večjezične zmogljivosti:**
- Qwen3 modeli podpirajo 119 jezikov in narečij.
- Močna zmogljivost v različnih jezikovnih in kulturnih kontekstih.

**Obseg usposabljanja:**
- Qwen3 uporablja skoraj dvakrat več podatkov, približno 36 bilijonov tokenov, ki pokrivajo 119 jezikov in narečij, v primerjavi z 18 bilijoni tokenov pri Qwen2.5.

### Primerjalna matrika modelov

| Serija modelov | Obseg parametrov | Dolžina konteksta | Ključne prednosti | Najboljše uporabe |
|----------------|------------------|-------------------|-------------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Uravnotežena zmogljivost, večjezičnost | Splošne aplikacije, produkcijska uporaba |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Generiranje kode, programiranje | Razvoj programske opreme, pomoč pri kodiranju |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Matematično sklepanje | Izobraževalne platforme, STEM aplikacije |
| **Qwen2.5-VL** | Različno | Spremenljivo | Razumevanje vizije in jezika | Multimodalne aplikacije, analiza slik |
| **Qwen3** | 0.6B-235B | Spremenljivo | Napredno sklepanje, način razmišljanja | Kompleksno sklepanje, raziskovalne aplikacije |
| **Qwen3 MoE** | 30B-235B skupaj | Spremenljivo | Učinkovita zmogljivost na velikem obsegu | Podjetniške aplikacije, potrebe po visoki zmogljivosti |

## Vodnik za izbiro modela

### Za osnovne aplikacije
- **Qwen2.5-0.5B/1.5B**: Mobilne aplikacije, robne naprave, aplikacije v realnem času
- **Qwen2.5-3B/7B**: Splošni klepetalni roboti, generiranje vsebine, sistemi vprašanj in odgovorov

### Za matematične in sklepne naloge
- **Qwen2.5-Math**: Reševanje matematičnih problemov in STEM izobraževanje
- **Qwen3 z načinom razmišljanja**: Kompleksno sklepanje, ki zahteva analizo korak za korakom

### Za programiranje in razvoj
- **Qwen2.5-Coder**: Generiranje kode, odpravljanje napak, pomoč pri programiranju
- **Qwen3**: Napredne naloge programiranja z zmogljivostjo sklepanja

### Za multimodalne aplikacije
- **Qwen2.5-VL**: Razumevanje slik, vizualno odgovarjanje na vprašanja
- **Qwen-Audio**: Obdelava zvoka in razumevanje govora

### Za podjetniško uporabo
- **Qwen2.5-32B/72B**: Visoko zmogljivo razumevanje jezika
- **Qwen3-235B-A22B**: Maksimalna zmogljivost za zahtevne aplikacije

## Platforme za uvajanje in dostopnost
### Oblakovne platforme
- **Hugging Face Hub**: Celovita zbirka modelov s podporo skupnosti
- **ModelScope**: Alibaba platforma za modele z orodji za optimizacijo
- **Različni ponudniki oblakov**: Podpora prek standardnih platform za strojno učenje

### Okviri za lokalni razvoj
- **Transformers**: Standardna integracija Hugging Face za enostavno uvajanje
- **vLLM**: Visoko zmogljivo strežnikovanje za produkcijska okolja
- **Ollama**: Poenostavljeno lokalno uvajanje in upravljanje
- **ONNX Runtime**: Optimizacija za različne strojne opreme
- **llama.cpp**: Učinkovita implementacija v C++ za različne platforme

### Učne vsebine
- **Dokumentacija Qwen**: Uradna dokumentacija in kartice modelov
- **Hugging Face Model Hub**: Interaktivni demoji in primeri skupnosti
- **Raziskovalni članki**: Tehnični članki na arxiv za poglobljeno razumevanje
- **Forumi skupnosti**: Aktivna podpora skupnosti in razprave

### Začetek z modeli Qwen

#### Platforme za razvoj
1. **Hugging Face Transformers**: Začnite s standardno Python integracijo
2. **ModelScope**: Raziščite optimizirana orodja za uvajanje od Alibabe
3. **Lokalno uvajanje**: Uporabite Ollama ali direktne transformers za lokalno testiranje

#### Učna pot
1. **Razumevanje osnovnih konceptov**: Preučite arhitekturo in zmogljivosti družine Qwen
2. **Eksperimentiranje z različicami**: Preizkusite različne velikosti modelov za razumevanje kompromisov zmogljivosti
3. **Praktična implementacija**: Uvajajte modele v razvojna okolja
4. **Optimizacija uvajanja**: Prilagodite za produkcijske primere uporabe

#### Najboljše prakse
- **Začnite z manjšimi modeli**: Začetek z manjšimi modeli (1.5B-7B) za začetni razvoj
- **Uporabite predloge za klepet**: Uporabite ustrezno oblikovanje za optimalne rezultate
- **Spremljajte vire**: Spremljajte porabo pomnilnika in hitrost sklepanja
- **Razmislite o specializaciji**: Izberite različice, specifične za področje, kadar je to primerno

## Napredni vzorci uporabe

### Primeri prilagajanja

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specializirano oblikovanje pozivov

**Za naloge kompleksnega sklepanja:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Za generiranje kode s kontekstom:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Večjezične aplikacije

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Vzorci produkcijskega uvajanja

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategije optimizacije zmogljivosti

### Optimizacija pomnilnika

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimizacija sklepanja

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Najboljše prakse in smernice

### Varnost in zasebnost

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Spremljanje in ocenjevanje

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Zaključek

Družina modelov Qwen predstavlja celovit pristop k demokratizaciji tehnologije umetne inteligence, hkrati pa ohranja konkurenčno zmogljivost v različnih aplikacijah. S svojo zavezanostjo odprtokodni dostopnosti, večjezičnim zmogljivostim in prilagodljivim možnostim uvajanja Qwen omogoča organizacijam in razvijalcem, da izkoristijo zmogljivosti umetne inteligence ne glede na njihove vire ali specifične zahteve.

### Ključne točke

**Odličnost odprte kode**: Qwen dokazuje, da lahko odprtokodni modeli dosežejo zmogljivost, konkurenčno lastniškim alternativam, hkrati pa zagotavljajo transparentnost, prilagodljivost in nadzor.

**Prilagodljiva arhitektura**: Obseg od 0.5B do 235B parametrov omogoča uvajanje v celotnem spektru računalniških okolij, od mobilnih naprav do podjetniških grozdov.

**Specializirane zmogljivosti**: Različice, specifične za področje, kot so Qwen-Coder, Qwen-Math in Qwen-VL, zagotavljajo specializirano strokovno znanje, hkrati pa ohranjajo splošno razumevanje jezika.

**Globalna dostopnost**: Močna podpora več kot 119 jezikom omogoča Qwenu uporabo v mednarodnih aplikacijah in raznolikih uporabniških bazah.

**Nenehne inovacije**: Evolucija od Qwen 1.0 do Qwen3 kaže na dosledno izboljšanje zmogljivosti, učinkovitosti in možnosti uvajanja.

### Pogled v prihodnost

Ko se družina Qwen še naprej razvija, lahko pričakujemo:

- **Izboljšano učinkovitost**: Nadaljnjo optimizacijo za boljše razmerje med zmogljivostjo in parametri
- **Razširjene multimodalne zmogljivosti**: Integracijo bolj sofisticirane obdelave vizije, zvoka in besedila
- **Izboljšano sklepanje**: Napredni mehanizmi razmišljanja in sposobnosti reševanja večkorakih problemov
- **Boljša orodja za uvajanje**: Izboljšani okviri in orodja za optimizacijo za različne scenarije uvajanja
- **Rast skupnosti**: Razširjen ekosistem orodij, aplikacij in prispevkov skupnosti

### Naslednji koraki

Ne glede na to, ali gradite klepetalni robot, razvijate izobraževalna orodja, ustvarjate pomočnike za kodiranje ali delate na večjezičnih aplikacijah, družina Qwen ponuja prilagodljive rešitve z močno podporo skupnosti in celovito dokumentacijo.

Za najnovejše posodobitve, izdaje modelov in podrobno tehnično dokumentacijo obiščite uradne repozitorije Qwen na Hugging Face in raziščite aktivne razprave skupnosti ter primere.

Prihodnost razvoja umetne inteligence leži v dostopnih, transparentnih in zmogljivih orodjih, ki omogočajo inovacije v vseh sektorjih in obsegih. Družina Qwen uteleša to vizijo, organizacijam in razvijalcem pa zagotavlja temelje za gradnjo naslednje generacije aplikacij, ki jih poganja umetna inteligenca.

## Dodatni viri

- **Uradna dokumentacija**: [Dokumentacija Qwen](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Tehnični članki**: [Raziskovalne publikacije Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Skupnost**: [GitHub razprave in težave](https://github.com/QwenLM/)
- **Platforma ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Učni cilji

Po zaključku tega modula boste lahko:

1. Razložili arhitekturne prednosti družine modelov Qwen in njen odprtokodni pristop
2. Izbrali ustrezno različico Qwen glede na specifične zahteve aplikacije in omejitve virov
3. Implementirali modele Qwen v različnih scenarijih uvajanja z optimiziranimi konfiguracijami
4. Uporabili tehnike kvantizacije in optimizacije za izboljšanje zmogljivosti modelov Qwen
5. Ocenili kompromise med velikostjo modela, zmogljivostjo in sposobnostmi v družini Qwen

## Kaj sledi

- [03: Osnove družine Gemma](03.GemmaFamily.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve AI za prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napačne razlage, ki izhajajo iz uporabe tega prevoda.