<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:28:36+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "sl"
}
-->
# Poglavje 4: Optimizacijski paket OpenVINO Toolkit

## Kazalo vsebine
1. [Uvod](../../../Module04)
2. [Kaj je OpenVINO?](../../../Module04)
3. [Namestitev](../../../Module04)
4. [Hitri vodič](../../../Module04)
5. [Primer: Pretvorba in optimizacija modelov z OpenVINO](../../../Module04)
6. [Napredna uporaba](../../../Module04)
7. [Najboljše prakse](../../../Module04)
8. [Odpravljanje težav](../../../Module04)
9. [Dodatni viri](../../../Module04)

## Uvod

OpenVINO (Open Visual Inference and Neural Network Optimization) je odprtokodni paket podjetja Intel za implementacijo zmogljivih rešitev umetne inteligence v oblaku, lokalnih okoljih in na robnih napravah. Ne glede na to, ali ciljate na CPU-je, GPU-je, VPU-je ali specializirane AI pospeševalnike, OpenVINO ponuja celovite zmogljivosti optimizacije, hkrati pa ohranja natančnost modela in omogoča večplatformsko implementacijo.

## Kaj je OpenVINO?

OpenVINO je odprtokodni paket, ki razvijalcem omogoča učinkovito optimizacijo, pretvorbo in implementacijo AI modelov na različnih strojnih platformah. Sestavljen je iz treh glavnih komponent: OpenVINO Runtime za inferenco, Neural Network Compression Framework (NNCF) za optimizacijo modelov in OpenVINO Model Server za skalabilno implementacijo.

### Ključne značilnosti

- **Večplatformska implementacija**: Podpora za Linux, Windows in macOS z Python, C++ in C API-ji
- **Strojna pospešitev**: Samodejno odkrivanje naprav in optimizacija za CPU, GPU, VPU in AI pospeševalnike
- **Okvir za stiskanje modelov**: Napredne tehnike kvantizacije, obrezovanja in optimizacije prek NNCF
- **Združljivost z ogrodji**: Neposredna podpora za modele TensorFlow, ONNX, PaddlePaddle in PyTorch
- **Podpora za generativno AI**: Specializirani OpenVINO GenAI za implementacijo velikih jezikovnih modelov in aplikacij generativne AI

### Prednosti

- **Optimizacija zmogljivosti**: Znatne izboljšave hitrosti z minimalno izgubo natančnosti
- **Zmanjšan odtis implementacije**: Minimalne zunanje odvisnosti poenostavijo namestitev in implementacijo
- **Izboljšan čas zagona**: Optimizirano nalaganje modelov in predpomnjenje za hitrejšo inicializacijo aplikacij
- **Skalabilna implementacija**: Od robnih naprav do infrastrukture v oblaku z doslednimi API-ji
- **Pripravljeno za produkcijo**: Zanesljivost na ravni podjetja z obsežno dokumentacijo in podporo skupnosti

## Namestitev

### Predpogoji

- Python 3.8 ali novejši
- Upravljalnik paketov pip
- Virtualno okolje (priporočeno)
- Združljiva strojna oprema (priporočeni Intel CPU-ji, vendar podpora za različne arhitekture)

### Osnovna namestitev

Ustvarite in aktivirajte virtualno okolje:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Namestite OpenVINO Runtime:

```bash
pip install openvino
```

Namestite NNCF za optimizacijo modelov:

```bash
pip install nncf
```

### Namestitev OpenVINO GenAI

Za aplikacije generativne AI:

```bash
pip install openvino-genai
```

### Izbirne odvisnosti

Dodatni paketi za specifične primere uporabe:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Preverjanje namestitve

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Če je uspešno, bi morali videti informacije o različici OpenVINO.

## Hitri vodič

### Vaša prva optimizacija modela

Pretvorimo in optimizirajmo Hugging Face model z OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Kaj ta postopek naredi

Delovni tok optimizacije vključuje: nalaganje izvirnega modela iz Hugging Face, pretvorbo v OpenVINO Intermediate Representation (IR) format, uporabo privzetih optimizacij in kompilacijo za ciljno strojno opremo.

### Pojasnitev ključnih parametrov

- `export=True`: Pretvori model v OpenVINO IR format
- `compile=False`: Odloži kompilacijo do časa izvajanja za večjo prilagodljivost
- `device`: Ciljna strojna oprema ("CPU", "GPU", "AUTO" za samodejno izbiro)
- `save_pretrained()`: Shrani optimiziran model za ponovno uporabo

## Primer: Pretvorba in optimizacija modelov z OpenVINO

### Korak 1: Pretvorba modela z NNCF kvantizacijo

Tukaj je, kako uporabiti kvantizacijo po treningu z NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Korak 2: Napredna optimizacija s stiskanjem uteži

Za modele, ki temeljijo na transformatorjih, uporabite stiskanje uteži:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Korak 3: Inferenca z optimiziranim modelom

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struktura izhoda

Po optimizaciji bo vaša mapa modela vsebovala:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Napredna uporaba

### Konfiguracija z NNCF YAML

Za kompleksne delovne tokove optimizacije uporabite konfiguracijske datoteke NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Uporabite konfiguracijo:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimizacija za GPU

Za pospešitev z GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimizacija obdelave v serijah

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Implementacija strežnika modelov

Implementirajte optimizirane modele z OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Odjemalska koda za strežnik modelov:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Najboljše prakse

### 1. Izbira in priprava modela
- Uporabljajte modele iz podprtih ogrodij (PyTorch, TensorFlow, ONNX)
- Poskrbite, da imajo vhodni podatki modela fiksne ali znane dinamične oblike
- Testirajte z reprezentativnimi nabori podatkov za kalibracijo

### 2. Izbira strategije optimizacije
- **Kvantizacija po treningu**: Začnite tukaj za hitro optimizacijo
- **Stiskanje uteži**: Idealno za velike jezikovne modele in transformatorje
- **Trening zavedanja kvantizacije**: Uporabite, ko je natančnost ključna

### 3. Optimizacija, specifična za strojno opremo
- **CPU**: Uporabite INT8 kvantizacijo za uravnoteženo zmogljivost
- **GPU**: Izkoristite FP16 natančnost in obdelavo v serijah
- **VPU**: Osredotočite se na poenostavitev modela in združevanje slojev

### 4. Izboljšanje zmogljivosti
- **Način prepustnosti**: Za obdelavo velikih količin podatkov v serijah
- **Način zakasnitve**: Za aplikacije v realnem času
- **AUTO naprava**: Dovolite OpenVINO, da izbere optimalno strojno opremo

### 5. Upravljanje pomnilnika
- Uporabljajte dinamične oblike premišljeno, da se izognete prekomerni porabi pomnilnika
- Implementirajte predpomnjenje modelov za hitrejše naknadne nalaganje
- Spremljajte porabo pomnilnika med optimizacijo

### 6. Validacija natančnosti
- Vedno validirajte optimizirane modele glede na izvirno zmogljivost
- Uporabljajte reprezentativne testne nabore podatkov za ocenjevanje
- Razmislite o postopni optimizaciji (začnite z bolj konservativnimi nastavitvami)

## Odpravljanje težav

### Pogoste težave

#### 1. Težave z namestitvijo
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Napake pri pretvorbi modela
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Težave z zmogljivostjo
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Težave s pomnilnikom
- Zmanjšajte velikost serije modela med optimizacijo
- Uporabite pretakanje za velike nabore podatkov
- Omogočite predpomnjenje modelov: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Poslabšanje natančnosti
- Uporabite višjo natančnost (INT8 namesto INT4)
- Povečajte velikost kalibracijskega nabora podatkov
- Uporabite optimizacijo z mešano natančnostjo

### Spremljanje zmogljivosti

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Pomoč

- **Dokumentacija**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub težave**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum skupnosti**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Dodatni viri

### Uradne povezave
- **Domača stran OpenVINO**: [openvino.ai](https://openvino.ai/)
- **GitHub repozitorij**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF repozitorij**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Učni viri
- **OpenVINO zvezki**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Hitri vodič**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Vodič za optimizacijo**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Orodja za integracijo
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Merila zmogljivosti
- **Uradna merila**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Primeri skupnosti
- **Jupyter zvezki**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Celoviti vodiči, dostopni v repozitoriju OpenVINO zvezkov
- **Primeri aplikacij**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Resnični primeri za različne domene (računalniški vid, NLP, zvok)
- **Objave na blogu**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Objave na blogu Intel AI in skupnosti z podrobnimi primeri uporabe

### Povezana orodja
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Dodatne tehnike optimizacije za Intel strojno opremo
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Za primerjave mobilne in robne implementacije
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativni večplatformski pogon za inferenco

## ➡️ Kaj sledi

- [05: Poglobljen pregled Apple MLX Framework](./05.AppleMLX.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna nesporazume ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.