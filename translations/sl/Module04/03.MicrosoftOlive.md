<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-23T00:14:37+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sl"
}
-->
# Poglavje 3: Microsoft Olive Optimization Suite

## Kazalo
1. [Uvod](../../../Module04)
2. [Kaj je Microsoft Olive?](../../../Module04)
3. [Namestitev](../../../Module04)
4. [Hitri vodič](../../../Module04)
5. [Primer: Pretvorba Qwen3 v ONNX INT4](../../../Module04)
6. [Napredna uporaba](../../../Module04)
7. [Najboljše prakse](../../../Module04)
8. [Odpravljanje težav](../../../Module04)
9. [Dodatni viri](../../../Module04)

## Uvod

Microsoft Olive je zmogljivo in enostavno orodje za optimizacijo modelov, ki upošteva strojno opremo. Poenostavi proces optimizacije modelov strojnega učenja za uporabo na različnih strojnih platformah. Ne glede na to, ali ciljate na CPU-je, GPU-je ali specializirane AI pospeševalnike, Olive omogoča doseganje optimalne zmogljivosti ob ohranjanju natančnosti modela.

## Kaj je Microsoft Olive?

Olive je enostavno orodje za optimizacijo modelov, ki upošteva strojno opremo in združuje vodilne tehnike na področju stiskanja modelov, optimizacije in prevajanja. Deluje z ONNX Runtime kot celovita rešitev za optimizacijo inferenčnega procesa.

### Ključne značilnosti

- **Optimizacija, ki upošteva strojno opremo**: Samodejno izbere najboljše tehnike optimizacije za ciljno strojno opremo
- **40+ vgrajenih komponent za optimizacijo**: Vključuje stiskanje modelov, kvantizacijo, optimizacijo grafov in več
- **Preprost CLI vmesnik**: Enostavni ukazi za pogoste naloge optimizacije
- **Podpora za več ogrodij**: Deluje s PyTorch, Hugging Face modeli in ONNX
- **Podpora za priljubljene modele**: Olive lahko samodejno optimizira priljubljene arhitekture modelov, kot so Llama, Phi, Qwen, Gemma itd.

### Prednosti

- **Skrajšan čas razvoja**: Ni potrebe po ročnem preizkušanju različnih tehnik optimizacije
- **Izboljšana zmogljivost**: Znatne izboljšave hitrosti (do 6x v nekaterih primerih)
- **Križno-platformska uporaba**: Optimizirani modeli delujejo na različnih strojnih in operacijskih sistemih
- **Ohranjena natančnost**: Optimizacije ohranjajo kakovost modela ob izboljšanju zmogljivosti

## Namestitev

### Predpogoji

- Python 3.8 ali novejši
- Upravljalnik paketov pip
- Virtualno okolje (priporočeno)

### Osnovna namestitev

Ustvarite in aktivirajte virtualno okolje:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Namestite Olive z funkcijami samodejne optimizacije:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Izbirne odvisnosti

Olive ponuja različne izbirne odvisnosti za dodatne funkcije:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Preverjanje namestitve

```bash
olive --help
```

Če je uspešno, bi morali videti sporočilo za pomoč Olive CLI.

## Hitri vodič

### Vaša prva optimizacija

Optimizirajmo majhen jezikovni model z uporabo funkcije samodejne optimizacije Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kaj naredi ta ukaz

Proces optimizacije vključuje: pridobitev modela iz lokalnega predpomnilnika, zajem ONNX grafa in shranjevanje uteži v ONNX podatkovno datoteko, optimizacijo ONNX grafa ter kvantizacijo modela na int4 z metodo RTN.

### Pojasnilo parametrov ukaza

- `--model_name_or_path`: Identifikator modela Hugging Face ali lokalna pot
- `--output_path`: Mapa, kamor bo shranjen optimizirani model
- `--device`: Ciljna naprava (cpu, gpu)
- `--provider`: Izvajalni ponudnik (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Uporaba ONNX Runtime Generate AI za inferenco
- `--precision`: Natančnost kvantizacije (int4, int8, fp16)
- `--log_level`: Stopnja podrobnosti dnevnika (0=minimalno, 1=podrobno)

## Primer: Pretvorba Qwen3 v ONNX INT4

Na podlagi podanega primera Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) je tukaj, kako optimizirati model Qwen3:

### Korak 1: Prenos modela (neobvezno)

Za zmanjšanje časa prenosa predpomnite le bistvene datoteke:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Korak 2: Optimizacija modela Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Korak 3: Testiranje optimiziranega modela

Ustvarite preprost Python skript za testiranje optimiziranega modela:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura izhoda

Po optimizaciji bo vaša izhodna mapa vsebovala:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Napredna uporaba

### Konfiguracijske datoteke

Za bolj zapletene delovne tokove optimizacije lahko uporabite JSON konfiguracijske datoteke:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Zaženite s konfiguracijo:

```bash
olive run --config config.json
```

### Optimizacija za GPU

Za optimizacijo CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Za DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fino uglaševanje z Olive

Olive podpira tudi fino uglaševanje modelov:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najboljše prakse

### 1. Izbira modela
- Začnite z manjšimi modeli za testiranje (npr. 0.5B-7B parametrov)
- Preverite, ali je vaša ciljna arhitektura modela podprta z Olive

### 2. Upoštevanje strojne opreme
- Prilagodite ciljno optimizacijo strojni opremi, na kateri bo model uporabljen
- Uporabite optimizacijo za GPU, če imate strojno opremo, združljivo s CUDA
- Razmislite o DirectML za Windows naprave z integrirano grafiko

### 3. Izbira natančnosti
- **INT4**: Največja stiskanje, rahla izguba natančnosti
- **INT8**: Dobra uravnoteženost med velikostjo in natančnostjo
- **FP16**: Minimalna izguba natančnosti, zmerno zmanjšanje velikosti

### 4. Testiranje in validacija
- Vedno testirajte optimizirane modele z vašimi specifičnimi primeri uporabe
- Primerjajte zmogljivostne metrike (zakasnitev, prepustnost, natančnost)
- Uporabite reprezentativne vhodne podatke za ocenjevanje

### 5. Iterativna optimizacija
- Začnite s samodejno optimizacijo za hitre rezultate
- Uporabite konfiguracijske datoteke za natančno prilagoditev
- Eksperimentirajte z različnimi prehodi optimizacije

## Odpravljanje težav

### Pogoste težave

#### 1. Težave pri namestitvi
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Težave s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Težave s pomnilnikom
- Uporabite manjše velikosti serij med optimizacijo
- Poskusite kvantizacijo z višjo natančnostjo najprej (int8 namesto int4)
- Prepričajte se, da imate dovolj prostora na disku za predpomnjenje modela

#### 4. Napake pri nalaganju modela
- Preverite pot modela in dovoljenja za dostop
- Preverite, ali model zahteva `trust_remote_code=True`
- Prepričajte se, da so vse potrebne datoteke modela prenesene

### Pomoč

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub težave**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Primeri**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Dodatni viri

### Uradne povezave
- **GitHub repozitorij**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face primer**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Primeri skupnosti
- **Jupyter zvezki**: Na voljo v Olive GitHub repozitoriju — https://github.com/microsoft/Olive/tree/main/examples
- **Razširitev za VS Code**: Pregled AI orodij za VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Objave na blogu**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Povezana orodja
- **ONNX Runtime**: Visoko zmogljiv inferenčni pogon — https://onnxruntime.ai/
- **Hugging Face Transformers**: Vir mnogih združljivih modelov — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Optimizacijski delovni tokovi v oblaku — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Kaj sledi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

