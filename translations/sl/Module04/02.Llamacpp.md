<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-19T00:51:19+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "sl"
}
-->
# Poglavje 2: Vodnik za implementacijo Llama.cpp

## Kazalo
1. [Uvod](../../../Module04)
2. [Kaj je Llama.cpp?](../../../Module04)
3. [Namestitev](../../../Module04)
4. [Gradnja iz izvorne kode](../../../Module04)
5. [Kvantizacija modela](../../../Module04)
6. [Osnovna uporaba](../../../Module04)
7. [Napredne funkcije](../../../Module04)
8. [Integracija s Pythonom](../../../Module04)
9. [Odpravljanje težav](../../../Module04)
10. [Najboljše prakse](../../../Module04)

## Uvod

Ta obsežen vodič vas bo popeljal skozi vse, kar morate vedeti o Llama.cpp, od osnovne namestitve do naprednih scenarijev uporabe. Llama.cpp je zmogljiva implementacija v jeziku C++, ki omogoča učinkovito izvajanje velikih jezikovnih modelov (LLM) z minimalno nastavitvijo in odlično zmogljivostjo na različnih strojnih konfiguracijah.

## Kaj je Llama.cpp?

Llama.cpp je okvir za izvajanje LLM, napisan v C/C++, ki omogoča lokalno izvajanje velikih jezikovnih modelov z minimalno nastavitvijo in vrhunsko zmogljivostjo na širokem spektru strojne opreme. Ključne značilnosti vključujejo:

### Osnovne značilnosti
- **Implementacija v čistem C/C++** brez odvisnosti
- **Združljivost med platformami** (Windows, macOS, Linux)
- **Optimizacija strojne opreme** za različne arhitekture
- **Podpora za kvantizacijo** (od 1,5-bitne do 8-bitne kvantizacije celih števil)
- **Pospeševanje na CPU in GPU**
- **Učinkovita poraba pomnilnika** za omejena okolja

### Prednosti
- Učinkovito deluje na CPU brez potrebe po specializirani strojni opremi
- Podpira več GPU backendov (CUDA, Metal, OpenCL, Vulkan)
- Lahek in prenosljiv
- Apple Silicon je prednostno obravnavan - optimiziran prek ARM NEON, Accelerate in Metal frameworkov
- Podpira različne ravni kvantizacije za zmanjšano porabo pomnilnika

## Namestitev

### Metoda 1: Vnaprej pripravljeni binarni paketi (priporočeno za začetnike)

#### Prenos iz GitHub Releases
1. Obiščite [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Prenesite ustrezen binarni paket za vaš sistem:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` za Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` za macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` za Linux

3. Razpakirajte arhiv in dodajte mapo v sistemsko pot PATH

#### Uporaba upraviteljev paketov

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (različne distribucije):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metoda 2: Python paket (llama-cpp-python)

#### Osnovna namestitev
```bash
pip install llama-cpp-python
```

#### S strojno pospešitvijo
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Gradnja iz izvorne kode

### Predpogoji

**Sistemske zahteve:**
- C++ prevajalnik (GCC, Clang ali MSVC)
- CMake (različica 3.14 ali novejša)
- Git
- Orodja za gradnjo za vašo platformo

**Namestitev predpogojev:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Namestite Visual Studio 2022 z orodji za razvoj v C++
- Namestite CMake z uradne spletne strani
- Namestite Git

### Osnovni postopek gradnje

1. **Klonirajte repozitorij:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurirajte gradnjo:**
```bash
cmake -B build
```

3. **Zgradite projekt:**
```bash
cmake --build build --config Release
```

Za hitrejšo kompilacijo uporabite vzporedna opravila:
```bash
cmake --build build --config Release -j 8
```

### Gradnje, specifične za strojno opremo

#### Podpora za CUDA (NVIDIA GPU)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Podpora za Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Podpora za OpenBLAS (optimizacija CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Podpora za Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Napredne možnosti gradnje

#### Debug gradnja
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Z dodatnimi funkcijami
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kvantizacija modela

### Razumevanje GGUF formata

GGUF (Generaliziran GGML Unified Format) je optimiziran format datotek, zasnovan za učinkovito izvajanje velikih jezikovnih modelov z uporabo Llama.cpp in drugih okvirjev. Ponuja:

- Standardizirano shranjevanje uteži modela
- Izboljšano združljivost med platformami
- Povečano zmogljivost
- Učinkovito upravljanje metapodatkov

### Vrste kvantizacije

Llama.cpp podpira različne ravni kvantizacije:

| Vrsta | Bitov | Opis | Primer uporabe |
|-------|-------|------|----------------|
| F16 | 16 | Polovična natančnost | Visoka kakovost, velik pomnilnik |
| Q8_0 | 8 | 8-bitna kvantizacija | Dobra uravnoteženost |
| Q4_0 | 4 | 4-bitna kvantizacija | Zmerna kakovost, manjša velikost |
| Q2_K | 2 | 2-bitna kvantizacija | Najmanjša velikost, nižja kakovost |

### Pretvorba modelov

#### Iz PyTorch v GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Neposreden prenos iz Hugging Face
Veliko modelov je na voljo v GGUF formatu na Hugging Face:
- Poiščite modele z "GGUF" v imenu
- Prenesite ustrezno raven kvantizacije
- Uporabite neposredno z Llama.cpp

## Osnovna uporaba

### Ukazna vrstica

#### Preprosto generiranje besedila
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Uporaba modelov iz Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Način strežnika
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Pogosti parametri

| Parameter | Opis | Primer |
|-----------|------|--------|
| `-m` | Pot do datoteke modela | `-m model.gguf` |
| `-p` | Besedilo poziva | `-p "Pozdravljen svet"` |
| `-n` | Število generiranih tokenov | `-n 100` |
| `-c` | Velikost konteksta | `-c 4096` |
| `-t` | Število niti | `-t 8` |
| `-ngl` | GPU sloji | `-ngl 32` |
| `-temp` | Temperatura | `-temp 0.7` |

### Interaktivni način

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Napredne funkcije

### API strežnika

#### Zagon strežnika
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Uporaba API-ja
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optimizacija zmogljivosti

#### Upravljanje pomnilnika
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Večnitnost
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Pospeševanje na GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integracija s Pythonom

### Osnovna uporaba z llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Klepetalni vmesnik

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Pretakanje odgovorov

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integracija z LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Odpravljanje težav

### Pogoste težave in rešitve

#### Napake pri gradnji

**Težava: CMake ni najden**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Težava: Prevajalnik ni najden**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Težave pri izvajanju

**Težava: Nalaganje modela ne uspe**
- Preverite pot do datoteke modela
- Preverite dovoljenja za datoteke
- Zagotovite dovolj RAM-a
- Poskusite z različnimi ravnmi kvantizacije

**Težava: Slaba zmogljivost**
- Omogočite strojno pospeševanje
- Povečajte število niti
- Uporabite ustrezno kvantizacijo
- Preverite uporabo GPU pomnilnika

#### Težave s pomnilnikom

**Težava: Pomanjkanje pomnilnika**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Težave, specifične za platformo

#### Windows
- Uporabite MinGW ali Visual Studio prevajalnik
- Zagotovite pravilno konfiguracijo PATH
- Preverite morebitne motnje antivirusnega programa

#### macOS
- Omogočite Metal za Apple Silicon
- Uporabite Rosetta 2 za združljivost, če je potrebno
- Preverite orodja ukazne vrstice Xcode

#### Linux
- Namestite razvojne pakete
- Preverite različice gonilnikov GPU
- Preverite namestitev CUDA orodij

## Najboljše prakse

### Izbira modela
1. **Izberite ustrezno kvantizacijo** glede na vašo strojno opremo
2. **Upoštevajte velikost modela** v primerjavi s kakovostjo
3. **Preizkusite različne modele** za vaš specifični primer uporabe

### Optimizacija zmogljivosti
1. **Uporabite pospeševanje na GPU**, če je na voljo
2. **Optimizirajte število niti** za vaš CPU
3. **Nastavite ustrezno velikost konteksta** za vaš primer uporabe
4. **Omogočite preslikavo pomnilnika** za velike modele

### Uporaba v produkciji
1. **Uporabite način strežnika** za dostop do API-ja
2. **Implementirajte ustrezno obravnavo napak**
3. **Spremljajte porabo virov**
4. **Vzpostavite beleženje in spremljanje**

### Razvojni potek dela
1. **Začnite z manjšimi modeli** za testiranje
2. **Uporabite nadzor različic** za konfiguracije modelov
3. **Dokumentirajte svoje konfiguracije**
4. **Testirajte na različnih platformah**

### Varnostni vidiki
1. **Preverite vhodne pozive**
2. **Implementirajte omejevanje hitrosti**
3. **Zavarujte API končne točke**
4. **Spremljajte vzorce zlorab**

## Zaključek

Llama.cpp ponuja zmogljiv in učinkovit način za lokalno izvajanje velikih jezikovnih modelov na različnih strojnih konfiguracijah. Ne glede na to, ali razvijate AI aplikacije, izvajate raziskave ali preprosto eksperimentirate z LLM-ji, ta okvir ponuja prilagodljivost in zmogljivost, ki sta potrebni za širok spekter primerov uporabe.

Ključne točke:
- Izberite način namestitve, ki najbolj ustreza vašim potrebam
- Optimizirajte za vašo specifično strojno konfiguracijo
- Začnite z osnovno uporabo in postopoma raziskujte napredne funkcije
- Razmislite o uporabi Python vezij za lažjo integracijo
- Upoštevajte najboljše prakse za produkcijsko uporabo

Za več informacij in posodobitve obiščite [uradni repozitorij Llama.cpp](https://github.com/ggml-org/llama.cpp) in si oglejte obsežno dokumentacijo ter razpoložljive skupnostne vire.

## ➡️ Kaj sledi

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne prevzemamo odgovornosti za morebitna napačna razumevanja ali napačne interpretacije, ki bi nastale zaradi uporabe tega prevoda.