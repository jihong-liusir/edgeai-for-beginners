<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-10-09T10:26:32+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "it"
}
-->
# EdgeAI per Principianti: Percorsi di Apprendimento e Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 2 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 3 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 4-5 | Modulo 4: Ottimizzazione del Modello (6 framework) | 4 ore |
| Giorno 6 | Modulo 5: SLMOps | 3 ore |
| Giorno 7 | Modulo 6-7: Agenti AI & Strumenti di Sviluppo | 4 ore |
| Giorno 8 | Modulo 8: Toolkit Locale Foundry (Implementazione Moderna) | 1 ora |

### Percorso di Apprendimento Concentrato (2 settimane)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione del Modello | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Agenti AI | 2 ore |
| Giorno 13-14 | Modulo 7: Strumenti di Sviluppo | 3 ore |

### Studio Part-Time (4 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Modulo 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Modulo 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione del Modello | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Sistemi Agentici SLM | 2 ore |
| Giorno 13-14 | Modulo 7: Esempi di Implementazione EdgeAI | 2 ore |

| Modulo | Data di Completamento | Ore Spese | Principali Conoscenze Acquisite |
|--------|-----------------------|-----------|--------------------------------|
| Modulo 0: Introduzione a EdgeAI | | | |
| Modulo 1: Fondamenti di EdgeAI | | | |
| Modulo 2: Fondamenti di SLM | | | |
| Modulo 3: Deployment di SLM | | | |
| Modulo 4: Ottimizzazione del Modello (6 framework) | | | |
| Modulo 5: SLMOps | | | |
| Modulo 6: Sistemi Agentici SLM | | | |
| Modulo 7: Esempi di Implementazione EdgeAI | | | |
| Esercizi Pratici | | | |
| Mini-Progetto | | | |

### Studio Part-Time (4 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Modulo 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Modulo 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

## Introduzione

Benvenuto nella guida di studio "EdgeAI per Principianti"! Questo documento è progettato per aiutarti a navigare efficacemente nei materiali del corso e massimizzare la tua esperienza di apprendimento. Fornisce percorsi di apprendimento strutturati, programmi di studio suggeriti, riassunti dei concetti chiave e risorse supplementari per approfondire la tua comprensione delle tecnologie Edge AI.

Si tratta di un corso conciso di 20 ore che offre conoscenze essenziali su EdgeAI in un formato efficiente, ideale per professionisti e studenti impegnati che desiderano acquisire rapidamente competenze pratiche in questo campo emergente.

## Panoramica del Corso

Il corso è organizzato in otto moduli completi:

0. **Introduzione a EdgeAI** - Fondamenti e contesto con applicazioni industriali e obiettivi di apprendimento
1. **Fondamenti e Trasformazione di EdgeAI** - Comprendere i concetti principali e il cambiamento tecnologico
2. **Fondamenti di Small Language Model** - Esplorare le diverse famiglie di SLM e le loro architetture
3. **Deployment di Small Language Model** - Implementare strategie pratiche di deployment
4. **Conversione del Formato del Modello e Quantizzazione** - Ottimizzazione avanzata con 6 framework, inclusi OpenVINO
5. **SLMOps - Operazioni sui Small Language Model** - Gestione del ciclo di vita e deployment in produzione
6. **Sistemi Agentici SLM** - Agenti AI, chiamata di funzioni e Protocollo di Contesto del Modello
7. **Esempi di Implementazione EdgeAI** - Toolkit AI, sviluppo su Windows e implementazioni specifiche per piattaforma
8. **Microsoft Foundry Local – Toolkit Completo per Sviluppatori** - Sviluppo locale con integrazione ibrida Azure (Modulo 08)

## Come Utilizzare Questa Guida di Studio

- **Apprendimento Progressivo**: Segui i moduli in ordine per un'esperienza di apprendimento più coerente
- **Checkpoint di Conoscenza**: Utilizza le domande di autovalutazione dopo ogni sezione
- **Pratica Pratica**: Completa gli esercizi suggeriti per rafforzare i concetti teorici
- **Risorse Supplementari**: Esplora materiali aggiuntivi per gli argomenti che ti interessano di più

## Raccomandazioni per il Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 6 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 8 ore |
| Giorno 5 | Modulo 3: Deployment di SLM | 3 ore |
| Giorno 6 | Modulo 8: Toolkit Locale Foundry | 3 ore |

### Studio Part-Time (3 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 0: Introduzione + Modulo 1: Fondamenti di EdgeAI | 7-9 ore |
| Settimana 2 | Modulo 2: Fondamenti di SLM | 7-8 ore |
| Settimana 3 | Modulo 3: Deployment di SLM (3h) + Modulo 8: Toolkit Locale Foundry (2-3h) | 5-6 ore |

## Modulo 0: Introduzione a EdgeAI

### Obiettivi Principali di Apprendimento

- Comprendere cos'è Edge AI e perché è rilevante nel panorama tecnologico odierno
- Identificare le principali industrie trasformate da Edge AI e i loro casi d'uso specifici
- Comprendere i vantaggi dei Small Language Models (SLM) per il deployment edge
- Stabilire aspettative di apprendimento chiare e risultati per l'intero corso
- Riconoscere opportunità di carriera e competenze richieste nel campo Edge AI

### Aree di Studio

#### Sezione 1: Paradigma e Definizione di Edge AI
- **Concetti Prioritari**: 
  - Edge AI vs. elaborazione AI tradizionale su cloud
  - La convergenza tra hardware, ottimizzazione del modello e esigenze aziendali
  - Deployment AI in tempo reale, preservazione della privacy e costi efficienti

#### Sezione 2: Applicazioni Industriali
- **Concetti Prioritari**: 
  - Manifattura & Industria 4.0: Manutenzione predittiva e controllo qualità
  - Sanità: Imaging diagnostico e monitoraggio dei pazienti
  - Sistemi Autonomi: Veicoli a guida autonoma e trasporti
  - Città Intelligenti: Gestione del traffico e sicurezza pubblica
  - Tecnologia di Consumo: Smartphone, dispositivi indossabili e case intelligenti

#### Sezione 3: Fondamenti dei Small Language Models
- **Concetti Prioritari**: 
  - Caratteristiche e confronti delle prestazioni degli SLM
  - Efficienza dei parametri vs. compromessi di capacità
  - Vincoli di deployment edge e strategie di ottimizzazione

#### Sezione 4: Framework di Apprendimento e Percorso di Carriera
- **Concetti Prioritari**: 
  - Architettura del corso e approccio di padronanza progressiva
  - Obiettivi di competenze tecniche e implementazione pratica
  - Opportunità di avanzamento di carriera e applicazioni industriali

### Domande di Autovalutazione

1. Quali sono le tre principali tendenze tecnologiche che hanno reso possibile Edge AI?
2. Confronta i vantaggi e le sfide di Edge AI rispetto all'AI basata su cloud.
3. Nomina tre industrie in cui Edge AI offre valore aziendale critico e spiega perché.
4. Come i Small Language Models rendono Edge AI pratico per il deployment nel mondo reale?
5. Quali sono le competenze tecniche chiave che svilupperai durante questo corso?
6. Descrivi l'approccio di apprendimento in quattro fasi utilizzato in questo corso.

### Esercizi Pratici

1. **Ricerca Industriale**: Scegli un'applicazione industriale e ricerca un'implementazione reale di Edge AI (30 minuti)
2. **Esplorazione del Modello**: Sfoglia i Small Language Models disponibili su Hugging Face e confronta il numero di parametri e le capacità (30 minuti)
3. **Pianificazione dell'Apprendimento**: Rivedi la struttura completa del corso e crea il tuo programma di studio personale (15 minuti)

### Materiali Supplementari

- [Panoramica del Mercato Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Panoramica dei Small Language Models - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Fondamenti di Edge Computing](https://www.edgecomputing.org/)

## Modulo 1: Fondamenti e Trasformazione di EdgeAI

### Obiettivi Principali di Apprendimento

- Comprendere le differenze tra AI basata su cloud e AI basata su edge
- Padroneggiare le tecniche di ottimizzazione principali per ambienti con risorse limitate
- Analizzare applicazioni reali delle tecnologie EdgeAI
- Configurare un ambiente di sviluppo per progetti EdgeAI

### Aree di Studio

#### Sezione 1: Fondamenti di EdgeAI
- **Concetti Prioritari**: 
  - Paradigmi di Edge vs. Cloud computing
  - Tecniche di quantizzazione del modello
  - Opzioni di accelerazione hardware (NPU, GPU, CPU)
  - Vantaggi in termini di privacy e sicurezza

- **Materiali Supplementari**:
  - [Documentazione TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentazione Edge Impulse](https://docs.edgeimpulse.com)

#### Sezione 2: Studi di Caso Reali
- **Concetti Prioritari**: 
  - Ecosistema di modelli Microsoft Phi & Mu
  - Implementazioni pratiche in diversi settori
  - Considerazioni sul deployment

#### Sezione 3: Guida Pratica all'Implementazione
- **Concetti Prioritari**: 
  - Configurazione dell'ambiente di sviluppo
  - Strumenti di quantizzazione e ottimizzazione
  - Metodi di valutazione per implementazioni EdgeAI

#### Sezione 4: Hardware per il Deployment Edge
- **Concetti Prioritari**: 
  - Confronti tra piattaforme hardware
  - Strategie di ottimizzazione per hardware specifico
  - Considerazioni sul deployment

### Domande di Autovalutazione

1. Confronta e contrasta l'AI basata su cloud con le implementazioni AI basate su edge.
2. Spiega tre tecniche chiave per ottimizzare i modelli per il deployment edge.
3. Quali sono i principali vantaggi di eseguire modelli AI sull'edge?
4. Descrivi il processo di quantizzazione di un modello e come influisce sulle prestazioni.
5. Spiega come diversi acceleratori hardware (NPU, GPU, CPU) influenzano il deployment EdgeAI.

### Esercizi Pratici

1. **Configurazione Rapida dell'Ambiente**: Configura un ambiente di sviluppo minimo con i pacchetti essenziali (30 minuti)
2. **Esplorazione del Modello**: Scarica ed esamina un modello di linguaggio piccolo pre-addestrato (1 ora)
3. **Quantizzazione di Base**: Prova una semplice quantizzazione su un modello piccolo (1 ora)

## Modulo 2: Fondamenti di Small Language Model

### Obiettivi Principali di Apprendimento

- Comprendere i principi architetturali delle diverse famiglie di SLM
- Confrontare le capacità dei modelli su diverse scale di parametri
- Valutare i modelli in base a efficienza, capacità e requisiti di deployment
- Riconoscere i casi d'uso appropriati per le diverse famiglie di modelli

### Aree di Studio

#### Sezione 1: Famiglia di Modelli Microsoft Phi
- **Concetti Prioritari**: 
  - Evoluzione della filosofia di design
  - Architettura orientata all'efficienza
  - Capacità specializzate

#### Sezione 2: Famiglia Qwen
- **Concetti Prioritari**: 
  - Contributi open source
  - Opzioni di deployment scalabili
  - Architettura avanzata per il ragionamento

#### Sezione 3: Famiglia Gemma
- **Concetti Prioritari**: 
  - Innovazione guidata dalla ricerca
  - Capacità multimodali
  - Ottimizzazione per dispositivi mobili

#### Sezione 4: Famiglia BitNET
- **Concetti Prioritari**: 
  - Tecnologia di quantizzazione a 1 bit
  - Framework di ottimizzazione per l'inferenza
  - Considerazioni sulla sostenibilità

#### Sezione 5: Modello Microsoft Mu
- **Concetti Prioritari**: 
  - Architettura orientata al dispositivo
  - Integrazione di sistema con Windows
  - Operazione che preserva la privacy

#### Sezione 6: Phi-Silica
- **Concetti Prioritari**: 
  - Architettura ottimizzata per NPU
  - Metriche di prestazione
  - Integrazione per sviluppatori

### Domande di Autovalutazione

1. Confronta gli approcci architetturali delle famiglie di modelli Phi e Qwen.
2. Spiega come la tecnologia di quantizzazione di BitNET differisce dalla quantizzazione tradizionale.
3. Quali sono i vantaggi unici del modello Mu per l'integrazione con Windows?
4. Descrivi come Phi-Silica sfrutta l'hardware NPU per ottimizzare le prestazioni.
5. Per un'applicazione mobile con connettività limitata, quale famiglia di modelli sarebbe più appropriata e perché?

### Esercizi pratici

1. **Confronto tra modelli**: Benchmark rapido di due diversi modelli SLM (1 ora)
2. **Generazione di testo semplice**: Implementazione di base della generazione di testo con un modello piccolo (1 ora)
3. **Ottimizzazione rapida**: Applicare una tecnica di ottimizzazione per migliorare la velocità di inferenza (1 ora)

## Modulo 3: Distribuzione di Small Language Models

### Obiettivi principali di apprendimento

- Selezionare modelli appropriati in base ai vincoli di distribuzione
- Padroneggiare tecniche di ottimizzazione per diversi scenari di distribuzione
- Implementare SLM sia in ambienti locali che cloud
- Progettare configurazioni pronte per la produzione per applicazioni EdgeAI

### Aree di studio principali

#### Sezione 1: Apprendimento avanzato degli SLM
- **Concetti prioritari**: 
  - Framework di classificazione dei parametri
  - Tecniche avanzate di ottimizzazione
  - Strategie di acquisizione dei modelli

#### Sezione 2: Distribuzione in ambienti locali
- **Concetti prioritari**: 
  - Distribuzione sulla piattaforma Ollama
  - Soluzioni locali Microsoft Foundry
  - Analisi comparativa dei framework

#### Sezione 3: Distribuzione cloud containerizzata
- **Concetti prioritari**: 
  - Inferenza ad alte prestazioni con vLLM
  - Orchestrazione dei container
  - Implementazione di ONNX Runtime

### Domande di autovalutazione

1. Quali fattori devono essere considerati nella scelta tra distribuzione locale e cloud?
2. Confronta Ollama e Microsoft Foundry Local come opzioni di distribuzione.
3. Spiega i vantaggi della containerizzazione per la distribuzione degli SLM.
4. Quali sono i principali metriche di prestazione da monitorare per un SLM distribuito su edge?
5. Descrivi un workflow completo di distribuzione, dalla selezione del modello all'implementazione in produzione.

### Esercizi pratici

1. **Distribuzione locale di base**: Distribuire un semplice SLM utilizzando Ollama (1 ora)
2. **Verifica delle prestazioni**: Eseguire un benchmark rapido sul modello distribuito (30 minuti)
3. **Integrazione semplice**: Creare un'applicazione minima che utilizza il modello distribuito (1 ora)

## Modulo 4: Conversione e quantizzazione dei formati dei modelli

### Obiettivi principali di apprendimento

- Padroneggiare tecniche avanzate di quantizzazione da 1-bit a 8-bit di precisione
- Comprendere le strategie di conversione dei formati (GGUF, ONNX)
- Implementare ottimizzazioni su sei framework (Llama.cpp, Olive, OpenVINO, MLX, sintesi dei workflow)
- Distribuire modelli ottimizzati per ambienti edge di produzione su hardware Intel, Apple e multipiattaforma

### Aree di studio principali

#### Sezione 1: Fondamenti di quantizzazione
- **Concetti prioritari**: 
  - Framework di classificazione della precisione
  - Compromessi tra prestazioni e accuratezza
  - Ottimizzazione dell'impronta di memoria

#### Sezione 2: Implementazione di Llama.cpp
- **Concetti prioritari**: 
  - Distribuzione multipiattaforma
  - Ottimizzazione del formato GGUF
  - Tecniche di accelerazione hardware

#### Sezione 3: Suite Microsoft Olive
- **Concetti prioritari**: 
  - Ottimizzazione consapevole dell'hardware
  - Distribuzione di livello enterprise
  - Workflow di ottimizzazione automatizzati

#### Sezione 4: Toolkit OpenVINO
- **Concetti prioritari**: 
  - Ottimizzazione hardware Intel
  - Framework di compressione delle reti neurali (NNCF)
  - Distribuzione di inferenza multipiattaforma
  - OpenVINO GenAI per la distribuzione di LLM

#### Sezione 5: Framework Apple MLX
- **Concetti prioritari**: 
  - Ottimizzazione per Apple Silicon
  - Architettura di memoria unificata
  - Capacità di fine-tuning LoRA

#### Sezione 6: Sintesi del workflow di sviluppo Edge AI
- **Concetti prioritari**: 
  - Architettura unificata del workflow
  - Alberi decisionali per la selezione dei framework
  - Validazione della prontezza per la produzione
  - Strategie per garantire la futura compatibilità

### Domande di autovalutazione

1. Confronta le strategie di quantizzazione tra diversi livelli di precisione (1-bit a 8-bit).
2. Spiega i vantaggi del formato GGUF per la distribuzione su edge.
3. In che modo l'ottimizzazione consapevole dell'hardware in Microsoft Olive migliora l'efficienza della distribuzione?
4. Quali sono i principali vantaggi del NNCF di OpenVINO per la compressione dei modelli?
5. Descrivi come Apple MLX sfrutta l'architettura di memoria unificata per l'ottimizzazione.
6. In che modo la sintesi del workflow aiuta nella selezione dei framework di ottimizzazione ottimali?

### Esercizi pratici

1. **Quantizzazione del modello**: Applicare diversi livelli di quantizzazione a un modello e confrontare i risultati (1 ora)
2. **Ottimizzazione OpenVINO**: Utilizzare NNCF per comprimere un modello per hardware Intel (1 ora)
3. **Confronto tra framework**: Testare lo stesso modello su tre diversi framework di ottimizzazione (1 ora)
4. **Benchmark delle prestazioni**: Misurare l'impatto dell'ottimizzazione sulla velocità di inferenza e sull'uso della memoria (1 ora)

## Modulo 5: SLMOps - Operazioni sui Small Language Models

### Obiettivi principali di apprendimento

- Comprendere i principi di gestione del ciclo di vita di SLMOps
- Padroneggiare tecniche di distillazione e fine-tuning per la distribuzione su edge
- Implementare strategie di distribuzione in produzione con monitoraggio
- Costruire workflow di operazioni e manutenzione di SLM di livello enterprise

### Aree di studio principali

#### Sezione 1: Introduzione a SLMOps
- **Concetti prioritari**: 
  - Cambiamento di paradigma di SLMOps nelle operazioni AI
  - Architettura orientata alla privacy e all'efficienza dei costi
  - Impatto strategico sul business e vantaggi competitivi

#### Sezione 2: Distillazione del modello
- **Concetti prioritari**: 
  - Tecniche di trasferimento della conoscenza
  - Implementazione del processo di distillazione a due fasi
  - Workflow di distillazione su Azure ML

#### Sezione 3: Strategie di fine-tuning
- **Concetti prioritari**: 
  - Fine-tuning efficiente dei parametri (PEFT)
  - Metodi avanzati LoRA e QLoRA
  - Addestramento multi-adapter e ottimizzazione degli iperparametri

#### Sezione 4: Distribuzione in produzione
- **Concetti prioritari**: 
  - Conversione e quantizzazione dei modelli per la produzione
  - Configurazione della distribuzione Foundry Local
  - Benchmark delle prestazioni e validazione della qualità

### Domande di autovalutazione

1. In che modo SLMOps differisce da MLOps tradizionali?
2. Spiega i vantaggi della distillazione del modello per la distribuzione su edge.
3. Quali sono le considerazioni chiave per il fine-tuning degli SLM in ambienti con risorse limitate?
4. Descrivi un pipeline completo di distribuzione in produzione per applicazioni Edge AI.

### Esercizi pratici

1. **Distillazione di base**: Creare un modello più piccolo da un modello insegnante più grande (1 ora)
2. **Esperimento di fine-tuning**: Fine-tuning di un modello per un dominio specifico (1 ora)
3. **Pipeline di distribuzione**: Configurare un pipeline CI/CD di base per la distribuzione del modello (1 ora)

## Modulo 6: Sistemi agentici SLM - Agenti AI e chiamata di funzioni

### Obiettivi principali di apprendimento

- Costruire agenti AI intelligenti per ambienti edge utilizzando Small Language Models
- Implementare capacità di chiamata di funzioni con workflow sistematici
- Padroneggiare l'integrazione del Model Context Protocol (MCP) per interazioni standardizzate con strumenti
- Creare sistemi agentici sofisticati con intervento umano minimo

### Aree di studio principali

#### Sezione 1: Agenti AI e fondamenti SLM
- **Concetti prioritari**: 
  - Framework di classificazione degli agenti (riflessivi, basati su modelli, basati su obiettivi, agenti di apprendimento)
  - Analisi dei compromessi tra SLM e LLM
  - Modelli di progettazione specifici per agenti su edge
  - Ottimizzazione delle risorse per gli agenti

#### Sezione 2: Chiamata di funzioni nei Small Language Models
- **Concetti prioritari**: 
  - Implementazione di workflow sistematici (rilevamento dell'intento, output JSON, esecuzione esterna)
  - Implementazioni specifiche per piattaforma (Phi-4-mini, modelli Qwen selezionati, Microsoft Foundry Local)
  - Esempi avanzati (collaborazione multi-agente, selezione dinamica degli strumenti)
  - Considerazioni per la produzione (limitazione del tasso, registrazione degli audit, misure di sicurezza)

#### Sezione 3: Integrazione del Model Context Protocol (MCP)
- **Concetti prioritari**: 
  - Architettura del protocollo e progettazione del sistema a strati
  - Supporto multi-backend (Ollama per lo sviluppo, vLLM per la produzione)
  - Protocolli di connessione (modalità STDIO e SSE)
  - Applicazioni reali (automazione web, elaborazione dati, integrazione API)

### Domande di autovalutazione

1. Quali sono le considerazioni architettoniche chiave per gli agenti AI su edge?
2. In che modo la chiamata di funzioni migliora le capacità degli agenti?
3. Spiega il ruolo del Model Context Protocol nella comunicazione degli agenti.

### Esercizi pratici

1. **Agente semplice**: Costruire un agente AI di base con chiamata di funzioni (1 ora)
2. **Integrazione MCP**: Implementare MCP in un'applicazione agente (30 minuti)

## Workshop: Percorso di apprendimento pratico

### Obiettivi principali di apprendimento

- Costruire applicazioni AI pronte per la produzione utilizzando Foundry Local SDK e best practice
- Implementare modelli completi di gestione degli errori e feedback degli utenti
- Creare pipeline RAG con valutazione della qualità e monitoraggio delle prestazioni
- Sviluppare sistemi multi-agente con modelli di coordinamento
- Padroneggiare il routing intelligente dei modelli per la selezione basata sui compiti
- Distribuire soluzioni AI locali con architetture orientate alla privacy

### Aree di studio principali

#### Sessione 01: Introduzione a Foundry Local
- **Concetti prioritari**:
  - Integrazione SDK FoundryLocalManager e scoperta automatica dei servizi
  - Implementazioni di chat di base e streaming
  - Modelli di gestione degli errori e feedback degli utenti
  - Configurazione basata sull'ambiente

#### Sessione 02: Costruire soluzioni AI con RAG
- **Concetti prioritari**:
  - Embedding vettoriali in memoria con sentence-transformers
  - Implementazione della pipeline RAG (retrieve → generate)
  - Valutazione della qualità con metriche RAGAS
  - Sicurezza degli import per dipendenze opzionali

#### Sessione 03: Modelli open source
- **Concetti prioritari**:
  - Strategie di benchmarking multi-modello
  - Misurazioni di latenza e throughput
  - Degradazione graduale e recupero dagli errori
  - Confronto delle prestazioni tra famiglie di modelli

#### Sessione 04: Modelli all'avanguardia
- **Concetti prioritari**:
  - Metodologia di confronto tra SLM e LLM
  - Suggerimenti sui tipi e formattazione completa degli output
  - Gestione degli errori per modello
  - Risultati strutturati per analisi

#### Sessione 05: Agenti AI potenziati
- **Concetti prioritari**:
  - Orchestrazione multi-agente con modello di coordinamento
  - Gestione della memoria degli agenti e tracciamento dello stato
  - Gestione degli errori della pipeline e registrazione delle fasi
  - Monitoraggio delle prestazioni e statistiche

#### Sessione 06: Modelli come strumenti
- **Concetti prioritari**:
  - Rilevamento dell'intento e corrispondenza dei pattern
  - Algoritmi di routing dei modelli basati su parole chiave
  - Pipeline multi-step (pianifica → esegui → perfeziona)
  - Documentazione completa delle funzioni

### Domande di autovalutazione

1. In che modo FoundryLocalManager semplifica la gestione dei servizi rispetto alle chiamate REST manuali?
2. Spiega l'importanza delle protezioni di import per dipendenze opzionali come sentence-transformers.
3. Quali strategie garantiscono una degradazione graduale nel benchmarking multi-modello?
4. In che modo il modello di coordinamento orchestra più agenti specialisti?
5. Descrivi i componenti di un router intelligente per modelli.
6. Quali sono gli elementi chiave per una gestione degli errori pronta per la produzione?

### Esercizi pratici

1. **Applicazione di chat**: Implementare una chat in streaming con gestione degli errori (45 minuti)
2. **Pipeline RAG**: Costruire una pipeline RAG minima con valutazione della qualità (1 ora)
3. **Benchmarking dei modelli**: Confrontare 3+ modelli sulle prestazioni (1 ora)
4. **Sistema multi-agente**: Creare un coordinatore con 2 agenti specialisti (1,5 ore)
5. **Router intelligente**: Costruire una selezione di modelli basata sui compiti (1 ora)
6. **Distribuzione in produzione**: Aggiungere monitoraggio e gestione completa degli errori (45 minuti)

### Allocazione del tempo

**Apprendimento concentrato (1 settimana)**:
- Giorno 1: Sessione 01-02 (Chat + RAG) - 3 ore
- Giorno 2: Sessione 03-04 (Benchmarking + Confronto) - 3 ore
- Giorno 3: Sessione 05-06 (Agenti + Routing) - 3 ore
- Giorno 4: Esercizi pratici e validazione - 2 ore

**Studio part-time (2 settimane)**:
- Settimana 1: Sessioni 01-03 (6 ore totali)
- Settimana 2: Sessioni 04-06 + esercizi (5 ore totali)

## Modulo 7: Esempi di implementazione EdgeAI

### Obiettivi principali di apprendimento

- Padroneggiare AI Toolkit per Visual Studio Code per workflow di sviluppo EdgeAI completi
- Acquisire competenze nella piattaforma Windows AI Foundry e nelle strategie di ottimizzazione NPU
- Implementare EdgeAI su più piattaforme hardware e scenari di distribuzione
- Costruire applicazioni EdgeAI pronte per la produzione con ottimizzazioni specifiche per piattaforma

### Aree di studio principali

#### Sezione 1: AI Toolkit per Visual Studio Code
- **Concetti prioritari**: 
  - Ambiente di sviluppo Edge AI completo all'interno di VS Code
  - Catalogo dei modelli e scoperta per la distribuzione su edge
  - Workflow di test locale, ottimizzazione e sviluppo di agenti
  - Monitoraggio delle prestazioni e valutazione per scenari edge

#### Sezione 2: Guida allo sviluppo Windows EdgeAI
- **Concetti prioritari**: 
  - Panoramica completa della piattaforma Windows AI Foundry
  - API Phi Silica per inferenza NPU efficiente
  - API di Computer Vision per elaborazione immagini e OCR
  - CLI Foundry Local per sviluppo e test locali

#### Sezione 3: Implementazioni specifiche per piattaforma
- **Concetti prioritari**: 
  - Distribuzione su NVIDIA Jetson Orin Nano (prestazioni AI 67 TOPS)
  - Applicazioni mobili con .NET MAUI e ONNX Runtime GenAI
  - Soluzioni Azure EdgeAI con architettura ibrida cloud-edge
  - Ottimizzazione Windows ML con supporto hardware universale
  - Applicazioni Foundry Local con implementazione RAG orientata alla privacy

### Domande di autovalutazione

1. In che modo AI Toolkit semplifica il workflow di sviluppo EdgeAI?
2. Confronta le strategie di distribuzione su diverse piattaforme hardware.
3. Quali sono i vantaggi di Windows AI Foundry per lo sviluppo su edge?
4. Spiega il ruolo dell'ottimizzazione NPU nelle moderne applicazioni di Edge AI.  
5. Come sfrutta l'API Phi Silica l'hardware NPU per ottimizzare le prestazioni?  
6. Confronta i vantaggi del deployment locale rispetto a quello su cloud per applicazioni sensibili alla privacy.  

### Esercizi pratici  

1. **Configurazione di AI Toolkit**: Configura AI Toolkit e ottimizza un modello (1 ora)  
2. **Windows AI Foundry**: Crea una semplice applicazione Windows AI utilizzando l'API Phi Silica (1 ora)  
3. **Deployment multipiattaforma**: Distribuisci lo stesso modello su due piattaforme diverse (1 ora)  
4. **Ottimizzazione NPU**: Testa le prestazioni della NPU con gli strumenti di Windows AI Foundry (30 minuti)  

## Modulo 8: Microsoft Foundry Local – Toolkit completo per sviluppatori (modernizzato)  

### Obiettivi principali di apprendimento  

- Installare e configurare Foundry Local con l'integrazione moderna dell'SDK  
- Implementare sistemi multi-agente avanzati con schemi di coordinamento  
- Creare router di modelli intelligenti con selezione automatica basata sui compiti  
- Distribuire soluzioni AI pronte per la produzione con monitoraggio completo  
- Integrare con Azure AI Foundry per scenari di deployment ibrido  
- Padroneggiare i modelli moderni di SDK con FoundryLocalManager e il client OpenAI  

### Aree di studio principali  

#### Sezione 1: Installazione e configurazione moderna  
- **Concetti prioritari**:  
  - Integrazione dell'SDK FoundryLocalManager  
  - Scoperta automatica dei servizi e monitoraggio dello stato  
  - Modelli di configurazione basati sull'ambiente  
  - Considerazioni per il deployment in produzione  

#### Sezione 2: Sistemi multi-agente avanzati  
- **Concetti prioritari**:  
  - Schema di coordinamento con agenti specializzati  
  - Specializzazione degli agenti per recupero, ragionamento ed esecuzione  
  - Meccanismi di feedback loop per il miglioramento  
  - Monitoraggio delle prestazioni e tracciamento delle statistiche  

#### Sezione 3: Routing intelligente dei modelli  
- **Concetti prioritari**:  
  - Algoritmi di selezione dei modelli basati su parole chiave  
  - Supporto per modelli multipli (generale, ragionamento, codice, creativo)  
  - Configurazione delle variabili d'ambiente per maggiore flessibilità  
  - Controllo dello stato del servizio e gestione degli errori  

#### Sezione 4: Implementazione pronta per la produzione  
- **Concetti prioritari**:  
  - Gestione completa degli errori e meccanismi di fallback  
  - Monitoraggio delle richieste e tracciamento delle prestazioni  
  - Esempi interattivi in Jupyter notebook con benchmark  
  - Modelli di integrazione con applicazioni esistenti  

### Domande di autovalutazione  

1. In che modo l'approccio moderno di FoundryLocalManager differisce dalle chiamate REST manuali?  
2. Spiega lo schema di coordinamento e come orchestra gli agenti specializzati.  
3. Come il router intelligente seleziona i modelli appropriati in base al contenuto della query?  
4. Quali sono i componenti chiave di un sistema di agenti AI pronto per la produzione?  
5. Come implementare un monitoraggio completo dello stato per i servizi di Foundry Local?  
6. Confronta i vantaggi dell'approccio modernizzato rispetto ai modelli di implementazione tradizionali.  

### Esercizi pratici  

1. **Configurazione moderna dell'SDK**: Configura FoundryLocalManager con scoperta automatica dei servizi (30 minuti)  
2. **Sistema multi-agente**: Esegui il coordinatore avanzato con agenti specializzati (30 minuti)  
3. **Routing intelligente**: Testa il router dei modelli con diversi tipi di query (30 minuti)  
4. **Esplorazione interattiva**: Usa i notebook Jupyter per esplorare funzionalità avanzate (45 minuti)  
5. **Deployment in produzione**: Implementa modelli di monitoraggio e gestione degli errori (30 minuti)  
6. **Integrazione ibrida**: Configura scenari di fallback con Azure AI Foundry (30 minuti)  

## Guida alla distribuzione del tempo  

Per aiutarti a sfruttare al meglio le 30 ore del corso (incluso il workshop), ecco una suddivisione suggerita del tempo:  

| Attività | Allocazione del tempo | Descrizione |  
|----------|-----------------------|-------------|  
| Lettura dei materiali principali | 12 ore | Concentrati sui concetti essenziali di ogni modulo |  
| Esercizi pratici | 10 ore | Implementazione pratica delle tecniche chiave (incluso il workshop) |  
| Autovalutazione | 3 ore | Verifica della comprensione tramite domande e riflessioni |  
| Mini-progetto | 5 ore | Applicazione delle conoscenze a una piccola implementazione pratica |  

### Aree di focus principali in base al tempo disponibile  

**Se hai solo 10 ore:**  
- Completa il Modulo 0 (Introduzione) e i Moduli 1, 2 e 3 (concetti base di EdgeAI)  
- Svolgi almeno un esercizio pratico per modulo  
- Concentrati sulla comprensione dei concetti principali piuttosto che sui dettagli di implementazione  

**Se puoi dedicare 20 ore:**  
- Completa tutti gli otto moduli (inclusa l'Introduzione)  
- Esegui gli esercizi pratici principali di ogni modulo  
- Completa un mini-progetto dal Modulo 7  
- Esplora almeno 2-3 risorse supplementari  

**Se hai più di 20 ore:**  
- Completa tutti i moduli (inclusa l'Introduzione) con esercizi dettagliati  
- Realizza più mini-progetti  
- Esplora tecniche avanzate di ottimizzazione nel Modulo 4  
- Implementa il deployment in produzione dal Modulo 5  

## Risorse essenziali  

Queste risorse selezionate con cura offrono il massimo valore per il tuo tempo di studio limitato:  

### Documentazione da leggere assolutamente  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Lo strumento più efficiente per l'ottimizzazione dei modelli  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Il modo più veloce per distribuire SLM localmente  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Riferimento per un modello ottimizzato per l'edge  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Toolkit completo di ottimizzazione di Intel  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Ambiente di sviluppo integrato per EdgeAI  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Piattaforma di sviluppo EdgeAI specifica per Windows  

### Strumenti per risparmiare tempo  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accesso rapido ai modelli e distribuzione  
- [Gradio](https://www.gradio.app/docs/interface) - Sviluppo rapido di interfacce utente per demo AI  
- [Microsoft Olive](https://github.com/microsoft/Olive) - Ottimizzazione semplificata dei modelli  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferenza efficiente su CPU  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework di compressione delle reti neurali  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit per la distribuzione di modelli di linguaggio di grandi dimensioni  

## Modello per il monitoraggio dei progressi  

Usa questo modello semplificato per monitorare i tuoi progressi di apprendimento durante il corso di 20 ore:  

| Modulo | Data di completamento | Ore dedicate | Punti chiave appresi |  
|--------|-----------------------|--------------|-----------------------|  
| Modulo 0: Introduzione a EdgeAI | | | |  
| Modulo 1: Fondamenti di EdgeAI | | | |  
| Modulo 2: Fondamenti di SLM | | | |  
| Modulo 3: Deployment di SLM | | | |  
| Modulo 4: Ottimizzazione dei modelli | | | |  
| Modulo 5: SLMOps | | | |  
| Modulo 6: Agenti AI | | | |  
| Modulo 7: Strumenti di sviluppo | | | |  
| Workshop: Apprendimento pratico | | | |  
| Modulo 8: Foundry Local Toolkit | | | |  
| Esercizi pratici | | | |  
| Mini-progetto | | | |  

## Idee per mini-progetti  

Considera di completare uno di questi progetti per mettere in pratica i concetti di EdgeAI (ciascuno progettato per durare 2-4 ore):  

### Progetti per principianti (2-3 ore ciascuno)  
1. **Assistente testuale per Edge**: Crea un semplice strumento offline per il completamento del testo utilizzando un piccolo modello linguistico  
2. **Dashboard di confronto modelli**: Crea una visualizzazione base delle metriche di prestazione per diversi SLM  
3. **Esperimento di ottimizzazione**: Misura l'impatto di diversi livelli di quantizzazione sullo stesso modello di base  

### Progetti intermedi (3-4 ore ciascuno)  
4. **Flusso di lavoro AI Toolkit**: Usa VS Code AI Toolkit per ottimizzare e distribuire un modello dall'inizio alla fine  
5. **Applicazione Windows AI Foundry**: Crea un'app Windows utilizzando l'API Phi Silica e l'ottimizzazione NPU  
6. **Deployment multipiattaforma**: Distribuisci lo stesso modello ottimizzato su Windows (OpenVINO) e mobile (.NET MAUI)  
7. **Agente con chiamata di funzioni**: Crea un agente AI con capacità di chiamata di funzioni per scenari edge  

### Progetti avanzati di integrazione (4-5 ore ciascuno)  
8. **Pipeline di ottimizzazione OpenVINO**: Implementa l'ottimizzazione completa del modello utilizzando NNCF e il toolkit GenAI  
9. **Pipeline SLMOps**: Implementa un ciclo di vita completo del modello, dalla formazione alla distribuzione su edge  
10. **Sistema Edge multi-modello**: Distribuisci più modelli specializzati che lavorano insieme su hardware edge  
11. **Sistema di integrazione MCP**: Crea un sistema agentico utilizzando il Model Context Protocol per l'interazione con gli strumenti  

## Riferimenti  

- Microsoft Learn (Foundry Local)  
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Inizia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Riferimento CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integrazione con SDK di inferenza: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Come utilizzare Open WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Compilare modelli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agenti (panoramica): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Strumenti di ottimizzazione e inferenza  
  - Microsoft Olive (docs): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (iniziare): https://onnxruntime.ai/docs/get-started/with-python.html  
  - Integrazione ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (docs): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (docs): https://ml-explore.github.io/mlx/build/html/index.html  
- Framework di distribuzione e modelli  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (docs): https://docs.vllm.ai/  
  - Ollama (quick start): https://github.com/ollama/ollama#get-started  
- Strumenti per sviluppatori (Windows e VS Code)  
  - AI Toolkit per VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (panoramica): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Comunità di apprendimento  

Unisciti alla discussione e connettiti con altri studenti:  
- Discussioni su GitHub nel [repository EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Conclusione  

EdgeAI rappresenta l'avanguardia dell'implementazione dell'intelligenza artificiale, portando potenti capacità direttamente sui dispositivi e affrontando preoccupazioni critiche come la privacy, la latenza e la connettività. Questo corso di 20 ore ti fornisce le conoscenze essenziali e le competenze pratiche per iniziare subito a lavorare con le tecnologie EdgeAI.  

Il corso è volutamente conciso e focalizzato sui concetti più importanti, permettendoti di acquisire rapidamente competenze preziose senza un impegno di tempo eccessivo. Ricorda che la pratica diretta, anche con esempi semplici, è la chiave per rafforzare ciò che hai appreso.  

Buono studio!  

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.