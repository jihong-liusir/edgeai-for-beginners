<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T08:35:56+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "it"
}
-->
# EdgeAI per Principianti: Percorsi di Apprendimento e Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Argomento | Ore Stimate |
|--------|-----------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 2 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 3 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 4-5 | Modulo 4: Ottimizzazione dei Modelli (6 framework) | 4 ore |
| Giorno 6 | Modulo 5: SLMOps | 3 ore |
| Giorno 7 | Moduli 6-7: Agenti AI & Strumenti di Sviluppo | 4 ore |
| Giorno 8 | Modulo 8: Foundry Local Toolkit (Implementazione Moderna) | 1 ora |

### Percorso di Apprendimento Concentrato (2 settimane)

| Giorno | Argomento | Ore Stimate |
|--------|-----------|-------------|
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione dei Modelli | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Agenti AI | 2 ore |
| Giorno 13-14 | Modulo 7: Strumenti di Sviluppo | 3 ore |

### Studio Part-Time (4 settimane)

| Settimana | Argomento | Ore Stimate |
|-----------|-----------|-------------|
| Settimana 1 | Moduli 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Moduli 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Moduli 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

| Giorno | Argomento | Ore Stimate |
|--------|-----------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione dei Modelli | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Sistemi Agentici SLM | 2 ore |
| Giorno 13-14 | Modulo 7: Esempi di Implementazione EdgeAI | 2 ore |

| Modulo | Data di Completamento | Ore Spese | Principali Apprendimenti |
|--------|-----------------------|-----------|--------------------------|
| Modulo 0: Introduzione a EdgeAI | | | |
| Modulo 1: Fondamenti di EdgeAI | | | |
| Modulo 2: Fondamenti di SLM | | | |
| Modulo 3: Deployment di SLM | | | |
| Modulo 4: Ottimizzazione dei Modelli (6 framework) | | | |
| Modulo 5: SLMOps | | | |
| Modulo 6: Sistemi Agentici SLM | | | |
| Modulo 7: Esempi di Implementazione EdgeAI | | | |
| Esercizi Pratici | | | |
| Mini-Progetto | | | |

### Studio Part-Time (4 settimane)

| Settimana | Argomento | Ore Stimate |
|-----------|-----------|-------------|
| Settimana 1 | Moduli 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Moduli 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Moduli 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

## Introduzione

Benvenuto nella guida di studio "EdgeAI per Principianti"! Questo documento è progettato per aiutarti a navigare efficacemente nei materiali del corso e massimizzare la tua esperienza di apprendimento. Fornisce percorsi di apprendimento strutturati, programmi di studio suggeriti, riassunti dei concetti chiave e risorse supplementari per approfondire la tua comprensione delle tecnologie Edge AI.

Questo corso, della durata di 20 ore, offre conoscenze essenziali su EdgeAI in un formato efficiente, ideale per professionisti e studenti impegnati che desiderano acquisire rapidamente competenze pratiche in questo campo emergente.

## Panoramica del Corso

Il corso è organizzato in otto moduli completi:

0. **Introduzione a EdgeAI** - Fondamenti e contesto con applicazioni industriali e obiettivi di apprendimento
1. **Fondamenti e Trasformazione di EdgeAI** - Comprendere i concetti principali e il cambiamento tecnologico
2. **Fondamenti di Small Language Models (SLM)** - Esplorare le diverse famiglie di SLM e le loro architetture
3. **Deployment di Small Language Models** - Implementare strategie pratiche di deployment
4. **Conversione e Quantizzazione dei Modelli** - Ottimizzazione avanzata con 6 framework, incluso OpenVINO
5. **SLMOps - Operazioni sui Small Language Models** - Gestione del ciclo di vita e deployment in produzione
6. **Sistemi Agentici SLM** - Agenti AI, chiamate di funzione e Model Context Protocol
7. **Esempi di Implementazione EdgeAI** - Toolkit AI, sviluppo su Windows e implementazioni specifiche per piattaforma
8. **Microsoft Foundry Local – Toolkit Completo per Sviluppatori** - Sviluppo locale con integrazione ibrida Azure (Modulo 08)

## Come Usare Questa Guida di Studio

- **Apprendimento Progressivo**: Segui i moduli in ordine per un'esperienza di apprendimento coerente
- **Checkpoint di Conoscenza**: Usa le domande di autovalutazione alla fine di ogni sezione
- **Pratica Pratica**: Completa gli esercizi suggeriti per rafforzare i concetti teorici
- **Risorse Supplementari**: Esplora materiali aggiuntivi per gli argomenti che ti interessano di più

## Raccomandazioni per il Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Argomento | Ore Stimate |
|--------|-----------|-------------|
| Giorno 0 | Modulo 0: Introduzione a EdgeAI | 1-2 ore |
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 6 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 8 ore |
| Giorno 5 | Modulo 3: Deployment di SLM | 3 ore |
| Giorno 6 | Modulo 8: Foundry Local Toolkit | 3 ore |

### Studio Part-Time (3 settimane)

| Settimana | Argomento | Ore Stimate |
|-----------|-----------|-------------|
| Settimana 1 | Modulo 0: Introduzione + Modulo 1: Fondamenti di EdgeAI | 7-9 ore |
| Settimana 2 | Modulo 2: Fondamenti di SLM | 7-8 ore |
| Settimana 3 | Modulo 3: Deployment di SLM (3h) + Modulo 8: Foundry Local Toolkit (2-3h) | 5-6 ore |

## Modulo 0: Introduzione a EdgeAI

### Obiettivi di Apprendimento Chiave

- Comprendere cos'è Edge AI e perché è rilevante nel panorama tecnologico odierno
- Identificare le principali industrie trasformate da Edge AI e i loro casi d'uso specifici
- Comprendere i vantaggi degli Small Language Models (SLM) per il deployment edge
- Stabilire aspettative e risultati di apprendimento chiari per l'intero corso
- Riconoscere le opportunità di carriera e le competenze richieste nel campo di Edge AI

### Aree di Studio

#### Sezione 1: Paradigma e Definizione di Edge AI
- **Concetti Prioritari**: 
  - Edge AI vs. elaborazione AI tradizionale su cloud
  - Convergenza tra hardware, ottimizzazione dei modelli e esigenze aziendali
  - Deployment AI in tempo reale, con preservazione della privacy e costi ridotti

#### Sezione 2: Applicazioni Industriali
- **Concetti Prioritari**: 
  - Manifattura & Industria 4.0: Manutenzione predittiva e controllo qualità
  - Sanità: Imaging diagnostico e monitoraggio dei pazienti
  - Sistemi Autonomi: Veicoli a guida autonoma e trasporti
  - Città Intelligenti: Gestione del traffico e sicurezza pubblica
  - Tecnologia di Consumo: Smartphone, dispositivi indossabili e case intelligenti

#### Sezione 3: Fondamenti di Small Language Models
- **Concetti Prioritari**: 
  - Caratteristiche e confronti di performance degli SLM
  - Trade-off tra efficienza dei parametri e capacità
  - Vincoli di deployment edge e strategie di ottimizzazione

#### Sezione 4: Struttura di Apprendimento e Percorso di Carriera
- **Concetti Prioritari**: 
  - Architettura del corso e approccio di padronanza progressiva
  - Competenze tecniche e obiettivi di implementazione pratica
  - Opportunità di carriera e applicazioni industriali

### Domande di Autovalutazione

1. Quali sono le tre principali tendenze tecnologiche che hanno reso possibile Edge AI?
2. Confronta i vantaggi e le sfide di Edge AI rispetto all'AI basata su cloud.
3. Nomina tre industrie in cui Edge AI offre valore aziendale critico e spiega perché.
4. Come gli Small Language Models rendono Edge AI pratico per il deployment nel mondo reale?
5. Quali sono le competenze tecniche chiave che svilupperai durante questo corso?
6. Descrivi l'approccio di apprendimento in quattro fasi utilizzato in questo corso.

### Esercizi Pratici

1. **Ricerca Industriale**: Scegli un'applicazione industriale e ricerca un'implementazione reale di Edge AI (30 minuti)
2. **Esplorazione Modelli**: Esamina gli Small Language Models disponibili su Hugging Face e confronta il numero di parametri e le capacità (30 minuti)
3. **Pianificazione dello Studio**: Rivedi la struttura completa del corso e crea il tuo programma di studio personale (15 minuti)

### Materiali Supplementari

- [Panoramica del Mercato Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Panoramica Small Language Models - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Fondazione Edge Computing](https://www.edgecomputing.org/)

## Modulo 1: Fondamenti e Trasformazione di EdgeAI

### Obiettivi di Apprendimento Chiave

- Comprendere le differenze tra AI basata su cloud e AI basata su edge
- Padroneggiare le tecniche di ottimizzazione principali per ambienti con risorse limitate
- Analizzare applicazioni reali delle tecnologie EdgeAI
- Configurare un ambiente di sviluppo per progetti EdgeAI

### Aree di Studio

#### Sezione 1: Fondamenti di EdgeAI
- **Concetti Prioritari**: 
  - Paradigmi di Edge vs. Cloud computing
  - Tecniche di quantizzazione dei modelli
  - Opzioni di accelerazione hardware (NPU, GPU, CPU)
  - Vantaggi in termini di privacy e sicurezza

- **Materiali Supplementari**:
  - [Documentazione TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentazione Edge Impulse](https://docs.edgeimpulse.com)

#### Sezione 2: Casi Studio Reali
- **Concetti Prioritari**: 
  - Ecosistema di modelli Microsoft Phi & Mu
  - Implementazioni pratiche in diversi settori
  - Considerazioni sul deployment

#### Sezione 3: Guida Pratica all'Implementazione
- **Concetti Prioritari**: 
  - Configurazione dell'ambiente di sviluppo
  - Strumenti di quantizzazione e ottimizzazione
  - Metodi di valutazione per implementazioni EdgeAI

#### Sezione 4: Hardware per il Deployment Edge
- **Concetti Prioritari**: 
  - Confronti tra piattaforme hardware
  - Strategie di ottimizzazione per hardware specifici
  - Considerazioni sul deployment

### Domande di Autovalutazione

1. Confronta e contrasta le implementazioni AI basate su cloud e quelle basate su edge.
2. Spiega tre tecniche chiave per ottimizzare i modelli per il deployment edge.
3. Quali sono i principali vantaggi dell'esecuzione di modelli AI sull'edge?
4. Descrivi il processo di quantizzazione di un modello e come influisce sulle prestazioni.
5. Spiega come i diversi acceleratori hardware (NPU, GPU, CPU) influenzano il deployment EdgeAI.

### Esercizi Pratici

1. **Configurazione Rapida dell'Ambiente**: Configura un ambiente di sviluppo minimo con i pacchetti essenziali (30 minuti)
2. **Esplorazione Modelli**: Scarica ed esamina un modello di linguaggio piccolo pre-addestrato (1 ora)
3. **Quantizzazione Base**: Prova una semplice quantizzazione su un modello piccolo (1 ora)

## Modulo 2: Fondamenti di Small Language Models

### Obiettivi di Apprendimento Chiave

- Comprendere i principi architetturali delle diverse famiglie di SLM
- Confrontare le capacità dei modelli su diverse scale di parametri
- Valutare i modelli in base a efficienza, capacità e requisiti di deployment
- Riconoscere i casi d'uso appropriati per le diverse famiglie di modelli

### Aree di Studio

#### Sezione 1: Famiglia di Modelli Microsoft Phi
- **Concetti Prioritari**: 
  - Evoluzione della filosofia di design
  - Architettura orientata all'efficienza
  - Capacità specializzate

#### Sezione 2: Famiglia Qwen
- **Concetti Prioritari**: 
  - Contributi open source
  - Opzioni di deployment scalabili
  - Architettura avanzata per il ragionamento

#### Sezione 3: Famiglia Gemma
- **Concetti Prioritari**: 
  - Innovazione guidata dalla ricerca
  - Capacità multimodali
  - Ottimizzazione per dispositivi mobili

#### Sezione 4: Famiglia BitNET
- **Concetti Prioritari**: 
  - Tecnologia di quantizzazione a 1 bit
  - Framework di ottimizzazione per l'inferenza
  - Considerazioni sulla sostenibilità

#### Sezione 5: Modello Microsoft Mu
- **Concetti Prioritari**: 
  - Architettura orientata ai dispositivi
  - Integrazione con Windows
  - Operazioni che preservano la privacy

#### Sezione 6: Phi-Silica
- **Concetti Prioritari**: 
  - Architettura ottimizzata per NPU
  - Metriche di performance
  - Integrazione per sviluppatori

### Domande di Autovalutazione

1. Confronta gli approcci architetturali delle famiglie di modelli Phi e Qwen.
2. Spiega come la tecnologia di quantizzazione di BitNET differisce dalla quantizzazione tradizionale.
3. Quali sono i vantaggi unici del modello Mu per l'integrazione con Windows?  
4. Descrivi come Phi-Silica sfrutta l'hardware NPU per ottimizzare le prestazioni.  
5. Per un'applicazione mobile con connettività limitata, quale famiglia di modelli sarebbe più appropriata e perché?  

### Esercizi pratici  

1. **Confronto tra modelli**: Benchmark rapido di due diversi modelli SLM (1 ora)  
2. **Generazione di testo semplice**: Implementazione di base della generazione di testo con un modello piccolo (1 ora)  
3. **Ottimizzazione rapida**: Applicare una tecnica di ottimizzazione per migliorare la velocità di inferenza (1 ora)  

## Modulo 3: Distribuzione di Small Language Models  

### Obiettivi principali di apprendimento  

- Selezionare modelli appropriati in base ai vincoli di distribuzione  
- Padroneggiare tecniche di ottimizzazione per diversi scenari di distribuzione  
- Implementare SLM sia in ambienti locali che cloud  
- Progettare configurazioni pronte per la produzione per applicazioni EdgeAI  

### Aree di studio  

#### Sezione 1: Apprendimento avanzato degli SLM  
- **Concetti prioritari**:  
  - Framework di classificazione dei parametri  
  - Tecniche avanzate di ottimizzazione  
  - Strategie di acquisizione dei modelli  

#### Sezione 2: Distribuzione in ambienti locali  
- **Concetti prioritari**:  
  - Distribuzione sulla piattaforma Ollama  
  - Soluzioni locali di Microsoft Foundry  
  - Analisi comparativa dei framework  

#### Sezione 3: Distribuzione cloud containerizzata  
- **Concetti prioritari**:  
  - Inferenza ad alte prestazioni con vLLM  
  - Orchestrazione dei container  
  - Implementazione di ONNX Runtime  

### Domande di autovalutazione  

1. Quali fattori devono essere considerati nella scelta tra distribuzione locale e cloud?  
2. Confronta Ollama e Microsoft Foundry Local come opzioni di distribuzione.  
3. Spiega i vantaggi della containerizzazione per la distribuzione degli SLM.  
4. Quali sono i principali metriche di prestazione da monitorare per un SLM distribuito su edge?  
5. Descrivi un workflow completo di distribuzione, dalla selezione del modello all'implementazione in produzione.  

### Esercizi pratici  

1. **Distribuzione locale di base**: Distribuire un semplice SLM utilizzando Ollama (1 ora)  
2. **Verifica delle prestazioni**: Eseguire un benchmark rapido sul modello distribuito (30 minuti)  
3. **Integrazione semplice**: Creare un'applicazione minima che utilizza il modello distribuito (1 ora)  

## Modulo 4: Conversione e quantizzazione del formato del modello  

### Obiettivi principali di apprendimento  

- Padroneggiare tecniche avanzate di quantizzazione da 1-bit a 8-bit di precisione  
- Comprendere le strategie di conversione del formato (GGUF, ONNX)  
- Implementare ottimizzazioni su sei framework (Llama.cpp, Olive, OpenVINO, MLX, sintesi del workflow)  
- Distribuire modelli ottimizzati per ambienti edge di produzione su hardware Intel, Apple e multipiattaforma  

### Aree di studio  

#### Sezione 1: Fondamenti di quantizzazione  
- **Concetti prioritari**:  
  - Framework di classificazione della precisione  
  - Compromessi tra prestazioni e accuratezza  
  - Ottimizzazione dell'impronta di memoria  

#### Sezione 2: Implementazione di Llama.cpp  
- **Concetti prioritari**:  
  - Distribuzione multipiattaforma  
  - Ottimizzazione del formato GGUF  
  - Tecniche di accelerazione hardware  

#### Sezione 3: Suite Microsoft Olive  
- **Concetti prioritari**:  
  - Ottimizzazione consapevole dell'hardware  
  - Distribuzione di livello enterprise  
  - Workflow di ottimizzazione automatizzati  

#### Sezione 4: Toolkit OpenVINO  
- **Concetti prioritari**:  
  - Ottimizzazione hardware Intel  
  - Framework di compressione delle reti neurali (NNCF)  
  - Distribuzione di inferenza multipiattaforma  
  - OpenVINO GenAI per la distribuzione di LLM  

#### Sezione 5: Framework Apple MLX  
- **Concetti prioritari**:  
  - Ottimizzazione per Apple Silicon  
  - Architettura di memoria unificata  
  - Capacità di fine-tuning LoRA  

#### Sezione 6: Sintesi del workflow di sviluppo Edge AI  
- **Concetti prioritari**:  
  - Architettura unificata del workflow  
  - Alberi decisionali per la selezione dei framework  
  - Validazione della prontezza per la produzione  
  - Strategie per garantire la compatibilità futura  

### Domande di autovalutazione  

1. Confronta le strategie di quantizzazione tra diversi livelli di precisione (1-bit a 8-bit).  
2. Spiega i vantaggi del formato GGUF per la distribuzione su edge.  
3. Come l'ottimizzazione consapevole dell'hardware in Microsoft Olive migliora l'efficienza della distribuzione?  
4. Quali sono i principali vantaggi del NNCF di OpenVINO per la compressione dei modelli?  
5. Descrivi come Apple MLX sfrutta l'architettura di memoria unificata per l'ottimizzazione.  
6. Come la sintesi del workflow aiuta nella selezione dei framework di ottimizzazione ottimali?  

### Esercizi pratici  

1. **Quantizzazione del modello**: Applicare diversi livelli di quantizzazione a un modello e confrontare i risultati (1 ora)  
2. **Ottimizzazione OpenVINO**: Utilizzare NNCF per comprimere un modello per hardware Intel (1 ora)  
3. **Confronto tra framework**: Testare lo stesso modello su tre diversi framework di ottimizzazione (1 ora)  
4. **Benchmark delle prestazioni**: Misurare l'impatto dell'ottimizzazione sulla velocità di inferenza e sull'uso della memoria (1 ora)  

## Modulo 5: SLMOps - Operazioni sui Small Language Models  

### Obiettivi principali di apprendimento  

- Comprendere i principi di gestione del ciclo di vita di SLMOps  
- Padroneggiare tecniche di distillazione e fine-tuning per la distribuzione su edge  
- Implementare strategie di distribuzione in produzione con monitoraggio  
- Costruire workflow di operazioni e manutenzione di SLM di livello enterprise  

### Aree di studio  

#### Sezione 1: Introduzione a SLMOps  
- **Concetti prioritari**:  
  - Cambiamento di paradigma di SLMOps nelle operazioni AI  
  - Architettura orientata alla privacy e all'efficienza dei costi  
  - Impatto strategico sul business e vantaggi competitivi  

#### Sezione 2: Distillazione del modello  
- **Concetti prioritari**:  
  - Tecniche di trasferimento della conoscenza  
  - Implementazione del processo di distillazione a due fasi  
  - Workflow di distillazione su Azure ML  

#### Sezione 3: Strategie di fine-tuning  
- **Concetti prioritari**:  
  - Fine-tuning efficiente dei parametri (PEFT)  
  - Metodi avanzati LoRA e QLoRA  
  - Addestramento multi-adattatore e ottimizzazione degli iperparametri  

#### Sezione 4: Distribuzione in produzione  
- **Concetti prioritari**:  
  - Conversione e quantizzazione del modello per la produzione  
  - Configurazione della distribuzione locale con Foundry  
  - Benchmark delle prestazioni e validazione della qualità  

### Domande di autovalutazione  

1. In cosa SLMOps differisce da MLOps tradizionali?  
2. Spiega i vantaggi della distillazione del modello per la distribuzione su edge.  
3. Quali sono le considerazioni chiave per il fine-tuning degli SLM in ambienti con risorse limitate?  
4. Descrivi una pipeline completa di distribuzione in produzione per applicazioni AI su edge.  

### Esercizi pratici  

1. **Distillazione di base**: Creare un modello più piccolo da un modello insegnante più grande (1 ora)  
2. **Esperimento di fine-tuning**: Effettuare il fine-tuning di un modello per un dominio specifico (1 ora)  
3. **Pipeline di distribuzione**: Configurare una pipeline CI/CD di base per la distribuzione del modello (1 ora)  

## Modulo 6: Sistemi agentici SLM - Agenti AI e chiamata di funzioni  

### Obiettivi principali di apprendimento  

- Costruire agenti AI intelligenti per ambienti edge utilizzando Small Language Models  
- Implementare capacità di chiamata di funzioni con workflow sistematici  
- Padroneggiare l'integrazione del Model Context Protocol (MCP) per interazioni standardizzate con strumenti  
- Creare sistemi agentici sofisticati con intervento umano minimo  

### Aree di studio  

#### Sezione 1: Agenti AI e fondamenti SLM  
- **Concetti prioritari**:  
  - Framework di classificazione degli agenti (riflessivi, basati su modelli, basati su obiettivi, agenti di apprendimento)  
  - Analisi dei compromessi tra SLM e LLM  
  - Pattern di progettazione specifici per agenti su edge  
  - Ottimizzazione delle risorse per gli agenti  

#### Sezione 2: Chiamata di funzioni nei Small Language Models  
- **Concetti prioritari**:  
  - Implementazione sistematica del workflow (rilevamento dell'intento, output JSON, esecuzione esterna)  
  - Implementazioni specifiche per piattaforma (Phi-4-mini, modelli Qwen selezionati, Microsoft Foundry Local)  
  - Esempi avanzati (collaborazione multi-agente, selezione dinamica degli strumenti)  
  - Considerazioni per la produzione (limitazione del tasso, registrazione degli audit, misure di sicurezza)  

#### Sezione 3: Integrazione del Model Context Protocol (MCP)  
- **Concetti prioritari**:  
  - Architettura del protocollo e design del sistema stratificato  
  - Supporto multi-backend (Ollama per sviluppo, vLLM per produzione)  
  - Protocolli di connessione (modalità STDIO e SSE)  
  - Applicazioni reali (automazione web, elaborazione dati, integrazione API)  

### Domande di autovalutazione  

1. Quali sono le considerazioni architettoniche chiave per gli agenti AI su edge?  
2. Come la chiamata di funzioni migliora le capacità degli agenti?  
3. Spiega il ruolo del Model Context Protocol nella comunicazione degli agenti.  

### Esercizi pratici  

1. **Agente semplice**: Costruire un agente AI di base con chiamata di funzioni (1 ora)  
2. **Integrazione MCP**: Implementare MCP in un'applicazione per agenti (30 minuti)  

## Modulo 7: Esempi di implementazione EdgeAI  

### Obiettivi principali di apprendimento  

- Padroneggiare AI Toolkit per Visual Studio Code per workflow di sviluppo EdgeAI completi  
- Acquisire competenze nella piattaforma Windows AI Foundry e nelle strategie di ottimizzazione NPU  
- Implementare EdgeAI su più piattaforme hardware e scenari di distribuzione  
- Costruire applicazioni EdgeAI pronte per la produzione con ottimizzazioni specifiche per piattaforma  

### Aree di studio  

#### Sezione 1: AI Toolkit per Visual Studio Code  
- **Concetti prioritari**:  
  - Ambiente di sviluppo Edge AI completo all'interno di VS Code  
  - Catalogo e scoperta dei modelli per la distribuzione su edge  
  - Workflow di test locale, ottimizzazione e sviluppo di agenti  
  - Monitoraggio delle prestazioni e valutazione per scenari edge  

#### Sezione 2: Guida allo sviluppo Windows EdgeAI  
- **Concetti prioritari**:  
  - Panoramica completa della piattaforma Windows AI Foundry  
  - API Phi Silica per inferenza NPU efficiente  
  - API di Computer Vision per elaborazione immagini e OCR  
  - CLI Foundry Local per sviluppo e test locali  

#### Sezione 3: Implementazioni specifiche per piattaforma  
- **Concetti prioritari**:  
  - Distribuzione su NVIDIA Jetson Orin Nano (prestazioni AI 67 TOPS)  
  - Applicazioni mobili con .NET MAUI e ONNX Runtime GenAI  
  - Soluzioni Azure EdgeAI con architettura ibrida cloud-edge  
  - Ottimizzazione Windows ML con supporto hardware universale  
  - Applicazioni Foundry Local con implementazione RAG orientata alla privacy  

### Domande di autovalutazione  

1. Come AI Toolkit semplifica il workflow di sviluppo EdgeAI?  
2. Confronta le strategie di distribuzione su diverse piattaforme hardware.  
3. Quali sono i vantaggi di Windows AI Foundry per lo sviluppo su edge?  
4. Spiega il ruolo dell'ottimizzazione NPU nelle moderne applicazioni Edge AI.  
5. Come l'API Phi Silica sfrutta l'hardware NPU per ottimizzare le prestazioni?  
6. Confronta i benefici della distribuzione locale rispetto a quella cloud per applicazioni sensibili alla privacy.  

### Esercizi pratici  

1. **Configurazione AI Toolkit**: Configurare AI Toolkit e ottimizzare un modello (1 ora)  
2. **Windows AI Foundry**: Costruire una semplice applicazione AI per Windows utilizzando l'API Phi Silica (1 ora)  
3. **Distribuzione multipiattaforma**: Distribuire lo stesso modello su due piattaforme diverse (1 ora)  
4. **Ottimizzazione NPU**: Testare le prestazioni NPU con gli strumenti di Windows AI Foundry (30 minuti)  

## Modulo 8: Microsoft Foundry Local – Toolkit completo per sviluppatori (modernizzato)  

### Obiettivi principali di apprendimento  

- Installare e configurare Foundry Local con integrazione SDK moderna  
- Implementare sistemi multi-agente avanzati con pattern di coordinamento  
- Costruire router di modelli intelligenti con selezione automatica basata su attività  
- Distribuire soluzioni AI pronte per la produzione con monitoraggio completo  
- Integrare con Azure AI Foundry per scenari di distribuzione ibrida  
- Padroneggiare pattern SDK moderni con FoundryLocalManager e client OpenAI  

### Aree di studio  

#### Sezione 1: Installazione e configurazione moderna  
- **Concetti prioritari**:  
  - Integrazione SDK FoundryLocalManager  
  - Scoperta automatica dei servizi e monitoraggio dello stato  
  - Pattern di configurazione basati sull'ambiente  
  - Considerazioni per la distribuzione in produzione  

#### Sezione 2: Sistemi multi-agente avanzati  
- **Concetti prioritari**:  
  - Pattern di coordinamento con agenti specialisti  
  - Specializzazione degli agenti per recupero, ragionamento ed esecuzione  
  - Meccanismi di feedback per il perfezionamento  
  - Monitoraggio delle prestazioni e tracciamento delle statistiche  

#### Sezione 3: Routing intelligente dei modelli  
- **Concetti prioritari**:  
  - Algoritmi di selezione dei modelli basati su parole chiave  
  - Supporto per modelli multipli (generale, ragionamento, codice, creativo)  
  - Configurazione delle variabili d'ambiente per flessibilità  
  - Controllo dello stato del servizio e gestione degli errori  

#### Sezione 4: Implementazione pronta per la produzione  
- **Concetti prioritari**:  
  - Gestione completa degli errori e meccanismi di fallback  
  - Monitoraggio delle richieste e tracciamento delle prestazioni  
  - Esempi interattivi con notebook Jupyter e benchmark  
  - Pattern di integrazione con applicazioni esistenti  

### Domande di autovalutazione  

1. In cosa l'approccio moderno di FoundryLocalManager differisce dalle chiamate REST manuali?  
2. Spiega il pattern di coordinamento e come orchestra gli agenti specialisti.  
3. Come il router intelligente seleziona i modelli appropriati in base al contenuto della query?  
4. Quali sono i componenti chiave di un sistema di agenti AI pronto per la produzione?  
5. Come implementare un monitoraggio completo dello stato per i servizi Foundry Local?  
6. Confronta i benefici dell'approccio modernizzato rispetto ai pattern di implementazione tradizionali.  

### Esercizi pratici  

1. **Configurazione SDK moderna**: Configurare FoundryLocalManager con scoperta automatica dei servizi (30 minuti)  
2. **Sistema multi-agente**: Eseguire il coordinatore avanzato con agenti specialisti (30 minuti)  
3. **Routing intelligente**: Testare il router di modelli con diversi tipi di query (30 minuti)  
4. **Esplorazione interattiva**: Utilizzare i notebook Jupyter per esplorare funzionalità avanzate (45 minuti)  
5. **Distribuzione in produzione**: Implementare pattern di monitoraggio e gestione degli errori (30 minuti)  
6. **Integrazione ibrida**: Configurare scenari di fallback con Azure AI Foundry (30 minuti)  

## Guida alla distribuzione del tempo  

Per aiutarti a sfruttare al meglio le 20 ore del corso, ecco una suddivisione suggerita su come allocare il tuo tempo:  

| Attività | Allocazione del tempo | Descrizione |  
|----------|-----------------------|-------------|  
| Lettura dei materiali principali | 9 ore | Concentrarsi sui concetti essenziali di ogni modulo |  
| Esercizi Pratici | 6 ore | Implementazione pratica delle tecniche principali |
| Autovalutazione | 2 ore | Verifica della comprensione tramite domande e riflessioni |
| Mini-Progetto | 3 ore | Applicazione delle conoscenze in una piccola implementazione pratica |

### Aree Chiave di Focus in Base al Tempo Disponibile

**Se hai solo 10 ore:**
- Completa il Modulo 0 (Introduzione) e i Moduli 1, 2 e 3 (concetti principali di EdgeAI)
- Fai almeno un esercizio pratico per modulo
- Concentrati sulla comprensione dei concetti principali piuttosto che sui dettagli di implementazione

**Se puoi dedicare l'intero corso di 20 ore:**
- Completa tutti gli otto moduli (inclusa l'Introduzione)
- Esegui gli esercizi pratici principali di ogni modulo
- Completa un mini-progetto dal Modulo 7
- Esplora almeno 2-3 risorse supplementari

**Se hai più di 20 ore:**
- Completa tutti i moduli (inclusa l'Introduzione) con esercizi dettagliati
- Realizza più mini-progetti
- Esplora tecniche avanzate di ottimizzazione nel Modulo 4
- Implementa il deployment in produzione dal Modulo 5

## Risorse Essenziali

Queste risorse selezionate con cura offrono il massimo valore per il tuo tempo di studio limitato:

### Documentazione da Leggere Assolutamente
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Lo strumento più efficiente per l'ottimizzazione dei modelli
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Il modo più veloce per distribuire SLM localmente
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Riferimento per un modello leader ottimizzato per l'edge
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Toolkit di ottimizzazione completo di Intel
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Ambiente di sviluppo integrato per EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Piattaforma di sviluppo EdgeAI specifica per Windows

### Strumenti per Risparmiare Tempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accesso rapido ai modelli e distribuzione
- [Gradio](https://www.gradio.app/docs/interface) - Sviluppo rapido di interfacce utente per demo AI
- [Microsoft Olive](https://github.com/microsoft/Olive) - Ottimizzazione dei modelli semplificata
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferenza efficiente su CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework per la compressione delle reti neurali
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit per la distribuzione di modelli di linguaggio di grandi dimensioni

## Modello per il Monitoraggio dei Progressi

Utilizza questo modello semplificato per monitorare i tuoi progressi di apprendimento durante il corso di 20 ore:

| Modulo | Data di Completamento | Ore Trascorse | Punti Chiave |
|--------|-----------------------|---------------|--------------|
| Modulo 0: Introduzione a EdgeAI | | | |
| Modulo 1: Fondamenti di EdgeAI | | | |
| Modulo 2: Fondamenti di SLM | | | |
| Modulo 3: Distribuzione di SLM | | | |
| Modulo 4: Ottimizzazione dei Modelli | | | |
| Modulo 5: SLMOps | | | |
| Modulo 6: Agenti AI | | | |
| Modulo 7: Strumenti di Sviluppo | | | |
| Modulo 8: Foundry Local Toolkit | | | |
| Esercizi Pratici | | | |
| Mini-Progetto | | | |

## Idee per Mini-Progetti

Considera di completare uno di questi progetti per mettere in pratica i concetti di EdgeAI (ognuno progettato per richiedere 2-4 ore):

### Progetti per Principianti (2-3 ore ciascuno)
1. **Assistente Testuale Edge**: Crea uno strumento semplice di completamento testuale offline utilizzando un piccolo modello linguistico
2. **Dashboard di Confronto Modelli**: Costruisci una visualizzazione base delle metriche di performance tra diversi SLM
3. **Esperimento di Ottimizzazione**: Misura l'impatto di diversi livelli di quantizzazione sullo stesso modello di base

### Progetti Intermedi (3-4 ore ciascuno)
4. **Workflow con AI Toolkit**: Usa il toolkit AI di VS Code per ottimizzare e distribuire un modello dall'inizio alla fine
5. **Applicazione Windows AI Foundry**: Crea un'app Windows utilizzando l'API Phi Silica e l'ottimizzazione NPU
6. **Distribuzione Cross-Platform**: Distribuisci lo stesso modello ottimizzato su Windows (OpenVINO) e mobile (.NET MAUI)
7. **Agente con Funzioni di Chiamata**: Costruisci un agente AI con capacità di chiamata di funzioni per scenari edge

### Progetti di Integrazione Avanzata (4-5 ore ciascuno)
8. **Pipeline di Ottimizzazione OpenVINO**: Implementa un'ottimizzazione completa del modello utilizzando NNCF e il toolkit GenAI
9. **Pipeline SLMOps**: Implementa un ciclo di vita completo del modello, dalla formazione alla distribuzione edge
10. **Sistema Edge Multi-Modello**: Distribuisci più modelli specializzati che lavorano insieme su hardware edge
11. **Sistema di Integrazione MCP**: Costruisci un sistema agentico utilizzando il Model Context Protocol per l'interazione con strumenti

## Riferimenti

- Microsoft Learn (Foundry Local)
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Inizia: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Riferimento CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integrazione con SDK di inferenza: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Come aprire WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilare modelli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenti (panoramica): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Strumenti di Ottimizzazione e Inferenza
  - Microsoft Olive (documentazione): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (iniziare): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integrazione Olive con ONNX Runtime: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (documentazione): https://docs.openvino.ai/2025/index.html
  - Apple MLX (documentazione): https://ml-explore.github.io/mlx/build/html/index.html
- Framework di Distribuzione e Modelli
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (documentazione): https://docs.vllm.ai/
  - Ollama (iniziare): https://github.com/ollama/ollama#get-started
- Strumenti per Sviluppatori (Windows e VS Code)
  - AI Toolkit per VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (panoramica): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Comunità di Apprendimento

Unisciti alla discussione e connettiti con altri studenti:
- Discussioni su GitHub nel [repository EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusione

EdgeAI rappresenta la frontiera dell'implementazione dell'intelligenza artificiale, portando potenti capacità direttamente sui dispositivi e affrontando questioni critiche come privacy, latenza e connettività. Questo corso di 20 ore ti fornisce le conoscenze essenziali e le competenze pratiche per iniziare subito a lavorare con le tecnologie EdgeAI.

Il corso è volutamente conciso e focalizzato sui concetti più importanti, permettendoti di acquisire rapidamente competenze preziose senza un impegno di tempo eccessivo. Ricorda che la pratica pratica, anche con esempi semplici, è la chiave per rafforzare ciò che hai appreso.

Buono studio!

---

