<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef04a48f3f1428fa008738033017e0e8",
  "translation_date": "2025-09-24T21:21:27+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "it"
}
-->
# EdgeAI per Principianti: Percorsi di Apprendimento e Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 1 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 2 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 3 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 4-5 | Modulo 4: Ottimizzazione del Modello (6 framework) | 4 ore |
| Giorno 6 | Modulo 5: SLMOps | 3 ore |
| Giorno 7 | Modulo 6-7: Agenti AI & Strumenti di Sviluppo | 4 ore |
| Giorno 8 | Modulo 8: Toolkit Locale Foundry (Implementazione Moderna) | 1 ora |

### Percorso di Apprendimento Concentrato (2 settimane)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione del Modello | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Agenti AI | 2 ore |
| Giorno 13-14 | Modulo 7: Strumenti di Sviluppo | 3 ore |

### Studio Part-Time (4 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Modulo 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Modulo 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 3 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 3 ore |
| Giorno 5-6 | Modulo 3: Deployment di SLM | 2 ore |
| Giorno 7-8 | Modulo 4: Ottimizzazione del Modello | 4 ore |
| Giorno 9-10 | Modulo 5: SLMOps | 3 ore |
| Giorno 11-12 | Modulo 6: Sistemi Agenti SLM | 2 ore |
| Giorno 13-14 | Modulo 7: Esempi di Implementazione EdgeAI | 2 ore |

| Modulo | Data di Completamento | Ore Spese | Principali Conoscenze Acquisite |
|--------|-----------------------|-----------|--------------------------------|
| Modulo 1: Fondamenti di EdgeAI | | | |
| Modulo 2: Fondamenti di SLM | | | |
| Modulo 3: Deployment di SLM | | | |
| Modulo 4: Ottimizzazione del Modello (6 framework) | | | |
| Modulo 5: SLMOps | | | |
| Modulo 6: Sistemi Agenti SLM | | | |
| Modulo 7: Esempi di Implementazione EdgeAI | | | |
| Esercizi Pratici | | | |
| Mini-Progetto | | | |

### Studio Part-Time (4 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 1-2: Fondamenti & Fondamenti di SLM | 6 ore |
| Settimana 2 | Modulo 3-4: Deployment & Ottimizzazione | 6 ore |
| Settimana 3 | Modulo 5-6: SLMOps & Agenti AI | 5 ore |
| Settimana 4 | Modulo 7: Strumenti di Sviluppo & Integrazione | 3 ore |

## Introduzione

Benvenuto nella guida di studio "EdgeAI per Principianti"! Questo documento è progettato per aiutarti a navigare efficacemente nei materiali del corso e massimizzare la tua esperienza di apprendimento. Fornisce percorsi di apprendimento strutturati, programmi di studio suggeriti, riassunti dei concetti chiave e risorse supplementari per approfondire la tua comprensione delle tecnologie EdgeAI.

Si tratta di un corso conciso di 20 ore che offre conoscenze essenziali su EdgeAI in un formato efficiente, ideale per professionisti e studenti impegnati che desiderano acquisire rapidamente competenze pratiche in questo campo emergente.

## Panoramica del Corso

Il corso è organizzato in sette moduli completi:

1. **Fondamenti e Trasformazione di EdgeAI** - Comprendere i concetti fondamentali e il cambiamento tecnologico
2. **Fondamenti di Small Language Model** - Esplorare le diverse famiglie di SLM e le loro architetture
3. **Deployment di Small Language Model** - Implementare strategie pratiche di deployment
4. **Conversione del Formato del Modello e Quantizzazione** - Ottimizzazione avanzata con 6 framework, inclusi OpenVINO
5. **SLMOps - Operazioni sui Small Language Model** - Gestione del ciclo di vita e deployment in produzione
6. **Sistemi Agenti SLM** - Agenti AI, chiamata di funzioni e Protocollo di Contesto del Modello
7. **Esempi di Implementazione EdgeAI** - Toolkit AI, sviluppo su Windows e implementazioni specifiche per piattaforma
8. **Microsoft Foundry Local – Toolkit Completo per Sviluppatori** - Sviluppo locale con integrazione ibrida Azure (Modulo 08)

## Come Utilizzare Questa Guida di Studio

- **Apprendimento Progressivo**: Segui i moduli in ordine per un'esperienza di apprendimento più coerente
- **Checkpoint di Conoscenza**: Usa le domande di autovalutazione dopo ogni sezione
- **Pratica Pratica**: Completa gli esercizi suggeriti per rafforzare i concetti teorici
- **Risorse Supplementari**: Esplora materiali aggiuntivi per gli argomenti che ti interessano di più

## Raccomandazioni per il Programma di Studio

### Percorso di Apprendimento Concentrato (1 settimana)

| Giorno | Focus | Ore Stimate |
|--------|-------|-------------|
| Giorno 1-2 | Modulo 1: Fondamenti di EdgeAI | 6 ore |
| Giorno 3-4 | Modulo 2: Fondamenti di SLM | 8 ore |
| Giorno 5 | Modulo 3: Deployment di SLM | 3 ore |
| Giorno 6 | Modulo 8: Toolkit Locale Foundry | 3 ore |

### Studio Part-Time (3 settimane)

| Settimana | Focus | Ore Stimate |
|-----------|-------|-------------|
| Settimana 1 | Modulo 1: Fondamenti di EdgeAI | 6-7 ore |
| Settimana 2 | Modulo 2: Fondamenti di SLM | 7-8 ore |
| Settimana 3 | Modulo 3: Deployment di SLM (3h) + Modulo 8: Toolkit Locale Foundry (2-3h) | 5-6 ore |

## Modulo 1: Fondamenti e Trasformazione di EdgeAI

### Obiettivi Principali di Apprendimento

- Comprendere le differenze tra AI basata su cloud e AI basata su edge
- Padroneggiare le tecniche di ottimizzazione fondamentali per ambienti con risorse limitate
- Analizzare applicazioni reali delle tecnologie EdgeAI
- Configurare un ambiente di sviluppo per progetti EdgeAI

### Aree di Studio

#### Sezione 1: Fondamenti di EdgeAI
- **Concetti Prioritari**: 
  - Paradigmi di calcolo Edge vs. Cloud
  - Tecniche di quantizzazione del modello
  - Opzioni di accelerazione hardware (NPU, GPU, CPU)
  - Vantaggi in termini di privacy e sicurezza

- **Materiali Supplementari**:
  - [Documentazione TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentazione Edge Impulse](https://docs.edgeimpulse.com)

#### Sezione 2: Studi di Caso Reali
- **Concetti Prioritari**: 
  - Ecosistema di modelli Microsoft Phi & Mu
  - Implementazioni pratiche in diversi settori
  - Considerazioni sul deployment

#### Sezione 3: Guida Pratica all'Implementazione
- **Concetti Prioritari**: 
  - Configurazione dell'ambiente di sviluppo
  - Strumenti di quantizzazione e ottimizzazione
  - Metodi di valutazione per implementazioni EdgeAI

#### Sezione 4: Hardware per il Deployment Edge
- **Concetti Prioritari**: 
  - Confronti tra piattaforme hardware
  - Strategie di ottimizzazione per hardware specifico
  - Considerazioni sul deployment

### Domande di Autovalutazione

1. Confronta e contrasta l'AI basata su cloud con le implementazioni AI basate su edge.
2. Spiega tre tecniche chiave per ottimizzare i modelli per il deployment edge.
3. Quali sono i principali vantaggi di eseguire modelli AI sull'edge?
4. Descrivi il processo di quantizzazione di un modello e come influisce sulle prestazioni.
5. Spiega come diversi acceleratori hardware (NPU, GPU, CPU) influenzano il deployment EdgeAI.

### Esercizi Pratici

1. **Configurazione Rapida dell'Ambiente**: Configura un ambiente di sviluppo minimo con i pacchetti essenziali (30 minuti)
2. **Esplorazione del Modello**: Scarica ed esamina un modello di linguaggio piccolo pre-addestrato (1 ora)
3. **Quantizzazione Base**: Prova una semplice quantizzazione su un modello piccolo (1 ora)

## Modulo 2: Fondamenti di Small Language Model

### Obiettivi Principali di Apprendimento

- Comprendere i principi architetturali delle diverse famiglie di SLM
- Confrontare le capacità dei modelli su diverse scale di parametri
- Valutare i modelli in base a efficienza, capacità e requisiti di deployment
- Riconoscere i casi d'uso appropriati per le diverse famiglie di modelli

### Aree di Studio

#### Sezione 1: Famiglia di Modelli Microsoft Phi
- **Concetti Prioritari**: 
  - Evoluzione della filosofia di design
  - Architettura orientata all'efficienza
  - Capacità specializzate

#### Sezione 2: Famiglia Qwen
- **Concetti Prioritari**: 
  - Contributi open source
  - Opzioni di deployment scalabili
  - Architettura avanzata per il ragionamento

#### Sezione 3: Famiglia Gemma
- **Concetti Prioritari**: 
  - Innovazione guidata dalla ricerca
  - Capacità multimodali
  - Ottimizzazione per dispositivi mobili

#### Sezione 4: Famiglia BitNET
- **Concetti Prioritari**: 
  - Tecnologia di quantizzazione a 1 bit
  - Framework di ottimizzazione per l'inferenza
  - Considerazioni sulla sostenibilità

#### Sezione 5: Modello Microsoft Mu
- **Concetti Prioritari**: 
  - Architettura orientata ai dispositivi
  - Integrazione di sistema con Windows
  - Operazione che preserva la privacy

#### Sezione 6: Phi-Silica
- **Concetti Prioritari**: 
  - Architettura ottimizzata per NPU
  - Metriche di prestazione
  - Integrazione per sviluppatori

### Domande di Autovalutazione

1. Confronta gli approcci architetturali delle famiglie di modelli Phi e Qwen.
2. Spiega come la tecnologia di quantizzazione di BitNET differisce dalla quantizzazione tradizionale.
3. Quali sono i vantaggi unici del modello Mu per l'integrazione con Windows?
4. Descrivi come Phi-Silica sfrutta l'hardware NPU per ottimizzare le prestazioni.
5. Per un'applicazione mobile con connettività limitata, quale famiglia di modelli sarebbe più appropriata e perché?

### Esercizi Pratici

1. **Confronto tra Modelli**: Benchmark rapido di due diversi modelli SLM (1 ora)
2. **Generazione di Testo Semplice**: Implementazione base di generazione di testo con un modello piccolo (1 ora)
3. **Ottimizzazione Rapida**: Applica una tecnica di ottimizzazione per migliorare la velocità di inferenza (1 ora)

## Modulo 3: Deployment di Small Language Model

### Obiettivi Principali di Apprendimento

- Selezionare modelli appropriati in base ai vincoli di deployment
- Padroneggiare tecniche di ottimizzazione per vari scenari di deployment
- Implementare SLM in ambienti locali e cloud
- Progettare configurazioni pronte per la produzione per applicazioni EdgeAI

### Aree di Studio

#### Sezione 1: Apprendimento Avanzato SLM
- **Concetti Prioritari**: 
  - Framework di classificazione dei parametri
  - Tecniche di ottimizzazione avanzate
  - Strategie di acquisizione dei modelli

#### Sezione 2: Deployment in Ambiente Locale
- **Concetti Prioritari**: 
  - Deployment sulla piattaforma Ollama
  - Soluzioni locali Microsoft Foundry
  - Analisi comparativa dei framework

#### Sezione 3: Deployment Cloud Containerizzato
- **Concetti Prioritari**: 
  - Inferenza ad alte prestazioni con vLLM
  - Orchestrazione dei container
  - Implementazione ONNX Runtime

### Domande di Autovalutazione

1. Quali fattori devono essere considerati nella scelta tra deployment locale e deployment cloud?
2. Confronta Ollama e Microsoft Foundry Local come opzioni di deployment.
3. Spiega i vantaggi della containerizzazione per il deployment di SLM.
4. Quali sono le metriche di prestazione chiave da monitorare per un SLM distribuito sull'edge?
5. Descrivi un workflow completo di deployment, dalla selezione del modello all'implementazione in produzione.

### Esercizi Pratici

1. **Deployment Locale Base**: Distribuisci un semplice SLM utilizzando Ollama (1 ora)
2. **Verifica delle Prestazioni**: Esegui un benchmark rapido sul modello distribuito (30 minuti)
3. **Integrazione Semplice**: Crea un'applicazione minima che utilizza il modello distribuito (1 ora)

## Modulo 4: Conversione del Formato del Modello e Quantizzazione

### Obiettivi Principali di Apprendimento

- Padroneggiare tecniche avanzate di quantizzazione da 1-bit a 8-bit di precisione
- Comprendere le strategie di conversione del formato (GGUF, ONNX)
- Implementare ottimizzazioni su sei framework (Llama.cpp, Olive, OpenVINO, MLX, sintesi del workflow)
- Distribuire modelli ottimizzati per ambienti edge di produzione su hardware Intel, Apple e multipiattaforma

### Aree di Studio

#### Sezione 1: Fondamenti di Quantizzazione
- **Concetti Prioritari**: 
  - Framework di classificazione della precisione
  - Compromessi tra prestazioni e accuratezza
  - Ottimizzazione della memoria

#### Sezione 2: Implementazione Llama.cpp
- **Concetti Prioritari**: 
  - Deployment multipiattaforma
  - Ottimizzazione del formato GGUF
  - Tecniche di accelerazione hardware

#### Sezione 3: Suite Microsoft Olive
- **Concetti Prioritari**: 
  - Ottimizzazione consapevole dell'hardware
  - Deployment di livello enterprise
  - Workflow di ottimizzazione automatizzati

#### Sezione 4: Toolkit OpenVINO
- **Concetti Prioritari**: 
  - Ottimizzazione hardware Intel
  - Neural Network Compression Framework (NNCF)
  - Deployment multipiattaforma per l'inferenza
- OpenVINO GenAI per il deployment di LLM

#### Sezione 5: Framework Apple MLX
- **Concetti prioritari**: 
  - Ottimizzazione per Apple Silicon
  - Architettura di memoria unificata
  - Capacità di fine-tuning con LoRA

#### Sezione 6: Sintesi del workflow di sviluppo Edge AI
- **Concetti prioritari**: 
  - Architettura unificata del workflow
  - Alberi decisionali per la selezione dei framework
  - Validazione della prontezza per la produzione
  - Strategie per garantire la longevità delle soluzioni

### Domande di autovalutazione

1. Confronta le strategie di quantizzazione tra diversi livelli di precisione (da 1-bit a 8-bit).
2. Spiega i vantaggi del formato GGUF per il deployment edge.
3. In che modo l'ottimizzazione hardware-aware in Microsoft Olive migliora l'efficienza del deployment?
4. Quali sono i principali benefici di NNCF di OpenVINO per la compressione dei modelli?
5. Descrivi come Apple MLX sfrutta l'architettura di memoria unificata per l'ottimizzazione.
6. In che modo la sintesi del workflow aiuta nella selezione dei framework di ottimizzazione ottimali?

### Esercizi pratici

1. **Quantizzazione del modello**: Applica diversi livelli di quantizzazione a un modello e confronta i risultati (1 ora)
2. **Ottimizzazione OpenVINO**: Usa NNCF per comprimere un modello per hardware Intel (1 ora)
3. **Confronto tra framework**: Testa lo stesso modello su tre diversi framework di ottimizzazione (1 ora)
4. **Benchmark delle prestazioni**: Misura l'impatto dell'ottimizzazione sulla velocità di inferenza e sull'uso della memoria (1 ora)

## Modulo 5: SLMOps - Operazioni per Small Language Models

### Obiettivi principali di apprendimento

- Comprendere i principi di gestione del ciclo di vita di SLMOps
- Padroneggiare tecniche di distillazione e fine-tuning per il deployment edge
- Implementare strategie di deployment in produzione con monitoraggio
- Costruire workflow di operazioni e manutenzione SLM di livello enterprise

### Aree di studio principali

#### Sezione 1: Introduzione a SLMOps
- **Concetti prioritari**: 
  - Cambiamento di paradigma SLMOps nelle operazioni AI
  - Architettura economica e orientata alla privacy
  - Impatto strategico sul business e vantaggi competitivi

#### Sezione 2: Distillazione del modello
- **Concetti prioritari**: 
  - Tecniche di trasferimento della conoscenza
  - Implementazione del processo di distillazione in due fasi
  - Workflow di distillazione con Azure ML

#### Sezione 3: Strategie di fine-tuning
- **Concetti prioritari**: 
  - Fine-tuning efficiente dei parametri (PEFT)
  - Metodi avanzati LoRA e QLoRA
  - Addestramento multi-adapter e ottimizzazione degli iperparametri

#### Sezione 4: Deployment in produzione
- **Concetti prioritari**: 
  - Conversione e quantizzazione del modello per la produzione
  - Configurazione del deployment locale con Foundry Local
  - Benchmark delle prestazioni e validazione della qualità

### Domande di autovalutazione

1. In che modo SLMOps differisce da MLOps tradizionali?
2. Spiega i benefici della distillazione del modello per il deployment edge.
3. Quali sono le considerazioni chiave per il fine-tuning di SLM in ambienti con risorse limitate?
4. Descrivi un pipeline completo di deployment in produzione per applicazioni AI edge.

### Esercizi pratici

1. **Distillazione di base**: Crea un modello più piccolo da un modello insegnante più grande (1 ora)
2. **Esperimento di fine-tuning**: Effettua il fine-tuning di un modello per un dominio specifico (1 ora)
3. **Pipeline di deployment**: Configura un pipeline CI/CD di base per il deployment del modello (1 ora)

## Modulo 6: Sistemi agentici SLM - Agenti AI e chiamata di funzioni

### Obiettivi principali di apprendimento

- Costruire agenti AI intelligenti per ambienti edge utilizzando Small Language Models
- Implementare capacità di chiamata di funzioni con workflow sistematici
- Padroneggiare l'integrazione del Model Context Protocol (MCP) per interazioni standardizzate con strumenti
- Creare sistemi agentici sofisticati con intervento umano minimo

### Aree di studio principali

#### Sezione 1: Agenti AI e fondamenti SLM
- **Concetti prioritari**: 
  - Framework di classificazione degli agenti (riflessivi, basati su modelli, basati su obiettivi, agenti di apprendimento)
  - Analisi dei compromessi tra SLM e LLM
  - Pattern di design specifici per agenti edge
  - Ottimizzazione delle risorse per gli agenti

#### Sezione 2: Chiamata di funzioni nei Small Language Models
- **Concetti prioritari**: 
  - Implementazione sistematica del workflow (rilevamento dell'intento, output JSON, esecuzione esterna)
  - Implementazioni specifiche per piattaforma (Phi-4-mini, modelli Qwen selezionati, Microsoft Foundry Local)
  - Esempi avanzati (collaborazione multi-agente, selezione dinamica degli strumenti)
  - Considerazioni per la produzione (limitazione del tasso, registrazione degli audit, misure di sicurezza)

#### Sezione 3: Integrazione del Model Context Protocol (MCP)
- **Concetti prioritari**: 
  - Architettura del protocollo e design del sistema a strati
  - Supporto multi-backend (Ollama per lo sviluppo, vLLM per la produzione)
  - Protocolli di connessione (modalità STDIO e SSE)
  - Applicazioni reali (automazione web, elaborazione dati, integrazione API)

### Domande di autovalutazione

1. Quali sono le considerazioni architetturali chiave per gli agenti AI edge?
2. In che modo la chiamata di funzioni migliora le capacità degli agenti?
3. Spiega il ruolo del Model Context Protocol nella comunicazione tra agenti.

### Esercizi pratici

1. **Agente semplice**: Costruisci un agente AI di base con chiamata di funzioni (1 ora)
2. **Integrazione MCP**: Implementa MCP in un'applicazione per agenti (30 minuti)

## Modulo 7: Esempi di implementazione EdgeAI

### Obiettivi principali di apprendimento

- Padroneggiare AI Toolkit per Visual Studio Code per workflow di sviluppo EdgeAI completi
- Acquisire competenze nella piattaforma Windows AI Foundry e nelle strategie di ottimizzazione NPU
- Implementare EdgeAI su più piattaforme hardware e scenari di deployment
- Costruire applicazioni EdgeAI pronte per la produzione con ottimizzazioni specifiche per piattaforma

### Aree di studio principali

#### Sezione 1: AI Toolkit per Visual Studio Code
- **Concetti prioritari**: 
  - Ambiente di sviluppo Edge AI completo all'interno di VS Code
  - Catalogo e scoperta dei modelli per il deployment edge
  - Workflow di test locale, ottimizzazione e sviluppo di agenti
  - Monitoraggio delle prestazioni e valutazione per scenari edge

#### Sezione 2: Guida allo sviluppo EdgeAI su Windows
- **Concetti prioritari**: 
  - Panoramica completa della piattaforma Windows AI Foundry
  - API Phi Silica per inferenza NPU efficiente
  - API di Computer Vision per elaborazione immagini e OCR
  - CLI Foundry Local per sviluppo e test locali

#### Sezione 3: Implementazioni specifiche per piattaforma
- **Concetti prioritari**: 
  - Deployment su NVIDIA Jetson Orin Nano (prestazioni AI da 67 TOPS)
  - Applicazioni mobili con .NET MAUI e ONNX Runtime GenAI
  - Soluzioni Azure EdgeAI con architettura ibrida cloud-edge
  - Ottimizzazione Windows ML con supporto hardware universale
  - Applicazioni Foundry Local con implementazione RAG orientata alla privacy

### Domande di autovalutazione

1. In che modo AI Toolkit semplifica il workflow di sviluppo EdgeAI?
2. Confronta le strategie di deployment su diverse piattaforme hardware.
3. Quali sono i vantaggi di Windows AI Foundry per lo sviluppo edge?
4. Spiega il ruolo dell'ottimizzazione NPU nelle moderne applicazioni AI edge.
5. In che modo l'API Phi Silica sfrutta l'hardware NPU per l'ottimizzazione delle prestazioni?
6. Confronta i benefici del deployment locale rispetto a quello cloud per applicazioni sensibili alla privacy.

### Esercizi pratici

1. **Configurazione AI Toolkit**: Configura AI Toolkit e ottimizza un modello (1 ora)
2. **Windows AI Foundry**: Costruisci una semplice applicazione AI su Windows utilizzando l'API Phi Silica (1 ora)
3. **Deployment cross-platform**: Effettua il deployment dello stesso modello su due piattaforme diverse (1 ora)
4. **Ottimizzazione NPU**: Testa le prestazioni NPU con gli strumenti di Windows AI Foundry (30 minuti)

## Modulo 8: Microsoft Foundry Local – Toolkit completo per sviluppatori (modernizzato)

### Obiettivi principali di apprendimento

- Installare e configurare Foundry Local con integrazione SDK moderna
- Implementare sistemi multi-agente avanzati con pattern di coordinamento
- Costruire router di modelli intelligenti con selezione automatica basata su task
- Effettuare il deployment di soluzioni AI pronte per la produzione con monitoraggio completo
- Integrare con Azure AI Foundry per scenari di deployment ibrido
- Padroneggiare pattern SDK moderni con FoundryLocalManager e client OpenAI

### Aree di studio principali

#### Sezione 1: Installazione e configurazione moderna
- **Concetti prioritari**: 
  - Integrazione SDK FoundryLocalManager
  - Scoperta automatica dei servizi e monitoraggio dello stato
  - Pattern di configurazione basati sull'ambiente
  - Considerazioni per il deployment in produzione

#### Sezione 2: Sistemi multi-agente avanzati
- **Concetti prioritari**: 
  - Pattern di coordinamento con agenti specialisti
  - Specializzazione degli agenti per recupero, ragionamento ed esecuzione
  - Meccanismi di feedback per il perfezionamento
  - Monitoraggio delle prestazioni e tracciamento delle statistiche

#### Sezione 3: Routing intelligente dei modelli
- **Concetti prioritari**: 
  - Algoritmi di selezione dei modelli basati su parole chiave
  - Supporto per modelli multipli (generale, ragionamento, codice, creativo)
  - Configurazione delle variabili d'ambiente per flessibilità
  - Controllo dello stato dei servizi e gestione degli errori

#### Sezione 4: Implementazione pronta per la produzione
- **Concetti prioritari**: 
  - Gestione completa degli errori e meccanismi di fallback
  - Monitoraggio delle richieste e tracciamento delle prestazioni
  - Esempi interattivi con Jupyter notebook e benchmark
  - Pattern di integrazione con applicazioni esistenti

### Domande di autovalutazione

1. In che modo l'approccio moderno di FoundryLocalManager differisce dalle chiamate REST manuali?
2. Spiega il pattern di coordinamento e come orchestra gli agenti specialisti.
3. In che modo il router intelligente seleziona i modelli appropriati in base al contenuto della query?
4. Quali sono i componenti chiave di un sistema di agenti AI pronto per la produzione?
5. Come implementare un monitoraggio completo dello stato per i servizi Foundry Local?
6. Confronta i benefici dell'approccio modernizzato rispetto ai pattern di implementazione tradizionali.

### Esercizi pratici

1. **Configurazione SDK moderna**: Configura FoundryLocalManager con scoperta automatica dei servizi (30 minuti)
2. **Sistema multi-agente**: Esegui il coordinatore avanzato con agenti specialisti (30 minuti)
3. **Routing intelligente**: Testa il router di modelli con diversi tipi di query (30 minuti)
4. **Esplorazione interattiva**: Usa i notebook Jupyter per esplorare funzionalità avanzate (45 minuti)
5. **Deployment in produzione**: Implementa pattern di monitoraggio e gestione degli errori (30 minuti)
6. **Integrazione ibrida**: Configura scenari di fallback con Azure AI Foundry (30 minuti)

## Guida alla distribuzione del tempo

Per aiutarti a sfruttare al meglio le 20 ore del corso, ecco una suddivisione suggerita:

| Attività | Allocazione del tempo | Descrizione |
|----------|-----------------------|-------------|
| Lettura dei materiali principali | 9 ore | Focus sui concetti essenziali di ogni modulo |
| Esercizi pratici | 6 ore | Implementazione pratica delle tecniche chiave |
| Autovalutazione | 2 ore | Test della comprensione tramite domande e riflessione |
| Mini-progetto | 3 ore | Applicazione delle conoscenze a una piccola implementazione pratica |

### Aree di focus principali in base al tempo disponibile

**Se hai solo 10 ore:**
- Completa i Moduli 1, 2 e 3 (concetti fondamentali di EdgeAI)
- Fai almeno un esercizio pratico per modulo
- Concentrati sulla comprensione dei concetti principali piuttosto che sui dettagli di implementazione

**Se puoi dedicare l'intero corso di 20 ore:**
- Completa tutti e sette i moduli
- Esegui gli esercizi pratici chiave di ogni modulo
- Completa un mini-progetto dal Modulo 7
- Esplora almeno 2-3 risorse supplementari

**Se hai più di 20 ore:**
- Completa tutti i moduli con esercizi dettagliati
- Costruisci più mini-progetti
- Esplora tecniche avanzate di ottimizzazione nel Modulo 4
- Implementa il deployment in produzione dal Modulo 5

## Risorse essenziali

Queste risorse selezionate con cura offrono il massimo valore per il tuo tempo di studio limitato:

### Documentazione da leggere assolutamente
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Lo strumento di ottimizzazione dei modelli più efficiente
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Il modo più veloce per effettuare il deployment di SLM localmente
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Riferimento per un modello leader ottimizzato per edge
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Toolkit di ottimizzazione completo di Intel
- [AI Toolkit per VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Ambiente di sviluppo EdgeAI integrato
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Piattaforma di sviluppo EdgeAI specifica per Windows

### Strumenti per risparmiare tempo
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accesso rapido ai modelli e deployment
- [Gradio](https://www.gradio.app/docs/interface) - Sviluppo rapido di UI per demo AI
- [Microsoft Olive](https://github.com/microsoft/Olive) - Ottimizzazione semplificata dei modelli
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferenza efficiente su CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework per la compressione delle reti neurali
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit per il deployment di modelli linguistici di grandi dimensioni

## Modello di tracciamento dei progressi

Usa questo modello semplificato per monitorare i tuoi progressi di apprendimento durante il corso di 20 ore:

| Modulo | Data di completamento | Ore dedicate | Principali takeaway |
|--------|-----------------------|--------------|---------------------|
| Modulo 1: Fondamenti di EdgeAI | | | |
| Modulo 2: Fondamenti di SLM | | | |
| Modulo 3: Deployment di SLM | | | |
| Modulo 4: Ottimizzazione del modello | | | |
| Modulo 5: SLMOps | | | |
| Modulo 6: Agenti AI | | | |
| Modulo 7: Strumenti di sviluppo | | | |
| Modulo 8: Toolkit Foundry Local | | | |
| Esercizi Pratici | | | |
| Mini-Progetto | | | |

## Idee per Mini-Progetti

Considera di completare uno di questi progetti per praticare i concetti di EdgeAI (ognuno progettato per richiedere 2-4 ore):

### Progetti per Principianti (2-3 ore ciascuno)
1. **Assistente Testuale Edge**: Crea uno strumento semplice di completamento testuale offline utilizzando un piccolo modello linguistico
2. **Dashboard di Confronto Modelli**: Costruisci una visualizzazione di base delle metriche di prestazione tra diversi SLM
3. **Esperimento di Ottimizzazione**: Misura l'impatto di diversi livelli di quantizzazione sullo stesso modello di base

### Progetti Intermedi (3-4 ore ciascuno)
4. **Workflow con AI Toolkit**: Utilizza VS Code AI Toolkit per ottimizzare e distribuire un modello dall'inizio alla fine
5. **Applicazione Windows AI Foundry**: Crea un'app Windows utilizzando l'API Phi Silica e l'ottimizzazione NPU
6. **Distribuzione Cross-Platform**: Distribuisci lo stesso modello ottimizzato su Windows (OpenVINO) e mobile (.NET MAUI)
7. **Agente con Funzioni di Chiamata**: Costruisci un agente AI con capacità di chiamata di funzioni per scenari edge

### Progetti di Integrazione Avanzata (4-5 ore ciascuno)
8. **Pipeline di Ottimizzazione OpenVINO**: Implementa un'ottimizzazione completa del modello utilizzando NNCF e il toolkit GenAI
9. **Pipeline SLMOps**: Implementa un ciclo di vita completo del modello, dalla formazione alla distribuzione edge
10. **Sistema Edge Multi-Modello**: Distribuisci più modelli specializzati che lavorano insieme su hardware edge
11. **Sistema di Integrazione MCP**: Costruisci un sistema agentico utilizzando il Model Context Protocol per l'interazione con strumenti

## Riferimenti

- Microsoft Learn (Foundry Local)
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Introduzione: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Riferimento CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integrazione con SDK di inferenza: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Come utilizzare Open WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilare modelli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Panoramica: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenti (panoramica): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Strumenti di Ottimizzazione e Inferenza
  - Microsoft Olive (documentazione): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (introduzione): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integrazione Olive con ONNX Runtime: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (documentazione): https://docs.openvino.ai/2025/index.html
  - Apple MLX (documentazione): https://ml-explore.github.io/mlx/build/html/index.html
- Framework di Distribuzione e Modelli
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (documentazione): https://docs.vllm.ai/
  - Ollama (introduzione): https://github.com/ollama/ollama#get-started
- Strumenti per Sviluppatori (Windows e VS Code)
  - AI Toolkit per VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (panoramica): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Comunità di Apprendimento

Unisciti alla discussione e connettiti con altri studenti:
- Discussioni su GitHub nel [repository EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusione

EdgeAI rappresenta l'avanguardia dell'implementazione dell'intelligenza artificiale, portando capacità potenti direttamente sui dispositivi e affrontando preoccupazioni critiche come privacy, latenza e connettività. Questo corso di 20 ore ti fornisce le conoscenze essenziali e le competenze pratiche per iniziare a lavorare con le tecnologie EdgeAI immediatamente.

Il corso è volutamente conciso e focalizzato sui concetti più importanti, permettendoti di acquisire rapidamente competenze preziose senza un impegno di tempo eccessivo. Ricorda che la pratica pratica, anche con esempi semplici, è la chiave per rafforzare ciò che hai imparato.

Buono studio!

---

