<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6a574846c3919c56f1d02bf1de2003ca",
  "translation_date": "2025-10-01T00:12:19+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "it"
}
-->
# Sessione 1: Introduzione a Foundry Local

## Panoramica

Microsoft Foundry Local porta le funzionalit√† di Azure AI Foundry direttamente nel tuo ambiente di sviluppo su Windows 11, consentendo lo sviluppo di AI con strumenti di livello enterprise, preservando la privacy e garantendo una bassa latenza. Questa sessione copre l'installazione completa, la configurazione e la distribuzione pratica di modelli popolari come phi, qwen, deepseek e GPT-OSS-20B.

## Obiettivi di apprendimento

Alla fine di questa sessione, sarai in grado di:
- Installare e configurare Foundry Local su Windows 11
- Padroneggiare i comandi CLI e le opzioni di configurazione
- Comprendere le strategie di caching dei modelli per prestazioni ottimali
- Eseguire con successo i modelli phi, qwen, deepseek e GPT-OSS-20B
- Creare la tua prima applicazione AI utilizzando Foundry Local

## Prerequisiti

### Requisiti di sistema
- **Windows 11**: Versione 22H2 o successiva
- **RAM**: Minimo 16GB, consigliati 32GB
- **Spazio di archiviazione**: 50GB di spazio libero per modelli e cache
- **Hardware**: Dispositivo con NPU o GPU preferito (PC Copilot+ o GPU NVIDIA)
- **Rete**: Connessione internet ad alta velocit√† per il download dei modelli

### Ambiente di sviluppo
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Parte 1: Installazione e configurazione

### Passaggio 1: Installare Foundry Local

Installa Foundry Local utilizzando Winget o scarica l'installer da GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Passaggio 2: Verificare l'installazione

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Parte 2: Comprendere la CLI

### Struttura dei comandi principali

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Parte 3: Gestione e caching dei modelli

Foundry Local implementa un caching intelligente dei modelli per ottimizzare prestazioni e spazio di archiviazione:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Parte 4: Distribuzione pratica dei modelli

### Esecuzione dei modelli Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Utilizzo dei modelli Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b
foundry model run qwen2.5-14b
```

### Esecuzione dei modelli DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-7b
```

### Esecuzione di GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Parte 5: Creazione della tua prima applicazione

### Applicazione di chat moderna (OpenAI SDK + Foundry Local)

Crea un'applicazione di chat pronta per la produzione utilizzando l'integrazione di OpenAI SDK con Foundry Local, seguendo i modelli del nostro Sample 01.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("‚ö†Ô∏è Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"üåê Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"üè† Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"‚ö†Ô∏è Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"üîß Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Eseguire l'applicazione di chat

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Parte 6: Risoluzione dei problemi e migliori pratiche

### Problemi comuni e soluzioni

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Monitoraggio delle risorse di sistema (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Variabili di ambiente

| Variabile | Descrizione | Predefinito | Necessario |
|----------|-------------|-------------|------------|
| `MODEL` | Alias o nome del modello | `phi-4-mini` | No |
| `BASE_URL` | URL base di Foundry Local | `http://localhost:8000` | No |
| `API_KEY` | Chiave API (di solito non necessaria in locale) | `""` | No |
| `AZURE_OPENAI_ENDPOINT` | Endpoint Azure OpenAI | - | Per Azure |
| `AZURE_OPENAI_API_KEY` | Chiave API Azure OpenAI | - | Per Azure |
| `AZURE_OPENAI_API_VERSION` | Versione API Azure | `2024-08-01-preview` | No |

### Migliori pratiche

- **Usa OpenAI SDK**: Preferisci OpenAI SDK rispetto alle richieste HTTP grezze per una migliore manutenibilit√†
- **FoundryLocalManager**: Utilizza l'SDK ufficiale per la gestione dei servizi quando disponibile
- **Gestione degli errori**: Implementa strategie di fallback adeguate per applicazioni in produzione
- **Aggiorna regolarmente**: Mantieni Foundry Local aggiornato per accedere a nuovi modelli e correzioni
- **Inizia con modelli piccoli**: Parti con modelli pi√π piccoli (Phi mini, Qwen 7B) e scala gradualmente
- **Monitora le risorse**: Tieni traccia di CPU/GPU/memoria mentre ottimizzi i prompt e le impostazioni

## Parte 7: Esercizi pratici

### Esercizio 1: Test rapido multi-modello

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Esercizio 2: Test di integrazione OpenAI SDK

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"‚úÖ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"‚ùå {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b"]
for model in models_to_test:
    test_model_integration(model)
```

### Esercizio 3: Controllo completo dello stato del servizio

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"‚úÖ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"‚úÖ {model_id}: Working")
            except Exception as e:
                print(f"‚ùå {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"‚ùå Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Riferimenti

- **Introduzione a Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **Riferimento CLI e panoramica dei comandi**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **Integrazione OpenAI SDK**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Compilare modelli Hugging Face**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: Chat rapida tramite OpenAI SDK**: samples/01/README.md
- **Sample 02: Integrazione SDK avanzata**: samples/02/README.md

---

**Disclaimer**:  
Questo documento √® stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un esperto umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.