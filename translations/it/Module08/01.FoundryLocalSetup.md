<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T18:30:05+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "it"
}
-->
# Sessione 1: Introduzione a Foundry Local

## Panoramica

Microsoft Foundry Local porta le funzionalità di Azure AI Foundry direttamente nel tuo ambiente di sviluppo su Windows 11, consentendo lo sviluppo di AI con strumenti di livello enterprise, preservando la privacy e garantendo una bassa latenza. Questa sessione copre l'installazione completa, la configurazione e il deployment pratico di modelli popolari come phi, qwen, deepseek e GPT-OSS-20B.

## Obiettivi di apprendimento

Alla fine di questa sessione, sarai in grado di:
- Installare e configurare Foundry Local su Windows 11
- Padroneggiare i comandi CLI e le opzioni di configurazione
- Comprendere le strategie di caching dei modelli per prestazioni ottimali
- Eseguire con successo i modelli phi, qwen, deepseek e GPT-OSS-20B
- Creare la tua prima applicazione AI utilizzando Foundry Local

## Prerequisiti

### Requisiti di sistema
- **Windows 11**: Versione 22H2 o successiva
- **RAM**: Minimo 16GB, consigliati 32GB
- **Spazio di archiviazione**: 50GB liberi per modelli e cache
- **Hardware**: Dispositivo con NPU o GPU preferito (PC Copilot+ o GPU NVIDIA)
- **Rete**: Connessione internet ad alta velocità per il download dei modelli

### Ambiente di sviluppo
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## Parte 1: Installazione e configurazione

### Passaggio 1: Installare Foundry Local

Installa Foundry Local utilizzando Winget o scarica l'installer da GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Passaggio 2: Verificare l'installazione

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Parte 2: Comprendere la CLI

### Struttura dei comandi principali

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Parte 3: Gestione e caching dei modelli

Foundry Local implementa un caching intelligente dei modelli per ottimizzare prestazioni e spazio di archiviazione:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Parte 4: Deployment pratico dei modelli

### Esecuzione dei modelli Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Utilizzo dei modelli Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### Esecuzione dei modelli DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### Esecuzione di GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Parte 5: Creazione della tua prima applicazione

### Interfaccia chat semplice (API compatibile con OpenAI)

Crea un'applicazione chat di base utilizzando l'API REST compatibile con OpenAI di Foundry Local. Assicurati che un modello sia in esecuzione in un altro terminale.

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### Esegui l'applicazione chat

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## Parte 6: Risoluzione dei problemi e migliori pratiche

### Problemi comuni e soluzioni

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### Monitoraggio delle risorse di sistema (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Migliori pratiche

- Preferisci i comandi `foundry model ...`, `foundry cache ...` e `foundry service ...` (vedi riferimento CLI)
- Aggiorna regolarmente per accedere a nuovi modelli e correzioni
- Inizia con modelli più piccoli (Phi mini, Qwen 7B) e scala gradualmente
- Monitora CPU/GPU/memoria mentre ottimizzi i prompt e le impostazioni

## Parte 7: Esercizi pratici

### Esercizio 1: Esecuzioni rapide multi-modello

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Esercizio 2: Benchmark di latenza di base

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## Riferimenti

- Introduzione a Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
- Riferimento CLI e panoramica dei comandi: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
- Compilare modelli Hugging Face per Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Microsoft Foundry Local GitHub: https://github.com/microsoft/Foundry-Local  

---

