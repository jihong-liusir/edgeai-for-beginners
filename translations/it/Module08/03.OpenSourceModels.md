<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T18:29:06+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "it"
}
-->
# Sessione 3: Modelli Open-Source con Foundry Local

## Panoramica

Questa sessione esplora come integrare modelli open-source in Foundry Local: selezionare modelli della comunità, integrare contenuti di Hugging Face e adottare strategie "porta il tuo modello" (BYOM). Scoprirai anche la serie Model Mondays per un apprendimento continuo e la scoperta di nuovi modelli.

Riferimenti:
- Documentazione Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compilare modelli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Obiettivi di apprendimento
- Scoprire e valutare modelli open-source per inferenza locale
- Compilare ed eseguire modelli selezionati di Hugging Face all'interno di Foundry Local
- Applicare strategie di selezione dei modelli per accuratezza, latenza e requisiti di risorse
- Gestire modelli localmente con cache e versionamento

## Parte 1: Scoperta e selezione dei modelli (Passo dopo passo)

Passo 1) Elencare i modelli disponibili nel catalogo locale  
```cmd
foundry model list
```
  
Passo 2) Prova rapida di due candidati (download automatico al primo utilizzo)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Passo 3) Annotare metriche di base  
- Osservare la latenza (soggettiva) e la qualità per un prompt fisso  
- Monitorare l'uso della memoria tramite Task Manager mentre ogni modello viene eseguito  

## Parte 2: Esecuzione di modelli del catalogo tramite CLI (Passo dopo passo)

Passo 1) Avviare un modello  
```cmd
foundry model run llama-3.2
```
  
Passo 2) Inviare un prompt di test tramite l'endpoint compatibile con OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Parte 3: BYOM – Compilare modelli Hugging Face (Passo dopo passo)

Segui la guida ufficiale per compilare i modelli. Flusso generale riportato di seguito—consulta l'articolo su Microsoft Learn per i comandi esatti e le configurazioni supportate.

Passo 1) Preparare una directory di lavoro  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Passo 2) Compilare un modello HF supportato  
- Usa i passaggi della documentazione Learn per convertire e posizionare il modello ONNX compilato nella directory `models`  
- Conferma con:  
```cmd
foundry cache ls
```
  
Dovresti vedere il nome del modello compilato (ad esempio, `llama-3.2`).  

Passo 3) Eseguire il modello compilato  
```cmd
foundry model run llama-3.2 --verbose
```
  
Note:  
- Assicurati di avere spazio su disco e RAM sufficienti per la compilazione e l'esecuzione  
- Inizia con modelli più piccoli per validare il flusso, poi scala gradualmente  

## Parte 4: Curazione pratica dei modelli (Passo dopo passo)

Passo 1) Creare un registro `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Passo 2) Script di selezione semplice  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Parte 5: Benchmark pratici (Passo dopo passo)

Passo 1) Benchmark di latenza semplice  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Passo 2) Controllo qualità  
- Usa un set di prompt fisso, cattura gli output in un file CSV/JSON  
- Valuta manualmente fluidità, rilevanza e correttezza (1–5)  

## Parte 6: Prossimi passi
- Iscriviti a Model Mondays per nuovi modelli e suggerimenti: https://aka.ms/model-mondays  
- Condividi i risultati con il registro `models.json` del tuo team  
- Preparati per la Sessione 4: confronto tra LLM e SLM, inferenza locale vs cloud e dimostrazioni pratiche  

---

