<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T21:24:30+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "it"
}
-->
# Sessione 3: Scoperta e Gestione di Modelli Open-Source

## Panoramica

Questa sessione si concentra sulla scoperta e gestione pratica dei modelli con Foundry Local. Imparerai a elencare i modelli disponibili, testare diverse opzioni e comprendere le caratteristiche di base delle prestazioni. L'approccio enfatizza l'esplorazione pratica con il CLI di Foundry per aiutarti a selezionare i modelli giusti per i tuoi casi d'uso.

## Obiettivi di Apprendimento

- Padroneggiare i comandi del CLI di Foundry per la scoperta e gestione dei modelli
- Comprendere i modelli di cache e archiviazione locale
- Imparare a testare e confrontare rapidamente diversi modelli
- Stabilire flussi di lavoro pratici per la selezione e il benchmarking dei modelli
- Esplorare l'ecosistema in crescita di modelli disponibili tramite Foundry Local

## Prerequisiti

- Completata la Sessione 1: Introduzione a Foundry Local
- CLI di Foundry Local installato e accessibile
- Spazio di archiviazione sufficiente per il download dei modelli (i modelli possono variare da 1GB a oltre 20GB)
- Comprensione di base dei tipi di modelli e dei casi d'uso

## Parte 6: Esercizio Pratico

### Esercizio: Scoperta e Confronto di Modelli

Crea il tuo script di valutazione dei modelli basato su Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Il tuo compito

1. **Esegui lo script Sample 03**: `samples\03\list_and_bench.cmd`
2. **Prova diversi modelli**: Testa almeno 3 modelli diversi
3. **Confronta le prestazioni**: Nota le differenze in velocità e qualità delle risposte
4. **Documenta i risultati**: Crea un semplice grafico di confronto

### Formato di Confronto Esempio

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Parte 7: Risoluzione dei Problemi e Migliori Pratiche

### Problemi Comuni e Soluzioni

**Il modello non si avvia:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Memoria insufficiente:**
- Inizia con modelli più piccoli (`phi-4-mini`)
- Chiudi altre applicazioni
- Aggiorna la RAM se riscontri frequentemente limiti

**Prestazioni lente:**
- Assicurati che il modello sia completamente caricato (controlla l'output dettagliato)
- Chiudi applicazioni in background non necessarie
- Considera un'archiviazione più veloce (SSD)

### Migliori Pratiche

1. **Inizia con modelli piccoli**: Usa `phi-4-mini` per validare la configurazione
2. **Un modello alla volta**: Ferma i modelli precedenti prima di avviarne di nuovi
3. **Monitora le risorse**: Tieni d'occhio l'utilizzo della memoria
4. **Testa in modo coerente**: Usa gli stessi prompt per confronti equi
5. **Documenta i risultati**: Prendi nota delle prestazioni dei modelli per i tuoi casi d'uso

## Parte 8: Prossimi Passi e Riferimenti

### Preparazione per la Sessione 4

- **Focus della Sessione 4**: Strumenti e tecniche di ottimizzazione
- **Prerequisiti**: Familiarità con il cambio di modelli e test di prestazioni di base
- **Consigliato**: Identifica 2-3 modelli preferiti da questa sessione

### Risorse Aggiuntive

- **[Documentazione Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Documentazione ufficiale
- **[Riferimento CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Riferimento completo ai comandi
- **[Model Mondays](https://aka.ms/model-mondays)**: Spotlight settimanale sui modelli
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Comunità e problemi
- **[Sample 03: Scoperta di Modelli](samples/03/README.md)**: Script di esempio pratico

### Punti Chiave

✅ **Scoperta di Modelli**: Usa `foundry model list` per esplorare i modelli disponibili  
✅ **Test Rapidi**: Il pattern `list_and_bench.cmd` per valutazioni rapide  
✅ **Monitoraggio delle Prestazioni**: Misurazione di utilizzo delle risorse e tempi di risposta  
✅ **Selezione dei Modelli**: Linee guida pratiche per scegliere modelli in base ai casi d'uso  
✅ **Gestione della Cache**: Comprendere procedure di archiviazione e pulizia  

Ora hai le competenze pratiche per scoprire, testare e selezionare modelli appropriati per le tue applicazioni AI utilizzando l'approccio semplice del CLI di Foundry Local.

## Obiettivi di Apprendimento

- Scoprire e valutare modelli open-source per inferenza locale
- Compilare ed eseguire modelli selezionati di Hugging Face all'interno di Foundry Local
- Applicare strategie di selezione dei modelli per accuratezza, latenza e necessità di risorse
- Gestire i modelli localmente con cache e versionamento

## Parte 1: Scoperta di Modelli con Foundry CLI

### Comandi Base per la Gestione dei Modelli

Il CLI di Foundry offre comandi semplici per la scoperta e gestione dei modelli:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Esecuzione dei Primi Modelli

Inizia con modelli popolari e ben testati per comprendere le caratteristiche di prestazione:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Nota:** Il flag `--verbose` fornisce informazioni dettagliate sull'avvio, tra cui:
- Progresso del download del modello (alla prima esecuzione)
- Dettagli sull'allocazione della memoria
- Informazioni sul binding del servizio
- Metriche di inizializzazione delle prestazioni

### Comprendere le Categorie di Modelli

**Small Language Models (SLMs):**
- `phi-4-mini`: Veloce, efficiente, ottimo per chat generali
- `phi-4`: Versione più capace con migliori capacità di ragionamento

**Modelli Medi:**
- `qwen2.5-7b-instruct`: Ottimo ragionamento e contesto più lungo
- `deepseek-r1-distill-qwen-7b`: Ottimizzato per generazione di codice

**Modelli Grandi:**
- `llama-3.2`: Ultimo modello open-source di Meta
- `qwen2.5-14b-instruct`: Ragionamento di livello aziendale

## Parte 2: Test Rapidi e Confronto di Modelli

### Approccio Sample 03: Lista e Benchmark Semplici

Basato sul pattern Sample 03, ecco il flusso di lavoro minimo:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Test delle Prestazioni dei Modelli

Una volta che un modello è in esecuzione, testalo con prompt coerenti:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternativa di Test con PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Parte 3: Gestione della Cache e Archiviazione dei Modelli

### Comprendere la Cache dei Modelli

Foundry Local gestisce automaticamente il download e la cache dei modelli:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Considerazioni sull'Archiviazione dei Modelli

**Dimensioni Tipiche dei Modelli:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Migliori Pratiche di Archiviazione:**
- Mantieni 2-3 modelli in cache per cambi rapidi
- Rimuovi modelli inutilizzati per liberare spazio: `foundry cache clean`
- Monitora l'uso del disco, specialmente su SSD più piccoli
- Considera il compromesso tra dimensione e capacità del modello

### Monitoraggio delle Prestazioni dei Modelli

Mentre i modelli sono in esecuzione, monitora le risorse di sistema:

**Task Manager di Windows:**
- Controlla l'uso della memoria (i modelli rimangono caricati in RAM)
- Monitora l'utilizzo della CPU durante l'inferenza
- Controlla l'I/O del disco durante il caricamento iniziale del modello

**Monitoraggio da Linea di Comando:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Parte 4: Linee Guida Pratiche per la Selezione dei Modelli

### Scelta dei Modelli in Base al Caso d'Uso

**Per Chat Generali e Q&A:**
- Inizia con: `phi-4-mini` (veloce, efficiente)
- Passa a: `phi-4` (miglior ragionamento)
- Avanzato: `qwen2.5-7b-instruct` (contesto più lungo)

**Per Generazione di Codice:**
- Consigliato: `deepseek-r1-distill-qwen-7b`
- Alternativa: `qwen2.5-7b-instruct` (anche buono per codice)

**Per Ragionamento Complesso:**
- Migliore: `qwen2.5-7b-instruct` o `qwen2.5-14b-instruct`
- Opzione economica: `phi-4`

### Guida ai Requisiti Hardware

**Requisiti Minimi di Sistema:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Consigliato per Prestazioni Ottimali:**
- RAM da 32GB+ per cambio comodo tra modelli
- Archiviazione SSD per caricamento più veloce dei modelli
- CPU moderna con buona prestazione single-thread
- Supporto NPU (PC con Windows 11 Copilot+) per accelerazione

### Flusso di Lavoro per il Cambio di Modelli

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Parte 5: Benchmarking Semplice dei Modelli

### Test di Prestazioni di Base

Ecco un approccio semplice per confrontare le prestazioni dei modelli:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Valutazione Manuale della Qualità

Per ogni modello, testalo con prompt coerenti e valuta manualmente:

**Prompt di Test:**
1. "Spiega il calcolo quantistico in termini semplici."
2. "Scrivi una funzione Python per ordinare una lista."
3. "Quali sono i pro e i contro del lavoro remoto?"
4. "Riassumi i benefici dell'AI edge."

**Criteri di Valutazione:**
- **Accuratezza**: Le informazioni sono corrette?
- **Chiarezza**: La spiegazione è facile da capire?
- **Completezza**: Risponde completamente alla domanda?
- **Velocità**: Quanto velocemente risponde?

### Monitoraggio dell'Uso delle Risorse

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Parte 6: Prossimi Passi

- Iscriviti a Model Mondays per nuovi modelli e suggerimenti: https://aka.ms/model-mondays
- Condividi i risultati con il file `models.json` del tuo team
- Preparati per la Sessione 4: confronto tra LLM e SLM, inferenza locale vs cloud e demo pratiche

---

