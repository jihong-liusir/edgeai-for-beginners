<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T23:37:09+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "it"
}
-->
# Sezione 3: Microsoft Olive Optimization Suite

## Indice
1. [Introduzione](../../../Module04)
2. [Cos'è Microsoft Olive?](../../../Module04)
3. [Installazione](../../../Module04)
4. [Guida rapida](../../../Module04)
5. [Esempio: Conversione di Qwen3 in ONNX INT4](../../../Module04)
6. [Utilizzo avanzato](../../../Module04)
7. [Best Practices](../../../Module04)
8. [Risoluzione dei problemi](../../../Module04)
9. [Risorse aggiuntive](../../../Module04)

## Introduzione

Microsoft Olive è un toolkit potente e facile da usare per l'ottimizzazione di modelli, progettato per essere consapevole dell'hardware. Semplifica il processo di ottimizzazione dei modelli di machine learning per il deployment su diverse piattaforme hardware. Che tu stia lavorando con CPU, GPU o acceleratori AI specializzati, Olive ti aiuta a ottenere prestazioni ottimali mantenendo l'accuratezza del modello.

## Cos'è Microsoft Olive?

Olive è uno strumento di ottimizzazione dei modelli consapevole dell'hardware che integra tecniche leader del settore per la compressione, l'ottimizzazione e la compilazione dei modelli. Funziona con ONNX Runtime come soluzione di ottimizzazione end-to-end per l'inferenza.

### Caratteristiche principali

- **Ottimizzazione consapevole dell'hardware**: Seleziona automaticamente le migliori tecniche di ottimizzazione per il tuo hardware di destinazione
- **Oltre 40 componenti di ottimizzazione integrati**: Include compressione del modello, quantizzazione, ottimizzazione del grafo e altro
- **Interfaccia CLI semplice**: Comandi intuitivi per attività di ottimizzazione comuni
- **Supporto multi-framework**: Compatibile con PyTorch, modelli Hugging Face e ONNX
- **Supporto per modelli popolari**: Olive può ottimizzare automaticamente architetture di modelli popolari come Llama, Phi, Qwen, Gemma, ecc.

### Vantaggi

- **Riduzione del tempo di sviluppo**: Non è necessario sperimentare manualmente diverse tecniche di ottimizzazione
- **Miglioramenti delle prestazioni**: Incrementi significativi di velocità (fino a 6 volte in alcuni casi)
- **Deployment cross-platform**: Modelli ottimizzati funzionano su diversi hardware e sistemi operativi
- **Accuratezza mantenuta**: Le ottimizzazioni preservano la qualità del modello migliorando le prestazioni

## Installazione

### Prerequisiti

- Python 3.8 o superiore
- Gestore di pacchetti pip
- Ambiente virtuale (consigliato)

### Installazione di base

Crea e attiva un ambiente virtuale:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installa Olive con funzionalità di ottimizzazione automatica:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dipendenze opzionali

Olive offre diverse dipendenze opzionali per funzionalità aggiuntive:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifica dell'installazione

```bash
olive --help
```

Se l'installazione è riuscita, dovresti vedere il messaggio di aiuto della CLI di Olive.

## Guida rapida

### La tua prima ottimizzazione

Ottimizziamo un piccolo modello linguistico utilizzando la funzione di ottimizzazione automatica di Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Cosa fa questo comando

Il processo di ottimizzazione include: acquisizione del modello dalla cache locale, cattura del grafo ONNX e salvataggio dei pesi in un file dati ONNX, ottimizzazione del grafo ONNX e quantizzazione del modello a int4 utilizzando il metodo RTN.

### Spiegazione dei parametri del comando

- `--model_name_or_path`: Identificatore del modello Hugging Face o percorso locale
- `--output_path`: Directory in cui verrà salvato il modello ottimizzato
- `--device`: Dispositivo di destinazione (cpu, gpu)
- `--provider`: Provider di esecuzione (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Utilizza ONNX Runtime Generate AI per l'inferenza
- `--precision`: Precisione della quantizzazione (int4, int8, fp16)
- `--log_level`: Livello di verbosità dei log (0=minimale, 1=verboso)

## Esempio: Conversione di Qwen3 in ONNX INT4

Basandoci sull'esempio fornito da Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ecco come ottimizzare un modello Qwen3:

### Passo 1: Scarica il modello (opzionale)

Per ridurre i tempi di download, memorizza nella cache solo i file essenziali:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Passo 2: Ottimizza il modello Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Passo 3: Testa il modello ottimizzato

Crea uno script Python semplice per testare il tuo modello ottimizzato:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struttura dell'output

Dopo l'ottimizzazione, la tua directory di output conterrà:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Utilizzo avanzato

### File di configurazione

Per flussi di lavoro di ottimizzazione più complessi, puoi utilizzare file di configurazione JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Esegui con configurazione:

```bash
olive run --config config.json
```

### Ottimizzazione GPU

Per l'ottimizzazione GPU con CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Per DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning con Olive

Olive supporta anche il fine-tuning dei modelli:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Selezione del modello
- Inizia con modelli più piccoli per i test (es. 0.5B-7B parametri)
- Assicurati che l'architettura del modello di destinazione sia supportata da Olive

### 2. Considerazioni sull'hardware
- Adatta il tuo obiettivo di ottimizzazione all'hardware di deployment
- Usa l'ottimizzazione GPU se disponi di hardware compatibile con CUDA
- Considera DirectML per macchine Windows con grafica integrata

### 3. Selezione della precisione
- **INT4**: Massima compressione, lieve perdita di accuratezza
- **INT8**: Buon equilibrio tra dimensioni e accuratezza
- **FP16**: Perdita minima di accuratezza, riduzione moderata delle dimensioni

### 4. Test e validazione
- Testa sempre i modelli ottimizzati con i tuoi casi d'uso specifici
- Confronta le metriche di prestazione (latenza, throughput, accuratezza)
- Usa dati di input rappresentativi per la valutazione

### 5. Ottimizzazione iterativa
- Inizia con l'ottimizzazione automatica per risultati rapidi
- Usa file di configurazione per un controllo più dettagliato
- Sperimenta con diversi passaggi di ottimizzazione

## Risoluzione dei problemi

### Problemi comuni

#### 1. Problemi di installazione
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemi con CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemi di memoria
- Usa dimensioni di batch più piccole durante l'ottimizzazione
- Prova la quantizzazione con una precisione più alta (int8 invece di int4)
- Assicurati di avere spazio su disco sufficiente per la cache del modello

#### 4. Errori di caricamento del modello
- Verifica il percorso del modello e i permessi di accesso
- Controlla se il modello richiede `trust_remote_code=True`
- Assicurati che tutti i file richiesti del modello siano scaricati

### Ottenere aiuto

- **Documentazione**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemi su GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Esempi**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Risorse aggiuntive

### Link ufficiali
- **Repository GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Documentazione ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Esempio Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Esempi della community
- **Notebook Jupyter**: Disponibili nel repository GitHub di Olive
- **Estensione VS Code**: L'estensione AI Toolkit utilizza Olive per l'ottimizzazione dei modelli
- **Post sul blog**: Il blog Microsoft Open Source contiene tutorial dettagliati su Olive

### Strumenti correlati
- **ONNX Runtime**: Motore di inferenza ad alte prestazioni
- **Hugging Face Transformers**: Fonte di molti modelli compatibili
- **Azure Machine Learning**: Flussi di lavoro di ottimizzazione basati su cloud

## ➡️ Cosa fare dopo

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.