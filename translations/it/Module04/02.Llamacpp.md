<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T23:42:08+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "it"
}
-->
# Sezione 2: Guida all'implementazione di Llama.cpp

## Indice
1. [Introduzione](../../../Module04)
2. [Cos'è Llama.cpp?](../../../Module04)
3. [Installazione](../../../Module04)
4. [Compilazione da sorgente](../../../Module04)
5. [Quantizzazione del modello](../../../Module04)
6. [Utilizzo di base](../../../Module04)
7. [Funzionalità avanzate](../../../Module04)
8. [Integrazione con Python](../../../Module04)
9. [Risoluzione dei problemi](../../../Module04)
10. [Migliori pratiche](../../../Module04)

## Introduzione

Questo tutorial completo ti guiderà attraverso tutto ciò che devi sapere su Llama.cpp, dall'installazione di base agli scenari di utilizzo avanzati. Llama.cpp è una potente implementazione in C++ che consente un'inferenza efficiente dei modelli di linguaggio di grandi dimensioni (LLM) con una configurazione minima e prestazioni eccellenti su diverse configurazioni hardware.

## Cos'è Llama.cpp?

Llama.cpp è un framework di inferenza per LLM scritto in C/C++ che permette di eseguire modelli di linguaggio di grandi dimensioni localmente con una configurazione minima e prestazioni all'avanguardia su una vasta gamma di hardware. Le caratteristiche principali includono:

### Caratteristiche principali
- **Implementazione in C/C++ puro** senza dipendenze
- **Compatibilità multipiattaforma** (Windows, macOS, Linux)
- **Ottimizzazione hardware** per diverse architetture
- **Supporto alla quantizzazione** (da 1,5 bit a 8 bit interi)
- **Accelerazione su CPU e GPU**
- **Efficienza della memoria** per ambienti con risorse limitate

### Vantaggi
- Funziona in modo efficiente su CPU senza richiedere hardware specializzato
- Supporta diversi backend GPU (CUDA, Metal, OpenCL, Vulkan)
- Leggero e portatile
- Apple Silicon è una priorità: ottimizzato tramite ARM NEON, Accelerate e Metal frameworks
- Supporta vari livelli di quantizzazione per ridurre l'uso della memoria

## Installazione

### Metodo 1: Binari precompilati (Consigliato per principianti)

#### Scarica da GitHub Releases
1. Visita [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Scarica il binario appropriato per il tuo sistema:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` per Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` per macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` per Linux

3. Estrai l'archivio e aggiungi la directory al PATH del tuo sistema

#### Utilizzo di gestori di pacchetti

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Varie distribuzioni):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metodo 2: Pacchetto Python (llama-cpp-python)

#### Installazione di base
```bash
pip install llama-cpp-python
```

#### Con accelerazione hardware
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Compilazione da sorgente

### Prerequisiti

**Requisiti di sistema:**
- Compilatore C++ (GCC, Clang o MSVC)
- CMake (versione 3.14 o superiore)
- Git
- Strumenti di build per la tua piattaforma

**Installazione dei prerequisiti:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Installa Visual Studio 2022 con strumenti di sviluppo C++
- Installa CMake dal sito ufficiale
- Installa Git

### Processo di compilazione di base

1. **Clona il repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Configura la build:**
```bash
cmake -B build
```

3. **Compila il progetto:**
```bash
cmake --build build --config Release
```

Per una compilazione più veloce, usa lavori paralleli:
```bash
cmake --build build --config Release -j 8
```

### Build specifiche per hardware

#### Supporto CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Supporto Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Supporto OpenBLAS (Ottimizzazione CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Supporto Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Opzioni di build avanzate

#### Build di debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Con funzionalità aggiuntive
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Quantizzazione del modello

### Comprendere il formato GGUF

GGUF (Generalized GGML Unified Format) è un formato di file ottimizzato progettato per eseguire modelli di linguaggio di grandi dimensioni in modo efficiente utilizzando Llama.cpp e altri framework. Offre:

- Archiviazione standardizzata dei pesi del modello
- Migliore compatibilità tra piattaforme
- Prestazioni migliorate
- Gestione efficiente dei metadati

### Tipi di quantizzazione

Llama.cpp supporta vari livelli di quantizzazione:

| Tipo | Bit | Descrizione | Caso d'uso |
|------|------|-------------|----------|
| F16 | 16 | Precisione dimezzata | Alta qualità, grande memoria |
| Q8_0 | 8 | Quantizzazione a 8 bit | Buon equilibrio |
| Q4_0 | 4 | Quantizzazione a 4 bit | Qualità moderata, dimensioni ridotte |
| Q2_K | 2 | Quantizzazione a 2 bit | Dimensioni minime, qualità inferiore |

### Conversione dei modelli

#### Da PyTorch a GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Download diretto da Hugging Face
Molti modelli sono disponibili in formato GGUF su Hugging Face:
- Cerca modelli con "GGUF" nel nome
- Scarica il livello di quantizzazione appropriato
- Usa direttamente con llama.cpp

## Utilizzo di base

### Interfaccia a riga di comando

#### Generazione semplice di testo
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Utilizzo di modelli da Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Modalità server
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Parametri comuni

| Parametro | Descrizione | Esempio |
|-----------|-------------|---------|
| `-m` | Percorso del file del modello | `-m model.gguf` |
| `-p` | Testo del prompt | `-p "Ciao mondo"` |
| `-n` | Numero di token da generare | `-n 100` |
| `-c` | Dimensione del contesto | `-c 4096` |
| `-t` | Numero di thread | `-t 8` |
| `-ngl` | Livelli GPU | `-ngl 32` |
| `-temp` | Temperatura | `-temp 0.7` |

### Modalità interattiva

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Funzionalità avanzate

### API del server

#### Avvio del server
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Utilizzo dell'API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Ottimizzazione delle prestazioni

#### Gestione della memoria
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multithreading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Accelerazione GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integrazione con Python

### Utilizzo di base con llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Interfaccia di chat

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Risposte in streaming

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integrazione con LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Risoluzione dei problemi

### Problemi comuni e soluzioni

#### Errori di compilazione

**Problema: CMake non trovato**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problema: Compilatore non trovato**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Problemi di runtime

**Problema: Caricamento del modello fallito**
- Verifica il percorso del file del modello
- Controlla i permessi del file
- Assicurati di avere RAM sufficiente
- Prova diversi livelli di quantizzazione

**Problema: Prestazioni scarse**
- Abilita l'accelerazione hardware
- Aumenta il numero di thread
- Usa una quantizzazione appropriata
- Controlla l'uso della memoria GPU

#### Problemi di memoria

**Problema: Memoria insufficiente**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problemi specifici della piattaforma

#### Windows
- Usa il compilatore MinGW o Visual Studio
- Configura correttamente il PATH
- Controlla eventuali interferenze dell'antivirus

#### macOS
- Abilita Metal per Apple Silicon
- Usa Rosetta 2 per la compatibilità, se necessario
- Controlla gli strumenti da riga di comando di Xcode

#### Linux
- Installa i pacchetti di sviluppo
- Controlla le versioni dei driver GPU
- Verifica l'installazione del toolkit CUDA

## Migliori pratiche

### Selezione del modello
1. **Scegli la quantizzazione appropriata** in base al tuo hardware
2. **Considera il compromesso tra dimensione del modello e qualità**
3. **Testa diversi modelli** per il tuo caso d'uso specifico

### Ottimizzazione delle prestazioni
1. **Usa l'accelerazione GPU** quando disponibile
2. **Ottimizza il numero di thread** per la tua CPU
3. **Imposta una dimensione del contesto appropriata** per il tuo caso d'uso
4. **Abilita il memory mapping** per modelli grandi

### Distribuzione in produzione
1. **Usa la modalità server** per l'accesso API
2. **Implementa una gestione degli errori adeguata**
3. **Monitora l'uso delle risorse**
4. **Configura il logging e il monitoraggio**

### Workflow di sviluppo
1. **Inizia con modelli più piccoli** per i test
2. **Usa il controllo di versione** per le configurazioni dei modelli
3. **Documenta le tue configurazioni**
4. **Testa su diverse piattaforme**

### Considerazioni sulla sicurezza
1. **Valida i prompt di input**
2. **Implementa il rate limiting**
3. **Proteggi gli endpoint API**
4. **Monitora i pattern di abuso**

## Conclusione

Llama.cpp offre un modo potente ed efficiente per eseguire modelli di linguaggio di grandi dimensioni localmente su diverse configurazioni hardware. Che tu stia sviluppando applicazioni AI, conducendo ricerche o semplicemente sperimentando con LLM, questo framework offre la flessibilità e le prestazioni necessarie per una vasta gamma di casi d'uso.

Punti chiave:
- Scegli il metodo di installazione che meglio si adatta alle tue esigenze
- Ottimizza per la configurazione hardware specifica
- Inizia con l'utilizzo di base ed esplora gradualmente le funzionalità avanzate
- Considera l'uso delle librerie Python per un'integrazione più semplice
- Segui le migliori pratiche per le distribuzioni in produzione

Per ulteriori informazioni e aggiornamenti, visita il [repository ufficiale di Llama.cpp](https://github.com/ggml-org/llama.cpp) e consulta la documentazione completa e le risorse della comunità disponibili.

## ➡️ Cosa fare dopo

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.