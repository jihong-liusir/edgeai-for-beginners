<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T23:31:32+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "it"
}
-->
# Sezione 4: Suite di Ottimizzazione OpenVINO Toolkit

## Indice
1. [Introduzione](../../../Module04)
2. [Cos'è OpenVINO?](../../../Module04)
3. [Installazione](../../../Module04)
4. [Guida Rapida](../../../Module04)
5. [Esempio: Conversione e Ottimizzazione di Modelli con OpenVINO](../../../Module04)
6. [Uso Avanzato](../../../Module04)
7. [Best Practices](../../../Module04)
8. [Risoluzione dei Problemi](../../../Module04)
9. [Risorse Aggiuntive](../../../Module04)

## Introduzione

OpenVINO (Open Visual Inference and Neural Network Optimization) è il toolkit open-source di Intel per distribuire soluzioni AI performanti su cloud, ambienti locali e edge. Che tu stia lavorando con CPU, GPU, VPU o acceleratori AI specializzati, OpenVINO offre capacità di ottimizzazione complete mantenendo l'accuratezza del modello e abilitando la distribuzione cross-platform.

## Cos'è OpenVINO?

OpenVINO è un toolkit open-source che consente agli sviluppatori di ottimizzare, convertire e distribuire modelli AI in modo efficiente su diverse piattaforme hardware. È composto da tre componenti principali: OpenVINO Runtime per l'inferenza, Neural Network Compression Framework (NNCF) per l'ottimizzazione dei modelli e OpenVINO Model Server per la distribuzione scalabile.

### Caratteristiche Principali

- **Distribuzione Cross-Platform**: Supporta Linux, Windows e macOS con API Python, C++ e C
- **Accelerazione Hardware**: Rilevamento automatico dei dispositivi e ottimizzazione per CPU, GPU, VPU e acceleratori AI
- **Framework di Compressione dei Modelli**: Tecniche avanzate di quantizzazione, pruning e ottimizzazione tramite NNCF
- **Compatibilità con Framework**: Supporto diretto per modelli TensorFlow, ONNX, PaddlePaddle e PyTorch
- **Supporto per AI Generativa**: OpenVINO GenAI specializzato per distribuire modelli di linguaggio di grandi dimensioni e applicazioni AI generativa

### Benefici

- **Ottimizzazione delle Prestazioni**: Miglioramenti significativi della velocità con perdita minima di accuratezza
- **Riduzione dell'Impronta di Distribuzione**: Dipendenze esterne minime semplificano installazione e distribuzione
- **Tempi di Avvio Ridotti**: Caricamento e caching ottimizzati dei modelli per un'inizializzazione più rapida delle applicazioni
- **Distribuzione Scalabile**: Da dispositivi edge a infrastrutture cloud con API coerenti
- **Pronto per la Produzione**: Affidabilità di livello enterprise con documentazione completa e supporto della comunità

## Installazione

### Prerequisiti

- Python 3.8 o superiore
- Gestore di pacchetti pip
- Ambiente virtuale (consigliato)
- Hardware compatibile (CPU Intel consigliate, ma supporta varie architetture)

### Installazione Base

Crea e attiva un ambiente virtuale:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installa OpenVINO Runtime:

```bash
pip install openvino
```

Installa NNCF per l'ottimizzazione dei modelli:

```bash
pip install nncf
```

### Installazione di OpenVINO GenAI

Per applicazioni AI generativa:

```bash
pip install openvino-genai
```

### Dipendenze Opzionali

Pacchetti aggiuntivi per casi d'uso specifici:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verifica dell'Installazione

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Se l'installazione è avvenuta con successo, dovresti vedere le informazioni sulla versione di OpenVINO.

## Guida Rapida

### La tua Prima Ottimizzazione di Modello

Convertiamo e ottimizziamo un modello Hugging Face utilizzando OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Cosa Fa Questo Processo

Il workflow di ottimizzazione include: caricamento del modello originale da Hugging Face, conversione al formato Intermediate Representation (IR) di OpenVINO, applicazione di ottimizzazioni predefinite e compilazione per hardware target.

### Parametri Chiave Spiegati

- `export=True`: Converte il modello al formato IR di OpenVINO
- `compile=False`: Ritarda la compilazione fino al runtime per maggiore flessibilità
- `device`: Hardware target ("CPU", "GPU", "AUTO" per selezione automatica)
- `save_pretrained()`: Salva il modello ottimizzato per riutilizzo

## Esempio: Conversione e Ottimizzazione di Modelli con OpenVINO

### Passo 1: Conversione del Modello con Quantizzazione NNCF

Ecco come applicare la quantizzazione post-training utilizzando NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Passo 2: Ottimizzazione Avanzata con Compressione dei Pesi

Per modelli basati su transformer, applica la compressione dei pesi:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Passo 3: Inferenza con Modello Ottimizzato

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struttura dell'Output

Dopo l'ottimizzazione, la directory del modello conterrà:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Uso Avanzato

### Configurazione con YAML di NNCF

Per workflow di ottimizzazione complessi, utilizza file di configurazione NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Applica la configurazione:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Ottimizzazione GPU

Per accelerazione GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Ottimizzazione per Elaborazione in Batch

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Distribuzione con Model Server

Distribuisci modelli ottimizzati con OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Codice client per il model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Best Practices

### 1. Selezione e Preparazione del Modello
- Utilizza modelli da framework supportati (PyTorch, TensorFlow, ONNX)
- Assicurati che gli input del modello abbiano forme fisse o dinamiche note
- Testa con dataset rappresentativi per la calibrazione

### 2. Selezione della Strategia di Ottimizzazione
- **Quantizzazione Post-Training**: Inizia qui per un'ottimizzazione rapida
- **Compressione dei Pesi**: Ideale per modelli di linguaggio di grandi dimensioni e transformer
- **Quantizzazione Consapevole dell'Addestramento**: Utilizza quando l'accuratezza è critica

### 3. Ottimizzazione Specifica per Hardware
- **CPU**: Usa la quantizzazione INT8 per prestazioni bilanciate
- **GPU**: Sfrutta la precisione FP16 e l'elaborazione in batch
- **VPU**: Concentrati sulla semplificazione del modello e fusione dei layer

### 4. Ottimizzazione delle Prestazioni
- **Modalità Throughput**: Per elaborazione in batch ad alto volume
- **Modalità Latency**: Per applicazioni interattive in tempo reale
- **Dispositivo AUTO**: Lascia che OpenVINO selezioni l'hardware ottimale

### 5. Gestione della Memoria
- Usa forme dinamiche con attenzione per evitare sovraccarico di memoria
- Implementa il caching dei modelli per caricamenti successivi più rapidi
- Monitora l'uso della memoria durante l'ottimizzazione

### 6. Validazione dell'Accuratezza
- Valida sempre i modelli ottimizzati rispetto alle prestazioni originali
- Utilizza dataset di test rappresentativi per la valutazione
- Considera un'ottimizzazione graduale (inizia con impostazioni conservative)

## Risoluzione dei Problemi

### Problemi Comuni

#### 1. Problemi di Installazione
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Errori di Conversione del Modello
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Problemi di Prestazioni
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Problemi di Memoria
- Riduci la dimensione del batch del modello durante l'ottimizzazione
- Usa lo streaming per dataset di grandi dimensioni
- Abilita il caching del modello: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Degradazione dell'Accuratezza
- Usa una precisione più alta (INT8 invece di INT4)
- Aumenta la dimensione del dataset di calibrazione
- Applica l'ottimizzazione a precisione mista

### Monitoraggio delle Prestazioni

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Ottenere Aiuto

- **Documentazione**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Problemi su GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum della Comunità**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Risorse Aggiuntive

### Link Ufficiali
- **Homepage OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Repository GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Repository NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Risorse di Apprendimento
- **Notebook OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Guida Rapida**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Guida all'Ottimizzazione**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Strumenti di Integrazione
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmark delle Prestazioni
- **Benchmark Ufficiali**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Esempi della Comunità
- **Notebook Jupyter**: [Repository Notebook OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Tutorial completi disponibili nel repository dei notebook OpenVINO
- **Applicazioni di Esempio**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Esempi reali per vari domini (visione artificiale, NLP, audio)
- **Post sul Blog**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Post sul blog di Intel AI e della comunità con casi d'uso dettagliati

### Strumenti Correlati
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Tecniche di ottimizzazione aggiuntive per hardware Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Per confronti di distribuzione su mobile e edge
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternative per motori di inferenza cross-platform

## ➡️ Cosa fare dopo

- [05: Approfondimento sul Framework Apple MLX](./05.AppleMLX.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un esperto umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.