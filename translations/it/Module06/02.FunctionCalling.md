<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8b05633cf46af274b3724ee2f7140100",
  "translation_date": "2025-09-17T23:03:15+00:00",
  "source_file": "Module06/02.FunctionCalling.md",
  "language_code": "it"
}
-->
# Sezione02: Chiamata di Funzioni nei Modelli Linguistici Ridotti (SLMs)

## Indice
1. [Cos'è la Chiamata di Funzioni?](../../../Module06)
2. [Come Funziona la Chiamata di Funzioni](../../../Module06)
3. [Scenari di Applicazione](../../../Module06)
4. [Configurazione della Chiamata di Funzioni con Phi-4-mini e Ollama](../../../Module06)
5. [Utilizzo della Chiamata di Funzioni con Qwen3](../../../Module06)
6. [Integrazione Locale con Foundry](../../../Module06)
7. [Best Practices e Risoluzione dei Problemi](../../../Module06)
8. [Esempi Avanzati](../../../Module06)

## Cos'è la Chiamata di Funzioni?

La chiamata di funzioni è una capacità potente che consente ai Modelli Linguistici Ridotti (SLMs) di interagire con strumenti esterni, API e servizi. Invece di essere limitati ai dati di addestramento, gli SLM possono ora:

- **Connettersi a API esterne** (servizi meteo, database, motori di ricerca)
- **Eseguire funzioni specifiche** basate sulle richieste degli utenti
- **Recuperare informazioni in tempo reale** da varie fonti
- **Svolgere compiti computazionali** tramite strumenti specializzati
- **Collegare più operazioni** per flussi di lavoro complessi

Questa capacità trasforma gli SLM da generatori di testo statici in agenti AI dinamici in grado di svolgere compiti reali.

## Come Funziona la Chiamata di Funzioni

Il processo di chiamata di funzioni segue un flusso di lavoro sistematico:

### 1. Integrazione degli Strumenti
- **Strumenti Esterni**: Gli SLM possono connettersi a API meteo, database, servizi web e altri sistemi esterni
- **Definizioni delle Funzioni**: Ogni strumento è definito con parametri specifici, formati di input/output e descrizioni
- **Compatibilità API**: Gli strumenti sono integrati tramite interfacce standardizzate (REST API, SDK, ecc.)

### 2. Definizione delle Funzioni
Le funzioni sono definite con tre componenti chiave:
```json
{
  "name": "function_name",
  "description": "Clear description of what the function does",
  "parameters": {
    "parameter_name": {
      "description": "What this parameter represents",
      "type": "data_type",
      "default": "default_value"
    }
  }
}
```

### 3. Rilevamento dell'Intento
- **Elaborazione del Linguaggio Naturale**: L'SLM analizza l'input dell'utente per comprendere l'intento
- **Corrispondenza delle Funzioni**: Determina quale/i funzione/i sono necessarie per soddisfare la richiesta
- **Estrazione dei Parametri**: Identifica ed estrae i parametri richiesti dal messaggio dell'utente

### 4. Generazione di Output JSON
L'SLM genera un JSON strutturato contenente:
- Nome della funzione da chiamare
- Parametri richiesti con valori appropriati
- Contesto di esecuzione e metadati

### 5. Esecuzione Esterna
- **Validazione dei Parametri**: Garantisce che tutti i parametri richiesti siano presenti e correttamente formattati
- **Esecuzione della Funzione**: L'applicazione esegue la funzione specificata con i parametri forniti
- **Gestione degli Errori**: Gestisce fallimenti, timeout e risposte non valide

### 6. Integrazione della Risposta
- **Elaborazione dei Risultati**: L'output della funzione viene restituito all'SLM
- **Integrazione del Contesto**: L'SLM incorpora i risultati nella sua risposta
- **Comunicazione con l'Utente**: Presenta le informazioni in un formato naturale e conversazionale

## Scenari di Applicazione

### Recupero Dati
Convertire query in linguaggio naturale in chiamate API strutturate:
- **"Mostra i miei ordini recenti"** → Query al database con ID utente e filtri di data
- **"Che tempo fa a Tokyo?"** → Chiamata API meteo con parametro di località
- **"Trova email da John della settimana scorsa"** → Query al servizio email con mittente e filtri di data

### Esecuzione di Operazioni
Trasformare richieste degli utenti in chiamate a funzioni specifiche:
- **"Programma una riunione per domani alle 14:00"** → Integrazione con API del calendario
- **"Invia un messaggio al team"** → API della piattaforma di comunicazione
- **"Crea un backup dei miei file"** → Operazione sul file system

### Compiti Computazionali
Gestire operazioni matematiche o logiche complesse:
- **"Calcola l'interesse composto su $10,000 al 5% per 10 anni"** → Funzione di calcolo finanziario
- **"Analizza questo dataset per individuare tendenze"** → Strumenti di analisi statistica
- **"Ottimizza questo percorso per la consegna"** → Algoritmi di ottimizzazione del percorso

### Flussi di Lavoro per l'Elaborazione dei Dati
Collegare più chiamate di funzioni per operazioni complesse:
1. **Recuperare dati** da più fonti
2. **Analizzare e validare** le informazioni
3. **Trasformare** i dati nel formato richiesto
4. **Archiviare i risultati** nei sistemi appropriati
5. **Generare report** o visualizzazioni

### Integrazione UI/UX
Abilitare aggiornamenti dinamici dell'interfaccia:
- **"Mostra i dati di vendita sulla dashboard"** → Generazione e visualizzazione di grafici
- **"Aggiorna la mappa con nuove località"** → Integrazione di dati geospaziali
- **"Aggiorna la visualizzazione dell'inventario"** → Sincronizzazione dati in tempo reale

## Configurazione della Chiamata di Funzioni con Phi-4-mini e Ollama

Phi-4-mini di Microsoft supporta sia chiamate di funzioni singole che parallele tramite Ollama. Ecco come configurarlo:

### Prerequisiti
- Ollama versione 0.5.13 o superiore
- Modello Phi-4-mini (raccomandato: `phi4-mini:3.8b-fp16`)

### Passaggi di Installazione

#### 1. Installare ed Eseguire Phi-4-mini
```bash
# Download the model (if not already present)
ollama run phi4-mini:3.8b-fp16

# Verify the model is available
ollama list
```

#### 2. Creare un Template Personalizzato per ModelFile
A causa di limitazioni attuali nei template predefiniti di Ollama, è necessario creare un ModelFile personalizzato con il seguente template:

```modelfile
TEMPLATE """
{{- if .Messages }}
{{- if or .System .Tools }}<|system|>
{{ if .System }}{{ .System }}
{{- end }}
In addition to plain text responses, you can chose to call one or more of the provided functions.
Use the following rule to decide when to call a function:
* if the response can be generated from your internal knowledge (e.g., as in the case of queries like "What is the capital of Poland?"), do so
* if you need external information that can be obtained by calling one or more of the provided functions, generate a function calls
If you decide to call functions:
* prefix function calls with functools marker (no closing marker required)
* all function calls should be generated in a single JSON list formatted as functools[{"name": [function name], "arguments": [function arguments as JSON]}, ...]
* follow the provided JSON schema. Do not hallucinate arguments or values. Do to blindly copy values from the provided samples
* respect the argument type formatting. E.g., if the type if number and format is float, write value 7 as 7.0
* make sure you pick the right functions that match the user intent
Available functions as JSON spec:
{{- if .Tools }}
{{ .Tools }}
{{- end }}<|end|>
{{- end }}
{{- range .Messages }}
{{- if ne .Role "system" }}<|{{ .Role }}|>
{{- if and .Content (eq .Role "tools") }}
{"result": {{ .Content }}}
{{- else if .Content }}
{{ .Content }}
{{- else if .ToolCalls }}
functools[
{{- range .ToolCalls }}{{ "{" }}"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}{{ "}" }}
{{- end }}]
{{- end }}<|end|>
{{- end }}
{{- end }}<|assistant|>
{{ else }}
{{- if .System }}<|system|>
{{ .System }}<|end|>{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>{{ end }}<|assistant|>
{{ end }}{{ .Response }}{{ if .Response }}<|user|>{{ end }}
"""
```

#### 3. Creare il Modello Personalizzato
```bash
# Save the template above as 'Modelfile' and run:
ollama create phi4-mini-fc:3.8b-fp16 -f ./Modelfile
```

### Esempio di Chiamata di Funzione Singola

```python
import json
import requests

# Define the tool/function
tools = [
    {
        "name": "get_weather",
        "description": "Get current weather information for a location",
        "parameters": {
            "location": {
                "description": "The city or location name",
                "type": "str",
                "default": "New York"
            },
            "units": {
                "description": "Temperature units (celsius or fahrenheit)",
                "type": "str",
                "default": "celsius"
            }
        }
    }
]

# Create the message with system prompt including tools
messages = [
    {
        "role": "system",
        "content": "You are a helpful weather assistant",
        "tools": json.dumps(tools)
    },
    {
        "role": "user",
        "content": "What's the weather like in London today?"
    }
]

# Make request to Ollama API
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "phi4-mini-fc:3.8b-fp16",
        "messages": messages,
        "stream": False
    }
)

print(response.json())
```

### Esempio di Chiamata di Funzioni Parallele

```python
import json
import requests

# Define multiple tools for parallel execution
AGENT_TOOLS = {
    "booking_flight": {
        "name": "booking_flight",
        "description": "Book a flight ticket",
        "parameters": {
            "departure": {
                "description": "Departure airport code",
                "type": "str"
            },
            "destination": {
                "description": "Destination airport code", 
                "type": "str"
            },
            "outbound_date": {
                "description": "Departure date (YYYY-MM-DD)",
                "type": "str"
            },
            "return_date": {
                "description": "Return date (YYYY-MM-DD)",
                "type": "str"
            }
        }
    },
    "booking_hotel": {
        "name": "booking_hotel",
        "description": "Book a hotel room",
        "parameters": {
            "city": {
                "description": "City name for hotel booking",
                "type": "str"
            },
            "check_in_date": {
                "description": "Check-in date (YYYY-MM-DD)",
                "type": "str"
            },
            "check_out_date": {
                "description": "Check-out date (YYYY-MM-DD)",
                "type": "str"
            }
        }
    }
}

SYSTEM_PROMPT = """
You are my travel agent with some tools available.
"""

messages = [
    {
        "role": "system",
        "content": SYSTEM_PROMPT,
        "tools": json.dumps(AGENT_TOOLS)
    },
    {
        "role": "user", 
        "content": "I need to travel from London to New York from March 21 2025 to March 27 2025. Please book both flight and hotel."
    }
]

# The model will generate parallel function calls
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "phi4-mini-fc:3.8b-fp16",
        "messages": messages,
        "stream": False
    }
)

print(response.json())
```

## Utilizzo della Chiamata di Funzioni con Qwen3

Qwen3 offre capacità avanzate di chiamata di funzioni con prestazioni e flessibilità eccellenti. Ecco come implementarlo:

### Utilizzo del Framework Qwen-Agent

Qwen-Agent fornisce un framework di alto livello che semplifica l'implementazione della chiamata di funzioni:

#### Installazione
```bash
pip install -U "qwen-agent[gui,rag,code_interpreter,mcp]"
```

#### Configurazione Base

```python
import os
from qwen_agent.agents import Assistant

# Configure the LLM
llm_cfg = {
    'model': 'Qwen3-8B',
    # Option 1: Use Alibaba Model Studio
    'model_type': 'qwen_dashscope',
    'api_key': os.getenv('DASHSCOPE_API_KEY'),
    
    # Option 2: Use local deployment
    # 'model_server': 'http://localhost:8000/v1',
    # 'api_key': 'EMPTY',
    
    # Optional configuration for thinking mode
    'generate_cfg': {
        'thought_in_content': True,  # Include reasoning in response
    }
}

# Define tools using MCP (Model Context Protocol)
tools = [
    {
        'mcpServers': {
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            'fetch': {
                'command': 'uvx', 
                'args': ['mcp-server-fetch']
            }
        }
    },
    'code_interpreter',  # Built-in code execution tool
]

# Create the assistant
bot = Assistant(llm=llm_cfg, function_list=tools)

# Example usage
messages = [
    {
        'role': 'user', 
        'content': 'What time is it now? Also, fetch the latest news from https://example.com/news'
    }
]

# Generate response with function calling
for response in bot.run(messages=messages):
    print(response)
```

### Implementazione di Funzioni Personalizzate

È possibile definire funzioni personalizzate per Qwen3:

```python
import json
from qwen_agent.tools.base import BaseTool

class WeatherTool(BaseTool):
    description = 'Get weather information for a specific location'
    parameters = [
        {
            'name': 'location',
            'type': 'string', 
            'description': 'City or location name',
            'required': True
        },
        {
            'name': 'units',
            'type': 'string',
            'description': 'Temperature units (celsius or fahrenheit)',
            'required': False,
            'default': 'celsius'
        }
    ]
    
    def call(self, params: str, **kwargs) -> str:
        """Execute the weather lookup"""
        params_dict = json.loads(params)
        location = params_dict.get('location')
        units = params_dict.get('units', 'celsius')
        
        # Simulate weather API call
        weather_data = {
            'location': location,
            'temperature': '22°C' if units == 'celsius' else '72°F',
            'condition': 'Partly cloudy',
            'humidity': '65%'
        }
        
        return json.dumps(weather_data)

# Use the custom tool
tools = [WeatherTool()]
bot = Assistant(llm=llm_cfg, function_list=tools)

messages = [{'role': 'user', 'content': 'What\'s the weather in Tokyo?'}]
response = bot.run(messages=messages)
print(list(response)[-1])
```

### Funzionalità Avanzate di Qwen3

#### Controllo della Modalità di Pensiero
Qwen3 supporta il passaggio dinamico tra modalità di pensiero e non-pensiero:

```python
# Enable thinking mode for complex reasoning
messages = [
    {
        'role': 'user',
        'content': '/think Solve this complex math problem: If a train travels 120 km in 1.5 hours, and another train travels 200 km in 2.5 hours, which train is faster and by how much?'
    }
]

# Disable thinking mode for simple queries
messages = [
    {
        'role': 'user', 
        'content': '/no_think What is the capital of France?'
    }
]
```

#### Chiamata di Funzioni Multi-step
Qwen3 eccelle nel collegare più chiamate di funzioni:

```python
# Complex workflow example
messages = [
    {
        'role': 'user',
        'content': '''
        I need to prepare for a business meeting:
        1. Check my calendar for conflicts tomorrow
        2. Get weather forecast for the meeting location (San Francisco)
        3. Find recent news about the client company (TechCorp)
        4. Calculate travel time from my office to their headquarters
        '''
    }
]

# Qwen3 will automatically determine the sequence of function calls needed
```

## Integrazione Locale con Foundry

Foundry Local di Microsoft fornisce un'API compatibile con OpenAI per eseguire modelli localmente con maggiore privacy e prestazioni.

### Configurazione e Installazione

#### Windows
Scarica l'installer dalla [pagina delle release di Foundry Local](https://github.com/microsoft/Foundry-Local/releases) e segui le istruzioni di installazione.

#### macOS
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

#### Utilizzo Base

```python
import openai
from foundry_local import FoundryLocalManager

# Initialize with model alias
alias = "phi-3.5-mini"  # Or any supported model
manager = FoundryLocalManager(alias)

# Create OpenAI client pointing to local endpoint
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key
)

# Define functions for the model
functions = [
    {
        "name": "calculate_tax",
        "description": "Calculate tax amount based on income and rate",
        "parameters": {
            "type": "object",
            "properties": {
                "income": {
                    "type": "number",
                    "description": "Annual income amount"
                },
                "tax_rate": {
                    "type": "number", 
                    "description": "Tax rate as decimal (e.g., 0.25 for 25%)"
                }
            },
            "required": ["income", "tax_rate"]
        }
    }
]

# Make function calling request
response = client.chat.completions.create(
    model=manager.model_info.id,
    messages=[
        {
            "role": "user",
            "content": "Calculate the tax for someone earning $75,000 with a 22% tax rate"
        }
    ],
    functions=functions,
    function_call="auto"
)

print(response.choices[0].message.content)
```

### Funzionalità Avanzate di Foundry Local

#### Gestione dei Modelli
```bash
# List available models
foundry model list

# Download specific model
foundry model download phi-3.5-mini

# Run model interactively
foundry model run phi-3.5-mini

# Remove model from cache
foundry model remove phi-3.5-mini

# Delete all cached models
foundry model remove "*"
```

#### Ottimizzazione delle Prestazioni
Foundry Local seleziona automaticamente la variante del modello migliore per il tuo hardware:
- **GPU CUDA**: Scarica modelli ottimizzati per GPU
- **NPU Qualcomm**: Utilizza varianti accelerate da NPU
- **Solo CPU**: Seleziona modelli ottimizzati per CPU

## Best Practices e Risoluzione dei Problemi

### Best Practices per la Definizione delle Funzioni

#### 1. Nomi Chiari e Descrittivi
```python
# Good
{
    "name": "get_stock_price",
    "description": "Retrieve current stock price for a given symbol"
}

# Avoid
{
    "name": "get_data", 
    "description": "Gets data"
}
```

#### 2. Definizioni Complete dei Parametri
```python
{
    "name": "send_email",
    "description": "Send an email message to specified recipients",
    "parameters": {
        "to": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of recipient email addresses",
            "required": True
        },
        "subject": {
            "type": "string",
            "description": "Email subject line",
            "required": True
        },
        "body": {
            "type": "string", 
            "description": "Email message content",
            "required": True
        },
        "priority": {
            "type": "string",
            "enum": ["low", "normal", "high"],
            "description": "Email priority level",
            "default": "normal",
            "required": False
        }
    }
}
```

#### 3. Validazione degli Input e Gestione degli Errori
```python
def execute_function(function_name, parameters):
    try:
        # Validate required parameters
        if function_name == "send_email":
            if not parameters.get("to") or not parameters.get("subject"):
                return {"error": "Missing required parameters: to, subject"}
            
            # Validate email format
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            for email in parameters["to"]:
                if not re.match(email_pattern, email):
                    return {"error": f"Invalid email format: {email}"}
        
        # Execute function logic
        result = perform_actual_function(function_name, parameters)
        return {"success": True, "data": result}
        
    except Exception as e:
        return {"error": str(e)}
```

### Problemi Comuni e Soluzioni

#### Problema 1: Funzione Non Chiamata
**Sintomi**: Il modello risponde con testo invece di chiamare la funzione

**Soluzioni**:
1. **Controlla la descrizione della funzione**: Assicurati che corrisponda chiaramente all'intento dell'utente
2. **Verifica le definizioni dei parametri**: Assicurati che tutti i parametri richiesti siano correttamente definiti
3. **Rivedi il prompt di sistema**: Includi istruzioni chiare su quando utilizzare le funzioni
4. **Testa con richieste esplicite**: Prova "Per favore usa la funzione meteo per ottenere i dati di Londra"

#### Problema 2: Parametri Errati
**Sintomi**: Funzione chiamata con parametri sbagliati o mancanti

**Soluzioni**:
1. **Aggiungi esempi di parametri**: Includi valori di esempio nelle descrizioni dei parametri
2. **Usa vincoli enum**: Limita i valori dei parametri a opzioni specifiche quando possibile
3. **Implementa valori di fallback**: Fornisci valori predefiniti sensati per i parametri opzionali

```python
{
    "name": "book_restaurant",
    "parameters": {
        "cuisine": {
            "type": "string",
            "enum": ["italian", "chinese", "mexican", "american", "french"],
            "description": "Type of cuisine (example: 'italian' for Italian food)"
        },
        "party_size": {
            "type": "integer",
            "minimum": 1,
            "maximum": 20,
            "description": "Number of people (example: 4 for a family of four)"
        }
    }
}
```

#### Problema 3: Fallimenti nella Chiamata di Funzioni Parallele
**Sintomi**: Solo una funzione viene eseguita quando dovrebbero essere eseguite più funzioni

**Soluzioni**:
1. **Controlla il supporto del modello**: Assicurati che il tuo modello supporti la chiamata di funzioni parallele
2. **Aggiorna il prompt di sistema**: Includi "alcuni strumenti" o "più strumenti" nel messaggio di sistema
3. **Usa versioni appropriate del modello**: Phi-4-mini:3.8b-fp16 raccomandato per Ollama

#### Problema 4: Problemi di Template con Ollama
**Sintomi**: La chiamata di funzioni non funziona con la configurazione predefinita di Ollama

**Soluzioni**:
1. **Usa ModelFile personalizzato**: Applica il template corretto fornito in questo tutorial
2. **Aggiorna Ollama**: Assicurati di utilizzare la versione 0.5.13 o superiore
3. **Controlla la quantizzazione del modello**: Livelli di quantizzazione più alti (Q8_0, fp16) funzionano meglio rispetto a versioni fortemente quantizzate

### Ottimizzazione delle Prestazioni

#### 1. Progettazione Efficiente delle Funzioni
- **Mantieni le funzioni focalizzate**: Ogni funzione dovrebbe avere uno scopo chiaro e unico
- **Minimizza le dipendenze esterne**: Riduci le chiamate API e le richieste di rete ove possibile
- **Cache dei risultati**: Archivia i dati richiesti frequentemente per migliorare i tempi di risposta

#### 2. Operazioni Batch e Async
```python
import asyncio
import aiohttp

async def batch_function_calls(function_calls):
    """Execute multiple function calls concurrently"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for call in function_calls:
            if call["name"] == "fetch_url":
                task = fetch_url_async(session, call["parameters"]["url"])
                tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results

async def fetch_url_async(session, url):
    async with session.get(url) as response:
        return await response.text()
```

#### 3. Gestione delle Risorse
- **Pooling delle connessioni**: Riutilizza connessioni a database e API
- **Limitazione della velocità**: Implementa una corretta limitazione della velocità per le API esterne
- **Gestione dei timeout**: Imposta timeout ragionevoli per tutte le chiamate esterne

## Esempi Avanzati

### Sistema di Collaborazione Multi-Agente

```python
import json
from typing import List, Dict
from qwen_agent.agents import Assistant

class MultiAgentSystem:
    def __init__(self):
        # Research Agent
        self.research_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=[
                {'mcpServers': {'search': {'command': 'uvx', 'args': ['mcp-server-search']}}},
                {'mcpServers': {'fetch': {'command': 'uvx', 'args': ['mcp-server-fetch']}}}
            ]
        )
        
        # Analysis Agent
        self.analysis_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=['code_interpreter']
        )
        
        # Communication Agent
        self.comm_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=[self.create_email_tool(), self.create_slack_tool()]
        )
    
    def create_email_tool(self):
        """Custom email sending tool"""
        class EmailTool:
            name = "send_email"
            description = "Send email to specified recipients"
            parameters = {
                "to": {"type": "string", "description": "Recipient email"},
                "subject": {"type": "string", "description": "Email subject"},
                "body": {"type": "string", "description": "Email content"}
            }
            
            def call(self, params):
                # Implement actual email sending logic
                return f"Email sent successfully to {params['to']}"
        
        return EmailTool()
    
    def create_slack_tool(self):
        """Custom Slack messaging tool"""  
        class SlackTool:
            name = "send_slack"
            description = "Send message to Slack channel"
            parameters = {
                "channel": {"type": "string", "description": "Slack channel"},
                "message": {"type": "string", "description": "Message content"}
            }
            
            def call(self, params):
                # Implement actual Slack API call
                return f"Message sent to {params['channel']}"
        
        return SlackTool()
    
    async def process_complex_request(self, user_request: str):
        """Process complex multi-step requests using multiple agents"""
        
        # Step 1: Research phase
        research_prompt = f"Research the following topic and gather relevant information: {user_request}"
        research_results = []
        for response in self.research_agent.run([{'role': 'user', 'content': research_prompt}]):
            research_results.append(response)
        
        # Step 2: Analysis phase
        analysis_prompt = f"Analyze the following research data and provide insights: {research_results[-1]}"
        analysis_results = []
        for response in self.analysis_agent.run([{'role': 'user', 'content': analysis_prompt}]):
            analysis_results.append(response)
        
        # Step 3: Communication phase
        comm_prompt = f"Create a summary report and send it via email: {analysis_results[-1]}"
        comm_results = []
        for response in self.comm_agent.run([{'role': 'user', 'content': comm_prompt}]):
            comm_results.append(response)
        
        return {
            'research': research_results[-1],
            'analysis': analysis_results[-1], 
            'communication': comm_results[-1]
        }

# Usage example
async def main():
    system = MultiAgentSystem()
    
    request = """
    Analyze the impact of remote work on productivity in tech companies. 
    Research recent studies, analyze the data, and send a summary to our team.
    """
    
    results = await system.process_complex_request(request)
    print("Multi-agent processing complete:", results)

# Run the example
# asyncio.run(main())
```

### Sistema di Selezione Dinamica degli Strumenti

```python
class DynamicToolSelector:
    def __init__(self):
        self.available_tools = {
            'weather': {
                'description': 'Get weather information',
                'domains': ['weather', 'temperature', 'forecast', 'climate'],
                'function': self.get_weather
            },
            'calculator': {
                'description': 'Perform mathematical calculations',
                'domains': ['math', 'calculate', 'compute', 'arithmetic'],
                'function': self.calculate
            },
            'web_search': {
                'description': 'Search the internet for information',
                'domains': ['search', 'find', 'lookup', 'research'],
                'function': self.web_search
            },
            'file_manager': {
                'description': 'Manage files and directories',
                'domains': ['file', 'directory', 'save', 'load', 'delete'],
                'function': self.manage_files
            }
        }
    
    def analyze_intent(self, user_input: str) -> List[str]:
        """Analyze user input to determine which tools might be needed"""
        user_words = user_input.lower().split()
        relevant_tools = []
        
        for tool_name, tool_info in self.available_tools.items():
            for domain in tool_info['domains']:
                if domain in user_words:
                    relevant_tools.append(tool_name)
                    break
        
        return relevant_tools
    
    def get_tool_definitions(self, tool_names: List[str]) -> List[Dict]:
        """Generate function definitions for selected tools"""
        definitions = []
        
        for tool_name in tool_names:
            if tool_name == 'weather':
                definitions.append({
                    'name': 'get_weather',
                    'description': 'Get current weather information',
                    'parameters': {
                        'location': {'type': 'string', 'description': 'City or location name'},
                        'units': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'default': 'celsius'}
                    }
                })
            elif tool_name == 'calculator':
                definitions.append({
                    'name': 'calculate',
                    'description': 'Perform mathematical calculations',
                    'parameters': {
                        'expression': {'type': 'string', 'description': 'Mathematical expression to evaluate'},
                        'precision': {'type': 'integer', 'default': 2, 'description': 'Decimal places for result'}
                    }
                })
            # Add more tool definitions as needed
        
        return definitions
    
    def get_weather(self, location: str, units: str = 'celsius') -> Dict:
        """Mock weather function"""
        return {
            'location': location,
            'temperature': '22°C' if units == 'celsius' else '72°F',
            'condition': 'Sunny',
            'humidity': '60%'
        }
    
    def calculate(self, expression: str, precision: int = 2) -> Dict:
        """Safe mathematical calculation"""
        try:
            # Simple evaluation for demo - in production, use a proper math parser
            import math
            allowed_names = {
                k: v for k, v in math.__dict__.items() if not k.startswith("__")
            }
            allowed_names.update({"abs": abs, "round": round})
            
            result = eval(expression, {"__builtins__": {}}, allowed_names)
            return {
                'expression': expression,
                'result': round(float(result), precision),
                'success': True
            }
        except Exception as e:
            return {
                'expression': expression,
                'error': str(e),
                'success': False
            }
    
    def web_search(self, query: str, max_results: int = 5) -> Dict:
        """Mock web search function"""
        return {
            'query': query,
            'results': [
                {'title': f'Result {i+1} for {query}', 'url': f'https://example{i+1}.com'}
                for i in range(max_results)
            ]
        }
    
    def manage_files(self, action: str, file_path: str, content: str = None) -> Dict:
        """Mock file management function"""
        return {
            'action': action,
            'file_path': file_path,
            'success': True,
            'message': f'Successfully {action}ed file: {file_path}'
        }

# Usage example
def smart_assistant_with_dynamic_tools():
    selector = DynamicToolSelector()
    
    user_requests = [
        "What's the weather like in New York and calculate 15% tip on $50?",
        "Search for recent AI developments and save the results to a file",
        "Calculate the area of a circle with radius 10 and check weather in Tokyo"
    ]
    
    for request in user_requests:
        print(f"\nUser Request: {request}")
        
        # Analyze which tools might be needed
        relevant_tools = selector.analyze_intent(request)
        print(f"Relevant Tools: {relevant_tools}")
        
        # Get function definitions for the LLM
        tool_definitions = selector.get_tool_definitions(relevant_tools)
        print(f"Tool Definitions: {len(tool_definitions)} functions available")
        
        # In a real implementation, you would pass these to your LLM
        # The LLM would then decide which functions to call and with what parameters

### Enterprise Integration Example

```
Sure, please provide the markdown file you'd like me to translate.
"""Esegui una funzione con gestione completa degli errori e registrazione"""
start_time = datetime.now()

try:
    # Verifica che la funzione esista
    if function_name not in self.functions:
        return FunctionResult(
            success=False,
            error=f"Funzione '{function_name}' non trovata",
            timestamp=start_time
        )
    
    # Controlla i limiti di utilizzo
    if not self._check_rate_limit(function_name):
        return FunctionResult(
            success=False,
            error=f"Limite di utilizzo superato per la funzione '{function_name}'",
            timestamp=start_time
        )
    
    # Valida i parametri
    validation_result = self._validate_parameters(function_name, parameters)
    if not validation_result.success:
        return validation_result
    
    # Esegui la funzione
    func_info = self.functions[function_name]
    handler = func_info['handler']
    
    if asyncio.iscoroutinefunction(handler):
        result_data = await handler(**parameters)
    else:
        result_data = handler(**parameters)
    
    execution_time = (datetime.now() - start_time).total_seconds()
    
    result = FunctionResult(
        success=True,
        data=result_data,
        execution_time=execution_time,
        timestamp=start_time
    )
    
    # Registra l'esecuzione riuscita
    self._log_function_call(function_name, parameters, result)
    
    return result
    
except Exception as e:
    execution_time = (datetime.now() - start_time).total_seconds()
    result = FunctionResult(
        success=False,
        error=str(e),
        execution_time=execution_time,
        timestamp=start_time
    )
    
    # Registra l'esecuzione fallita
    self._log_function_call(function_name, parameters, result)
    
    return result

def _check_rate_limit(self, function_name: str) -> bool:
    """Controlla se la chiamata alla funzione rientra nei limiti di utilizzo"""
    func_info = self.functions[function_name]
    now = datetime.now()
    
    # Resetta il contatore se è passato un minuto
    if (now - func_info['last_reset']).seconds >= 60:
        func_info['call_count'] = 0
        func_info['last_reset'] = now
    
    # Controlla se il limite è stato superato
    if func_info['call_count'] >= func_info['rate_limit']:
        return False
    
    func_info['call_count'] += 1
    return True

def _validate_parameters(self, function_name: str, parameters: Dict) -> FunctionResult:
    """Valida i parametri della funzione"""
    func_params = self.functions[function_name]['parameters']
    
    # Controlla i parametri richiesti
    for param_name, param_info in func_params.items():
        if param_info.get('required', False) and param_name not in parameters:
            return FunctionResult(
                success=False,
                error=f"Parametro richiesto mancante: {param_name}"
            )
    
    # Valida i tipi e i vincoli dei parametri
    for param_name, value in parameters.items():
        if param_name in func_params:
            param_info = func_params[param_name]
            
            # Validazione del tipo
            expected_type = param_info.get('type')
            if expected_type == 'string' and not isinstance(value, str):
                return FunctionResult(
                    success=False,
                    error=f"Il parametro '{param_name}' deve essere una stringa"
                )
            elif expected_type == 'number' and not isinstance(value, (int, float)):
                return FunctionResult(
                    success=False,
                    error=f"Il parametro '{param_name}' deve essere un numero"
                )
            elif expected_type == 'boolean' and not isinstance(value, bool):
                return FunctionResult(
                    success=False,
                    error=f"Il parametro '{param_name}' deve essere un booleano"
                )
            
            # Validazione enum
            if 'enum' in param_info and value not in param_info['enum']:
                return FunctionResult(
                    success=False,
                    error=f"Il parametro '{param_name}' deve essere uno dei seguenti: {param_info['enum']}"
                )
    
    return FunctionResult(success=True)

def _log_function_call(self, function_name: str, parameters: Dict, result: FunctionResult):
    """Registra la chiamata alla funzione per scopi di audit"""
    log_entry = {
        'timestamp': result.timestamp.isoformat(),
        'function_name': function_name,
        'parameters': parameters,
        'success': result.success,
        'execution_time': result.execution_time,
        'error': result.error if not result.success else None
    }
    
    self.audit_log.append(log_entry)
    
    # Opzionalmente scrivi su un sistema di registrazione esterno
    if self.config.get('enable_external_logging', False):
        self._write_to_external_log(log_entry)

def _write_to_external_log(self, log_entry: Dict):
    """Scrivi la voce di registro su un sistema di registrazione esterno"""
    # L'implementazione dipenderebbe dalla tua infrastruttura di registrazione
    # Ad esempio, invia a ELK stack, CloudWatch, ecc.
    pass

# Implementazioni delle funzioni aziendali
async def _get_customer_info(self, customer_id: str, include_history: bool = False) -> Dict:
    """Recupera informazioni sul cliente dal sistema CRM"""
    # Simula chiamata al database/API
    await asyncio.sleep(0.1)  # Simula ritardo di rete
    
    customer_data = {
        'customer_id': customer_id,
        'name': 'John Doe',
        'email': 'john.doe@example.com',
        'phone': '+1-555-0123',
        'status': 'active',
        'tier': 'premium'
    }
    
    if include_history:
        customer_data['purchase_history'] = [
            {'date': '2024-01-15', 'product': 'Product A', 'amount': 1500},
            {'date': '2024-03-22', 'product': 'Product B', 'amount': 2300}
        ]
    
    return customer_data

async def _create_sales_opportunity(self, customer_id: str, product_id: str, 
                                    estimated_value: float, expected_close_date: str) -> Dict:
    """Crea una nuova opportunità di vendita"""
    # Simula chiamata API CRM
    await asyncio.sleep(0.2)
    
    opportunity_id = f"OPP-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    
    return {
        'opportunity_id': opportunity_id,
        'customer_id': customer_id,
        'product_id': product_id,
        'estimated_value': estimated_value,
        'expected_close_date': expected_close_date,
        'status': 'open',
        'created_date': datetime.now().isoformat()
    }

async def _generate_sales_report(self, period: str, region: str = None, 
                                 product_category: str = None) -> Dict:
    """Genera un report completo sulle vendite"""
    # Simula aggregazione dei dati
    await asyncio.sleep(0.5)
    
    return {
        'report_id': f"RPT-{datetime.now().strftime('%Y%m%d%H%M%S')}",
        'period': period,
        'region': region,
        'product_category': product_category,
        'total_sales': 125000.00,
        'total_opportunities': 45,
        'conversion_rate': 0.67,
        'top_products': [
            {'product_id': 'PROD-001', 'sales': 45000},
            {'product_id': 'PROD-002', 'sales': 32000}
        ],
        'generated_at': datetime.now().isoformat()
    }

async def _send_notification(self, recipients: List[str], message: str, 
                             priority: str = 'medium', channel: str = 'email') -> Dict:
    """Invia una notifica tramite il canale specificato"""
    # Simula chiamata al servizio di notifica
    await asyncio.sleep(0.1)
    
    notification_id = f"NOTIF-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    
    return {
        'notification_id': notification_id,
        'recipients': recipients,
        'channel': channel,
        'priority': priority,
        'status': 'sent',
        'sent_at': datetime.now().isoformat()
    }

def get_function_definitions(self) -> List[Dict]:
    """Ottieni definizioni di funzioni compatibili con OpenAI per tutte le funzioni registrate"""
    definitions = []
    
    for func_name, func_info in self.functions.items():
        definition = {
            'name': func_name,
            'description': func_info['description'],
            'parameters': {
                'type': 'object',
                'properties': {},
                'required': []
            }
        }
        
        for param_name, param_info in func_info['parameters'].items():
            definition['parameters']['properties'][param_name] = {
                'type': param_info['type'],
                'description': param_info.get('description', '')
            }
            
            if 'enum' in param_info:
                definition['parameters']['properties'][param_name]['enum'] = param_info['enum']
            
            if 'default' in param_info:
                definition['parameters']['properties'][param_name]['default'] = param_info['default']
            
            if param_info.get('required', False):
                definition['parameters']['required'].append(param_name)
        
        definitions.append(definition)
    
    return definitions

# Esempio di utilizzo per integrazione aziendale
async def enterprise_demo():
    """Dimostra le capacità dell'agente AI aziendale"""
    
    config = {
        'enable_external_logging': True,
        'max_concurrent_functions': 10,
        'default_timeout': 30
    }
    
    agent = EnterpriseAIAgent(config)
    
    # Esempio 1: Elaborazione delle richieste dei clienti
    print("=== Elaborazione delle richieste dei clienti ===")
    
    # Ottieni informazioni sul cliente
    result = await agent.execute_function(
        'get_customer_info',
        {'customer_id': 'CUST-12345', 'include_history': True}
    )
    
    if result.success:
        print(f"Informazioni sul cliente recuperate: {result.data['name']}")
        print(f"Tempo di esecuzione: {result.execution_time:.3f}s")
    
    # Esempio 2: Creazione di opportunità di vendita
    print("\n=== Creazione di opportunità di vendita ===")
    
    result = await agent.execute_function(
        'create_sales_opportunity',
        {
            'customer_id': 'CUST-12345',
            'product_id': 'PROD-001',
            'estimated_value': 15000.0,
            'expected_close_date': '2025-09-30'
        }
    )
    
    if result.success:
        print(f"Opportunità creata: {result.data['opportunity_id']}")
    
    # Esempio 3: Operazioni batch
    print("\n=== Operazioni batch ===")
    
    tasks = [
        agent.execute_function('generate_sales_report', {'period': 'monthly'}),
        agent.execute_function('send_notification', {
            'recipients': ['manager@company.com'],
            'message': 'Nuova opportunità creata',
            'priority': 'high',
            'channel': 'email'
        })
    ]
    
    results = await asyncio.gather(*tasks)
    
    for i, result in enumerate(results):
        if result.success:
            print(f"Attività {i+1} completata con successo")
        else:
            print(f"Attività {i+1} fallita: {result.error}")
    
    # Mostra registro di audit
    print(f"\n=== Registro di audit ({len(agent.audit_log)} voci) ===")
    for entry in agent.audit_log[-3:]:  # Mostra le ultime 3 voci
        print(f"{entry['timestamp']}: {entry['function_name']} - {'SUCCESSO' if entry['success'] else 'FALLITO'}")

# Esegui la demo aziendale
# asyncio.run(enterprise_demo())
- **Modelli Phi-4**: [Collezione Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Documentazione Qwen3**: [Documentazione ufficiale Qwen](https://qwen.readthedocs.io/)
- **Ollama**: [Sito ufficiale](https://ollama.com/)
- **Foundry Local**: [Repository GitHub](https://github.com/microsoft/Foundry-Local)
- **Best Practices per Function Calling**: [Guida Hugging Face](https://huggingface.co/docs/hugs/en/guides/function-calling)

Ricorda che il function calling è un campo in continua evoluzione, e rimanere aggiornati sugli ultimi sviluppi nei framework e modelli scelti ti aiuterà a costruire agenti AI più efficaci.


## ➡️ Cosa fare dopo

- [03: Integrazione del Model Context Protocol (MCP)](./03.IntroduceMCP.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.