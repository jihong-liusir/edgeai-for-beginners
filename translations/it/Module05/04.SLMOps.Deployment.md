<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T23:56:19+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "it"
}
-->
# Sezione 4: Implementazione di un Modello Pronto per la Produzione

## Panoramica

Questo tutorial completo ti guider√† attraverso l'intero processo di distribuzione di modelli quantizzati ottimizzati utilizzando Foundry Local. Tratteremo la conversione del modello, l'ottimizzazione della quantizzazione e la configurazione della distribuzione dall'inizio alla fine.

## Prerequisiti

Prima di iniziare, assicurati di avere quanto segue:

- ‚úÖ Un modello onnx ottimizzato pronto per la distribuzione
- ‚úÖ Computer Windows o Mac
- ‚úÖ Python 3.10 o superiore
- ‚úÖ Almeno 8GB di RAM disponibile
- ‚úÖ Foundry Local installato sul tuo sistema

## Parte 1: Configurazione dell'Ambiente

### Installazione degli Strumenti Necessari

Apri il terminale (Prompt dei Comandi su Windows, Terminale su Mac) ed esegui i seguenti comandi in sequenza:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Nota Importante**: Avrai bisogno anche della versione 3.31 o successiva di CMake, che pu√≤ essere scaricata da [cmake.org](https://cmake.org/download/).

## Parte 2: Conversione e Quantizzazione del Modello

### Scelta del Formato Giusto

Per modelli di linguaggio di piccole dimensioni ottimizzati, consigliamo di utilizzare il formato **ONNX** perch√© offre:

- üöÄ Migliore ottimizzazione delle prestazioni
- üîß Distribuzione indipendente dall'hardware
- üè≠ Capacit√† pronte per la produzione
- üì± Compatibilit√† cross-platform

### Metodo 1: Conversione con un Solo Comando (Consigliato)

Utilizza il seguente comando per convertire direttamente il tuo modello ottimizzato:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Spiegazione dei Parametri:**
- `--model_name_or_path`: Percorso del tuo modello ottimizzato
- `--device cpu`: Utilizza la CPU per l'ottimizzazione
- `--precision int4`: Utilizza la quantizzazione INT4 (riduzione della dimensione di circa il 75%)
- `--output_path`: Percorso di output per il modello convertito

### Metodo 2: Approccio con File di Configurazione (Utenti Avanzati)

Crea un file di configurazione chiamato `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Quindi esegui:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Confronto delle Opzioni di Quantizzazione

| Precisione | Dimensione File | Velocit√† di Inferenza | Qualit√† del Modello | Utilizzo Consigliato |
|------------|-----------------|-----------------------|---------------------|-----------------------|
| FP16       | Baseline √ó 0.5 | Veloce               | Migliore           | Hardware di fascia alta |
| INT8       | Baseline √ó 0.25 | Molto Veloce         | Buona              | Scelta bilanciata |
| INT4       | Baseline √ó 0.125 | Velocit√† Massima     | Accettabile        | Risorse limitate |

üí° **Raccomandazione**: Inizia con la quantizzazione INT4 per la tua prima distribuzione. Se la qualit√† non √® soddisfacente, prova INT8 o FP16.

## Parte 3: Configurazione della Distribuzione con Foundry Local

### Creazione della Configurazione del Modello

Naviga nella directory dei modelli di Foundry Local:

```bash
foundry cache cd ./models/
```

Crea la struttura della directory del tuo modello:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Crea il file di configurazione `inference_model.json` nella directory del tuo modello:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configurazioni Template Specifiche per il Modello

#### Per Modelli della Serie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Parte 4: Test e Ottimizzazione del Modello

### Verifica dell'Installazione del Modello

Controlla se Foundry Local riesce a riconoscere il tuo modello:

```bash
foundry cache ls
```

Dovresti vedere `your-finetuned-model-int4` nell'elenco.

### Avvio del Test del Modello

```bash
foundry model run your-finetuned-model-int4
```

### Benchmark delle Prestazioni

Monitora i principali parametri durante il test:

1. **Tempo di Risposta**: Misura il tempo medio per risposta
2. **Utilizzo della Memoria**: Monitora il consumo di RAM
3. **Utilizzo della CPU**: Controlla il carico del processore
4. **Qualit√† dell'Output**: Valuta la rilevanza e la coerenza delle risposte

### Checklist di Validazione della Qualit√†

- ‚úÖ Il modello risponde correttamente alle query del dominio ottimizzato
- ‚úÖ Il formato delle risposte corrisponde alla struttura di output prevista
- ‚úÖ Nessuna perdita di memoria durante un utilizzo prolungato
- ‚úÖ Prestazioni consistenti con diverse lunghezze di input
- ‚úÖ Gestione corretta di casi limite e input non validi

## Riepilogo

Congratulazioni! Hai completato con successo:

- ‚úÖ Conversione del formato del modello ottimizzato
- ‚úÖ Ottimizzazione della quantizzazione del modello
- ‚úÖ Configurazione della distribuzione con Foundry Local
- ‚úÖ Ottimizzazione delle prestazioni e risoluzione dei problemi

---

**Disclaimer**:  
Questo documento √® stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un esperto umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.