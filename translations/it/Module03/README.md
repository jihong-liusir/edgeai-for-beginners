<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-17T23:57:47+00:00",
  "source_file": "Module03/README.md",
  "language_code": "it"
}
-->
# Capitolo 03: Distribuzione di Small Language Models (SLMs)

Questo capitolo completo esplora l'intero ciclo di vita della distribuzione di Small Language Models (SLMs), coprendo le basi teoriche, le strategie di implementazione pratica e soluzioni containerizzate pronte per la produzione. Il capitolo è strutturato in tre sezioni progressive che guidano i lettori dai concetti fondamentali agli scenari di distribuzione avanzati.

## Struttura del Capitolo e Percorso di Apprendimento

### **[Sezione 1: Apprendimento Avanzato degli SLM - Fondamenti e Ottimizzazione](./01.SLMAdvancedLearning.md)**
La sezione iniziale stabilisce le basi teoriche per comprendere gli Small Language Models e la loro importanza strategica nelle distribuzioni AI edge. Questa sezione copre:

- **Framework di Classificazione dei Parametri**: Esplorazione dettagliata delle categorie di SLM, dai Micro SLMs (100M-1.4B parametri) ai Medium SLMs (14B-30B parametri), con particolare attenzione a modelli come Phi-4-mini-3.8B, serie Qwen3 e Google Gemma3, inclusa l'analisi dei requisiti hardware e dell'impronta di memoria per ogni livello di modello
- **Tecniche di Ottimizzazione Avanzate**: Copertura completa dei metodi di quantizzazione utilizzando i framework Llama.cpp, Microsoft Olive e Apple MLX, inclusa la quantizzazione BitNET a 1 bit all'avanguardia con esempi pratici di codice che mostrano pipeline di quantizzazione e risultati di benchmarking
- **Strategie di Acquisizione dei Modelli**: Analisi approfondita dell'ecosistema Hugging Face e del catalogo di modelli Azure AI Foundry per la distribuzione di SLM di livello enterprise, con esempi di codice per il download programmatico dei modelli, la validazione e la conversione di formato
- **API per Sviluppatori**: Esempi di codice in Python, C++ e C# che mostrano come caricare modelli, eseguire inferenze e integrare con framework popolari come PyTorch, TensorFlow e ONNX Runtime

Questa sezione fondamentale enfatizza l'equilibrio tra efficienza operativa, flessibilità di distribuzione e convenienza economica che rende gli SLM ideali per scenari di edge computing, con esempi pratici di codice che gli sviluppatori possono implementare direttamente nei loro progetti.

### **[Sezione 2: Distribuzione in Ambiente Locale - Soluzioni Privacy-First](./02.DeployingSLMinLocalEnv.md)**
La seconda sezione passa dalla teoria all'implementazione pratica, concentrandosi su strategie di distribuzione locale che danno priorità alla sovranità dei dati e all'indipendenza operativa. Le aree chiave includono:

- **Piattaforma Universale Ollama**: Esplorazione completa della distribuzione cross-platform con enfasi sui flussi di lavoro orientati agli sviluppatori, gestione del ciclo di vita dei modelli e personalizzazione tramite Modelfiles, inclusi esempi completi di integrazione REST API e script di automazione CLI
- **Microsoft Foundry Local**: Soluzioni di distribuzione di livello enterprise con ottimizzazione basata su ONNX, integrazione Windows ML e funzionalità di sicurezza complete, con esempi di codice in C# e Python per l'integrazione nativa delle applicazioni
- **Analisi Comparativa**: Confronto dettagliato dei framework che copre l'architettura tecnica, le caratteristiche di prestazione e le linee guida per l'ottimizzazione dei casi d'uso, con codice di benchmark per valutare la velocità di inferenza e l'uso della memoria su diversi hardware
- **Integrazione API**: Applicazioni di esempio che mostrano come costruire servizi web, applicazioni di chat e pipeline di elaborazione dati utilizzando distribuzioni SLM locali, con esempi di codice in Node.js, Python Flask/FastAPI e ASP.NET Core
- **Framework di Test**: Approcci di test automatizzati per la garanzia di qualità dei modelli, inclusi esempi di test unitari e di integrazione per implementazioni SLM

Questa sezione fornisce una guida pratica per le organizzazioni che cercano di implementare soluzioni AI che preservano la privacy, mantenendo il pieno controllo sul loro ambiente di distribuzione, con esempi di codice pronti all'uso che gli sviluppatori possono adattare alle loro esigenze specifiche.

### **[Sezione 3: Distribuzione Containerizzata nel Cloud - Soluzioni su Scala di Produzione](./03.DeployingSLMinCloud.md)**
La sezione finale culmina in strategie avanzate di distribuzione containerizzata, con il modello Phi-4-mini-instruct di Microsoft come caso di studio principale. Questa sezione copre:

- **Distribuzione vLLM**: Ottimizzazione delle inferenze ad alte prestazioni con API compatibili OpenAI, accelerazione GPU avanzata e configurazione di livello produzione, inclusi Dockerfile completi, manifest Kubernetes e parametri di tuning delle prestazioni
- **Orchestrazione dei Container Ollama**: Flussi di lavoro di distribuzione semplificati con Docker Compose, varianti di ottimizzazione dei modelli e integrazione con interfaccia web, con esempi di pipeline CI/CD per distribuzione e test automatizzati
- **Implementazione ONNX Runtime**: Distribuzione ottimizzata per edge con conversione completa dei modelli, strategie di quantizzazione e compatibilità cross-platform, inclusi esempi dettagliati di codice per ottimizzazione e distribuzione dei modelli
- **Monitoraggio e Osservabilità**: Implementazione di dashboard Prometheus/Grafana con metriche personalizzate per il monitoraggio delle prestazioni degli SLM, incluse configurazioni di allerta e aggregazione dei log
- **Bilanciamento del Carico e Scalabilità**: Esempi pratici di strategie di scalabilità orizzontale e verticale con configurazioni di autoscaling basate sull'utilizzo di CPU/GPU e sui pattern di richieste
- **Rafforzamento della Sicurezza**: Migliori pratiche di sicurezza per i container, inclusa la riduzione dei privilegi, politiche di rete e gestione dei segreti per chiavi API e credenziali di accesso ai modelli

Ogni approccio di distribuzione è presentato con esempi completi di configurazione, procedure di test, checklist di prontezza per la produzione e modelli di infrastruttura come codice che gli sviluppatori possono applicare direttamente ai loro flussi di lavoro di distribuzione.

## Risultati Chiave di Apprendimento

Completando questo capitolo, i lettori padroneggeranno:

1. **Selezione Strategica dei Modelli**: Comprendere i limiti dei parametri e selezionare gli SLM appropriati in base ai vincoli di risorse e ai requisiti di prestazione
2. **Padronanza dell'Ottimizzazione**: Implementare tecniche di quantizzazione avanzate su diversi framework per raggiungere un equilibrio ottimale tra prestazioni ed efficienza
3. **Flessibilità di Distribuzione**: Scegliere tra soluzioni locali orientate alla privacy e distribuzioni containerizzate scalabili in base alle esigenze organizzative
4. **Prontezza per la Produzione**: Configurare sistemi di monitoraggio, sicurezza e scalabilità per distribuzioni SLM di livello enterprise

## Focus Pratico e Applicazioni Reali

Il capitolo mantiene un forte orientamento pratico, includendo:

- **Esempi Pratici**: File di configurazione completi, procedure di test API e script di distribuzione
- **Benchmark delle Prestazioni**: Confronti dettagliati di velocità di inferenza, uso della memoria e requisiti di risorse
- **Considerazioni sulla Sicurezza**: Pratiche di sicurezza di livello enterprise, framework di conformità e strategie di protezione dei dati
- **Migliori Pratiche**: Linee guida comprovate per monitoraggio, scalabilità e manutenzione

## Prospettiva Orientata al Futuro

Il capitolo si conclude con approfondimenti sulle tendenze emergenti, tra cui:

- Architetture di modelli avanzate con rapporti di efficienza migliorati
- Integrazione hardware più profonda con acceleratori AI specializzati
- Evoluzione dell'ecosistema verso standardizzazione e interoperabilità
- Modelli di adozione aziendale guidati da privacy e requisiti di conformità

Questo approccio completo garantisce che i lettori siano ben equipaggiati per affrontare sia le sfide attuali della distribuzione degli SLM sia gli sviluppi tecnologici futuri, prendendo decisioni informate che si allineano ai requisiti e ai vincoli specifici della loro organizzazione.

Il capitolo funge sia da guida pratica per l'implementazione immediata sia da risorsa strategica per la pianificazione a lungo termine della distribuzione AI, enfatizzando l'equilibrio critico tra capacità, efficienza ed eccellenza operativa che definisce le distribuzioni SLM di successo.

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.