<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T00:07:14+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "it"
}
-->
# Distribuzione Cloud Containerizzata - Soluzioni su Scala di Produzione

Questo tutorial completo copre tre approcci principali per distribuire il modello Phi-4-mini-instruct di Microsoft in ambienti containerizzati: vLLM, Ollama e SLM Engine con ONNX Runtime. Questo modello da 3,8 miliardi di parametri rappresenta una scelta ottimale per compiti di ragionamento, mantenendo al contempo efficienza per la distribuzione su dispositivi edge.

## Indice

1. [Introduzione alla Distribuzione Containerizzata di Phi-4-mini](../../../Module03)
2. [Obiettivi di Apprendimento](../../../Module03)
3. [Comprendere la Classificazione di Phi-4-mini](../../../Module03)
4. [Distribuzione Containerizzata con vLLM](../../../Module03)
5. [Distribuzione Containerizzata con Ollama](../../../Module03)
6. [SLM Engine con ONNX Runtime](../../../Module03)
7. [Framework di Confronto](../../../Module03)
8. [Best Practices](../../../Module03)

## Introduzione alla Distribuzione Containerizzata di Phi-4-mini

I modelli linguistici di piccole dimensioni (SLM) rappresentano un avanzamento cruciale nell'EdgeAI, abilitando capacità sofisticate di elaborazione del linguaggio naturale su dispositivi con risorse limitate. Questo tutorial si concentra sulle strategie di distribuzione containerizzata per Phi-4-mini-instruct di Microsoft, un modello di ragionamento all'avanguardia che bilancia capacità e efficienza.

### Modello in Evidenza: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 miliardi di parametri)**: L'ultimo modello leggero di Microsoft, ottimizzato per ambienti con memoria e capacità di calcolo limitate, con capacità eccezionali in:
- **Ragionamento matematico e calcoli complessi**
- **Generazione, debug e analisi di codice**
- **Risoluzione di problemi logici e ragionamento passo-passo**
- **Applicazioni educative che richiedono spiegazioni dettagliate**
- **Chiamata di funzioni e integrazione di strumenti**

Parte della categoria "Small SLMs" (1,5B - 13,9B parametri), Phi-4-mini raggiunge un equilibrio ottimale tra capacità di ragionamento ed efficienza delle risorse.

### Vantaggi della Distribuzione Containerizzata di Phi-4-mini

- **Efficienza Operativa**: Inferenza rapida per compiti di ragionamento con requisiti computazionali ridotti
- **Flessibilità di Distribuzione**: Capacità AI on-device con maggiore privacy grazie all'elaborazione locale
- **Convenienza Economica**: Costi operativi ridotti rispetto ai modelli più grandi, mantenendo la qualità
- **Isolamento**: Separazione pulita tra istanze del modello e ambienti di esecuzione sicuri
- **Scalabilità**: Scalabilità orizzontale semplice per aumentare la capacità di ragionamento

## Obiettivi di Apprendimento

Alla fine di questo tutorial, sarai in grado di:

- Distribuire e ottimizzare Phi-4-mini-instruct in vari ambienti containerizzati
- Implementare strategie avanzate di quantizzazione e compressione per diversi scenari di distribuzione
- Configurare orchestrazione containerizzata pronta per la produzione per carichi di lavoro di ragionamento
- Valutare e selezionare framework di distribuzione appropriati in base ai requisiti specifici del caso d'uso
- Applicare best practices di sicurezza, monitoraggio e scalabilità per distribuzioni containerizzate di SLM

## Comprendere la Classificazione di Phi-4-mini

### Specifiche del Modello

**Dettagli Tecnici:**
- **Parametri**: 3,8 miliardi (categoria Small SLM)
- **Architettura**: Transformer decoder-only denso con grouped-query attention
- **Lunghezza del Contesto**: 128K token (32K raccomandati per prestazioni ottimali)
- **Vocabolario**: 200K token con supporto multilingue
- **Dati di Addestramento**: 5T token di contenuti di alta qualità e ricchi di ragionamento

### Requisiti di Risorse

| Tipo di Distribuzione | RAM Minima | RAM Raccomandata | VRAM (GPU) | Storage | Casi d'Uso Tipici |
|-----------------------|------------|------------------|------------|---------|-------------------|
| **Sviluppo**          | 6GB        | 8GB              | -          | 8GB     | Test locale, prototipazione |
| **Produzione CPU**    | 8GB        | 12GB             | -          | 10GB    | Server edge, distribuzione ottimizzata per i costi |
| **Produzione GPU**    | 6GB        | 8GB              | 4-6GB      | 8GB     | Servizi di ragionamento ad alta capacità |
| **Ottimizzato per Edge** | 4GB      | 6GB              | -          | 6GB     | Distribuzione quantizzata, gateway IoT |

### Capacità di Phi-4-mini

- **Eccellenza Matematica**: Risoluzione avanzata di problemi di aritmetica, algebra e calcolo
- **Intelligenza del Codice**: Generazione di codice in Python, JavaScript e linguaggi multipli con debug
- **Ragionamento Logico**: Decomposizione passo-passo dei problemi e costruzione delle soluzioni
- **Supporto Educativo**: Spiegazioni dettagliate adatte a scenari di apprendimento e insegnamento
- **Chiamata di Funzioni**: Supporto nativo per integrazione di strumenti e interazioni API

## Distribuzione Containerizzata con vLLM

vLLM offre un eccellente supporto per Phi-4-mini-instruct con prestazioni di inferenza ottimizzate e API compatibili con OpenAI, rendendolo ideale per servizi di ragionamento in produzione.

### Esempi di Avvio Rapido

#### Distribuzione Base su CPU (Sviluppo)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Distribuzione in Produzione Accelerata su GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Configurazione per la Produzione

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Test delle Capacità di Ragionamento di Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Distribuzione Containerizzata con Ollama

Ollama offre un eccellente supporto per Phi-4-mini-instruct con distribuzione e gestione semplificate, rendendolo ideale per lo sviluppo e distribuzioni bilanciate in produzione.

### Configurazione Rapida

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Configurazione per la Produzione

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Ottimizzazione del Modello e Varianti

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Esempi di Utilizzo delle API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine con ONNX Runtime

ONNX Runtime offre prestazioni ottimali per la distribuzione edge di Phi-4-mini-instruct con ottimizzazione avanzata e compatibilità cross-platform.

### Configurazione Base

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Implementazione Semplificata del Server

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script di Conversione del Modello

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Configurazione per la Produzione

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Test della Distribuzione ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Framework di Confronto

### Confronto tra Framework per Phi-4-mini

| Caratteristica         | vLLM       | Ollama     | ONNX Runtime |
|------------------------|------------|------------|--------------|
| **Complessità di Setup** | Moderata   | Facile     | Complessa    |
| **Prestazioni (GPU)**   | Eccellente (~25 tok/s) | Molto Buona (~20 tok/s) | Buona (~15 tok/s) |
| **Prestazioni (CPU)**   | Buona (~8 tok/s) | Molto Buona (~12 tok/s) | Eccellente (~15 tok/s) |
| **Utilizzo della Memoria** | 8-12GB    | 6-10GB     | 4-8GB        |
| **Compatibilità API**    | Compatibile con OpenAI | REST Personalizzato | FastAPI Personalizzato |
| **Chiamata di Funzioni** | ✅ Nativa  | ✅ Supportata | ⚠️ Implementazione Personalizzata |
| **Supporto Quantizzazione** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Quantizzazione ONNX |
| **Pronto per la Produzione** | ✅ Eccellente | ✅ Molto Buono | ✅ Buono |
| **Distribuzione Edge**  | Buona      | Eccellente | Straordinaria |

## Risorse Aggiuntive

### Documentazione Ufficiale
- **Microsoft Phi-4 Model Card**: Specifiche dettagliate e linee guida di utilizzo
- **Documentazione vLLM**: Opzioni avanzate di configurazione e ottimizzazione
- **Libreria Modelli Ollama**: Modelli della community ed esempi di personalizzazione
- **Guide ONNX Runtime**: Strategie di ottimizzazione delle prestazioni e distribuzione

### Strumenti di Sviluppo
- **Hugging Face Transformers**: Per interazione e personalizzazione del modello
- **Specifiche API OpenAI**: Per test di compatibilità con vLLM
- **Best Practices Docker**: Linee guida per sicurezza e ottimizzazione dei container
- **Distribuzione Kubernetes**: Pattern di orchestrazione per scalabilità in produzione

### Risorse di Apprendimento
- **Benchmarking delle Prestazioni SLM**: Metodologie di analisi comparativa
- **Distribuzione Edge AI**: Best practices per ambienti con risorse limitate
- **Ottimizzazione dei Compiti di Ragionamento**: Strategie di prompting per problemi matematici e logici
- **Sicurezza dei Container**: Pratiche di hardening per distribuzioni di modelli AI

## Risultati di Apprendimento

Dopo aver completato questo modulo, sarai in grado di:

1. Distribuire il modello Phi-4-mini-instruct in ambienti containerizzati utilizzando diversi framework
2. Configurare e ottimizzare distribuzioni SLM per diversi ambienti hardware
3. Implementare best practices di sicurezza per distribuzioni AI containerizzate
4. Confrontare e selezionare framework di distribuzione appropriati in base ai requisiti specifici del caso d'uso
5. Applicare strategie di monitoraggio e scalabilità per servizi SLM di livello produttivo

## Prossimi Passi

- Torna a [Modulo 1](../Module01/README.md)
- Torna a [Modulo 2](../Module02/README.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.