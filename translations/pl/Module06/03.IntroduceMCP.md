<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T15:20:34+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "pl"
}
-->
# Sekcja 03 - Integracja protokołu Model Context Protocol (MCP)

## Wprowadzenie do MCP (Model Context Protocol)

Model Context Protocol (MCP) to rewolucyjny framework umożliwiający modelom językowym interakcję z zewnętrznymi narzędziami i systemami w ustandaryzowany sposób. W przeciwieństwie do tradycyjnych podejść, gdzie modele są izolowane, MCP tworzy pomost między modelami AI a rzeczywistym światem poprzez dobrze zdefiniowany protokół.

### Czym jest MCP?

MCP działa jako protokół komunikacyjny, który pozwala modelom językowym:
- Łączyć się z zewnętrznymi źródłami danych
- Wykonywać narzędzia i funkcje
- Interakcję z API i usługami
- Dostęp do informacji w czasie rzeczywistym
- Realizację złożonych operacji wieloetapowych

Ten protokół przekształca statyczne modele językowe w dynamiczne agentów zdolnych do wykonywania praktycznych zadań wykraczających poza generowanie tekstu.

## Małe modele językowe (SLM) w MCP

Małe modele językowe (Small Language Models) reprezentują efektywne podejście do wdrażania AI, oferując wiele korzyści:

### Korzyści z SLM
- **Efektywność zasobów**: Niższe wymagania obliczeniowe
- **Szybszy czas odpowiedzi**: Zredukowane opóźnienia w aplikacjach czasu rzeczywistego  
- **Opłacalność**: Minimalne potrzeby infrastrukturalne
- **Prywatność**: Możliwość działania lokalnie bez przesyłania danych
- **Dostosowanie**: Łatwiejsze dostosowanie do specyficznych dziedzin

### Dlaczego SLM dobrze współpracują z MCP

Połączenie SLM z MCP tworzy potężną kombinację, w której zdolności rozumowania modelu są wspierane przez zewnętrzne narzędzia, kompensując mniejszą liczbę parametrów poprzez zwiększoną funkcjonalność.

## Przegląd Python MCP SDK

Python MCP SDK stanowi podstawę do budowy aplikacji obsługujących MCP. SDK zawiera:

- **Biblioteki klienckie**: Do łączenia się z serwerami MCP
- **Framework serwera**: Do tworzenia niestandardowych serwerów MCP
- **Obsługę protokołu**: Do zarządzania komunikacją
- **Integrację narzędzi**: Do wykonywania zewnętrznych funkcji

## Praktyczna implementacja: Klient MCP Phi-4

Przyjrzyjmy się rzeczywistej implementacji z wykorzystaniem mini modelu Phi-4 firmy Microsoft, zintegrowanego z funkcjonalnościami MCP.

### Architektura systemu

Implementacja opiera się na warstwowej architekturze:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Główne komponenty

#### 1. Klasy klienta MCP

**BaseMCPClient**: Abstrakcyjna podstawa zapewniająca wspólną funkcjonalność
- Asynchroniczny protokół zarządzania kontekstem
- Standardowa definicja interfejsu
- Zarządzanie zasobami

**Phi4MiniMCPClient**: Implementacja oparta na STDIO
- Lokalna komunikacja procesów
- Obsługa standardowego wejścia/wyjścia
- Zarządzanie podprocesami

**Phi4MiniSSEMCPClient**: Implementacja zdarzeń przesyłanych przez serwer
- Komunikacja strumieniowa HTTP
- Obsługa zdarzeń w czasie rzeczywistym
- Łączność z serwerami opartymi na sieci

#### 2. Integracja LLM

**OllamaClient**: Hosting modelu lokalnego
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Wydajne serwowanie
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Pipeline przetwarzania narzędzi

Pipeline przetwarzania narzędzi przekształca narzędzia MCP w formaty kompatybilne z modelami językowymi:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Pierwsze kroki: Przewodnik krok po kroku

### Krok 1: Konfiguracja środowiska

Zainstaluj wymagane zależności:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Krok 2: Podstawowa konfiguracja

Skonfiguruj zmienne środowiskowe:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Krok 3: Uruchomienie pierwszego klienta MCP

**Podstawowa konfiguracja Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Użycie backendu vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Połączenie zdarzeń przesyłanych przez serwer:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Niestandardowy serwer MCP:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Krok 4: Programowe użycie

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Zaawansowane funkcje

### Obsługa wielu backendów

Implementacja obsługuje zarówno backendy Ollama, jak i vLLM, pozwalając na wybór w zależności od potrzeb:

- **Ollama**: Lepszy do lokalnego rozwoju i testowania
- **vLLM**: Optymalizowany pod kątem produkcji i scenariuszy wymagających dużej przepustowości

### Elastyczne protokoły połączeń

Obsługiwane są dwa tryby połączeń:

**Tryb STDIO**: Bezpośrednia komunikacja procesów
- Niższe opóźnienia
- Odpowiedni dla lokalnych narzędzi
- Prosta konfiguracja

**Tryb SSE**: Strumieniowanie oparte na HTTP
- Możliwość pracy w sieci
- Lepszy dla systemów rozproszonych
- Aktualizacje w czasie rzeczywistym

### Możliwości integracji narzędzi

System może integrować się z różnymi narzędziami:
- Automatyzacja sieci (Playwright)
- Operacje na plikach
- Interakcje z API
- Polecenia systemowe
- Funkcje niestandardowe

## Obsługa błędów i najlepsze praktyki

### Kompleksowe zarządzanie błędami

Implementacja zawiera solidne mechanizmy obsługi błędów dla:

**Błędy połączenia:**
- Awaria serwera MCP
- Przekroczenie czasu sieciowego
- Problemy z łącznością

**Błędy wykonania narzędzi:**
- Brak narzędzi
- Walidacja parametrów
- Problemy z wykonaniem

**Błędy przetwarzania odpowiedzi:**
- Problemy z parsowaniem JSON
- Niespójności formatów
- Anomalie w odpowiedziach LLM

### Najlepsze praktyki

1. **Zarządzanie zasobami**: Korzystaj z asynchronicznych menedżerów kontekstu
2. **Obsługa błędów**: Implementuj kompleksowe bloki try-catch
3. **Logowanie**: Włącz odpowiednie poziomy logowania
4. **Bezpieczeństwo**: Waliduj dane wejściowe i oczyszczaj dane wyjściowe
5. **Wydajność**: Korzystaj z puli połączeń i mechanizmów cache

## Zastosowania w rzeczywistości

### Automatyzacja sieci
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Przetwarzanie danych
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integracja API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optymalizacja wydajności

### Zarządzanie pamięcią
- Efektywne zarządzanie historią wiadomości
- Odpowiednie czyszczenie zasobów
- Korzystanie z puli połączeń

### Optymalizacja sieci
- Asynchroniczne operacje HTTP
- Konfigurowalne limity czasu
- Łagodne odzyskiwanie po błędach

### Przetwarzanie równoległe
- Nieblokujące operacje I/O
- Równoległe wykonywanie narzędzi
- Efektywne wzorce asynchroniczne

## Rozważania dotyczące bezpieczeństwa

### Ochrona danych
- Bezpieczne zarządzanie kluczami API
- Walidacja danych wejściowych
- Oczyszczanie danych wyjściowych

### Bezpieczeństwo sieci
- Obsługa HTTPS
- Domyślne lokalne punkty końcowe
- Bezpieczne zarządzanie tokenami

### Bezpieczeństwo wykonania
- Filtrowanie narzędzi
- Środowiska sandbox
- Logowanie audytowe

## Podsumowanie

SLM zintegrowane z MCP reprezentują zmianę paradygmatu w rozwoju aplikacji AI. Łącząc efektywność małych modeli z mocą zewnętrznych narzędzi, programiści mogą tworzyć inteligentne systemy, które są zarówno zasobooszczędne, jak i bardzo funkcjonalne.

Implementacja klienta MCP Phi-4 pokazuje, jak można osiągnąć tę integrację w praktyce, zapewniając solidną podstawę do budowy zaawansowanych aplikacji wspieranych przez AI.

Kluczowe wnioski:
- MCP łączy modele językowe z zewnętrznymi systemami
- SLM oferują efektywność bez utraty możliwości dzięki wsparciu narzędzi
- Modułowa architektura umożliwia łatwe rozszerzanie i dostosowywanie
- Odpowiednia obsługa błędów i środki bezpieczeństwa są kluczowe dla zastosowań produkcyjnych

Ten poradnik stanowi podstawę do budowy własnych aplikacji MCP wspieranych przez SLM, otwierając możliwości automatyzacji, przetwarzania danych i integracji inteligentnych systemów.

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.