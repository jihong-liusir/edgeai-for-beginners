<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T13:36:12+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "pl"
}
-->
# Sesja 3: Modele Open-Source z Foundry Local

## Przegląd

W tej sesji dowiesz się, jak wprowadzać modele open-source do Foundry Local: wybór modeli społecznościowych, integracja treści Hugging Face oraz strategie „przynieś swój własny model” (BYOM). Odkryjesz również serię Model Mondays, która wspiera ciągłe uczenie się i odkrywanie modeli.

Referencje:
- Dokumentacja Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilacja modeli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Cele nauki
- Odkrywanie i ocena modeli open-source do lokalnego wnioskowania
- Kompilacja i uruchamianie wybranych modeli Hugging Face w Foundry Local
- Stosowanie strategii wyboru modeli pod kątem dokładności, opóźnień i wymagań zasobowych
- Zarządzanie modelami lokalnie za pomocą pamięci podręcznej i wersjonowania

## Część 1: Odkrywanie i wybór modeli (krok po kroku)

Krok 1) Wylistuj dostępne modele w lokalnym katalogu  
```cmd
foundry model list
```
  
Krok 2) Szybkie wypróbowanie dwóch kandydatów (automatyczne pobieranie przy pierwszym uruchomieniu)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Krok 3) Zanotuj podstawowe metryki  
- Obserwuj opóźnienia (subiektywne) i jakość dla ustalonego promptu  
- Monitoruj użycie pamięci w Menedżerze Zadań podczas uruchamiania każdego modelu  

## Część 2: Uruchamianie modeli z katalogu przez CLI (krok po kroku)

Krok 1) Uruchom model  
```cmd
foundry model run llama-3.2
```
  
Krok 2) Wyślij testowy prompt przez endpoint kompatybilny z OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Część 3: BYOM – Kompilacja modeli Hugging Face (krok po kroku)

Postępuj zgodnie z oficjalnym przewodnikiem dotyczącym kompilacji modeli. Poniżej ogólny przepływ — szczegółowe komendy i obsługiwane konfiguracje znajdziesz w artykule Microsoft Learn.

Krok 1) Przygotuj katalog roboczy  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Krok 2) Skompiluj obsługiwany model HF  
- Skorzystaj z kroków z dokumentacji Learn, aby skonwertować i umieścić skompilowany model ONNX w katalogu `models`  
- Potwierdź za pomocą:  
```cmd
foundry cache ls
```
  
Powinieneś zobaczyć nazwę swojego skompilowanego modelu (na przykład `llama-3.2`).  

Krok 3) Uruchom skompilowany model  
```cmd
foundry model run llama-3.2 --verbose
```
  
Uwagi:  
- Upewnij się, że masz wystarczającą ilość miejsca na dysku i pamięci RAM do kompilacji i uruchamiania  
- Zacznij od mniejszych modeli, aby zweryfikować przepływ, a następnie przejdź do większych  

## Część 4: Praktyczna kuracja modeli (krok po kroku)

Krok 1) Utwórz rejestr `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Krok 2) Mały skrypt selektora  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Część 5: Benchmarki praktyczne (krok po kroku)

Krok 1) Prosty benchmark opóźnień  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Krok 2) Kontrola jakości  
- Użyj ustalonego zestawu promptów, zapisz wyniki do pliku CSV/JSON  
- Ręcznie oceń płynność, trafność i poprawność (1–5)  

## Część 6: Kolejne kroki
- Subskrybuj Model Mondays, aby otrzymywać nowe modele i wskazówki: https://aka.ms/model-mondays  
- Przekaż swoje odkrycia do zespołowego `models.json`  
- Przygotuj się na Sesję 4: porównanie LLM vs SLM, lokalne vs chmurowe wnioskowanie oraz praktyczne demonstracje  

---

