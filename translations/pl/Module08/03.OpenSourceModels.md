<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T12:30:11+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "pl"
}
-->
# Sesja 3: Odkrywanie i zarządzanie modelami open-source

## Przegląd

Ta sesja koncentruje się na praktycznym odkrywaniu i zarządzaniu modelami za pomocą Foundry Local. Nauczysz się, jak wyświetlać dostępne modele, testować różne opcje i rozumieć podstawowe charakterystyki wydajności. Podejście kładzie nacisk na praktyczną eksplorację za pomocą foundry CLI, aby pomóc Ci wybrać odpowiednie modele do swoich zastosowań.

## Cele nauki

- Opanowanie poleceń foundry CLI do odkrywania i zarządzania modelami
- Zrozumienie wzorców pamięci podręcznej modeli i lokalnego przechowywania
- Nauka szybkiego testowania i porównywania różnych modeli
- Ustalenie praktycznych przepływów pracy dla wyboru i benchmarkingu modeli
- Eksploracja rosnącego ekosystemu modeli dostępnych przez Foundry Local

## Wymagania wstępne

- Ukończona Sesja 1: Wprowadzenie do Foundry Local
- Zainstalowany i dostępny Foundry Local CLI
- Wystarczająca ilość miejsca na dysku na pobieranie modeli (modele mogą mieć od 1GB do ponad 20GB)
- Podstawowa wiedza na temat typów modeli i ich zastosowań

## Część 6: Ćwiczenie praktyczne

### Ćwiczenie: Odkrywanie i porównywanie modeli

Stwórz własny skrypt oceny modeli na podstawie Przykładu 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Twoje zadanie

1. **Uruchom skrypt Przykład 03**: `samples\03\list_and_bench.cmd`
2. **Wypróbuj różne modele**: Przetestuj co najmniej 3 różne modele
3. **Porównaj wydajność**: Zauważ różnice w szybkości i jakości odpowiedzi
4. **Udokumentuj wyniki**: Stwórz prosty wykres porównawczy

### Przykładowy format porównania

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Część 7: Rozwiązywanie problemów i najlepsze praktyki

### Typowe problemy i rozwiązania

**Model nie uruchamia się:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Niewystarczająca pamięć:**
- Zacznij od mniejszych modeli (`phi-4-mini`)
- Zamknij inne aplikacje
- Rozważ rozbudowę pamięci RAM, jeśli często napotykasz ograniczenia

**Wolna wydajność:**
- Upewnij się, że model jest w pełni załadowany (sprawdź szczegółowy output)
- Zamknij niepotrzebne aplikacje w tle
- Rozważ szybsze przechowywanie (SSD)

### Najlepsze praktyki

1. **Zacznij od małych modeli**: Rozpocznij od `phi-4-mini`, aby zweryfikować konfigurację
2. **Jeden model na raz**: Zatrzymaj poprzednie modele przed uruchomieniem nowych
3. **Monitoruj zasoby**: Obserwuj zużycie pamięci
4. **Testuj konsekwentnie**: Używaj tych samych promptów dla uczciwych porównań
5. **Dokumentuj wyniki**: Notuj wydajność modeli dla swoich zastosowań

## Część 8: Kolejne kroki i materiały

### Przygotowanie do Sesji 4

- **Temat Sesji 4**: Narzędzia i techniki optymalizacji
- **Wymagania wstępne**: Swobodne przełączanie modeli i podstawowe testowanie wydajności
- **Zalecane**: Wybranie 2-3 ulubionych modeli z tej sesji

### Dodatkowe materiały

- **[Dokumentacja Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Oficjalna dokumentacja
- **[Referencja CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Kompletny przewodnik po poleceniach
- **[Model Mondays](https://aka.ms/model-mondays)**: Cotygodniowe prezentacje modeli
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Społeczność i zgłaszanie problemów
- **[Przykład 03: Odkrywanie modeli](samples/03/README.md)**: Praktyczny skrypt przykładowy

### Kluczowe wnioski

✅ **Odkrywanie modeli**: Użyj `foundry model list`, aby eksplorować dostępne modele  
✅ **Szybkie testowanie**: Wzorzec `list_and_bench.cmd` do szybkiej oceny  
✅ **Monitorowanie wydajności**: Podstawowe pomiary zużycia zasobów i czasu odpowiedzi  
✅ **Wybór modeli**: Praktyczne wskazówki dotyczące wyboru modeli według zastosowań  
✅ **Zarządzanie pamięcią podręczną**: Zrozumienie procedur przechowywania i czyszczenia  

Teraz posiadasz praktyczne umiejętności odkrywania, testowania i wyboru odpowiednich modeli dla swoich aplikacji AI, korzystając z prostego podejścia CLI Foundry Local.

## Cele nauki

- Odkrywanie i ocena modeli open-source do lokalnego wnioskowania
- Kompilacja i uruchamianie wybranych modeli Hugging Face w Foundry Local
- Stosowanie strategii wyboru modeli pod kątem dokładności, opóźnień i potrzeb zasobów
- Zarządzanie modelami lokalnie z wykorzystaniem pamięci podręcznej i wersjonowania

## Część 1: Odkrywanie modeli za pomocą Foundry CLI

### Podstawowe polecenia zarządzania modelami

Foundry CLI oferuje proste polecenia do odkrywania i zarządzania modelami:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Uruchamianie pierwszych modeli

Zacznij od popularnych, dobrze przetestowanych modeli, aby zrozumieć ich charakterystyki wydajności:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Uwaga:** Flaga `--verbose` dostarcza szczegółowych informacji o uruchamianiu, w tym:
- Postęp pobierania modelu (przy pierwszym uruchomieniu)
- Szczegóły alokacji pamięci
- Informacje o wiązaniu usług
- Metryki inicjalizacji wydajności

### Zrozumienie kategorii modeli

**Małe modele językowe (SLM):**
- `phi-4-mini`: Szybki, wydajny, idealny do ogólnych rozmów
- `phi-4`: Bardziej zaawansowana wersja z lepszym rozumowaniem

**Średnie modele:**
- `qwen2.5-7b-instruct`: Doskonałe rozumowanie i dłuższy kontekst
- `deepseek-r1-distill-qwen-7b`: Optymalizowany do generowania kodu

**Duże modele:**
- `llama-3.2`: Najnowszy model open-source od Meta
- `qwen2.5-14b-instruct`: Rozumowanie na poziomie korporacyjnym

## Część 2: Szybkie testowanie i porównywanie modeli

### Podejście Przykład 03: Prosta lista i benchmark

Na podstawie wzorca Przykład 03, oto minimalny przepływ pracy:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testowanie wydajności modeli

Po uruchomieniu modelu, testuj go za pomocą spójnych promptów:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternatywa testowania w PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Część 3: Zarządzanie pamięcią podręczną i przechowywaniem modeli

### Zrozumienie pamięci podręcznej modeli

Foundry Local automatycznie zarządza pobieraniem i pamięcią podręczną modeli:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Rozważania dotyczące przechowywania modeli

**Typowe rozmiary modeli:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Najlepsze praktyki przechowywania:**
- Przechowuj 2-3 modele w pamięci podręcznej dla szybkiego przełączania
- Usuwaj nieużywane modele, aby zwolnić miejsce: `foundry cache clean`
- Monitoruj użycie dysku, szczególnie na mniejszych SSD
- Rozważ kompromis między rozmiarem modelu a jego możliwościami

### Monitorowanie wydajności modeli

Podczas działania modeli monitoruj zasoby systemowe:

**Menedżer zadań Windows:**
- Obserwuj zużycie pamięci (modele pozostają załadowane w RAM)
- Monitoruj wykorzystanie CPU podczas wnioskowania
- Sprawdzaj operacje dyskowe podczas początkowego ładowania modelu

**Monitorowanie w wierszu poleceń:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Część 4: Praktyczne wskazówki dotyczące wyboru modeli

### Wybór modeli według zastosowań

**Do ogólnych rozmów i pytań:**
- Zacznij od: `phi-4-mini` (szybki, wydajny)
- Przejdź do: `phi-4` (lepsze rozumowanie)
- Zaawansowane: `qwen2.5-7b-instruct` (dłuższy kontekst)

**Do generowania kodu:**
- Zalecane: `deepseek-r1-distill-qwen-7b`
- Alternatywa: `qwen2.5-7b-instruct` (również dobry do kodu)

**Do złożonego rozumowania:**
- Najlepsze: `qwen2.5-7b-instruct` lub `qwen2.5-14b-instruct`
- Opcja budżetowa: `phi-4`

### Przewodnik po wymaganiach sprzętowych

**Minimalne wymagania systemowe:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Zalecane dla najlepszej wydajności:**
- 32GB+ RAM dla wygodnego przełączania modeli
- Dysk SSD dla szybszego ładowania modeli
- Nowoczesny procesor z dobrą wydajnością jednowątkową
- Wsparcie NPU (komputery z Windows 11 Copilot+) dla przyspieszenia

### Przepływ pracy przełączania modeli

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Część 5: Prosty benchmarking modeli

### Podstawowe testowanie wydajności

Oto prosty sposób na porównanie wydajności modeli:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Ręczna ocena jakości

Dla każdego modelu testuj za pomocą spójnych promptów i oceniaj ręcznie:

**Prompty testowe:**
1. "Wyjaśnij obliczenia kwantowe w prostych słowach."
2. "Napisz funkcję w Pythonie do sortowania listy."
3. "Jakie są zalety i wady pracy zdalnej?"
4. "Podsumuj korzyści płynące z edge AI."

**Kryteria oceny:**
- **Dokładność**: Czy informacje są poprawne?
- **Jasność**: Czy wyjaśnienie jest łatwe do zrozumienia?
- **Kompletność**: Czy odpowiedź obejmuje całe pytanie?
- **Szybkość**: Jak szybko model odpowiada?

### Monitorowanie zużycia zasobów

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Część 6: Kolejne kroki

- Subskrybuj Model Mondays, aby poznawać nowe modele i wskazówki: https://aka.ms/model-mondays
- Przekaż wyniki swojemu zespołowi do `models.json`
- Przygotuj się na Sesję 4: porównanie LLM vs SLM, lokalne vs chmurowe wnioskowanie oraz praktyczne demonstracje

---

