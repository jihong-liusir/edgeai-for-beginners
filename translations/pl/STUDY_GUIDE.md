<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T13:15:58+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "pl"
}
-->
# EdgeAI dla Początkujących: Ścieżki Nauki i Harmonogram Nauki

### Skondensowana Ścieżka Nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 1 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 2 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 3 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 4-5 | Moduł 4: Optymalizacja Modeli (6 frameworków) | 4 godziny |
| Dzień 6 | Moduł 5: SLMOps | 3 godziny |
| Dzień 7 | Moduł 6-7: Agenci AI i Narzędzia Rozwojowe | 5 godzin |

### Skondensowana Ścieżka Nauki (2 tygodnie)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja Modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Agenci AI | 2 godziny |
| Dzień 13-14 | Moduł 7: Narzędzia Rozwojowe | 3 godziny |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i Podstawy SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i Optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i Agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia Rozwojowe i Integracja | 3 godziny |

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja Modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Systemy Agentowe SLM | 2 godziny |
| Dzień 13-14 | Moduł 7: Przykłady Implementacji EdgeAI | 2 godziny |

| Moduł | Data ukończenia | Czas poświęcony | Kluczowe wnioski |
|-------|-----------------|-----------------|------------------|
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Podstawy SLM | | | |
| Moduł 3: Wdrażanie SLM | | | |
| Moduł 4: Optymalizacja Modeli (6 frameworków) | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Systemy Agentowe SLM | | | |
| Moduł 7: Przykłady Implementacji EdgeAI | | | |
| Ćwiczenia praktyczne | | | |
| Mini-projekt | | | |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i Podstawy SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i Optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i Agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia Rozwojowe i Integracja | 3 godziny |

## Wprowadzenie

Witamy w przewodniku nauki EdgeAI dla początkujących! Ten dokument został zaprojektowany, aby pomóc Ci skutecznie przejść przez materiały kursu i maksymalnie wykorzystać proces nauki. Zawiera uporządkowane ścieżki nauki, sugerowane harmonogramy, podsumowania kluczowych koncepcji oraz dodatkowe zasoby, które pogłębią Twoją wiedzę na temat technologii EdgeAI.

To zwięzły kurs trwający 20 godzin, który dostarcza podstawowej wiedzy o EdgeAI w efektywnym czasowo formacie, idealnym dla zapracowanych profesjonalistów i studentów, którzy chcą szybko zdobyć praktyczne umiejętności w tej rozwijającej się dziedzinie.

## Przegląd kursu

Kurs jest podzielony na siedem kompleksowych modułów:

1. **Podstawy EdgeAI i Transformacja** - Zrozumienie podstawowych koncepcji i zmiany technologicznej
2. **Podstawy Małych Modeli Językowych (SLM)** - Eksploracja różnych rodzin SLM i ich architektur
3. **Wdrażanie Małych Modeli Językowych (SLM)** - Wdrażanie praktycznych strategii implementacji
4. **Konwersja Formatów Modeli i Kwantyzacja** - Zaawansowana optymalizacja z użyciem 6 frameworków, w tym OpenVINO
5. **SLMOps - Operacje Małych Modeli Językowych** - Zarządzanie cyklem życia produkcji i wdrażania
6. **Systemy Agentowe SLM** - Agenci AI, wywoływanie funkcji i protokół kontekstu modelu
7. **Przykłady Implementacji EdgeAI** - Narzędzia AI, rozwój na Windows i implementacje specyficzne dla platform
8. **Microsoft Foundry Local – Kompletny Zestaw Narzędzi dla Programistów** - Rozwój lokalny z hybrydową integracją Azure (Moduł 08)

## Jak korzystać z tego przewodnika nauki

- **Nauka progresywna**: Przechodź przez moduły w kolejności, aby uzyskać najbardziej spójne doświadczenie nauki
- **Punkty kontrolne wiedzy**: Korzystaj z pytań do samooceny po każdej sekcji
- **Ćwiczenia praktyczne**: Wykonuj sugerowane ćwiczenia, aby utrwalić teoretyczne koncepcje
- **Dodatkowe zasoby**: Eksploruj dodatkowe materiały dotyczące tematów, które najbardziej Cię interesują

## Rekomendacje dotyczące harmonogramu nauki

### Skondensowana Ścieżka Nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 6 godzin |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 8 godzin |
| Dzień 5 | Moduł 3: Wdrażanie SLM | 3 godziny |
| Dzień 6 | Moduł 8: Zestaw Narzędzi Foundry Local | 3 godziny |

### Nauka w niepełnym wymiarze godzin (3 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1: Podstawy EdgeAI | 6-7 godzin |
| Tydzień 2 | Moduł 2: Podstawy SLM | 7-8 godzin |
| Tydzień 3 | Moduł 3: Wdrażanie SLM (3h) + Moduł 8: Zestaw Narzędzi Foundry Local (2-3h) | 5-6 godzin |

## Moduł 1: Podstawy EdgeAI i Transformacja

### Kluczowe cele nauki

- Zrozumienie różnic między AI opartym na chmurze a AI opartym na urządzeniach brzegowych
- Opanowanie podstawowych technik optymalizacji dla środowisk o ograniczonych zasobach
- Analiza rzeczywistych zastosowań technologii EdgeAI
- Konfiguracja środowiska rozwojowego dla projektów EdgeAI

### Obszary nauki

#### Sekcja 1: Podstawy EdgeAI
- **Kluczowe koncepcje**: 
  - Paradigmaty obliczeń brzegowych vs. chmurowych
  - Techniki kwantyzacji modeli
  - Opcje akceleracji sprzętowej (NPU, GPU, CPU)
  - Zalety w zakresie prywatności i bezpieczeństwa

- **Dodatkowe materiały**:
  - [Dokumentacja TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Dokumentacja Edge Impulse](https://docs.edgeimpulse.com)

#### Sekcja 2: Studium przypadków
- **Kluczowe koncepcje**: 
  - Ekosystem modeli Microsoft Phi & Mu
  - Praktyczne implementacje w różnych branżach
  - Rozważania dotyczące wdrożenia

#### Sekcja 3: Przewodnik po implementacji
- **Kluczowe koncepcje**: 
  - Konfiguracja środowiska rozwojowego
  - Narzędzia do kwantyzacji i optymalizacji
  - Metody oceny implementacji EdgeAI

#### Sekcja 4: Sprzęt do wdrożeń brzegowych
- **Kluczowe koncepcje**: 
  - Porównania platform sprzętowych
  - Strategie optymalizacji dla konkretnego sprzętu
  - Rozważania dotyczące wdrożenia

### Pytania do samooceny

1. Porównaj i skontrastuj implementacje AI oparte na chmurze z implementacjami AI opartymi na urządzeniach brzegowych.
2. Wyjaśnij trzy kluczowe techniki optymalizacji modeli dla wdrożeń brzegowych.
3. Jakie są główne zalety uruchamiania modeli AI na urządzeniach brzegowych?
4. Opisz proces kwantyzacji modelu i jego wpływ na wydajność.
5. Wyjaśnij, jak różne akceleratory sprzętowe (NPU, GPU, CPU) wpływają na wdrożenie EdgeAI.

### Ćwiczenia praktyczne

1. **Szybka konfiguracja środowiska**: Skonfiguruj minimalne środowisko rozwojowe z niezbędnymi pakietami (30 minut)
2. **Eksploracja modelu**: Pobierz i przeanalizuj wstępnie wytrenowany mały model językowy (1 godzina)
3. **Podstawowa kwantyzacja**: Wypróbuj prostą kwantyzację na małym modelu (1 godzina)

## Moduł 2: Podstawy Małych Modeli Językowych (SLM)

### Kluczowe cele nauki

- Zrozumienie zasad architektonicznych różnych rodzin SLM
- Porównanie możliwości modeli w różnych skalach parametrów
- Ocena modeli pod kątem efektywności, możliwości i wymagań wdrożeniowych
- Rozpoznanie odpowiednich zastosowań dla różnych rodzin modeli

### Obszary nauki

#### Sekcja 1: Rodzina modeli Microsoft Phi
- **Kluczowe koncepcje**: 
  - Ewolucja filozofii projektowania
  - Architektura zorientowana na efektywność
  - Specjalistyczne możliwości

#### Sekcja 2: Rodzina Qwen
- **Kluczowe koncepcje**: 
  - Wkład open source
  - Skalowalne opcje wdrożeniowe
  - Zaawansowana architektura rozumowania

#### Sekcja 3: Rodzina Gemma
- **Kluczowe koncepcje**: 
  - Innowacje napędzane badaniami
  - Możliwości multimodalne
  - Optymalizacja mobilna

#### Sekcja 4: Rodzina BitNET
- **Kluczowe koncepcje**: 
  - Technologia kwantyzacji 1-bitowej
  - Framework optymalizacji inferencji
  - Rozważania dotyczące zrównoważonego rozwoju

#### Sekcja 5: Model Microsoft Mu
- **Kluczowe koncepcje**: 
  - Architektura zorientowana na urządzenia
  - Integracja systemowa z Windows
  - Operacja zachowująca prywatność

#### Sekcja 6: Phi-Silica
- **Kluczowe koncepcje**: 
  - Architektura zoptymalizowana pod kątem NPU
  - Metryki wydajności
  - Integracja dla programistów

### Pytania do samooceny

1. Porównaj podejścia architektoniczne rodzin modeli Phi i Qwen.
2. Wyjaśnij, jak technologia kwantyzacji BitNET różni się od tradycyjnej kwantyzacji.
3. Jakie są unikalne zalety modelu Mu dla integracji z Windows?
4. Opisz, jak Phi-Silica wykorzystuje sprzęt NPU do optymalizacji wydajności.
5. Dla aplikacji mobilnej z ograniczoną łącznością, która rodzina modeli byłaby najbardziej odpowiednia i dlaczego?

### Ćwiczenia praktyczne

1. **Porównanie modeli**: Szybki benchmark dwóch różnych modeli SLM (1 godzina)
2. **Prosta generacja tekstu**: Podstawowa implementacja generacji tekstu z małym modelem (1 godzina)
3. **Szybka optymalizacja**: Zastosowanie jednej techniki optymalizacji w celu poprawy szybkości inferencji (1 godzina)

## Moduł 3: Wdrażanie Małych Modeli Językowych (SLM)

### Kluczowe cele nauki

- Wybór odpowiednich modeli na podstawie ograniczeń wdrożeniowych
- Opanowanie technik optymalizacji dla różnych scenariuszy wdrożeniowych
- Implementacja SLM w środowiskach lokalnych i chmurowych
- Projektowanie konfiguracji gotowych do produkcji dla aplikacji EdgeAI

### Obszary nauki

#### Sekcja 1: Zaawansowana nauka SLM
- **Kluczowe koncepcje**: 
  - Ramy klasyfikacji parametrów
  - Zaawansowane techniki optymalizacji
  - Strategie pozyskiwania modeli

#### Sekcja 2: Wdrażanie w środowisku lokalnym
- **Kluczowe koncepcje**: 
  - Wdrażanie na platformie Ollama
  - Rozwiązania lokalne Microsoft Foundry
  - Analiza porównawcza frameworków

#### Sekcja 3: Wdrażanie w chmurze z konteneryzacją
- **Kluczowe koncepcje**: 
  - Inferencja wysokiej wydajności vLLM
  - Orkiestracja kontenerów
  - Implementacja ONNX Runtime

### Pytania do samooceny

1. Jakie czynniki należy wziąć pod uwagę przy wyborze między wdrożeniem lokalnym a wdrożeniem w chmurze?
2. Porównaj Ollama i Microsoft Foundry Local jako opcje wdrożeniowe.
3. Wyjaśnij korzyści płynące z konteneryzacji dla wdrożenia SLM.
4. Jakie są kluczowe metryki wydajności do monitorowania dla SLM wdrożonego na urządzeniach brzegowych?
5. Opisz kompletny workflow wdrożeniowy od wyboru modelu do implementacji produkcyjnej.

### Ćwiczenia praktyczne

1. **Podstawowe wdrożenie lokalne**: Wdrożenie prostego SLM za pomocą Ollama (1 godzina)
2. **Sprawdzenie wydajności**: Szybki benchmark wdrożonego modelu (30 minut)
3. **Prosta integracja**: Stworzenie minimalnej aplikacji korzystającej z wdrożonego modelu (1 godzina)

## Moduł 4: Konwersja Formatów Modeli i Kwantyzacja

### Kluczowe cele nauki

- Opanowanie zaawansowanych technik kwantyzacji od precyzji 1-bitowej do 8-bitowej
- Zrozumienie strategii konwersji formatów (GGUF, ONNX)
- Implementacja optymalizacji w sześciu frameworkach (Llama.cpp, Olive, OpenVINO, MLX, synteza workflow)
- Wdrażanie zoptymalizowanych modeli w środowiskach produkcyjnych na urządzeniach brzegowych, w tym Intel, Apple i sprzęcie wieloplatformowym

### Obszary nauki

####
- **Priorytetowe Koncepcje**: 
  - Optymalizacja dla Apple Silicon
  - Zunifikowana architektura pamięci
  - Możliwości dostrajania LoRA

#### Sekcja 6: Synteza Workflow Rozwoju Edge AI
- **Priorytetowe Koncepcje**: 
  - Zunifikowana architektura workflow
  - Drzewa decyzyjne wyboru frameworków
  - Walidacja gotowości produkcyjnej
  - Strategie zapewnienia przyszłościowej architektury

### Pytania do Samooceny

1. Porównaj strategie kwantyzacji na różnych poziomach precyzji (od 1-bit do 8-bit).
2. Wyjaśnij zalety formatu GGUF dla wdrożeń na krawędzi.
3. Jak optymalizacja uwzględniająca sprzęt w Microsoft Olive poprawia efektywność wdrożeń?
4. Jakie są kluczowe korzyści z wykorzystania NNCF w OpenVINO do kompresji modeli?
5. Opisz, jak Apple MLX wykorzystuje zunifikowaną architekturę pamięci do optymalizacji.
6. Jak synteza workflow pomaga w wyborze optymalnych frameworków optymalizacyjnych?

### Ćwiczenia Praktyczne

1. **Kwantyzacja Modelu**: Zastosuj różne poziomy kwantyzacji do modelu i porównaj wyniki (1 godzina)
2. **Optymalizacja OpenVINO**: Użyj NNCF do kompresji modelu dla sprzętu Intel (1 godzina)
3. **Porównanie Frameworków**: Przetestuj ten sam model w trzech różnych frameworkach optymalizacyjnych (1 godzina)
4. **Benchmarking Wydajności**: Zmierz wpływ optymalizacji na szybkość wnioskowania i wykorzystanie pamięci (1 godzina)

## Moduł 5: SLMOps - Operacje na Małych Modelach Językowych

### Kluczowe Cele Nauki

- Zrozumienie zasad zarządzania cyklem życia SLMOps
- Opanowanie technik destylacji i dostrajania dla wdrożeń na krawędzi
- Implementacja strategii wdrożeń produkcyjnych z monitorowaniem
- Budowa workflow operacyjnych i utrzymaniowych klasy korporacyjnej dla SLM

### Obszary Skupienia

#### Sekcja 1: Wprowadzenie do SLMOps
- **Priorytetowe Koncepcje**: 
  - Przełomowy paradygmat SLMOps w operacjach AI
  - Architektura zorientowana na efektywność kosztową i prywatność
  - Strategiczny wpływ biznesowy i przewagi konkurencyjne

#### Sekcja 2: Destylacja Modelu
- **Priorytetowe Koncepcje**: 
  - Techniki transferu wiedzy
  - Implementacja procesu destylacji dwustopniowej
  - Workflow destylacji w Azure ML

#### Sekcja 3: Strategie Dostrajania
- **Priorytetowe Koncepcje**: 
  - Dostrajanie efektywne pod względem parametrów (PEFT)
  - Zaawansowane metody LoRA i QLoRA
  - Trening wieloadapterowy i optymalizacja hiperparametrów

#### Sekcja 4: Wdrożenie Produkcyjne
- **Priorytetowe Koncepcje**: 
  - Konwersja i kwantyzacja modelu na potrzeby produkcji
  - Konfiguracja wdrożenia Foundry Local
  - Benchmarking wydajności i walidacja jakości

### Pytania do Samooceny

1. Jak SLMOps różni się od tradycyjnego MLOps?
2. Wyjaśnij korzyści z destylacji modelu dla wdrożeń na krawędzi.
3. Jakie są kluczowe aspekty dostrajania SLM w środowiskach o ograniczonych zasobach?
4. Opisz kompletny pipeline wdrożeniowy dla aplikacji Edge AI.

### Ćwiczenia Praktyczne

1. **Podstawowa Destylacja**: Stwórz mniejszy model na podstawie większego modelu nauczyciela (1 godzina)
2. **Eksperyment Dostrajania**: Dostrój model dla konkretnej dziedziny (1 godzina)
3. **Pipeline Wdrożeniowy**: Skonfiguruj podstawowy pipeline CI/CD dla wdrożenia modelu (1 godzina)

## Moduł 6: Systemy Agentowe SLM - Agenci AI i Wywoływanie Funkcji

### Kluczowe Cele Nauki

- Tworzenie inteligentnych agentów AI dla środowisk krawędziowych z wykorzystaniem Małych Modeli Językowych
- Implementacja możliwości wywoływania funkcji w ramach systematycznych workflow
- Opanowanie integracji Model Context Protocol (MCP) dla standaryzowanej interakcji z narzędziami
- Tworzenie zaawansowanych systemów agentowych z minimalną interwencją człowieka

### Obszary Skupienia

#### Sekcja 1: Agenci AI i Podstawy SLM
- **Priorytetowe Koncepcje**: 
  - Ramy klasyfikacji agentów (refleksyjni, oparte na modelu, oparte na celach, uczący się agenci)
  - Analiza kompromisów między SLM a LLM
  - Wzorce projektowe dla agentów specyficznych dla krawędzi
  - Optymalizacja zasobów dla agentów

#### Sekcja 2: Wywoływanie Funkcji w Małych Modelach Językowych
- **Priorytetowe Koncepcje**: 
  - Implementacja systematycznego workflow (detekcja intencji, wyjście JSON, wykonanie zewnętrzne)
  - Implementacje specyficzne dla platform (Phi-4-mini, wybrane modele Qwen, Microsoft Foundry Local)
  - Zaawansowane przykłady (współpraca wieloagentowa, dynamiczny wybór narzędzi)
  - Rozważania produkcyjne (ograniczenia szybkości, logowanie audytowe, środki bezpieczeństwa)

#### Sekcja 3: Integracja Model Context Protocol (MCP)
- **Priorytetowe Koncepcje**: 
  - Architektura protokołu i projekt systemu warstwowego
  - Obsługa wielu backendów (Ollama dla rozwoju, vLLM dla produkcji)
  - Protokoły połączeń (tryby STDIO i SSE)
  - Zastosowania w rzeczywistości (automatyzacja webowa, przetwarzanie danych, integracja API)

### Pytania do Samooceny

1. Jakie są kluczowe aspekty architektoniczne dla agentów AI na krawędzi?
2. Jak wywoływanie funkcji zwiększa możliwości agentów?
3. Wyjaśnij rolę Model Context Protocol w komunikacji agentów.

### Ćwiczenia Praktyczne

1. **Prosty Agent**: Zbuduj podstawowego agenta AI z wywoływaniem funkcji (1 godzina)
2. **Integracja MCP**: Zaimplementuj MCP w aplikacji agenta (30 minut)

## Moduł 7: Przykłady Implementacji EdgeAI

### Kluczowe Cele Nauki

- Opanowanie AI Toolkit dla Visual Studio Code w celu kompleksowego workflow rozwoju EdgeAI
- Zdobycie wiedzy na temat platformy Windows AI Foundry i strategii optymalizacji NPU
- Implementacja EdgeAI na różnych platformach sprzętowych i w scenariuszach wdrożeniowych
- Tworzenie aplikacji EdgeAI gotowych do produkcji z optymalizacjami specyficznymi dla platformy

### Obszary Skupienia

#### Sekcja 1: AI Toolkit dla Visual Studio Code
- **Priorytetowe Koncepcje**: 
  - Kompleksowe środowisko rozwoju Edge AI w VS Code
  - Katalog modeli i ich odkrywanie dla wdrożeń na krawędzi
  - Workflow lokalnego testowania, optymalizacji i rozwoju agentów
  - Monitorowanie wydajności i ocena dla scenariuszy krawędziowych

#### Sekcja 2: Przewodnik Rozwoju EdgeAI dla Windows
- **Priorytetowe Koncepcje**: 
  - Kompleksowy przegląd platformy Windows AI Foundry
  - API Phi Silica dla efektywnego wnioskowania na NPU
  - API Computer Vision dla przetwarzania obrazów i OCR
  - Foundry Local CLI dla lokalnego rozwoju i testowania

#### Sekcja 3: Implementacje Specyficzne dla Platformy
- **Priorytetowe Koncepcje**: 
  - Wdrożenie na NVIDIA Jetson Orin Nano (wydajność AI 67 TOPS)
  - Aplikacje mobilne z .NET MAUI i ONNX Runtime GenAI
  - Rozwiązania Azure EdgeAI z hybrydową architekturą chmura-krawędź
  - Optymalizacja Windows ML z uniwersalnym wsparciem sprzętowym
  - Aplikacje Foundry Local z implementacją RAG zorientowaną na prywatność

### Pytania do Samooceny

1. Jak AI Toolkit usprawnia workflow rozwoju EdgeAI?
2. Porównaj strategie wdrożeniowe na różnych platformach sprzętowych.
3. Jakie są zalety Windows AI Foundry dla rozwoju na krawędzi?
4. Wyjaśnij rolę optymalizacji NPU w nowoczesnych aplikacjach Edge AI.
5. Jak API Phi Silica wykorzystuje sprzęt NPU do optymalizacji wydajności?
6. Porównaj korzyści z wdrożeń lokalnych i w chmurze dla aplikacji wrażliwych na prywatność.

### Ćwiczenia Praktyczne

1. **Konfiguracja AI Toolkit**: Skonfiguruj AI Toolkit i zoptymalizuj model (1 godzina)
2. **Windows AI Foundry**: Zbuduj prostą aplikację Windows AI z wykorzystaniem API Phi Silica (1 godzina)
3. **Wdrożenie Międzyplatformowe**: Wdróż ten sam model na dwóch różnych platformach (1 godzina)
4. **Optymalizacja NPU**: Przetestuj wydajność NPU za pomocą narzędzi Windows AI Foundry (30 minut)

## Moduł 8: Microsoft Foundry Local – Kompletny Zestaw Narzędzi dla Programistów

### Kluczowe Cele Nauki

- Instalacja i konfiguracja Foundry Local na Windows
- Uruchamianie, odkrywanie i zarządzanie modelami lokalnie za pomocą Foundry CLI
- Integracja z klientami REST i SDK kompatybilnymi z OpenAI
- Tworzenie praktycznych przykładów: Chainlit chat, agenci i router modeli
- Zrozumienie hybrydowych wzorców z Azure AI Foundry

### Obszary Skupienia

- Instalacja i podstawy CLI (model, usługa, cache)
- Integracja SDK (klienci kompatybilni z OpenAI i Azure OpenAI)
- Szybka walidacja Open WebUI
- Wzorce agentów i wywoływania funkcji
- Modele jako narzędzia (projekt routera i rejestru)

### Pytania do Samooceny

1. Jak odkryć lokalny endpoint i wyświetlić dostępne modele?
2. Jakie są różnice między użyciem Foundry Local REST a Azure OpenAI?
3. Jak zaprojektować prosty router do wyboru modeli jako narzędzi?
4. Które kategorie CLI są najbardziej istotne dla codziennego rozwoju?
5. Jak zweryfikować gotowość Foundry Local przed uruchomieniem aplikacji?

### Ćwiczenia Praktyczne

1. Zainstaluj/zaktualizuj Foundry Local i uruchom `phi-4-mini` lokalnie (30 minut)
2. Wywołaj `/v1/models` i uruchom prosty chat przez REST (30 minut)
3. Uruchom przykładową aplikację Chainlit i rozmawiaj lokalnie (30 minut)
4. Uruchom koordynatora wieloagentowego i przeanalizuj wyniki (30 minut)
5. Wypróbuj router modeli jako narzędzi z nadpisaniami opartymi na środowisku (30 minut)

## Przewodnik Alokacji Czasu

Aby maksymalnie wykorzystać 20-godzinny czas kursu, oto sugerowany podział czasu:

| Aktywność | Alokacja Czasu | Opis |
|----------|----------------|-------------|
| Czytanie Materiałów Podstawowych | 9 godzin | Skupienie na kluczowych koncepcjach w każdym module |
| Ćwiczenia Praktyczne | 6 godzin | Praktyczna implementacja kluczowych technik |
| Samoocena | 2 godziny | Testowanie zrozumienia poprzez pytania i refleksję |
| Mini-Projekt | 3 godziny | Zastosowanie wiedzy w małej praktycznej implementacji |

### Kluczowe Obszary Skupienia w Zależności od Ograniczeń Czasowych

**Jeśli masz tylko 10 godzin:**
- Ukończ Moduły 1, 2 i 3 (podstawowe koncepcje EdgeAI)
- Wykonaj co najmniej jedno ćwiczenie praktyczne na moduł
- Skup się na zrozumieniu kluczowych koncepcji zamiast szczegółów implementacji

**Jeśli możesz poświęcić pełne 20 godzin:**
- Ukończ wszystkie siedem modułów
- Wykonaj kluczowe ćwiczenia praktyczne z każdego modułu
- Ukończ jeden mini-projekt z Modułu 7
- Przeglądaj co najmniej 2-3 dodatkowe zasoby

**Jeśli masz więcej niż 20 godzin:**
- Ukończ wszystkie moduły z szczegółowymi ćwiczeniami
- Zbuduj wiele mini-projektów
- Eksploruj zaawansowane techniki optymalizacji w Moduł 4
- Implementuj wdrożenia produkcyjne z Modułu 5

## Zasoby Niezbędne

Te starannie wybrane zasoby zapewniają największą wartość w ograniczonym czasie nauki:

### Dokumentacja do Przeczytania
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Najbardziej efektywne narzędzie do optymalizacji modeli
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Najszybszy sposób na lokalne wdrożenie SLM
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Odniesienie do wiodącego modelu zoptymalizowanego dla krawędzi
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kompleksowy zestaw narzędzi optymalizacyjnych Intela
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Zintegrowane środowisko rozwoju EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Platforma rozwoju EdgeAI specyficzna dla Windows

### Narzędzia Oszczędzające Czas
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Szybki dostęp do modeli i wdrożeń
- [Gradio](https://www.gradio.app/docs/interface) - Szybki rozwój interfejsów dla demonstracji AI
- [Microsoft Olive](https://github.com/microsoft/Olive) - Uproszczona optymalizacja modeli
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efektywne wnioskowanie na CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework kompresji sieci neuronowych
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Zestaw narzędzi do wdrożenia dużych modeli językowych

## Szablon Śledzenia Postępów

Użyj tego uproszczonego szablonu, aby śledzić swoje postępy w nauce podczas 20-godzinnego kursu:

| Moduł | Data Ukończenia | Czas Spędzony | Kluczowe Wnioski |
|--------|----------------|-------------|---------------|
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Podstawy SLM | | | |
| Moduł 3: Wdrożenie SLM | | | |
| Moduł 4: Optymalizacja Modelu | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Agenci AI | | | |
| Moduł 7: Narzędzia Rozwojowe | | | |
| Moduł 8: Zestaw Narzędzi Foundry Local | | | |
| Ćwiczenia Praktyczne | | | |
| Mini-Projekt | | | |

## Pomysły na Mini-Projekty

Rozważ ukończenie jednego z tych projektów, aby przećwiczyć koncepcje EdgeAI (każdy zaprojektowany na 2-4 godziny):

### Projekty dla Początkujących (2-3 godziny każdy)
1. **Asystent Tekstowy na Krawędzi**: Stwórz prosty narzędzie do uzupełniania tekstu offline z użyciem małego modelu językowego
2. **Dashboard Porównania Modeli**: Zbuduj podstawową wizualizację metryk wydajności dla różnych SLM
3
8. **Pipeline optymalizacji OpenVINO**: Wdrożenie pełnej optymalizacji modelu za pomocą NNCF i narzędzia GenAI  
9. **Pipeline SLMOps**: Wdrożenie pełnego cyklu życia modelu od treningu po wdrożenie na urządzeniach brzegowych  
10. **System wielomodelowy na brzegu**: Wdrożenie wielu wyspecjalizowanych modeli współpracujących na sprzęcie brzegowym  
11. **System integracji MCP**: Budowa systemu agentowego wykorzystującego Model Context Protocol do interakcji z narzędziami  

## Źródła  

- Microsoft Learn (Foundry Local)  
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Pierwsze kroki: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Referencje CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integracja z SDK inferencji: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Jak otworzyć WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Kompilacja modeli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agenci (przegląd): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Narzędzia optymalizacji i inferencji  
  - Microsoft Olive (dokumentacja): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (pierwsze kroki): https://onnxruntime.ai/docs/get-started/with-python.html  
  - Integracja ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (dokumentacja): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (dokumentacja): https://ml-explore.github.io/mlx/build/html/index.html  
- Frameworki wdrożeniowe i modele  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (dokumentacja): https://docs.vllm.ai/  
  - Ollama (pierwsze kroki): https://github.com/ollama/ollama#get-started  
- Narzędzia dla deweloperów (Windows i VS Code)  
  - AI Toolkit dla VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (przegląd): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Społeczność edukacyjna  

Dołącz do dyskusji i nawiąż kontakt z innymi uczącymi się:  
- Dyskusje na GitHub w [repozytorium EdgeAI dla początkujących](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Podsumowanie  

EdgeAI to przyszłość wdrażania sztucznej inteligencji, oferująca potężne możliwości bezpośrednio na urządzeniach, jednocześnie rozwiązując kluczowe kwestie związane z prywatnością, opóźnieniami i łącznością. Ten 20-godzinny kurs dostarcza niezbędnej wiedzy i praktycznych umiejętności, aby natychmiast rozpocząć pracę z technologiami EdgeAI.  

Kurs jest celowo zwięzły i skupia się na najważniejszych koncepcjach, umożliwiając szybkie zdobycie cennej wiedzy bez nadmiernego obciążenia czasowego. Pamiętaj, że praktyka, nawet z prostymi przykładami, jest kluczem do utrwalenia zdobytej wiedzy.  

Powodzenia w nauce!  

---

