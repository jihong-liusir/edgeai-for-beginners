<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef04a48f3f1428fa008738033017e0e8",
  "translation_date": "2025-09-24T12:25:17+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "pl"
}
-->
# EdgeAI dla Początkujących: Ścieżki Nauki i Harmonogram Nauki

### Skoncentrowana Ścieżka Nauki (1 tydzień)

| Dzień | Temat | Szacowany Czas |
|-------|-------|----------------|
| Dzień 1 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 2 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 3 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 4-5 | Moduł 4: Optymalizacja Modeli (6 frameworków) | 4 godziny |
| Dzień 6 | Moduł 5: SLMOps | 3 godziny |
| Dzień 7 | Moduł 6-7: Agenci AI i Narzędzia Rozwojowe | 4 godziny |
| Dzień 8 | Moduł 8: Foundry Local Toolkit (Nowoczesna Implementacja) | 1 godzina |

### Skoncentrowana Ścieżka Nauki (2 tygodnie)

| Dzień | Temat | Szacowany Czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja Modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Agenci AI | 2 godziny |
| Dzień 13-14 | Moduł 7: Narzędzia Rozwojowe | 3 godziny |

### Nauka w Niepełnym Wymiarze (4 tygodnie)

| Tydzień | Temat | Szacowany Czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i Fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i Optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i Agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia Rozwojowe i Integracja | 3 godziny |

| Dzień | Temat | Szacowany Czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja Modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Systemy Agentowe SLM | 2 godziny |
| Dzień 13-14 | Moduł 7: Przykłady Implementacji EdgeAI | 2 godziny |

| Moduł | Data Zakończenia | Czas Spędzony | Kluczowe Wnioski |
|-------|------------------|---------------|------------------|
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Fundamenty SLM | | | |
| Moduł 3: Wdrażanie SLM | | | |
| Moduł 4: Optymalizacja Modeli (6 frameworków) | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Systemy Agentowe SLM | | | |
| Moduł 7: Przykłady Implementacji EdgeAI | | | |
| Ćwiczenia Praktyczne | | | |
| Mini-Projekt | | | |

### Nauka w Niepełnym Wymiarze (4 tygodnie)

| Tydzień | Temat | Szacowany Czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i Fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i Optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i Agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia Rozwojowe i Integracja | 3 godziny |

## Wprowadzenie

Witamy w przewodniku nauki EdgeAI dla początkujących! Ten dokument został zaprojektowany, aby pomóc Ci skutecznie przejść przez materiały kursu i maksymalnie wykorzystać proces nauki. Zawiera uporządkowane ścieżki nauki, sugerowane harmonogramy, podsumowania kluczowych koncepcji oraz dodatkowe zasoby, które pogłębią Twoją wiedzę na temat technologii EdgeAI.

Jest to zwięzły kurs trwający 20 godzin, który dostarcza podstawowej wiedzy o EdgeAI w efektywnym czasowo formacie, idealnym dla zapracowanych profesjonalistów i studentów, którzy chcą szybko zdobyć praktyczne umiejętności w tej rozwijającej się dziedzinie.

## Przegląd Kursu

Kurs jest podzielony na siedem kompleksowych modułów:

1. **Podstawy EdgeAI i Transformacja** - Zrozumienie kluczowych koncepcji i zmiany technologicznej
2. **Fundamenty Małych Modeli Językowych (SLM)** - Eksploracja różnych rodzin SLM i ich architektur
3. **Wdrażanie Małych Modeli Językowych** - Praktyczne strategie wdrażania
4. **Konwersja Formatów Modeli i Kwantyzacja** - Zaawansowana optymalizacja z użyciem 6 frameworków, w tym OpenVINO
5. **SLMOps - Operacje Małych Modeli Językowych** - Zarządzanie cyklem życia produkcji i wdrażania
6. **Systemy Agentowe SLM** - Agenci AI, wywoływanie funkcji i protokół kontekstu modelu
7. **Przykłady Implementacji EdgeAI** - Narzędzia AI, rozwój na Windows i implementacje specyficzne dla platform
8. **Microsoft Foundry Local – Kompletny Zestaw Narzędzi dla Programistów** - Rozwój lokalny z hybrydową integracją Azure (Moduł 08)

## Jak Korzystać z Tego Przewodnika Nauki

- **Nauka Progresywna**: Przechodź przez moduły w kolejności, aby uzyskać najbardziej spójne doświadczenie nauki
- **Punkty Kontroli Wiedzy**: Korzystaj z pytań samooceny po każdej sekcji
- **Praktyka**: Wykonuj sugerowane ćwiczenia, aby utrwalić teoretyczne koncepcje
- **Dodatkowe Zasoby**: Eksploruj dodatkowe materiały na tematy, które najbardziej Cię interesują

## Rekomendacje Harmonogramu Nauki

### Skoncentrowana Ścieżka Nauki (1 tydzień)

| Dzień | Temat | Szacowany Czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 6 godzin |
| Dzień 3-4 | Moduł 2: Fundamenty SLM | 8 godzin |
| Dzień 5 | Moduł 3: Wdrażanie SLM | 3 godziny |
| Dzień 6 | Moduł 8: Foundry Local Toolkit | 3 godziny |

### Nauka w Niepełnym Wymiarze (3 tygodnie)

| Tydzień | Temat | Szacowany Czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1: Podstawy EdgeAI | 6-7 godzin |
| Tydzień 2 | Moduł 2: Fundamenty SLM | 7-8 godzin |
| Tydzień 3 | Moduł 3: Wdrażanie SLM (3h) + Moduł 8: Foundry Local Toolkit (2-3h) | 5-6 godzin |

## Moduł 1: Podstawy EdgeAI i Transformacja

### Kluczowe Cele Nauki

- Zrozumienie różnic między AI opartym na chmurze a AI opartym na urządzeniach brzegowych
- Opanowanie podstawowych technik optymalizacji dla środowisk o ograniczonych zasobach
- Analiza rzeczywistych zastosowań technologii EdgeAI
- Konfiguracja środowiska rozwojowego dla projektów EdgeAI

### Obszary Nauki

#### Sekcja 1: Podstawy EdgeAI
- **Kluczowe Koncepcje**: 
  - Paradigmaty obliczeń brzegowych vs. chmurowych
  - Techniki kwantyzacji modeli
  - Opcje akceleracji sprzętowej (NPU, GPU, CPU)
  - Zalety w zakresie prywatności i bezpieczeństwa

- **Dodatkowe Materiały**:
  - [TensorFlow Lite Dokumentacja](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Dokumentacja](https://docs.edgeimpulse.com)

#### Sekcja 2: Studium Przypadków
- **Kluczowe Koncepcje**: 
  - Ekosystem modeli Microsoft Phi & Mu
  - Praktyczne implementacje w różnych branżach
  - Rozważania dotyczące wdrażania

#### Sekcja 3: Przewodnik Praktycznej Implementacji
- **Kluczowe Koncepcje**: 
  - Konfiguracja środowiska rozwojowego
  - Narzędzia do kwantyzacji i optymalizacji
  - Metody oceny implementacji EdgeAI

#### Sekcja 4: Sprzęt do Wdrażania Edge
- **Kluczowe Koncepcje**: 
  - Porównania platform sprzętowych
  - Strategie optymalizacji dla konkretnego sprzętu
  - Rozważania dotyczące wdrażania

### Pytania Samooceny

1. Porównaj i skontrastuj implementacje AI oparte na chmurze i na urządzeniach brzegowych.
2. Wyjaśnij trzy kluczowe techniki optymalizacji modeli dla wdrożeń brzegowych.
3. Jakie są główne zalety uruchamiania modeli AI na urządzeniach brzegowych?
4. Opisz proces kwantyzacji modelu i jego wpływ na wydajność.
5. Wyjaśnij, jak różne akceleratory sprzętowe (NPU, GPU, CPU) wpływają na wdrażanie EdgeAI.

### Ćwiczenia Praktyczne

1. **Szybka Konfiguracja Środowiska**: Skonfiguruj minimalne środowisko rozwojowe z niezbędnymi pakietami (30 minut)
2. **Eksploracja Modelu**: Pobierz i przeanalizuj wstępnie wytrenowany mały model językowy (1 godzina)
3. **Podstawowa Kwantyzacja**: Wypróbuj prostą kwantyzację na małym modelu (1 godzina)

## Moduł 2: Fundamenty Małych Modeli Językowych

### Kluczowe Cele Nauki

- Zrozumienie zasad architektonicznych różnych rodzin SLM
- Porównanie możliwości modeli w różnych skalach parametrów
- Ocena modeli pod kątem efektywności, możliwości i wymagań wdrożeniowych
- Rozpoznanie odpowiednich zastosowań dla różnych rodzin modeli

### Obszary Nauki

#### Sekcja 1: Rodzina Modeli Microsoft Phi
- **Kluczowe Koncepcje**: 
  - Ewolucja filozofii projektowania
  - Architektura zorientowana na efektywność
  - Specjalistyczne możliwości

#### Sekcja 2: Rodzina Qwen
- **Kluczowe Koncepcje**: 
  - Wkład open source
  - Skalowalne opcje wdrażania
  - Zaawansowana architektura rozumowania

#### Sekcja 3: Rodzina Gemma
- **Kluczowe Koncepcje**: 
  - Innowacje napędzane badaniami
  - Możliwości multimodalne
  - Optymalizacja mobilna

#### Sekcja 4: Rodzina BitNET
- **Kluczowe Koncepcje**: 
  - Technologia kwantyzacji 1-bitowej
  - Framework optymalizacji inferencji
  - Rozważania dotyczące zrównoważonego rozwoju

#### Sekcja 5: Model Microsoft Mu
- **Kluczowe Koncepcje**: 
  - Architektura zorientowana na urządzenia
  - Integracja systemowa z Windows
  - Operacja chroniąca prywatność

#### Sekcja 6: Phi-Silica
- **Kluczowe Koncepcje**: 
  - Architektura zoptymalizowana pod kątem NPU
  - Metryki wydajności
  - Integracja dla programistów

### Pytania Samooceny

1. Porównaj podejścia architektoniczne rodzin modeli Phi i Qwen.
2. Wyjaśnij, jak technologia kwantyzacji BitNET różni się od tradycyjnej kwantyzacji.
3. Jakie są unikalne zalety modelu Mu dla integracji z Windows?
4. Opisz, jak Phi-Silica wykorzystuje sprzęt NPU do optymalizacji wydajności.
5. Dla aplikacji mobilnej z ograniczoną łącznością, która rodzina modeli byłaby najbardziej odpowiednia i dlaczego?

### Ćwiczenia Praktyczne

1. **Porównanie Modeli**: Szybki benchmark dwóch różnych modeli SLM (1 godzina)
2. **Prosta Generacja Tekstu**: Podstawowa implementacja generacji tekstu z małym modelem (1 godzina)
3. **Szybka Optymalizacja**: Zastosowanie jednej techniki optymalizacji w celu poprawy szybkości inferencji (1 godzina)

## Moduł 3: Wdrażanie Małych Modeli Językowych

### Kluczowe Cele Nauki

- Wybór odpowiednich modeli na podstawie ograniczeń wdrożeniowych
- Opanowanie technik optymalizacji dla różnych scenariuszy wdrożeniowych
- Implementacja SLM w środowiskach lokalnych i chmurowych
- Projektowanie konfiguracji gotowych do produkcji dla aplikacji EdgeAI

### Obszary Nauki

#### Sekcja 1: Zaawansowana Nauka SLM
- **Kluczowe Koncepcje**: 
  - Ramy klasyfikacji parametrów
  - Zaawansowane techniki optymalizacji
  - Strategie pozyskiwania modeli

#### Sekcja 2: Wdrażanie w Środowisku Lokalnym
- **Kluczowe Koncepcje**: 
  - Wdrażanie na platformie Ollama
  - Rozwiązania lokalne Microsoft Foundry
  - Analiza porównawcza frameworków

#### Sekcja 3: Wdrażanie w Chmurze z Konteneryzacją
- **Kluczowe Koncepcje**: 
  - Inferencja wysokiej wydajności vLLM
  - Orkiestracja kontenerów
  - Implementacja ONNX Runtime

### Pytania Samooceny

1. Jakie czynniki należy wziąć pod uwagę przy wyborze między wdrożeniem lokalnym a chmurowym?
2. Porównaj Ollama i Microsoft Foundry Local jako opcje wdrożeniowe.
3. Wyjaśnij korzyści płynące z konteneryzacji dla wdrożenia SLM.
4. Jakie są kluczowe metryki wydajności do monitorowania dla SLM wdrożonego na urządzeniach brzegowych?
5. Opisz kompletny workflow wdrożeniowy od wyboru modelu do implementacji produkcyjnej.

### Ćwiczenia Praktyczne

1. **Podstawowe Wdrożenie Lokalne**: Wdróż prosty SLM za pomocą Ollama (1 godzina)
2. **Sprawdzenie Wydajności**: Przeprowadź szybki benchmark na wdrożonym modelu (30 minut)
3. **Prosta Integracja**: Stwórz minimalną aplikację korzystającą z wdrożonego modelu (1 godzina)

## Moduł 4: Konwersja Formatów Modeli i Kwantyzacja

### Kluczowe Cele Nauki

- Opanowanie zaawansowanych technik kwantyzacji od precyzji 1-bitowej do 8-bitowej
- Zrozumienie strategii konwersji formatów (GGUF, ONNX)
- Implementacja optymalizacji w sześciu frameworkach (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)
- Wdrażanie zoptymalizowanych modeli w środowiskach produkcyjnych na urządzeniach brzegowych, w tym Intel, Apple i sprzętach wieloplatformowych
- OpenVINO GenAI dla wdrożenia LLM

#### Sekcja 5: Framework Apple MLX
- **Kluczowe pojęcia**: 
  - Optymalizacja dla Apple Silicon
  - Zunifikowana architektura pamięci
  - Możliwości dostrajania LoRA

#### Sekcja 6: Synteza przepływu pracy Edge AI
- **Kluczowe pojęcia**: 
  - Zunifikowana architektura przepływu pracy
  - Drzewa decyzyjne wyboru frameworków
  - Walidacja gotowości produkcyjnej
  - Strategie przyszłościowe

### Pytania do samooceny

1. Porównaj strategie kwantyzacji na różnych poziomach precyzji (od 1-bit do 8-bit).
2. Wyjaśnij zalety formatu GGUF dla wdrożeń na krawędzi.
3. Jak optymalizacja uwzględniająca sprzęt w Microsoft Olive poprawia efektywność wdrożeń?
4. Jakie są kluczowe korzyści z użycia NNCF w OpenVINO do kompresji modeli?
5. Opisz, jak Apple MLX wykorzystuje zunifikowaną architekturę pamięci do optymalizacji.
6. Jak synteza przepływu pracy pomaga w wyborze optymalnych frameworków optymalizacyjnych?

### Ćwiczenia praktyczne

1. **Kwantyzacja modelu**: Zastosuj różne poziomy kwantyzacji do modelu i porównaj wyniki (1 godzina)
2. **Optymalizacja OpenVINO**: Użyj NNCF do kompresji modelu dla sprzętu Intel (1 godzina)
3. **Porównanie frameworków**: Przetestuj ten sam model w trzech różnych frameworkach optymalizacyjnych (1 godzina)
4. **Benchmark wydajności**: Zmierz wpływ optymalizacji na szybkość inferencji i użycie pamięci (1 godzina)

## Moduł 5: SLMOps - Operacje na Małych Modelach Językowych

### Kluczowe cele nauki

- Zrozumienie zasad zarządzania cyklem życia SLMOps
- Opanowanie technik destylacji i dostrajania dla wdrożeń na krawędzi
- Implementacja strategii wdrożenia produkcyjnego z monitorowaniem
- Tworzenie operacji i przepływów konserwacyjnych klasy korporacyjnej dla SLM

### Obszary do nauki

#### Sekcja 1: Wprowadzenie do SLMOps
- **Kluczowe pojęcia**: 
  - Przełomowy paradygmat SLMOps w operacjach AI
  - Efektywność kosztowa i architektura z priorytetem prywatności
  - Strategiczny wpływ biznesowy i przewagi konkurencyjne

#### Sekcja 2: Destylacja modelu
- **Kluczowe pojęcia**: 
  - Techniki transferu wiedzy
  - Implementacja procesu destylacji dwustopniowej
  - Przepływy pracy destylacji w Azure ML

#### Sekcja 3: Strategie dostrajania
- **Kluczowe pojęcia**: 
  - Dostrajanie efektywne pod względem parametrów (PEFT)
  - Zaawansowane metody LoRA i QLoRA
  - Trening wieloadapterowy i optymalizacja hiperparametrów

#### Sekcja 4: Wdrożenie produkcyjne
- **Kluczowe pojęcia**: 
  - Konwersja i kwantyzacja modelu na potrzeby produkcji
  - Konfiguracja wdrożenia Foundry Local
  - Benchmark wydajności i walidacja jakości

### Pytania do samooceny

1. Jak SLMOps różni się od tradycyjnego MLOps?
2. Wyjaśnij korzyści z destylacji modelu dla wdrożeń na krawędzi.
3. Jakie są kluczowe aspekty dostrajania SLM w środowiskach o ograniczonych zasobach?
4. Opisz kompletny pipeline wdrożenia produkcyjnego dla aplikacji Edge AI.

### Ćwiczenia praktyczne

1. **Podstawowa destylacja**: Utwórz mniejszy model na podstawie większego modelu nauczyciela (1 godzina)
2. **Eksperyment z dostrajaniem**: Dostosuj model do konkretnej dziedziny (1 godzina)
3. **Pipeline wdrożeniowy**: Skonfiguruj podstawowy pipeline CI/CD dla wdrożenia modelu (1 godzina)

## Moduł 6: Systemy Agentowe SLM - Agenci AI i Wywoływanie Funkcji

### Kluczowe cele nauki

- Tworzenie inteligentnych agentów AI dla środowisk krawędziowych z użyciem Małych Modeli Językowych
- Implementacja możliwości wywoływania funkcji w ramach systematycznych przepływów pracy
- Opanowanie integracji Model Context Protocol (MCP) dla standaryzowanej interakcji z narzędziami
- Tworzenie zaawansowanych systemów agentowych z minimalną interwencją człowieka

### Obszary do nauki

#### Sekcja 1: Agenci AI i podstawy SLM
- **Kluczowe pojęcia**: 
  - Ramy klasyfikacji agentów (refleksyjni, oparte na modelu, oparte na celach, uczący się agenci)
  - Analiza kompromisów między SLM a LLM
  - Wzorce projektowe agentów specyficzne dla krawędzi
  - Optymalizacja zasobów dla agentów

#### Sekcja 2: Wywoływanie funkcji w Małych Modelach Językowych
- **Kluczowe pojęcia**: 
  - Implementacja systematycznych przepływów pracy (detekcja intencji, wyjście JSON, wykonanie zewnętrzne)
  - Implementacje specyficzne dla platform (Phi-4-mini, wybrane modele Qwen, Microsoft Foundry Local)
  - Zaawansowane przykłady (współpraca wieloagentowa, dynamiczny wybór narzędzi)
  - Rozważania produkcyjne (ograniczenia szybkości, logowanie audytowe, środki bezpieczeństwa)

#### Sekcja 3: Integracja Model Context Protocol (MCP)
- **Kluczowe pojęcia**: 
  - Architektura protokołu i projektowanie warstwowego systemu
  - Obsługa wielu backendów (Ollama dla rozwoju, vLLM dla produkcji)
  - Protokoły połączeń (tryby STDIO i SSE)
  - Zastosowania w rzeczywistości (automatyzacja webowa, przetwarzanie danych, integracja API)

### Pytania do samooceny

1. Jakie są kluczowe aspekty architektoniczne dla agentów AI na krawędzi?
2. Jak wywoływanie funkcji zwiększa możliwości agentów?
3. Wyjaśnij rolę Model Context Protocol w komunikacji agentów.

### Ćwiczenia praktyczne

1. **Prosty agent**: Zbuduj podstawowego agenta AI z wywoływaniem funkcji (1 godzina)
2. **Integracja MCP**: Zaimplementuj MCP w aplikacji agenta (30 minut)

## Moduł 7: Przykłady implementacji EdgeAI

### Kluczowe cele nauki

- Opanowanie AI Toolkit dla Visual Studio Code dla kompleksowych przepływów pracy EdgeAI
- Zdobycie wiedzy na temat platformy Windows AI Foundry i strategii optymalizacji NPU
- Implementacja EdgeAI na różnych platformach sprzętowych i scenariuszach wdrożeniowych
- Tworzenie aplikacji EdgeAI gotowych do produkcji z optymalizacjami specyficznymi dla platformy

### Obszary do nauki

#### Sekcja 1: AI Toolkit dla Visual Studio Code
- **Kluczowe pojęcia**: 
  - Kompleksowe środowisko rozwoju Edge AI w VS Code
  - Katalog modeli i ich odkrywanie dla wdrożeń na krawędzi
  - Lokalne testowanie, optymalizacja i przepływy pracy rozwoju agentów
  - Monitorowanie wydajności i ocena dla scenariuszy krawędziowych

#### Sekcja 2: Przewodnik rozwoju EdgeAI dla Windows
- **Kluczowe pojęcia**: 
  - Kompleksowy przegląd platformy Windows AI Foundry
  - API Phi Silica dla efektywnego wnioskowania NPU
  - API Computer Vision dla przetwarzania obrazów i OCR
  - Foundry Local CLI dla lokalnego rozwoju i testowania

#### Sekcja 3: Implementacje specyficzne dla platformy
- **Kluczowe pojęcia**: 
  - Wdrożenie NVIDIA Jetson Orin Nano (wydajność AI 67 TOPS)
  - Aplikacje mobilne z .NET MAUI i ONNX Runtime GenAI
  - Rozwiązania Azure EdgeAI z hybrydową architekturą chmura-krawędź
  - Optymalizacja Windows ML z uniwersalnym wsparciem sprzętowym
  - Aplikacje Foundry Local z implementacją RAG z priorytetem prywatności

### Pytania do samooceny

1. Jak AI Toolkit usprawnia przepływ pracy rozwoju EdgeAI?
2. Porównaj strategie wdrożeniowe na różnych platformach sprzętowych.
3. Jakie są zalety Windows AI Foundry dla rozwoju na krawędzi?
4. Wyjaśnij rolę optymalizacji NPU w nowoczesnych aplikacjach Edge AI.
5. Jak API Phi Silica wykorzystuje sprzęt NPU do optymalizacji wydajności?
6. Porównaj korzyści z lokalnego i chmurowego wdrożenia dla aplikacji wrażliwych na prywatność.

### Ćwiczenia praktyczne

1. **Konfiguracja AI Toolkit**: Skonfiguruj AI Toolkit i zoptymalizuj model (1 godzina)
2. **Windows AI Foundry**: Zbuduj prostą aplikację AI dla Windows z użyciem API Phi Silica (1 godzina)
3. **Wdrożenie międzyplatformowe**: Wdróż ten sam model na dwóch różnych platformach (1 godzina)
4. **Optymalizacja NPU**: Przetestuj wydajność NPU za pomocą narzędzi Windows AI Foundry (30 minut)

## Moduł 8: Microsoft Foundry Local – Kompletny zestaw narzędzi dla deweloperów (zmodernizowany)

### Kluczowe cele nauki

- Instalacja i konfiguracja Foundry Local z nowoczesną integracją SDK
- Implementacja zaawansowanych systemów wieloagentowych z wzorcami koordynatorów
- Tworzenie inteligentnych routerów modeli z automatycznym wyborem opartym na zadaniach
- Wdrożenie gotowych do produkcji rozwiązań AI z kompleksowym monitorowaniem
- Integracja z Azure AI Foundry dla scenariuszy hybrydowego wdrożenia
- Opanowanie nowoczesnych wzorców SDK z FoundryLocalManager i klientem OpenAI

### Obszary do nauki

#### Sekcja 1: Nowoczesna instalacja i konfiguracja
- **Kluczowe pojęcia**: 
  - Integracja SDK FoundryLocalManager
  - Automatyczne wykrywanie usług i monitorowanie stanu
  - Wzorce konfiguracji oparte na środowisku
  - Rozważania dotyczące wdrożenia produkcyjnego

#### Sekcja 2: Zaawansowane systemy wieloagentowe
- **Kluczowe pojęcia**: 
  - Wzorzec koordynatora z wyspecjalizowanymi agentami
  - Specjalizacja agentów w zakresie pobierania, rozumowania i wykonywania
  - Mechanizmy pętli zwrotnej dla udoskonalania
  - Monitorowanie wydajności i śledzenie statystyk

#### Sekcja 3: Inteligentne trasowanie modeli
- **Kluczowe pojęcia**: 
  - Algorytmy wyboru modeli oparte na słowach kluczowych
  - Obsługa wielu modeli (ogólne, rozumowanie, kod, kreatywne)
  - Konfiguracja zmiennych środowiskowych dla elastyczności
  - Sprawdzanie stanu usług i obsługa błędów

#### Sekcja 4: Implementacja gotowa do produkcji
- **Kluczowe pojęcia**: 
  - Kompleksowa obsługa błędów i mechanizmy awaryjne
  - Monitorowanie żądań i śledzenie wydajności
  - Interaktywne przykłady w Jupyter notebook z benchmarkami
  - Wzorce integracji z istniejącymi aplikacjami

### Pytania do samooceny

1. Jak nowoczesne podejście FoundryLocalManager różni się od ręcznych wywołań REST?
2. Wyjaśnij wzorzec koordynatora i sposób, w jaki organizuje wyspecjalizowanych agentów.
3. Jak inteligentny router wybiera odpowiednie modele na podstawie treści zapytania?
4. Jakie są kluczowe komponenty systemu agentowego gotowego do produkcji?
5. Jak zaimplementować kompleksowe monitorowanie stanu usług Foundry Local?
6. Porównaj korzyści zmodernizowanego podejścia z tradycyjnymi wzorcami implementacji.

### Ćwiczenia praktyczne

1. **Konfiguracja nowoczesnego SDK**: Skonfiguruj FoundryLocalManager z automatycznym wykrywaniem usług (30 minut)
2. **System wieloagentowy**: Uruchom zaawansowany koordynator z wyspecjalizowanymi agentami (30 minut)
3. **Inteligentne trasowanie**: Przetestuj router modeli z różnymi typami zapytań (30 minut)
4. **Interaktywna eksploracja**: Użyj Jupyter notebooków do eksploracji zaawansowanych funkcji (45 minut)
5. **Wdrożenie produkcyjne**: Zaimplementuj wzorce monitorowania i obsługi błędów (30 minut)
6. **Integracja hybrydowa**: Skonfiguruj scenariusze awaryjne Azure AI Foundry (30 minut)

## Przewodnik alokacji czasu

Aby jak najlepiej wykorzystać 20-godzinny czas kursu, oto sugerowany podział czasu:

| Aktywność | Alokacja czasu | Opis |
|----------|----------------|-------------|
| Czytanie materiałów podstawowych | 9 godzin | Skupienie na kluczowych pojęciach w każdym module |
| Ćwiczenia praktyczne | 6 godzin | Praktyczna implementacja kluczowych technik |
| Samoocena | 2 godziny | Testowanie zrozumienia poprzez pytania i refleksję |
| Mini-projekt | 3 godziny | Zastosowanie wiedzy w małej praktycznej implementacji |

### Kluczowe obszary w zależności od ograniczeń czasowych

**Jeśli masz tylko 10 godzin:**
- Ukończ moduły 1, 2 i 3 (podstawowe pojęcia EdgeAI)
- Wykonaj co najmniej jedno ćwiczenie praktyczne na moduł
- Skup się na zrozumieniu kluczowych pojęć zamiast szczegółów implementacji

**Jeśli możesz poświęcić pełne 20 godzin:**
- Ukończ wszystkie siedem modułów
- Wykonaj kluczowe ćwiczenia praktyczne z każdego modułu
- Ukończ jeden mini-projekt z modułu 7
- Przeglądaj co najmniej 2-3 dodatkowe zasoby

**Jeśli masz więcej niż 20 godzin:**
- Ukończ wszystkie moduły z szczegółowymi ćwiczeniami
- Zbuduj wiele mini-projektów
- Eksploruj zaawansowane techniki optymalizacji w module 4
- Zaimplementuj wdrożenie produkcyjne z modułu 5

## Kluczowe zasoby

Te starannie wybrane zasoby zapewniają największą wartość w ograniczonym czasie nauki:

### Dokumentacja obowiązkowa
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Najbardziej efektywne narzędzie do optymalizacji modeli
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Najszybszy sposób na lokalne wdrożenie SLM
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Odniesienie do wiodącego modelu zoptymalizowanego dla krawędzi
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kompleksowy zestaw narzędzi optymalizacyjnych Intela
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Zintegrowane środowisko rozwoju EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Specyficzna dla Windows platforma rozwoju EdgeAI

### Narzędzia oszczędzające czas
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Szybki dostęp do modeli i ich wdrożenie
- [Gradio](https://www.gradio.app/docs/interface) - Szybki rozwój interfejsów użytkownika dla demonstracji AI
- [Microsoft Olive](https://github.com/microsoft/Olive) - Uproszczona optymalizacja modeli
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efektywne wnioskowanie na CPU
- [OpenVINO NNC
| Ćwiczenia praktyczne | | | |
| Mini-Projekt | | | |

## Pomysły na Mini-Projekty

Rozważ realizację jednego z poniższych projektów, aby przećwiczyć koncepcje EdgeAI (każdy zaprojektowany na 2-4 godziny pracy):

### Projekty dla początkujących (2-3 godziny każdy)
1. **Asystent Tekstowy na Krawędzi**: Stwórz prosty narzędziowy model do uzupełniania tekstu offline, korzystając z małego modelu językowego
2. **Dashboard Porównania Modeli**: Zbuduj podstawową wizualizację metryk wydajności dla różnych SLM-ów
3. **Eksperyment Optymalizacyjny**: Zmierz wpływ różnych poziomów kwantyzacji na ten sam model bazowy

### Projekty średniozaawansowane (3-4 godziny każdy)
4. **Przepływ pracy AI Toolkit**: Użyj VS Code AI Toolkit, aby zoptymalizować i wdrożyć model od początku do końca
5. **Aplikacja Windows AI Foundry**: Stwórz aplikację Windows, korzystając z API Phi Silica i optymalizacji NPU
6. **Wdrożenie międzyplatformowe**: Wdróż ten sam zoptymalizowany model na Windows (OpenVINO) i urządzenia mobilne (.NET MAUI)
7. **Agent Wywołujący Funkcje**: Zbuduj agenta AI z możliwością wywoływania funkcji dla scenariuszy na krawędzi

### Zaawansowane projekty integracyjne (4-5 godzin każdy)
8. **Pipeline Optymalizacji OpenVINO**: Zaimplementuj kompletną optymalizację modelu, korzystając z NNCF i narzędzi GenAI
9. **Pipeline SLMOps**: Zaimplementuj pełny cykl życia modelu od treningu po wdrożenie na krawędzi
10. **System Edge z Wieloma Modelami**: Wdróż wiele wyspecjalizowanych modeli współpracujących na sprzęcie krawędziowym
11. **System Integracji MCP**: Zbuduj system agentowy, korzystając z Model Context Protocol do interakcji z narzędziami

## Źródła

- Microsoft Learn (Foundry Local)
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Pierwsze kroki: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Referencja CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integracja z SDK inferencji: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Jak otworzyć WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Kompilacja modeli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenci (przegląd): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Narzędzia optymalizacji i inferencji
  - Microsoft Olive (dokumentacja): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (pierwsze kroki): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integracja ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (dokumentacja): https://docs.openvino.ai/2025/index.html
  - Apple MLX (dokumentacja): https://ml-explore.github.io/mlx/build/html/index.html
- Frameworki wdrożeniowe i modele
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (dokumentacja): https://docs.vllm.ai/
  - Ollama (pierwsze kroki): https://github.com/ollama/ollama#get-started
- Narzędzia dla deweloperów (Windows i VS Code)
  - AI Toolkit dla VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (przegląd): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Społeczność edukacyjna

Dołącz do dyskusji i nawiąż kontakt z innymi uczącymi się:
- Dyskusje na GitHub w [repozytorium EdgeAI dla początkujących](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Podsumowanie

EdgeAI to nowoczesny kierunek wdrażania sztucznej inteligencji, który przenosi potężne możliwości bezpośrednio na urządzenia, jednocześnie rozwiązując kluczowe kwestie związane z prywatnością, opóźnieniami i łącznością. Ten 20-godzinny kurs dostarcza niezbędnej wiedzy i praktycznych umiejętności, aby natychmiast rozpocząć pracę z technologiami EdgeAI.

Kurs jest celowo zwięzły i skoncentrowany na najważniejszych koncepcjach, co pozwala szybko zdobyć cenną wiedzę bez przytłaczającego nakładu czasu. Pamiętaj, że praktyka, nawet z prostymi przykładami, jest kluczem do utrwalenia zdobytej wiedzy.

Powodzenia w nauce!

---

