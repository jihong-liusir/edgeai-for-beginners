<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-10-08T21:26:29+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "pl"
}
-->
# EdgeAI dla początkujących: Ścieżki nauki i harmonogram nauki

### Skoncentrowana ścieżka nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 2 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 3 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 4-5 | Moduł 4: Optymalizacja modeli (6 frameworków) | 4 godziny |
| Dzień 6 | Moduł 5: SLMOps | 3 godziny |
| Dzień 7 | Moduł 6-7: Agenci AI i narzędzia deweloperskie | 4 godziny |
| Dzień 8 | Moduł 8: Lokalny zestaw narzędzi Foundry (nowoczesna implementacja) | 1 godzina |

### Skoncentrowana ścieżka nauki (2 tygodnie)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Agenci AI | 2 godziny |
| Dzień 13-14 | Moduł 7: Narzędzia deweloperskie | 3 godziny |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia deweloperskie i integracja | 3 godziny |

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Systemy agentów SLM | 2 godziny |
| Dzień 13-14 | Moduł 7: Przykłady implementacji EdgeAI | 2 godziny |

| Moduł | Data ukończenia | Czas poświęcony | Kluczowe wnioski |
|-------|-----------------|-----------------|------------------|
| Moduł 0: Wprowadzenie do EdgeAI | | | |
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Fundamenty SLM | | | |
| Moduł 3: Wdrażanie SLM | | | |
| Moduł 4: Optymalizacja modeli (6 frameworków) | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Systemy agentów SLM | | | |
| Moduł 7: Przykłady implementacji EdgeAI | | | |
| Ćwiczenia praktyczne | | | |
| Mini-projekt | | | |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 1-2: Podstawy i fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia deweloperskie i integracja | 3 godziny |

## Wprowadzenie

Witamy w przewodniku nauki EdgeAI dla początkujących! Ten dokument został zaprojektowany, aby pomóc Ci skutecznie przejść przez materiały kursu i maksymalnie wykorzystać proces nauki. Zawiera uporządkowane ścieżki nauki, sugerowane harmonogramy, podsumowania kluczowych koncepcji oraz dodatkowe zasoby, które pogłębią Twoje zrozumienie technologii Edge AI.

Jest to zwięzły kurs trwający 20 godzin, który dostarcza podstawowej wiedzy o EdgeAI w efektywnym czasowo formacie, idealnym dla zapracowanych profesjonalistów i studentów, którzy chcą szybko zdobyć praktyczne umiejętności w tej rozwijającej się dziedzinie.

## Przegląd kursu

Kurs jest podzielony na osiem kompleksowych modułów:

0. **Wprowadzenie do EdgeAI** - Podstawy i kontekst z zastosowaniami w przemyśle oraz celami nauki  
1. **Podstawy i transformacja EdgeAI** - Zrozumienie kluczowych koncepcji i zmiany technologicznej  
2. **Fundamenty małych modeli językowych (SLM)** - Eksploracja różnych rodzin SLM i ich architektur  
3. **Wdrażanie małych modeli językowych (SLM)** - Praktyczne strategie wdrażania  
4. **Konwersja formatów modeli i kwantyzacja** - Zaawansowana optymalizacja z użyciem 6 frameworków, w tym OpenVINO  
5. **SLMOps - Operacje na małych modelach językowych** - Zarządzanie cyklem życia produkcji i wdrażania  
6. **Systemy agentów SLM** - Agenci AI, wywoływanie funkcji i protokół kontekstu modelu  
7. **Przykłady implementacji EdgeAI** - Zestaw narzędzi AI, rozwój na Windows i implementacje specyficzne dla platform  
8. **Microsoft Foundry Local – Kompletny zestaw narzędzi deweloperskich** - Rozwój lokalny z hybrydową integracją Azure (Moduł 08)

## Jak korzystać z tego przewodnika nauki

- **Nauka progresywna**: Przechodź przez moduły w kolejności, aby uzyskać najbardziej spójne doświadczenie nauki  
- **Punkty kontrolne wiedzy**: Korzystaj z pytań do samooceny po każdej sekcji  
- **Praktyka praktyczna**: Wykonuj sugerowane ćwiczenia, aby utrwalić teoretyczne koncepcje  
- **Dodatkowe zasoby**: Eksploruj dodatkowe materiały dla tematów, które najbardziej Cię interesują  

## Rekomendacje harmonogramu nauki

### Skoncentrowana ścieżka nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|-------|-------|----------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 6 godzin |
| Dzień 3-4 | Moduł 2: Fundamenty SLM | 8 godzin |
| Dzień 5 | Moduł 3: Wdrażanie SLM | 3 godziny |
| Dzień 6 | Moduł 8: Lokalny zestaw narzędzi Foundry | 3 godziny |

### Nauka w niepełnym wymiarze godzin (3 tygodnie)

| Tydzień | Temat | Szacowany czas |
|---------|-------|----------------|
| Tydzień 1 | Moduł 0: Wprowadzenie + Moduł 1: Podstawy EdgeAI | 7-9 godzin |
| Tydzień 2 | Moduł 2: Fundamenty SLM | 7-8 godzin |
| Tydzień 3 | Moduł 3: Wdrażanie SLM (3h) + Moduł 8: Lokalny zestaw narzędzi Foundry (2-3h) | 5-6 godzin |

## Moduł 0: Wprowadzenie do EdgeAI

### Kluczowe cele nauki

- Zrozumienie, czym jest Edge AI i dlaczego ma znaczenie w dzisiejszym krajobrazie technologicznym  
- Identyfikacja głównych branż przekształconych przez Edge AI i ich specyficznych zastosowań  
- Zrozumienie zalet małych modeli językowych (SLM) dla wdrożeń na krawędzi  
- Ustalenie jasnych oczekiwań i wyników nauki dla całego kursu  
- Rozpoznanie możliwości kariery i wymagań umiejętności w dziedzinie Edge AI  

### Obszary nauki

#### Sekcja 1: Paradygmat i definicja Edge AI
- **Kluczowe koncepcje**:  
  - Edge AI vs. tradycyjne przetwarzanie w chmurze  
  - Konwergencja sprzętu, optymalizacji modeli i wymagań biznesowych  
  - Wdrażanie AI w czasie rzeczywistym, z zachowaniem prywatności i efektywnością kosztową  

#### Sekcja 2: Zastosowania w przemyśle
- **Kluczowe koncepcje**:  
  - Produkcja i Przemysł 4.0: Predykcyjne utrzymanie ruchu i kontrola jakości  
  - Opieka zdrowotna: Diagnostyka obrazowa i monitorowanie pacjentów  
  - Systemy autonomiczne: Pojazdy autonomiczne i transport  
  - Inteligentne miasta: Zarządzanie ruchem i bezpieczeństwo publiczne  
  - Technologia konsumencka: Smartfony, urządzenia noszone i inteligentne domy  

#### Sekcja 3: Fundamenty małych modeli językowych
- **Kluczowe koncepcje**:  
  - Charakterystyka SLM i porównania wydajności  
  - Efektywność parametrów vs. kompromisy w zakresie możliwości  
  - Ograniczenia wdrożeń na krawędzi i strategie optymalizacji  

#### Sekcja 4: Ramy nauki i ścieżka kariery
- **Kluczowe koncepcje**:  
  - Architektura kursu i podejście do progresywnego opanowania materiału  
  - Cele techniczne i praktyczne wdrożenia  
  - Możliwości rozwoju kariery i zastosowania w przemyśle  

### Pytania do samooceny

1. Jakie są trzy główne trendy technologiczne, które umożliwiły Edge AI?  
2. Porównaj zalety i wyzwania Edge AI vs. AI opartego na chmurze.  
3. Wymień trzy branże, w których Edge AI dostarcza kluczową wartość biznesową i wyjaśnij dlaczego.  
4. Jak małe modele językowe sprawiają, że Edge AI jest praktyczne w rzeczywistych wdrożeniach?  
5. Jakie kluczowe umiejętności techniczne rozwiniesz podczas tego kursu?  
6. Opisz czterofazowe podejście do nauki stosowane w tym kursie.  

### Ćwiczenia praktyczne

1. **Badanie przemysłu**: Wybierz jedno zastosowanie przemysłowe i zbadaj rzeczywiste wdrożenie Edge AI (30 minut)  
2. **Eksploracja modeli**: Przeglądaj dostępne małe modele językowe na Hugging Face i porównaj ich liczbę parametrów oraz możliwości (30 minut)  
3. **Planowanie nauki**: Przejrzyj pełną strukturę kursu i stwórz swój osobisty harmonogram nauki (15 minut)  

### Dodatkowe materiały

- [Przegląd rynku Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Przegląd małych modeli językowych - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Fundamenty Edge Computing](https://www.edgecomputing.org/)  

## Moduł 1: Podstawy i transformacja EdgeAI

### Kluczowe cele nauki

- Zrozumienie różnic między AI opartym na chmurze a AI opartym na krawędzi  
- Opanowanie podstawowych technik optymalizacji dla środowisk o ograniczonych zasobach  
- Analiza rzeczywistych zastosowań technologii EdgeAI  
- Konfiguracja środowiska deweloperskiego dla projektów EdgeAI  

### Obszary nauki

#### Sekcja 1: Podstawy EdgeAI
- **Kluczowe koncepcje**:  
  - Paradygmaty Edge vs. Cloud computing  
  - Techniki kwantyzacji modeli  
  - Opcje akceleracji sprzętowej (NPU, GPU, CPU)  
  - Zalety w zakresie prywatności i bezpieczeństwa  

- **Dodatkowe materiały**:  
  - [Dokumentacja TensorFlow Lite](https://www.tensorflow.org/lite)  
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)  
  - [Dokumentacja Edge Impulse](https://docs.edgeimpulse.com)  

#### Sekcja 2: Studium przypadków
- **Kluczowe koncepcje**:  
  - Ekosystem modeli Microsoft Phi & Mu  
  - Praktyczne wdrożenia w różnych branżach  
  - Rozważania dotyczące wdrożeń  

#### Sekcja 3: Praktyczny przewodnik wdrożeniowy
- **Kluczowe koncepcje**:  
  - Konfiguracja środowiska deweloperskiego  
  - Narzędzia do kwantyzacji i optymalizacji  
  - Metody oceny wdrożeń EdgeAI  

#### Sekcja 4: Sprzęt do wdrożeń na krawędzi
- **Kluczowe koncepcje**:  
  - Porównania platform sprzętowych  
  - Strategie optymalizacji dla konkretnego sprzętu  
  - Rozważania dotyczące wdrożeń  

### Pytania do samooceny

1. Porównaj i skontrastuj AI oparte na chmurze z wdrożeniami AI na krawędzi.  
2. Wyjaśnij trzy kluczowe techniki optymalizacji modeli dla wdrożeń na krawędzi.  
3. Jakie są główne zalety uruchamiania modeli AI na krawędzi?  
4. Opisz proces kwantyzacji modelu i jego wpływ na wydajność.  
5. Wyjaśnij, jak różne akceleratory sprzętowe (NPU, GPU, CPU) wpływają na wdrożenia EdgeAI.  

### Ćwiczenia praktyczne

1. **Szybka konfiguracja środowiska**: Skonfiguruj minimalne środowisko deweloperskie z niezbędnymi pakietami (30 minut)  
2. **Eksploracja modeli**: Pobierz i przeanalizuj wstępnie wytrenowany mały model językowy (1 godzina)  
3. **Podstawowa kwantyzacja**: Wypróbuj prostą kwantyzację na małym modelu (1 godzina)  

## Moduł 2: Fundamenty małych modeli językowych

### Kluczowe cele nauki

- Zrozumienie zasad architektonicznych różnych rodzin SLM  
- Porównanie możliwości modeli w różnych skalach parametrów  
- Ocena modeli na podstawie efektywności, możliwości i wymagań wdrożeniowych  
- Rozpoznanie odpowiednich zastosowań dla różnych rodzin modeli  

### Obszary nauki

#### Sekcja 1: Rodzina modeli Microsoft Phi
- **Kluczowe koncepcje**:  
  - Ewolucja filozofii projektowania  
  - Architektura z naciskiem na efektywność  
  - Specjalistyczne możliwości  

#### Sekcja 2: Rodzina Qwen
- **Kluczowe koncepcje**:  
  - Wkład open source  
  - Opcje skalowalnych wdrożeń  
  - Zaawansowana architektura rozumowania  

#### Sekcja 3: Rodzina Gemma
- **Kluczowe koncepcje**:  
  - Innowacje napędzane badaniami  
  - Możliwości multimodalne  
  - Optymalizacja mobilna  


3. Jakie są unikalne zalety modelu Mu w integracji z Windows?
4. Opisz, w jaki sposób Phi-Silica wykorzystuje sprzęt NPU do optymalizacji wydajności.
5. Dla aplikacji mobilnej z ograniczoną łącznością, która rodzina modeli byłaby najbardziej odpowiednia i dlaczego?

### Ćwiczenia praktyczne

1. **Porównanie modeli**: Szybki benchmark dwóch różnych modeli SLM (1 godzina)
2. **Prosta generacja tekstu**: Podstawowa implementacja generacji tekstu z użyciem małego modelu (1 godzina)
3. **Szybka optymalizacja**: Zastosowanie jednej techniki optymalizacji w celu poprawy szybkości wnioskowania (1 godzina)

## Moduł 3: Wdrażanie Małych Modeli Językowych (SLM)

### Kluczowe cele nauki

- Wybór odpowiednich modeli w zależności od ograniczeń wdrożeniowych
- Opanowanie technik optymalizacji dla różnych scenariuszy wdrożeniowych
- Implementacja SLM w środowiskach lokalnych i chmurowych
- Projektowanie konfiguracji gotowych do produkcji dla aplikacji EdgeAI

### Obszary nauki

#### Sekcja 1: Zaawansowana nauka SLM
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji parametrów
  - Zaawansowane techniki optymalizacji
  - Strategie pozyskiwania modeli

#### Sekcja 2: Wdrażanie w środowisku lokalnym
- **Priorytetowe zagadnienia**: 
  - Wdrażanie na platformie Ollama
  - Lokalne rozwiązania Microsoft Foundry
  - Analiza porównawcza frameworków

#### Sekcja 3: Wdrażanie w chmurze z konteneryzacją
- **Priorytetowe zagadnienia**: 
  - Wnioskowanie wysokiej wydajności z vLLM
  - Orkiestracja kontenerów
  - Implementacja ONNX Runtime

### Pytania do samooceny

1. Jakie czynniki należy wziąć pod uwagę przy wyborze między wdrożeniem lokalnym a chmurowym?
2. Porównaj Ollama i Microsoft Foundry Local jako opcje wdrożeniowe.
3. Wyjaśnij korzyści płynące z konteneryzacji w przypadku wdrożenia SLM.
4. Jakie są kluczowe metryki wydajności do monitorowania dla SLM wdrożonego na brzegu sieci?
5. Opisz kompletny workflow wdrożeniowy od wyboru modelu do implementacji produkcyjnej.

### Ćwiczenia praktyczne

1. **Podstawowe wdrożenie lokalne**: Wdróż prosty SLM za pomocą Ollama (1 godzina)
2. **Sprawdzenie wydajności**: Przeprowadź szybki benchmark na wdrożonym modelu (30 minut)
3. **Prosta integracja**: Stwórz minimalną aplikację korzystającą z wdrożonego modelu (1 godzina)

## Moduł 4: Konwersja formatów modeli i kwantyzacja

### Kluczowe cele nauki

- Opanowanie zaawansowanych technik kwantyzacji od precyzji 1-bitowej do 8-bitowej
- Zrozumienie strategii konwersji formatów (GGUF, ONNX)
- Implementacja optymalizacji w sześciu frameworkach (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)
- Wdrażanie zoptymalizowanych modeli w środowiskach produkcyjnych na brzegu sieci dla sprzętu Intel, Apple i platform wielosystemowych

### Obszary nauki

#### Sekcja 1: Podstawy kwantyzacji
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji precyzji
  - Kompromis między wydajnością a dokładnością
  - Optymalizacja pamięci

#### Sekcja 2: Implementacja Llama.cpp
- **Priorytetowe zagadnienia**: 
  - Wdrażanie wieloplatformowe
  - Optymalizacja formatu GGUF
  - Techniki przyspieszenia sprzętowego

#### Sekcja 3: Microsoft Olive Suite
- **Priorytetowe zagadnienia**: 
  - Optymalizacja uwzględniająca sprzęt
  - Wdrożenie na poziomie przedsiębiorstwa
  - Zautomatyzowane workflow optymalizacyjne

#### Sekcja 4: OpenVINO Toolkit
- **Priorytetowe zagadnienia**: 
  - Optymalizacja dla sprzętu Intel
  - Framework kompresji sieci neuronowych (NNCF)
  - Wdrażanie wnioskowania wieloplatformowego
  - OpenVINO GenAI dla wdrożenia LLM

#### Sekcja 5: Framework Apple MLX
- **Priorytetowe zagadnienia**: 
  - Optymalizacja dla Apple Silicon
  - Zunifikowana architektura pamięci
  - Możliwości fine-tuningu LoRA

#### Sekcja 6: Synteza workflow dla Edge AI
- **Priorytetowe zagadnienia**: 
  - Zunifikowana architektura workflow
  - Drzewa decyzyjne wyboru frameworków
  - Walidacja gotowości produkcyjnej
  - Strategie przyszłościowe

### Pytania do samooceny

1. Porównaj strategie kwantyzacji dla różnych poziomów precyzji (1-bit do 8-bit).
2. Wyjaśnij zalety formatu GGUF dla wdrożenia na brzegu sieci.
3. Jak optymalizacja uwzględniająca sprzęt w Microsoft Olive poprawia efektywność wdrożenia?
4. Jakie są kluczowe korzyści z NNCF OpenVINO dla kompresji modeli?
5. Opisz, w jaki sposób Apple MLX wykorzystuje zunifikowaną architekturę pamięci do optymalizacji.
6. Jak synteza workflow pomaga w wyborze optymalnych frameworków optymalizacyjnych?

### Ćwiczenia praktyczne

1. **Kwantyzacja modelu**: Zastosuj różne poziomy kwantyzacji do modelu i porównaj wyniki (1 godzina)
2. **Optymalizacja OpenVINO**: Użyj NNCF do kompresji modelu dla sprzętu Intel (1 godzina)
3. **Porównanie frameworków**: Przetestuj ten sam model w trzech różnych frameworkach optymalizacyjnych (1 godzina)
4. **Benchmark wydajności**: Zmierz wpływ optymalizacji na szybkość wnioskowania i użycie pamięci (1 godzina)

## Moduł 5: SLMOps - Operacje na Małych Modelach Językowych

### Kluczowe cele nauki

- Zrozumienie zasad zarządzania cyklem życia SLMOps
- Opanowanie technik destylacji i fine-tuningu dla wdrożenia na brzegu sieci
- Implementacja strategii wdrożenia produkcyjnego z monitorowaniem
- Budowa workflow operacyjnych i utrzymaniowych dla SLM na poziomie przedsiębiorstwa

### Obszary nauki

#### Sekcja 1: Wprowadzenie do SLMOps
- **Priorytetowe zagadnienia**: 
  - Przełomowy paradygmat SLMOps w operacjach AI
  - Architektura zorientowana na efektywność kosztową i prywatność
  - Strategiczny wpływ biznesowy i przewagi konkurencyjne

#### Sekcja 2: Destylacja modelu
- **Priorytetowe zagadnienia**: 
  - Techniki transferu wiedzy
  - Implementacja procesu destylacji dwustopniowej
  - Workflow destylacji w Azure ML

#### Sekcja 3: Strategie fine-tuningu
- **Priorytetowe zagadnienia**: 
  - Fine-tuning z efektywnym wykorzystaniem parametrów (PEFT)
  - Zaawansowane metody LoRA i QLoRA
  - Trening wieloadapterowy i optymalizacja hiperparametrów

#### Sekcja 4: Wdrożenie produkcyjne
- **Priorytetowe zagadnienia**: 
  - Konwersja i kwantyzacja modeli dla produkcji
  - Konfiguracja wdrożenia Foundry Local
  - Benchmark wydajności i walidacja jakości

### Pytania do samooceny

1. Jak SLMOps różni się od tradycyjnego MLOps?
2. Wyjaśnij korzyści z destylacji modeli dla wdrożenia na brzegu sieci.
3. Jakie są kluczowe aspekty fine-tuningu SLM w środowiskach o ograniczonych zasobach?
4. Opisz kompletny pipeline wdrożeniowy dla aplikacji Edge AI.

### Ćwiczenia praktyczne

1. **Podstawowa destylacja**: Stwórz mniejszy model na podstawie większego modelu nauczyciela (1 godzina)
2. **Eksperyment fine-tuningu**: Fine-tuning modelu dla konkretnej dziedziny (1 godzina)
3. **Pipeline wdrożeniowy**: Skonfiguruj podstawowy pipeline CI/CD dla wdrożenia modelu (1 godzina)

## Moduł 6: Systemy Agentowe SLM - Agenci AI i Wywoływanie Funkcji

### Kluczowe cele nauki

- Tworzenie inteligentnych agentów AI dla środowisk brzegowych z wykorzystaniem Małych Modeli Językowych
- Implementacja możliwości wywoływania funkcji z systematycznymi workflow
- Opanowanie integracji Model Context Protocol (MCP) dla standaryzowanej interakcji z narzędziami
- Tworzenie zaawansowanych systemów agentowych z minimalną interwencją człowieka

### Obszary nauki

#### Sekcja 1: Agenci AI i podstawy SLM
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji agentów (refleksyjni, oparte na modelu, oparte na celach, uczący się agenci)
  - Analiza kompromisów między SLM a LLM
  - Wzorce projektowe agentów specyficzne dla brzegu sieci
  - Optymalizacja zasobów dla agentów

#### Sekcja 2: Wywoływanie funkcji w Małych Modelach Językowych
- **Priorytetowe zagadnienia**: 
  - Implementacja systematycznego workflow (detekcja intencji, wyjście JSON, wykonanie zewnętrzne)
  - Implementacje specyficzne dla platform (Phi-4-mini, wybrane modele Qwen, Microsoft Foundry Local)
  - Zaawansowane przykłady (współpraca wieloagentowa, dynamiczny wybór narzędzi)
  - Rozważania produkcyjne (ograniczenia szybkości, logowanie audytowe, środki bezpieczeństwa)

#### Sekcja 3: Integracja Model Context Protocol (MCP)
- **Priorytetowe zagadnienia**: 
  - Architektura protokołu i projekt systemu warstwowego
  - Obsługa wielu backendów (Ollama dla rozwoju, vLLM dla produkcji)
  - Protokoły połączeń (tryby STDIO i SSE)
  - Zastosowania w rzeczywistości (automatyzacja webowa, przetwarzanie danych, integracja API)

### Pytania do samooceny

1. Jakie są kluczowe aspekty architektoniczne dla agentów AI na brzegu sieci?
2. W jaki sposób wywoływanie funkcji zwiększa możliwości agentów?
3. Wyjaśnij rolę Model Context Protocol w komunikacji agentów.

### Ćwiczenia praktyczne

1. **Prosty agent**: Stwórz podstawowego agenta AI z wywoływaniem funkcji (1 godzina)
2. **Integracja MCP**: Zaimplementuj MCP w aplikacji agenta (30 minut)

## Warsztat: Ścieżka nauki praktycznej

### Kluczowe cele nauki

- Tworzenie aplikacji AI gotowych do produkcji z wykorzystaniem Foundry Local SDK i najlepszych praktyk
- Implementacja kompleksowego zarządzania błędami i wzorców informacji zwrotnej od użytkownika
- Tworzenie pipeline RAG z oceną jakości i monitorowaniem wydajności
- Rozwój systemów wieloagentowych z wzorcami koordynatorów
- Opanowanie inteligentnego routingu modeli dla selekcji modeli opartych na zadaniach
- Wdrażanie rozwiązań AI z priorytetem lokalnym i architekturami chroniącymi prywatność

### Obszary nauki

#### Sesja 01: Rozpoczęcie pracy z Foundry Local
- **Priorytetowe zagadnienia**:
  - Integracja SDK FoundryLocalManager i automatyczne wykrywanie usług
  - Podstawowe i strumieniowe implementacje czatu
  - Wzorce obsługi błędów i informacje zwrotne od użytkownika
  - Konfiguracja oparta na środowisku

#### Sesja 02: Tworzenie rozwiązań AI z RAG
- **Priorytetowe zagadnienia**:
  - Wektory w pamięci z sentence-transformers
  - Implementacja pipeline RAG (pobierz → wygeneruj)
  - Ocena jakości z metrykami RAGAS
  - Bezpieczeństwo importu dla opcjonalnych zależności

#### Sesja 03: Modele open source
- **Priorytetowe zagadnienia**:
  - Strategie benchmarkingu wielomodelowego
  - Pomiar opóźnień i przepustowości
  - Łagodna degradacja i odzyskiwanie błędów
  - Porównanie wydajności między rodzinami modeli

#### Sesja 04: Modele najnowszej generacji
- **Priorytetowe zagadnienia**:
  - Metodologia porównania SLM vs LLM
  - Wskazówki typów i kompleksowe formatowanie wyników
  - Obsługa błędów dla każdego modelu
  - Strukturalne wyniki do analizy

#### Sesja 05: Agenci zasilani AI
- **Priorytetowe zagadnienia**:
  - Orkiestracja wieloagentowa z wzorcem koordynatora
  - Zarządzanie pamięcią agentów i śledzenie stanu
  - Obsługa błędów w pipeline i logowanie etapów
  - Monitorowanie wydajności i statystyki

#### Sesja 06: Modele jako narzędzia
- **Priorytetowe zagadnienia**:
  - Detekcja intencji i dopasowywanie wzorców
  - Algorytmy routingu modeli oparte na słowach kluczowych
  - Pipeline wieloetapowe (planuj → wykonaj → popraw)
  - Kompleksowa dokumentacja funkcji

### Pytania do samooceny

1. Jak FoundryLocalManager upraszcza zarządzanie usługami w porównaniu do ręcznych wywołań REST?
2. Wyjaśnij znaczenie zabezpieczeń importu dla opcjonalnych zależności, takich jak sentence-transformers.
3. Jakie strategie zapewniają łagodną degradację w benchmarkingu wielomodelowym?
4. Jak wzorzec koordynatora organizuje pracę wielu wyspecjalizowanych agentów?
5. Opisz komponenty inteligentnego routera modeli.
6. Jakie są kluczowe elementy obsługi błędów gotowej do produkcji?

### Ćwiczenia praktyczne

1. **Aplikacja czatu**: Zaimplementuj strumieniowy czat z obsługą błędów (45 minut)
2. **Pipeline RAG**: Zbuduj minimalny RAG z oceną jakości (1 godzina)
3. **Benchmark modeli**: Porównaj 3+ modele pod kątem wydajności (1 godzina)
4. **System wieloagentowy**: Stwórz koordynatora z 2 wyspecjalizowanymi agentami (1,5 godziny)
5. **Inteligentny router**: Zbuduj selekcję modeli opartą na zadaniach (1 godzina)
6. **Wdrożenie produkcyjne**: Dodaj monitorowanie i kompleksową obsługę błędów (45 minut)

### Podział czasu

**Intensywna nauka (1 tydzień)**:
- Dzień 1: Sesje 01-02 (Czat + RAG) - 3 godziny
- Dzień 2: Sesje 03-04 (Benchmarking + Porównanie) - 3 godziny
- Dzień 3: Sesje 05-06 (Agenci + Routing) - 3 godziny
- Dzień 4: Ćwiczenia praktyczne i walidacja - 2 godziny

**Nauka w niepełnym wymiarze (2 tygodnie)**:
- Tydzień 1: Sesje 01-03 (6 godzin łącznie)
- Tydzień 2: Sesje 04-06 + ćwiczenia (5 godzin łącznie)

## Moduł 7: Przykłady implementacji EdgeAI

### Kluczowe cele nauki

- Opanowanie AI Toolkit dla Visual Studio Code w celu kompleksowych workflow rozwoju EdgeAI
- Zdobycie wiedzy na temat platformy Windows AI Foundry i strategii optymalizacji NPU
- Implementacja EdgeAI na różnych platformach sprzętowych i w scenariuszach wdrożeniowych
- Tworzenie aplikacji EdgeAI gotowych do produkcji z optymalizacjami specyficznymi dla platformy

### Obszary nauki

#### Sekcja 1: AI Toolkit dla Visual Studio Code
-
4. Wyjaśnij rolę optymalizacji NPU w nowoczesnych aplikacjach AI na urządzeniach brzegowych.  
5. Jak API Phi Silica wykorzystuje sprzęt NPU do optymalizacji wydajności?  
6. Porównaj korzyści lokalnego wdrożenia w porównaniu z wdrożeniem w chmurze dla aplikacji wrażliwych na prywatność.  

### Ćwiczenia praktyczne  

1. **Konfiguracja narzędzi AI**: Skonfiguruj narzędzia AI i zoptymalizuj model (1 godzina)  
2. **Windows AI Foundry**: Zbuduj prostą aplikację AI dla Windows z użyciem API Phi Silica (1 godzina)  
3. **Wdrożenie międzyplatformowe**: Wdróż ten sam model na dwóch różnych platformach (1 godzina)  
4. **Optymalizacja NPU**: Przetestuj wydajność NPU za pomocą narzędzi Windows AI Foundry (30 minut)  

## Moduł 8: Microsoft Foundry Local – Kompletny zestaw narzędzi dla deweloperów (zmodernizowany)  

### Kluczowe cele nauki  

- Zainstaluj i skonfiguruj Foundry Local z nowoczesną integracją SDK  
- Wdrożenie zaawansowanych systemów wieloagentowych z wzorcami koordynatorów  
- Budowa inteligentnych routerów modeli z automatycznym wyborem opartym na zadaniach  
- Wdrożenie gotowych do produkcji rozwiązań AI z kompleksowym monitorowaniem  
- Integracja z Azure AI Foundry dla scenariuszy hybrydowych wdrożeń  
- Opanowanie nowoczesnych wzorców SDK z FoundryLocalManager i klientem OpenAI  

### Obszary nauki  

#### Sekcja 1: Nowoczesna instalacja i konfiguracja  
- **Priorytetowe koncepcje**:  
  - Integracja SDK FoundryLocalManager  
  - Automatyczne wykrywanie usług i monitorowanie stanu  
  - Wzorce konfiguracji oparte na środowisku  
  - Rozważania dotyczące wdrożeń produkcyjnych  

#### Sekcja 2: Zaawansowane systemy wieloagentowe  
- **Priorytetowe koncepcje**:  
  - Wzorzec koordynatora z wyspecjalizowanymi agentami  
  - Specjalizacja agentów w zakresie wyszukiwania, rozumowania i wykonywania  
  - Mechanizmy pętli zwrotnej dla udoskonaleń  
  - Monitorowanie wydajności i śledzenie statystyk  

#### Sekcja 3: Inteligentne trasowanie modeli  
- **Priorytetowe koncepcje**:  
  - Algorytmy wyboru modeli oparte na słowach kluczowych  
  - Obsługa wielu modeli (ogólnych, rozumowania, kodowania, kreatywnych)  
  - Konfiguracja zmiennych środowiskowych dla elastyczności  
  - Sprawdzanie stanu usług i obsługa błędów  

#### Sekcja 4: Wdrożenie gotowe do produkcji  
- **Priorytetowe koncepcje**:  
  - Kompleksowa obsługa błędów i mechanizmy awaryjne  
  - Monitorowanie żądań i śledzenie wydajności  
  - Interaktywne przykłady w Jupyter notebook z benchmarkami  
  - Wzorce integracji z istniejącymi aplikacjami  

### Pytania do samooceny  

1. Jak nowoczesne podejście FoundryLocalManager różni się od ręcznych wywołań REST?  
2. Wyjaśnij wzorzec koordynatora i sposób, w jaki organizuje pracę wyspecjalizowanych agentów.  
3. Jak inteligentny router wybiera odpowiednie modele na podstawie treści zapytania?  
4. Jakie są kluczowe komponenty systemu agentów AI gotowego do produkcji?  
5. Jak wdrożyć kompleksowe monitorowanie stanu usług Foundry Local?  
6. Porównaj korzyści zmodernizowanego podejścia w porównaniu z tradycyjnymi wzorcami implementacji.  

### Ćwiczenia praktyczne  

1. **Konfiguracja nowoczesnego SDK**: Skonfiguruj FoundryLocalManager z automatycznym wykrywaniem usług (30 minut)  
2. **System wieloagentowy**: Uruchom zaawansowany koordynator z wyspecjalizowanymi agentami (30 minut)  
3. **Inteligentne trasowanie**: Przetestuj router modeli z różnymi typami zapytań (30 minut)  
4. **Interaktywna eksploracja**: Użyj Jupyter notebooków do eksploracji zaawansowanych funkcji (45 minut)  
5. **Wdrożenie produkcyjne**: Wdrożenie wzorców monitorowania i obsługi błędów (30 minut)  
6. **Integracja hybrydowa**: Skonfiguruj scenariusze awaryjne z Azure AI Foundry (30 minut)  

## Przewodnik po alokacji czasu  

Aby jak najlepiej wykorzystać rozszerzony 30-godzinny harmonogram kursu (w tym warsztaty), oto sugerowany podział czasu:  

| Aktywność | Alokacja czasu | Opis |  
|----------|----------------|-------------|  
| Czytanie materiałów podstawowych | 12 godzin | Skupienie na kluczowych koncepcjach w każdym module |  
| Ćwiczenia praktyczne | 10 godzin | Praktyczna implementacja kluczowych technik (w tym warsztaty) |  
| Samoocena | 3 godziny | Testowanie zrozumienia poprzez pytania i refleksję |  
| Mini-projekt | 5 godzin | Zastosowanie wiedzy w małej praktycznej implementacji |  

### Kluczowe obszary w zależności od ograniczeń czasowych  

**Jeśli masz tylko 10 godzin:**  
- Ukończ moduł 0 (Wprowadzenie) oraz moduły 1, 2 i 3 (podstawowe koncepcje EdgeAI)  
- Wykonaj co najmniej jedno ćwiczenie praktyczne na moduł  
- Skup się na zrozumieniu kluczowych koncepcji zamiast szczegółów implementacji  

**Jeśli możesz poświęcić pełne 20 godzin:**  
- Ukończ wszystkie osiem modułów (w tym Wprowadzenie)  
- Wykonaj kluczowe ćwiczenia praktyczne z każdego modułu  
- Ukończ jeden mini-projekt z modułu 7  
- Eksploruj co najmniej 2-3 dodatkowe zasoby  

**Jeśli masz więcej niż 20 godzin:**  
- Ukończ wszystkie moduły (w tym Wprowadzenie) z szczegółowymi ćwiczeniami  
- Zbuduj wiele mini-projektów  
- Eksploruj zaawansowane techniki optymalizacji w module 4  
- Wdrożenie produkcyjne z modułu 5  

## Kluczowe zasoby  

Te starannie wybrane zasoby zapewniają największą wartość w ograniczonym czasie nauki:  

### Dokumentacja obowiązkowa  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Najbardziej efektywne narzędzie do optymalizacji modeli  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Najszybszy sposób na lokalne wdrożenie SLM  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Odniesienie do wiodącego modelu zoptymalizowanego dla urządzeń brzegowych  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kompleksowy zestaw narzędzi optymalizacyjnych Intela  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Zintegrowane środowisko rozwoju EdgeAI  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Specyficzna dla Windows platforma rozwoju EdgeAI  

### Narzędzia oszczędzające czas  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Szybki dostęp do modeli i ich wdrożenie  
- [Gradio](https://www.gradio.app/docs/interface) - Szybki rozwój interfejsów UI dla demonstracji AI  
- [Microsoft Olive](https://github.com/microsoft/Olive) - Uproszczona optymalizacja modeli  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efektywne wnioskowanie na CPU  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework kompresji sieci neuronowych  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Zestaw narzędzi do wdrożenia dużych modeli językowych  

## Szablon śledzenia postępów  

Użyj tego uproszczonego szablonu do śledzenia postępów w 20-godzinnym kursie:  

| Moduł | Data ukończenia | Czas spędzony | Kluczowe wnioski |  
|--------|----------------|-------------|---------------|  
| Moduł 0: Wprowadzenie do EdgeAI | | | |  
| Moduł 1: Podstawy EdgeAI | | | |  
| Moduł 2: Podstawy SLM | | | |  
| Moduł 3: Wdrożenie SLM | | | |  
| Moduł 4: Optymalizacja modeli | | | |  
| Moduł 5: SLMOps | | | |  
| Moduł 6: Agenci AI | | | |  
| Moduł 7: Narzędzia deweloperskie | | | |  
| Warsztat: Nauka praktyczna | | | |  
| Moduł 8: Zestaw narzędzi Foundry Local | | | |  
| Ćwiczenia praktyczne | | | |  
| Mini-projekt | | | |  

## Pomysły na mini-projekty  

Rozważ ukończenie jednego z tych projektów, aby przećwiczyć koncepcje EdgeAI (każdy zaprojektowany na 2-4 godziny):  

### Projekty dla początkujących (2-3 godziny każdy)  
1. **Asystent tekstowy na urządzeniu brzegowym**: Stwórz prosty narzędzie do uzupełniania tekstu offline z użyciem małego modelu językowego  
2. **Dashboard porównania modeli**: Zbuduj podstawową wizualizację metryk wydajności dla różnych SLM  
3. **Eksperyment optymalizacyjny**: Zmierz wpływ różnych poziomów kwantyzacji na ten sam model bazowy  

### Projekty średniozaawansowane (3-4 godziny każdy)  
4. **Workflow narzędzi AI**: Użyj narzędzi AI w VS Code do optymalizacji i wdrożenia modelu od początku do końca  
5. **Aplikacja Windows AI Foundry**: Stwórz aplikację Windows z użyciem API Phi Silica i optymalizacji NPU  
6. **Wdrożenie międzyplatformowe**: Wdróż ten sam zoptymalizowany model na Windows (OpenVINO) i mobilne (.NET MAUI)  
7. **Agent wywołujący funkcje**: Zbuduj agenta AI z funkcją wywoływania dla scenariuszy brzegowych  

### Zaawansowane projekty integracyjne (4-5 godzin każdy)  
8. **Pipeline optymalizacji OpenVINO**: Wdrożenie kompletnej optymalizacji modelu z użyciem NNCF i zestawu narzędzi GenAI  
9. **Pipeline SLMOps**: Wdrożenie pełnego cyklu życia modelu od treningu do wdrożenia na urządzeniach brzegowych  
10. **System wielomodelowy na urządzeniach brzegowych**: Wdrożenie wielu wyspecjalizowanych modeli współpracujących na sprzęcie brzegowym  
11. **System integracyjny MCP**: Zbuduj system agentowy z użyciem Model Context Protocol do interakcji z narzędziami  

## Referencje  

- Microsoft Learn (Foundry Local)  
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Rozpocznij: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Referencja CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integracja z SDK inferencji: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Jak otworzyć WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Kompilacja modeli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agenci (przegląd): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Narzędzia optymalizacji i inferencji  
  - Microsoft Olive (dokumentacja): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (rozpocznij): https://onnxruntime.ai/docs/get-started/with-python.html  
  - Integracja ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (dokumentacja): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (dokumentacja): https://ml-explore.github.io/mlx/build/html/index.html  
- Frameworki wdrożeniowe i modele  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (dokumentacja): https://docs.vllm.ai/  
  - Ollama (rozpocznij): https://github.com/ollama/ollama#get-started  
- Narzędzia deweloperskie (Windows i VS Code)  
  - AI Toolkit for VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (przegląd): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Społeczność nauki  

Dołącz do dyskusji i nawiąż kontakt z innymi uczącymi się:  
- Dyskusje na GitHub w [repozytorium EdgeAI dla początkujących](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Podsumowanie  

EdgeAI reprezentuje granicę wdrożeń sztucznej inteligencji, oferując potężne możliwości bezpośrednio na urządzeniach, jednocześnie rozwiązując kluczowe problemy związane z prywatnością, opóźnieniami i łącznością. Ten 20-godzinny kurs dostarcza niezbędnej wiedzy i praktycznych umiejętności, aby natychmiast rozpocząć pracę z technologiami EdgeAI.  

Kurs jest celowo zwięzły i skoncentrowany na najważniejszych koncepcjach, pozwalając szybko zdobyć cenną wiedzę bez przytłaczającego nakładu czasu. Pamiętaj, że praktyka, nawet z prostymi przykładami, jest kluczem do utrwalenia zdobytej wiedzy.  

Powodzenia w nauce!  

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za autorytatywne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.