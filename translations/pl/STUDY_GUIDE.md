<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T08:40:39+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "pl"
}
-->
# EdgeAI dla początkujących: Ścieżki nauki i harmonogram nauki

### Skoncentrowana ścieżka nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|------|-------|------------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 2 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 3 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 4-5 | Moduł 4: Optymalizacja modeli (6 frameworków) | 4 godziny |
| Dzień 6 | Moduł 5: SLMOps | 3 godziny |
| Dzień 7 | Moduł 6-7: Agenci AI i narzędzia deweloperskie | 4 godziny |
| Dzień 8 | Moduł 8: Foundry Local Toolkit (nowoczesna implementacja) | 1 godzina |

### Skoncentrowana ścieżka nauki (2 tygodnie)

| Dzień | Temat | Szacowany czas |
|------|-------|------------------|
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Agenci AI | 2 godziny |
| Dzień 13-14 | Moduł 7: Narzędzia deweloperskie | 3 godziny |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|------|-------|------------------|
| Tydzień 1 | Moduł 1-2: Podstawy i fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia deweloperskie i integracja | 3 godziny |

| Dzień | Temat | Szacowany czas |
|------|-------|------------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 3 godziny |
| Dzień 3-4 | Moduł 2: Podstawy SLM | 3 godziny |
| Dzień 5-6 | Moduł 3: Wdrażanie SLM | 2 godziny |
| Dzień 7-8 | Moduł 4: Optymalizacja modeli | 4 godziny |
| Dzień 9-10 | Moduł 5: SLMOps | 3 godziny |
| Dzień 11-12 | Moduł 6: Systemy agentowe SLM | 2 godziny |
| Dzień 13-14 | Moduł 7: Przykłady implementacji EdgeAI | 2 godziny |

| Moduł | Data ukończenia | Czas poświęcony | Kluczowe wnioski |
|--------|----------------|-------------|--------------|
| Moduł 0: Wprowadzenie do EdgeAI | | | |
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Fundamenty SLM | | | |
| Moduł 3: Wdrażanie SLM | | | |
| Moduł 4: Optymalizacja modeli (6 frameworków) | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Systemy agentowe SLM | | | |
| Moduł 7: Przykłady implementacji EdgeAI | | | |
| Ćwiczenia praktyczne | | | |
| Mini-projekt | | | |

### Nauka w niepełnym wymiarze godzin (4 tygodnie)

| Tydzień | Temat | Szacowany czas |
|------|-------|------------------|
| Tydzień 1 | Moduł 1-2: Podstawy i fundamenty SLM | 6 godzin |
| Tydzień 2 | Moduł 3-4: Wdrażanie i optymalizacja | 6 godzin |
| Tydzień 3 | Moduł 5-6: SLMOps i agenci AI | 5 godzin |
| Tydzień 4 | Moduł 7: Narzędzia deweloperskie i integracja | 3 godziny |

## Wprowadzenie

Witamy w przewodniku nauki EdgeAI dla początkujących! Ten dokument został zaprojektowany, aby pomóc Ci efektywnie przejść przez materiały kursu i maksymalnie wykorzystać proces nauki. Zawiera strukturalne ścieżki nauki, sugerowane harmonogramy, podsumowania kluczowych koncepcji oraz dodatkowe zasoby, które pogłębią Twoje zrozumienie technologii Edge AI.

Jest to zwięzły kurs trwający 20 godzin, który dostarcza podstawowej wiedzy o EdgeAI w efektywnym czasowo formacie, idealnym dla zapracowanych profesjonalistów i studentów, którzy chcą szybko zdobyć praktyczne umiejętności w tej rozwijającej się dziedzinie.

## Przegląd kursu

Kurs jest podzielony na osiem kompleksowych modułów:

0. **Wprowadzenie do EdgeAI** - Podstawy i kontekst z zastosowaniami w przemyśle oraz celami nauki  
1. **Podstawy EdgeAI i transformacja** - Zrozumienie kluczowych koncepcji i technologicznej zmiany  
2. **Fundamenty małych modeli językowych (SLM)** - Eksploracja różnych rodzin SLM i ich architektur  
3. **Wdrażanie małych modeli językowych (SLM)** - Praktyczne strategie wdrażania  
4. **Konwersja formatów modeli i kwantyzacja** - Zaawansowana optymalizacja z 6 frameworkami, w tym OpenVINO  
5. **SLMOps - Operacje na małych modelach językowych** - Zarządzanie cyklem życia produkcji i wdrażania  
6. **Systemy agentowe SLM** - Agenci AI, wywoływanie funkcji i protokół kontekstu modelu  
7. **Przykłady implementacji EdgeAI** - Narzędzia AI, rozwój na Windows i implementacje specyficzne dla platform  
8. **Microsoft Foundry Local – Kompletny zestaw narzędzi deweloperskich** - Rozwój lokalny z hybrydową integracją Azure (Moduł 08)

## Jak korzystać z tego przewodnika nauki

- **Nauka progresywna**: Przechodź przez moduły w kolejności dla najbardziej spójnego doświadczenia nauki  
- **Punkty kontrolne wiedzy**: Korzystaj z pytań do samooceny po każdej sekcji  
- **Praktyka**: Wykonuj sugerowane ćwiczenia, aby utrwalić teoretyczne koncepcje  
- **Dodatkowe zasoby**: Eksploruj dodatkowe materiały dla tematów, które najbardziej Cię interesują  

## Rekomendacje harmonogramu nauki

### Skoncentrowana ścieżka nauki (1 tydzień)

| Dzień | Temat | Szacowany czas |
|------|-------|------------------|
| Dzień 0 | Moduł 0: Wprowadzenie do EdgeAI | 1-2 godziny |
| Dzień 1-2 | Moduł 1: Podstawy EdgeAI | 6 godzin |
| Dzień 3-4 | Moduł 2: Fundamenty SLM | 8 godzin |
| Dzień 5 | Moduł 3: Wdrażanie SLM | 3 godziny |
| Dzień 6 | Moduł 8: Foundry Local Toolkit | 3 godziny |

### Nauka w niepełnym wymiarze godzin (3 tygodnie)

| Tydzień | Temat | Szacowany czas |
|------|-------|------------------|
| Tydzień 1 | Moduł 0: Wprowadzenie + Moduł 1: Podstawy EdgeAI | 7-9 godzin |
| Tydzień 2 | Moduł 2: Fundamenty SLM | 7-8 godzin |
| Tydzień 3 | Moduł 3: Wdrażanie SLM (3h) + Moduł 8: Foundry Local Toolkit (2-3h) | 5-6 godzin |

## Moduł 0: Wprowadzenie do EdgeAI

### Kluczowe cele nauki

- Zrozumienie, czym jest Edge AI i dlaczego ma znaczenie w dzisiejszym krajobrazie technologicznym  
- Identyfikacja głównych branż przekształconych przez Edge AI i ich specyficznych zastosowań  
- Zrozumienie zalet małych modeli językowych (SLM) dla wdrożeń na krawędzi  
- Ustalenie jasnych oczekiwań i wyników nauki dla całego kursu  
- Rozpoznanie możliwości kariery i wymaganych umiejętności w dziedzinie Edge AI  

### Obszary nauki

#### Sekcja 1: Paradygmat Edge AI i definicja
- **Kluczowe koncepcje**:  
  - Edge AI vs. tradycyjne przetwarzanie w chmurze  
  - Konwergencja sprzętu, optymalizacji modeli i wymagań biznesowych  
  - Wdrażanie AI w czasie rzeczywistym, z zachowaniem prywatności i efektywnością kosztową  

#### Sekcja 2: Zastosowania w przemyśle
- **Kluczowe koncepcje**:  
  - Produkcja i Przemysł 4.0: Predykcyjne utrzymanie ruchu i kontrola jakości  
  - Opieka zdrowotna: Diagnostyka obrazowa i monitorowanie pacjentów  
  - Systemy autonomiczne: Pojazdy autonomiczne i transport  
  - Inteligentne miasta: Zarządzanie ruchem i bezpieczeństwo publiczne  
  - Technologia konsumencka: Smartfony, urządzenia noszone i inteligentne domy  

#### Sekcja 3: Fundamenty małych modeli językowych
- **Kluczowe koncepcje**:  
  - Charakterystyka SLM i porównania wydajności  
  - Efektywność parametrów vs. kompromisy w możliwościach  
  - Ograniczenia wdrożeń na krawędzi i strategie optymalizacji  

#### Sekcja 4: Ramy nauki i ścieżka kariery
- **Kluczowe koncepcje**:  
  - Architektura kursu i podejście do progresywnego opanowania materiału  
  - Cele techniczne i praktyczne wdrożenia  
  - Możliwości rozwoju kariery i zastosowania w przemyśle  

### Pytania do samooceny

1. Jakie są trzy główne trendy technologiczne, które umożliwiły rozwój Edge AI?  
2. Porównaj zalety i wyzwania Edge AI w porównaniu z AI opartym na chmurze.  
3. Wymień trzy branże, w których Edge AI przynosi kluczową wartość biznesową i wyjaśnij dlaczego.  
4. Jak małe modele językowe sprawiają, że Edge AI jest praktyczne w rzeczywistych wdrożeniach?  
5. Jakie kluczowe umiejętności techniczne rozwiniesz podczas tego kursu?  
6. Opisz czterofazowe podejście do nauki zastosowane w tym kursie.  

### Ćwiczenia praktyczne

1. **Badanie przemysłu**: Wybierz jedno zastosowanie przemysłowe i zbadaj rzeczywiste wdrożenie Edge AI (30 minut)  
2. **Eksploracja modeli**: Przeglądaj dostępne małe modele językowe na Hugging Face i porównaj ich liczby parametrów oraz możliwości (30 minut)  
3. **Planowanie nauki**: Przejrzyj strukturę całego kursu i stwórz swój osobisty harmonogram nauki (15 minut)  

### Dodatkowe materiały

- [Przegląd rynku Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Przegląd małych modeli językowych - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Fundamenty Edge Computing](https://www.edgecomputing.org/)  

## Moduł 1: Podstawy EdgeAI i transformacja

### Kluczowe cele nauki

- Zrozumienie różnic między AI opartym na chmurze a AI opartym na krawędzi  
- Opanowanie podstawowych technik optymalizacji dla środowisk o ograniczonych zasobach  
- Analiza rzeczywistych zastosowań technologii EdgeAI  
- Konfiguracja środowiska deweloperskiego dla projektów EdgeAI  

### Obszary nauki

#### Sekcja 1: Podstawy EdgeAI
- **Kluczowe koncepcje**:  
  - Paradygmaty Edge vs. Cloud computing  
  - Techniki kwantyzacji modeli  
  - Opcje akceleracji sprzętowej (NPU, GPU, CPU)  
  - Zalety prywatności i bezpieczeństwa  

- **Dodatkowe materiały**:  
  - [Dokumentacja TensorFlow Lite](https://www.tensorflow.org/lite)  
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)  
  - [Dokumentacja Edge Impulse](https://docs.edgeimpulse.com)  

#### Sekcja 2: Studium przypadków
- **Kluczowe koncepcje**:  
  - Ekosystem modeli Microsoft Phi & Mu  
  - Praktyczne wdrożenia w różnych branżach  
  - Rozważania dotyczące wdrożeń  

#### Sekcja 3: Praktyczny przewodnik wdrożeniowy
- **Kluczowe koncepcje**:  
  - Konfiguracja środowiska deweloperskiego  
  - Narzędzia do kwantyzacji i optymalizacji  
  - Metody oceny wdrożeń EdgeAI  

#### Sekcja 4: Sprzęt do wdrożeń na krawędzi
- **Kluczowe koncepcje**:  
  - Porównania platform sprzętowych  
  - Strategie optymalizacji dla konkretnego sprzętu  
  - Rozważania dotyczące wdrożeń  

### Pytania do samooceny

1. Porównaj i skontrastuj wdrożenia AI oparte na chmurze z wdrożeniami AI na krawędzi.  
2. Wyjaśnij trzy kluczowe techniki optymalizacji modeli dla wdrożeń na krawędzi.  
3. Jakie są główne zalety uruchamiania modeli AI na krawędzi?  
4. Opisz proces kwantyzacji modelu i jego wpływ na wydajność.  
5. Wyjaśnij, jak różne akceleratory sprzętowe (NPU, GPU, CPU) wpływają na wdrożenia EdgeAI.  

### Ćwiczenia praktyczne

1. **Szybka konfiguracja środowiska**: Skonfiguruj minimalne środowisko deweloperskie z niezbędnymi pakietami (30 minut)  
2. **Eksploracja modeli**: Pobierz i przeanalizuj wstępnie wytrenowany mały model językowy (1 godzina)  
3. **Podstawowa kwantyzacja**: Wypróbuj prostą kwantyzację na małym modelu (1 godzina)  

## Moduł 2: Fundamenty małych modeli językowych

### Kluczowe cele nauki

- Zrozumienie zasad architektonicznych różnych rodzin SLM  
- Porównanie możliwości modeli w różnych skalach parametrów  
- Ocena modeli pod kątem efektywności, możliwości i wymagań wdrożeniowych  
- Rozpoznanie odpowiednich zastosowań dla różnych rodzin modeli  

### Obszary nauki

#### Sekcja 1: Rodzina modeli Microsoft Phi
- **Kluczowe koncepcje**:  
  - Ewolucja filozofii projektowania  
  - Architektura z naciskiem na efektywność  
  - Specjalistyczne możliwości  

#### Sekcja 2: Rodzina Qwen
- **Kluczowe koncepcje**:  
  - Wkład open source  
  - Skalowalne opcje wdrożeniowe  
  - Zaawansowana architektura rozumowania  

#### Sekcja 3: Rodzina Gemma
- **Kluczowe koncepcje**:  
  - Innowacje napędzane badaniami  
  - Możliwości multimodalne  
  - Optymalizacja mobilna  

#### Sekcja 4: Rodzina BitNET
- **Kluczowe kon
3. Jakie są unikalne zalety modelu Mu w integracji z Windows?
4. Opisz, w jaki sposób Phi-Silica wykorzystuje sprzęt NPU do optymalizacji wydajności.
5. Która rodzina modeli byłaby najbardziej odpowiednia dla aplikacji mobilnej z ograniczoną łącznością i dlaczego?

### Ćwiczenia praktyczne

1. **Porównanie modeli**: Szybkie porównanie dwóch różnych modeli SLM (1 godzina)
2. **Prosta generacja tekstu**: Podstawowa implementacja generacji tekstu z użyciem małego modelu (1 godzina)
3. **Szybka optymalizacja**: Zastosowanie jednej techniki optymalizacji w celu poprawy szybkości wnioskowania (1 godzina)

## Moduł 3: Wdrożenie Małych Modeli Językowych

### Kluczowe cele nauki

- Wybór odpowiednich modeli w zależności od ograniczeń wdrożeniowych
- Opanowanie technik optymalizacji dla różnych scenariuszy wdrożeniowych
- Implementacja SLM w środowiskach lokalnych i chmurowych
- Projektowanie konfiguracji gotowych do produkcji dla aplikacji EdgeAI

### Obszary nauki

#### Sekcja 1: Zaawansowana nauka SLM
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji parametrów
  - Zaawansowane techniki optymalizacji
  - Strategie pozyskiwania modeli

#### Sekcja 2: Wdrożenie w środowisku lokalnym
- **Priorytetowe zagadnienia**: 
  - Wdrożenie na platformie Ollama
  - Lokalne rozwiązania Microsoft Foundry
  - Analiza porównawcza frameworków

#### Sekcja 3: Wdrożenie w chmurze z konteneryzacją
- **Priorytetowe zagadnienia**: 
  - Wnioskowanie wysokiej wydajności z vLLM
  - Orkiestracja kontenerów
  - Implementacja ONNX Runtime

### Pytania do samooceny

1. Jakie czynniki należy wziąć pod uwagę przy wyborze między wdrożeniem lokalnym a chmurowym?
2. Porównaj Ollama i Microsoft Foundry Local jako opcje wdrożeniowe.
3. Wyjaśnij korzyści płynące z konteneryzacji dla wdrożenia SLM.
4. Jakie są kluczowe metryki wydajności do monitorowania dla SLM wdrożonego na brzegu sieci?
5. Opisz kompletny przepływ pracy wdrożeniowego od wyboru modelu do implementacji produkcyjnej.

### Ćwiczenia praktyczne

1. **Podstawowe wdrożenie lokalne**: Wdrożenie prostego SLM z użyciem Ollama (1 godzina)
2. **Sprawdzenie wydajności**: Szybkie porównanie wydajności wdrożonego modelu (30 minut)
3. **Prosta integracja**: Stworzenie minimalnej aplikacji korzystającej z wdrożonego modelu (1 godzina)

## Moduł 4: Konwersja formatów modeli i kwantyzacja

### Kluczowe cele nauki

- Opanowanie zaawansowanych technik kwantyzacji od precyzji 1-bitowej do 8-bitowej
- Zrozumienie strategii konwersji formatów (GGUF, ONNX)
- Implementacja optymalizacji w sześciu frameworkach (Llama.cpp, Olive, OpenVINO, MLX, synteza przepływu pracy)
- Wdrożenie zoptymalizowanych modeli w środowiskach produkcyjnych na brzegu sieci na sprzęcie Intel, Apple i wieloplatformowym

### Obszary nauki

#### Sekcja 1: Podstawy kwantyzacji
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji precyzji
  - Kompromisy między wydajnością a dokładnością
  - Optymalizacja zużycia pamięci

#### Sekcja 2: Implementacja Llama.cpp
- **Priorytetowe zagadnienia**: 
  - Wdrożenie wieloplatformowe
  - Optymalizacja formatu GGUF
  - Techniki przyspieszenia sprzętowego

#### Sekcja 3: Microsoft Olive Suite
- **Priorytetowe zagadnienia**: 
  - Optymalizacja uwzględniająca sprzęt
  - Wdrożenie na poziomie przedsiębiorstwa
  - Zautomatyzowane przepływy pracy optymalizacyjne

#### Sekcja 4: OpenVINO Toolkit
- **Priorytetowe zagadnienia**: 
  - Optymalizacja sprzętu Intel
  - Framework kompresji sieci neuronowych (NNCF)
  - Wdrożenie wnioskowania wieloplatformowego
  - OpenVINO GenAI dla wdrożenia LLM

#### Sekcja 5: Framework Apple MLX
- **Priorytetowe zagadnienia**: 
  - Optymalizacja dla Apple Silicon
  - Zunifikowana architektura pamięci
  - Możliwości fine-tuningu LoRA

#### Sekcja 6: Synteza przepływu pracy Edge AI
- **Priorytetowe zagadnienia**: 
  - Zunifikowana architektura przepływu pracy
  - Drzewa decyzyjne wyboru frameworków
  - Walidacja gotowości produkcyjnej
  - Strategie przyszłościowe

### Pytania do samooceny

1. Porównaj strategie kwantyzacji na różnych poziomach precyzji (1-bit do 8-bit).
2. Wyjaśnij zalety formatu GGUF dla wdrożenia na brzegu sieci.
3. Jak optymalizacja uwzględniająca sprzęt w Microsoft Olive poprawia efektywność wdrożenia?
4. Jakie są kluczowe korzyści z NNCF OpenVINO dla kompresji modeli?
5. Opisz, w jaki sposób Apple MLX wykorzystuje zunifikowaną architekturę pamięci do optymalizacji.
6. Jak synteza przepływu pracy pomaga w wyborze optymalnych frameworków optymalizacyjnych?

### Ćwiczenia praktyczne

1. **Kwantyzacja modelu**: Zastosowanie różnych poziomów kwantyzacji do modelu i porównanie wyników (1 godzina)
2. **Optymalizacja OpenVINO**: Użycie NNCF do kompresji modelu dla sprzętu Intel (1 godzina)
3. **Porównanie frameworków**: Testowanie tego samego modelu w trzech różnych frameworkach optymalizacyjnych (1 godzina)
4. **Benchmark wydajności**: Pomiar wpływu optymalizacji na szybkość wnioskowania i zużycie pamięci (1 godzina)

## Moduł 5: SLMOps - Operacje Małych Modeli Językowych

### Kluczowe cele nauki

- Zrozumienie zasad zarządzania cyklem życia SLMOps
- Opanowanie technik destylacji i fine-tuningu dla wdrożenia na brzegu sieci
- Implementacja strategii wdrożenia produkcyjnego z monitorowaniem
- Budowa operacji i przepływów konserwacyjnych SLM na poziomie przedsiębiorstwa

### Obszary nauki

#### Sekcja 1: Wprowadzenie do SLMOps
- **Priorytetowe zagadnienia**: 
  - Przełomowy paradygmat SLMOps w operacjach AI
  - Architektura zorientowana na koszty i prywatność
  - Strategiczny wpływ biznesowy i przewagi konkurencyjne

#### Sekcja 2: Destylacja modelu
- **Priorytetowe zagadnienia**: 
  - Techniki transferu wiedzy
  - Implementacja procesu destylacji dwustopniowej
  - Przepływy pracy destylacji w Azure ML

#### Sekcja 3: Strategie fine-tuningu
- **Priorytetowe zagadnienia**: 
  - Fine-tuning efektywny pod względem parametrów (PEFT)
  - Zaawansowane metody LoRA i QLoRA
  - Trening wieloadapterowy i optymalizacja hiperparametrów

#### Sekcja 4: Wdrożenie produkcyjne
- **Priorytetowe zagadnienia**: 
  - Konwersja i kwantyzacja modeli na potrzeby produkcji
  - Konfiguracja wdrożenia Foundry Local
  - Benchmark wydajności i walidacja jakości

### Pytania do samooceny

1. Jak SLMOps różni się od tradycyjnego MLOps?
2. Wyjaśnij korzyści z destylacji modelu dla wdrożenia na brzegu sieci.
3. Jakie są kluczowe aspekty fine-tuningu SLM w środowiskach o ograniczonych zasobach?
4. Opisz kompletny pipeline wdrożeniowy dla aplikacji Edge AI.

### Ćwiczenia praktyczne

1. **Podstawowa destylacja**: Stworzenie mniejszego modelu z większego modelu nauczyciela (1 godzina)
2. **Eksperyment fine-tuningu**: Fine-tuning modelu dla konkretnej dziedziny (1 godzina)
3. **Pipeline wdrożeniowy**: Konfiguracja podstawowego pipeline CI/CD dla wdrożenia modelu (1 godzina)

## Moduł 6: Agentowe Systemy SLM - Agenci AI i Wywoływanie Funkcji

### Kluczowe cele nauki

- Budowa inteligentnych agentów AI dla środowisk brzegowych z użyciem Małych Modeli Językowych
- Implementacja możliwości wywoływania funkcji z systematycznymi przepływami pracy
- Opanowanie integracji Model Context Protocol (MCP) dla standaryzowanej interakcji z narzędziami
- Tworzenie zaawansowanych systemów agentowych z minimalną interwencją człowieka

### Obszary nauki

#### Sekcja 1: Agenci AI i podstawy SLM
- **Priorytetowe zagadnienia**: 
  - Ramy klasyfikacji agentów (refleksyjni, oparte na modelu, oparte na celach, uczący się agenci)
  - Analiza kompromisów SLM vs LLM
  - Wzorce projektowe agentów specyficzne dla brzegu sieci
  - Optymalizacja zasobów dla agentów

#### Sekcja 2: Wywoływanie funkcji w Małych Modelach Językowych
- **Priorytetowe zagadnienia**: 
  - Implementacja systematycznych przepływów pracy (detekcja intencji, wyjście JSON, wykonanie zewnętrzne)
  - Implementacje specyficzne dla platform (Phi-4-mini, wybrane modele Qwen, Microsoft Foundry Local)
  - Zaawansowane przykłady (współpraca wieloagentowa, dynamiczny wybór narzędzi)
  - Rozważania produkcyjne (ograniczenia szybkości, logowanie audytowe, środki bezpieczeństwa)

#### Sekcja 3: Integracja Model Context Protocol (MCP)
- **Priorytetowe zagadnienia**: 
  - Architektura protokołu i projekt systemu warstwowego
  - Obsługa wielu backendów (Ollama dla rozwoju, vLLM dla produkcji)
  - Protokoły połączeń (tryby STDIO i SSE)
  - Zastosowania w rzeczywistości (automatyzacja webowa, przetwarzanie danych, integracja API)

### Pytania do samooceny

1. Jakie są kluczowe rozważania architektoniczne dla agentów AI na brzegu sieci?
2. W jaki sposób wywoływanie funkcji zwiększa możliwości agentów?
3. Wyjaśnij rolę Model Context Protocol w komunikacji agentów.

### Ćwiczenia praktyczne

1. **Prosty agent**: Budowa podstawowego agenta AI z wywoływaniem funkcji (1 godzina)
2. **Integracja MCP**: Implementacja MCP w aplikacji agenta (30 minut)

## Moduł 7: Przykłady implementacji EdgeAI

### Kluczowe cele nauki

- Opanowanie AI Toolkit dla Visual Studio Code dla kompleksowych przepływów pracy EdgeAI
- Zdobycie wiedzy na temat platformy Windows AI Foundry i strategii optymalizacji NPU
- Implementacja EdgeAI na wielu platformach sprzętowych i scenariuszach wdrożeniowych
- Tworzenie aplikacji EdgeAI gotowych do produkcji z optymalizacjami specyficznymi dla platformy

### Obszary nauki

#### Sekcja 1: AI Toolkit dla Visual Studio Code
- **Priorytetowe zagadnienia**: 
  - Kompleksowe środowisko rozwoju Edge AI w VS Code
  - Katalog modeli i ich odkrywanie dla wdrożenia na brzegu sieci
  - Lokalne testowanie, optymalizacja i przepływy pracy rozwoju agentów
  - Monitorowanie wydajności i ocena dla scenariuszy brzegowych

#### Sekcja 2: Przewodnik rozwoju Windows EdgeAI
- **Priorytetowe zagadnienia**: 
  - Kompleksowy przegląd platformy Windows AI Foundry
  - API Phi Silica dla efektywnego wnioskowania NPU
  - API Computer Vision dla przetwarzania obrazów i OCR
  - Foundry Local CLI dla lokalnego rozwoju i testowania

#### Sekcja 3: Implementacje specyficzne dla platformy
- **Priorytetowe zagadnienia**: 
  - Wdrożenie na NVIDIA Jetson Orin Nano (67 TOPS wydajności AI)
  - Aplikacje mobilne z .NET MAUI i ONNX Runtime GenAI
  - Rozwiązania Azure EdgeAI z hybrydową architekturą chmura-brzeg
  - Optymalizacja Windows ML z uniwersalnym wsparciem sprzętowym
  - Aplikacje Foundry Local z implementacją RAG zorientowaną na prywatność

### Pytania do samooceny

1. Jak AI Toolkit usprawnia przepływ pracy rozwoju EdgeAI?
2. Porównaj strategie wdrożeniowe na różnych platformach sprzętowych.
3. Jakie są zalety Windows AI Foundry dla rozwoju na brzegu sieci?
4. Wyjaśnij rolę optymalizacji NPU w nowoczesnych aplikacjach Edge AI.
5. W jaki sposób API Phi Silica wykorzystuje sprzęt NPU do optymalizacji wydajności?
6. Porównaj korzyści z wdrożenia lokalnego i chmurowego dla aplikacji wrażliwych na prywatność.

### Ćwiczenia praktyczne

1. **Konfiguracja AI Toolkit**: Konfiguracja AI Toolkit i optymalizacja modelu (1 godzina)
2. **Windows AI Foundry**: Budowa prostej aplikacji Windows AI z użyciem API Phi Silica (1 godzina)
3. **Wdrożenie wieloplatformowe**: Wdrożenie tego samego modelu na dwóch różnych platformach (1 godzina)
4. **Optymalizacja NPU**: Testowanie wydajności NPU z narzędziami Windows AI Foundry (30 minut)

## Moduł 8: Microsoft Foundry Local – Kompletny zestaw narzędzi dla deweloperów (zmodernizowany)

### Kluczowe cele nauki

- Instalacja i konfiguracja Foundry Local z nowoczesną integracją SDK
- Implementacja zaawansowanych systemów wieloagentowych z wzorcami koordynatora
- Budowa inteligentnych routerów modeli z automatycznym wyborem opartym na zadaniach
- Wdrożenie rozwiązań AI gotowych do produkcji z kompleksowym monitorowaniem
- Integracja z Azure AI Foundry dla scenariuszy hybrydowego wdrożenia
- Opanowanie nowoczesnych wzorców SDK z FoundryLocalManager i klientem OpenAI

### Obszary nauki

#### Sekcja 1: Nowoczesna instalacja i konfiguracja
- **Priorytetowe zagadnienia**: 
  - Integracja SDK FoundryLocalManager
  - Automatyczne wykrywanie usług i monitorowanie stanu
  - Wzorce konfiguracji oparte na środowisku
  - Rozważania dotyczące wdrożenia produkcyjnego

#### Sekcja 2: Zaawansowane systemy wieloagentowe
- **Priorytetowe zagadnienia**: 
  - Wzorzec koordynatora z agentami specjalistycznymi
  - Specjalizacja agentów w zakresie wyszukiwania, rozumowania i wykonania
  - Mechanizmy pętli zwrotnej dla udoskonalania
  - Monitorowanie wydajności i śledzenie statystyk

#### Sekcja 3: Inteligentne routowanie modeli
- **Priorytetowe zagadnienia**: 
  - Algorytmy wyboru modeli oparte na słowach kluczowych
  - Obsługa wielu modeli (ogólne, rozumowanie, kod, kreatywne)
  - Konfiguracja zmiennych środowiskowych dla elastyczności
  - Sprawdzanie stanu usług i obsługa błędów

#### Sekcja 4: Implementacja gotowa do produkcji
- **Priorytetowe zagadnienia**: 
  - Kompleksowa obsługa błędów i mechanizmy awaryjne
  - Monitorowanie żądań i śledzenie wydajności
  - Interaktywne przykłady w Jupyter notebook z benchmarkami
  - Wzorce integracji z istniejącymi aplikacjami

### Pytania do samooceny

1. Jak nowoczesne podejście FoundryLocalManager różni się od ręcznych
| Ćwiczenia praktyczne | 6 godzin | Praktyczne wdrożenie kluczowych technik |
| Samoocena | 2 godziny | Testowanie swojej wiedzy poprzez pytania i refleksję |
| Mini-projekt | 3 godziny | Zastosowanie wiedzy w małym praktycznym wdrożeniu |

### Kluczowe obszary w zależności od dostępnego czasu

**Jeśli masz tylko 10 godzin:**
- Ukończ moduł 0 (Wprowadzenie) oraz moduły 1, 2 i 3 (podstawowe koncepcje EdgeAI)
- Wykonaj co najmniej jedno ćwiczenie praktyczne w każdym module
- Skup się na zrozumieniu kluczowych koncepcji, a nie na szczegółach implementacji

**Jeśli możesz poświęcić pełne 20 godzin:**
- Ukończ wszystkie osiem modułów (w tym Wprowadzenie)
- Wykonaj kluczowe ćwiczenia praktyczne z każdego modułu
- Zrealizuj jeden mini-projekt z modułu 7
- Przejrzyj co najmniej 2-3 dodatkowe zasoby

**Jeśli masz więcej niż 20 godzin:**
- Ukończ wszystkie moduły (w tym Wprowadzenie) z dokładnymi ćwiczeniami
- Zbuduj kilka mini-projektów
- Zbadaj zaawansowane techniki optymalizacji w module 4
- Wdrożenie produkcyjne z modułu 5

## Kluczowe zasoby

Te starannie wybrane zasoby zapewniają największą wartość przy ograniczonym czasie nauki:

### Dokumentacja, którą warto przeczytać
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Najbardziej efektywne narzędzie do optymalizacji modeli
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Najszybszy sposób na lokalne wdrożenie SLM
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Odniesienie do wiodącego modelu zoptymalizowanego dla Edge
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kompleksowy zestaw narzędzi optymalizacyjnych Intela
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Zintegrowane środowisko rozwoju EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Platforma rozwoju EdgeAI specyficzna dla Windows

### Narzędzia oszczędzające czas
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Szybki dostęp do modeli i ich wdrożenie
- [Gradio](https://www.gradio.app/docs/interface) - Szybki rozwój interfejsów użytkownika dla demonstracji AI
- [Microsoft Olive](https://github.com/microsoft/Olive) - Uproszczona optymalizacja modeli
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efektywne wnioskowanie na CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework do kompresji sieci neuronowych
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Zestaw narzędzi do wdrożenia dużych modeli językowych

## Szablon śledzenia postępów

Użyj tego uproszczonego szablonu, aby śledzić swoje postępy w 20-godzinnym kursie:

| Moduł | Data ukończenia | Czas poświęcony | Kluczowe wnioski |
|-------|-----------------|-----------------|------------------|
| Moduł 0: Wprowadzenie do EdgeAI | | | |
| Moduł 1: Podstawy EdgeAI | | | |
| Moduł 2: Podstawy SLM | | | |
| Moduł 3: Wdrożenie SLM | | | |
| Moduł 4: Optymalizacja modeli | | | |
| Moduł 5: SLMOps | | | |
| Moduł 6: Agenci AI | | | |
| Moduł 7: Narzędzia rozwojowe | | | |
| Moduł 8: Lokalny zestaw narzędzi Foundry | | | |
| Ćwiczenia praktyczne | | | |
| Mini-projekt | | | |

## Pomysły na mini-projekty

Rozważ realizację jednego z tych projektów, aby przećwiczyć koncepcje EdgeAI (każdy zaprojektowany na 2-4 godziny):

### Projekty dla początkujących (2-3 godziny każdy)
1. **Asystent tekstowy na Edge**: Stwórz prosty offline'owy narzędzie do uzupełniania tekstu z użyciem małego modelu językowego
2. **Dashboard porównania modeli**: Zbuduj podstawową wizualizację metryk wydajności różnych SLM
3. **Eksperyment optymalizacyjny**: Zmierz wpływ różnych poziomów kwantyzacji na ten sam model bazowy

### Projekty średniozaawansowane (3-4 godziny każdy)
4. **Workflow narzędzi AI**: Użyj AI Toolkit w VS Code do optymalizacji i wdrożenia modelu od początku do końca
5. **Aplikacja Windows AI Foundry**: Stwórz aplikację Windows z użyciem API Phi Silica i optymalizacji NPU
6. **Wdrożenie międzyplatformowe**: Wdróż ten sam zoptymalizowany model na Windows (OpenVINO) i mobilne (.NET MAUI)
7. **Agent wywołujący funkcje**: Zbuduj agenta AI z możliwością wywoływania funkcji dla scenariuszy na Edge

### Zaawansowane projekty integracyjne (4-5 godzin każdy)
8. **Pipeline optymalizacji OpenVINO**: Zaimplementuj kompletną optymalizację modelu z użyciem NNCF i zestawu narzędzi GenAI
9. **Pipeline SLMOps**: Zaimplementuj pełny cykl życia modelu od treningu do wdrożenia na Edge
10. **System Edge z wieloma modelami**: Wdróż wiele wyspecjalizowanych modeli współpracujących na sprzęcie Edge
11. **System integracji MCP**: Zbuduj system agentowy z użyciem Model Context Protocol do interakcji z narzędziami

## Odniesienia

- Microsoft Learn (Foundry Local)
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Rozpocznij: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Referencja CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integracja z SDK wnioskowania: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Jak otworzyć WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Kompilacja modeli Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Przegląd: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenci (przegląd): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Narzędzia optymalizacji i wnioskowania
  - Microsoft Olive (dokumentacja): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (rozpocznij): https://onnxruntime.ai/docs/get-started/with-python.html
  - Integracja ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (dokumentacja): https://docs.openvino.ai/2025/index.html
  - Apple MLX (dokumentacja): https://ml-explore.github.io/mlx/build/html/index.html
- Frameworki wdrożeniowe i modele
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (dokumentacja): https://docs.vllm.ai/
  - Ollama (rozpocznij): https://github.com/ollama/ollama#get-started
- Narzędzia dla deweloperów (Windows i VS Code)
  - AI Toolkit dla VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (przegląd): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Społeczność nauki

Dołącz do dyskusji i nawiąż kontakt z innymi uczącymi się:
- Dyskusje na GitHub w [repozytorium EdgeAI dla początkujących](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Podsumowanie

EdgeAI to granica wdrożenia sztucznej inteligencji, która przynosi potężne możliwości bezpośrednio na urządzenia, jednocześnie rozwiązując kluczowe problemy związane z prywatnością, opóźnieniami i łącznością. Ten 20-godzinny kurs dostarcza niezbędnej wiedzy i praktycznych umiejętności, aby natychmiast rozpocząć pracę z technologiami EdgeAI.

Kurs jest celowo zwięzły i skoncentrowany na najważniejszych koncepcjach, pozwalając szybko zdobyć cenną wiedzę bez przytłaczającego zaangażowania czasowego. Pamiętaj, że praktyka, nawet z prostymi przykładami, jest kluczem do utrwalenia zdobytej wiedzy.

Powodzenia w nauce!

---

