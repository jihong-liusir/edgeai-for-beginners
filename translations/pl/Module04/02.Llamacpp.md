<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T15:44:23+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "pl"
}
-->
# Sekcja 2: Przewodnik po implementacji Llama.cpp

## Spis treści
1. [Wprowadzenie](../../../Module04)
2. [Czym jest Llama.cpp?](../../../Module04)
3. [Instalacja](../../../Module04)
4. [Budowanie ze źródła](../../../Module04)
5. [Kwantyzacja modelu](../../../Module04)
6. [Podstawowe użycie](../../../Module04)
7. [Zaawansowane funkcje](../../../Module04)
8. [Integracja z Pythonem](../../../Module04)
9. [Rozwiązywanie problemów](../../../Module04)
10. [Najlepsze praktyki](../../../Module04)

## Wprowadzenie

Ten kompleksowy poradnik przeprowadzi Cię przez wszystko, co musisz wiedzieć o Llama.cpp – od podstawowej instalacji po zaawansowane scenariusze użycia. Llama.cpp to potężna implementacja w C++, która umożliwia efektywne wnioskowanie z dużych modeli językowych (LLM) przy minimalnej konfiguracji i doskonałej wydajności na różnych konfiguracjach sprzętowych.

## Czym jest Llama.cpp?

Llama.cpp to framework do wnioskowania z LLM napisany w C/C++, który pozwala na lokalne uruchamianie dużych modeli językowych przy minimalnej konfiguracji i osiąganiu najnowocześniejszej wydajności na szerokiej gamie sprzętu. Kluczowe funkcje obejmują:

### Główne funkcje
- **Implementacja w czystym C/C++** bez zależności
- **Kompatybilność międzyplatformowa** (Windows, macOS, Linux)
- **Optymalizacja sprzętowa** dla różnych architektur
- **Wsparcie dla kwantyzacji** (od 1,5-bitowej do 8-bitowej kwantyzacji całkowitej)
- **Przyspieszenie na CPU i GPU**
- **Efektywność pamięciowa** w środowiskach o ograniczonych zasobach

### Zalety
- Działa efektywnie na CPU bez potrzeby specjalistycznego sprzętu
- Obsługuje wiele backendów GPU (CUDA, Metal, OpenCL, Vulkan)
- Lekki i przenośny
- Apple Silicon jako priorytet – zoptymalizowany dzięki ARM NEON, Accelerate i Metal
- Obsługuje różne poziomy kwantyzacji dla zmniejszenia zużycia pamięci

## Instalacja

### Metoda 1: Gotowe pliki binarne (zalecane dla początkujących)

#### Pobierz z GitHub Releases
1. Odwiedź [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Pobierz odpowiedni plik binarny dla swojego systemu:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` dla Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` dla macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` dla Linux

3. Rozpakuj archiwum i dodaj katalog do zmiennej PATH systemu

#### Korzystanie z menedżerów pakietów

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (różne dystrybucje):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metoda 2: Pakiet Python (llama-cpp-python)

#### Podstawowa instalacja
```bash
pip install llama-cpp-python
```

#### Z przyspieszeniem sprzętowym
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Budowanie ze źródła

### Wymagania wstępne

**Wymagania systemowe:**
- Kompilator C++ (GCC, Clang lub MSVC)
- CMake (wersja 3.14 lub wyższa)
- Git
- Narzędzia do budowania dla Twojej platformy

**Instalacja wymagań wstępnych:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Zainstaluj Visual Studio 2022 z narzędziami do programowania w C++
- Zainstaluj CMake z oficjalnej strony
- Zainstaluj Git

### Podstawowy proces budowania

1. **Sklonuj repozytorium:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Skonfiguruj budowanie:**
```bash
cmake -B build
```

3. **Zbuduj projekt:**
```bash
cmake --build build --config Release
```

Aby przyspieszyć kompilację, użyj równoległych zadań:
```bash
cmake --build build --config Release -j 8
```

### Budowanie specyficzne dla sprzętu

#### Wsparcie CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Wsparcie Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Wsparcie OpenBLAS (optymalizacja CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Wsparcie Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Zaawansowane opcje budowania

#### Budowanie w trybie debugowania
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Z dodatkowymi funkcjami
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kwantyzacja modelu

### Zrozumienie formatu GGUF

GGUF (Generalized GGML Unified Format) to zoptymalizowany format plików zaprojektowany do efektywnego uruchamiania dużych modeli językowych za pomocą Llama.cpp i innych frameworków. Oferuje:

- Standaryzowane przechowywanie wag modelu
- Poprawioną kompatybilność między platformami
- Zwiększoną wydajność
- Efektywne zarządzanie metadanymi

### Typy kwantyzacji

Llama.cpp obsługuje różne poziomy kwantyzacji:

| Typ | Bity | Opis | Zastosowanie |
|-----|------|------|--------------|
| F16 | 16 | Półprecyzja | Wysoka jakość, duża pamięć |
| Q8_0 | 8 | Kwantyzacja 8-bitowa | Dobry balans |
| Q4_0 | 4 | Kwantyzacja 4-bitowa | Umiarkowana jakość, mniejszy rozmiar |
| Q2_K | 2 | Kwantyzacja 2-bitowa | Najmniejszy rozmiar, niższa jakość |

### Konwersja modeli

#### Z PyTorch do GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Bezpośrednie pobieranie z Hugging Face
Wiele modeli jest dostępnych w formacie GGUF na Hugging Face:
- Wyszukaj modele z "GGUF" w nazwie
- Pobierz odpowiedni poziom kwantyzacji
- Użyj bezpośrednio z Llama.cpp

## Podstawowe użycie

### Interfejs wiersza poleceń

#### Prosta generacja tekstu
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Korzystanie z modeli z Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Tryb serwera
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Często używane parametry

| Parametr | Opis | Przykład |
|----------|------|---------|
| `-m` | Ścieżka do pliku modelu | `-m model.gguf` |
| `-p` | Tekst podpowiedzi | `-p "Hello world"` |
| `-n` | Liczba generowanych tokenów | `-n 100` |
| `-c` | Rozmiar kontekstu | `-c 4096` |
| `-t` | Liczba wątków | `-t 8` |
| `-ngl` | Warstwy GPU | `-ngl 32` |
| `-temp` | Temperatura | `-temp 0.7` |

### Tryb interaktywny

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Zaawansowane funkcje

### API serwera

#### Uruchamianie serwera
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Korzystanie z API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optymalizacja wydajności

#### Zarządzanie pamięcią
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Wielowątkowość
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Przyspieszenie GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integracja z Pythonem

### Podstawowe użycie z llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Interfejs czatu

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Strumieniowanie odpowiedzi

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integracja z LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Rozwiązywanie problemów

### Typowe problemy i rozwiązania

#### Błędy budowania

**Problem: CMake nie znaleziono**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problem: Kompilator nie znaleziono**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Problemy w czasie działania

**Problem: Niepowodzenie ładowania modelu**
- Sprawdź ścieżkę do pliku modelu
- Sprawdź uprawnienia do pliku
- Upewnij się, że masz wystarczającą ilość RAM
- Wypróbuj różne poziomy kwantyzacji

**Problem: Słaba wydajność**
- Włącz przyspieszenie sprzętowe
- Zwiększ liczbę wątków
- Użyj odpowiedniej kwantyzacji
- Sprawdź użycie pamięci GPU

#### Problemy z pamięcią

**Problem: Brak pamięci**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problemy specyficzne dla platformy

#### Windows
- Użyj kompilatora MinGW lub Visual Studio
- Upewnij się, że PATH jest poprawnie skonfigurowany
- Sprawdź, czy antywirus nie zakłóca działania

#### macOS
- Włącz Metal dla Apple Silicon
- Użyj Rosetta 2 dla kompatybilności, jeśli to konieczne
- Sprawdź narzędzia wiersza poleceń Xcode

#### Linux
- Zainstaluj pakiety deweloperskie
- Sprawdź wersje sterowników GPU
- Zweryfikuj instalację zestawu narzędzi CUDA

## Najlepsze praktyki

### Wybór modelu
1. **Wybierz odpowiednią kwantyzację** w zależności od sprzętu
2. **Rozważ rozmiar modelu** w kontekście kompromisu jakości
3. **Testuj różne modele** dla swojego konkretnego przypadku użycia

### Optymalizacja wydajności
1. **Używaj przyspieszenia GPU**, jeśli dostępne
2. **Optymalizuj liczbę wątków** dla swojego CPU
3. **Ustaw odpowiedni rozmiar kontekstu** dla swojego przypadku użycia
4. **Włącz mapowanie pamięci** dla dużych modeli

### Wdrożenie produkcyjne
1. **Używaj trybu serwera** do dostępu przez API
2. **Zaimplementuj odpowiednie obsługi błędów**
3. **Monitoruj użycie zasobów**
4. **Skonfiguruj logowanie i monitorowanie**

### Przepływ pracy deweloperskiej
1. **Zacznij od mniejszych modeli** do testowania
2. **Używaj kontroli wersji** dla konfiguracji modeli
3. **Dokumentuj swoje konfiguracje**
4. **Testuj na różnych platformach**

### Zagadnienia bezpieczeństwa
1. **Waliduj podpowiedzi wejściowe**
2. **Zaimplementuj ograniczenia szybkości**
3. **Zabezpiecz punkty końcowe API**
4. **Monitoruj wzorce nadużyć**

## Podsumowanie

Llama.cpp oferuje potężny i efektywny sposób na lokalne uruchamianie dużych modeli językowych na różnych konfiguracjach sprzętowych. Niezależnie od tego, czy tworzysz aplikacje AI, prowadzisz badania, czy po prostu eksperymentujesz z LLM, ten framework zapewnia elastyczność i wydajność potrzebną do szerokiego zakresu zastosowań.

Kluczowe wnioski:
- Wybierz metodę instalacji, która najlepiej odpowiada Twoim potrzebom
- Optymalizuj konfigurację sprzętową
- Zacznij od podstawowego użycia i stopniowo eksploruj zaawansowane funkcje
- Rozważ użycie powiązań Python dla łatwiejszej integracji
- Przestrzegaj najlepszych praktyk przy wdrożeniach produkcyjnych

Więcej informacji i aktualizacji znajdziesz w [oficjalnym repozytorium Llama.cpp](https://github.com/ggml-org/llama.cpp) oraz w dostępnej dokumentacji i zasobach społeczności.

## ➡️ Co dalej?

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby zapewnić poprawność tłumaczenia, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za autorytatywne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.