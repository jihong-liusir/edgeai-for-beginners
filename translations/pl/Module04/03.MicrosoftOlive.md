<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T15:40:46+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "pl"
}
-->
# Sekcja 3: Microsoft Olive Optimization Suite

## Spis treści
1. [Wprowadzenie](../../../Module04)
2. [Czym jest Microsoft Olive?](../../../Module04)
3. [Instalacja](../../../Module04)
4. [Przewodnik szybkiego startu](../../../Module04)
5. [Przykład: Konwersja Qwen3 do ONNX INT4](../../../Module04)
6. [Zaawansowane użycie](../../../Module04)
7. [Najlepsze praktyki](../../../Module04)
8. [Rozwiązywanie problemów](../../../Module04)
9. [Dodatkowe zasoby](../../../Module04)

## Wprowadzenie

Microsoft Olive to potężne, łatwe w użyciu narzędzie do optymalizacji modeli, które uwzględnia specyfikę sprzętu i upraszcza proces optymalizacji modeli uczenia maszynowego do wdrożenia na różnych platformach sprzętowych. Niezależnie od tego, czy celem są procesory CPU, GPU, czy specjalistyczne akceleratory AI, Olive pomaga osiągnąć optymalną wydajność przy zachowaniu dokładności modelu.

## Czym jest Microsoft Olive?

Olive to łatwe w użyciu narzędzie do optymalizacji modeli uwzględniające specyfikę sprzętu, które łączy w sobie wiodące techniki w zakresie kompresji modeli, optymalizacji i kompilacji. Współpracuje z ONNX Runtime jako kompleksowe rozwiązanie do optymalizacji wnioskowania.

### Kluczowe funkcje

- **Optymalizacja uwzględniająca sprzęt**: Automatycznie wybiera najlepsze techniki optymalizacji dla docelowego sprzętu
- **Ponad 40 wbudowanych komponentów optymalizacyjnych**: Obejmuje kompresję modeli, kwantyzację, optymalizację grafów i inne
- **Prosty interfejs CLI**: Łatwe polecenia do typowych zadań optymalizacyjnych
- **Wsparcie dla wielu frameworków**: Współpracuje z PyTorch, modelami Hugging Face i ONNX
- **Wsparcie dla popularnych modeli**: Olive automatycznie optymalizuje popularne architektury modeli, takie jak Llama, Phi, Qwen, Gemma i inne

### Korzyści

- **Skrócony czas rozwoju**: Brak konieczności ręcznego eksperymentowania z różnymi technikami optymalizacji
- **Zyski wydajnościowe**: Znaczące przyspieszenie (nawet do 6x w niektórych przypadkach)
- **Wieloplatformowe wdrożenie**: Modele zoptymalizowane działają na różnych sprzętach i systemach operacyjnych
- **Zachowana dokładność**: Optymalizacje zachowują jakość modelu przy jednoczesnym zwiększeniu wydajności

## Instalacja

### Wymagania wstępne

- Python 3.8 lub nowszy
- Menedżer pakietów pip
- Wirtualne środowisko (zalecane)

### Podstawowa instalacja

Utwórz i aktywuj wirtualne środowisko:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Zainstaluj Olive z funkcjami automatycznej optymalizacji:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcjonalne zależności

Olive oferuje różne opcjonalne zależności dla dodatkowych funkcji:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Weryfikacja instalacji

```bash
olive --help
```

Jeśli instalacja przebiegła pomyślnie, powinien pojawić się komunikat pomocy CLI Olive.

## Przewodnik szybkiego startu

### Twoja pierwsza optymalizacja

Zoptymalizuj mały model językowy za pomocą funkcji automatycznej optymalizacji Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Co robi to polecenie

Proces optymalizacji obejmuje: pobranie modelu z lokalnej pamięci podręcznej, przechwycenie grafu ONNX i zapisanie wag w pliku danych ONNX, optymalizację grafu ONNX oraz kwantyzację modelu do int4 metodą RTN.

### Wyjaśnienie parametrów polecenia

- `--model_name_or_path`: Identyfikator modelu Hugging Face lub lokalna ścieżka
- `--output_path`: Katalog, w którym zostanie zapisany zoptymalizowany model
- `--device`: Docelowe urządzenie (cpu, gpu)
- `--provider`: Dostawca wykonania (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Użycie ONNX Runtime Generate AI do wnioskowania
- `--precision`: Precyzja kwantyzacji (int4, int8, fp16)
- `--log_level`: Szczegółowość logów (0=minimalna, 1=szczegółowa)

## Przykład: Konwersja Qwen3 do ONNX INT4

Na podstawie podanego przykładu Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), oto jak zoptymalizować model Qwen3:

### Krok 1: Pobierz model (opcjonalne)

Aby zminimalizować czas pobierania, zapisz tylko niezbędne pliki:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Zoptymalizuj model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Przetestuj zoptymalizowany model

Utwórz prosty skrypt w Pythonie, aby przetestować zoptymalizowany model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura wyników

Po optymalizacji katalog wynikowy będzie zawierał:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Zaawansowane użycie

### Pliki konfiguracyjne

Dla bardziej złożonych przepływów optymalizacyjnych możesz użyć plików konfiguracyjnych JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Uruchom z konfiguracją:

```bash
olive run --config config.json
```

### Optymalizacja GPU

Dla optymalizacji GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Dla DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Dostosowanie modeli z Olive

Olive obsługuje również dostosowywanie modeli:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najlepsze praktyki

### 1. Wybór modelu
- Zacznij od mniejszych modeli do testów (np. 0.5B-7B parametrów)
- Upewnij się, że docelowa architektura modelu jest obsługiwana przez Olive

### 2. Uwzględnienie sprzętu
- Dopasuj cel optymalizacji do sprzętu wdrożeniowego
- Użyj optymalizacji GPU, jeśli masz sprzęt zgodny z CUDA
- Rozważ DirectML dla komputerów z Windows i zintegrowaną grafiką

### 3. Wybór precyzji
- **INT4**: Maksymalna kompresja, niewielka utrata dokładności
- **INT8**: Dobry balans między rozmiarem a dokładnością
- **FP16**: Minimalna utrata dokładności, umiarkowane zmniejszenie rozmiaru

### 4. Testowanie i walidacja
- Zawsze testuj zoptymalizowane modele w swoich konkretnych przypadkach użycia
- Porównuj metryki wydajności (opóźnienie, przepustowość, dokładność)
- Używaj reprezentatywnych danych wejściowych do oceny

### 5. Iteracyjna optymalizacja
- Zacznij od automatycznej optymalizacji dla szybkich wyników
- Używaj plików konfiguracyjnych dla precyzyjnej kontroli
- Eksperymentuj z różnymi etapami optymalizacji

## Rozwiązywanie problemów

### Typowe problemy

#### 1. Problemy z instalacją
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemy z CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemy z pamięcią
- Użyj mniejszych rozmiarów partii podczas optymalizacji
- Spróbuj kwantyzacji z wyższą precyzją (int8 zamiast int4)
- Upewnij się, że masz wystarczającą ilość miejsca na dysku na pamięć podręczną modelu

#### 4. Błędy ładowania modelu
- Sprawdź ścieżkę modelu i uprawnienia dostępu
- Upewnij się, że model wymaga `trust_remote_code=True`
- Sprawdź, czy wszystkie wymagane pliki modelu zostały pobrane

### Uzyskiwanie pomocy

- **Dokumentacja**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemy na GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Przykłady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Dodatkowe zasoby

### Oficjalne linki
- **Repozytorium GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Dokumentacja ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Przykład Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Przykłady społeczności
- **Notatniki Jupyter**: Dostępne w repozytorium GitHub Olive
- **Rozszerzenie VS Code**: Rozszerzenie AI Toolkit wykorzystuje Olive do optymalizacji modeli
- **Posty na blogu**: Blog Microsoft Open Source zawiera szczegółowe samouczki Olive

### Powiązane narzędzia
- **ONNX Runtime**: Silnik wnioskowania o wysokiej wydajności
- **Hugging Face Transformers**: Źródło wielu kompatybilnych modeli
- **Azure Machine Learning**: Chmurowe przepływy pracy optymalizacyjne

## ➡️ Co dalej

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za autorytatywne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.