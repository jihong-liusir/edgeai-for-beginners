<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-17T15:56:00+00:00",
  "source_file": "Module03/README.md",
  "language_code": "pl"
}
-->
# Rozdział 03: Wdrażanie Małych Modeli Językowych (SLM)

Ten obszerny rozdział omawia pełny cykl życia wdrażania Małych Modeli Językowych (SLM), obejmując podstawy teoretyczne, praktyczne strategie implementacji oraz gotowe do produkcji rozwiązania konteneryzowane. Rozdział jest podzielony na trzy progresywne sekcje, które prowadzą czytelników od podstawowych po zaawansowane scenariusze wdrożeniowe.

## Struktura Rozdziału i Ścieżka Nauki

### **[Sekcja 1: Zaawansowana Nauka SLM - Podstawy i Optymalizacja](./01.SLMAdvancedLearning.md)**
Pierwsza sekcja ustanawia teoretyczne podstawy zrozumienia Małych Modeli Językowych oraz ich strategicznego znaczenia w wdrożeniach AI na urządzeniach brzegowych. Sekcja obejmuje:

- **Ramka Klasyfikacji Parametrów**: Szczegółowe omówienie kategorii SLM, od Mikro SLM (100M-1,4B parametrów) do Średnich SLM (14B-30B parametrów), ze szczególnym uwzględnieniem modeli takich jak Phi-4-mini-3.8B, seria Qwen3 i Google Gemma3, w tym analiza wymagań sprzętowych i pamięci dla każdego poziomu modelu
- **Zaawansowane Techniki Optymalizacji**: Kompleksowe omówienie metod kwantyzacji z wykorzystaniem frameworków Llama.cpp, Microsoft Olive i Apple MLX, w tym nowoczesnej kwantyzacji BitNET 1-bit z praktycznymi przykładami kodu pokazującymi procesy kwantyzacji i wyniki benchmarków
- **Strategie Pozyskiwania Modeli**: Dogłębna analiza ekosystemu Hugging Face i katalogu modeli Azure AI Foundry dla wdrożeń SLM na poziomie przedsiębiorstwa, z przykładami kodu do programowego pobierania, walidacji i konwersji formatów modeli
- **API dla Programistów**: Przykłady kodu w Pythonie, C++ i C#, pokazujące jak ładować modele, przeprowadzać wnioskowanie i integrować z popularnymi frameworkami, takimi jak PyTorch, TensorFlow i ONNX Runtime

Ta podstawowa sekcja podkreśla równowagę między efektywnością operacyjną, elastycznością wdrożenia i opłacalnością, która sprawia, że SLM są idealne dla scenariuszy obliczeń brzegowych, z praktycznymi przykładami kodu, które programiści mogą bezpośrednio wdrożyć w swoich projektach.

### **[Sekcja 2: Wdrożenie w Lokalnym Środowisku - Rozwiązania Priorytetowe dla Prywatności](./02.DeployingSLMinLocalEnv.md)**
Druga sekcja przechodzi od teorii do praktycznej implementacji, koncentrując się na strategiach lokalnego wdrożenia, które priorytetowo traktują suwerenność danych i niezależność operacyjną. Kluczowe obszary obejmują:

- **Uniwersalna Platforma Ollama**: Kompleksowe omówienie wdrożenia międzyplatformowego z naciskiem na przyjazne dla programistów przepływy pracy, zarządzanie cyklem życia modeli i dostosowanie za pomocą Modelfiles, w tym pełne przykłady integracji REST API i skrypty automatyzacji CLI
- **Microsoft Foundry Local**: Rozwiązania wdrożeniowe na poziomie przedsiębiorstwa z optymalizacją opartą na ONNX, integracją Windows ML i kompleksowymi funkcjami bezpieczeństwa, z przykładami kodu w C# i Pythonie do natywnej integracji aplikacji
- **Analiza Porównawcza**: Szczegółowe porównanie frameworków obejmujące architekturę techniczną, charakterystyki wydajności i wytyczne optymalizacji przypadków użycia, z kodem benchmarkowym do oceny szybkości wnioskowania i wykorzystania pamięci na różnych urządzeniach
- **Integracja API**: Przykładowe aplikacje pokazujące, jak budować usługi internetowe, aplikacje czatowe i potoki przetwarzania danych z wykorzystaniem lokalnych wdrożeń SLM, z przykładami kodu w Node.js, Python Flask/FastAPI i ASP.NET Core
- **Frameworki Testowe**: Podejścia do automatycznego testowania jakości modeli, w tym przykłady testów jednostkowych i integracyjnych dla implementacji SLM

Ta sekcja dostarcza praktycznych wskazówek dla organizacji, które chcą wdrożyć rozwiązania AI chroniące prywatność, jednocześnie zachowując pełną kontrolę nad swoim środowiskiem wdrożeniowym, z gotowymi do użycia przykładami kodu, które programiści mogą dostosować do swoich specyficznych wymagań.

### **[Sekcja 3: Konteneryzowane Wdrożenie w Chmurze - Rozwiązania na Skalę Produkcyjną](./03.DeployingSLMinCloud.md)**
Ostatnia sekcja kończy się zaawansowanymi strategiami wdrożenia konteneryzowanego, z modelem Microsoft Phi-4-mini-instruct jako głównym studium przypadku. Sekcja obejmuje:

- **Wdrożenie vLLM**: Optymalizacja wnioskowania o wysokiej wydajności z kompatybilnymi API OpenAI, zaawansowaną akceleracją GPU i konfiguracją na poziomie produkcyjnym, w tym kompletne Dockerfiles, manifesty Kubernetes i parametry dostrajania wydajności
- **Orkiestracja Kontenerów Ollama**: Uproszczone przepływy pracy wdrożeniowego z Docker Compose, warianty optymalizacji modeli i integracja interfejsu webowego, z przykładami pipeline'ów CI/CD do automatycznego wdrożenia i testowania
- **Implementacja ONNX Runtime**: Wdrożenie zoptymalizowane dla urządzeń brzegowych z kompleksową konwersją modeli, strategiami kwantyzacji i kompatybilnością międzyplatformową, w tym szczegółowe przykłady kodu do optymalizacji i wdrożenia modeli
- **Monitorowanie i Obserwowalność**: Implementacja dashboardów Prometheus/Grafana z niestandardowymi metrykami do monitorowania wydajności SLM, w tym konfiguracje alertów i agregacja logów
- **Równoważenie Obciążenia i Skalowanie**: Praktyczne przykłady strategii skalowania poziomego i pionowego z konfiguracjami autoskalowania opartymi na wykorzystaniu CPU/GPU i wzorcach żądań
- **Wzmocnienie Bezpieczeństwa**: Najlepsze praktyki bezpieczeństwa kontenerów, w tym redukcja uprawnień, polityki sieciowe i zarządzanie tajemnicami dla kluczy API i poświadczeń dostępu do modeli

Każde podejście wdrożeniowe jest przedstawione z kompletnymi przykładami konfiguracji, procedurami testowymi, listami kontrolnymi gotowości produkcyjnej i szablonami infrastruktury jako kodu, które programiści mogą bezpośrednio zastosować w swoich przepływach pracy wdrożeniowej.

## Kluczowe Wyniki Nauki

Po ukończeniu tego rozdziału czytelnicy opanują:

1. **Strategiczny Dobór Modeli**: Zrozumienie granic parametrów i wybór odpowiednich SLM na podstawie ograniczeń zasobów i wymagań wydajnościowych
2. **Mistrzostwo Optymalizacji**: Implementacja zaawansowanych technik kwantyzacji w różnych frameworkach w celu osiągnięcia optymalnej równowagi między wydajnością a efektywnością
3. **Elastyczność Wdrożenia**: Wybór między lokalnymi rozwiązaniami priorytetowymi dla prywatności a skalowalnymi wdrożeniami konteneryzowanymi w zależności od potrzeb organizacji
4. **Gotowość Produkcyjna**: Konfiguracja systemów monitorowania, bezpieczeństwa i skalowania dla wdrożeń SLM na poziomie przedsiębiorstwa

## Praktyczne Skupienie i Zastosowania w Rzeczywistości

Rozdział utrzymuje silną orientację praktyczną, prezentując:

- **Przykłady Praktyczne**: Kompletną konfigurację plików, procedury testowania API i skrypty wdrożeniowe
- **Benchmarking Wydajności**: Szczegółowe porównania szybkości wnioskowania, wykorzystania pamięci i wymagań zasobowych
- **Rozważania Bezpieczeństwa**: Praktyki bezpieczeństwa na poziomie przedsiębiorstwa, ramy zgodności i strategie ochrony danych
- **Najlepsze Praktyki**: Wytyczne sprawdzone w produkcji dotyczące monitorowania, skalowania i utrzymania

## Perspektywa Przyszłościowa

Rozdział kończy się spojrzeniem w przyszłość na pojawiające się trendy, w tym:

- Zaawansowane architektury modeli z poprawionymi współczynnikami efektywności
- Głębszą integrację sprzętową ze specjalistycznymi akceleratorami AI
- Ewolucję ekosystemu w kierunku standaryzacji i interoperacyjności
- Wzorce adopcji w przedsiębiorstwach napędzane prywatnością i wymaganiami zgodności

To kompleksowe podejście zapewnia, że czytelnicy są dobrze przygotowani do radzenia sobie zarówno z obecnymi wyzwaniami wdrożeniowymi SLM, jak i przyszłymi rozwojami technologicznymi, podejmując świadome decyzje, które są zgodne z ich specyficznymi wymaganiami i ograniczeniami organizacyjnymi.

Rozdział służy zarówno jako praktyczny przewodnik do natychmiastowej implementacji, jak i strategiczny zasób do długoterminowego planowania wdrożeń AI, podkreślając kluczową równowagę między możliwościami, efektywnością i doskonałością operacyjną, która definiuje udane wdrożenia SLM.

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za autorytatywne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.