<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T16:04:50+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "pl"
}
-->
# Kontenerowa Implementacja w Chmurze - Rozwiązania na Skalę Produkcyjną

Ten kompleksowy poradnik obejmuje trzy główne podejścia do wdrażania modelu Phi-4-mini-instruct firmy Microsoft w środowiskach kontenerowych: vLLM, Ollama oraz SLM Engine z ONNX Runtime. Ten model o 3,8 miliarda parametrów stanowi optymalny wybór do zadań wymagających rozumowania, jednocześnie zachowując efektywność dla wdrożeń na urządzeniach brzegowych.

## Spis Treści

1. [Wprowadzenie do wdrożenia Phi-4-mini w kontenerach](../../../Module03)
2. [Cele nauki](../../../Module03)
3. [Zrozumienie klasyfikacji Phi-4-mini](../../../Module03)
4. [Wdrożenie kontenerowe vLLM](../../../Module03)
5. [Wdrożenie kontenerowe Ollama](../../../Module03)
6. [SLM Engine z ONNX Runtime](../../../Module03)
7. [Porównanie frameworków](../../../Module03)
8. [Najlepsze praktyki](../../../Module03)

## Wprowadzenie do wdrożenia Phi-4-mini w kontenerach

Małe modele językowe (SLM) stanowią kluczowy postęp w EdgeAI, umożliwiając zaawansowane przetwarzanie języka naturalnego na urządzeniach o ograniczonych zasobach. Ten poradnik koncentruje się na strategiach wdrożenia kontenerowego dla modelu Phi-4-mini-instruct firmy Microsoft, który łączy zaawansowane możliwości rozumowania z efektywnością.

### Prezentowany model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 miliarda parametrów)**: Najnowszy lekki model firmy Microsoft, dostosowany do środowisk o ograniczonej pamięci i mocy obliczeniowej, oferujący wyjątkowe możliwości w:
- **Rozumowaniu matematycznym i skomplikowanych obliczeniach**
- **Generowaniu, debugowaniu i analizie kodu**
- **Rozwiązywaniu problemów logicznych i rozumowaniu krok po kroku**
- **Zastosowaniach edukacyjnych wymagających szczegółowych wyjaśnień**
- **Wywoływaniu funkcji i integracji narzędzi**

Phi-4-mini należy do kategorii "Małych SLM" (1,5B - 13,9B parametrów), oferując idealne połączenie zdolności rozumowania i efektywności zasobowej.

### Korzyści z wdrożenia Phi-4-mini w kontenerach

- **Efektywność operacyjna**: Szybkie wnioskowanie dla zadań rozumowania przy niższych wymaganiach obliczeniowych
- **Elastyczność wdrożenia**: Możliwości AI na urządzeniu z poprawioną prywatnością dzięki lokalnemu przetwarzaniu
- **Opłacalność**: Niższe koszty operacyjne w porównaniu do większych modeli przy zachowaniu wysokiej jakości
- **Izolacja**: Czyste oddzielenie instancji modelu i bezpieczne środowiska wykonawcze
- **Skalowalność**: Łatwe skalowanie poziome dla zwiększenia przepustowości rozumowania

## Cele nauki

Po ukończeniu tego poradnika będziesz w stanie:

- Wdrażać i optymalizować Phi-4-mini-instruct w różnych środowiskach kontenerowych
- Stosować zaawansowane strategie kwantyzacji i kompresji dla różnych scenariuszy wdrożenia
- Konfigurować kontenerową orkiestrację gotową do produkcji dla obciążeń związanych z rozumowaniem
- Ocenić i wybrać odpowiednie frameworki wdrożeniowe na podstawie specyficznych wymagań
- Stosować najlepsze praktyki w zakresie bezpieczeństwa, monitorowania i skalowania dla kontenerowych wdrożeń SLM

## Zrozumienie klasyfikacji Phi-4-mini

### Specyfikacje modelu

**Szczegóły techniczne:**
- **Parametry**: 3,8 miliarda (kategoria Małych SLM)
- **Architektura**: Gęsty dekoderowy Transformer z grupowaną uwagą zapytań
- **Długość kontekstu**: 128K tokenów (32K zalecane dla optymalnej wydajności)
- **Słownictwo**: 200K tokenów z obsługą wielojęzyczną
- **Dane treningowe**: 5T tokenów wysokiej jakości treści bogatej w rozumowanie

### Wymagania zasobowe

| Typ wdrożenia | Min RAM | Zalecana RAM | VRAM (GPU) | Pamięć | Typowe zastosowania |
|---------------|---------|--------------|------------|--------|---------------------|
| **Rozwój** | 6GB | 8GB | - | 8GB | Testowanie lokalne, prototypowanie |
| **Produkcja CPU** | 8GB | 12GB | - | 10GB | Serwery brzegowe, wdrożenie zoptymalizowane kosztowo |
| **Produkcja GPU** | 6GB | 8GB | 4-6GB | 8GB | Usługi rozumowania o wysokiej przepustowości |
| **Optymalizacja brzegowa** | 4GB | 6GB | - | 6GB | Wdrożenie kwantyzowane, bramy IoT |

### Możliwości Phi-4-mini

- **Doskonałość matematyczna**: Zaawansowane rozwiązywanie problemów z arytmetyki, algebry i rachunku różniczkowego
- **Inteligencja kodu**: Generowanie kodu w Pythonie, JavaScript i innych językach z debugowaniem
- **Rozumowanie logiczne**: Rozkładanie problemów na kroki i konstruowanie rozwiązań
- **Wsparcie edukacyjne**: Szczegółowe wyjaśnienia odpowiednie do nauki i nauczania
- **Wywoływanie funkcji**: Wbudowana obsługa integracji narzędzi i interakcji z API

## Wdrożenie kontenerowe vLLM

vLLM oferuje doskonałe wsparcie dla Phi-4-mini-instruct dzięki zoptymalizowanej wydajności wnioskowania i kompatybilnym API OpenAI, co czyni go idealnym rozwiązaniem dla usług rozumowania w produkcji.

### Przykłady szybkiego startu

#### Podstawowe wdrożenie CPU (Rozwój)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Wdrożenie produkcyjne z akceleracją GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Konfiguracja produkcyjna

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testowanie możliwości rozumowania Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Wdrożenie kontenerowe Ollama

Ollama oferuje doskonałe wsparcie dla Phi-4-mini-instruct dzięki uproszczonemu wdrożeniu i zarządzaniu, co czyni go idealnym rozwiązaniem dla rozwoju i zrównoważonych wdrożeń produkcyjnych.

### Szybka konfiguracja

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Konfiguracja produkcyjna

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optymalizacja modelu i warianty

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Przykłady użycia API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine z ONNX Runtime

ONNX Runtime zapewnia optymalną wydajność dla wdrożeń brzegowych Phi-4-mini-instruct dzięki zaawansowanej optymalizacji i kompatybilności międzyplatformowej.

### Podstawowa konfiguracja

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Uproszczona implementacja serwera

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Skrypt konwersji modelu

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Konfiguracja produkcyjna

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testowanie wdrożenia ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Porównanie frameworków

### Porównanie frameworków dla Phi-4-mini

| Funkcja | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Złożoność konfiguracji** | Średnia | Łatwa | Złożona |
| **Wydajność (GPU)** | Doskonała (~25 tok/s) | Bardzo dobra (~20 tok/s) | Dobra (~15 tok/s) |
| **Wydajność (CPU)** | Dobra (~8 tok/s) | Bardzo dobra (~12 tok/s) | Doskonała (~15 tok/s) |
| **Zużycie pamięci** | 8-12GB | 6-10GB | 4-8GB |
| **Kompatybilność API** | Kompatybilne z OpenAI | REST własne | FastAPI własne |
| **Wywoływanie funkcji** | ✅ Wbudowane | ✅ Obsługiwane | ⚠️ Własna implementacja |
| **Wsparcie kwantyzacji** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Kwantyzacja ONNX |
| **Gotowość produkcyjna** | ✅ Doskonała | ✅ Bardzo dobra | ✅ Dobra |
| **Wdrożenie brzegowe** | Dobre | Doskonałe | Wybitne |

## Dodatkowe zasoby

### Oficjalna dokumentacja
- **Karta modelu Phi-4 firmy Microsoft**: Szczegółowe specyfikacje i wytyczne dotyczące użytkowania
- **Dokumentacja vLLM**: Zaawansowane opcje konfiguracji i optymalizacji
- **Biblioteka modeli Ollama**: Modele społecznościowe i przykłady dostosowania
- **Przewodniki ONNX Runtime**: Optymalizacja wydajności i strategie wdrożenia

### Narzędzia rozwojowe
- **Hugging Face Transformers**: Do interakcji z modelem i dostosowania
- **Specyfikacja API OpenAI**: Do testowania kompatybilności vLLM
- **Najlepsze praktyki Docker**: Bezpieczeństwo kontenerów i optymalizacja
- **Wdrożenie Kubernetes**: Wzorce orkiestracji dla skalowania produkcji

### Zasoby edukacyjne
- **Benchmarking wydajności SLM**: Metodologie analizy porównawczej
- **Wdrożenie Edge AI**: Najlepsze praktyki dla środowisk o ograniczonych zasobach
- **Optymalizacja zadań rozumowania**: Strategie podpowiedzi dla problemów matematycznych i logicznych
- **Bezpieczeństwo kontenerów**: Praktyki wzmacniania dla wdrożeń modeli AI

## Efekty nauki

Po ukończeniu tego modułu będziesz w stanie:

1. Wdrażać model Phi-4-mini-instruct w środowiskach kontenerowych za pomocą różnych frameworków
2. Konfigurować i optymalizować wdrożenia SLM dla różnych środowisk sprzętowych
3. Wdrażać najlepsze praktyki bezpieczeństwa dla kontenerowych wdrożeń AI
4. Porównywać i wybierać odpowiednie frameworki wdrożeniowe na podstawie specyficznych wymagań
5. Stosować strategie monitorowania i skalowania dla usług SLM na poziomie produkcyjnym

## Co dalej

- Powrót do [Modułu 1](../Module01/README.md)
- Powrót do [Modułu 2](../Module02/README.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby zapewnić poprawność tłumaczenia, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.