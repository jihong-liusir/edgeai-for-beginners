<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-08T21:29:18+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "pl"
}
-->
# Sekcja 1: Zaawansowane nauczanie SLM - Podstawy i optymalizacja

Małe modele językowe (SLM) stanowią kluczowy postęp w EdgeAI, umożliwiając zaawansowane przetwarzanie języka naturalnego na urządzeniach o ograniczonych zasobach. Zrozumienie, jak skutecznie wdrażać, optymalizować i wykorzystywać SLM, jest niezbędne do budowania praktycznych rozwiązań AI opartych na edge.

## Wprowadzenie

W tej lekcji zgłębimy temat małych modeli językowych (SLM) oraz ich zaawansowanych strategii implementacji. Omówimy podstawowe pojęcia dotyczące SLM, ich granice parametrów i klasyfikacje, techniki optymalizacji oraz praktyczne strategie wdrażania w środowiskach obliczeniowych edge.

## Cele nauczania

Po ukończeniu tej lekcji będziesz w stanie:

- 🔢 Zrozumieć granice parametrów i klasyfikacje małych modeli językowych.
- 🛠️ Zidentyfikować kluczowe techniki optymalizacji dla wdrożenia SLM na urządzeniach edge.
- 🚀 Poznać zaawansowane strategie kwantyzacji i kompresji dla SLM.

## Zrozumienie granic parametrów i klasyfikacji SLM

Małe modele językowe (SLM) to modele AI zaprojektowane do przetwarzania, rozumienia i generowania treści w języku naturalnym, posiadające znacznie mniej parametrów niż ich większe odpowiedniki. Podczas gdy duże modele językowe (LLM) zawierają setki miliardów do bilionów parametrów, SLM są specjalnie zaprojektowane z myślą o efektywności i wdrożeniu na urządzeniach edge.

Ramka klasyfikacji parametrów pomaga zrozumieć różne kategorie SLM i ich odpowiednie zastosowania. Ta klasyfikacja jest kluczowa przy wyborze odpowiedniego modelu dla konkretnych scenariuszy obliczeniowych edge.

### Ramka klasyfikacji parametrów

Zrozumienie granic parametrów pomaga w wyborze odpowiednich modeli dla różnych scenariuszy obliczeniowych edge:

- **🔬 Mikro SLM**: 100M - 1,4B parametrów (ultralekkie dla urządzeń mobilnych)
- **📱 Małe SLM**: 1,5B - 13,9B parametrów (zrównoważona wydajność i efektywność)
- **⚖️ Średnie SLM**: 14B - 30B parametrów (zbliżające się do możliwości LLM, zachowując efektywność)

Dokładne granice pozostają płynne w społeczności badawczej, ale większość praktyków uważa modele z mniej niż 30 miliardami parametrów za "małe", a niektóre źródła ustalają próg nawet niżej, na poziomie 10 miliardów parametrów.

### Kluczowe zalety SLM

SLM oferują kilka podstawowych zalet, które czynią je idealnymi dla aplikacji obliczeniowych edge:

**Efektywność operacyjna**: SLM zapewniają szybsze czasy wnioskowania dzięki mniejszej liczbie parametrów do przetworzenia, co czyni je idealnymi dla aplikacji w czasie rzeczywistym. Wymagają mniejszych zasobów obliczeniowych, umożliwiając wdrożenie na urządzeniach o ograniczonych zasobach, jednocześnie zużywając mniej energii i zmniejszając ślad węglowy.

**Elastyczność wdrożenia**: Modele te umożliwiają funkcje AI na urządzeniu bez konieczności połączenia z internetem, zwiększają prywatność i bezpieczeństwo dzięki lokalnemu przetwarzaniu, mogą być dostosowane do aplikacji specyficznych dla danej dziedziny i nadają się do różnych środowisk obliczeniowych edge.

**Efektywność kosztowa**: SLM oferują opłacalne szkolenie i wdrożenie w porównaniu z LLM, z niższymi kosztami operacyjnymi i mniejszymi wymaganiami dotyczącymi przepustowości dla aplikacji edge.

## Zaawansowane strategie pozyskiwania modeli

### Ekosystem Hugging Face

Hugging Face jest głównym centrum odkrywania i uzyskiwania dostępu do najnowocześniejszych SLM. Platforma oferuje kompleksowe zasoby do odkrywania i wdrażania modeli:

**Funkcje odkrywania modeli**: Platforma oferuje zaawansowane filtrowanie według liczby parametrów, rodzaju licencji i metryk wydajności. Użytkownicy mogą korzystać z narzędzi do porównywania modeli obok siebie, wyników benchmarków wydajności w czasie rzeczywistym oraz demonstracji WebGPU do natychmiastowego testowania.

**Kolekcje SLM**: Popularne modele obejmują Phi-4-mini-3.8B do zaawansowanych zadań rozumowania, serię Qwen3 (0.6B/1.7B/4B) do aplikacji wielojęzycznych, Google Gemma3 do efektywnych zadań ogólnego przeznaczenia oraz modele eksperymentalne, takie jak BitNET do wdrożeń o ultra-niskiej precyzji. Platforma zawiera również kolekcje tworzone przez społeczność z wyspecjalizowanymi modelami dla określonych dziedzin oraz warianty wstępnie wytrenowane i dostosowane do instrukcji, zoptymalizowane pod kątem różnych zastosowań.

### Katalog modeli Azure AI Foundry

Katalog modeli Azure AI Foundry zapewnia dostęp do SLM klasy korporacyjnej z ulepszonymi możliwościami integracji:

**Integracja korporacyjna**: Katalog zawiera modele sprzedawane bezpośrednio przez Azure z wsparciem klasy korporacyjnej i umowami SLA, w tym Phi-4-mini-3.8B do zaawansowanych możliwości rozumowania oraz Llama 3-8B do wdrożeń produkcyjnych. Zawiera również modele, takie jak Qwen3 8B, od zaufanych zewnętrznych źródeł open source.

**Korzyści dla przedsiębiorstw**: Wbudowane narzędzia do dostrajania, obserwowalności i odpowiedzialnej AI są zintegrowane z elastycznym Provisioned Throughput w różnych rodzinach modeli. Bezpośrednie wsparcie Microsoftu z umowami SLA dla przedsiębiorstw, zintegrowane funkcje bezpieczeństwa i zgodności oraz kompleksowe przepływy pracy wdrożeniowe poprawiają doświadczenie korporacyjne.

## Zaawansowane techniki kwantyzacji i optymalizacji

### Framework optymalizacji Llama.cpp

Llama.cpp oferuje najnowocześniejsze techniki kwantyzacji dla maksymalnej efektywności w wdrożeniach edge:

**Metody kwantyzacji**: Framework obsługuje różne poziomy kwantyzacji, w tym Q4_0 (kwantyzacja 4-bitowa z doskonałą redukcją rozmiaru - idealna dla mobilnych wdrożeń Qwen3-0.6B), Q5_1 (kwantyzacja 5-bitowa równoważąca jakość i kompresję - odpowiednia dla wnioskowania edge Phi-4-mini-3.8B) oraz Q8_0 (kwantyzacja 8-bitowa dla jakości zbliżonej do oryginału - zalecana dla produkcyjnego użycia Google Gemma3). BitNET reprezentuje najnowsze osiągnięcia z kwantyzacją 1-bitową dla ekstremalnych scenariuszy kompresji.

**Korzyści z implementacji**: Wnioskowanie zoptymalizowane pod kątem CPU z akceleracją SIMD zapewnia efektywne ładowanie i wykonywanie modeli w pamięci. Kompatybilność międzyplatformowa na architekturach x86, ARM i Apple Silicon umożliwia wdrożenia niezależne od sprzętu.

**Przykład praktycznej implementacji**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Porównanie zużycia pamięci**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Suite optymalizacji Microsoft Olive

Microsoft Olive oferuje kompleksowe przepływy pracy optymalizacji modeli zaprojektowane dla środowisk produkcyjnych:

**Techniki optymalizacji**: Suite obejmuje dynamiczną kwantyzację do automatycznego wyboru precyzji (szczególnie skuteczną w przypadku modeli serii Qwen3), optymalizację grafów i fuzję operatorów (zoptymalizowaną dla architektury Google Gemma3), optymalizacje specyficzne dla sprzętu dla CPU, GPU i NPU (ze specjalnym wsparciem dla Phi-4-mini-3.8B na urządzeniach ARM) oraz wieloetapowe przepływy pracy optymalizacji. Modele BitNET wymagają specjalistycznych przepływów pracy kwantyzacji 1-bitowej w ramach Olive.

**Automatyzacja przepływów pracy**: Automatyczne benchmarki dla wariantów optymalizacji zapewniają zachowanie metryk jakości podczas optymalizacji. Integracja z popularnymi frameworkami ML, takimi jak PyTorch i ONNX, zapewnia optymalizację wdrożeń w chmurze i na edge.

**Przykład praktycznej implementacji**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Framework Apple MLX

Apple MLX oferuje natywną optymalizację zaprojektowaną specjalnie dla urządzeń Apple Silicon:

**Optymalizacja dla Apple Silicon**: Framework wykorzystuje zintegrowaną architekturę pamięci z integracją Metal Performance Shaders, automatyczne wnioskowanie o mieszanej precyzji (szczególnie skuteczne w przypadku Google Gemma3) oraz zoptymalizowane wykorzystanie przepustowości pamięci. Phi-4-mini-3.8B wykazuje wyjątkową wydajność na chipach serii M, podczas gdy Qwen3-1.7B zapewnia optymalną równowagę dla wdrożeń na MacBook Air.

**Funkcje rozwojowe**: Obsługa API w Pythonie i Swift z operacjami na tablicach kompatybilnymi z NumPy, możliwości automatycznego różnicowania oraz bezproblemowa integracja z narzędziami rozwojowymi Apple zapewniają kompleksowe środowisko rozwojowe.

**Przykład praktycznej implementacji**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategie wdrożenia produkcyjnego i wnioskowania

### Ollama: Uproszczone lokalne wdrożenie

Ollama upraszcza wdrożenie SLM dzięki funkcjom gotowym do użycia w środowiskach lokalnych i edge:

**Możliwości wdrożenia**: Instalacja i wykonanie modelu za pomocą jednego polecenia z automatycznym pobieraniem i buforowaniem modeli. Obsługa Phi-4-mini-3.8B, całej serii Qwen3 (0.6B/1.7B/4B) oraz Google Gemma3 z REST API do integracji aplikacji oraz możliwości zarządzania i przełączania między wieloma modelami. Modele BitNET wymagają eksperymentalnych konfiguracji buildów dla wsparcia kwantyzacji 1-bitowej.

**Zaawansowane funkcje**: Obsługa dostrajania modeli, generowanie plików Dockerfile do wdrożeń kontenerowych, akceleracja GPU z automatycznym wykrywaniem oraz opcje kwantyzacji i optymalizacji modeli zapewniają kompleksową elastyczność wdrożenia.

### VLLM: Wnioskowanie o wysokiej wydajności

VLLM oferuje optymalizację wnioskowania klasy produkcyjnej dla scenariuszy o wysokiej przepustowości:

**Optymalizacje wydajności**: PagedAttention dla efektywnego obliczania uwagi w pamięci (szczególnie korzystne dla architektury transformatora Phi-4-mini-3.8B), dynamiczne grupowanie dla optymalizacji przepustowości (zoptymalizowane dla równoległego przetwarzania serii Qwen3), równoległość tensorów dla skalowania na wielu GPU (obsługa Google Gemma3) oraz dekodowanie spekulacyjne dla redukcji opóźnień. Modele BitNET wymagają specjalistycznych jąder wnioskowania dla operacji 1-bitowych.

**Integracja korporacyjna**: Punkty końcowe API kompatybilne z OpenAI, wsparcie wdrożenia Kubernetes, integracja monitorowania i obserwowalności oraz możliwości automatycznego skalowania zapewniają rozwiązania wdrożeniowe klasy korporacyjnej.

### Foundry Local: Rozwiązanie edge Microsoftu

Foundry Local oferuje kompleksowe możliwości wdrożenia edge dla środowisk korporacyjnych:

**Funkcje obliczeniowe edge**: Projektowanie architektury offline-first z optymalizacją ograniczeń zasobów, zarządzanie lokalnym rejestrem modeli oraz możliwości synchronizacji edge-to-cloud zapewniają niezawodne wdrożenie edge.

**Bezpieczeństwo i zgodność**: Lokalna obróbka danych dla zachowania prywatności, kontrola bezpieczeństwa klasy korporacyjnej, rejestrowanie audytów i raportowanie zgodności oraz zarządzanie dostępem opartym na rolach zapewniają kompleksowe bezpieczeństwo dla wdrożeń edge.

## Najlepsze praktyki wdrożenia SLM

### Wytyczne dotyczące wyboru modelu

Podczas wyboru SLM do wdrożenia edge należy wziąć pod uwagę następujące czynniki:

**Rozważania dotyczące liczby parametrów**: Wybierz mikro SLM, takie jak Qwen3-0.6B, do ultralekkich aplikacji mobilnych, małe SLM, takie jak Qwen3-1.7B lub Google Gemma3, do scenariuszy zrównoważonej wydajności, oraz średnie SLM, takie jak Phi-4-mini-3.8B lub Qwen3-4B, gdy zbliżasz się do możliwości LLM, zachowując efektywność. Modele BitNET oferują eksperymentalną ultra-kompresję dla określonych zastosowań badawczych.

**Dopasowanie do przypadku użycia**: Dopasuj możliwości modelu do specyficznych wymagań aplikacji, biorąc pod uwagę takie czynniki jak jakość odpowiedzi, szybkość wnioskowania, ograniczenia pamięci i wymagania dotyczące pracy offline.

### Wybór strategii optymalizacji

**Podejście do kwantyzacji**: Wybierz odpowiednie poziomy kwantyzacji w zależności od wymagań jakościowych i ograniczeń sprzętowych. Rozważ Q4_0 dla maksymalnej kompresji (idealne dla mobilnych wdrożeń Qwen3-0.6B), Q5_1 dla zrównoważonego kompromisu między jakością a kompresją (odpowiednie dla Phi-4-mini-3.8B i Google Gemma3) oraz Q8_0 dla zachowania jakości zbliżonej do oryginału (zalecane dla środowisk produkcyjnych Qwen3-4B). Kwantyzacja 1-bitowa BitNET reprezentuje ekstremalny front kompresji dla wyspecjalizowanych zastosowań.

**Wybór frameworku**: Wybierz frameworki optymalizacyjne w zależności od docelowego sprzętu i wymagań wdrożeniowych. Użyj Llama.cpp do wdrożeń zoptymalizowanych pod kątem CPU, Microsoft Olive do kompleksowych przepływów pracy optymalizacyjnych oraz Apple MLX dla urządzeń Apple Silicon.

## Praktyczne przykłady modeli i przypadki użycia

### Scenariusze wdrożenia w rzeczywistych warunkach

**Aplikacje mobilne**: Qwen3-0.6B doskonale sprawdza się w aplikacjach chatbotów na smartfony z minimalnym zużyciem pamięci, podczas gdy Google Gemma3 zapewnia zrównoważoną wydajność dla narzędzi edukacyjnych na tabletach. Phi-4-mini-3.8B oferuje zaawansowane możliwości rozumowania dla aplikacji produktywności mobilnej.

**Komputery stacjonarne i obliczenia edge**: Qwen3-1.7B zapewnia optymalną wydajność dla aplikacji asystentów na komputerach stacjonarnych, Phi-4-mini-3.8B oferuje zaawansowane możliwości generowania kodu dla narzędzi dla programistów, a Qwen3-4B umożliwia zaawansowaną analizę dokumentów w środowiskach stacji roboczych.

**Badania i eksperymenty**: Modele BitNET umożliwiają eksplorację ultra-niskiej precyzji wnioskowania dla badań akademickich i aplikacji typu proof-of-concept wymagających ekstremalnych ograniczeń zasobów.

### Benchmarki wydajności i porównania

**Szybkość wnioskowania**: Qwen3-0.6B osiąga najszybsze czasy wnioskowania na mobilnych CPU, Google Gemma3 zapewnia zrównoważony stosunek szybkości do jakości dla ogólnych aplikacji, Phi-4-mini-3.8B oferuje doskonałą szybkość rozumowania dla złożonych zadań, a BitNET zapewnia teoretyczne maksymalne przepustowości przy użyciu wyspecjalizowanego sprzętu.

**Wymagania pamięciowe**: Zużycie pamięci przez modele waha się od Qwen3-0.6B (poniżej 1GB po kwantyzacji) do Phi-4-mini-3.8B

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za autorytatywne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.