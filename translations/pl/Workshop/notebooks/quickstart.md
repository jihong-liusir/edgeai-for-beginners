<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T22:04:16+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "pl"
}
-->
# Warsztaty Notebooks - Szybki Przewodnik

## Spis Tre≈õci

- [Wymagania wstƒôpne](../../../../Workshop/notebooks)
- [PoczƒÖtkowa konfiguracja](../../../../Workshop/notebooks)
- [Sesja 04: Por√≥wnanie modeli](../../../../Workshop/notebooks)
- [Sesja 05: Orkiestrator wieloagentowy](../../../../Workshop/notebooks)
- [Sesja 06: Routing modeli oparty na intencjach](../../../../Workshop/notebooks)
- [Zmienne ≈õrodowiskowe](../../../../Workshop/notebooks)
- [Popularne polecenia](../../../../Workshop/notebooks)

---

## Wymagania wstƒôpne

### 1. Zainstaluj Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Weryfikacja instalacji:**
```bash
foundry --version
```

### 2. Zainstaluj zale≈ºno≈õci Pythona

```bash
cd Workshop
pip install -r requirements.txt
```

Lub zainstaluj pojedynczo:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## PoczƒÖtkowa konfiguracja

### Uruchom us≈Çugƒô Foundry Local

**Wymagane przed uruchomieniem jakiegokolwiek notebooka:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

Oczekiwany wynik:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### Pobierz i za≈Çaduj modele

Notatniki domy≈õlnie u≈ºywajƒÖ nastƒôpujƒÖcych modeli:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Weryfikacja konfiguracji

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Sesja 04: Por√≥wnanie modeli

### Cel
Por√≥wnanie wydajno≈õci miƒôdzy ma≈Çymi modelami jƒôzykowymi (SLM) a du≈ºymi modelami jƒôzykowymi (LLM).

### Szybka konfiguracja

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Uruchamianie notebooka

1. **Otw√≥rz** `session04_model_compare.ipynb` w VS Code lub Jupyter
2. **Zrestartuj kernel** (Kernel ‚Üí Restart Kernel)
3. **Uruchom wszystkie kom√≥rki** w kolejno≈õci

### Kluczowa konfiguracja

**Domy≈õlne modele:**
- **SLM:** `phi-4-mini` (~4GB RAM, szybszy)
- **LLM:** `qwen2.5-3b` (~3GB RAM, zoptymalizowany pod kƒÖtem pamiƒôci)

**Zmienne ≈õrodowiskowe (opcjonalne):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### Oczekiwany wynik

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### Dostosowanie

**U≈ºyj innych modeli:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**W≈Çasny prompt:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Lista kontrolna weryfikacji

- [ ] Kom√≥rka 12 pokazuje poprawne modele (phi-4-mini, qwen2.5-3b)
- [ ] Kom√≥rka 12 pokazuje poprawny endpoint (port 59959)
- [ ] Diagnostyka w kom√≥rce 16 przechodzi (‚úÖ Us≈Çuga dzia≈Ça)
- [ ] Kontrola przed uruchomieniem w kom√≥rce 20 przechodzi (oba modele ok)
- [ ] Por√≥wnanie w kom√≥rce 22 ko≈Ñczy siƒô z warto≈õciami op√≥≈∫nienia
- [ ] Walidacja w kom√≥rce 24 pokazuje üéâ WSZYSTKIE TESTY ZALICZONE!

### Szacowany czas
- **Pierwsze uruchomienie:** 5-10 minut (w tym pobieranie modeli)
- **Kolejne uruchomienia:** 1-2 minuty

---

## Sesja 05: Orkiestrator wieloagentowy

### Cel
Demonstracja wsp√≥≈Çpracy wieloagentowej za pomocƒÖ Foundry Local SDK - agenci wsp√≥≈ÇpracujƒÖ, aby uzyskaƒá bardziej dopracowane wyniki.

### Szybka konfiguracja

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Uruchamianie notebooka

1. **Otw√≥rz** `session05_agents_orchestrator.ipynb`
2. **Zrestartuj kernel**
3. **Uruchom wszystkie kom√≥rki** w kolejno≈õci

### Kluczowa konfiguracja

**Domy≈õlna konfiguracja (ten sam model dla obu agent√≥w):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Zaawansowana konfiguracja (r√≥≈ºne modele):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Architektura

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### Oczekiwany wynik

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Rozszerzenia

**Dodaj wiƒôcej agent√≥w:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Testowanie wsadowe:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Szacowany czas
- **Pierwsze uruchomienie:** 3-5 minut
- **Kolejne uruchomienia:** 1-2 minuty na pytanie

---

## Sesja 06: Routing modeli oparty na intencjach

### Cel
Inteligentne kierowanie prompt√≥w do wyspecjalizowanych modeli na podstawie wykrytej intencji.

### Szybka konfiguracja

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Uwaga:** Sesja 06 domy≈õlnie u≈ºywa modeli CPU dla maksymalnej kompatybilno≈õci.

### Uruchamianie notebooka

1. **Otw√≥rz** `session06_models_router.ipynb`
2. **Zrestartuj kernel**
3. **Uruchom wszystkie kom√≥rki** w kolejno≈õci

### Kluczowa konfiguracja

**Domy≈õlny katalog (modele CPU):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternatywa (modele GPU):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Wykrywanie intencji

Router u≈ºywa wzorc√≥w regex do wykrywania intencji:

| Intencja | Przyk≈Çady wzorc√≥w | Kierowane do |
|----------|-------------------|--------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | Wszystko inne | phi-4-mini-cpu |

### Oczekiwany wynik

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### Dostosowanie

**Dodaj w≈ÇasnƒÖ intencjƒô:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**W≈ÇƒÖcz ≈õledzenie token√≥w:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### Prze≈ÇƒÖczanie na modele GPU

Je≈õli masz 8GB+ VRAM:

1. W **kom√≥rce #6**, zakomentuj katalog CPU
2. Odkomentuj katalog GPU
3. Za≈Çaduj modele GPU:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Zrestartuj kernel i uruchom notebook ponownie

### Szacowany czas
- **Pierwsze uruchomienie:** 5-10 minut (≈Çadowanie modeli)
- **Kolejne uruchomienia:** 30-60 sekund na test

---

## Zmienne ≈õrodowiskowe

### Globalna konfiguracja

Ustaw przed uruchomieniem Jupyter/VS Code:

**Windows (Command Prompt):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Konfiguracja w notebooku

Ustaw na poczƒÖtku dowolnego notebooka:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Popularne polecenia

### ZarzƒÖdzanie us≈ÇugƒÖ

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### ZarzƒÖdzanie modelami

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Testowanie endpoint√≥w

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Polecenia diagnostyczne

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Najlepsze praktyki

### Przed uruchomieniem jakiegokolwiek notebooka

1. **Sprawd≈∫, czy us≈Çuga dzia≈Ça:**
   ```bash
   foundry service status
   ```

2. **Zweryfikuj, czy modele sƒÖ za≈Çadowane:**
   ```bash
   foundry model ls
   ```

3. **Zrestartuj kernel notebooka** w przypadku ponownego uruchamiania

4. **Wyczy≈õƒá wszystkie wyniki** dla czystego uruchomienia

### ZarzƒÖdzanie zasobami

1. **U≈ºywaj modeli CPU domy≈õlnie** dla kompatybilno≈õci
2. **Prze≈ÇƒÖcz na modele GPU** tylko je≈õli masz 8GB+ VRAM
3. **Zamknij inne aplikacje GPU** przed uruchomieniem
4. **Utrzymuj dzia≈ÇajƒÖcƒÖ us≈Çugƒô** miƒôdzy sesjami notebooka
5. **Monitoruj u≈ºycie zasob√≥w** za pomocƒÖ Task Manager / nvidia-smi

### RozwiƒÖzywanie problem√≥w

1. **Zawsze najpierw sprawd≈∫ us≈Çugƒô** przed debugowaniem kodu
2. **Zrestartuj kernel** w przypadku przestarza≈Çej konfiguracji
3. **Uruchom ponownie kom√≥rki diagnostyczne** po jakichkolwiek zmianach
4. **Sprawd≈∫ nazwy modeli**, czy pasujƒÖ do za≈Çadowanych
5. **Zweryfikuj port endpointu**, czy pasuje do statusu us≈Çugi

---

## Szybki przeglƒÖd: Alias modeli

### Popularne modele

| Alias | Rozmiar | Najlepsze zastosowanie | RAM/VRAM | Warianty |
|-------|---------|------------------------|----------|----------|
| `phi-4-mini` | ~4B | Og√≥lne rozmowy, podsumowania | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Generowanie kodu, refaktoryzacja | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Zadania og√≥lne, efektywno≈õƒá | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Szybko≈õƒá, niskie zasoby | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Klasyfikacja, minimalne zasoby | 1-2GB | `-cpu`, `-cuda-gpu` |

### Nazewnictwo wariant√≥w

- **Nazwa podstawowa** (np. `phi-4-mini`): Automatycznie wybiera najlepszy wariant dla twojego sprzƒôtu
- **`-cpu`**: Optymalizacja dla CPU, dzia≈Ça wszƒôdzie
- **`-cuda-gpu`**: Optymalizacja dla NVIDIA GPU, wymaga 8GB+ VRAM
- **`-npu`**: Optymalizacja dla Qualcomm NPU, wymaga sterownik√≥w NPU

**Rekomendacja:** U≈ºywaj nazw podstawowych (bez sufiksu) i pozw√≥l Foundry Local automatycznie wybraƒá najlepszy wariant.

---

## Wska≈∫niki sukcesu

Jeste≈õ gotowy, gdy widzisz:

‚úÖ `foundry service status` pokazuje "running"
‚úÖ `foundry model ls` pokazuje wymagane modele
‚úÖ Us≈Çuga dostƒôpna na poprawnym endpointcie
‚úÖ Kontrola zdrowia zwraca 200 OK
‚úÖ Kom√≥rki diagnostyczne w notebooku przechodzƒÖ
‚úÖ Brak b≈Çƒôd√≥w po≈ÇƒÖczenia w wynikach

---

## Uzyskiwanie pomocy

### Dokumentacja
- **G≈Ç√≥wne repozytorium**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **Referencja CLI**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **RozwiƒÖzywanie problem√≥w**: Zobacz `troubleshooting.md` w tym katalogu

### Problemy na GitHub
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Ostatnia aktualizacja:** 8 pa≈∫dziernika 2025
**Wersja:** Warsztaty Notebooks 2.0

---

**Zastrze≈ºenie**:  
Ten dokument zosta≈Ç przet≈Çumaczony za pomocƒÖ us≈Çugi t≈Çumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia≈º dok≈Çadamy wszelkich stara≈Ñ, aby t≈Çumaczenie by≈Ço precyzyjne, prosimy pamiƒôtaƒá, ≈ºe automatyczne t≈Çumaczenia mogƒÖ zawieraƒá b≈Çƒôdy lub nie≈õcis≈Ço≈õci. Oryginalny dokument w jego jƒôzyku ≈∫r√≥d≈Çowym powinien byƒá uznawany za autorytatywne ≈∫r√≥d≈Ço. W przypadku informacji o kluczowym znaczeniu zaleca siƒô skorzystanie z profesjonalnego t≈Çumaczenia przez cz≈Çowieka. Nie ponosimy odpowiedzialno≈õci za jakiekolwiek nieporozumienia lub b≈Çƒôdne interpretacje wynikajƒÖce z u≈ºycia tego t≈Çumaczenia.