<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-17T15:00:06+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "pl"
}
-->
# Sekcja 3: Podstawy rodziny Gemma

Rodzina modeli Gemma reprezentuje kompleksowe podejście Google do otwartoźródłowych dużych modeli językowych i multimodalnej sztucznej inteligencji, pokazując, że dostępne modele mogą osiągać niezwykłą wydajność, jednocześnie będąc wdrażalnymi w różnych scenariuszach – od urządzeń mobilnych po stacje robocze w przedsiębiorstwach. Ważne jest, aby zrozumieć, jak rodzina Gemma umożliwia potężne możliwości AI z elastycznymi opcjami wdrożenia, zachowując konkurencyjną wydajność i odpowiedzialne praktyki AI.

## Wprowadzenie

W tym poradniku przyjrzymy się rodzinie modeli Gemma od Google i jej podstawowym koncepcjom. Omówimy ewolucję rodziny Gemma, innowacyjne metody szkoleniowe, które sprawiają, że modele Gemma są skuteczne, kluczowe warianty w rodzinie oraz praktyczne zastosowania w różnych scenariuszach wdrożeniowych.

## Cele nauki

Po ukończeniu tego poradnika będziesz w stanie:

- Zrozumieć filozofię projektowania i ewolucję rodziny modeli Gemma od Google
- Zidentyfikować kluczowe innowacje, które pozwalają modelom Gemma osiągać wysoką wydajność w różnych rozmiarach parametrów
- Rozpoznać korzyści i ograniczenia różnych wariantów modeli Gemma
- Wykorzystać wiedzę o modelach Gemma do wyboru odpowiednich wariantów dla rzeczywistych scenariuszy

## Zrozumienie współczesnego krajobrazu modeli AI

Krajobraz AI ewoluował znacząco, a różne organizacje realizują różne podejścia do rozwoju modeli językowych. Podczas gdy niektóre skupiają się na zamkniętych modelach dostępnych tylko przez API, inne kładą nacisk na otwartoźródłową dostępność i przejrzystość. Tradycyjne podejście obejmuje albo ogromne, zamknięte modele z ciągłymi kosztami, albo otwartoźródłowe modele, które mogą wymagać znacznej wiedzy technicznej do wdrożenia.

Ten paradygmat stwarza wyzwania dla organizacji poszukujących potężnych możliwości AI, jednocześnie zachowując kontrolę nad swoimi danymi, kosztami i elastycznością wdrożenia. Tradycyjne podejście często wymaga wyboru między najnowocześniejszą wydajnością a praktycznymi względami wdrożeniowymi.

## Wyzwanie dostępnej doskonałości AI

Potrzeba wysokiej jakości, dostępnej AI staje się coraz ważniejsza w różnych scenariuszach. Weźmy pod uwagę aplikacje wymagające elastycznych opcji wdrożenia dla różnych potrzeb organizacyjnych, opłacalne implementacje, w których koszty API mogą stać się znaczące, możliwości multimodalne dla kompleksowego zrozumienia lub specjalistyczne wdrożenia na urządzeniach mobilnych i brzegowych.

### Kluczowe wymagania wdrożeniowe

Współczesne wdrożenia AI stoją przed kilkoma podstawowymi wymaganiami, które ograniczają ich praktyczne zastosowanie:

- **Dostępność**: Otwartoźródłowa dostępność dla przejrzystości i dostosowania
- **Efektywność kosztowa**: Rozsądne wymagania obliczeniowe dla różnych budżetów
- **Elastyczność**: Różne rozmiary modeli dla różnych scenariuszy wdrożeniowych
- **Zrozumienie multimodalne**: Możliwości przetwarzania wizji, tekstu i dźwięku
- **Wdrożenie na brzegu**: Optymalna wydajność na urządzeniach mobilnych i o ograniczonych zasobach

## Filozofia modeli Gemma

Rodzina modeli Gemma reprezentuje kompleksowe podejście Google do rozwoju modeli AI, priorytetyzując otwartoźródłową dostępność, możliwości multimodalne i praktyczne wdrożenie, jednocześnie zachowując konkurencyjne cechy wydajnościowe. Modele Gemma osiągają to dzięki różnorodnym rozmiarom modeli, wysokiej jakości metodologiom szkoleniowym wywodzącym się z badań nad Gemini oraz specjalistycznym wariantom dla różnych domen i scenariuszy wdrożeniowych.

Rodzina Gemma obejmuje różne podejścia zaprojektowane tak, aby zapewnić opcje w całym spektrum wydajności i efektywności, umożliwiając wdrożenie od urządzeń mobilnych po serwery przedsiębiorstw, jednocześnie dostarczając znaczące możliwości AI. Celem jest demokratyzacja dostępu do wysokiej jakości technologii AI przy jednoczesnym zapewnieniu elastyczności w wyborze wdrożenia.

### Podstawowe zasady projektowania Gemma

Modele Gemma opierają się na kilku fundamentalnych zasadach, które wyróżniają je na tle innych rodzin modeli językowych:

- **Otwartoźródłowość na pierwszym miejscu**: Pełna przejrzystość i dostępność do celów badawczych i komercyjnych
- **Rozwój oparty na badaniach**: Oparte na tych samych badaniach i technologii, które napędzają modele Gemini
- **Skalowalna architektura**: Różne rozmiary modeli dopasowane do różnych wymagań obliczeniowych
- **Odpowiedzialna AI**: Zintegrowane środki bezpieczeństwa i odpowiedzialne praktyki rozwojowe

## Kluczowe technologie umożliwiające rodzinę Gemma

### Zaawansowane metody szkoleniowe

Jednym z wyróżniających aspektów rodziny Gemma jest zaawansowane podejście szkoleniowe wywodzące się z badań nad Gemini. Modele Gemma wykorzystują destylację z większych modeli, uczenie przez wzmocnienie z opinii ludzi (RLHF) oraz techniki łączenia modeli, aby osiągnąć lepszą wydajność w matematyce, kodowaniu i podążaniu za instrukcjami.

Proces szkoleniowy obejmuje destylację z większych modeli instruktażowych, uczenie przez wzmocnienie z opinii ludzi (RLHF) w celu dostosowania do preferencji ludzkich, uczenie przez wzmocnienie z opinii maszyn (RLMF) dla rozumowania matematycznego oraz uczenie przez wzmocnienie z opinii wykonania (RLEF) dla możliwości kodowania.

### Integracja i zrozumienie multimodalne

Najnowsze modele Gemma integrują zaawansowane możliwości multimodalne, które umożliwiają kompleksowe zrozumienie różnych typów danych wejściowych:

**Integracja wizji i języka (Gemma 3)**: Gemma 3 potrafi przetwarzać tekst i obrazy jednocześnie, umożliwiając analizę obrazów, odpowiadanie na pytania dotyczące treści wizualnych, wyodrębnianie tekstu z obrazów i rozumienie złożonych danych wizualnych.

**Przetwarzanie dźwięku (Gemma 3n)**: Gemma 3n oferuje zaawansowane możliwości przetwarzania dźwięku, w tym automatyczne rozpoznawanie mowy (ASR) i automatyczne tłumaczenie mowy (AST), z szczególnie dobrą wydajnością w tłumaczeniu między angielskim a hiszpańskim, francuskim, włoskim i portugalskim.

**Przetwarzanie przeplatanych danych wejściowych**: Modele Gemma obsługują przeplatane dane wejściowe w różnych modalnościach, umożliwiając zrozumienie złożonych interakcji multimodalnych, w których tekst, obrazy i dźwięk mogą być przetwarzane razem.

### Innowacje architektoniczne

Rodzina Gemma wprowadza kilka optymalizacji architektonicznych zaprojektowanych zarówno pod kątem wydajności, jak i efektywności:

**Rozszerzenie okna kontekstowego**: Modele Gemma 3 oferują okno kontekstowe o długości 128 tys. tokenów, 16 razy większe niż w poprzednich modelach Gemma, umożliwiając przetwarzanie ogromnych ilości informacji, w tym wielu dokumentów lub setek obrazów.

**Architektura mobilna (Gemma 3n)**: Gemma 3n wykorzystuje technologię Per-Layer Embeddings (PLE) i architekturę MatFormer, pozwalając większym modelom działać z pamięcią porównywalną do mniejszych tradycyjnych modeli.

**Możliwości wywoływania funkcji**: Gemma 3 obsługuje wywoływanie funkcji, umożliwiając programistom tworzenie interfejsów języka naturalnego dla interfejsów programistycznych i tworzenie inteligentnych systemów automatyzacji.

## Rozmiar modeli i opcje wdrożenia

Współczesne środowiska wdrożeniowe korzystają z elastyczności modeli Gemma w różnych wymaganiach obliczeniowych:

### Małe modele (0,6B-4B)

Gemma oferuje wydajne małe modele odpowiednie do wdrożeń brzegowych, aplikacji mobilnych i środowisk o ograniczonych zasobach, jednocześnie zachowując imponujące możliwości. Model 1B jest idealny dla małych aplikacji, podczas gdy model 4B oferuje zrównoważoną wydajność i elastyczność z obsługą multimodalną.

### Średnie modele (8B-14B)

Modele średniego zasięgu oferują ulepszone możliwości dla zastosowań profesjonalnych, zapewniając doskonałą równowagę między wydajnością a wymaganiami obliczeniowymi dla wdrożeń na stacjach roboczych i serwerach.

### Duże modele (27B+)

Modele pełnoskalowe zapewniają najnowocześniejszą wydajność dla wymagających aplikacji, badań i wdrożeń w przedsiębiorstwach wymagających maksymalnych możliwości. Model 27B reprezentuje najbardziej zaawansowaną opcję, która nadal może działać na pojedynczym GPU.

### Modele zoptymalizowane pod kątem urządzeń mobilnych (Gemma 3n)

Modele Gemma 3n E2B i E4B zostały specjalnie zaprojektowane do wdrożeń mobilnych i brzegowych, z efektywną liczbą parametrów wynoszącą odpowiednio 2B i 4B, przy użyciu innowacyjnej architektury minimalizującej zużycie pamięci do zaledwie 2 GB dla E2B i 3 GB dla E4B.

## Korzyści z rodziny modeli Gemma

### Otwartoźródłowa dostępność

Modele Gemma zapewniają pełną przejrzystość i możliwości dostosowania dzięki otwartym wagom, które pozwalają na odpowiedzialne wykorzystanie komercyjne, umożliwiając organizacjom dostrajanie i wdrażanie ich w swoich projektach i aplikacjach.

### Elastyczność wdrożenia

Zakres rozmiarów modeli umożliwia wdrożenie na różnorodnych konfiguracjach sprzętowych, od urządzeń mobilnych po zaawansowane serwery, z optymalizacją dla różnych platform, w tym Google Cloud TPU, NVIDIA GPU, AMD GPU za pomocą ROCm i wykonania na CPU za pomocą Gemma.cpp.

### Doskonałość wielojęzyczna

Modele Gemma wyróżniają się w zrozumieniu i generowaniu w ponad 140 językach, co czyni je odpowiednimi dla globalnych aplikacji.

### Konkurencyjna wydajność

Modele Gemma konsekwentnie osiągają konkurencyjne wyniki w benchmarkach, a Gemma 3 zajmuje wysokie miejsca zarówno wśród popularnych modeli zamkniętych, jak i otwartych w ocenach preferencji użytkowników.

### Specjalistyczne możliwości

Aplikacje specyficzne dla danej dziedziny korzystają z multimodalnego zrozumienia Gemma, możliwości wywoływania funkcji i zoptymalizowanej wydajności na różnych platformach sprzętowych.
- Gemma 3 oferuje potężne możliwości dla programistów dzięki zaawansowanym funkcjom rozumienia tekstu i obrazu, wspierając wejścia w formie obrazów i tekstu dla multimodalnego rozumienia.
- Gemma 3n zajmuje wysokie pozycje w rankingu Elo Chatbot Arena zarówno wśród popularnych modeli własnościowych, jak i otwartych, co wskazuje na silne preferencje użytkowników.

**Osiągnięcia w zakresie efektywności:**
- Modele Gemma 3 obsługują wejścia do 128K tokenów, co stanowi 16-krotnie większe okno kontekstowe w porównaniu do wcześniejszych modeli Gemma.
- Gemma 3n wykorzystuje Per-Layer Embeddings (PLE), co znacząco redukuje zużycie pamięci RAM przy zachowaniu możliwości większych modeli.

**Optymalizacja mobilna:**
- Gemma 3n E2B działa przy użyciu zaledwie 2GB pamięci, podczas gdy E4B wymaga tylko 3GB, mimo że liczba surowych parametrów wynosi odpowiednio 5B i 8B.
- Możliwości AI w czasie rzeczywistym bezpośrednio na urządzeniach mobilnych z priorytetem prywatności i gotowością do pracy offline.

**Skala treningu:**
- Gemma 3 była trenowana na 2T tokenów dla modelu 1B, 4T dla 4B, 12T dla 12B i 14T tokenów dla modeli 27B, wykorzystując Google TPUs i Framework JAX.

### Macierz porównawcza modeli

| Seria modelu | Zakres parametrów | Długość kontekstu | Kluczowe zalety | Najlepsze zastosowania |
|--------------|------------------|----------------|---------------|----------------|
| **Gemma 3** | 1B-27B | 128K | Multimodalne rozumienie, wywoływanie funkcji | Zastosowania ogólne, zadania wizualno-językowe |
| **Gemma 3n** | E2B (5B), E4B (8B) | Zmienna | Optymalizacja mobilna, przetwarzanie audio | Aplikacje mobilne, edge computing, AI w czasie rzeczywistym |
| **Gemma 2.5** | 0.5B-72B | 32K-128K | Zrównoważona wydajność, wielojęzyczność | Wdrożenia produkcyjne, istniejące przepływy pracy |
| **Gemma-VL** | Różne | Zmienna | Specjalizacja wizualno-językowa | Analiza obrazów, odpowiadanie na pytania wizualne |

## Przewodnik wyboru modelu

### Dla podstawowych zastosowań
- **Gemma 3-1B**: Lekkie zadania tekstowe, proste aplikacje mobilne
- **Gemma 3-4B**: Zrównoważona wydajność z multimodalnym wsparciem do ogólnego użytku

### Dla zastosowań multimodalnych
- **Gemma 3-4B/12B**: Rozumienie obrazów, odpowiadanie na pytania wizualne
- **Gemma 3n**: Multimodalne aplikacje mobilne z możliwościami przetwarzania audio

### Dla wdrożeń mobilnych i edge
- **Gemma 3n E2B**: Urządzenia o ograniczonych zasobach, AI mobilne w czasie rzeczywistym
- **Gemma 3n E4B**: Zwiększona wydajność mobilna z możliwościami audio

### Dla wdrożeń korporacyjnych
- **Gemma 3-12B/27B**: Wysokowydajne rozumienie języka i obrazu
- **Możliwości wywoływania funkcji**: Tworzenie inteligentnych systemów automatyzacji

### Dla globalnych zastosowań
- **Dowolny wariant Gemma 3**: Wsparcie dla 140+ języków z rozumieniem kulturowym
- **Gemma 3n**: Aplikacje mobilne z pierwszeństwem dla globalnych zastosowań i tłumaczenia audio

## Platformy wdrożeniowe i dostępność

### Platformy chmurowe
- **Vertex AI**: Kompleksowe możliwości MLOps z bezserwerowym doświadczeniem
- **Google Kubernetes Engine (GKE)**: Skalowalne wdrożenie kontenerów dla złożonych obciążeń
- **Google GenAI API**: Bezpośredni dostęp do API dla szybkiego prototypowania
- **NVIDIA API Catalog**: Optymalna wydajność na GPU NVIDIA

### Frameworki lokalnego rozwoju
- **Hugging Face Transformers**: Standardowa integracja dla rozwoju
- **Ollama**: Uproszczone lokalne wdrożenie i zarządzanie
- **vLLM**: Wysokowydajne serwowanie dla produkcji
- **Gemma.cpp**: Wykonanie zoptymalizowane dla CPU
- **Google AI Edge**: Optymalizacja wdrożeń mobilnych i edge

### Zasoby edukacyjne
- **Google AI Studio**: Wypróbuj modele Gemma w kilku kliknięciach
- **Kaggle i Hugging Face**: Pobierz wagi modeli i przykłady społeczności
- **Raporty techniczne**: Kompleksowa dokumentacja i publikacje badawcze
- **Fora społecznościowe**: Aktywne wsparcie społeczności i dyskusje

### Rozpoczęcie pracy z modelami Gemma

#### Platformy rozwojowe
1. **Google AI Studio**: Rozpocznij eksperymenty w oparciu o przeglądarkę
2. **Hugging Face Hub**: Odkrywaj modele i implementacje społeczności
3. **Lokalne wdrożenie**: Użyj Ollama lub Transformers do rozwoju

#### Ścieżka nauki
1. **Zrozum podstawowe pojęcia**: Przestudiuj możliwości multimodalne i opcje wdrożeniowe
2. **Eksperymentuj z wariantami**: Wypróbuj różne rozmiary modeli i wersje specjalistyczne
3. **Ćwicz implementację**: Wdrażaj modele w środowiskach rozwojowych
4. **Optymalizuj dla produkcji**: Dostosuj do specyficznych przypadków użycia i platform

#### Najlepsze praktyki
- **Zacznij od małych modeli**: Rozpocznij od Gemma 3-4B dla początkowego rozwoju i testowania
- **Używaj oficjalnych szablonów**: Zastosuj odpowiednie szablony czatu dla optymalnych wyników
- **Monitoruj zasoby**: Śledź zużycie pamięci i wydajność inferencji
- **Rozważ specjalizację**: Wybierz odpowiednie warianty dla potrzeb multimodalnych lub mobilnych

## Zaawansowane wzorce użycia

### Przykłady dostrajania

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```

### Specjalistyczne projektowanie promptów

**Dla zadań multimodalnych:**
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```

**Dla wywoływania funkcji z kontekstem:**
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```

### Wielojęzyczne aplikacje z kontekstem kulturowym

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```

### Wzorce wdrożenia produkcyjnego

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```

## Strategie optymalizacji wydajności

### Optymalizacja pamięci

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```

### Optymalizacja inferencji

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```

## Najlepsze praktyki i wytyczne

### Bezpieczeństwo i prywatność

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```

### Monitorowanie i ocena

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```

## Podsumowanie

Rodzina modeli Gemma reprezentuje kompleksowe podejście Google do demokratyzacji technologii AI, jednocześnie utrzymując konkurencyjną wydajność w różnorodnych zastosowaniach i scenariuszach wdrożeniowych. Dzięki zaangażowaniu w dostępność open-source, możliwości multimodalne i innowacyjne projekty architektoniczne, Gemma umożliwia organizacjom i programistom korzystanie z potężnych możliwości AI niezależnie od ich zasobów czy specyficznych wymagań.

### Kluczowe wnioski

**Doskonałość open-source**: Gemma pokazuje, że modele open-source mogą osiągać wydajność konkurencyjną wobec alternatyw własnościowych, oferując jednocześnie przejrzystość, możliwość dostosowania i kontrolę nad wdrożeniem AI.

**Innowacja multimodalna**: Integracja tekstu, obrazu i dźwięku w Gemma 3 i Gemma 3n stanowi znaczący postęp w dostępnej multimodalnej AI, umożliwiając kompleksowe rozumienie różnych typów wejść.

**Architektura mobilna**: Przełomowa technologia Per-Layer Embeddings (PLE) i optymalizacja mobilna w Gemma 3n pokazują, że potężne AI może działać efektywnie na urządzeniach o ograniczonych zasobach bez utraty możliwości.

**Skalowalne wdrożenie**: Zakres od 1B do 27B parametrów, z wyspecjalizowanymi wariantami mobilnymi, umożliwia wdrożenie w pełnym spektrum środowisk obliczeniowych przy zachowaniu spójnej jakości i wydajności.

**Odpowiedzialna integracja AI**: Wbudowane środki bezpieczeństwa poprzez ShieldGemma 2 i odpowiedzialne praktyki rozwojowe zapewniają, że potężne możliwości AI mogą być wdrażane w sposób bezpieczny i etyczny.

### Perspektywy na przyszłość

W miarę rozwoju rodziny Gemma możemy oczekiwać:

**Zwiększonych możliwości mobilnych**: Dalszej optymalizacji dla wdrożeń mobilnych i edge z integracją architektury Gemma 3n w głównych platformach, takich jak Android i Chrome.

**Rozszerzonego rozumienia multimodalnego**: Kontynuacji postępów w integracji wizualno-językowo-dźwiękowej dla bardziej kompleksowych doświadczeń AI.

**Poprawionej efektywności**: Ciągłych innowacji architektonicznych w celu zapewnienia lepszych wskaźników wydajności na parametr i zmniejszenia wymagań obliczeniowych.

**Szerszej integracji ekosystemu**: Zwiększonego wsparcia w ramach frameworków rozwojowych, platform chmurowych i narzędzi wdrożeniowych dla płynnej integracji z istniejącymi przepływami pracy.

**Rozwoju społeczności**: Kontynuacji ekspansji Gemmaverse z modelami, narzędziami i aplikacjami tworzonymi przez społeczność, które rozszerzają podstawowe możliwości.

### Kolejne kroki

Niezależnie od tego, czy tworzysz aplikacje mobilne z możliwościami AI w czasie rzeczywistym, rozwijasz multimodalne narzędzia edukacyjne, budujesz inteligentne systemy automatyzacji, czy pracujesz nad globalnymi aplikacjami wymagającymi wsparcia wielojęzycznego, rodzina Gemma oferuje skalowalne rozwiązania z silnym wsparciem społeczności i kompleksową dokumentacją.

**Rekomendacje na start:**
1. **Eksperymentuj z Google AI Studio** dla natychmiastowego doświadczenia praktycznego
2. **Pobierz modele z Hugging Face** dla lokalnego rozwoju i dostosowania
3. **Odkrywaj wyspecjalizowane warianty**, takie jak Gemma 3n dla aplikacji mobilnych
4. **Wdrażaj możliwości multimodalne** dla kompleksowych doświadczeń AI
5. **Przestrzegaj najlepszych praktyk bezpieczeństwa** dla wdrożeń produkcyjnych

**Dla rozwoju mobilnego**: Rozpocznij od Gemma 3n E2B dla efektywnego wdrożenia z możliwościami audio i wizualnymi.

**Dla aplikacji korporacyjnych**: Rozważ modele Gemma 3-12B lub 27B dla maksymalnych możliwości z wywoływaniem funkcji i zaawansowanym rozumowaniem.

**Dla globalnych zastosowań**: Wykorzystaj wsparcie Gemma dla 140+ języków z inżynierią promptów uwzględniającą kontekst kulturowy.

**Dla wyspecjalizowanych przypadków użycia**: Eksploruj podejścia do dostrajania i techniki optymalizacji specyficzne dla domeny.

### 🔮 Demokratyzacja AI

Rodzina Gemma jest przykładem przyszłości rozwoju AI, gdzie potężne, zdolne modele są dostępne dla każdego, od indywidualnych programistów po duże przedsiębiorstwa. Dzięki połączeniu najnowocześniejszych badań z dostępnością open-source, Google stworzyło fundament, który umożliwia innowacje we wszystkich sektorach i na każdą skalę.

Sukces Gemma, z ponad 100 milionami pobrań i 60 000+ wariantami społeczności, pokazuje siłę otwartej współpracy w rozwijaniu technologii AI. W miarę postępów rodzina Gemma będzie nadal służyć jako katalizator innowacji AI, umożliwiając rozwój aplikacji, które wcześniej były możliwe tylko dzięki własnościowym, kosztownym modelom.

Przyszłość AI jest otwarta, dostępna i potężna – a rodzina Gemma prowadzi drogę do realizacji tej wizji.

## Dodatkowe zasoby

**Oficjalna dokumentacja i modele:**
- **Google AI Studio**: [Wypróbuj modele Gemma bezpośrednio](https://aistudio.google.com)
- **Kolekcje Hugging Face**: 
  - [Wydanie Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
  - [Podgląd Gemma 3n](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Dokumentacja dla programistów Google AI**: [Kompleksowe przewodniki Gemma](https://ai.google.dev/gemma)
- **Dokumentacja Vertex AI**: [Przewodniki wdrożeniowe dla przedsiębiorstw](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)

**Zasoby techniczne:**
- **Publikacje badawcze i raporty techniczne**: [Publikacje Google DeepMind](https://deepmind.google/models/gemma/)
- **Posty na blogu dla programistów**: [Najnowsze ogłoszenia i samouczki](https://developers.googleblog.com)
- **Karty modeli**: Szczegółowe specyfikacje techniczne i benchmarki wydajności

**Społeczność i wsparcie:**
- **Społeczność Hugging Face**: Aktywne dyskusje i przykłady społeczności
- **Repozytoria GitHub**: Implementacje open-source i narzędzia
- **Fora dla programistów**: Wsparcie społeczności Google AI Developer
- **Stack Overflow**: Otagowane pytania i rozwiązania społeczności

**Narzędzia rozwojowe:**
- **Ollama**: [Proste lokalne wdrożenie](https://ollama.ai)
- **vLLM**: [Wysokowydajne serwowanie](https://github.com/vllm-project/vllm)
- **Biblioteka Transformers**: [Integracja Hugging Face](https://huggingface.co/docs/transformers)
- **Google AI Edge**: Optymalizacja wdrożeń mobilnych i edge

**Ścieżki nauki:**
- **Początkujący**: Rozpocznij od Google AI Studio → Przykłady Hugging Face → Lokalne wdrożenie
- **Programista**: Integracja Transformers → Aplikacje niestandardowe → Wdrożenie produkcyjne
- **Badacz**: Publikacje techniczne → Dostrajanie → Nowatorskie aplikacje
- **Przedsiębiorstwo**: Wdrożenie Vertex AI → Implementacja bezpieczeństwa → Optymalizacja skali

Rodzina modeli Gemma reprezentuje nie tylko kolekcję modeli AI, ale kompletny ekosystem do budowania przyszłości dostępnych, potężnych i odpowiedzialnych aplikacji AI. Zacznij eksplorować już dziś i dołącz do rosnącej społeczności programistów i badaczy przesuwających granice tego, co możliwe dzięki AI open-source.



## Dodatkowe zasoby

### Oficjalna dokumentacja
- Techniczna dokumentacja Google Gemma
- Karty modeli i wytyczne dotyczące użytkowania
- Przewodnik wdrożenia odpowiedzialnego AI
- Przewodnik integracji Google Vertex AI

### Narzędzia rozwojowe
- Google AI Studio do wdrożeń w chmurze
- Hugging Face Transformers do integracji modeli
- vLLM do wysokowydajnego serwowania
- Gemma.cpp do inferencji zoptymalizowanej dla CPU

### Zasoby edukacyjne
- Publikacje techniczne Gemma 3 i Gemma 3n
- Blog Google AI i samouczki
- Przewodniki optymalizacji i kwantyzacji modeli
- Fora społecznościowe i grupy dyskusyjne

## Efekty nauki

Po ukończeniu tego modułu będziesz w stanie:

1. Wyja

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.