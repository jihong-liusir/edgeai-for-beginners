<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-17T15:22:55+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "pl"
}
-->
# Sekcja 3: Praktyczny przewodnik wdrożeniowy

## Przegląd

Ten kompleksowy przewodnik pomoże Ci przygotować się do kursu EdgeAI, który koncentruje się na tworzeniu praktycznych rozwiązań AI działających efektywnie na urządzeniach brzegowych. Kurs kładzie nacisk na praktyczne podejście do rozwoju z wykorzystaniem nowoczesnych frameworków i najnowocześniejszych modeli zoptymalizowanych pod kątem wdrożeń na urządzeniach brzegowych.

## 1. Konfiguracja środowiska programistycznego

### Języki programowania i frameworki

**Środowisko Python**
- **Wersja**: Python 3.10 lub nowszy (zalecane: Python 3.11)
- **Menedżer pakietów**: pip lub conda
- **Wirtualne środowisko**: Używaj venv lub środowisk conda dla izolacji
- **Kluczowe biblioteki**: Specyficzne biblioteki EdgeAI zostaną zainstalowane podczas kursu

**Środowisko Microsoft .NET**
- **Wersja**: .NET 8 lub nowszy
- **IDE**: Visual Studio 2022, Visual Studio Code lub JetBrains Rider
- **SDK**: Upewnij się, że zainstalowano .NET SDK dla rozwoju międzyplatformowego

### Narzędzia programistyczne

**Edytory kodu i IDE**
- Visual Studio Code (zalecane dla rozwoju międzyplatformowego)
- PyCharm lub Visual Studio (dla rozwoju specyficznego dla języka)
- Jupyter Notebooks do interaktywnego rozwoju i prototypowania

**Kontrola wersji**
- Git (najbardziej aktualna wersja)
- Konto GitHub do dostępu do repozytoriów i współpracy

## 2. Wymagania sprzętowe i rekomendacje

### Minimalne wymagania systemowe
- **CPU**: Procesor wielordzeniowy (Intel i5/AMD Ryzen 5 lub równoważny)
- **RAM**: Minimum 8GB, zalecane 16GB
- **Pamięć**: 50GB wolnego miejsca na modele i narzędzia programistyczne
- **OS**: Windows 10/11, macOS 10.15+ lub Linux (Ubuntu 20.04+)

### Strategia zasobów obliczeniowych
Kurs został zaprojektowany tak, aby był dostępny na różnych konfiguracjach sprzętowych:

**Rozwój lokalny (skupienie na CPU/NPU)**
- Główne działania rozwojowe będą wykorzystywać CPU i akcelerację NPU
- Odpowiednie dla większości nowoczesnych laptopów i komputerów stacjonarnych
- Skupienie na efektywności i praktycznych scenariuszach wdrożeniowych

**Zasoby GPU w chmurze (opcjonalne)**
- **Azure Machine Learning**: Do intensywnego treningu i eksperymentów
- **Google Colab**: Dostępny darmowy poziom dla celów edukacyjnych
- **Kaggle Notebooks**: Alternatywna platforma obliczeniowa w chmurze

### Rozważania dotyczące urządzeń brzegowych
- Znajomość procesorów opartych na ARM
- Wiedza na temat ograniczeń sprzętu mobilnego i IoT
- Zrozumienie optymalizacji zużycia energii

## 3. Główne rodziny modeli i zasoby

### Główne rodziny modeli

**Rodzina Microsoft Phi-4**
- **Opis**: Kompaktowe, wydajne modele zaprojektowane do wdrożeń na urządzeniach brzegowych
- **Mocne strony**: Doskonały stosunek wydajności do rozmiaru, zoptymalizowane pod kątem zadań rozumowania
- **Zasób**: [Phi-4 Collection na Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Zastosowania**: Generowanie kodu, rozumowanie matematyczne, ogólne rozmowy

**Rodzina Qwen-3**
- **Opis**: Najnowsza generacja modeli wielojęzycznych od Alibaba
- **Mocne strony**: Silne możliwości wielojęzyczne, wydajna architektura
- **Zasób**: [Qwen-3 Collection na Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Zastosowania**: Aplikacje wielojęzyczne, rozwiązania AI międzykulturowe

**Rodzina Google Gemma-3n**
- **Opis**: Lekkie modele Google zoptymalizowane pod kątem wdrożeń na urządzeniach brzegowych
- **Mocne strony**: Szybkie wnioskowanie, przyjazna dla urządzeń mobilnych architektura
- **Zasób**: [Gemma-3n Collection na Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Zastosowania**: Aplikacje mobilne, przetwarzanie w czasie rzeczywistym

### Kryteria wyboru modelu
- **Kompromis między wydajnością a rozmiarem**: Zrozumienie, kiedy wybrać mniejsze lub większe modele
- **Optymalizacja pod kątem zadań**: Dopasowanie modeli do konkretnych zastosowań
- **Ograniczenia wdrożeniowe**: Pamięć, opóźnienia i zużycie energii

## 4. Narzędzia do kwantyzacji i optymalizacji

### Framework Llama.cpp
- **Repozytorium**: [Llama.cpp na GitHub](https://github.com/ggml-org/llama.cpp)
- **Cel**: Silnik wnioskowania o wysokiej wydajności dla LLM
- **Kluczowe funkcje**:
  - Wnioskowanie zoptymalizowane pod kątem CPU
  - Wiele formatów kwantyzacji (Q4, Q5, Q8)
  - Kompatybilność międzyplatformowa
  - Wykonanie oszczędzające pamięć
- **Instalacja i podstawowe użycie**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repozytorium**: [Microsoft Olive na GitHub](https://github.com/microsoft/olive)
- **Cel**: Narzędzie do optymalizacji modeli dla wdrożeń na urządzeniach brzegowych
- **Kluczowe funkcje**:
  - Zautomatyzowane przepływy pracy optymalizacji modeli
  - Optymalizacja uwzględniająca sprzęt
  - Integracja z ONNX Runtime
  - Narzędzia do benchmarkingu wydajności
- **Instalacja i podstawowe użycie**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Definicja modelu i konfiguracji optymalizacji
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Uruchomienie przepływu pracy optymalizacji
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Zapisanie zoptymalizowanego modelu
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # Instalacja MLX
  pip install mlx
  
  # Przykładowy skrypt Python do ładowania i optymalizacji modelu
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repozytorium**: [ONNX Runtime na GitHub](https://github.com/microsoft/onnxruntime)
- **Cel**: Przyspieszenie wnioskowania międzyplatformowego dla modeli ONNX
- **Kluczowe funkcje**:
  - Optymalizacje specyficzne dla sprzętu (CPU, GPU, NPU)
  - Optymalizacje grafu dla wnioskowania
  - Obsługa kwantyzacji
  - Obsługa wielu języków (Python, C++, C#, JavaScript)
- **Instalacja i podstawowe użycie**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. Zalecana literatura i zasoby

### Kluczowa dokumentacja
- **Dokumentacja ONNX Runtime**: Zrozumienie wnioskowania międzyplatformowego
- **Przewodnik Hugging Face Transformers**: Ładowanie modeli i wnioskowanie
- **Wzorce projektowe Edge AI**: Najlepsze praktyki dla wdrożeń na urządzeniach brzegowych

### Artykuły techniczne
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Zasoby społecznościowe
- **Społeczności EdgeAI na Slack/Discord**: Wsparcie i dyskusje z rówieśnikami
- **Repozytoria GitHub**: Przykładowe implementacje i tutoriale
- **Kanały YouTube**: Dogłębne analizy techniczne i tutoriale

## 6. Ocena i weryfikacja

### Lista kontrolna przed kursem
- [ ] Zainstalowany i zweryfikowany Python 3.10+
- [ ] Zainstalowany i zweryfikowany .NET 8+
- [ ] Skonfigurowane środowisko programistyczne
- [ ] Utworzone konto Hugging Face
- [ ] Podstawowa znajomość docelowych rodzin modeli
- [ ] Zainstalowane i przetestowane narzędzia do kwantyzacji
- [ ] Spełnione wymagania sprzętowe
- [ ] Skonfigurowane konta w chmurze (jeśli potrzebne)

## Kluczowe cele nauki

Na koniec tego przewodnika będziesz w stanie:

1. Skonfigurować kompletne środowisko programistyczne dla aplikacji EdgeAI
2. Zainstalować i skonfigurować niezbędne narzędzia i frameworki do optymalizacji modeli
3. Wybrać odpowiednie konfiguracje sprzętowe i programowe dla swoich projektów EdgeAI
4. Zrozumieć kluczowe aspekty wdrażania modeli AI na urządzeniach brzegowych
5. Przygotować swój system do ćwiczeń praktycznych w ramach kursu

## Dodatkowe zasoby

### Oficjalna dokumentacja
- **Dokumentacja Python**: Oficjalna dokumentacja języka Python
- **Dokumentacja Microsoft .NET**: Oficjalne zasoby dla rozwoju w .NET
- **Dokumentacja ONNX Runtime**: Kompleksowy przewodnik po ONNX Runtime
- **Dokumentacja TensorFlow Lite**: Oficjalna dokumentacja TensorFlow Lite

### Narzędzia programistyczne
- **Visual Studio Code**: Lekki edytor kodu z rozszerzeniami do rozwoju AI
- **Jupyter Notebooks**: Interaktywne środowisko obliczeniowe do eksperymentów ML
- **Docker**: Platforma konteneryzacji dla spójnych środowisk rozwojowych
- **Git**: System kontroli wersji do zarządzania kodem

### Zasoby edukacyjne
- **Artykuły badawcze EdgeAI**: Najnowsze badania akademickie na temat wydajnych modeli
- **Kursy online**: Dodatkowe materiały edukacyjne na temat optymalizacji AI
- **Fora społecznościowe**: Platformy Q&A dla wyzwań związanych z rozwojem EdgeAI
- **Zbiory danych benchmarkowe**: Standardowe zbiory danych do oceny wydajności modeli

## Efekty nauki

Po ukończeniu tego przewodnika przygotowawczego będziesz:

1. Mieć w pełni skonfigurowane środowisko programistyczne gotowe do rozwoju EdgeAI
2. Rozumieć wymagania sprzętowe i programowe dla różnych scenariuszy wdrożeniowych
3. Znać kluczowe frameworki i narzędzia używane w trakcie kursu
4. Umieć wybrać odpowiednie modele na podstawie ograniczeń urządzeń i wymagań
5. Posiadać podstawową wiedzę na temat technik optymalizacji dla wdrożeń na urządzeniach brzegowych

## ➡️ Co dalej

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za wiarygodne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.