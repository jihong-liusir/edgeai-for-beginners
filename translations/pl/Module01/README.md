<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddfe62b8e130979b7034bc6fbb7d510c",
  "translation_date": "2025-09-17T15:21:52+00:00",
  "source_file": "Module01/README.md",
  "language_code": "pl"
}
-->
# Rozdział 01: Transformacja wdrażania AI na urządzeniach brzegowych

EdgeAI reprezentuje przełomowy kierunek w wdrażaniu sztucznej inteligencji, przenosząc możliwości AI z przetwarzania w chmurze na lokalne urządzenia brzegowe. Ten rozdział omawia podstawowe pojęcia, kluczowe technologie oraz praktyczne zastosowania, które definiują tę transformacyjną metodę implementacji AI.

## Struktura modułu

### [Sekcja 1: Podstawy EdgeAI](./01.EdgeAIFundamentals.md)
Ta sekcja tworzy fundament, porównując tradycyjne modele AI oparte na chmurze z wdrożeniami AI na urządzeniach brzegowych. Analizujemy kluczowe technologie umożliwiające, takie jak kwantyzacja modeli, optymalizacja kompresji oraz Małe Modele Językowe (SLM), które pokonują ograniczenia obliczeniowe urządzeń brzegowych. Dyskusja podkreśla, w jaki sposób te innowacje zapewniają lepszą ochronę prywatności, ultraniskie opóźnienia oraz solidne możliwości przetwarzania offline.

### [Sekcja 2: Przypadki użycia w rzeczywistych scenariuszach](./02.RealWorldCaseStudies.md)
Na przykładach takich jak ekosystemy modeli Phi i Mu firmy Microsoft oraz system raportowania AI linii lotniczych Japan Airlines, ta sekcja pokazuje udane wdrożenia EdgeAI w różnych branżach. Te studia przypadków potwierdzają wyjątkową wydajność SLM w wyspecjalizowanych zadaniach i ilustrują praktyczne korzyści wynikające ze strategii wdrożeń na urządzeniach brzegowych.

### [Sekcja 3: Praktyczny przewodnik wdrożeniowy](./03.PracticalImplementationGuide.md)
Ta sekcja dostarcza kompleksowych wytycznych dotyczących przygotowania środowiska do nauki praktycznej, obejmując niezbędne narzędzia deweloperskie, wymagania sprzętowe, zasoby modeli podstawowych oraz ramy optymalizacyjne. Tworzy techniczne podstawy potrzebne do budowy i wdrażania własnych rozwiązań EdgeAI.

### [Sekcja 4: Platformy sprzętowe do wdrażania Edge AI](./04.EdgeDeployment.md)
Ta sekcja bada ekosystem sprzętowy umożliwiający wdrażanie EdgeAI, obejmując platformy od Intel, Qualcomm, NVIDIA oraz Windows AI PC. Dostarcza szczegółowych porównań możliwości sprzętowych, technik optymalizacji specyficznych dla platform oraz praktycznych rozważań dotyczących wdrożeń w różnych scenariuszach obliczeń brzegowych.

## Kluczowe rezultaty nauki

Po ukończeniu tego rozdziału czytelnicy będą rozumieć:
- Podstawowe różnice między architekturami AI opartymi na chmurze a urządzeniami brzegowymi
- Kluczowe techniki optymalizacji dla wdrożeń na urządzeniach brzegowych
- Zastosowania w rzeczywistych scenariuszach i historie sukcesu
- Praktyczne umiejętności w zakresie implementacji rozwiązań EdgeAI
- Wybór platform sprzętowych i podejścia do optymalizacji specyficzne dla platform
- Benchmarking wydajności oraz najlepsze praktyki wdrożeniowe

## Przyszłe implikacje

EdgeAI wyłania się jako kluczowy trend kształtujący przyszłość wdrażania AI, torując drogę dla rozproszonych, wydajnych i chroniących prywatność systemów AI, które mogą działać niezależnie od połączenia z chmurą, jednocześnie utrzymując wysokie standardy wydajności.

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za wiarygodne źródło. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.