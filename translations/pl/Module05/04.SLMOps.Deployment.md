<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T15:54:52+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "pl"
}
-->
# Sekcja 4: Wdrożenie - Implementacja Modelu Gotowego do Produkcji

## Przegląd

Ten kompleksowy poradnik poprowadzi Cię przez cały proces wdrażania dostrojonych modeli kwantyzowanych za pomocą Foundry Local. Omówimy konwersję modelu, optymalizację kwantyzacji oraz konfigurację wdrożenia od początku do końca.

## Wymagania wstępne

Przed rozpoczęciem upewnij się, że masz:

- ✅ Dostrojony model w formacie onnx gotowy do wdrożenia
- ✅ Komputer z systemem Windows lub Mac
- ✅ Python 3.10 lub nowszy
- ✅ Co najmniej 8GB dostępnej pamięci RAM
- ✅ Zainstalowany Foundry Local na Twoim systemie

## Część 1: Konfiguracja środowiska

### Instalacja wymaganych narzędzi

Otwórz terminal (Command Prompt na Windows, Terminal na Mac) i uruchom następujące polecenia w kolejności:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **Ważna uwaga**: Potrzebujesz również CMake w wersji 3.31 lub nowszej, który można pobrać ze strony [cmake.org](https://cmake.org/download/).

## Część 2: Konwersja i kwantyzacja modelu

### Wybór odpowiedniego formatu

Dla dostrojonych małych modeli językowych zalecamy użycie formatu **ONNX**, ponieważ oferuje:

- 🚀 Lepszą optymalizację wydajności
- 🔧 Niezależność od sprzętu
- 🏭 Możliwości gotowe do produkcji
- 📱 Kompatybilność międzyplatformowa

### Metoda 1: Konwersja jednym poleceniem (zalecana)

Użyj następującego polecenia, aby bezpośrednio przekonwertować dostrojony model:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Wyjaśnienie parametrów:**
- `--model_name_or_path`: Ścieżka do dostrojonego modelu
- `--device cpu`: Użycie CPU do optymalizacji
- `--precision int4`: Użycie kwantyzacji INT4 (redukcja rozmiaru o około 75%)
- `--output_path`: Ścieżka wyjściowa dla przekonwertowanego modelu

### Metoda 2: Podejście z plikiem konfiguracyjnym (zaawansowani użytkownicy)

Utwórz plik konfiguracyjny o nazwie `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Następnie uruchom:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Porównanie opcji kwantyzacji

| Precyzja | Rozmiar pliku | Szybkość inferencji | Jakość modelu | Zalecane zastosowanie |
|----------|---------------|---------------------|---------------|-----------------------|
| FP16     | Podstawowy × 0.5 | Szybka | Najlepsza | Zaawansowany sprzęt |
| INT8     | Podstawowy × 0.25 | Bardzo szybka | Dobra | Zrównoważony wybór |
| INT4     | Podstawowy × 0.125 | Najszybsza | Akceptowalna | Ograniczone zasoby |

💡 **Rekomendacja**: Zacznij od kwantyzacji INT4 przy pierwszym wdrożeniu. Jeśli jakość nie będzie zadowalająca, spróbuj INT8 lub FP16.

## Część 3: Konfiguracja wdrożenia w Foundry Local

### Tworzenie konfiguracji modelu

Przejdź do katalogu modeli Foundry Local:

```bash
foundry cache cd ./models/
```

Utwórz strukturę katalogów dla swojego modelu:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Utwórz plik konfiguracyjny `inference_model.json` w katalogu modelu:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Szablony konfiguracji specyficzne dla modelu

#### Dla modeli z serii Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Część 4: Testowanie i optymalizacja modelu

### Weryfikacja instalacji modelu

Sprawdź, czy Foundry Local rozpoznaje Twój model:

```bash
foundry cache ls
```

Powinieneś zobaczyć `your-finetuned-model-int4` na liście.

### Rozpoczęcie testowania modelu

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking wydajności

Monitoruj kluczowe metryki podczas testowania:

1. **Czas odpowiedzi**: Mierz średni czas na odpowiedź
2. **Zużycie pamięci**: Monitoruj wykorzystanie RAM
3. **Wykorzystanie CPU**: Sprawdzaj obciążenie procesora
4. **Jakość wyjścia**: Oceń trafność i spójność odpowiedzi

### Lista kontrolna walidacji jakości

- ✅ Model odpowiednio reaguje na zapytania z dostrojonej domeny
- ✅ Format odpowiedzi odpowiada oczekiwanej strukturze wyjściowej
- ✅ Brak wycieków pamięci podczas długotrwałego użytkowania
- ✅ Spójna wydajność dla różnych długości wejść
- ✅ Poprawne obsługiwanie przypadków brzegowych i nieprawidłowych danych wejściowych

## Podsumowanie

Gratulacje! Pomyślnie ukończyłeś:

- ✅ Konwersję formatu dostrojonego modelu
- ✅ Optymalizację kwantyzacji modelu
- ✅ Konfigurację wdrożenia w Foundry Local
- ✅ Strojenie wydajności i rozwiązywanie problemów

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego języku źródłowym powinien być uznawany za autorytatywne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.