<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T01:59:50+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "lt"
}
-->
# Sesija 4: Produkcinių pokalbių programų kūrimas su Chainlit

## Apžvalga

Šioje sesijoje sužinosite, kaip kurti produkcijai paruoštas pokalbių programas naudojant Chainlit ir Microsoft Foundry Local. Išmoksite kurti modernias interneto sąsajas AI pokalbiams, įgyvendinti srautinį atsakymų pateikimą ir diegti patikimas pokalbių programas su tinkamu klaidų valdymu bei vartotojo patirties dizainu.

**Ką sukursite:**
- **Chainlit pokalbių programa**: Moderni interneto sąsaja su srautiniais atsakymais
- **WebGPU demonstracija**: Naršyklėje vykdomas AI modelis privatumui užtikrinti  
- **Open WebUI integracija**: Profesionali pokalbių sąsaja su Foundry Local
- **Produkcijos šablonai**: Klaidų valdymas, stebėjimas ir diegimo strategijos

## Mokymosi tikslai

- Kurti produkcijai paruoštas pokalbių programas su Chainlit
- Įgyvendinti srautinį atsakymų pateikimą, kad pagerintumėte vartotojo patirtį
- Įvaldyti Foundry Local SDK integracijos šablonus
- Taikyti tinkamą klaidų valdymą ir sklandų degradavimą
- Diegti ir konfigūruoti pokalbių programas skirtingoms aplinkoms
- Suprasti modernius interneto sąsajos šablonus pokalbių AI

## Reikalavimai

- **Foundry Local**: Įdiegta ir veikianti ([Diegimo vadovas](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 ar naujesnė versija su virtualios aplinkos galimybėmis
- **Modelis**: Bent vienas įkeltas modelis (`foundry model run phi-4-mini`)
- **Naršyklė**: Moderni naršyklė su WebGPU palaikymu (Chrome/Edge)
- **Docker**: Open WebUI integracijai (neprivaloma)

## 1 dalis: Modernių pokalbių programų supratimas

### Architektūros apžvalga

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Pagrindinės technologijos

**Foundry Local SDK šablonai:**
- `FoundryLocalManager(alias)`: Automatinis paslaugų valdymas
- `manager.endpoint` ir `manager.api_key`: Prisijungimo duomenys
- `manager.get_model_info(alias).id`: Modelio identifikacija

**Chainlit sistema:**
- `@cl.on_chat_start`: Pokalbių sesijų inicijavimas
- `@cl.on_message`: Gaunamų vartotojo žinučių apdorojimas  
- `cl.Message().stream_token()`: Real-time srautiniai atsakymai
- Automatinis UI generavimas ir WebSocket valdymas

## 2 dalis: Vietinio ir debesų sprendimų palyginimas

### Našumo charakteristikos

| Aspektas | Vietinis (Foundry) | Debesų (Azure OpenAI) |
|----------|--------------------|-----------------------|
| **Vėlinimas** | 🚀 50-200ms (be tinklo) | ⏱️ 200-2000ms (priklauso nuo tinklo) |
| **Privatumas** | 🔒 Duomenys neišeina iš įrenginio | ⚠️ Duomenys siunčiami į debesį |
| **Kaina** | 💰 Nemokama po įrangos įsigijimo | 💸 Mokestis už kiekvieną žodį |
| **Offline** | ✅ Veikia be interneto | ❌ Reikalingas internetas |
| **Modelio dydis** | ⚠️ Ribotas dėl įrangos | ✅ Prieiga prie didžiausių modelių |
| **Mastelio keitimas** | ⚠️ Priklauso nuo įrangos | ✅ Neribotas mastelio keitimas |

### Hibridinės strategijos šablonai

**Vietinis pirmiausia su atsarginiu sprendimu:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Užduočių pagrindu vykdomas maršrutizavimas:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## 3 dalis: Pavyzdys 04 - Chainlit pokalbių programa

### Greitas startas

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Programa automatiškai atsidaro adresu `http://localhost:8080` su modernia pokalbių sąsaja.

### Pagrindinė įgyvendinimo dalis

Pavyzdys 04 demonstruoja produkcijai paruoštus šablonus:

**Automatinis paslaugų aptikimas:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Srautinio pokalbio apdorojimas:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Konfigūracijos parinktys

**Aplinkos kintamieji:**

| Kintamasis | Aprašymas | Numatytasis | Pavyzdys |
|------------|-----------|-------------|----------|
| `MODEL` | Naudojamas modelio alias | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local endpoint | Automatiškai aptinkamas | `http://localhost:51211` |
| `API_KEY` | API raktas (neprivalomas vietiniam) | `""` | `your-api-key` |

**Išplėstinis naudojimas:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## 4 dalis: Jupyter užrašų knygų kūrimas ir naudojimas

### Užrašų knygų palaikymo apžvalga

Pavyzdys 04 apima išsamią Jupyter užrašų knygą (`chainlit_app.ipynb`), kuri suteikia:

- **📚 Mokomąją medžiagą**: Žingsnis po žingsnio mokymosi medžiagą
- **🔬 Interaktyvų tyrinėjimą**: Galimybę vykdyti ir eksperimentuoti su kodų langeliais
- **📊 Vizualinius demonstravimus**: Diagramas, schemas ir rezultatų vizualizaciją
- **🛠️ Kūrimo įrankius**: Testavimo ir derinimo galimybes

### Savo užrašų knygų kūrimas

#### 1 žingsnis: Jupyter aplinkos nustatymas

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### 2 žingsnis: Naujos užrašų knygos kūrimas

**Naudojant VS Code:**
1. Atidarykite VS Code Module08 kataloge
2. Sukurkite naują failą su `.ipynb` plėtiniu
3. Pasirinkite "Foundry Local" branduolį, kai paprašoma
4. Pradėkite pridėti langelius su savo turiniu

**Naudojant Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Užrašų knygos struktūros geriausios praktikos

#### Langelio organizavimas

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Interaktyvūs pavyzdžiai ir pratimai

#### Pratimas 1: Kliento konfigūracijos testavimas

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Pratimas 2: Srautinio atsakymo simuliacija

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## 5 dalis: WebGPU naršyklės demonstracija

### Apžvalga

WebGPU leidžia vykdyti AI modelius tiesiogiai naršyklėje, užtikrinant maksimalų privatumą ir nereikalaujant diegimo. Šis pavyzdys demonstruoja ONNX Runtime Web su WebGPU vykdymu.

### 1 žingsnis: WebGPU palaikymo patikrinimas

**Naršyklės reikalavimai:**
- Chrome/Edge 113+ su įjungtu WebGPU
- Patikrinimas: `chrome://gpu` → patvirtinkite "WebGPU" statusą
- Programinis patikrinimas: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### 2 žingsnis: WebGPU demonstracijos kūrimas

Sukurkite katalogą: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### 3 žingsnis: Demonstracijos vykdymas

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## 6 dalis: Open WebUI integracija

### Apžvalga

Open WebUI suteikia profesionalią ChatGPT tipo sąsają, kuri jungiasi prie Foundry Local OpenAI suderinamos API.

### 1 žingsnis: Reikalavimai

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### 2 žingsnis: Docker nustatymas (rekomenduojama)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Pastaba:** `host.docker.internal` leidžia Docker konteineriams pasiekti pagrindinį kompiuterį Windows sistemoje.

### 3 žingsnis: Konfigūracija

1. **Atidarykite naršyklę:** Eikite į `http://localhost:3000`
2. **Pradinis nustatymas:** Sukurkite administratoriaus paskyrą
3. **Modelio konfigūracija:**
   - Nustatymai → Modeliai → OpenAI API  
   - Bazinis URL: `http://host.docker.internal:51211/v1`
   - API raktas: `foundry-local-key` (bet kokia reikšmė tinka)
4. **Testuokite ryšį:** Modeliai turėtų pasirodyti išskleidžiamajame meniu

### Trikčių šalinimas

**Dažnos problemos:**

1. **Ryšys atmestas:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modeliai nepasirodo:**
   - Patikrinkite, ar modelis įkeltas: `foundry model list`
   - Patikrinkite API atsakymą: `curl http://localhost:51211/v1/models`
   - Perkraukite Open WebUI konteinerį

## 7 dalis: Produkcijos diegimo svarstymai

### Aplinkos konfigūracija

**Kūrimo nustatymas:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Produkcijos diegimas:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Dažnos portų problemos ir sprendimai

**Porto 51211 konfliktų prevencija:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Našumo stebėjimas

**Sveikatos patikrinimo įgyvendinimas:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Santrauka

4 sesijoje aptarėme produkcijai paruoštų Chainlit programų kūrimą pokalbių AI. Sužinojote apie:

- ✅ **Chainlit sistema**: Moderni UI ir srautinė parama pokalbių programoms
- ✅ **Foundry Local integracija**: SDK naudojimo ir konfigūracijos šablonai  
- ✅ **WebGPU vykdymas**: Naršyklėje vykdomas AI maksimaliai privatumui
- ✅ **Open WebUI nustatymas**: Profesionali pokalbių sąsajos diegimas
- ✅ **Produkcijos šablonai**: Klaidų valdymas, stebėjimas ir mastelio keitimas

Pavyzdys 04 demonstruoja geriausias praktikas kuriant patikimas pokalbių sąsajas, kurios naudoja vietinius AI modelius per Microsoft Foundry Local, užtikrinant puikią vartotojo patirtį.

## Nuorodos

- **[Pavyzdys 04: Chainlit programa](samples/04/README.md)**: Pilna programa su dokumentacija
- **[Chainlit mokomoji užrašų knyga](samples/04/chainlit_app.ipynb)**: Interaktyvi mokymosi medžiaga
- **[Foundry Local dokumentacija](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Pilna platformos dokumentacija
- **[Chainlit dokumentacija](https://docs.chainlit.io/)**: Oficialus sistemos dokumentacija
- **[Open WebUI integracijos vadovas](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Oficialus vadovas

---

**Atsakomybės atsisakymas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, kylančius dėl šio vertimo naudojimo.