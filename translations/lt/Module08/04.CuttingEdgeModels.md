<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-23T01:14:43+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "lt"
}
-->
# Sesija 4: Pažangūs modeliai – LLM, SLM ir įrenginių vidinė analizė

## Apžvalga

Palyginkite LLM ir SLM, įvertinkite vietinės ir debesų analizės kompromisus, bei įgyvendinkite demonstracijas, kurios parodo EdgeAI scenarijus naudojant Phi ir ONNX Runtime. Taip pat aptarsime Chainlit RAG, WebGPU analizės galimybes ir Open WebUI integraciją.

Nuorodos:
- Foundry Local dokumentacija: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI instrukcija (pokalbių programa su Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Mokymosi tikslai
- Suprasti LLM ir SLM kompromisus dėl kainos, vėlavimo ir tikslumo
- Pasirinkti vietinę arba debesų analizę pagal konkrečius verslo poreikius
- Įgyvendinti mažą RAG demonstraciją su Chainlit
- Išbandyti WebGPU naršyklės pagreičio galimybes
- Prijungti Open WebUI prie Foundry Local

## 1 dalis: LLM vs SLM – sprendimų matrica

Apsvarstykite:
- Vėlavimas: SLM įrenginyje dažnai pateikia atsakymus per mažiau nei sekundę
- Kaina: vietinė analizė sumažina debesų išlaidas
- Privatumas: jautrūs duomenys lieka įrenginyje
- Gebėjimai: LLM gali pranokti SLM sudėtingose užduotyse
- Patikimumas: hibridinės strategijos sumažina prastovų riziką

## 2 dalis: Vietinė vs debesų analizė – hibridinės strategijos

- Pirmiausia vietinė analizė su debesų atsarginiu variantu dideliems/sudėtingiems užklausoms
- Pirmiausia debesų analizė su vietine galimybe privatumo jautriems ar neprisijungusiems scenarijams
- Maršrutizavimas pagal užduoties tipą (kodo generavimas į DeepSeek, bendras pokalbis į Phi/Qwen)

## 3 dalis: RAG pokalbių programa su Chainlit (minimalus)

Įdiekite priklausomybes:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Paleiskite:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Išplėskite: pridėkite paprastą ieškiklį (vietiniai failai) ir pridėkite surastą kontekstą prie vartotojo užklausos.

## 4 dalis: WebGPU analizė (įspėjimas)

Paleiskite mažus modelius tiesiogiai naršyklėje naudodami WebGPU. Tai idealiai tinka privatumo pirmumo demonstracijoms ir patirtims be diegimo. Žemiau pateikiamas minimalus, žingsnis po žingsnio pavyzdys naudojant ONNX Runtime Web su WebGPU vykdymo tiekėju.

1) Patikrinkite WebGPU palaikymą
- Chromium naršyklės: chrome://gpu → patvirtinkite, kad „WebGPU“ įjungtas
- Programinis patikrinimas (taip pat patikrinsime kode): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) Sukurkite minimalų projektą
Sukurkite aplanką ir du failus: `index.html` ir `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Paleiskite vietoje (Windows cmd.exe)
Naudokite paprastą statinį serverį, kad naršyklė galėtų pasiekti modelį.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Atidarykite http://localhost:5173 savo naršyklėje. Turėtumėte matyti inicializacijos žurnalus, sesijos sukūrimą su WebGPU ir argmax prognozę.

4) Trikčių šalinimas
- Jei WebGPU nėra: atnaujinkite Chrome/Edge ir įsitikinkite, kad GPU tvarkyklės yra naujausios, tada patikrinkite chrome://flags dėl „Enable WebGPU“.
- Jei kyla CORS ar fetch klaidų: įsitikinkite, kad failus pateikiate per http:// (ne file://) ir modelio URL leidžia kryžminės kilmės užklausas.
- Atsarginis variantas CPU: pakeiskite `executionProviders: ['wasm']`, kad patikrintumėte bazinį elgesį.

5) Kiti žingsniai
- Pakeiskite į domeno specifinį ONNX modelį (pvz., vaizdų klasifikavimas ar mažas teksto modelis).
- Pridėkite išankstinio/apdorojimo logiką realiems įvestims.
- Didesniems modeliams ar gamybos vėlavimui, rinkitės Foundry Local arba ONNX Runtime Server.

## 5 dalis: Open WebUI + Foundry Local (žingsnis po žingsnio)

Tai sujungia Open WebUI su Foundry Local OpenAI suderinamu API, kad būtų sukurta vietinė pokalbių sąsaja.

1) Reikalavimai
- Foundry Local įdiegtas ir veikia (`foundry --version`)
- Vienas modelis paruoštas vietiniam paleidimui (pvz., `phi-4-mini`)
- Įdiegtas Docker Desktop (rekomenduojama Open WebUI)

2) Paleiskite modelį su Foundry Local
```powershell
foundry model run phi-4-mini
```
Tai suteikia OpenAI suderinamą API adresu `http://localhost:8000`.

3) Paleiskite Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Pastabos:
- Windows sistemoje `host.docker.internal` leidžia konteineriui pasiekti jūsų kompiuterį adresu `localhost`.
- Nustatome `OPENAI_API_BASE_URL` į Foundry Local API adresą ir fiktyvų `OPENAI_API_KEY`.

4) Konfigūruokite iš Open WebUI sąsajos (alternatyva)
- Naršykite į http://localhost:3000
- Užbaikite pradinį nustatymą (admin vartotojas)
- Eikite į Settings → Models/Providers
- Nustatykite Base URL: `http://host.docker.internal:8000/v1`
- Nustatykite API Key: `local-key` (vietos raktas)
- Išsaugokite

5) Paleiskite testinę užklausą
- Open WebUI pokalbių sąsajoje pasirinkite arba įveskite modelio pavadinimą `phi-4-mini`
- Užklausa: „Išvardinkite penkis privalumus įrenginio vidinės AI analizės.“
- Turėtumėte matyti atsakymą, transliuojamą iš jūsų vietinio modelio

6) Trikčių šalinimas
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Pasirinktinai: išsaugokite Open WebUI duomenis
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Praktinių užduočių sąrašas
- [ ] Palyginkite atsakymus/vėlavimą tarp SLM ir LLM vietoje
- [ ] Paleiskite Chainlit demonstraciją su bent dviem modeliais
- [ ] Prijunkite Open WebUI prie savo vietinio API ir išbandykite

## Kiti žingsniai
- Pasiruoškite agentų darbo eigoms 5 sesijoje
- Nustatykite scenarijus, kur hibridinė vietinė/debesų analizė pagerina investicijų grąžą

---

