<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8641f7baaf43de0f266276b64c5f5020",
  "translation_date": "2025-09-25T02:43:24+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "lt"
}
-->
# ## Apžvalga

Naudokite AI modelius kaip modulius, pritaikomus įrankius, veikiančius tiesiogiai įrenginyje su Foundry Local. Ši sesija pabrėžia praktinius darbo procesus, skirtus privatumo išsaugojimui, mažo delsimo prognozėms ir kaip integruoti šiuos įrankius per SDK, API ar CLI. Taip pat sužinosite, kaip prireikus pereiti prie Azure AI Foundry.

> **🔄 Atnaujinta pagal modernų SDK**: Šis modulis suderintas su naujausiais Microsoft Foundry-Local saugyklos šablonais ir atitinka intelektualaus maršrutizavimo įgyvendinimą `samples/06/`. Pavyzdžiai dabar naudoja modernų `foundry-local-sdk` ir pažangias modelių pasirinkimo strategijas.

**🏗️ Architektūros akcentai:**
- **Intelektualus modelių maršrutizavimas**: Raktinių žodžių pagrindu vykdomas pasirinkimas tarp bendrųjų, loginio mąstymo, kodo ir kūrybinių modelių
- **Moderni SDK integracija**: Naudojamas `FoundryLocalManager` su automatiniu paslaugų aptikimu
- **Aplinkos konfigūracija**: Lankstus modelių priskyrimas per aplinkos kintamuosius
- **Sveikatos stebėjimas**: Paslaugų validacija ir modelių prieinamumo tikrinimas
- **Paruošta gamybai**: Išsamus klaidų tvarkymas ir atsarginiai mechanizmai

**📁 Vietinė įgyvendinimas:**
- `samples/06/router.py` - Intelektualus modelių maršrutizatorius su raktinių žodžių pagrindu vykdomu pasirinkimu
- `samples/06/model_router.ipynb` - Interaktyvūs pavyzdžiai ir našumo testai
- `samples/06/README.md` - Konfigūracijos ir naudojimo instrukcijos

Nuorodos:
- Foundry Local dokumentacija: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integracija su prognozavimo SDK: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face modelių kompiliavimas: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models6: Foundry Local – Modeliai kaip įrankiai

## Apžvalga

Naudokite AI modelius kaip modulius, pritaikomus įrankius, veikiančius tiesiogiai įrenginyje su Foundry Local. Ši sesija pabrėžia praktinius darbo procesus, skirtus privatumo išsaugojimui, mažo delsimo prognozėms ir kaip integruoti šiuos įrankius per SDK, API ar CLI. Taip pat sužinosite, kaip prireikus pereiti prie Azure AI Foundry.

Nuorodos:
- Foundry Local dokumentacija: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integracija su prognozavimo SDK: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face modelių kompiliavimas: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Mokymosi tikslai
- Kurti modelio-kaip-įrankio šablonus įrenginyje
- Integruoti per OpenAI suderinamą REST API arba SDK
- Pritaikyti modelius specifiniams domenams
- Planuoti hibridinį mastelį su Azure AI Foundry

## 1 dalis: Intelektualus modelių maršrutizatorius (modernus įgyvendinimas)

Tikslas: Įgyvendinti intelektualų modelių pasirinkimą su automatiniu maršrutizavimu pagal užklausos turinį.

> **📋 Pastaba**: Šis įgyvendinimas atitinka šablonus, naudojamus `samples/06/router.py` su pažangiu raktinių žodžių pagrindu vykdomu modelių pasirinkimu.

1 žingsnis) Apibrėžkite modernų modelių maršrutizatorių su FoundryLocalManager  
```python
# router/intelligent_router.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import Dict, Any, Optional
import os
import json

class ModelRouter:
    """Intelligent model router that selects appropriate models for different task types."""
    
    def __init__(self):
        self.client = None
        self.base_url = None
        self.tools = self._load_tool_registry()
        self._initialize_client()
    
    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load tool registry from environment or use defaults."""
        default_tools = {
            "general": {
                "model": os.environ.get("GENERAL_MODEL", "phi-4-mini"),
                "notes": "Fast general-purpose chat and Q&A",
                "temperature": 0.7
            },
            "reasoning": {
                "model": os.environ.get("REASONING_MODEL", "deepseek-r1-distill-qwen-7b"),
                "notes": "Step-by-step analysis and logical reasoning",
                "temperature": 0.3
            },
            "code": {
                "model": os.environ.get("CODE_MODEL", "qwen2.5-7b-instruct"),
                "notes": "Code generation, debugging, and technical tasks",
                "temperature": 0.2
            },
            "creative": {
                "model": os.environ.get("CREATIVE_MODEL", "phi-4-mini"),
                "notes": "Creative writing and storytelling",
                "temperature": 0.9
            }
        }
        
        # Check for environment override
        tools_env = os.environ.get("TOOL_REGISTRY")
        if tools_env:
            try:
                return json.loads(tools_env)
            except json.JSONDecodeError:
                print("Warning: Invalid TOOL_REGISTRY JSON, using defaults")
        
        return default_tools
```
  
2 žingsnis) Inicializuokite klientą su moderniu SDK ir paslaugų aptikimu  
```python
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local or fallback configuration."""
        try:
            from foundry_local import FoundryLocalManager
            # Try to use any available model for client initialization
            first_model = next(iter(self.tools.values()))["model"]
            manager = FoundryLocalManager(first_model)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.base_url = manager.endpoint
            print(f"✅ Foundry Local SDK initialized")
        except Exception as e:
            print(f"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration")
            # Fallback to manual configuration
            self.base_url = os.environ.get("BASE_URL", "http://localhost:8000")
            api_key = os.environ.get("API_KEY", "")
            
            self.client = OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key=api_key
            )
            print(f"Initialized manual configuration at {self.base_url}")
    
    def select_tool(self, user_query: str) -> str:
        """Select the most appropriate tool based on the user query."""
        query_lower = user_query.lower()
        
        # Code-related keywords
        code_keywords = ["code", "python", "function", "class", "method", "bug", "debug", 
                        "programming", "script", "algorithm", "implementation", "refactor"]
        if any(keyword in query_lower for keyword in code_keywords):
            return "code"
        
        # Reasoning keywords
        reasoning_keywords = ["why", "how", "explain", "step-by-step", "reason", "analyze", 
                             "think", "logic", "because", "cause", "compare", "evaluate"]
        if any(keyword in query_lower for keyword in reasoning_keywords):
            return "reasoning"
        
        # Creative keywords
        creative_keywords = ["story", "poem", "creative", "imagine", "write", "tale", 
                           "narrative", "fiction", "character", "plot"]
        if any(keyword in query_lower for keyword in creative_keywords):
            return "creative"
        
        # Default to general
        return "general"
    
    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:
        """Send chat completion request to the specified model."""
        try:
            params = {
                "model": model,
                "messages": [{"role": "user", "content": content}],
                "max_tokens": max_tokens
            }
            
            if temperature is not None:
                params["temperature"] = temperature
            
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response with model {model}: {str(e)}"
```
  
3 žingsnis) Įgyvendinkite intelektualų maršrutizavimą ir vykdymą (žr. `samples/06/router.py`)  
```python
    def route_and_run(self, prompt: str) -> Dict[str, Any]:
        """Route the prompt to the appropriate model and generate response."""
        tool_key = self.select_tool(prompt)
        tool_config = self.tools[tool_key]
        model = tool_config["model"]
        temperature = tool_config.get("temperature", 0.7)
        
        print(f"🎯 Selected tool: {tool_key} (model: {model})")
        
        answer = self.chat(
            model=model, 
            content=prompt, 
            max_tokens=400, 
            temperature=temperature
        )
        
        return {
            "tool": tool_key,
            "model": model,
            "tool_description": tool_config["notes"],
            "temperature": temperature,
            "answer": answer
        }
    
    def check_service_health(self) -> Dict[str, Any]:
        """Check Foundry Local service health and available models."""
        try:
            models_response = self.client.models.list()
            available_models = [model.id for model in models_response.data]
            
            return {
                "status": "healthy",
                "base_url": self.base_url,
                "available_models": available_models,
                "tools_configured": list(self.tools.keys())
            }
        except Exception as e:
            return {
                "status": "error",
                "base_url": self.base_url,
                "error": str(e)
            }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    router = ModelRouter()
    
    # Check health
    health = router.check_service_health()
    print(f"Service Health: {json.dumps(health, indent=2)}")
    
    # Test different query types
    queries = [
        "Write a Python function to calculate fibonacci numbers",  # -> code
        "Explain step-by-step why the sky is blue",  # -> reasoning
        "Tell me a creative story about AI",  # -> creative
        "What's the weather like today?"  # -> general
    ]
    
    for query in queries:
        result = router.route_and_run(query)
        print(f"\nQuery: {query}")
        print(f"Selected: {result['tool']} -> {result['model']}")
        print(f"Answer: {result['answer'][:100]}...")
```
  

## 2 dalis: Moderni SDK integracija (žingsnis po žingsnio)

Tikslas: Naudoti Foundry Local SDK su OpenAI Python SDK sklandžiai integracijai.

1 žingsnis) Įdiekite priklausomybes  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install foundry-local-sdk openai
```
  
2 žingsnis) Konfigūruokite aplinką (pasirinktinai - žr. `samples/06/README.md`)  
```cmd
REM Override default models per tool
set GENERAL_MODEL=phi-4-mini
set REASONING_MODEL=deepseek-r1-distill-qwen-7b
set CODE_MODEL=qwen2.5-7b-instruct
REM Or provide a full JSON registry
set TOOL_REGISTRY={"general":{"model":"phi-4-mini"},"reasoning":{"model":"deepseek-r1-distill-qwen-7b"}}
```
  
3 žingsnis) Moderni SDK integracija  
```python
# modern_sdk_demo.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import sys

def main():
    """Demonstrate modern SDK integration."""
    try:
        # Initialize with FoundryLocalManager
        alias = "phi-4-mini"
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client using Foundry Local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Get model info
        model_info = manager.get_model_info(alias)
        print(f"Using model: {model_info.id}")
        
        # Make request with streaming
        stream = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Explain edge AI benefits in one paragraph."}],
            stream=True,
            max_tokens=200
        )
        
        print("Response: ", end="")
        for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"Error: {e}")
        print("Ensure Foundry Local is running with: foundry model run phi-4-mini")
        sys.exit(1)

if __name__ == "__main__":
    main()
```
  

## 3 dalis: Domeno pritaikymas (žingsnis po žingsnio)

Tikslas: Pritaikyti rezultatus domenui naudojant šablonus ir JSON schemą.

1 žingsnis) Sukurkite domeno šabloną  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
2 žingsnis) Užtikrinkite JSON išvestį  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## 4 dalis: Neprisijungus ir saugumo laikysena (žingsnis po žingsnio)

Tikslas: Užtikrinti privatumą ir atsparumą, kai modeliai veikia kaip įrankiai vietoje.

1 žingsnis) Iš anksto paruoškite ir patikrinkite vietinį galinį tašką  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
2 žingsnis) Išvalykite įvestis  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
3 žingsnis) Vietinis režimas ir žurnalavimas  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## 5 dalis: Gamybos diegimas ir mastelio keitimas

Tikslas: Diegti intelektualų maršrutizatorių su stebėjimu ir Azure AI Foundry integracija.

> **📋 Pastaba**: Vietinė įgyvendinimas `samples/06/model_router.ipynb` apima išsamius gamybos diegimo šablonų pavyzdžius.

1 žingsnis) Gamybos maršrutizatorius su stebėjimu (žr. `samples/06/router.py`)  
```python
# production/router.py
from router.intelligent_router import ModelRouter
import json
import time
import sys

class ProductionModelRouter(ModelRouter):
    """Production-ready model router with monitoring and logging."""
    
    def __init__(self):
        super().__init__()
        self.request_count = 0
        self.error_count = 0
        self.start_time = time.time()
    
    def route_and_run_with_monitoring(self, prompt: str) -> Dict[str, Any]:
        """Route with comprehensive monitoring and error handling."""
        start_time = time.time()
        self.request_count += 1
        
        try:
            result = self.route_and_run(prompt)
            processing_time = time.time() - start_time
            
            # Log successful request
            self._log_request({
                "status": "success",
                "tool": result["tool"],
                "model": result["model"],
                "processing_time": processing_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            result["processing_time"] = processing_time
            return result
            
        except Exception as e:
            self.error_count += 1
            error_result = {
                "status": "error",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            self._log_request(error_result)
            return error_result
    
    def _log_request(self, data: Dict[str, Any]):
        """Log request data for monitoring."""
        print(f"📊 {json.dumps(data)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get router statistics."""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "total_requests": self.request_count,
            "error_count": self.error_count,
            "success_rate": (self.request_count - self.error_count) / max(1, self.request_count),
            "requests_per_minute": self.request_count / max(1, uptime / 60)
        }

def main():
    """Production router demo."""
    router = ProductionModelRouter()
    
    # Health check
    health = router.check_service_health()
    if health["status"] == "error":
        print(f"❌ Service health check failed: {health['error']}")
        sys.exit(1)
    
    print(f"✅ Service healthy with {len(health['available_models'])} models")
    
    # Process user query
    user_prompt = " ".join(sys.argv[1:]) or "Write three benefits of on-device AI in JSON format."
    print(f"\n🎯 Processing: {user_prompt}")
    
    result = router.route_and_run_with_monitoring(user_prompt)
    
    if result.get("status") == "error":
        print(f"❌ Error: {result['error']}")
    else:
        print(f"\n📋 Result:")
        print(f"Tool: {result['tool']} -> Model: {result['model']}")
        print(f"Processing Time: {result['processing_time']:.2f}s")
        print(f"Answer: {result['answer']}")
    
    # Show stats
    stats = router.get_stats()
    print(f"\n📊 Statistics: {json.dumps(stats, indent=2)}")

if __name__ == "__main__":
    main()
```
  

## Praktinis kontrolinis sąrašas
- [ ] Įgyvendinti intelektualų modelių maršrutizatorių su raktinių žodžių pagrindu vykdomu pasirinkimu (`samples/06/router.py`)
- [ ] Konfigūruoti kelis specializuotus modelius (bendrieji, loginio mąstymo, kodo, kūrybiniai)
- [ ] Išbandyti interaktyvų Jupyter užrašų knygelę (`samples/06/model_router.ipynb`)
- [ ] Nustatyti aplinkos pagrindu vykdomą modelių konfigūraciją
- [ ] Įgyvendinti paslaugų sveikatos stebėjimą ir klaidų tvarkymą
- [ ] Diegti gamybos maršrutizatorių su išsamiu žurnalavimu

## Vietinė pavyzdžių integracija

Vykdykite pilną įgyvendinimą:  
```cmd
cd Module08
.\.venv\Scripts\activate

REM Start required models
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
foundry model run deepseek-r1-distill-qwen-7b

REM Test the intelligent router
python samples\06\router.py "Write a Python function to sort a list"
python samples\06\router.py "Explain step-by-step how bubble sort works"
python samples\06\router.py "Tell me a creative story about robots"

REM Explore the interactive notebook
jupyter notebook samples/06/model_router.ipynb
```
  

## Nuorodos ir tolesni žingsniai
- **Vietinė įgyvendinimas**: `samples/06/` - Pilnas intelektualus maršrutizatorius su kelių modelių palaikymu
- **Microsoft pavyzdžiai**: [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
- **Integracijos dokumentacija**: [Integracija su prognozavimo SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks)
- **Pažangūs šablonai**: Tyrinėkite funkcijų kvietimą ir daugiaveiksnių agentų orkestravimą 5 modulyje

## Apibendrinimas

Foundry Local suteikia galimybę naudoti patikimą AI tiesiogiai įrenginyje, kur modeliai tampa intelektualiais, specializuotais įrankiais. Su automatiniu modelių pasirinkimu, išsamiu stebėjimu ir gamybai paruoštais šablonais komandos gali kurti sudėtingas AI programas, kurios prisitaiko prie skirtingų užduočių tipų, išlaikydamos privatumą ir našumą. Čia pademonstruotas intelektualaus maršrutizatoriaus šablonas suteikia pagrindą kurti sudėtingas AI sistemas, kurios gali būti masteliais nuo vietinio kūrimo iki gamybos diegimo.

---

