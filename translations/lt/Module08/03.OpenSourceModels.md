<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:03:54+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "lt"
}
-->
# Sesija 3: Atviro kodo modeliai su Foundry Local

## Apžvalga

Šioje sesijoje nagrinėjama, kaip integruoti atviro kodo modelius į Foundry Local: pasirinkti bendruomenės modelius, integruoti Hugging Face turinį ir taikyti „atsinešk savo modelį“ (BYOM) strategijas. Taip pat sužinosite apie „Model Mondays“ seriją, skirtą nuolatiniam mokymuisi ir modelių atradimui.

Nuorodos:
- Foundry Local dokumentacija: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face modelių kompiliavimas: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Mokymosi tikslai
- Atrasti ir įvertinti atviro kodo modelius vietiniam naudojimui
- Kompiliuoti ir paleisti pasirinktus Hugging Face modelius Foundry Local aplinkoje
- Taikyti modelių pasirinkimo strategijas, atsižvelgiant į tikslumą, vėlavimą ir resursų poreikius
- Vietiniu būdu valdyti modelius naudojant talpyklą ir versijų kontrolę

## 1 dalis: Modelių atranka ir pasirinkimas (žingsnis po žingsnio)

1 žingsnis) Išvardykite galimus modelius vietiniame kataloge  
```cmd
foundry model list
```
  
2 žingsnis) Greitai išbandykite du kandidatus (automatiškai atsisiunčiami pirmo paleidimo metu)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
3 žingsnis) Atkreipkite dėmesį į pagrindinius rodiklius  
- Stebėkite vėlavimą (subjektyviai) ir kokybę, naudodami fiksuotą užklausą  
- Stebėkite atminties naudojimą per Task Manager, kai kiekvienas modelis veikia  

## 2 dalis: Modelių paleidimas per CLI (žingsnis po žingsnio)

1 žingsnis) Paleiskite modelį  
```cmd
foundry model run llama-3.2
```
  
2 žingsnis) Siųskite testinę užklausą per OpenAI suderinamą galinį tašką  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## 3 dalis: BYOM – Hugging Face modelių kompiliavimas (žingsnis po žingsnio)

Sekite oficialų vadovą modelių kompiliavimui. Žemiau pateikiama aukšto lygio eiga – tikslias komandas ir palaikomas konfigūracijas rasite Microsoft Learn straipsnyje.

1 žingsnis) Paruoškite darbo katalogą  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
2 žingsnis) Kompiliuokite palaikomą HF modelį  
- Naudokite Learn dokumente pateiktus žingsnius, kad konvertuotumėte ir patalpintumėte kompiliuotą ONNX modelį į savo `models` katalogą  
- Patvirtinkite su:  
```cmd
foundry cache ls
```
  
Turėtumėte matyti savo kompiliuoto modelio pavadinimą (pvz., `llama-3.2`).  

3 žingsnis) Paleiskite kompiliuotą modelį  
```cmd
foundry model run llama-3.2 --verbose
```
  
Pastabos:  
- Užtikrinkite pakankamą disko ir RAM vietą kompiliavimui ir paleidimui  
- Pradėkite nuo mažesnių modelių, kad patikrintumėte eigą, tada pereikite prie didesnių  

## 4 dalis: Praktinis modelių atrinkimas (žingsnis po žingsnio)

1 žingsnis) Sukurkite `models.json` registrą  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
2 žingsnis) Mažas atrankos skriptas  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## 5 dalis: Praktiniai testai (žingsnis po žingsnio)

1 žingsnis) Paprastas vėlavimo testas  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
2 žingsnis) Kokybės patikrinimas  
- Naudokite fiksuotą užklausų rinkinį, išsaugokite rezultatus CSV/JSON formatu  
- Rankiniu būdu įvertinkite sklandumą, aktualumą ir teisingumą (1–5)  

## 6 dalis: Tolimesni žingsniai
- Prenumeruokite „Model Mondays“, kad gautumėte naujų modelių ir patarimų: https://aka.ms/model-mondays  
- Pasidalinkite atradimais su savo komanda, papildydami `models.json`  
- Pasiruoškite 4 sesijai: LLM ir SLM palyginimas, vietinis ir debesų naudojimas, praktiniai demonstravimai  

---

