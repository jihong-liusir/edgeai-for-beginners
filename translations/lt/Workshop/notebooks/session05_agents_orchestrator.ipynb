{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# Sesija 5 – Daugiaveiksnių agentų orkestratorius\n",
    "\n",
    "Pateikiamas paprastas dviejų agentų procesas (Tyrėjas -> Redaktorius), naudojant Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Priklausomybių įdiegimas\n",
    "Įdiegia `foundry-local-sdk` ir `openai`, reikalingus vietiniam modelio pasiekiamumui ir pokalbių užbaigimui. Idempotentiška.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# Scenarijus\n",
    "Įgyvendina minimalų dviejų agentų orkestravimo modelį:\n",
    "- **Tyrėjo agentas** renka glaustus faktinius punktus\n",
    "- **Redaktoriaus agentas** perrašo tekstą, kad jis būtų aiškus vadovams\n",
    "\n",
    "Parodo bendrą atmintį kiekvienam agentui, nuoseklų tarpinio rezultato perdavimą ir paprastą veikimo grandinę. Galima išplėsti į daugiau vaidmenų (pvz., Kritikas, Tikrintojas) arba lygiagrečias šakas.\n",
    "\n",
    "**Aplinkos kintamieji:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - Numatytoji naudojama modelio versija (numatytoji: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - Pagrindinis agento modelis (pakeičia ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - Redaktoriaus agento modelis (numatytoji: pagrindinis)\n",
    "\n",
    "**SDK Nuoroda:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**Kaip tai veikia:**\n",
    "1. **FoundryLocalManager** automatiškai paleidžia Foundry Local paslaugą\n",
    "2. Atsisiunčia ir įkelia nurodytą modelį (arba naudoja talpykloje esančią versiją)\n",
    "3. Suteikia OpenAI suderinamą sąsają sąveikai\n",
    "4. Kiekvienas agentas gali naudoti skirtingą modelį specializuotoms užduotims\n",
    "5. Įmontuota pakartotinio bandymo logika tvarko laikinus gedimus efektyviai\n",
    "\n",
    "**Pagrindinės savybės:**\n",
    "- ✅ Automatinis paslaugų aptikimas ir inicializavimas\n",
    "- ✅ Modelio gyvavimo ciklo valdymas (atsisiuntimas, talpykla, įkėlimas)\n",
    "- ✅ OpenAI SDK suderinamumas su pažįstama API\n",
    "- ✅ Daugiamodelinė parama agentų specializacijai\n",
    "- ✅ Patikimas klaidų tvarkymas su pakartotinio bandymo logika\n",
    "- ✅ Vietinė analizė (nereikia debesų API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Pagrindiniai importai ir tipavimas\n",
    "Pristatomi dataklasės agento žinučių saugojimui ir tipavimo užuominos aiškumui užtikrinti. Importuojamas Foundry vietinis valdytojas ir OpenAI klientas tolimesniems agento veiksmams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Modelio inicijavimas (SDK šablonas)\n",
    "Naudojamas Foundry vietinis Python SDK patikimam modelių valdymui:\n",
    "- **FoundryLocalManager(alias)** - Automatiškai paleidžia paslaugą ir įkelia modelį pagal alias\n",
    "- **get_model_info(alias)** - Alias susieja su konkrečiu modelio ID\n",
    "- **manager.endpoint** - Suteikia paslaugos galinį tašką OpenAI klientui\n",
    "- **manager.api_key** - Suteikia API raktą (nebūtinas vietiniam naudojimui)\n",
    "- Palaiko atskirus modelius skirtingiems agentams (pagrindinis ir redaktorius)\n",
    "- Įmontuota pakartotinio bandymo logika su eksponentiniu laukimo laiku užtikrina patikimumą\n",
    "- Ryšio patikrinimas, kad paslauga būtų paruošta naudoti\n",
    "\n",
    "**Pagrindinis SDK šablonas:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**Gyvavimo ciklo valdymas:**\n",
    "- Valdytojai saugomi globaliai, kad būtų tinkamai uždaryti\n",
    "- Kiekvienas agentas gali naudoti skirtingą modelį specializacijai\n",
    "- Automatinis paslaugų aptikimas ir ryšio valdymas\n",
    "- Sklandus pakartotinis bandymas su eksponentiniu laukimo laiku gedimų atveju\n",
    "\n",
    "Tai užtikrina tinkamą inicijavimą prieš pradedant agentų orkestravimą.\n",
    "\n",
    "**Nuoroda:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Agentų ir Atminties Klasės\n",
    "Apibrėžia lengvąją `AgentMsg` klasę atminties įrašams ir `Agent` klasę, kuri apima:\n",
    "- **Sistemos vaidmenį** - Agentų asmenybę ir instrukcijas\n",
    "- **Žinučių istoriją** - Išlaiko pokalbio kontekstą\n",
    "- **act() metodą** - Atlieka veiksmus su tinkamu klaidų tvarkymu\n",
    "\n",
    "Agentas gali naudoti skirtingus modelius (pagrindinį ir redaktoriaus) ir palaiko izoliuotą kontekstą kiekvienam agentui. Šis modelis leidžia:\n",
    "- Atminties išsaugojimą tarp veiksmų\n",
    "- Lankstų modelio priskyrimą kiekvienam agentui\n",
    "- Klaidų izoliaciją ir atkūrimą\n",
    "- Paprastą grandinimą ir koordinavimą\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Orkestruotas Vamzdynas\n",
    "Sukuria du specializuotus agentus:\n",
    "- **Tyrėjas**: Naudoja pagrindinį modelį, renka faktinę informaciją\n",
    "- **Redaktorius**: Gali naudoti atskirą modelį (jei sukonfigūruota), tobulina ir perrašo\n",
    "\n",
    "Funkcija `pipeline`:\n",
    "1. Tyrėjas surenka pirminę informaciją\n",
    "2. Redaktorius ją patobulina į galutinį, paruoštą naudoti rezultatą\n",
    "3. Grąžina tiek tarpinį, tiek galutinį rezultatą\n",
    "\n",
    "Šis modelis leidžia:\n",
    "- Modelių specializaciją (skirtingi modeliai skirtingoms užduotims)\n",
    "- Kokybės gerinimą per daugiapakopį apdorojimą\n",
    "- Informacijos transformacijos atsekamumą\n",
    "- Lengvą plėtrą į daugiau agentų ar lygiagretų apdorojimą\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### Paaiškinimas: Vykdymas ir rezultatai\n",
    "Atlieka kelių agentų veikimo procesą, susijusį su atitikties ir delsos tematikos klausimu, siekiant pademonstruoti:\n",
    "- Daugiapakopę informacijos transformaciją\n",
    "- Agentų specializaciją ir bendradarbiavimą\n",
    "- Rezultatų kokybės gerinimą per tobulinimą\n",
    "- Atsekamumą (išsaugomi tiek tarpiniai, tiek galutiniai rezultatai)\n",
    "\n",
    "**Rezultatų struktūra:**\n",
    "- `question` - Originalus vartotojo klausimas\n",
    "- `research` - Pirminiai tyrimo rezultatai (faktiniai punktai)\n",
    "- `final` - Patobulinta vadovaujančioji santrauka\n",
    "- `models` - Kokie modeliai buvo naudojami kiekviename etape\n",
    "\n",
    "**Plėtros idėjos:**\n",
    "1. Pridėti Kritiko agentą kokybės peržiūrai\n",
    "2. Įdiegti lygiagrečius tyrimo agentus skirtingiems aspektams\n",
    "3. Pridėti Tikrintojo agentą faktų patikrinimui\n",
    "4. Naudoti skirtingus modelius skirtingo sudėtingumo lygiams\n",
    "5. Įgyvendinti grįžtamojo ryšio ciklus iteraciniam tobulinimui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### Išplėstinis: Individualus agento konfigūravimas\n",
    "\n",
    "Pabandykite pritaikyti agento elgesį, pakeisdami aplinkos kintamuosius prieš paleisdami inicializacijos langelį:\n",
    "\n",
    "**Galimi modeliai:**\n",
    "- Naudokite `foundry model ls` terminale, kad pamatytumėte visus galimus modelius\n",
    "- Pavyzdžiai: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b ir kt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Atsakomybės atsisakymas**:  \nŠis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, kilusius dėl šio vertimo naudojimo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-09T22:07:26+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}