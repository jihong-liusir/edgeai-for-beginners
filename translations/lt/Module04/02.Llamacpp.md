<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-19T00:53:06+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "lt"
}
-->
# 2 skyrius: Llama.cpp įgyvendinimo vadovas

## Turinys
1. [Įvadas](../../../Module04)
2. [Kas yra Llama.cpp?](../../../Module04)
3. [Įdiegimas](../../../Module04)
4. [Kompiliavimas iš šaltinio kodo](../../../Module04)
5. [Modelio kvantizacija](../../../Module04)
6. [Pagrindinis naudojimas](../../../Module04)
7. [Pažangios funkcijos](../../../Module04)
8. [Python integracija](../../../Module04)
9. [Trikčių šalinimas](../../../Module04)
10. [Geriausia praktika](../../../Module04)

## Įvadas

Šis išsamus vadovas padės jums suprasti viską, ką reikia žinoti apie Llama.cpp – nuo pagrindinio įdiegimo iki pažangių naudojimo scenarijų. Llama.cpp yra galinga C++ įgyvendinimo sistema, leidžianti efektyviai vykdyti didelius kalbos modelius (LLM) su minimaliu nustatymu ir puikiu našumu įvairiose aparatinės įrangos konfigūracijose.

## Kas yra Llama.cpp?

Llama.cpp yra LLM vykdymo sistema, parašyta C/C++, leidžianti vietoje vykdyti didelius kalbos modelius su minimaliu nustatymu ir pažangiu našumu įvairioje aparatinėje įrangoje. Pagrindinės funkcijos:

### Pagrindinės funkcijos
- **Paprasta C/C++ įgyvendinimo sistema** be priklausomybių
- **Suderinamumas su įvairiomis platformomis** (Windows, macOS, Linux)
- **Aparatinės įrangos optimizacija** skirtingoms architektūroms
- **Kvantizacijos palaikymas** (nuo 1,5 iki 8 bitų sveikųjų skaičių kvantizacija)
- **CPU ir GPU pagreitinimas**
- **Atminties efektyvumas** ribotose aplinkose

### Privalumai
- Efektyviai veikia su CPU, nereikalaujant specializuotos aparatinės įrangos
- Palaiko kelis GPU galinius sprendimus (CUDA, Metal, OpenCL, Vulkan)
- Lengvas ir nešiojamas
- Apple Silicon yra prioritetas – optimizuotas naudojant ARM NEON, Accelerate ir Metal sistemas
- Palaiko įvairius kvantizacijos lygius, kad sumažintų atminties naudojimą

## Įdiegimas

### 1 metodas: Iš anksto paruošti dvejetainiai failai (rekomenduojama pradedantiesiems)

#### Atsisiųskite iš GitHub Releases
1. Apsilankykite [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Atsisiųskite tinkamą dvejetainį failą savo sistemai:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` Linux

3. Išskleiskite archyvą ir pridėkite katalogą prie sistemos PATH

#### Naudojant paketų tvarkykles

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (įvairios distribucijos):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### 2 metodas: Python paketas (llama-cpp-python)

#### Pagrindinis įdiegimas
```bash
pip install llama-cpp-python
```

#### Su aparatinės įrangos pagreitinimu
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Kompiliavimas iš šaltinio kodo

### Būtinos sąlygos

**Sistemos reikalavimai:**
- C++ kompiliatorius (GCC, Clang arba MSVC)
- CMake (3.14 ar naujesnė versija)
- Git
- Kompiliavimo įrankiai jūsų platformai

**Būtinų sąlygų įdiegimas:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Įdiekite Visual Studio 2022 su C++ kūrimo įrankiais
- Atsisiųskite CMake iš oficialios svetainės
- Įdiekite Git

### Pagrindinis kompiliavimo procesas

1. **Klonuokite saugyklą:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigūruokite kompiliavimą:**
```bash
cmake -B build
```

3. **Kompiliuokite projektą:**
```bash
cmake --build build --config Release
```

Norėdami greitesnio kompiliavimo, naudokite lygiagrečius darbus:
```bash
cmake --build build --config Release -j 8
```

### Aparatinės įrangos specifiniai kompiliavimai

#### CUDA palaikymas (NVIDIA GPU)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal palaikymas (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS palaikymas (CPU optimizacija)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan palaikymas
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Pažangios kompiliavimo parinktys

#### Derinimo kompiliavimas
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Su papildomomis funkcijomis
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Modelio kvantizacija

### GGUF formato supratimas

GGUF (Generalizuotas GGML Vieningas Formatas) yra optimizuotas failų formatas, skirtas efektyviai vykdyti didelius kalbos modelius naudojant Llama.cpp ir kitas sistemas. Jis siūlo:

- Standartizuotą modelio svorių saugojimą
- Geresnį suderinamumą tarp platformų
- Pagerintą našumą
- Efektyvų metaduomenų tvarkymą

### Kvantizacijos tipai

Llama.cpp palaiko įvairius kvantizacijos lygius:

| Tipas | Bitai | Aprašymas | Naudojimo atvejis |
|------|------|-------------|----------|
| F16 | 16 | Pusė tikslumo | Aukšta kokybė, didelė atmintis |
| Q8_0 | 8 | 8 bitų kvantizacija | Geras balansas |
| Q4_0 | 4 | 4 bitų kvantizacija | Vidutinė kokybė, mažesnis dydis |
| Q2_K | 2 | 2 bitų kvantizacija | Mažiausias dydis, žemesnė kokybė |

### Modelių konvertavimas

#### Iš PyTorch į GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Tiesioginis atsisiuntimas iš Hugging Face
Daugelis modelių yra prieinami GGUF formatu Hugging Face:
- Ieškokite modelių su "GGUF" pavadinime
- Atsisiųskite tinkamą kvantizacijos lygį
- Naudokite tiesiogiai su Llama.cpp

## Pagrindinis naudojimas

### Komandinės eilutės sąsaja

#### Paprastas teksto generavimas
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Modelių naudojimas iš Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Serverio režimas
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Dažniausiai naudojami parametrai

| Parametras | Aprašymas | Pavyzdys |
|-----------|-------------|---------|
| `-m` | Modelio failo kelias | `-m model.gguf` |
| `-p` | Teksto užklausa | `-p "Hello world"` |
| `-n` | Generuojamų žodžių skaičius | `-n 100` |
| `-c` | Konteksto dydis | `-c 4096` |
| `-t` | Gijų skaičius | `-t 8` |
| `-ngl` | GPU sluoksniai | `-ngl 32` |
| `-temp` | Temperatūra | `-temp 0.7` |

### Interaktyvus režimas

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Pažangios funkcijos

### Serverio API

#### Serverio paleidimas
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API naudojimas
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Našumo optimizavimas

#### Atminties valdymas
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Daugiafunkcinis apdorojimas
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU pagreitinimas
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python integracija

### Pagrindinis naudojimas su llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Pokalbių sąsaja

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Atsakymų transliavimas

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integracija su LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Trikčių šalinimas

### Dažnos problemos ir sprendimai

#### Kompiliavimo klaidos

**Problema: CMake nerastas**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problema: Kompiliatorius nerastas**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Vykdymo klaidos

**Problema: Modelio įkėlimas nepavyksta**
- Patikrinkite modelio failo kelią
- Patikrinkite failo leidimus
- Įsitikinkite, kad yra pakankamai RAM
- Išbandykite skirtingus kvantizacijos lygius

**Problema: Prastas našumas**
- Įjunkite aparatinės įrangos pagreitinimą
- Padidinkite gijų skaičių
- Naudokite tinkamą kvantizaciją
- Patikrinkite GPU atminties naudojimą

#### Atminties problemos

**Problema: Nepakanka atminties**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platformai specifinės problemos

#### Windows
- Naudokite MinGW arba Visual Studio kompiliatorių
- Įsitikinkite, kad PATH tinkamai sukonfigūruotas
- Patikrinkite antivirusinės programos trukdžius

#### macOS
- Įjunkite Metal Apple Silicon
- Naudokite Rosetta 2, jei reikia suderinamumo
- Patikrinkite Xcode komandų eilutės įrankius

#### Linux
- Įdiekite kūrimo paketus
- Patikrinkite GPU tvarkyklių versijas
- Patikrinkite CUDA įrankių rinkinio įdiegimą

## Geriausia praktika

### Modelio pasirinkimas
1. **Pasirinkite tinkamą kvantizaciją** pagal savo aparatinę įrangą
2. **Apsvarstykite modelio dydžio** ir kokybės kompromisus
3. **Išbandykite skirtingus modelius** pagal savo specifinį naudojimo atvejį

### Našumo optimizavimas
1. **Naudokite GPU pagreitinimą**, jei įmanoma
2. **Optimizuokite gijų skaičių** pagal savo CPU
3. **Nustatykite tinkamą konteksto dydį** pagal savo poreikius
4. **Įjunkite atminties žemėlapį** dideliems modeliams

### Produkcijos diegimas
1. **Naudokite serverio režimą** API prieigai
2. **Įgyvendinkite tinkamą klaidų tvarkymą**
3. **Stebėkite resursų naudojimą**
4. **Nustatykite žurnalų ir stebėjimo sistemas**

### Kūrimo darbo eiga
1. **Pradėkite nuo mažesnių modelių** testavimui
2. **Naudokite versijų kontrolę** modelio konfigūracijoms
3. **Dokumentuokite savo konfigūracijas**
4. **Testuokite skirtingose platformose**

### Saugumo aspektai
1. **Patikrinkite įvesties užklausas**
2. **Įgyvendinkite užklausų ribojimą**
3. **Apsaugokite API galinius taškus**
4. **Stebėkite piktnaudžiavimo modelius**

## Išvada

Llama.cpp suteikia galingą ir efektyvų būdą vietoje vykdyti didelius kalbos modelius įvairiose aparatinės įrangos konfigūracijose. Nesvarbu, ar kuriate AI programas, atliekate tyrimus, ar tiesiog eksperimentuojate su LLM, ši sistema siūlo lankstumą ir našumą, reikalingą įvairiems naudojimo atvejams.

Pagrindinės išvados:
- Pasirinkite įdiegimo metodą, kuris geriausiai atitinka jūsų poreikius
- Optimizuokite pagal savo specifinę aparatinės įrangos konfigūraciją
- Pradėkite nuo pagrindinio naudojimo ir palaipsniui tyrinėkite pažangias funkcijas
- Apsvarstykite Python jungtis, kad būtų lengviau integruoti
- Laikykitės geriausios praktikos produkcijos diegimui

Daugiau informacijos ir naujienų rasite [oficialioje Llama.cpp saugykloje](https://github.com/ggml-org/llama.cpp) ir išsamioje dokumentacijoje bei bendruomenės resursuose.

## ➡️ Kas toliau

- [03: Microsoft Olive optimizavimo rinkinys](./03.MicrosoftOlive.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius dėl šio vertimo naudojimo.