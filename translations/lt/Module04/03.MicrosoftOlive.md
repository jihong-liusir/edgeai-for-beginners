<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-19T00:44:05+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "lt"
}
-->
# 3 skyrius: Microsoft Olive optimizavimo rinkinys

## Turinys
1. [Įvadas](../../../Module04)
2. [Kas yra Microsoft Olive?](../../../Module04)
3. [Įdiegimas](../../../Module04)
4. [Greito starto vadovas](../../../Module04)
5. [Pavyzdys: Qwen3 konvertavimas į ONNX INT4](../../../Module04)
6. [Išplėstinis naudojimas](../../../Module04)
7. [Geriausia praktika](../../../Module04)
8. [Trikčių šalinimas](../../../Module04)
9. [Papildomi ištekliai](../../../Module04)

## Įvadas

Microsoft Olive yra galingas ir lengvai naudojamas įrankis, skirtas modelių optimizavimui, atsižvelgiant į aparatinę įrangą. Jis supaprastina mašininio mokymosi modelių optimizavimo procesą, kad jie būtų pritaikyti įvairioms aparatinės įrangos platformoms. Nesvarbu, ar taikote procesoriams (CPU), grafikos procesoriams (GPU), ar specializuotiems AI akceleratoriams, Olive padeda pasiekti optimalų našumą, išlaikant modelio tikslumą.

## Kas yra Microsoft Olive?

Olive yra lengvai naudojamas įrankis, skirtas modelių optimizavimui, atsižvelgiant į aparatinę įrangą. Jis apjungia pažangiausias technologijas modelių suspaudimo, optimizavimo ir kompiliavimo srityse. Olive veikia kartu su ONNX Runtime kaip pilnas sprendimas inferencijos optimizavimui.

### Pagrindinės savybės

- **Optimizavimas pagal aparatinę įrangą**: Automatiškai parenka geriausius optimizavimo metodus jūsų tikslinės aparatinės įrangos atžvilgiu
- **40+ įmontuotų optimizavimo komponentų**: Apima modelių suspaudimą, kvantizaciją, grafų optimizavimą ir dar daugiau
- **Paprasta CLI sąsaja**: Lengvi komandų įvedimai dažniausiai naudojamiems optimizavimo užduotims
- **Daugiarėmė parama**: Veikia su PyTorch, Hugging Face modeliais ir ONNX
- **Populiarių modelių palaikymas**: Olive automatiškai optimizuoja populiarias modelių architektūras, tokias kaip Llama, Phi, Qwen, Gemma ir kt.

### Privalumai

- **Sutrumpintas kūrimo laikas**: Nebereikia rankiniu būdu eksperimentuoti su skirtingais optimizavimo metodais
- **Našumo padidėjimas**: Žymiai greitesnis veikimas (kai kuriais atvejais iki 6 kartų)
- **Kryžminė platformų diegimo galimybė**: Optimizuoti modeliai veikia skirtingose aparatinės įrangos ir operacinių sistemų platformose
- **Išlaikytas tikslumas**: Optimizacijos išlaiko modelio kokybę, tuo pačiu gerindamos našumą

## Įdiegimas

### Būtinos sąlygos

- Python 3.8 ar naujesnė versija
- pip paketų tvarkyklė
- Virtuali aplinka (rekomenduojama)

### Pagrindinis įdiegimas

Sukurkite ir aktyvuokite virtualią aplinką:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Įdiekite Olive su automatinio optimizavimo funkcijomis:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Pasirenkamos priklausomybės

Olive siūlo įvairias pasirenkamas priklausomybes papildomoms funkcijoms:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Įdiegimo patikrinimas

```bash
olive --help
```

Jei įdiegimas sėkmingas, turėtumėte matyti Olive CLI pagalbos pranešimą.

## Greito starto vadovas

### Pirmasis optimizavimas

Optimizuokime mažą kalbos modelį naudodami Olive automatinio optimizavimo funkciją:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ką daro ši komanda

Optimizavimo procesas apima: modelio gavimą iš vietinio talpyklos, ONNX grafų užfiksavimą ir svorių saugojimą ONNX duomenų faile, grafų optimizavimą ir modelio kvantizavimą į int4 naudojant RTN metodą.

### Komandos parametrų paaiškinimas

- `--model_name_or_path`: Hugging Face modelio identifikatorius arba vietinis kelias
- `--output_path`: Katalogas, kuriame bus išsaugotas optimizuotas modelis
- `--device`: Tikslinis įrenginys (cpu, gpu)
- `--provider`: Vykdymo tiekėjas (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Naudoti ONNX Runtime Generate AI inferencijai
- `--precision`: Kvantizacijos tikslumas (int4, int8, fp16)
- `--log_level`: Žurnalų detalumo lygis (0=minimalus, 1=detalus)

## Pavyzdys: Qwen3 konvertavimas į ONNX INT4

Remiantis pateiktu Hugging Face pavyzdžiu [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), štai kaip optimizuoti Qwen3 modelį:

### 1 žingsnis: Atsisiųskite modelį (pasirinktinai)

Norėdami sumažinti atsisiuntimo laiką, talpinkite tik būtinus failus:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 2 žingsnis: Optimizuokite Qwen3 modelį

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 3 žingsnis: Išbandykite optimizuotą modelį

Sukurkite paprastą Python scenarijų, kad išbandytumėte optimizuotą modelį:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Išvesties struktūra

Po optimizavimo jūsų išvesties kataloge bus:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Išplėstinis naudojimas

### Konfigūracijos failai

Sudėtingesniems optimizavimo procesams galite naudoti JSON konfigūracijos failus:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Vykdykite su konfigūracija:

```bash
olive run --config config.json
```

### GPU optimizavimas

CUDA GPU optimizavimui:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) optimizavimui:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Modelių pritaikymas su Olive

Olive taip pat palaiko modelių pritaikymą:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Geriausia praktika

### 1. Modelio pasirinkimas
- Pradėkite nuo mažesnių modelių testavimui (pvz., 0.5B-7B parametrų)
- Įsitikinkite, kad jūsų tikslinė modelio architektūra yra palaikoma Olive

### 2. Aparatinės įrangos apsvarstymai
- Suderinkite optimizavimo tikslą su diegimo aparatine įranga
- Naudokite GPU optimizavimą, jei turite CUDA suderinamą aparatinę įrangą
- Apsvarstykite DirectML Windows kompiuteriams su integruota grafika

### 3. Tikslumo pasirinkimas
- **INT4**: Maksimalus suspaudimas, nedidelis tikslumo praradimas
- **INT8**: Geras dydžio ir tikslumo balansas
- **FP16**: Minimalus tikslumo praradimas, vidutinis dydžio sumažinimas

### 4. Testavimas ir validacija
- Visada testuokite optimizuotus modelius pagal savo specifinius naudojimo atvejus
- Palyginkite našumo metrikas (vėlavimą, pralaidumą, tikslumą)
- Naudokite reprezentatyvius įvesties duomenis vertinimui

### 5. Iteracinis optimizavimas
- Pradėkite nuo automatinio optimizavimo greitiems rezultatams
- Naudokite konfigūracijos failus detaliam valdymui
- Eksperimentuokite su skirtingais optimizavimo etapais

## Trikčių šalinimas

### Dažnos problemos

#### 1. Įdiegimo problemos
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU problemos
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Atminties problemos
- Naudokite mažesnius partijų dydžius optimizavimo metu
- Pabandykite kvantizaciją su aukštesniu tikslumu (int8 vietoj int4)
- Įsitikinkite, kad turite pakankamai disko vietos modelių talpinimui

#### 4. Modelio įkėlimo klaidos
- Patikrinkite modelio kelią ir prieigos leidimus
- Patikrinkite, ar modelis reikalauja `trust_remote_code=True`
- Įsitikinkite, kad visi būtini modelio failai yra atsisiųsti

### Pagalbos gavimas

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub problemos**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Pavyzdžiai**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Papildomi ištekliai

### Oficialios nuorodos
- **GitHub saugykla**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face pavyzdys**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Bendruomenės pavyzdžiai
- **Jupyter užrašų knygos**: Prieinamos Olive GitHub saugykloje
- **VS Code plėtinys**: AI Toolkit plėtinys naudoja Olive modelių optimizavimui
- **Tinklaraščio įrašai**: Microsoft Open Source tinklaraštyje pateikiami išsamūs Olive vadovai

### Susiję įrankiai
- **ONNX Runtime**: Aukštos kokybės inferencijos variklis
- **Hugging Face Transformers**: Daugelio suderinamų modelių šaltinis
- **Azure Machine Learning**: Debesų pagrindu veikiantys optimizavimo procesai

## ➡️ Kas toliau

- [04: OpenVINO Toolkit optimizavimo rinkinys](./04.openvino.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.