<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:30:27+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "lt"
}
-->
# 4 skyrius: OpenVINO įrankių rinkinys optimizavimui

## Turinys
1. [Įvadas](../../../Module04)
2. [Kas yra OpenVINO?](../../../Module04)
3. [Įdiegimas](../../../Module04)
4. [Greito starto vadovas](../../../Module04)
5. [Pavyzdys: modelių konvertavimas ir optimizavimas naudojant OpenVINO](../../../Module04)
6. [Išplėstinis naudojimas](../../../Module04)
7. [Geriausios praktikos](../../../Module04)
8. [Trikčių šalinimas](../../../Module04)
9. [Papildomi ištekliai](../../../Module04)

## Įvadas

OpenVINO (Open Visual Inference and Neural Network Optimization) yra Intel atvirojo kodo įrankių rinkinys, skirtas efektyviam dirbtinio intelekto sprendimų diegimui debesyje, vietiniuose serveriuose ir kraštiniuose įrenginiuose. Nesvarbu, ar taikote CPU, GPU, VPU ar specializuotus AI akceleratorius, OpenVINO siūlo išsamias optimizavimo galimybes, išlaikant modelio tikslumą ir leidžiant diegti sprendimus įvairiose platformose.

## Kas yra OpenVINO?

OpenVINO yra atvirojo kodo įrankių rinkinys, leidžiantis kūrėjams efektyviai optimizuoti, konvertuoti ir diegti AI modelius įvairiose aparatinės įrangos platformose. Jį sudaro trys pagrindiniai komponentai: OpenVINO Runtime (inference), Neural Network Compression Framework (NNCF) modelių optimizavimui ir OpenVINO Model Server mastelio diegimui.

### Pagrindinės savybės

- **Diegimas įvairiose platformose**: Palaiko Linux, Windows ir macOS su Python, C++ ir C API
- **Aparatinės įrangos akceleracija**: Automatinis įrenginių aptikimas ir optimizavimas CPU, GPU, VPU ir AI akceleratoriams
- **Modelių suspaudimo sistema**: Pažangios kvantavimo, genėjimo ir optimizavimo technikos per NNCF
- **Suderinamumas su sistemomis**: Tiesioginis TensorFlow, ONNX, PaddlePaddle ir PyTorch modelių palaikymas
- **Generatyvinis AI palaikymas**: Specializuotas OpenVINO GenAI didelių kalbos modelių ir generatyvinio AI programų diegimui

### Privalumai

- **Našumo optimizavimas**: Žymiai greitesnis veikimas su minimaliais tikslumo praradimais
- **Mažesnis diegimo pėdsakas**: Minimalios išorinės priklausomybės supaprastina įdiegimą ir naudojimą
- **Greitesnis paleidimo laikas**: Optimizuotas modelių įkėlimas ir talpyklos naudojimas greitesniam programų paleidimui
- **Mastelio diegimas**: Nuo kraštinių įrenginių iki debesų infrastruktūros su nuosekliais API
- **Paruoštas gamybai**: Patikimumas verslo lygmeniu su išsamia dokumentacija ir bendruomenės palaikymu

## Įdiegimas

### Būtinos sąlygos

- Python 3.8 ar naujesnė versija
- pip paketų tvarkyklė
- Virtuali aplinka (rekomenduojama)
- Suderinama aparatinė įranga (rekomenduojami Intel CPU, bet palaikomos įvairios architektūros)

### Pagrindinis įdiegimas

Sukurkite ir aktyvuokite virtualią aplinką:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Įdiekite OpenVINO Runtime:

```bash
pip install openvino
```

Įdiekite NNCF modelių optimizavimui:

```bash
pip install nncf
```

### OpenVINO GenAI įdiegimas

Generatyvinio AI programoms:

```bash
pip install openvino-genai
```

### Pasirenkamos priklausomybės

Papildomi paketai specifiniams naudojimo atvejams:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Įdiegimo patikrinimas

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Jei sėkminga, turėtumėte matyti OpenVINO versijos informaciją.

## Greito starto vadovas

### Pirmasis modelio optimizavimas

Konvertuokime ir optimizuokime Hugging Face modelį naudojant OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Ką daro šis procesas

Optimizavimo procesas apima: pradinio modelio įkėlimą iš Hugging Face, konvertavimą į OpenVINO Intermediate Representation (IR) formatą, numatytų optimizacijų taikymą ir kompiliavimą tikslinei aparatinės įrangos platformai.

### Pagrindinių parametrų paaiškinimas

- `export=True`: Konvertuoja modelį į OpenVINO IR formatą
- `compile=False`: Atideda kompiliavimą iki vykdymo laiko, suteikiant lankstumo
- `device`: Tikslinė aparatinė įranga ("CPU", "GPU", "AUTO" automatinis pasirinkimas)
- `save_pretrained()`: Išsaugo optimizuotą modelį pakartotiniam naudojimui

## Pavyzdys: modelių konvertavimas ir optimizavimas naudojant OpenVINO

### 1 žingsnis: modelio konvertavimas su NNCF kvantavimu

Kaip taikyti kvantavimą po mokymo naudojant NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### 2 žingsnis: pažangi optimizacija su svorio suspaudimu

Transformerio tipo modeliams taikykite svorio suspaudimą:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### 3 žingsnis: inferencija su optimizuotu modeliu

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Išvesties struktūra

Po optimizavimo jūsų modelio kataloge bus:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Išplėstinis naudojimas

### Konfigūracija su NNCF YAML

Sudėtingiems optimizavimo procesams naudokite NNCF konfigūracijos failus:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Taikykite konfigūraciją:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU optimizavimas

GPU akceleracijai:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Partijų apdorojimo optimizavimas

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modelio serverio diegimas

Diekite optimizuotus modelius su OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Kliento kodas modelio serveriui:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Geriausios praktikos

### 1. Modelio pasirinkimas ir paruošimas
- Naudokite modelius iš palaikomų sistemų (PyTorch, TensorFlow, ONNX)
- Užtikrinkite, kad modelio įvestys turėtų fiksuotas arba žinomas dinamiškas formas
- Testuokite su reprezentatyviais duomenų rinkiniais kalibravimui

### 2. Optimizavimo strategijos pasirinkimas
- **Kvantavimas po mokymo**: Pradėkite nuo šio greitam optimizavimui
- **Svorio suspaudimas**: Tinka dideliems kalbos modeliams ir transformeriams
- **Kvantavimo sąmoningas mokymas**: Naudokite, kai tikslumas yra kritinis

### 3. Aparatinės įrangos specifinis optimizavimas
- **CPU**: Naudokite INT8 kvantavimą subalansuotam našumui
- **GPU**: Pasinaudokite FP16 tikslumu ir partijų apdorojimu
- **VPU**: Susitelkite į modelio supaprastinimą ir sluoksnių sujungimą

### 4. Našumo derinimas
- **Perdavimo režimas**: Didelio kiekio partijų apdorojimui
- **Vėlinimo režimas**: Realaus laiko interaktyvioms programoms
- **AUTO įrenginys**: Leiskite OpenVINO pasirinkti optimalią aparatinę įrangą

### 5. Atminties valdymas
- Naudokite dinamiškas formas atsargiai, kad išvengtumėte atminties pertekliaus
- Įgyvendinkite modelio talpyklą greitesniam pakartotiniam įkėlimui
- Stebėkite atminties naudojimą optimizavimo metu

### 6. Tikslumo patikrinimas
- Visada patikrinkite optimizuotus modelius pagal pradinį našumą
- Naudokite reprezentatyvius testų duomenų rinkinius vertinimui
- Apsvarstykite laipsnišką optimizavimą (pradėkite nuo konservatyvių nustatymų)

## Trikčių šalinimas

### Dažnos problemos

#### 1. Įdiegimo problemos
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modelio konvertavimo klaidos
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Našumo problemos
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Atminties problemos
- Sumažinkite modelio partijos dydį optimizavimo metu
- Naudokite srautą dideliems duomenų rinkiniams
- Įgalinkite modelio talpyklą: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Tikslumo pablogėjimas
- Naudokite aukštesnį tikslumą (INT8 vietoj INT4)
- Padidinkite kalibravimo duomenų rinkinio dydį
- Taikykite mišraus tikslumo optimizavimą

### Našumo stebėjimas

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Pagalbos gavimas

- **Dokumentacija**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub problemos**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Bendruomenės forumas**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Papildomi ištekliai

### Oficialios nuorodos
- **OpenVINO pagrindinis puslapis**: [openvino.ai](https://openvino.ai/)
- **GitHub saugykla**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF saugykla**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Modelių zoologijos sodas**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Mokymosi ištekliai
- **OpenVINO užrašų knygelės**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Greito starto vadovas**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimizavimo vadovas**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integracijos įrankiai
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Našumo etalonai
- **Oficialūs etalonai**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF modelių zoologijos sodas**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Bendruomenės pavyzdžiai
- **Jupyter užrašų knygelės**: [OpenVINO užrašų knygelių saugykla](https://github.com/openvinotoolkit/openvino_notebooks) - Išsamūs mokymai OpenVINO užrašų knygelių saugykloje
- **Pavyzdinės programos**: [OpenVINO modelių zoologijos sodas](https://github.com/openvinotoolkit/open_model_zoo) - Tikrojo pasaulio pavyzdžiai įvairioms sritims (kompiuterinė vizija, NLP, garsas)
- **Tinklaraščio įrašai**: [Intel AI tinklaraštis](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI ir bendruomenės tinklaraščio įrašai su išsamiais naudojimo atvejais

### Susiję įrankiai
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Papildomos optimizavimo technikos Intel aparatinės įrangos naudotojams
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Mobiliojo ir kraštinio diegimo palyginimui
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternatyvus kryžminės platformos inferencijos variklis

## ➡️ Kas toliau

- [05: Apple MLX Framework išsamus vadovas](./05.AppleMLX.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.