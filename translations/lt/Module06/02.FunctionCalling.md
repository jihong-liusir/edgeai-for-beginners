<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8b05633cf46af274b3724ee2f7140100",
  "translation_date": "2025-09-18T23:36:11+00:00",
  "source_file": "Module06/02.FunctionCalling.md",
  "language_code": "lt"
}
-->
# Section02 : Funkcijų iškvietimas mažų kalbos modelių (SLM) kontekste

## Turinys
1. [Kas yra funkcijų iškvietimas?](../../../Module06)
2. [Kaip veikia funkcijų iškvietimas](../../../Module06)
3. [Taikymo scenarijai](../../../Module06)
4. [Funkcijų iškvietimo nustatymas su Phi-4-mini ir Ollama](../../../Module06)
5. [Darbas su Qwen3 funkcijų iškvietimu](../../../Module06)
6. [Foundry vietinė integracija](../../../Module06)
7. [Geriausia praktika ir trikčių šalinimas](../../../Module06)
8. [Pažangūs pavyzdžiai](../../../Module06)

## Kas yra funkcijų iškvietimas?

Funkcijų iškvietimas yra galinga galimybė, leidžianti mažiems kalbos modeliams (SLM) sąveikauti su išoriniais įrankiais, API ir paslaugomis. Vietoj to, kad būtų apriboti savo mokymo duomenimis, SLM dabar gali:

- **Prisijungti prie išorinių API** (orų paslaugos, duomenų bazės, paieškos varikliai)
- **Vykdyti specifines funkcijas** pagal vartotojo užklausas
- **Gauti realaus laiko informaciją** iš įvairių šaltinių
- **Atlikti skaičiavimo užduotis** naudojant specializuotus įrankius
- **Sujungti kelias operacijas** sudėtingiems darbo procesams

Ši galimybė transformuoja SLM iš statinių teksto generatorių į dinamiškus AI agentus, galinčius atlikti realaus pasaulio užduotis.

## Kaip veikia funkcijų iškvietimas

Funkcijų iškvietimo procesas vyksta pagal sistemingą darbo eigą:

### 1. Įrankių integracija
- **Išoriniai įrankiai**: SLM gali prisijungti prie orų API, duomenų bazių, interneto paslaugų ir kitų išorinių sistemų
- **Funkcijų apibrėžimai**: Kiekvienas įrankis apibrėžiamas su specifiniais parametrais, įvesties/išvesties formatais ir aprašymais
- **API suderinamumas**: Įrankiai integruojami per standartizuotas sąsajas (REST API, SDK ir kt.)

### 2. Funkcijų apibrėžimas
Funkcijos apibrėžiamos trimis pagrindiniais komponentais:
```json
{
  "name": "function_name",
  "description": "Clear description of what the function does",
  "parameters": {
    "parameter_name": {
      "description": "What this parameter represents",
      "type": "data_type",
      "default": "default_value"
    }
  }
}
```

### 3. Ketinimų nustatymas
- **Natūralios kalbos apdorojimas**: SLM analizuoja vartotojo įvestį, kad suprastų ketinimą
- **Funkcijų atitikimas**: Nustatoma, kurios funkcijos reikalingos užklausai įvykdyti
- **Parametrų ištraukimas**: Identifikuojami ir ištraukiami reikalingi parametrai iš vartotojo pranešimo

### 4. JSON išvesties generavimas
SLM generuoja struktūrizuotą JSON, kuriame yra:
- Funkcijos pavadinimas, kurią reikia iškviesti
- Reikalingi parametrai su tinkamomis reikšmėmis
- Vykdymo kontekstas ir metaduomenys

### 5. Išorinis vykdymas
- **Parametrų patikrinimas**: Užtikrinama, kad visi reikalingi parametrai yra pateikti ir tinkamai suformatuoti
- **Funkcijos vykdymas**: Programa vykdo nurodytą funkciją su pateiktais parametrais
- **Klaidų tvarkymas**: Valdomi gedimai, laiko limitai ir neteisingi atsakymai

### 6. Atsakymo integracija
- **Rezultatų apdorojimas**: Funkcijos išvestis grąžinama SLM
- **Konteksto integracija**: SLM įtraukia rezultatus į savo atsakymą
- **Vartotojo komunikacija**: Informacija pateikiama natūraliu, pokalbio formatu

## Taikymo scenarijai

### Duomenų gavimas
Paversti natūralios kalbos užklausas į struktūrizuotus API skambučius:
- **"Parodyk mano naujausius užsakymus"** → Duomenų bazės užklausa su vartotojo ID ir datų filtrais
- **"Koks oras Tokijuje?"** → Orų API skambutis su vietos parametru
- **"Rask laiškus nuo Jono praėjusią savaitę"** → El. pašto paslaugos užklausa su siuntėjo ir datų filtrais

### Operacijų vykdymas
Paversti vartotojo užklausas į specifinius funkcijų skambučius:
- **"Suplanuok susitikimą rytoj 14 val."** → Kalendoriaus API integracija
- **"Išsiųsk žinutę komandai"** → Komunikacijos platformos API
- **"Sukurk mano failų atsarginę kopiją"** → Failų sistemos operacija

### Skaičiavimo užduotys
Atlikti sudėtingas matematines ar logines operacijas:
- **"Apskaičiuok sudėtinę palūkaną už $10,000 su 5% per 10 metų"** → Finansinių skaičiavimų funkcija
- **"Analizuok šį duomenų rinkinį dėl tendencijų"** → Statistinės analizės įrankiai
- **"Optimizuok šį maršrutą pristatymui"** → Maršruto optimizavimo algoritmai

### Duomenų apdorojimo darbo eigos
Sujungti kelis funkcijų skambučius sudėtingoms operacijoms:
1. **Gauti duomenis** iš kelių šaltinių
2. **Analizuoti ir patikrinti** informaciją
3. **Transformuoti** duomenis į reikiamą formatą
4. **Saugojimo rezultatai** tinkamose sistemose
5. **Generuoti ataskaitas** ar vizualizacijas

### UI/UX integracija
Leisti dinamiškus sąsajos atnaujinimus:
- **"Parodyk pardavimų duomenis prietaisų skydelyje"** → Diagramų generavimas ir rodymas
- **"Atnaujink žemėlapį su naujomis vietomis"** → Geografinės duomenų integracija
- **"Atnaujink inventoriaus rodymą"** → Realaus laiko duomenų sinchronizacija

## Funkcijų iškvietimo nustatymas su Phi-4-mini ir Ollama

Microsoft Phi-4-mini palaiko tiek vieno, tiek lygiagretaus funkcijų iškvietimo galimybes per Ollama. Štai kaip tai nustatyti:

### Būtinos sąlygos
- Ollama versija 0.5.13 ar naujesnė
- Phi-4-mini modelis (rekomenduojama: `phi4-mini:3.8b-fp16`)

### Diegimo žingsniai

#### 1. Įdiegti ir paleisti Phi-4-mini
```bash
# Download the model (if not already present)
ollama run phi4-mini:3.8b-fp16

# Verify the model is available
ollama list
```

#### 2. Sukurti pritaikytą ModelFile šabloną
Dėl dabartinių Ollama numatytųjų šablonų apribojimų reikia sukurti pritaikytą ModelFile su šiuo šablonu:

```modelfile
TEMPLATE """
{{- if .Messages }}
{{- if or .System .Tools }}<|system|>
{{ if .System }}{{ .System }}
{{- end }}
In addition to plain text responses, you can chose to call one or more of the provided functions.
Use the following rule to decide when to call a function:
* if the response can be generated from your internal knowledge (e.g., as in the case of queries like "What is the capital of Poland?"), do so
* if you need external information that can be obtained by calling one or more of the provided functions, generate a function calls
If you decide to call functions:
* prefix function calls with functools marker (no closing marker required)
* all function calls should be generated in a single JSON list formatted as functools[{"name": [function name], "arguments": [function arguments as JSON]}, ...]
* follow the provided JSON schema. Do not hallucinate arguments or values. Do to blindly copy values from the provided samples
* respect the argument type formatting. E.g., if the type if number and format is float, write value 7 as 7.0
* make sure you pick the right functions that match the user intent
Available functions as JSON spec:
{{- if .Tools }}
{{ .Tools }}
{{- end }}<|end|>
{{- end }}
{{- range .Messages }}
{{- if ne .Role "system" }}<|{{ .Role }}|>
{{- if and .Content (eq .Role "tools") }}
{"result": {{ .Content }}}
{{- else if .Content }}
{{ .Content }}
{{- else if .ToolCalls }}
functools[
{{- range .ToolCalls }}{{ "{" }}"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}{{ "}" }}
{{- end }}]
{{- end }}<|end|>
{{- end }}
{{- end }}<|assistant|>
{{ else }}
{{- if .System }}<|system|>
{{ .System }}<|end|>{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>{{ end }}<|assistant|>
{{ end }}{{ .Response }}{{ if .Response }}<|user|>{{ end }}
"""
```

#### 3. Sukurti pritaikytą modelį
```bash
# Save the template above as 'Modelfile' and run:
ollama create phi4-mini-fc:3.8b-fp16 -f ./Modelfile
```

### Vieno funkcijų iškvietimo pavyzdys

```python
import json
import requests

# Define the tool/function
tools = [
    {
        "name": "get_weather",
        "description": "Get current weather information for a location",
        "parameters": {
            "location": {
                "description": "The city or location name",
                "type": "str",
                "default": "New York"
            },
            "units": {
                "description": "Temperature units (celsius or fahrenheit)",
                "type": "str",
                "default": "celsius"
            }
        }
    }
]

# Create the message with system prompt including tools
messages = [
    {
        "role": "system",
        "content": "You are a helpful weather assistant",
        "tools": json.dumps(tools)
    },
    {
        "role": "user",
        "content": "What's the weather like in London today?"
    }
]

# Make request to Ollama API
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "phi4-mini-fc:3.8b-fp16",
        "messages": messages,
        "stream": False
    }
)

print(response.json())
```

### Lygiagretaus funkcijų iškvietimo pavyzdys

```python
import json
import requests

# Define multiple tools for parallel execution
AGENT_TOOLS = {
    "booking_flight": {
        "name": "booking_flight",
        "description": "Book a flight ticket",
        "parameters": {
            "departure": {
                "description": "Departure airport code",
                "type": "str"
            },
            "destination": {
                "description": "Destination airport code", 
                "type": "str"
            },
            "outbound_date": {
                "description": "Departure date (YYYY-MM-DD)",
                "type": "str"
            },
            "return_date": {
                "description": "Return date (YYYY-MM-DD)",
                "type": "str"
            }
        }
    },
    "booking_hotel": {
        "name": "booking_hotel",
        "description": "Book a hotel room",
        "parameters": {
            "city": {
                "description": "City name for hotel booking",
                "type": "str"
            },
            "check_in_date": {
                "description": "Check-in date (YYYY-MM-DD)",
                "type": "str"
            },
            "check_out_date": {
                "description": "Check-out date (YYYY-MM-DD)",
                "type": "str"
            }
        }
    }
}

SYSTEM_PROMPT = """
You are my travel agent with some tools available.
"""

messages = [
    {
        "role": "system",
        "content": SYSTEM_PROMPT,
        "tools": json.dumps(AGENT_TOOLS)
    },
    {
        "role": "user", 
        "content": "I need to travel from London to New York from March 21 2025 to March 27 2025. Please book both flight and hotel."
    }
]

# The model will generate parallel function calls
response = requests.post(
    "http://localhost:11434/api/chat",
    json={
        "model": "phi4-mini-fc:3.8b-fp16",
        "messages": messages,
        "stream": False
    }
)

print(response.json())
```

## Darbas su Qwen3 funkcijų iškvietimu

Qwen3 siūlo pažangias funkcijų iškvietimo galimybes su puikiu našumu ir lankstumu. Štai kaip tai įgyvendinti:

### Naudojant Qwen-Agent sistemą

Qwen-Agent suteikia aukšto lygio sistemą, kuri supaprastina funkcijų iškvietimo įgyvendinimą:

#### Diegimas
```bash
pip install -U "qwen-agent[gui,rag,code_interpreter,mcp]"
```

#### Pagrindinis nustatymas

```python
import os
from qwen_agent.agents import Assistant

# Configure the LLM
llm_cfg = {
    'model': 'Qwen3-8B',
    # Option 1: Use Alibaba Model Studio
    'model_type': 'qwen_dashscope',
    'api_key': os.getenv('DASHSCOPE_API_KEY'),
    
    # Option 2: Use local deployment
    # 'model_server': 'http://localhost:8000/v1',
    # 'api_key': 'EMPTY',
    
    # Optional configuration for thinking mode
    'generate_cfg': {
        'thought_in_content': True,  # Include reasoning in response
    }
}

# Define tools using MCP (Model Context Protocol)
tools = [
    {
        'mcpServers': {
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            'fetch': {
                'command': 'uvx', 
                'args': ['mcp-server-fetch']
            }
        }
    },
    'code_interpreter',  # Built-in code execution tool
]

# Create the assistant
bot = Assistant(llm=llm_cfg, function_list=tools)

# Example usage
messages = [
    {
        'role': 'user', 
        'content': 'What time is it now? Also, fetch the latest news from https://example.com/news'
    }
]

# Generate response with function calling
for response in bot.run(messages=messages):
    print(response)
```

### Pritaikytų funkcijų įgyvendinimas

Taip pat galite apibrėžti pritaikytas funkcijas Qwen3:

```python
import json
from qwen_agent.tools.base import BaseTool

class WeatherTool(BaseTool):
    description = 'Get weather information for a specific location'
    parameters = [
        {
            'name': 'location',
            'type': 'string', 
            'description': 'City or location name',
            'required': True
        },
        {
            'name': 'units',
            'type': 'string',
            'description': 'Temperature units (celsius or fahrenheit)',
            'required': False,
            'default': 'celsius'
        }
    ]
    
    def call(self, params: str, **kwargs) -> str:
        """Execute the weather lookup"""
        params_dict = json.loads(params)
        location = params_dict.get('location')
        units = params_dict.get('units', 'celsius')
        
        # Simulate weather API call
        weather_data = {
            'location': location,
            'temperature': '22°C' if units == 'celsius' else '72°F',
            'condition': 'Partly cloudy',
            'humidity': '65%'
        }
        
        return json.dumps(weather_data)

# Use the custom tool
tools = [WeatherTool()]
bot = Assistant(llm=llm_cfg, function_list=tools)

messages = [{'role': 'user', 'content': 'What\'s the weather in Tokyo?'}]
response = bot.run(messages=messages)
print(list(response)[-1])
```

### Pažangios Qwen3 funkcijos

#### Mąstymo režimo valdymas
Qwen3 palaiko dinaminį perjungimą tarp mąstymo ir nemąstymo režimų:

```python
# Enable thinking mode for complex reasoning
messages = [
    {
        'role': 'user',
        'content': '/think Solve this complex math problem: If a train travels 120 km in 1.5 hours, and another train travels 200 km in 2.5 hours, which train is faster and by how much?'
    }
]

# Disable thinking mode for simple queries
messages = [
    {
        'role': 'user', 
        'content': '/no_think What is the capital of France?'
    }
]
```

#### Daugiapakopis funkcijų iškvietimas
Qwen3 puikiai tinka sujungti kelis funkcijų skambučius:

```python
# Complex workflow example
messages = [
    {
        'role': 'user',
        'content': '''
        I need to prepare for a business meeting:
        1. Check my calendar for conflicts tomorrow
        2. Get weather forecast for the meeting location (San Francisco)
        3. Find recent news about the client company (TechCorp)
        4. Calculate travel time from my office to their headquarters
        '''
    }
]

# Qwen3 will automatically determine the sequence of function calls needed
```

## Foundry vietinė integracija

Microsoft Foundry Local suteikia OpenAI suderinamą API, skirtą modeliams paleisti vietoje su patobulintu privatumu ir našumu.

### Nustatymas ir diegimas

#### Windows
Atsisiųskite diegimo programą iš [Foundry Local išleidimų puslapio](https://github.com/microsoft/Foundry-Local/releases) ir vykdykite diegimo instrukcijas.

#### macOS
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

#### Pagrindinis naudojimas

```python
import openai
from foundry_local import FoundryLocalManager

# Initialize with model alias
alias = "phi-3.5-mini"  # Or any supported model
manager = FoundryLocalManager(alias)

# Create OpenAI client pointing to local endpoint
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key
)

# Define functions for the model
functions = [
    {
        "name": "calculate_tax",
        "description": "Calculate tax amount based on income and rate",
        "parameters": {
            "type": "object",
            "properties": {
                "income": {
                    "type": "number",
                    "description": "Annual income amount"
                },
                "tax_rate": {
                    "type": "number", 
                    "description": "Tax rate as decimal (e.g., 0.25 for 25%)"
                }
            },
            "required": ["income", "tax_rate"]
        }
    }
]

# Make function calling request
response = client.chat.completions.create(
    model=manager.model_info.id,
    messages=[
        {
            "role": "user",
            "content": "Calculate the tax for someone earning $75,000 with a 22% tax rate"
        }
    ],
    functions=functions,
    function_call="auto"
)

print(response.choices[0].message.content)
```

### Pažangios Foundry vietinės funkcijos

#### Modelių valdymas
```bash
# List available models
foundry model list

# Download specific model
foundry model download phi-3.5-mini

# Run model interactively
foundry model run phi-3.5-mini

# Remove model from cache
foundry model remove phi-3.5-mini

# Delete all cached models
foundry model remove "*"
```

#### Našumo optimizavimas
Foundry Local automatiškai parenka geriausią modelio variantą jūsų aparatinei įrangai:
- **CUDA GPU**: Atsisiunčia GPU optimizuotus modelius
- **Qualcomm NPU**: Naudoja NPU pagreitintus variantus
- **Tik CPU**: Pasirenka CPU optimizuotus modelius

## Geriausia praktika ir trikčių šalinimas

### Funkcijų apibrėžimo geriausia praktika

#### 1. Aiškus ir aprašomas pavadinimas
```python
# Good
{
    "name": "get_stock_price",
    "description": "Retrieve current stock price for a given symbol"
}

# Avoid
{
    "name": "get_data", 
    "description": "Gets data"
}
```

#### 2. Išsamūs parametrų apibrėžimai
```python
{
    "name": "send_email",
    "description": "Send an email message to specified recipients",
    "parameters": {
        "to": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of recipient email addresses",
            "required": True
        },
        "subject": {
            "type": "string",
            "description": "Email subject line",
            "required": True
        },
        "body": {
            "type": "string", 
            "description": "Email message content",
            "required": True
        },
        "priority": {
            "type": "string",
            "enum": ["low", "normal", "high"],
            "description": "Email priority level",
            "default": "normal",
            "required": False
        }
    }
}
```

#### 3. Įvesties patikrinimas ir klaidų tvarkymas
```python
def execute_function(function_name, parameters):
    try:
        # Validate required parameters
        if function_name == "send_email":
            if not parameters.get("to") or not parameters.get("subject"):
                return {"error": "Missing required parameters: to, subject"}
            
            # Validate email format
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            for email in parameters["to"]:
                if not re.match(email_pattern, email):
                    return {"error": f"Invalid email format: {email}"}
        
        # Execute function logic
        result = perform_actual_function(function_name, parameters)
        return {"success": True, "data": result}
        
    except Exception as e:
        return {"error": str(e)}
```

### Dažnos problemos ir sprendimai

#### Problema 1: Funkcija nėra iškviečiama
**Simptomai**: Modelis atsako tekstu, o ne iškviečia funkciją

**Sprendimai**:
1. **Patikrinkite funkcijos aprašymą**: Įsitikinkite, kad jis aiškiai atitinka vartotojo ketinimą
2. **Patikrinkite parametrų apibrėžimus**: Įsitikinkite, kad visi reikalingi parametrai yra tinkamai apibrėžti
3. **Peržiūrėkite sistemos raginimą**: Įtraukite aiškias instrukcijas, kada naudoti funkcijas
4. **Testuokite su aiškiomis užklausomis**: Pabandykite "Prašau naudoti orų funkciją, kad gautumėte duomenis apie Londoną"

#### Problema 2: Netinkami parametrai
**Simptomai**: Funkcija iškviečiama su neteisingais arba trūkstamais parametrais

**Sprendimai**:
1. **Pridėkite parametrų pavyzdžius**: Įtraukite pavyzdines reikšmes į parametrų aprašymus
2. **Naudokite enum apribojimus**: Apribokite parametrų reikšmes specifinėmis parinktimis, kai įmanoma
3. **Įgyvendinkite atsargines reikšmes**: Pateikite prasmingus numatytuosius parametrus neprivalomiems parametrams

```python
{
    "name": "book_restaurant",
    "parameters": {
        "cuisine": {
            "type": "string",
            "enum": ["italian", "chinese", "mexican", "american", "french"],
            "description": "Type of cuisine (example: 'italian' for Italian food)"
        },
        "party_size": {
            "type": "integer",
            "minimum": 1,
            "maximum": 20,
            "description": "Number of people (example: 4 for a family of four)"
        }
    }
}
```

#### Problema 3: Lygiagretaus funkcijų iškvietimo gedimai
**Simptomai**: Vykdoma tik viena funkcija, kai turėtų būti vykdomos kelios

**Sprendimai**:
1. **Patikrinkite modelio palaikymą**: Įsitikinkite, kad jūsų modelis palaiko lygiagretų funkcijų iškvietimą
2. **Atnaujinkite sistemos raginimą**: Įtraukite "kai kurie įrankiai" arba "kelios priemonės" į sistemos pranešimą
3. **Naudokite tinkamas modelio versijas**: Phi-4-mini:3.8b-fp16 rekomenduojama Ollama

#### Problema 4: Šablono problemos su Ollama
**Simptomai**: Funkcijų iškvietimas neveikia su numatytuoju Ollama nustatymu

**Sprendimai**:
1. **Naudokite pritaikytą ModelFile**: Taikykite pataisytą šabloną, pateiktą šiame vadove
2. **Atnaujinkite Ollama**: Įsitikinkite, kad naudojate versiją 0.5.13 ar naujesnę
3. **Patikrinkite modelio kvantizaciją**: Aukštesni kvantizacijos lygiai (Q8_0, fp16) veikia geriau nei stipriai kvantizuoti variantai

### Našumo optimizavimas

#### 1. Efektyvus funkcijų dizainas
- **Laikykite funkcijas aiškias**: Kiekviena funkcija turėtų turėti vieną, aiškų tikslą
- **Minimizuokite išorines priklausomybes**: Sumažinkite API skambučius ir tinklo užklausas, kur įmanoma
- **Kešuokite rezultatus**: Saugojimo dažnai prašomus duomenis, kad pagerintumėte atsako laiką

#### 2. Grupavimas ir asinchroninės operacijos
```python
import asyncio
import aiohttp

async def batch_function_calls(function_calls):
    """Execute multiple function calls concurrently"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for call in function_calls:
            if call["name"] == "fetch_url":
                task = fetch_url_async(session, call["parameters"]["url"])
                tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results

async def fetch_url_async(session, url):
    async with session.get(url) as response:
        return await response.text()
```

#### 3. Išteklių valdymas
- **Jungčių grupavimas**: Pakartotinai naudokite duomenų bazės ir API jungtis
- **Kainų ribojimas**: Įgyvendinkite tinkamą kainų ribojimą išorinėms API
- **Laiko limitų tvarkymas**: Nustatykite pagrįstus laiko limitus visiems išoriniams skambučiams

## Pažangūs pavyzdžiai

### Daugiagentė bendradarbiavimo sistema

```python
import json
from typing import List, Dict
from qwen_agent.agents import Assistant

class MultiAgentSystem:
    def __init__(self):
        # Research Agent
        self.research_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=[
                {'mcpServers': {'search': {'command': 'uvx', 'args': ['mcp-server-search']}}},
                {'mcpServers': {'fetch': {'command': 'uvx', 'args': ['mcp-server-fetch']}}}
            ]
        )
        
        # Analysis Agent
        self.analysis_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=['code_interpreter']
        )
        
        # Communication Agent
        self.comm_agent = Assistant(
            llm={'model': 'Qwen3-8B', 'model_server': 'http://localhost:8000/v1'},
            function_list=[self.create_email_tool(), self.create_slack_tool()]
        )
    
    def create_email_tool(self):
        """Custom email sending tool"""
        class EmailTool:
            name = "send_email"
            description = "Send email to specified recipients"
            parameters = {
                "to": {"type": "string", "description": "Recipient email"},
                "subject": {"type": "string", "description": "Email subject"},
                "body": {"type": "string", "description": "Email content"}
            }
            
            def call(self, params):
                # Implement actual email sending logic
                return f"Email sent successfully to {params['to']}"
        
        return EmailTool()
    
    def create_slack_tool(self):
        """Custom Slack messaging tool"""  
        class SlackTool:
            name = "send_slack"
            description = "Send message to Slack channel"
            parameters = {
                "channel": {"type": "string", "description": "Slack channel"},
                "message": {"type": "string", "description": "Message content"}
            }
            
            def call(self, params):
                # Implement actual Slack API call
                return f"Message sent to {params['channel']}"
        
        return SlackTool()
    
    async def process_complex_request(self, user_request: str):
        """Process complex multi-step requests using multiple agents"""
        
        # Step 1: Research phase
        research_prompt = f"Research the following topic and gather relevant information: {user_request}"
        research_results = []
        for response in self.research_agent.run([{'role': 'user', 'content': research_prompt}]):
            research_results.append(response)
        
        # Step 2: Analysis phase
        analysis_prompt = f"Analyze the following research data and provide insights: {research_results[-1]}"
        analysis_results = []
        for response in self.analysis_agent.run([{'role': 'user', 'content': analysis_prompt}]):
            analysis_results.append(response)
        
        # Step 3: Communication phase
        comm_prompt = f"Create a summary report and send it via email: {analysis_results[-1]}"
        comm_results = []
        for response in self.comm_agent.run([{'role': 'user', 'content': comm_prompt}]):
            comm_results.append(response)
        
        return {
            'research': research_results[-1],
            'analysis': analysis_results[-1], 
            'communication': comm_results[-1]
        }

# Usage example
async def main():
    system = MultiAgentSystem()
    
    request = """
    Analyze the impact of remote work on productivity in tech companies. 
    Research recent studies, analyze the data, and send a summary to our team.
    """
    
    results = await system.process_complex_request(request)
    print("Multi-agent processing complete:", results)

# Run the example
# asyncio.run(main())
```

### Dinaminė įrankių pasirinkimo sistema

```python
class DynamicToolSelector:
    def __init__(self):
        self.available_tools = {
            'weather': {
                'description': 'Get weather information',
                'domains': ['weather', 'temperature', 'forecast', 'climate'],
                'function': self.get_weather
            },
            'calculator': {
                'description': 'Perform mathematical calculations',
                'domains': ['math', 'calculate', 'compute', 'arithmetic'],
                'function': self.calculate
            },
            'web_search': {
                'description': 'Search the internet for information',
                'domains': ['search', 'find', 'lookup', 'research'],
                'function': self.web_search
            },
            'file_manager': {
                'description': 'Manage files and directories',
                'domains': ['file', 'directory', 'save', 'load', 'delete'],
                'function': self.manage_files
            }
        }
    
    def analyze_intent(self, user_input: str) -> List[str]:
        """Analyze user input to determine which tools might be needed"""
        user_words = user_input.lower().split()
        relevant_tools = []
        
        for tool_name, tool_info in self.available_tools.items():
            for domain in tool_info['domains']:
                if domain in user_words:
                    relevant_tools.append(tool_name)
                    break
        
        return relevant_tools
    
    def get_tool_definitions(self, tool_names: List[str]) -> List[Dict]:
        """Generate function definitions for selected tools"""
        definitions = []
        
        for tool_name in tool_names:
            if tool_name == 'weather':
                definitions.append({
                    'name': 'get_weather',
                    'description': 'Get current weather information',
                    'parameters': {
                        'location': {'type': 'string', 'description': 'City or location name'},
                        'units': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'default': 'celsius'}
                    }
                })
            elif tool_name == 'calculator':
                definitions.append({
                    'name': 'calculate',
                    'description': 'Perform mathematical calculations',
                    'parameters': {
                        'expression': {'type': 'string', 'description': 'Mathematical expression to evaluate'},
                        'precision': {'type': 'integer', 'default': 2, 'description': 'Decimal places for result'}
                    }
                })
            # Add more tool definitions as needed
        
        return definitions
    
    def get_weather(self, location: str, units: str = 'celsius') -> Dict:
        """Mock weather function"""
        return {
            'location': location,
            'temperature': '22°C' if units == 'celsius' else '72°F',
            'condition': 'Sunny',
            'humidity': '60%'
        }
    
    def calculate(self, expression: str, precision: int = 2) -> Dict:
        """Safe mathematical calculation"""
        try:
            # Simple evaluation for demo - in production, use a proper math parser
            import math
            allowed_names = {
                k: v for k, v in math.__dict__.items() if not k.startswith("__")
            }
            allowed_names.update({"abs": abs, "round": round})
            
            result = eval(expression, {"__builtins__": {}}, allowed_names)
            return {
                'expression': expression,
                'result': round(float(result), precision),
                'success': True
            }
        except Exception as e:
            return {
                'expression': expression,
                'error': str(e),
                'success': False
            }
    
    def web_search(self, query: str, max_results: int = 5) -> Dict:
        """Mock web search function"""
        return {
            'query': query,
            'results': [
                {'title': f'Result {i+1} for {query}', 'url': f'https://example{i+1}.com'}
                for i in range(max_results)
            ]
        }
    
    def manage_files(self, action: str, file_path: str, content: str = None) -> Dict:
        """Mock file management function"""
        return {
            'action': action,
            'file_path': file_path,
            'success': True,
            'message': f'Successfully {action}ed file: {file_path}'
        }

# Usage example
def smart_assistant_with_dynamic_tools():
    selector = DynamicToolSelector()
    
    user_requests = [
        "What's the weather like in New York and calculate 15% tip on $50?",
        "Search for recent AI developments and save the results to a file",
        "Calculate the area of a circle with radius 10 and check weather in Tokyo"
    ]
    
    for request in user_requests:
        print(f"\nUser Request: {request}")
        
        # Analyze which tools might be needed
        relevant_tools = selector.analyze_intent(request)
        print(f"Relevant Tools: {relevant_tools}")
        
        # Get function definitions for the LLM
        tool_definitions = selector.get_tool_definitions(relevant_tools)
        print(f"Tool Definitions: {len(tool_definitions)} functions available")
        
        # In a real implementation, you would pass these to your LLM
        # The LLM would then decide which functions to call and with what parameters

### Enterprise Integration Example

```
Please provide the markdown file content you'd like me to translate.
## Išvada

Funkcijų iškvietimas mažose kalbos modeliuose (SLM) žymi paradigmos pokytį nuo statinių AI asistentų iki dinamiškų, pajėgių agentų, galinčių sąveikauti su realiu pasauliu. Ši pamoka apėmė:

### Pagrindinės įžvalgos

1. **Pagrindų supratimas**: Funkcijų iškvietimas leidžia SLM išplėsti savo galimybes už mokymosi duomenų ribų, jungiantis prie išorinių įrankių ir paslaugų.

2. **Lankstumas įgyvendinant**: Yra įvairių požiūrių, nuo žemo lygio įgyvendinimų su pritaikytais šablonais iki aukšto lygio sistemų, tokių kaip Qwen-Agent ir Foundry Local.

3. **Produkcijos aspektai**: Įmonių diegimai reikalauja dėmesio klaidų tvarkymui, kvietimų ribojimui, saugumui ir audito žurnalams.

4. **Našumo optimizavimas**: Tinkamas funkcijų dizainas, efektyvus vykdymas ir protingas kešavimas gali žymiai pagerinti atsako laiką.

### Ateities kryptys

Kadangi SLM technologija toliau vystosi, galime tikėtis:

- **Patobulinto funkcijų iškvietimo tikslumo**: Geresnio ketinimų nustatymo ir parametrų ištraukimo.
- **Išplėstinio lygiagretaus apdorojimo**: Sudėtingesnės daugelio funkcijų orkestracijos.
- **Geresnių integracijos standartų**: Standartizuotų protokolų įrankių integracijai.
- **Pažangių saugumo funkcijų**: Patobulintos autentifikacijos ir autorizacijos mechanizmų.
- **Išplėstos ekosistemos**: Augančios iš anksto sukurtų funkcijų ir integracijų bibliotekos.

### Pradėti

Norėdami pradėti įgyvendinti funkcijų iškvietimą savo projektuose:

1. **Pradėkite paprastai**: Pradėkite nuo paprastų vienos funkcijos scenarijų.
2. **Pasirinkite savo sistemą**: Pasirinkite tarp tiesioginio įgyvendinimo (Ollama/Phi-4) arba sistemos pagalbos (Qwen-Agent).
3. **Kruopščiai kurkite funkcijas**: Sutelkite dėmesį į aiškias, gerai dokumentuotas funkcijų apibrėžtis.
4. **Įgyvendinkite klaidų tvarkymą**: Nuo pat pradžių kurkite patikimą klaidų tvarkymą.
5. **Palaipsniui plėskite**: Pereikite nuo paprastų prie sudėtingų scenarijų, kai įgysite patirties.

Funkcijų iškvietimas transformuoja SLM iš įspūdingų teksto generatorių į praktiškus AI agentus, galinčius spręsti realaus pasaulio problemas. Vadovaudamiesi šioje pamokoje pateiktais modeliais ir praktikomis, galite sukurti galingas, patikimas AI sistemas, kurios gerokai pranoksta tradicines pokalbių sąsajas.

### Ištekliai ir nuorodos
- **Phi-4 Modeliai**: [Hugging Face Kolekcija](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Qwen3 Dokumentacija**: [Oficiali Qwen Dokumentacija](https://qwen.readthedocs.io/)
- **Ollama**: [Oficiali Svetainė](https://ollama.com/)
- **Foundry Local**: [GitHub Saugykla](https://github.com/microsoft/Foundry-Local)
- **Geriausia Praktika Funkcijų Kvietimui**: [Hugging Face Gidas](https://huggingface.co/docs/hugs/en/guides/function-calling)

Atminkite, kad funkcijų kvietimas yra nuolat besivystanti sritis, todėl nuolatinis atnaujinimas apie naujausius pokyčius jūsų pasirinktuose karkasuose ir modeliuose padės kurti efektyvesnius dirbtinio intelekto agentus.


## ➡️ Kas toliau

- [03: Modelio Konteksto Protokolo (MCP) Integracija](./03.IntroduceMCP.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.