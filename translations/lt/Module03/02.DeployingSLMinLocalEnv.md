<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-19T01:35:30+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "lt"
}
-->
# 2 skyrius: Vietinio aplinkos diegimas – privatumo pirmumo sprendimai

Vietinis Mažų Kalbos Modelių (SLM) diegimas žymi paradigmos pokytį link privatumo išsaugojimo ir ekonomiškai efektyvių dirbtinio intelekto sprendimų. Ši išsami instrukcija nagrinėja du galingus pagrindus – Ollama ir Microsoft Foundry Local – kurie leidžia kūrėjams maksimaliai išnaudoti SLM galimybes, išlaikant visišką kontrolę savo diegimo aplinkoje.

## Įvadas

Šioje pamokoje nagrinėsime pažangias Mažų Kalbos Modelių diegimo strategijas vietinėse aplinkose. Aptarsime pagrindines vietinio dirbtinio intelekto diegimo koncepcijas, išanalizuosime dvi pirmaujančias platformas (Ollama ir Microsoft Foundry Local) ir pateiksime praktines gaires, kaip įgyvendinti sprendimus, paruoštus gamybai.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- Suprasti vietinių SLM diegimo pagrindų architektūrą ir privalumus.
- Įgyvendinti gamybai paruoštus diegimus naudojant Ollama ir Microsoft Foundry Local.
- Palyginti ir pasirinkti tinkamą platformą pagal specifinius reikalavimus ir apribojimus.
- Optimizuoti vietinius diegimus našumui, saugumui ir mastelio didinimui.

## Vietinių SLM diegimo architektūrų supratimas

Vietinis SLM diegimas žymi esminį pokytį nuo debesų priklausomų dirbtinio intelekto paslaugų link vietinių, privatumo išsaugojimo sprendimų. Šis požiūris leidžia organizacijoms išlaikyti visišką kontrolę savo dirbtinio intelekto infrastruktūrai, užtikrinant duomenų suverenitetą ir operacinę nepriklausomybę.

### Diegimo pagrindų klasifikacija

Skirtingų diegimo metodų supratimas padeda pasirinkti tinkamą strategiją konkretiems naudojimo atvejams:

- **Skirta kūrimui**: Supaprastintas nustatymas eksperimentavimui ir prototipų kūrimui.
- **Įmonės lygio**: Sprendimai, paruošti gamybai, su įmonės integracijos galimybėmis.
- **Kryžminė platforma**: Universalus suderinamumas su skirtingomis operacinėmis sistemomis ir aparatine įranga.

### Pagrindiniai vietinio SLM diegimo privalumai

Vietinis SLM diegimas siūlo keletą esminių privalumų, kurie daro jį idealų įmonėms ir privatumo jautrioms programoms:

**Privatumas ir saugumas**: Vietinis apdorojimas užtikrina, kad jautrūs duomenys niekada nepaliks organizacijos infrastruktūros, leidžiant laikytis GDPR, HIPAA ir kitų reguliavimo reikalavimų. Galimi izoliuoti diegimai klasifikuotoms aplinkoms, o visiškos audito ataskaitos užtikrina saugumo priežiūrą.

**Ekonomiškumas**: Atsisakymas kainodaros modelių už kiekvieną žodį žymiai sumažina veiklos išlaidas. Mažesni pralaidumo reikalavimai ir sumažinta priklausomybė nuo debesų suteikia nuspėjamą išlaidų struktūrą įmonės biudžetui.

**Našumas ir patikimumas**: Greitesni apdorojimo laikai be tinklo vėlavimo leidžia realaus laiko programoms. Neprisijungus veikianti funkcija užtikrina nenutrūkstamą veikimą nepriklausomai nuo interneto ryšio, o vietinių išteklių optimizavimas užtikrina nuoseklų našumą.

## Ollama: Universalus vietinio diegimo pagrindas

### Pagrindinė architektūra ir filosofija

Ollama sukurta kaip universali, kūrėjams draugiška platforma, kuri demokratizuoja vietinį LLM diegimą įvairiose aparatinės įrangos konfigūracijose ir operacinėse sistemose.

**Techninis pagrindas**: Sukurta ant patikimos llama.cpp struktūros, Ollama naudoja efektyvų GGUF modelio formatą optimaliam našumui. Kryžminė platformos suderinamumas užtikrina nuoseklų veikimą Windows, macOS ir Linux aplinkose, o intelektualus išteklių valdymas optimizuoja CPU, GPU ir atminties naudojimą.

**Dizaino filosofija**: Ollama teikia pirmenybę paprastumui, neaukojant funkcionalumo, siūlydama diegimą be konfigūracijos, kad būtų galima iškart pradėti dirbti. Platforma palaiko platų modelių suderinamumą, tuo pačiu užtikrindama nuoseklius API skirtingoms modelių architektūroms.

### Pažangios funkcijos ir galimybės

**Modelių valdymo meistriškumas**: Ollama siūlo išsamų modelių gyvavimo ciklo valdymą su automatiniu atsisiuntimu, talpyklos naudojimu ir versijavimu. Platforma palaiko platų modelių ekosistemą, įskaitant Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral ir specializuotus įterpimo modelius.

**Individualizavimas per Modelfiles**: Pažengę vartotojai gali kurti individualizuotas modelių konfigūracijas su specifiniais parametrais, sistemos raginimais ir elgesio modifikacijomis. Tai leidžia optimizuoti konkrečias sritis ir specializuotus programų reikalavimus.

**Našumo optimizavimas**: Ollama automatiškai aptinka ir naudoja galimą aparatinės įrangos pagreitį, įskaitant NVIDIA CUDA, Apple Metal ir OpenCL. Intelektualus atminties valdymas užtikrina optimalų išteklių naudojimą skirtingose aparatinės įrangos konfigūracijose.

### Gamybos įgyvendinimo strategijos

**Įdiegimas ir nustatymas**: Ollama siūlo supaprastintą diegimą įvairiose platformose per vietinius diegimo įrankius, paketų tvarkykles (WinGet, Homebrew, APT) ir Docker konteinerius konteinerizuotam diegimui.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Pagrindinės komandos ir operacijos**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Pažangi konfigūracija**: Modelfiles leidžia sudėtingą pritaikymą įmonės poreikiams:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Kūrėjų integracijos pavyzdžiai

**Python API integracija**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript integracija (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API naudojimas su cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Našumo derinimas ir optimizavimas

**Atminties ir gijų konfigūracija**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Kvantizacijos pasirinkimas skirtingai aparatinės įrangai**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Įmonės Edge AI platforma

### Įmonės lygio architektūra

Microsoft Foundry Local yra išsamus įmonės sprendimas, sukurtas specialiai gamybos Edge AI diegimams su gilia integracija į Microsoft ekosistemą.

**ONNX pagrindas**: Sukurtas ant pramonės standarto ONNX Runtime, Foundry Local užtikrina optimizuotą našumą įvairiose aparatinės įrangos architektūrose. Platforma naudoja Windows ML integraciją, kad optimizuotų veikimą Windows aplinkoje, tuo pačiu išlaikydama kryžminę platformos suderinamumą.

**Aparatinės įrangos pagreitinimo meistriškumas**: Foundry Local siūlo intelektualų aparatinės įrangos aptikimą ir optimizavimą CPU, GPU ir NPU. Glaudus bendradarbiavimas su aparatinės įrangos tiekėjais (AMD, Intel, NVIDIA, Qualcomm) užtikrina optimalų našumą įmonės aparatinės įrangos konfigūracijose.

### Pažangi kūrėjų patirtis

**Daugiainterfeisinė prieiga**: Foundry Local siūlo išsamias kūrimo sąsajas, įskaitant galingą CLI modelių valdymui ir diegimui, daugiakalbius SDK (Python, NodeJS) natūraliai integracijai ir RESTful API su OpenAI suderinamumu, kad būtų užtikrintas sklandus migravimas.

**Visual Studio integracija**: Platforma sklandžiai integruojasi su AI Toolkit for VS Code, siūlydama modelių konvertavimo, kvantizacijos ir optimizavimo įrankius kūrimo aplinkoje. Ši integracija pagreitina kūrimo procesus ir sumažina diegimo sudėtingumą.

**Modelių optimizavimo procesas**: Microsoft Olive integracija leidžia sudėtingus modelių optimizavimo procesus, įskaitant dinaminę kvantizaciją, grafų optimizavimą ir aparatinės įrangos specifinį derinimą. Debesų pagrindu veikiančios konvertavimo galimybės per Azure ML užtikrina mastelio didinimą dideliems modeliams.

### Gamybos įgyvendinimo strategijos

**Įdiegimas ir konfigūracija**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Modelių valdymo operacijos**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Pažangi diegimo konfigūracija**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Įmonės ekosistemos integracija

**Saugumas ir atitiktis**: Foundry Local siūlo įmonės lygio saugumo funkcijas, įskaitant prieigos kontrolę pagal vaidmenis, audito žurnalus, atitikties ataskaitas ir užšifruotą modelių saugojimą. Integracija su Microsoft saugumo infrastruktūra užtikrina laikymąsi įmonės saugumo politikos.

**Įmontuotos AI paslaugos**: Platforma siūlo paruoštas naudoti AI galimybes, įskaitant Phi Silica vietiniam kalbos apdorojimui, AI Imaging vaizdų gerinimui ir analizei bei specializuotus API dažniems įmonės AI užduotims.

## Lyginamoji analizė: Ollama vs Foundry Local

### Techninės architektūros palyginimas

| **Aspektas** | **Ollama** | **Foundry Local** |
|--------------|------------|-------------------|
| **Modelio formatas** | GGUF (per llama.cpp) | ONNX (per ONNX Runtime) |
| **Platformos fokusas** | Universalus kryžminė platforma | Windows/Įmonės optimizacija |
| **Aparatinės įrangos integracija** | Bendras GPU/CPU palaikymas | Gili Windows ML, NPU palaikymas |
| **Optimizacija** | llama.cpp kvantizacija | Microsoft Olive + ONNX Runtime |
| **Įmonės funkcijos** | Bendruomenės valdomas | Įmonės lygio su SLA |

### Našumo charakteristikos

**Ollama našumo stiprybės**:
- Išskirtinis CPU našumas per llama.cpp optimizaciją.
- Nuoseklus veikimas skirtingose platformose ir aparatinėje įrangoje.
- Efektyvus atminties naudojimas su intelektualiu modelių įkrovimu.
- Greitas paleidimas šaltuoju režimu kūrimui ir testavimui.

**Foundry Local našumo privalumai**:
- Puikus NPU naudojimas modernioje Windows aparatinėje įrangoje.
- Optimizuotas GPU pagreitis per tiekėjų partnerystes.
- Įmonės lygio našumo stebėjimas ir optimizavimas.
- Mastelio didinimo galimybės gamybos aplinkoms.

### Kūrimo patirties analizė

**Ollama kūrimo patirtis**:
- Minimalūs nustatymo reikalavimai, leidžiantys iškart pradėti dirbti.
- Intuityvi komandų eilutės sąsaja visoms operacijoms.
- Plati bendruomenės parama ir dokumentacija.
- Lankstus pritaikymas per Modelfiles.

**Foundry Local kūrimo patirtis**:
- Išsami IDE integracija su Visual Studio ekosistema.
- Įmonės kūrimo procesai su komandos bendradarbiavimo funkcijomis.
- Profesionalūs palaikymo kanalai su Microsoft užnugariu.
- Pažangūs derinimo ir optimizavimo įrankiai.

### Naudojimo atvejų optimizavimas

**Pasirinkite Ollama, kai**:
- Kuriate kryžminės platformos programas, kurioms reikalingas nuoseklus veikimas.
- Pirmenybę teikiate atvirojo kodo skaidrumui ir bendruomenės indėliui.
- Dirbate su ribotais ištekliais arba biudžeto apribojimais.
- Kuriate eksperimentines arba mokslinių tyrimų programas.
- Reikalingas platus modelių suderinamumas skirtingose architektūrose.

**Pasirinkite Foundry Local, kai**:
- Diegiate įmonės programas su griežtais našumo reikalavimais.
- Naudojate Windows specifines aparatinės įrangos optimizacijas (NPU, Windows ML).
- Reikalingas įmonės palaikymas, SLA ir atitikties funkcijos.
- Kuriate gamybos programas su Microsoft ekosistemos integracija.
- Reikalingi pažangūs optimizavimo įrankiai ir profesionalūs kūrimo procesai.

## Pažangios diegimo strategijos

### Konteinerizuoti diegimo modeliai

**Ollama konteinerizacija**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local įmonės diegimas**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Našumo optimizavimo technikos

**Ollama optimizavimo strategijos**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local optimizavimas**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Saugumo ir atitikties svarstymai

### Įmonės saugumo įgyvendinimas

**Ollama saugumo geriausios praktikos**:
- Tinklo izoliacija su ugniasienės taisyklėmis ir VPN prieiga.
- Autentifikacija per atvirkštinio tarpinio serverio integraciją.
- Modelio vientisumo patikrinimas ir saugus modelių platinimas.
- Audito žurnalai API prieigai ir modelių operacijoms.

**Foundry Local įmonės saugumas**:
- Įmontuota prieigos kontrolė pagal vaidmenis su Active Directory integracija.
- Išsamūs audito žurnalai su atitikties ataskaitomis.
- Užšifruotas modelių saugojimas ir saugus modelių diegimas.
- Integracija su Microsoft saugumo infrastruktūra.

### Atitikties ir reguliavimo reikalavimai

Abi platformos palaiko reguliavimo atitiktį per:
- Duomenų buvimo kontrolę, užtikrinančią vietinį apdorojimą.
- Audito žurnalus reguliavimo ataskaitų reikalavimams.
- Prieigos kontrolę jautrių duomenų tvarkymui.
- Šifravimą ramybės būsenoje ir perduodant duomenų apsaugai.

## Geriausios praktikos gamybos diegimui

### Stebėjimas ir stebimumas

**Pagrindiniai stebimi rodikliai**:
- Modelio apdorojimo vėlavimas ir pralaidumas.
- Išteklių naudojimas (CPU, GPU, atmintis).
- API atsako laikai ir klaidų rodikliai.
- Modelio tikslumas ir našumo nukrypimai.

**Stebėjimo įgyvendinimas**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Nuolatinė integracija ir diegimas

**CI/CD proceso integracija**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Ateities tendencijos ir svarstymai

### Atsirandančios technologijos

Vietinio SLM diegimo kraštovaizdis toliau vystosi su keliomis pagrindinėmis tendencijomis:

**Pažangios modelių architektūros**: Naujos kartos SLM su pagerintais efektyvumo ir galimybių santykiais, įskaitant ekspertų mišinio modelius dinamiškam mastelio didinimui ir specializuotas architektūras krašto diegimui.

**Aparatinės įrangos integracija**: Gilesnė integracija su specializuota dirbtinio intelekto aparatine įranga, įskaitant NPU, pritaikytą silicį ir krašto kompiuterijos pagreitintuvus, suteiks patobulintas našumo galimy

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.