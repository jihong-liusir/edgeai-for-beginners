<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T21:03:32+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "lt"
}
-->
# 1 skyrius: SLM pažangus mokymasis – pagrindai ir optimizavimas

Maži kalbos modeliai (SLM) yra svarbus EdgeAI pažangos etapas, leidžiantis sudėtingą natūralios kalbos apdorojimą įrenginiuose su ribotais ištekliais. Suprasti, kaip efektyviai diegti, optimizuoti ir naudoti SLM, yra būtina kuriant praktiškus AI sprendimus kraštiniuose įrenginiuose.

## Įvadas

Šioje pamokoje nagrinėsime mažus kalbos modelius (SLM) ir jų pažangias įgyvendinimo strategijas. Aptarsime pagrindines SLM sąvokas, jų parametrų ribas ir klasifikacijas, optimizavimo technikas bei praktines diegimo strategijas kraštiniuose kompiuterijos aplinkose.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- 🔢 Suprasti mažų kalbos modelių parametrų ribas ir klasifikacijas.
- 🛠️ Nustatyti pagrindines optimizavimo technikas SLM diegimui kraštiniuose įrenginiuose.
- 🚀 Išmokti įgyvendinti pažangias kvantavimo ir suspaudimo strategijas SLM.

## SLM parametrų ribų ir klasifikacijų supratimas

Maži kalbos modeliai (SLM) yra AI modeliai, sukurti apdoroti, suprasti ir generuoti natūralios kalbos turinį, turint žymiai mažiau parametrų nei jų didieji analogai. Nors dideli kalbos modeliai (LLM) turi šimtus milijardų ar trilijonus parametrų, SLM yra specialiai sukurti efektyvumui ir diegimui kraštiniuose įrenginiuose.

Parametrų klasifikavimo sistema padeda suprasti skirtingas SLM kategorijas ir jų tinkamus naudojimo atvejus. Ši klasifikacija yra labai svarbi renkantis tinkamą modelį konkretiems kraštinės kompiuterijos scenarijams.

### Parametrų klasifikavimo sistema

Parametrų ribų supratimas padeda pasirinkti tinkamus modelius skirtingiems kraštinės kompiuterijos scenarijams:

- **🔬 Mikro SLM**: 100M - 1,4B parametrai (ypač lengvi mobiliesiems įrenginiams)
- **📱 Maži SLM**: 1,5B - 13,9B parametrai (subalansuotas našumas ir efektyvumas)
- **⚖️ Vidutiniai SLM**: 14B - 30B parametrai (artėjant prie LLM galimybių, išlaikant efektyvumą)

Tikslūs parametrai lieka lankstūs mokslinių tyrimų bendruomenėje, tačiau dauguma praktikų modelius, turinčius mažiau nei 30 milijardų parametrų, laiko „mažais“, kai kurie šaltiniai ribą nustato dar žemiau – 10 milijardų parametrų.

### Pagrindiniai SLM privalumai

SLM turi keletą pagrindinių privalumų, dėl kurių jie yra idealūs kraštinės kompiuterijos programoms:

**Operacinis efektyvumas**: SLM užtikrina greitesnį išvedimą dėl mažesnio parametrų kiekio, todėl jie yra idealūs realaus laiko programoms. Jie reikalauja mažiau skaičiavimo išteklių, leidžia diegti įrenginiuose su ribotais ištekliais, sunaudoja mažiau energijos ir mažina anglies pėdsaką.

**Diegimo lankstumas**: Šie modeliai leidžia AI veikti įrenginiuose be interneto ryšio, didina privatumą ir saugumą per vietinį apdorojimą, gali būti pritaikyti specifinėms sritims ir yra tinkami įvairioms kraštinės kompiuterijos aplinkoms.

**Ekonomiškumas**: SLM siūlo ekonomišką mokymą ir diegimą, palyginti su LLM, mažesnes eksploatavimo išlaidas ir mažesnius pralaidumo reikalavimus kraštinėms programoms.

## Pažangios modelių įsigijimo strategijos

### Hugging Face ekosistema

Hugging Face yra pagrindinis centras, skirtas atrasti ir pasiekti pažangius SLM. Platforma siūlo išsamius išteklius modelių atradimui ir diegimui:

**Modelių atradimo funkcijos**: Platforma siūlo pažangų filtravimą pagal parametrų skaičių, licencijos tipą ir našumo metrikas. Vartotojai gali naudotis modelių palyginimo įrankiais, realaus laiko našumo etalonais ir vertinimo rezultatais bei WebGPU demonstracijomis, skirtomis nedelsiant išbandyti.

**Kuruotos SLM kolekcijos**: Populiarūs modeliai apima Phi-4-mini-3.8B pažangioms loginėms užduotims, Qwen3 seriją (0.6B/1.7B/4B) daugiakalbėms programoms, Google Gemma3 efektyvioms bendrojo naudojimo užduotims ir eksperimentinius modelius, tokius kaip BitNET, skirtus itin mažo tikslumo diegimui. Platforma taip pat siūlo bendruomenės kuruotas kolekcijas su specializuotais modeliais konkrečioms sritims ir iš anksto apmokytus bei instrukcijomis pritaikytus variantus, optimizuotus skirtingiems naudojimo atvejams.

### Azure AI Foundry modelių katalogas

Azure AI Foundry modelių katalogas siūlo įmonės lygio prieigą prie SLM su patobulintomis integracijos galimybėmis:

**Įmonės integracija**: Kataloge yra modeliai, parduodami tiesiogiai per Azure su įmonės lygio palaikymu ir SLA, įskaitant Phi-4-mini-3.8B pažangioms loginėms užduotims ir Llama 3-8B gamybos diegimui. Taip pat yra modeliai, tokie kaip Qwen3 8B, iš patikimų trečiųjų šalių atvirojo kodo modelių.

**Įmonės privalumai**: Įmontuoti įrankiai pritaikymui, stebėjimui ir atsakingam AI, integruoti su lanksčiu Provisioned Throughput visose modelių šeimose. Tiesioginis Microsoft palaikymas su įmonės SLA, integruotos saugumo ir atitikties funkcijos bei išsamūs diegimo darbo srautai pagerina įmonės patirtį.

## Pažangios kvantavimo ir optimizavimo technikos

### Llama.cpp optimizavimo sistema

Llama.cpp siūlo pažangias kvantavimo technikas, užtikrinančias maksimalų efektyvumą kraštiniuose diegimuose:

**Kvantavimo metodai**: Sistema palaiko įvairius kvantavimo lygius, įskaitant Q4_0 (4 bitų kvantavimas su puikiu dydžio sumažinimu – idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 (5 bitų kvantavimas, subalansuojantis kokybę ir suspaudimą – tinkamas Phi-4-mini-3.8B kraštiniam išvedimui) ir Q8_0 (8 bitų kvantavimas, užtikrinantis beveik originalią kokybę – rekomenduojamas Google Gemma3 gamybos naudojimui). BitNET yra pažangiausias su 1 bitų kvantavimu ekstremaliems suspaudimo scenarijams.

**Įgyvendinimo privalumai**: CPU optimizuotas išvedimas su SIMD pagreitinimu užtikrina efektyvų modelio įkrovimą ir vykdymą. Kryžminė platformų suderinamumas su x86, ARM ir Apple Silicon architektūromis leidžia diegti nepriklausomai nuo aparatūros.

**Praktinis įgyvendinimo pavyzdys**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Atminties pėdsako palyginimas**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive optimizavimo rinkinys

Microsoft Olive siūlo išsamius modelių optimizavimo darbo srautus, skirtus gamybos aplinkoms:

**Optimizavimo technikos**: Rinkinys apima dinaminį kvantavimą automatiškai parenkant tikslumą (ypač efektyvus su Qwen3 serijos modeliais), grafų optimizavimą ir operatorių sujungimą (optimizuotas Google Gemma3 architektūrai), aparatūros specifines optimizacijas CPU, GPU ir NPU (su specialiu palaikymu Phi-4-mini-3.8B ARM įrenginiuose) ir daugiapakopius optimizavimo darbo srautus. BitNET modeliams reikalingi specializuoti 1 bitų kvantavimo darbo srautai Olive sistemoje.

**Darbo srautų automatizavimas**: Automatinis optimizavimo variantų etalonų nustatymas užtikrina kokybės metrikų išsaugojimą optimizavimo metu. Integracija su populiariomis ML sistemomis, tokiomis kaip PyTorch ir ONNX, suteikia debesų ir kraštinių diegimo optimizavimo galimybes.

**Praktinis įgyvendinimo pavyzdys**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX sistema

Apple MLX siūlo natyvią optimizaciją, specialiai sukurtą Apple Silicon įrenginiams:

**Apple Silicon optimizacija**: Sistema naudoja vieningą atminties architektūrą su Metal Performance Shaders integracija, automatinį mišraus tikslumo išvedimą (ypač efektyvus su Google Gemma3) ir optimizuotą atminties pralaidumo panaudojimą. Phi-4-mini-3.8B rodo išskirtinį našumą M serijos lustuose, o Qwen3-1.7B užtikrina optimalų balansą MacBook Air diegimui.

**Kūrimo funkcijos**: Python ir Swift API palaikymas su NumPy suderinamomis masyvo operacijomis, automatinio diferencijavimo galimybėmis ir sklandžia integracija su Apple kūrimo įrankiais suteikia išsamų kūrimo aplinką.

**Praktinis įgyvendinimo pavyzdys**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Gamybos diegimas ir išvedimo strategijos

### Ollama: supaprastintas vietinis diegimas

Ollama supaprastina SLM diegimą su įmonės lygio funkcijomis vietinėms ir kraštinėms aplinkoms:

**Diegimo galimybės**: Vieno komandos modelio įdiegimas ir vykdymas su automatiniu modelio atsisiuntimu ir talpyklos naudojimu. Palaikymas Phi-4-mini-3.8B, visai Qwen3 serijai (0.6B/1.7B/4B) ir Google Gemma3 su REST API programų integracijai bei daugiamodelių valdymo ir perjungimo galimybėmis. BitNET modeliams reikalingos eksperimentinės konfigūracijos 1 bitų kvantavimo palaikymui.

**Pažangios funkcijos**: Pritaikytų modelių pritaikymo palaikymas, Dockerfile generavimas konteinerizuotam diegimui, GPU pagreitinimas su automatiniu aptikimu ir modelio kvantavimo bei optimizavimo galimybės suteikia išsamų diegimo lankstumą.

### VLLM: didelio našumo išvedimas

VLLM užtikrina gamybos lygio išvedimo optimizaciją didelio pralaidumo scenarijams:

**Našumo optimizacijos**: PagedAttention atminties efektyviam dėmesio skaičiavimui (ypač naudingas Phi-4-mini-3.8B transformatoriaus architektūrai), dinaminis paketavimas pralaidumo optimizavimui (optimizuotas Qwen3 serijos lygiagrečiam apdorojimui), tensorinis lygiagretumas daugiagrafiniam masteliui (Google Gemma3 palaikymas) ir spekuliatyvus dekodavimas latencijos mažinimui. BitNET modeliams reikalingi specializuoti išvedimo branduoliai 1 bitų operacijoms.

**Įmonės integracija**: OpenAI suderinami API galiniai taškai, Kubernetes diegimo palaikymas, stebėjimo ir stebėjimo integracija bei automatinio mastelio galimybės užtikrina įmonės lygio diegimo sprendimus.

### Foundry Local: Microsoft kraštinis sprendimas

Foundry Local siūlo išsamias kraštinio diegimo galimybes įmonės aplinkoms:

**Kraštinės kompiuterijos funkcijos**: Offline-first architektūros dizainas su išteklių apribojimų optimizavimu, vietinio modelio registro valdymu ir kraštinio-debesų sinchronizavimo galimybėmis užtikrina patikimą kraštinį diegimą.

**Saugumas ir atitiktis**: Vietinis duomenų apdorojimas privatumo išsaugojimui, įmonės saugumo kontrolė, audito žurnalų ir atitikties ataskaitų teikimas bei prieigos valdymas pagal vaidmenis užtikrina išsamų saugumą kraštiniuose diegimuose.

## Geriausios SLM įgyvendinimo praktikos

### Modelio pasirinkimo gairės

Renkantis SLM kraštiniam diegimui, atsižvelkite į šiuos veiksnius:

**Parametrų skaičiaus svarstymai**: Pasirinkite mikro SLM, tokius kaip Qwen3-0.6B, itin lengvoms mobiliosioms programoms, mažus SLM, tokius kaip Qwen3-1.7B ar Google Gemma3, subalansuotoms našumo situacijoms, ir vidutinius SLM, tokius kaip Phi-4-mini-3.8B ar Qwen3-4B, kai artėjama prie LLM galimybių, išlaikant efektyvumą. BitNET modeliai siūlo eksperimentinį itin suspaudimą specifiniams mokslinių tyrimų taikymams.

**Naudojimo atvejų suderinimas**: Suderinkite modelio galimybes su konkrečiais programų reikalavimais, atsižvelgdami į tokius veiksnius kaip atsako kokybė, išvedimo greitis, atminties apribojimai ir neprisijungus veikimo reikalavimai.

### Optimizavimo strategijos pasirinkimas

**Kvantavimo metodas**: Pasirinkite tinkamus kvantavimo lygius pagal kokybės reikalavimus ir aparatūros apribojimus. Apsvarstykite Q4_0 maksimaliai suspaudimui (idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 subalansuotam kokybės-suspaudimo kompromisui (tinkamas Phi-4-mini-3.8B ir Google Gemma3), ir Q8_0 beveik originalios kokybės išsaugojimui (rekomenduojamas Qwen3-4B gamybos aplinkoms). BitNET 1 bitų kvantavimas yra ekstremalaus suspaudimo riba specializuotoms programoms.

**Sistemos pasirinkimas**: Pasirinkite optimizavimo sistemas pagal tikslinę aparatūrą ir diegimo reikalavimus. Naudokite Llama.cpp CPU optimizuotam diegimui, Microsoft Olive išsamiems optimizavimo darbo srautams ir Apple MLX Apple Silicon įrenginiams.

## Praktiniai modelių pavyzdžiai ir naudojimo atvejai

### Realūs diegimo scenarijai

**Mobiliosios programos**: Qwen3-0.6B puikiai tinka išmaniojo telefono pokalbių programoms su minimaliu atminties pėdsaku, o Google Gemma3 užtikrina subalansuotą našumą planšetinių kompiuterių edukaciniams įrankiams. Phi-4-mini-3.8B siūlo aukščiausio lygio loginio mąstymo galimybes mobiliosiose produktyvumo programose.

**Darbalaukio ir kraštinė kompiuterija**: Qwen3-1.7B užtikrina optimalų našumą darbalaukio asistento programoms, Phi-4-mini-3.8B siūlo pažangias kodo generavimo galimybes kūrėjų įrankiams, o Qwen3-4B leidžia sudėtingą dokumentų analizę darbo vietos aplinkose.

**Moksliniai tyrimai ir eksperimentai**: BitNET modeliai leidžia tyrinėti itin mažo tikslumo išvedimą akademiniams tyrimams ir koncepcijos įrodymo programoms, reikalaujančioms ekstremalių išteklių apribojimų.

### Našumo etalonai ir palyginimai

**Išvedimo greitis**: Qwen3-0.6B pasiekia greičiausią išvedimo laik

---

**Atsakomybės atsisakymas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamės užtikrinti tikslumą, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus interpretavimus, atsiradusius dėl šio vertimo naudojimo.