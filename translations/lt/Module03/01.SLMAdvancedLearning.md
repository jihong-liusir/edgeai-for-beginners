<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-19T01:42:25+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "lt"
}
-->
# 1 skyrius: SLM pažangus mokymasis – pagrindai ir optimizavimas

Maži kalbos modeliai (SLM) yra svarbus EdgeAI pažangos etapas, leidžiantis sudėtingą natūralios kalbos apdorojimą įgyvendinti ribotų resursų įrenginiuose. Suprasti, kaip efektyviai diegti, optimizuoti ir naudoti SLM yra būtina kuriant praktiškus AI sprendimus kraštiniuose įrenginiuose.

## Įvadas

Šioje pamokoje nagrinėsime mažus kalbos modelius (SLM) ir jų pažangias įgyvendinimo strategijas. Aptarsime pagrindines SLM sąvokas, jų parametrų ribas ir klasifikacijas, optimizavimo technikas bei praktines diegimo strategijas kraštiniuose kompiuteriniuose aplinkose.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- 🔢 Suprasti mažų kalbos modelių parametrų ribas ir klasifikacijas.
- 🛠️ Identifikuoti pagrindines optimizavimo technikas SLM diegimui kraštiniuose įrenginiuose.
- 🚀 Išmokti įgyvendinti pažangias kvantavimo ir suspaudimo strategijas SLM modeliams.

## SLM parametrų ribų ir klasifikacijų supratimas

Maži kalbos modeliai (SLM) yra AI modeliai, sukurti apdoroti, suprasti ir generuoti natūralios kalbos turinį, turint žymiai mažiau parametrų nei dideli modeliai. Nors dideli kalbos modeliai (LLM) turi šimtus milijardų ar trilijonus parametrų, SLM yra specialiai sukurti efektyvumui ir diegimui kraštiniuose įrenginiuose.

Parametrų klasifikavimo sistema padeda suprasti skirtingas SLM kategorijas ir jų tinkamus naudojimo atvejus. Ši klasifikacija yra labai svarbi renkantis tinkamą modelį specifinėms kraštinio kompiuterio scenarijoms.

### Parametrų klasifikavimo sistema

Parametrų ribų supratimas padeda pasirinkti tinkamus modelius skirtingiems kraštinio kompiuterio scenarijams:

- **🔬 Mikro SLM**: 100M – 1,4B parametrai (ypač lengvi mobiliesiems įrenginiams)
- **📱 Maži SLM**: 1,5B – 13,9B parametrai (subalansuotas našumas ir efektyvumas)
- **⚖️ Vidutiniai SLM**: 14B – 30B parametrai (artėjantys prie LLM galimybių, išlaikant efektyvumą)

Tiksli riba lieka lanksti mokslininkų bendruomenėje, tačiau dauguma praktikų modelius, turinčius mažiau nei 30 milijardų parametrų, laiko „mažais“, kai kurie šaltiniai ribą nustato dar žemiau – 10 milijardų parametrų.

### Pagrindiniai SLM privalumai

SLM turi keletą esminių privalumų, kurie daro juos idealius kraštinio kompiuterio programoms:

**Operacinis efektyvumas**: SLM užtikrina greitesnį išvedimą dėl mažesnio parametrų kiekio, todėl jie puikiai tinka realaus laiko programoms. Jie reikalauja mažiau skaičiavimo resursų, leidžia diegti ribotų resursų įrenginiuose, sunaudoja mažiau energijos ir mažina anglies pėdsaką.

**Diegimo lankstumas**: Šie modeliai leidžia AI veikti įrenginyje be interneto ryšio, didina privatumą ir saugumą per vietinį apdorojimą, gali būti pritaikyti specifinėms sritims ir yra tinkami įvairioms kraštinio kompiuterio aplinkoms.

**Ekonomiškumas**: SLM siūlo ekonomišką mokymą ir diegimą, palyginti su LLM, mažesnes eksploatavimo išlaidas ir mažesnius pralaidumo reikalavimus kraštinėms programoms.

## Pažangios modelių įsigijimo strategijos

### Hugging Face ekosistema

Hugging Face yra pagrindinis centras, skirtas atrasti ir pasiekti pažangius SLM modelius. Platforma siūlo išsamius išteklius modelių atradimui ir diegimui:

**Modelių atradimo funkcijos**: Platforma siūlo pažangų filtravimą pagal parametrų skaičių, licencijos tipą ir našumo rodiklius. Vartotojai gali naudotis modelių palyginimo įrankiais, realaus laiko našumo testais ir vertinimo rezultatais bei WebGPU demonstracijomis, skirtomis tiesioginiam testavimui.

**Kuruotos SLM kolekcijos**: Populiarūs modeliai apima Phi-4-mini-3.8B pažangiems samprotavimo uždaviniams, Qwen3 seriją (0.6B/1.7B/4B) daugiakalbėms programoms, Google Gemma3 efektyvioms bendros paskirties užduotims ir eksperimentinius modelius, tokius kaip BitNET, skirtus itin mažo tikslumo diegimui. Platforma taip pat siūlo bendruomenės kuruotas kolekcijas su specializuotais modeliais specifinėms sritims ir iš anksto apmokytus bei instrukcijoms pritaikytus variantus, optimizuotus skirtingiems naudojimo atvejams.

### Azure AI Foundry modelių katalogas

Azure AI Foundry modelių katalogas siūlo įmonės lygio prieigą prie SLM modelių su patobulintomis integracijos galimybėmis:

**Įmonės integracija**: Kataloge yra modeliai, parduodami tiesiogiai per Azure su įmonės lygio palaikymu ir SLA, įskaitant Phi-4-mini-3.8B pažangiems samprotavimo gebėjimams ir Llama 3-8B gamybos diegimui. Taip pat yra modeliai, tokie kaip Qwen3 8B, iš patikimų trečiųjų šalių atvirojo kodo modelių.

**Įmonės privalumai**: Integruoti įrankiai pritaikymui, stebėjimui ir atsakingam AI, kartu su lanksčiu Provisioned Throughput tarp modelių šeimų. Tiesioginis Microsoft palaikymas su įmonės SLA, integruotos saugumo ir atitikties funkcijos bei išsamūs diegimo darbo srautai pagerina įmonės patirtį.

## Pažangios kvantavimo ir optimizavimo technikos

### Llama.cpp optimizavimo sistema

Llama.cpp siūlo pažangias kvantavimo technikas, skirtas maksimaliam efektyvumui kraštiniuose diegimuose:

**Kvantavimo metodai**: Sistema palaiko įvairius kvantavimo lygius, įskaitant Q4_0 (4 bitų kvantavimas su puikiu dydžio sumažinimu – idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 (5 bitų kvantavimas, subalansuojantis kokybę ir suspaudimą – tinkamas Phi-4-mini-3.8B kraštiniam išvedimui) ir Q8_0 (8 bitų kvantavimas, artimas originaliai kokybei – rekomenduojamas Google Gemma3 gamybos naudojimui). BitNET yra pažangiausias su 1 bitų kvantavimu ekstremaliems suspaudimo scenarijams.

**Įgyvendinimo privalumai**: CPU optimizuotas išvedimas su SIMD pagreitinimu užtikrina efektyvų modelio įkrovimą ir vykdymą. Kryžminė platformų suderinamumas tarp x86, ARM ir Apple Silicon architektūrų leidžia diegimą nepriklausomai nuo techninės įrangos.

**Praktinio įgyvendinimo pavyzdys**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Atminties pėdsako palyginimas**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive optimizavimo rinkinys

Microsoft Olive siūlo išsamius modelių optimizavimo darbo srautus, skirtus gamybos aplinkoms:

**Optimizavimo technikos**: Rinkinys apima dinaminį kvantavimą automatiškai parenkant tikslumą (ypač efektyvus su Qwen3 serijos modeliais), grafų optimizavimą ir operatorių sujungimą (optimizuotas Google Gemma3 architektūrai), techninės įrangos specifines optimizacijas CPU, GPU ir NPU (ypač palaikomas Phi-4-mini-3.8B ARM įrenginiuose) ir daugiapakopius optimizavimo darbo srautus. BitNET modeliams reikalingi specializuoti 1 bitų kvantavimo darbo srautai Olive sistemoje.

**Darbo srautų automatizavimas**: Automatinis optimizavimo variantų testavimas užtikrina kokybės metrikų išsaugojimą optimizavimo metu. Integracija su populiariomis ML sistemomis, tokiomis kaip PyTorch ir ONNX, suteikia debesų ir kraštinio diegimo optimizavimo galimybes.

**Praktinio įgyvendinimo pavyzdys**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX sistema

Apple MLX siūlo natyvią optimizaciją, specialiai sukurtą Apple Silicon įrenginiams:

**Apple Silicon optimizacija**: Sistema naudoja vieningą atminties architektūrą su Metal Performance Shaders integracija, automatinį mišrų tikslumo išvedimą (ypač efektyvus su Google Gemma3) ir optimizuotą atminties pralaidumo naudojimą. Phi-4-mini-3.8B rodo išskirtinį našumą M serijos lustuose, o Qwen3-1.7B užtikrina optimalų balansą MacBook Air diegimui.

**Kūrimo funkcijos**: Python ir Swift API palaikymas su NumPy suderinamomis masyvo operacijomis, automatinio diferencijavimo galimybėmis ir sklandžia integracija su Apple kūrimo įrankiais suteikia išsamų kūrimo aplinką.

**Praktinio įgyvendinimo pavyzdys**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Gamybos diegimo ir išvedimo strategijos

### Ollama: supaprastintas vietinis diegimas

Ollama supaprastina SLM diegimą su įmonės lygio funkcijomis vietinėms ir kraštinėms aplinkoms:

**Diegimo galimybės**: Vieno komandos modelio įdiegimas ir vykdymas su automatiniu modelio atsisiuntimu ir talpinimu. Palaikymas Phi-4-mini-3.8B, visai Qwen3 serijai (0.6B/1.7B/4B) ir Google Gemma3 su REST API integracijai į programas bei daugiamodelių valdymo ir perjungimo galimybėmis. BitNET modeliams reikalingos eksperimentinės konfigūracijos 1 bitų kvantavimo palaikymui.

**Pažangios funkcijos**: Pritaikytas modelių pritaikymas, Dockerfile generavimas konteinerizuotam diegimui, GPU pagreitinimas su automatiniu aptikimu ir modelių kvantavimo bei optimizavimo galimybės suteikia išsamų diegimo lankstumą.

### VLLM: aukštos kokybės išvedimas

VLLM užtikrina gamybos lygio išvedimo optimizaciją didelio pralaidumo scenarijams:

**Našumo optimizacijos**: PagedAttention atminties efektyviam dėmesio skaičiavimui (ypač naudingas Phi-4-mini-3.8B transformatorių architektūrai), dinaminis paketavimas pralaidumo optimizavimui (optimizuotas Qwen3 serijos lygiagrečiam apdorojimui), tensorų paralelizmas daugiagrafių masteliui (Google Gemma3 palaikymas) ir spekuliatyvus dekodavimas vėlavimo mažinimui. BitNET modeliams reikalingi specializuoti išvedimo branduoliai 1 bitų operacijoms.

**Įmonės integracija**: OpenAI suderinami API galiniai taškai, Kubernetes diegimo palaikymas, stebėjimo ir stebėjimo integracija bei automatinio mastelio galimybės užtikrina įmonės lygio diegimo sprendimus.

### Foundry Local: Microsoft kraštinis sprendimas

Foundry Local siūlo išsamias kraštinio diegimo galimybes įmonės aplinkoms:

**Kraštinio kompiuterio funkcijos**: Offline-first architektūros dizainas su resursų apribojimų optimizavimu, vietinio modelio registrų valdymu ir kraštinio-debesų sinchronizavimo galimybėmis užtikrina patikimą kraštinį diegimą.

**Saugumas ir atitiktis**: Vietinis duomenų apdorojimas privatumo išsaugojimui, įmonės saugumo kontrolės, audito žurnalų ir atitikties ataskaitų teikimas bei prieigos valdymas pagal vaidmenis užtikrina išsamų saugumą kraštiniuose diegimuose.

## Geriausios SLM įgyvendinimo praktikos

### Modelio pasirinkimo gairės

Renkantis SLM kraštiniam diegimui, atsižvelkite į šiuos veiksnius:

**Parametrų skaičiaus svarstymai**: Pasirinkite mikro SLM, tokius kaip Qwen3-0.6B, itin lengvoms mobiliosioms programoms, mažus SLM, tokius kaip Qwen3-1.7B ar Google Gemma3, subalansuotam našumui, ir vidutinius SLM, tokius kaip Phi-4-mini-3.8B ar Qwen3-4B, kai artėjama prie LLM galimybių, išlaikant efektyvumą. BitNET modeliai siūlo eksperimentinį itin suspaudimą specifiniams moksliniams tyrimams.

**Naudojimo atvejų suderinimas**: Suderinkite modelio galimybes su specifiniais programos reikalavimais, atsižvelgdami į tokius veiksnius kaip atsako kokybė, išvedimo greitis, atminties apribojimai ir reikalavimai veikti neprisijungus.

### Optimizavimo strategijos pasirinkimas

**Kvantavimo metodas**: Pasirinkite tinkamus kvantavimo lygius pagal kokybės reikalavimus ir techninės įrangos apribojimus. Apsvarstykite Q4_0 maksimaliam suspaudimui (idealus Qwen3-0.6B mobiliajam diegimui), Q5_1 subalansuotam kokybės-suspaudimo kompromisui (tinkamas Phi-4-mini-3.8B ir Google Gemma3), ir Q8_0 artimai originalios kokybės išsaugojimui (rekomenduojamas Qwen3-4B gamybos aplinkoms). BitNET 1 bitų kvantavimas yra ekstremalaus suspaudimo riba specializuotoms programoms.

**Sistemos pasirinkimas**: Pasirinkite optimizavimo sistemas pagal tikslinę techninę įrangą ir diegimo reikalavimus. Naudokite Llama.cpp CPU optimizuotam diegimui, Microsoft Olive išsamiems optimizavimo darbo srautams ir Apple MLX Apple Silicon įrenginiams.

## Praktiniai modelių pavyzdžiai ir naudojimo atvejai

### Realūs diegimo scenarijai

**Mobiliosios programos**: Qwen3-0.6B puikiai tinka išmaniojo telefono pokalbių programoms su minimaliu atminties pėdsaku, o Google Gemma3 užtikrina subalansuotą našumą planšetinių kompiuterių edukaciniams įrankiams. Phi-4-mini-3.8B siūlo aukščiausio lygio samprotavimo galimybes mobiliosioms produktyvumo programoms.

**Darbalaukio ir kraštinis kompiuteris**: Qwen3-1.7B užtikrina optimalų našumą darbalaukio asistento programoms, Phi-4-mini-3.8B siūlo pažangias kodo generavimo galimybes kūrėjų įrankiams, o Qwen3-4B leidžia sudėtingą dokumentų analizę darbo vietos aplinkose.

**Moksliniai tyrimai ir eksperimentai**: BitNET modeliai leidžia tyrinėti itin mažo tikslumo išvedimą akademiniams tyrimams ir koncepcijos įrodymo programoms, reikalaujančioms ekstremalių resursų apribojimų.

### Našumo testai ir palyginimai

**Išvedimo greitis**: Qwen3-0.6B pasiekia greičiausią išvedimo laiką mobiliose CPU, Google Gemma3 užtikrina

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius dėl šio vertimo naudojimo.