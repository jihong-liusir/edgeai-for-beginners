<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-18T23:12:23+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "lt"
}
-->
# 3 skyrius: Gemma šeimos pagrindai

Gemma modelių šeima atspindi „Google“ visapusišką požiūrį į atvirojo kodo didelius kalbos modelius ir multimodalinę dirbtinio intelekto technologiją, įrodydama, kad prieinami modeliai gali pasiekti puikių rezultatų ir būti pritaikomi įvairiuose scenarijuose – nuo mobiliųjų įrenginių iki verslo darbo stočių. Svarbu suprasti, kaip Gemma šeima suteikia galingas dirbtinio intelekto galimybes su lanksčiais diegimo pasirinkimais, išlaikant konkurencingą našumą ir atsakingo dirbtinio intelekto praktiką.

## Įvadas

Šiame vadove mes nagrinėsime „Google“ Gemma modelių šeimą ir jos pagrindinius principus. Aptarsime Gemma šeimos evoliuciją, inovatyvius mokymo metodus, kurie daro Gemma modelius efektyviais, pagrindinius šeimos variantus ir praktinius pritaikymus skirtinguose diegimo scenarijuose.

## Mokymosi tikslai

Šio vadovo pabaigoje jūs galėsite:

- Suprasti „Google“ Gemma modelių šeimos dizaino filosofiją ir evoliuciją
- Atpažinti pagrindines inovacijas, leidžiančias Gemma modeliams pasiekti aukštą našumą įvairių parametrų dydžių diapazone
- Įvertinti skirtingų Gemma modelių variantų privalumus ir trūkumus
- Pritaikyti žinias apie Gemma modelius, kad pasirinktumėte tinkamus variantus realioms situacijoms

## Šiuolaikinio dirbtinio intelekto modelių kraštovaizdžio supratimas

Dirbtinio intelekto kraštovaizdis smarkiai pasikeitė, kai skirtingos organizacijos ėmė taikyti įvairius požiūrius į kalbos modelių kūrimą. Kai kurios organizacijos orientuojasi į uždarus, patentuotus modelius, prieinamus tik per API, o kitos pabrėžia atvirojo kodo prieinamumą ir skaidrumą. Tradicinis požiūris apima arba masyvius patentuotus modelius su nuolatinėmis išlaidomis, arba atvirojo kodo modelius, kuriems gali prireikti didelės techninės kompetencijos diegimui.

Ši paradigma kelia iššūkių organizacijoms, siekiančioms galingų dirbtinio intelekto galimybių, išlaikant duomenų kontrolę, išlaidas ir diegimo lankstumą. Tradicinis požiūris dažnai reikalauja rinktis tarp pažangiausio našumo ir praktinių diegimo aspektų.

## Prieinamo dirbtinio intelekto kokybės iššūkis

Aukštos kokybės, prieinamo dirbtinio intelekto poreikis tampa vis svarbesnis įvairiuose scenarijuose. Pavyzdžiui, pritaikymuose, kuriems reikalingos lanksčios diegimo galimybės skirtingiems organizaciniams poreikiams, ekonomiški sprendimai, kai API išlaidos gali tapti reikšmingos, multimodalinės galimybės visapusiškam supratimui arba specializuotas diegimas mobiliuosiuose ir kraštiniuose įrenginiuose.

### Pagrindiniai diegimo reikalavimai

Šiuolaikiniai dirbtinio intelekto diegimai susiduria su keliais pagrindiniais reikalavimais, kurie riboja praktinį pritaikomumą:

- **Prieinamumas**: Atvirojo kodo prieinamumas skaidrumui ir pritaikymui
- **Ekonomiškumas**: Protingi skaičiavimo reikalavimai įvairiems biudžetams
- **Lankstumas**: Įvairūs modelių dydžiai skirtingiems diegimo scenarijams
- **Multimodalinis supratimas**: Vaizdų, teksto ir garso apdorojimo galimybės
- **Kraštinis diegimas**: Optimizuotas našumas mobiliuosiuose ir ribotų išteklių įrenginiuose

## Gemma modelių filosofija

Gemma modelių šeima atspindi „Google“ visapusišką požiūrį į dirbtinio intelekto modelių kūrimą, pirmenybę teikiant atvirojo kodo prieinamumui, multimodalinėms galimybėms ir praktiniam diegimui, išlaikant konkurencingas našumo charakteristikas. Gemma modeliai tai pasiekia per įvairius modelių dydžius, aukštos kokybės mokymo metodikas, kilusias iš Gemini tyrimų, ir specializuotus variantus skirtingoms sritims bei diegimo scenarijams.

Gemma šeima apima įvairius požiūrius, skirtus suteikti pasirinkimus našumo ir efektyvumo spektre, leidžiant diegimą nuo mobiliųjų įrenginių iki verslo serverių, tuo pačiu užtikrinant prasmingas dirbtinio intelekto galimybes. Tikslas – demokratizuoti prieigą prie aukštos kokybės dirbtinio intelekto technologijos, suteikiant lankstumą diegimo pasirinkimuose.

### Pagrindiniai Gemma dizaino principai

Gemma modeliai yra sukurti remiantis keliais pagrindiniais principais, kurie juos išskiria iš kitų kalbos modelių šeimų:

- **Atvirojo kodo prioritetas**: Visiškas skaidrumas ir prieinamumas tyrimams bei komerciniam naudojimui
- **Tyrimų pagrįstas vystymas**: Sukurta naudojant tą pačią technologiją, kuri maitina Gemini modelius
- **Skalabili architektūra**: Įvairūs modelių dydžiai, atitinkantys skirtingus skaičiavimo reikalavimus
- **Atsakingas dirbtinis intelektas**: Integruotos saugumo priemonės ir atsakingo vystymo praktikos

## Pagrindinės technologijos, leidžiančios Gemma šeimai veikti

### Pažangios mokymo metodikos

Vienas iš Gemma šeimos išskirtinių aspektų yra sudėtingas mokymo požiūris, kilęs iš „Google“ Gemini tyrimų. Gemma modeliai naudoja distiliaciją iš didesnių modelių, stiprinimo mokymą iš žmonių atsiliepimų (RLHF) ir modelių sujungimo technikas, kad pasiektų geresnį našumą matematikos, kodavimo ir instrukcijų vykdymo srityse.

Mokymo procesas apima distiliaciją iš didesnių instrukcijų modelių, stiprinimo mokymą iš žmonių atsiliepimų (RLHF), kad būtų suderinta su žmonių pageidavimais, stiprinimo mokymą iš mašinos atsiliepimų (RLMF) matematinio samprotavimo srityje ir stiprinimo mokymą iš vykdymo atsiliepimų (RLEF) kodavimo galimybėms.

### Multimodalinė integracija ir supratimas

Naujausi Gemma modeliai apima sudėtingas multimodalines galimybes, leidžiančias visapusiškai suprasti skirtingus įvesties tipus:

**Vaizdo ir teksto integracija (Gemma 3)**: Gemma 3 gali vienu metu apdoroti tekstą ir vaizdus, leidžiant analizuoti vaizdus, atsakyti į klausimus apie vizualinį turinį, išgauti tekstą iš vaizdų ir suprasti sudėtingus vizualinius duomenis.

**Garso apdorojimas (Gemma 3n)**: Gemma 3n turi pažangias garso galimybes, įskaitant automatinį kalbos atpažinimą (ASR) ir automatinį kalbos vertimą (AST), ypač stipriai verčiant tarp anglų, ispanų, prancūzų, italų ir portugalų kalbų.

**Sujungtų įvesties apdorojimas**: Gemma modeliai palaiko sujungtas įvestis tarp modalumų, leidžiant suprasti sudėtingas multimodalines sąveikas, kur tekstas, vaizdai ir garsas gali būti apdorojami kartu.

### Architektūrinės inovacijos

Gemma šeima apima kelias architektūrines optimizacijas, skirtas tiek našumui, tiek efektyvumui:

**Konteksto lango išplėtimas**: Gemma 3 modeliai turi 128K žetonų konteksto langą, 16 kartų didesnį nei ankstesni Gemma modeliai, leidžiant apdoroti didžiulius informacijos kiekius, įskaitant kelis dokumentus ar šimtus vaizdų.

**Mobiliesiems pritaikyta architektūra (Gemma 3n)**: Gemma 3n naudoja Per-Layer Embeddings (PLE) technologiją ir MatFormer architektūrą, leidžiančią didesniems modeliams veikti su atminties poreikiais, panašiais į mažesnius tradicinius modelius.

**Funkcijų kvietimo galimybės**: Gemma 3 palaiko funkcijų kvietimą, leidžiant kūrėjams kurti natūralios kalbos sąsajas programavimo sąsajoms ir kurti intelektualias automatizavimo sistemas.

## Modelių dydžiai ir diegimo galimybės

Šiuolaikinės diegimo aplinkos naudoja Gemma modelių lankstumą įvairiems skaičiavimo reikalavimams:

### Maži modeliai (0.6B-4B)

Gemma siūlo efektyvius mažus modelius, tinkamus kraštiniam diegimui, mobilioms aplikacijoms ir ribotų išteklių aplinkoms, išlaikant įspūdingas galimybes. 1B modelis yra idealus mažoms aplikacijoms, o 4B modelis siūlo subalansuotą našumą ir lankstumą su multimodaliniu palaikymu.

### Vidutiniai modeliai (8B-14B)

Vidutinio diapazono modeliai siūlo patobulintas galimybes profesionalioms aplikacijoms, užtikrinant puikų balansą tarp našumo ir skaičiavimo reikalavimų darbo stočių ir serverių diegimui.

### Dideli modeliai (27B+)

Pilno masto modeliai užtikrina pažangiausią našumą reiklioms aplikacijoms, tyrimams ir verslo diegimams, kuriems reikalingos maksimalios galimybės. 27B modelis yra pats pajėgiausias variantas, kuris vis dar gali veikti viename GPU.

### Mobiliesiems optimizuoti modeliai (Gemma 3n)

Gemma 3n E2B ir E4B modeliai yra specialiai sukurti mobiliesiems ir kraštiniam diegimui, su efektyviais parametrų skaičiais 2B ir 4B, tuo pačiu naudojant inovatyvią architektūrą, kad sumažintų atminties poreikį iki vos 2GB E2B ir 3GB E4B.

## Gemma modelių šeimos privalumai

### Atvirojo kodo prieinamumas

Gemma modeliai suteikia visišką skaidrumą ir pritaikymo galimybes su atvirais svoriais, leidžiančiais atsakingą komercinį naudojimą, suteikiant organizacijoms galimybę juos pritaikyti savo projektams ir aplikacijoms.

### Diegimo lankstumas

Modelių dydžių diapazonas leidžia diegimą įvairiose aparatinės įrangos konfigūracijose, nuo mobiliųjų įrenginių iki aukštos klasės serverių, su optimizacija įvairioms platformoms, įskaitant „Google Cloud TPUs“, „NVIDIA GPUs“, „AMD GPUs“ per ROCm ir CPU vykdymą per Gemma.cpp.

### Daugiakalbė kompetencija

Gemma modeliai pasižymi puikiu daugiakalbiu supratimu ir generavimu, palaikydami daugiau nei 140 kalbų su neprilygstamomis daugiakalbėmis galimybėmis, todėl jie tinkami globalioms aplikacijoms.

### Konkurencingas našumas

Gemma modeliai nuolat pasiekia konkurencingus rezultatus testuose, o Gemma 3 užima aukštas vietas tiek tarp populiarių patentuotų, tiek atvirų modelių vartotojų pageidavimų vertinimuose.

### Specializuotos galimybės

Specifinės srities aplikacijos gauna naudos iš Gemma multimodalinio supratimo, funkcijų kvietimo galimybių ir optimizuoto našumo įvairiose aparatinės įrangos platformose.

## Praktiniai pavyzdžiai ir naudojimo atvejai

Prieš gilindamiesi į technines detales, pažvelkime į keletą konkrečių pavyzdžių, ką Gemma modeliai gali pasiekti:

### Multimodalinio supratimo pavyzdys

Gemma 3 gali vienu metu apdoroti tekstą ir vaizdus visapusiškai analizei:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### Daugiakalbės paramos pavyzdys

Gemma modeliai demonstruoja stiprias daugiakalbes galimybes daugiau nei 140 kalbų:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### Funkcijų kvietimo pavyzdys

Gemma 3 palaiko funkcijų kvietimą natūralios kalbos sąsajų kūrimui:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### Mobiliojo diegimo pavyzdys (Gemma 3n)

Gemma 3n yra optimizuotas mobiliesiems ir kraštiniam diegimui su efektyviu atminties naudojimu:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### Garso apdorojimo pavyzdys (Gemma 3n)

Gemma 3n apima pažangias garso galimybes kalbos atpažinimui ir vertimui:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Gemma šeimos evoliucija

### Gemma 1.0 ir 2.0: Pagrindiniai modeliai

Ankstyvieji Gemma modeliai nustatė pagrindinius atvirojo kodo prieinamumo ir praktinio diegimo principus:

- **Gemma-2B ir 7B**: Pradinis leidimas, orientuotas į efektyvų kalbos supratimą
- **Gemma 1.5 serija**: Išplėstas konteksto apdorojimas ir patobulintas našumas
- **Gemma 2 šeima**: Multimodalinių galimybių ir išplėstų modelių dydžių įvedimas

### Gemma 3: Multimodalinė kompetencija

Gemma 3 serija žymėjo reikšmingą pažangą multimodalinėse galimybėse ir našume. Sukurta remiantis ta pačia technologija, kuri maitina Gemini 2.0 modelius, Gemma 3 pristatė vaizdo ir teksto supratimą, 128K žetonų konteksto langus, funkcijų kvietimą ir palaikymą daugiau nei 140 kalbų.

Pagrindinės Gemma 3 savybės apima:
- **Gemma 3-1B iki 27B**: Išsamus diapazonas įvairiems diegimo poreikiams
- **Multimodalinis supratimas**: Pažangios teksto ir vizualinio samprotavimo galimybės
- **Išplėstas kontekstas**: 128K žetonų apdorojimo galimybė
- **Funkcijų kvietimas**: Natūralios kalbos sąsajų kūrimas
- **Patobulintas mokymas**: Optimizuotas naudojant distiliaciją ir stiprinimo mokymą

### Gemma 3n: Mobiliesiems pritaikyta inovacija

Gemma 3n yra proveržis mobiliesiems pritaikyto dirbtinio intelekto architektūroje, pasižymintis revoliucine Per-Layer Embeddings (PLE) technologija, MatFormer architektūra skaičiavimo lankstumui ir visapusiškomis multimodalinėmis galimybėmis, įskaitant garso apdorojimą.

Gemma 3n inovacijos apima:
- **E2B ir E4B modeliai**: Efektyvus 2B ir 4B parametrų našumas su sumažintu atminties poreikiu
- **Garso galimybės**: Aukštos kokybės ASR ir kalbos vertimas
- **Vaizdo supratimas**: Žymiai patobulintos vaizdo apdorojimo galimybės
- **Mobilioji optimizacija**: Sukurta realaus laiko dirbtiniam intelektui telefonuose ir planšetėse

## Gemma modelių pritaikymas

### Verslo aplikacijos

Organizacijos naudoja Gemma modelius dokumentų analizei su vizualiniu turiniu, klientų aptarnavimo automatizavimui su multimodaliniu palaikymu, intelektualiam kodavimo asistentui ir verslo analitikos aplikacijoms. Atvirojo kodo pobūdis leidžia pritaiky
- Gemma 3 suteikia galingas galimybes kūrėjams, pasižymėdama pažangiu teksto ir vaizdų supratimu, palaikydama vaizdų ir teksto įvestį multimodaliam supratimui.  
- Gemma 3n užima aukštas pozicijas tiek tarp populiarių patentuotų, tiek tarp atvirų modelių Chatbot Arena Elo reitinguose, rodančiuose stiprų vartotojų pritarimą.

**Efektyvumo pasiekimai:**  
- Gemma 3 modeliai gali apdoroti iki 128K žetonų įvestį, tai yra 16 kartų didesnis konteksto langas nei ankstesniuose Gemma modeliuose.  
- Gemma 3n naudoja Per-Layer Embeddings (PLE), kurie žymiai sumažina RAM naudojimą, išlaikant didesnių modelių galimybes.  

**Optimizacija mobiliesiems:**  
- Gemma 3n E2B veikia su vos 2GB atminties, o E4B reikalauja tik 3GB, nepaisant to, kad jų parametrų skaičius yra atitinkamai 5B ir 8B.  
- Realaus laiko AI galimybės tiesiogiai mobiliuosiuose įrenginiuose su privatumo pirmumo ir neprisijungus veikiančia operacija.  

**Mokymo mastas:**  
- Gemma 3 buvo apmokyta naudojant 2T žetonų 1B modeliui, 4T – 4B modeliui, 12T – 12B modeliui ir 14T žetonų 27B modeliui, naudojant Google TPUs ir JAX Framework.  

### Modelių palyginimo matrica  

| Modelių serija | Parametrų diapazonas | Konteksto ilgis | Pagrindiniai privalumai | Geriausi naudojimo atvejai |  
|----------------|----------------------|-----------------|-------------------------|----------------------------|  
| **Gemma 3**    | 1B-27B              | 128K            | Multimodalinis supratimas, funkcijų kvietimas | Bendros paskirties užduotys, vizijos ir kalbos užduotys |  
| **Gemma 3n**   | E2B (5B), E4B (8B)  | Kintamas        | Optimizacija mobiliesiems, garso apdorojimas | Mobiliosios programos, edge kompiuterija, realaus laiko AI |  
| **Gemma 2.5**  | 0.5B-72B            | 32K-128K        | Subalansuotas našumas, daugiakalbis | Produkcijos diegimas, esami darbo procesai |  
| **Gemma-VL**   | Įvairūs             | Kintamas        | Vizijos ir kalbos specializacija | Vaizdų analizė, vizualinių klausimų atsakymai |  

## Modelio pasirinkimo vadovas  

### Pagrindinėms programoms  
- **Gemma 3-1B**: Lengvos teksto užduotys, paprastos mobiliosios programos  
- **Gemma 3-4B**: Subalansuotas našumas su multimodaliniu palaikymu bendram naudojimui  

### Multimodalinėms programoms  
- **Gemma 3-4B/12B**: Vaizdų supratimas, vizualinių klausimų atsakymai  
- **Gemma 3n**: Mobiliosios multimodalinės programos su garso apdorojimo galimybėmis  

### Mobiliesiems ir edge diegimui  
- **Gemma 3n E2B**: Įrenginiai su ribotais resursais, realaus laiko mobilusis AI  
- **Gemma 3n E4B**: Patobulintas mobilusis našumas su garso galimybėmis  

### Įmonių diegimui  
- **Gemma 3-12B/27B**: Aukštos kokybės kalbos ir vizijos supratimas  
- **Funkcijų kvietimo galimybės**: Protingų automatizavimo sistemų kūrimas  

### Pasaulinėms programoms  
- **Bet kuris Gemma 3 variantas**: 140+ kalbų palaikymas su kultūriniu supratimu  
- **Gemma 3n**: Mobiliosios pirmumo pasaulinės programos su garso vertimu  

## Diegimo platformos ir prieinamumas  

### Debesų platformos  
- **Vertex AI**: Pilnos MLOps galimybės su serverless patirtimi  
- **Google Kubernetes Engine (GKE)**: Mastelio konteinerių diegimas sudėtingoms užduotims  
- **Google GenAI API**: Tiesioginė API prieiga greitam prototipų kūrimui  
- **NVIDIA API katalogas**: Optimizuotas našumas NVIDIA GPU  

### Vietinės kūrimo aplinkos  
- **Hugging Face Transformers**: Standartinė integracija kūrimui  
- **Ollama**: Supaprastintas vietinis diegimas ir valdymas  
- **vLLM**: Aukštos kokybės aptarnavimas produkcijai  
- **Gemma.cpp**: CPU optimizuotas vykdymas  
- **Google AI Edge**: Optimizacija mobiliesiems ir edge diegimui  

### Mokymosi ištekliai  
- **Google AI Studio**: Išbandykite Gemma modelius vos keliais paspaudimais  
- **Kaggle ir Hugging Face**: Atsisiųskite modelių svorius ir bendruomenės pavyzdžius  
- **Techninės ataskaitos**: Išsamūs dokumentai ir moksliniai straipsniai  
- **Bendruomenės forumai**: Aktyvi bendruomenės pagalba ir diskusijos  

### Pradžia su Gemma modeliais  

#### Kūrybos platformos  
1. **Google AI Studio**: Pradėkite nuo internetinio eksperimentavimo  
2. **Hugging Face Hub**: Tyrinėkite modelius ir bendruomenės įgyvendinimus  
3. **Vietinis diegimas**: Naudokite Ollama arba Transformers kūrimui  

#### Mokymosi kelias  
1. **Supraskite pagrindines sąvokas**: Studijuokite multimodalines galimybes ir diegimo parinktis  
2. **Eksperimentuokite su variantais**: Išbandykite skirtingus modelių dydžius ir specializuotas versijas  
3. **Praktikuokite įgyvendinimą**: Diegkite modelius kūrimo aplinkose  
4. **Optimizuokite produkcijai**: Tobulinkite specifiniams naudojimo atvejams ir platformoms  

#### Geriausia praktika  
- **Pradėkite nuo mažo**: Pradėkite nuo Gemma 3-4B pradiniam kūrimui ir testavimui  
- **Naudokite oficialius šablonus**: Taikykite tinkamus pokalbių šablonus optimaliems rezultatams  
- **Stebėkite resursus**: Sekite atminties naudojimą ir įžvalgų našumą  
- **Apsvarstykite specializaciją**: Pasirinkite tinkamus variantus multimodaliniams ar mobiliems poreikiams  

## Pažangūs naudojimo modeliai  

### Fine-tuning pavyzdžiai  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### Specializuotas promptų kūrimas  

**Multimodalinėms užduotims:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**Funkcijų kvietimui su kontekstu:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### Daugiakalbės programos su kultūriniu kontekstu  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### Produkcijos diegimo modeliai  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## Našumo optimizavimo strategijos  

### Atminties optimizavimas  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### Įžvalgų optimizavimas  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## Geriausia praktika ir gairės  

### Saugumas ir privatumas  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### Stebėjimas ir vertinimas  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## Išvada  

Gemma modelių šeima atspindi Google visapusišką požiūrį į AI technologijų demokratizavimą, išlaikant konkurencingą našumą įvairiose programose ir diegimo scenarijuose. Per įsipareigojimą atviram šaltiniui, multimodalines galimybes ir inovatyvius architektūrinius sprendimus, Gemma suteikia organizacijoms ir kūrėjams galimybę pasinaudoti galingomis AI galimybėmis, nepaisant jų resursų ar specifinių poreikių.  

### Pagrindinės išvados  

**Atviro kodo pranašumas**: Gemma parodo, kad atviro kodo modeliai gali pasiekti našumą, konkuruojantį su patentuotais alternatyvomis, tuo pačiu suteikiant skaidrumą, pritaikymą ir kontrolę AI diegimui.  

**Multimodalinė inovacija**: Teksto, vizijos ir garso galimybių integracija Gemma 3 ir Gemma 3n modeliuose žymi reikšmingą pažangą prieinamoje multimodalinėje AI, leidžiant visapusiškai suprasti skirtingus įvesties tipus.  

**Mobilioji architektūra**: Gemma 3n proveržis su Per-Layer Embeddings (PLE) technologija ir optimizacija mobiliesiems parodo, kad galinga AI gali efektyviai veikti ribotų resursų įrenginiuose, neprarandant galimybių.  

**Mastelio diegimas**: Parametrų diapazonas nuo 1B iki 27B, su specializuotais mobiliaisiais variantais, leidžia diegimą visame skaičiavimo aplinkų spektre, išlaikant nuoseklią kokybę ir našumą.  

**Atsakinga AI integracija**: Integruotos saugumo priemonės per ShieldGemma 2 ir atsakingos kūrimo praktikos užtikrina, kad galingos AI galimybės gali būti diegiamos saugiai ir etiškai.  

### Ateities perspektyvos  

Kaip Gemma šeima toliau vystosi, galime tikėtis:  

**Patobulintos mobiliosios galimybės**: Tolimesnė optimizacija mobiliesiems ir edge diegimui su Gemma 3n architektūros integracija į pagrindines platformas, tokias kaip Android ir Chrome.  

**Išplėstas multimodalinis supratimas**: Tolimesnė pažanga vizijos-kalbos-garso integracijoje, siekiant visapusiškesnių AI patirčių.  

**Patobulintas efektyvumas**: Nuolatinės architektūrinės inovacijos, siekiant geresnių našumo ir parametrų santykių bei sumažintų skaičiavimo reikalavimų.  

**Platesnė ekosistemos integracija**: Patobulintas palaikymas kūrimo sistemose, debesų platformose ir diegimo įrankiuose, siekiant sklandžios integracijos į esamus darbo procesus.  

**Bendruomenės augimas**: Tolimesnė Gemmaverse plėtra su bendruomenės sukurtais modeliais, įrankiais ir programomis, plečiančiomis pagrindines galimybes.  

### Kiti žingsniai  

Nesvarbu, ar kuriate mobilias programas su realaus laiko AI galimybėmis, multimodalinius edukacinius įrankius, protingas automatizavimo sistemas, ar pasaulines programas, reikalaujančias daugiakalbio palaikymo, Gemma šeima siūlo mastelio sprendimus su stipria bendruomenės parama ir išsamia dokumentacija.  

**Pradžios rekomendacijos:**  
1. **Eksperimentuokite su Google AI Studio** norėdami iš karto pradėti praktinį darbą  
2. **Atsisiųskite modelius iš Hugging Face** vietiniam kūrimui ir pritaikymui  
3. **Tyrinėkite specializuotus variantus**, tokius kaip Gemma 3n mobiliosioms programoms  
4. **Įgyvendinkite multimodalines galimybes** visapusiškoms AI patirtims  
5. **Laikykitės saugumo geriausios praktikos** produkcijos diegimui  

**Mobiliajam kūrimui**: Pradėkite nuo Gemma 3n E2B, kad efektyviai diegtumėte su garso ir vizijos galimybėmis.  

**Įmonių programoms**: Apsvarstykite Gemma 3-12B arba 27B modelius, kad pasiektumėte maksimalias galimybes su funkcijų kvietimu ir pažangiu supratimu.  

**Pasaulinėms programoms**: Pasinaudokite Gemma 140+ kalbų palaikymu su kultūriškai jautriu promptų kūrimu.  

**Specializuotiems naudojimo atvejams**: Tyrinėkite fine-tuning metodus ir optimizavimo technikas konkrečioms sritims.  

### 🔮 AI demokratizacija  

Gemma šeima įkūnija AI kūrimo ateitį, kur galingi, pajėgūs modeliai yra prieinami visiems – nuo individualių kūrėjų iki didelių įmonių. Derindama pažangius tyrimus su atviro kodo prieinamumu, Google sukūrė pagrindą, leidžiantį inovacijas visose srityse ir mastuose.  

Gemma sėkmė su daugiau nei 100 milijonų atsisiuntimų ir 60,000+ bendruomenės variantų demonstruoja atviro bendradarbiavimo galią AI technologijos pažangoje. Judant į priekį, Gemma šeima ir toliau bus AI inovacijų katalizatorius, leidžiantis kurti programas, kurios anksčiau buvo įmanomos tik su patentuotais, brangiais modeliais.  

AI ateitis yra atvira, prieinama ir galinga – ir Gemma šeima yra lyderė, padedanti šią viziją paversti realybe.  

## Papildomi ištekliai  

**Oficiali dokumentacija ir modeliai:**  
- **Google AI Studio**: [Išbandykite Gemma modelius tiesiogiai](https://aistudio.google.com)  
- **Hugging Face kolekcijos**:  
  - [Gemma 3 išleidimas](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [Gemma 3n peržiūra](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **Google AI kūrėjų dokumentacija**: [Išsamūs Gemma vadovai](https://ai.google.dev/gemma)  
- **Vertex AI dokumentacija**: [Įmonių diegimo vadovai](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**Techniniai ištekliai:**  
- **Moksliniai straipsniai ir techninės ataskaitos**: [Google DeepMind publikacijos](https://deepmind.google/models/gemma/)  
- **Kūrėjų tinklaraščio įrašai**: [Naujausi pranešimai ir pamokos](https://developers.googleblog.com)  
- **Modelių kortelės**: Išsamios techninės specifikacijos ir našumo rodikliai  

**Bendruomenė ir palaikymas:**  
- **Hugging Face bendruomenė**: Aktyvios diskusijos ir bendruomenės pavyzdžiai  
- **GitHub saugyklos**: Atviro kodo įgyvendinimai ir įrankiai  
- **Kūrėjų forumai**: Google AI kūrėjų bendruomenės palaikymas  
- **Stack Overflow**: Žymėtos klausimai ir bendruomenės sprendimai  

**Kūrimo įrankiai:**  
- **Ollama**: [Paprastas vietinis diegimas](https://ollama.ai)  
- **vLLM**: [Aukštos kokybės aptarnavimas](https://github.com/vllm-project/vllm)  
- **Transformers biblioteka**: [Hugging Face integracija](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: Optimizacija mobiliesiems ir edge diegimui  

**Mokymosi keliai:**  
- **Pradedantysis**: Pradėkite nuo Google AI Studio → Hugging Face pavyzdžių → Vietinis diegimas  
- **Kūrėjas**: Transformers integracija → Individualios programos → Produkcijos diegimas  
- **Tyrėjas**: Techniniai straipsniai → Fine-tuning → Naujos programos  
- **Įmonė**: Vertex AI diegimas → Saugumo įgyvendinimas → Mastelio optimizacija  

Gemma modelių šeima atspindi ne tik AI modelių rinkinį, bet ir visą ekosistemą, skirtą kurti prieinamas, galingas ir atsakingas AI programas. Pradėkite tyrinėti šiandien ir prisijunkite prie auganč

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus aiškinimus, kylančius dėl šio vertimo naudojimo.