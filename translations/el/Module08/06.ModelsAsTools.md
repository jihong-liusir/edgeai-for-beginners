<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "33ecd8ecf0e9347a2b4839a9916e49fb",
  "translation_date": "2025-10-01T00:26:22+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "el"
}
-->
# ## Επισκόπηση

Χρησιμοποιήστε τα μοντέλα AI ως αρθρωτά, προσαρμόσιμα εργαλεία που εκτελούνται απευθείας στη συσκευή με το Foundry Local. Αυτή η συνεδρία δίνει έμφαση σε πρακτικές ροές εργασίας για διατήρηση της ιδιωτικότητας, γρήγορη επεξεργασία και πώς να ενσωματώσετε αυτά τα εργαλεία μέσω SDKs, APIs ή CLI. Θα μάθετε επίσης πώς να κλιμακώσετε στο Azure AI Foundry όταν χρειάζεται.

> **🔄 Ενημερωμένο για το Σύγχρονο SDK**: Αυτή η ενότητα έχει ευθυγραμμιστεί με τα πιο πρόσφατα πρότυπα του αποθετηρίου Microsoft Foundry-Local και ταιριάζει με την υλοποίηση έξυπνης δρομολόγησης στο `samples/06/`. Τα παραδείγματα χρησιμοποιούν πλέον το σύγχρονο `foundry-local-sdk` και προηγμένες στρατηγικές επιλογής μοντέλων.

**🏗️ Σημεία Αρχιτεκτονικής:**
- **Έξυπνη Δρομολόγηση Μοντέλων**: Επιλογή βάσει λέξεων-κλειδιών μεταξύ γενικών, λογικών, κώδικα και δημιουργικών μοντέλων
- **Ενσωμάτωση Σύγχρονου SDK**: Χρησιμοποιεί το `FoundryLocalManager` με αυτόματη ανακάλυψη υπηρεσιών
- **Διαμόρφωση Περιβάλλοντος**: Ευέλικτη ανάθεση μοντέλων μέσω μεταβλητών περιβάλλοντος
- **Παρακολούθηση Υγείας**: Επικύρωση υπηρεσιών και έλεγχος διαθεσιμότητας μοντέλων
- **Έτοιμο για Παραγωγή**: Ολοκληρωμένος χειρισμός σφαλμάτων και μηχανισμοί εναλλακτικής λύσης

**📁 Τοπική Υλοποίηση:**
- `samples/06/router.py` - Έξυπνη δρομολόγηση μοντέλων με επιλογή βάσει λέξεων-κλειδιών
- `samples/06/model_router.ipynb` - Διαδραστικά παραδείγματα και μετρήσεις
- `samples/06/README.md` - Οδηγίες διαμόρφωσης και χρήσης

Αναφορές:
- Έγγραφα Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Ενσωμάτωση με SDKs επεξεργασίας: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Συμπίεση μοντέλων Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Επισκόπηση

Χρησιμοποιήστε τα μοντέλα AI ως αρθρωτά, προσαρμόσιμα εργαλεία που εκτελούνται απευθείας στη συσκευή με το Foundry Local. Αυτή η συνεδρία δίνει έμφαση σε πρακτικές ροές εργασίας για διατήρηση της ιδιωτικότητας, γρήγορη επεξεργασία και πώς να ενσωματώσετε αυτά τα εργαλεία μέσω SDKs, APIs ή CLI. Θα μάθετε επίσης πώς να κλιμακώσετε στο Azure AI Foundry όταν χρειάζεται.

Αναφορές:
- Έγγραφα Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Ενσωμάτωση με SDKs επεξεργασίας: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Συμπίεση μοντέλων Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Στόχοι Μάθησης
- Σχεδιάστε μοτίβα μοντέλων ως εργαλεία στη συσκευή
- Ενσωματώστε μέσω REST API συμβατού με OpenAI ή SDKs
- Προσαρμόστε τα μοντέλα σε περιπτώσεις χρήσης συγκεκριμένου τομέα
- Σχεδιάστε για υβριδική κλιμάκωση στο Azure AI Foundry

## Μέρος 1: Έξυπνη Δρομολόγηση Μοντέλων (Σύγχρονη Υλοποίηση)

Στόχος: Υλοποιήστε έξυπνη επιλογή μοντέλων με αυτόματη δρομολόγηση βάσει του περιεχομένου ερωτήματος.

> **📋 Σημείωση**: Αυτή η υλοποίηση ταιριάζει με τα πρότυπα που χρησιμοποιούνται στο `samples/06/router.py` με προηγμένη επιλογή μοντέλων βάσει λέξεων-κλειδιών.

Βήμα 1) Ορίστε σύγχρονη δρομολόγηση μοντέλων με το FoundryLocalManager  
```python
# router/intelligent_router.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import Dict, Any, Optional
import os
import json

class ModelRouter:
    """Intelligent model router that selects appropriate models for different task types."""
    
    def __init__(self):
        self.client = None
        self.base_url = None
        self.tools = self._load_tool_registry()
        self._initialize_client()
    
    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load tool registry from environment or use defaults."""
        default_tools = {
            "general": {
                "model": os.environ.get("GENERAL_MODEL", "phi-4-mini"),
                "notes": "Fast general-purpose chat and Q&A",
                "temperature": 0.7
            },
            "reasoning": {
                "model": os.environ.get("REASONING_MODEL", "deepseek-r1-7b"),
                "notes": "Step-by-step analysis and logical reasoning",
                "temperature": 0.3
            },
            "code": {
                "model": os.environ.get("CODE_MODEL", "qwen2.5-7b"),
                "notes": "Code generation, debugging, and technical tasks",
                "temperature": 0.2
            },
            "creative": {
                "model": os.environ.get("CREATIVE_MODEL", "phi-4-mini"),
                "notes": "Creative writing and storytelling",
                "temperature": 0.9
            }
        }
        
        # Check for environment override
        tools_env = os.environ.get("TOOL_REGISTRY")
        if tools_env:
            try:
                return json.loads(tools_env)
            except json.JSONDecodeError:
                print("Warning: Invalid TOOL_REGISTRY JSON, using defaults")
        
        return default_tools
```
  
Βήμα 2) Αρχικοποιήστε τον πελάτη με το σύγχρονο SDK και την ανακάλυψη υπηρεσιών  
```python
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local or fallback configuration."""
        try:
            from foundry_local import FoundryLocalManager
            # Try to use any available model for client initialization
            first_model = next(iter(self.tools.values()))["model"]
            manager = FoundryLocalManager(first_model)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.base_url = manager.endpoint
            print(f"✅ Foundry Local SDK initialized")
        except Exception as e:
            print(f"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration")
            # Fallback to manual configuration
            self.base_url = os.environ.get("BASE_URL", "http://localhost:8000")
            api_key = os.environ.get("API_KEY", "")
            
            self.client = OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key=api_key
            )
            print(f"Initialized manual configuration at {self.base_url}")
    
    def select_tool(self, user_query: str) -> str:
        """Select the most appropriate tool based on the user query."""
        query_lower = user_query.lower()
        
        # Code-related keywords
        code_keywords = ["code", "python", "function", "class", "method", "bug", "debug", 
                        "programming", "script", "algorithm", "implementation", "refactor"]
        if any(keyword in query_lower for keyword in code_keywords):
            return "code"
        
        # Reasoning keywords
        reasoning_keywords = ["why", "how", "explain", "step-by-step", "reason", "analyze", 
                             "think", "logic", "because", "cause", "compare", "evaluate"]
        if any(keyword in query_lower for keyword in reasoning_keywords):
            return "reasoning"
        
        # Creative keywords
        creative_keywords = ["story", "poem", "creative", "imagine", "write", "tale", 
                           "narrative", "fiction", "character", "plot"]
        if any(keyword in query_lower for keyword in creative_keywords):
            return "creative"
        
        # Default to general
        return "general"
    
    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:
        """Send chat completion request to the specified model."""
        try:
            params = {
                "model": model,
                "messages": [{"role": "user", "content": content}],
                "max_tokens": max_tokens
            }
            
            if temperature is not None:
                params["temperature"] = temperature
            
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response with model {model}: {str(e)}"
```
  
Βήμα 3) Υλοποιήστε έξυπνη δρομολόγηση και εκτέλεση (δείτε `samples/06/router.py`)  
```python
    def route_and_run(self, prompt: str) -> Dict[str, Any]:
        """Route the prompt to the appropriate model and generate response."""
        tool_key = self.select_tool(prompt)
        tool_config = self.tools[tool_key]
        model = tool_config["model"]
        temperature = tool_config.get("temperature", 0.7)
        
        print(f"🎯 Selected tool: {tool_key} (model: {model})")
        
        answer = self.chat(
            model=model, 
            content=prompt, 
            max_tokens=400, 
            temperature=temperature
        )
        
        return {
            "tool": tool_key,
            "model": model,
            "tool_description": tool_config["notes"],
            "temperature": temperature,
            "answer": answer
        }
    
    def check_service_health(self) -> Dict[str, Any]:
        """Check Foundry Local service health and available models."""
        try:
            models_response = self.client.models.list()
            available_models = [model.id for model in models_response.data]
            
            return {
                "status": "healthy",
                "base_url": self.base_url,
                "available_models": available_models,
                "tools_configured": list(self.tools.keys())
            }
        except Exception as e:
            return {
                "status": "error",
                "base_url": self.base_url,
                "error": str(e)
            }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    router = ModelRouter()
    
    # Check health
    health = router.check_service_health()
    print(f"Service Health: {json.dumps(health, indent=2)}")
    
    # Test different query types
    queries = [
        "Write a Python function to calculate fibonacci numbers",  # -> code
        "Explain step-by-step why the sky is blue",  # -> reasoning
        "Tell me a creative story about AI",  # -> creative
        "What's the weather like today?"  # -> general
    ]
    
    for query in queries:
        result = router.route_and_run(query)
        print(f"\nQuery: {query}")
        print(f"Selected: {result['tool']} -> {result['model']}")
        print(f"Answer: {result['answer'][:100]}...")
```
  

## Μέρος 2: Ενσωμάτωση Σύγχρονου SDK (Βήμα-βήμα)

Στόχος: Χρησιμοποιήστε το Foundry Local SDK με το OpenAI Python SDK για απρόσκοπτη ενσωμάτωση.

Βήμα 1) Εγκαταστήστε τις εξαρτήσεις  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install foundry-local-sdk openai
```
  
Βήμα 2) Διαμορφώστε το περιβάλλον (προαιρετικό - δείτε `samples/06/README.md`)  
```cmd
REM Override default models per tool
set GENERAL_MODEL=phi-4-mini
set REASONING_MODEL=deepseek-r1-7b
set CODE_MODEL=qwen2.5-7b
REM Or provide a full JSON registry
set TOOL_REGISTRY={"general":{"model":"phi-4-mini"},"reasoning":{"model":"deepseek-r1-7b"}}
```
  
Βήμα 3) Ενσωμάτωση σύγχρονου SDK  
```python
# modern_sdk_demo.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import sys

def main():
    """Demonstrate modern SDK integration."""
    try:
        # Initialize with FoundryLocalManager
        alias = "phi-4-mini"
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client using Foundry Local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Get model info
        model_info = manager.get_model_info(alias)
        print(f"Using model: {model_info.id}")
        
        # Make request with streaming
        stream = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Explain edge AI benefits in one paragraph."}],
            stream=True,
            max_tokens=200
        )
        
        print("Response: ", end="")
        for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"Error: {e}")
        print("Ensure Foundry Local is running with: foundry model run phi-4-mini")
        sys.exit(1)

if __name__ == "__main__":
    main()
```
  

## Μέρος 3: Προσαρμογή Τομέα (Βήμα-βήμα)

Στόχος: Προσαρμόστε τα αποτελέσματα για έναν τομέα χρησιμοποιώντας πρότυπα προτροπών και σχήμα JSON.

Βήμα 1) Δημιουργήστε ένα πρότυπο προτροπής τομέα  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Βήμα 2) Επιβάλετε έξοδο JSON  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Μέρος 4: Λειτουργία Εκτός Σύνδεσης και Θέση Ασφαλείας (Βήμα-βήμα)

Στόχος: Εξασφαλίστε ιδιωτικότητα και ανθεκτικότητα όταν εκτελείτε μοντέλα ως εργαλεία τοπικά.

Βήμα 1) Προθερμάνετε και επικυρώστε το τοπικό endpoint  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Βήμα 2) Καθαρίστε τις εισόδους  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Βήμα 3) Σημαία μόνο τοπικής λειτουργίας και καταγραφή  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Μέρος 5: Ανάπτυξη Παραγωγής και Κλιμάκωση

Στόχος: Αναπτύξτε την έξυπνη δρομολόγηση με παρακολούθηση και ενσωμάτωση στο Azure AI Foundry.

> **📋 Σημείωση**: Η τοπική υλοποίηση στο `samples/06/model_router.ipynb` περιλαμβάνει ολοκληρωμένα παραδείγματα μοτίβων ανάπτυξης παραγωγής.

Βήμα 1) Δρομολογητής παραγωγής με παρακολούθηση (δείτε `samples/06/router.py`)  
```python
# production/router.py
from router.intelligent_router import ModelRouter
import json
import time
import sys

class ProductionModelRouter(ModelRouter):
    """Production-ready model router with monitoring and logging."""
    
    def __init__(self):
        super().__init__()
        self.request_count = 0
        self.error_count = 0
        self.start_time = time.time()
    
    def route_and_run_with_monitoring(self, prompt: str) -> Dict[str, Any]:
        """Route with comprehensive monitoring and error handling."""
        start_time = time.time()
        self.request_count += 1
        
        try:
            result = self.route_and_run(prompt)
            processing_time = time.time() - start_time
            
            # Log successful request
            self._log_request({
                "status": "success",
                "tool": result["tool"],
                "model": result["model"],
                "processing_time": processing_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            result["processing_time"] = processing_time
            return result
            
        except Exception as e:
            self.error_count += 1
            error_result = {
                "status": "error",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            self._log_request(error_result)
            return error_result
    
    def _log_request(self, data: Dict[str, Any]):
        """Log request data for monitoring."""
        print(f"📊 {json.dumps(data)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get router statistics."""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "total_requests": self.request_count,
            "error_count": self.error_count,
            "success_rate": (self.request_count - self.error_count) / max(1, self.request_count),
            "requests_per_minute": self.request_count / max(1, uptime / 60)
        }

def main():
    """Production router demo."""
    router = ProductionModelRouter()
    
    # Health check
    health = router.check_service_health()
    if health["status"] == "error":
        print(f"❌ Service health check failed: {health['error']}")
        sys.exit(1)
    
    print(f"✅ Service healthy with {len(health['available_models'])} models")
    
    # Process user query
    user_prompt = " ".join(sys.argv[1:]) or "Write three benefits of on-device AI in JSON format."
    print(f"\n🎯 Processing: {user_prompt}")
    
    result = router.route_and_run_with_monitoring(user_prompt)
    
    if result.get("status") == "error":
        print(f"❌ Error: {result['error']}")
    else:
        print(f"\n📋 Result:")
        print(f"Tool: {result['tool']} -> Model: {result['model']}")
        print(f"Processing Time: {result['processing_time']:.2f}s")
        print(f"Answer: {result['answer']}")
    
    # Show stats
    stats = router.get_stats()
    print(f"\n📊 Statistics: {json.dumps(stats, indent=2)}")

if __name__ == "__main__":
    main()
```
  

## Λίστα Ελέγχου Πρακτικής Εξάσκησης
- [ ] Υλοποιήστε έξυπνη δρομολόγηση μοντέλων με επιλογή βάσει λέξεων-κλειδιών (`samples/06/router.py`)
- [ ] Διαμορφώστε πολλαπλά εξειδικευμένα μοντέλα (γενικά, λογικά, κώδικα, δημιουργικά)
- [ ] Δοκιμάστε το διαδραστικό Jupyter notebook (`samples/06/model_router.ipynb`)
- [ ] Ρυθμίστε τη διαμόρφωση μοντέλων βάσει περιβάλλοντος
- [ ] Υλοποιήστε παρακολούθηση υγείας υπηρεσιών και χειρισμό σφαλμάτων
- [ ] Αναπτύξτε δρομολογητή παραγωγής με ολοκληρωμένη καταγραφή

## Ενσωμάτωση Τοπικού Δείγματος

Εκτελέστε την πλήρη υλοποίηση:  
```cmd
cd Module08
.\.venv\Scripts\activate

REM Start required models
foundry model run phi-4-mini
foundry model run qwen2.5-7b
foundry model run deepseek-r1-7b

REM Test the intelligent router
python samples\06\router.py "Write a Python function to sort a list"
python samples\06\router.py "Explain step-by-step how bubble sort works"
python samples\06\router.py "Tell me a creative story about robots"

REM Explore the interactive notebook
jupyter notebook samples/06/model_router.ipynb
```
  

## Αναφορές και Επόμενα Βήματα
- **Τοπική Υλοποίηση**: `samples/06/` - Πλήρης έξυπνος δρομολογητής με υποστήριξη πολλαπλών μοντέλων
- **Δείγματα Microsoft**: [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
- **Έγγραφα Ενσωμάτωσης**: [Ενσωμάτωση με SDKs Επεξεργασίας](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks)
- **Προηγμένα Μοτίβα**: Εξερευνήστε κλήσεις λειτουργιών και ορχήστρωση πολλαπλών πρακτόρων στην Ενότητα 5

## Επίλογος

Το Foundry Local επιτρέπει ισχυρή AI στη συσκευή, όπου τα μοντέλα γίνονται έξυπνα, εξειδικευμένα εργαλεία. Με αυτόματη επιλογή μοντέλων, ολοκληρωμένη παρακολούθηση και μοτίβα έτοιμα για παραγωγή, οι ομάδες μπορούν να αναπτύξουν προηγμένες εφαρμογές AI που προσαρμόζονται σε διαφορετικούς τύπους εργασιών, διατηρώντας παράλληλα την ιδιωτικότητα και την απόδοση. Το μοτίβο έξυπνου δρομολογητή που παρουσιάστηκε εδώ παρέχει τη βάση για την κατασκευή σύνθετων συστημάτων AI που μπορούν να κλιμακωθούν από τοπική ανάπτυξη σε παραγωγή.

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.