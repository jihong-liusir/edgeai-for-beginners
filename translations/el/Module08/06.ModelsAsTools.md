<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T19:20:16+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "el"
}
-->
# Συνεδρία 6: Foundry Local – Τα Μοντέλα ως Εργαλεία

## Επισκόπηση

Χρησιμοποιήστε τα μοντέλα AI ως αρθρωτά, προσαρμόσιμα εργαλεία που εκτελούνται απευθείας στη συσκευή με το Foundry Local. Αυτή η συνεδρία δίνει έμφαση σε πρακτικές ροές εργασίας για διατήρηση της ιδιωτικότητας, χαμηλή καθυστέρηση στην επεξεργασία και πώς να ενσωματώσετε αυτά τα εργαλεία μέσω SDKs, APIs ή CLI. Θα μάθετε επίσης πώς να κλιμακώσετε στο Azure AI Foundry όταν χρειάζεται.

Αναφορές:
- Έγγραφα Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Ενσωμάτωση με inference SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Συμπίεση μοντέλων Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Στόχοι Μάθησης
- Σχεδιάστε μοτίβα "μοντέλο-ως-εργαλείο" στη συσκευή
- Ενσωματώστε μέσω OpenAI-compatible REST API ή SDKs
- Προσαρμόστε μοντέλα σε περιπτώσεις χρήσης συγκεκριμένου τομέα
- Σχεδιάστε για υβριδική κλιμάκωση στο Azure AI Foundry

## Μέρος 1: Αφηρημένες Έννοιες Εργαλείων (Βήμα-βήμα)

Στόχος: Αναπαραστήστε τα μοντέλα ως εργαλεία με σαφείς συμβάσεις και έναν απλό δρομολογητή.

Βήμα 1) Ορίστε τη διεπαφή εργαλείου και το μητρώο
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```

Βήμα 2) Υλοποιήστε δύο εργαλεία που υποστηρίζονται από το Foundry Local
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```

Βήμα 3) Δρομολογητής ανά εργασία
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```


## Μέρος 2: Ενσωμάτωση SDK και API (Βήμα-βήμα)

Στόχος: Χρησιμοποιήστε το OpenAI Python SDK με το Foundry Local endpoint.

Βήμα 1) Εγκατάσταση
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```

Βήμα 2) Ρύθμιση μεταβλητών περιβάλλοντος
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```

Βήμα 3) Κλήση του chat API
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```


## Μέρος 3: Προσαρμογή Τομέα (Βήμα-βήμα)

Στόχος: Προσαρμόστε τα αποτελέσματα για έναν τομέα χρησιμοποιώντας πρότυπα προτροπών και JSON schema.

Βήμα 1) Δημιουργήστε ένα πρότυπο προτροπής τομέα
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```

Βήμα 2) Επιβάλετε έξοδο JSON
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```


## Μέρος 4: Λειτουργία Εκτός Σύνδεσης και Θέση Ασφαλείας (Βήμα-βήμα)

Στόχος: Εξασφαλίστε ιδιωτικότητα και ανθεκτικότητα όταν εκτελείτε μοντέλα ως εργαλεία τοπικά.

Βήμα 1) Προετοιμάστε και επικυρώστε το τοπικό endpoint
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```

Βήμα 2) Καθαρίστε τις εισόδους
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

Βήμα 3) Σημαία μόνο τοπικής λειτουργίας και καταγραφή
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```


## Μέρος 5: Κλιμάκωση στο Azure AI Foundry (Βήμα-βήμα)

Στόχος: Καθρεφτίστε τα τοπικά μοντέλα με endpoints του Azure για δυνατότητα υπερχείλισης.

Βήμα 1) Αποφασίστε στρατηγική δρομολόγησης
- Πρώτα τοπικά για ιδιωτικότητα/χαμηλή καθυστέρηση, fallback στο Azure σε σφάλματα ή μεγάλα αιτήματα

Βήμα 2) Υλοποιήστε ένα απλό stub δρομολογητή
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```


## Λίστα Ελέγχου Πρακτικής Εξάσκησης
- [ ] Καταχωρίστε τουλάχιστον δύο εργαλεία και δρομολογήστε αιτήματα
- [ ] Κάντε κλήση στο Foundry Local μέσω OpenAI SDK και raw REST
- [ ] Επιβάλετε εξόδους JSON για ένα πρότυπο τομέα
- [ ] Καθαρίστε και καταγράψτε κλήσεις τοπικά
- [ ] Υλοποιήστε έναν απλό υβριδικό δρομολογητή με fallback στο Azure

## Επίλογος

Το Foundry Local επιτρέπει ισχυρή AI στη συσκευή όπου τα μοντέλα γίνονται συνθέσιμα εργαλεία. Με σαφείς διεπαφές, διακυβέρνηση και υβριδική κλιμάκωση, οι ομάδες μπορούν να δημιουργήσουν εφαρμογές AI σε πραγματικό χρόνο, ασφαλείς, που σέβονται την ιδιωτικότητα των χρηστών ενώ παραμένουν έτοιμες για επιχειρηματική χρήση.

---

