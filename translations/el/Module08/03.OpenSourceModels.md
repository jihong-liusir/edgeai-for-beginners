<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T19:18:31+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "el"
}
-->
# Συνεδρία 3: Μοντέλα Ανοιχτού Κώδικα με Foundry Local

## Επισκόπηση

Αυτή η συνεδρία εξετάζει πώς να φέρετε μοντέλα ανοιχτού κώδικα στο Foundry Local: επιλογή μοντέλων από την κοινότητα, ενσωμάτωση περιεχομένου από το Hugging Face και υιοθέτηση στρατηγικών "φέρτε το δικό σας μοντέλο" (BYOM). Θα ανακαλύψετε επίσης τη σειρά Model Mondays για συνεχή μάθηση και ανακάλυψη μοντέλων.

Αναφορές:
- Έγγραφα Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Μεταγλώττιση μοντέλων Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Στόχοι Μάθησης
- Ανακαλύψτε και αξιολογήστε μοντέλα ανοιχτού κώδικα για τοπική επεξεργασία
- Μεταγλωττίστε και εκτελέστε επιλεγμένα μοντέλα Hugging Face στο Foundry Local
- Εφαρμόστε στρατηγικές επιλογής μοντέλων για ακρίβεια, καθυστέρηση και ανάγκες πόρων
- Διαχειριστείτε μοντέλα τοπικά με cache και εκδόσεις

## Μέρος 1: Ανακάλυψη και Επιλογή Μοντέλων (Βήμα-βήμα)

Βήμα 1) Λίστα διαθέσιμων μοντέλων στον τοπικό κατάλογο  
```cmd
foundry model list
```
  
Βήμα 2) Γρήγορη δοκιμή δύο υποψηφίων (αυτόματη λήψη κατά την πρώτη εκτέλεση)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Βήμα 3) Σημειώστε βασικές μετρήσεις  
- Παρατηρήστε την καθυστέρηση (υποκειμενική) και την ποιότητα για μια σταθερή προτροπή  
- Παρακολουθήστε τη χρήση μνήμης μέσω του Task Manager ενώ εκτελείται κάθε μοντέλο  

## Μέρος 2: Εκτέλεση Μοντέλων Καταλόγου μέσω CLI (Βήμα-βήμα)

Βήμα 1) Ξεκινήστε ένα μοντέλο  
```cmd
foundry model run llama-3.2
```
  
Βήμα 2) Στείλτε μια δοκιμαστική προτροπή μέσω του συμβατού με OpenAI endpoint  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Μέρος 3: BYOM – Μεταγλώττιση Μοντέλων Hugging Face (Βήμα-βήμα)

Ακολουθήστε τον επίσημο οδηγό για τη μεταγλώττιση μοντέλων. Υψηλού επιπέδου ροή παρακάτω—δείτε το άρθρο στο Microsoft Learn για ακριβείς εντολές και υποστηριζόμενες διαμορφώσεις.

Βήμα 1) Προετοιμάστε έναν φάκελο εργασίας  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Βήμα 2) Μεταγλωττίστε ένα υποστηριζόμενο μοντέλο HF  
- Χρησιμοποιήστε τα βήματα από το έγγραφο Learn για να μετατρέψετε και να τοποθετήσετε το μεταγλωττισμένο μοντέλο ONNX στον φάκελο `models`  
- Επιβεβαιώστε με:  
```cmd
foundry cache ls
```
  
Θα πρέπει να δείτε το όνομα του μεταγλωττισμένου μοντέλου σας (για παράδειγμα, `llama-3.2`).  

Βήμα 3) Εκτελέστε το μεταγλωττισμένο μοντέλο  
```cmd
foundry model run llama-3.2 --verbose
```
  
Σημειώσεις:  
- Βεβαιωθείτε ότι έχετε επαρκή δίσκο και RAM για τη μεταγλώττιση και την εκτέλεση  
- Ξεκινήστε με μικρότερα μοντέλα για να επικυρώσετε τη ροή, και στη συνέχεια κλιμακώστε  

## Μέρος 4: Πρακτική Επιμέλεια Μοντέλων (Βήμα-βήμα)

Βήμα 1) Δημιουργήστε ένα μητρώο `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Βήμα 2) Μικρό script επιλογής  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Μέρος 5: Πρακτικά Benchmarks Μοντέλων (Βήμα-βήμα)

Βήμα 1) Απλό benchmark καθυστέρησης  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Βήμα 2) Έλεγχος ποιότητας  
- Χρησιμοποιήστε ένα σταθερό σύνολο προτροπών, καταγράψτε εξόδους σε CSV/JSON  
- Αξιολογήστε χειροκίνητα τη ροή, τη συνάφεια και την ορθότητα (1–5)  

## Μέρος 6: Επόμενα Βήματα
- Εγγραφείτε στο Model Mondays για νέα μοντέλα και συμβουλές: https://aka.ms/model-mondays  
- Συνεισφέρετε ευρήματα στο `models.json` της ομάδας σας  
- Προετοιμαστείτε για τη Συνεδρία 4: σύγκριση LLMs vs SLMs, τοπική vs cloud επεξεργασία, και πρακτικές επιδείξεις  

---

