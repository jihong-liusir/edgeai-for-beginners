<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T19:21:19+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "el"
}
-->
# Συνεδρία 4: Σύγχρονα Μοντέλα – LLMs, SLMs και Ενσωμάτωση Συσκευών

## Επισκόπηση

Συγκρίνουμε LLMs και SLMs, αξιολογούμε τα πλεονεκτήματα και μειονεκτήματα της τοπικής και cloud επεξεργασίας, και υλοποιούμε demos που αναδεικνύουν σενάρια EdgeAI χρησιμοποιώντας Phi και ONNX Runtime. Επίσης, θα αναφερθούμε στο Chainlit RAG, τις επιλογές επεξεργασίας με WebGPU και την ενσωμάτωση του Open WebUI.

Αναφορές:
- Foundry Local docs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI how-to (chat app with Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Στόχοι Μάθησης
- Κατανόηση των πλεονεκτημάτων και μειονεκτημάτων LLMs και SLMs σε κόστος, καθυστέρηση και ακρίβεια
- Επιλογή μεταξύ τοπικής και cloud επεξεργασίας για συγκεκριμένες επιχειρηματικές ανάγκες
- Υλοποίηση ενός μικρού RAG demo με Chainlit
- Εξερεύνηση του WebGPU για επιτάχυνση στον browser
- Σύνδεση του Open WebUI με το Foundry Local

## Μέρος 1: LLM vs SLM – Πίνακας Αποφάσεων

Σκεφτείτε:
- Καθυστέρηση: Τα SLMs στη συσκευή συχνά προσφέρουν απαντήσεις σε λιγότερο από ένα δευτερόλεπτο
- Κόστος: Η τοπική επεξεργασία μειώνει τα κόστη cloud
- Ιδιωτικότητα: Τα ευαίσθητα δεδομένα παραμένουν στη συσκευή
- Δυνατότητες: Τα LLMs μπορεί να υπερέχουν σε πιο σύνθετες εργασίες
- Αξιοπιστία: Οι υβριδικές στρατηγικές μειώνουν τον κίνδυνο διακοπών

## Μέρος 2: Τοπική vs Cloud – Υβριδικά Μοτίβα

- Τοπική επεξεργασία πρώτα με fallback στο cloud για μεγάλα/σύνθετα ερωτήματα
- Cloud πρώτα με τοπική επεξεργασία για σενάρια που απαιτούν ιδιωτικότητα ή offline λειτουργία
- Δρομολόγηση ανά τύπο εργασίας (π.χ. code-gen στο DeepSeek, γενική συνομιλία στο Phi/Qwen)

## Μέρος 3: RAG Chat App με Chainlit (Μίνιμαλ)

Εγκαταστήστε εξαρτήσεις:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Εκτέλεση:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Επέκταση: προσθέστε έναν απλό retriever (τοπικά αρχεία) και προσαρμόστε το περιεχόμενο που ανακτήθηκε στην ερώτηση του χρήστη.

## Μέρος 4: Επεξεργασία με WebGPU (Heads-up)

Εκτελέστε μικρά μοντέλα απευθείας στον browser χρησιμοποιώντας WebGPU. Ιδανικό για demos που δίνουν προτεραιότητα στην ιδιωτικότητα και εμπειρίες χωρίς εγκατάσταση. Ακολουθεί ένα μίνιμαλ παράδειγμα βήμα προς βήμα με ONNX Runtime Web και τον WebGPU execution provider.

1) Έλεγχος υποστήριξης WebGPU
- Περιηγητές Chromium: chrome://gpu → επιβεβαιώστε ότι το “WebGPU” είναι ενεργοποιημένο
- Προγραμματικός έλεγχος (θα το ελέγξουμε και στον κώδικα): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) Δημιουργία μίνιμαλ project
Δημιουργήστε έναν φάκελο και δύο αρχεία: `index.html` και `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Τοπική εξυπηρέτηση (Windows cmd.exe)
Χρησιμοποιήστε έναν απλό static server ώστε ο browser να μπορεί να ανακτήσει το μοντέλο.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Ανοίξτε το http://localhost:5173 στον browser σας. Θα πρέπει να δείτε logs αρχικοποίησης, δημιουργία session με WebGPU και μια πρόβλεψη argmax.

4) Αντιμετώπιση προβλημάτων
- Αν το WebGPU δεν είναι διαθέσιμο: ενημερώστε το Chrome/Edge και βεβαιωθείτε ότι οι drivers της GPU είναι ενημερωμένοι, και ελέγξτε το chrome://flags για “Enable WebGPU”.
- Αν υπάρχουν σφάλματα CORS ή fetch: βεβαιωθείτε ότι εξυπηρετείτε αρχεία μέσω http:// (όχι file://) και ότι το URL του μοντέλου επιτρέπει cross-origin requests.
- Εναλλακτική CPU: αλλάξτε `executionProviders: ['wasm']` για να επαληθεύσετε τη βασική λειτουργία.

5) Επόμενα βήματα
- Αντικαταστήστε με ένα μοντέλο ONNX ειδικό για τον τομέα (π.χ. ταξινόμηση εικόνων ή ένα μικρό μοντέλο κειμένου).
- Προσθέστε λογική προεπεξεργασίας/μεταεπεξεργασίας για πραγματικά δεδομένα.
- Για μεγαλύτερα μοντέλα ή παραγωγική καθυστέρηση, προτιμήστε το Foundry Local ή τον ONNX Runtime Server.

## Μέρος 5: Open WebUI + Foundry Local (Βήμα προς βήμα)

Αυτό συνδέει το Open WebUI με το OpenAI-compatible endpoint του Foundry Local για ένα τοπικό chat UI.

1) Προαπαιτούμενα
- Εγκατεστημένο και λειτουργικό Foundry Local (`foundry --version`)
- Ένα μοντέλο έτοιμο για τοπική εκτέλεση (π.χ. `phi-4-mini`)
- Εγκατεστημένο Docker Desktop (συνιστάται για Open WebUI)

2) Εκκίνηση μοντέλου με Foundry Local
```powershell
foundry model run phi-4-mini
```
Αυτό εκθέτει ένα OpenAI-compatible API στο `http://localhost:8000`.

3) Εκκίνηση Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Σημειώσεις:
- Στα Windows, το `host.docker.internal` επιτρέπει στο container να φτάσει τον host στο `localhost`.
- Ορίζουμε το `OPENAI_API_BASE_URL` στο endpoint του Foundry Local και ένα dummy `OPENAI_API_KEY`.

4) Ρύθμιση από το UI του Open WebUI (εναλλακτική)
- Περιηγηθείτε στο http://localhost:3000
- Ολοκληρώστε την αρχική ρύθμιση (admin user)
- Μεταβείτε στις Ρυθμίσεις → Models/Providers
- Ορίστε Base URL: `http://host.docker.internal:8000/v1`
- Ορίστε API Key: `local-key` (placeholder)
- Αποθήκευση

5) Δοκιμή ερωτήματος
- Στο Open WebUI chat, επιλέξτε ή εισάγετε το όνομα του μοντέλου `phi-4-mini`
- Ερώτημα: “Αναφέρετε πέντε πλεονεκτήματα της επεξεργασίας AI στη συσκευή.”
- Θα πρέπει να δείτε μια απάντηση που μεταδίδεται από το τοπικό μοντέλο σας.

6) Αντιμετώπιση προβλημάτων
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Προαιρετικό: Επίμονη αποθήκευση δεδομένων Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Λίστα Ελέγχου Hands-On
- [ ] Σύγκριση απαντήσεων/καθυστέρησης μεταξύ SLM και LLM τοπικά
- [ ] Εκτέλεση του Chainlit demo με τουλάχιστον δύο μοντέλα
- [ ] Σύνδεση του Open WebUI με το τοπικό endpoint σας και δοκιμή

## Επόμενα Βήματα
- Προετοιμασία για workflows πρακτόρων στη Συνεδρία 5
- Αναγνώριση σεναρίων όπου η υβριδική τοπική/cloud επεξεργασία βελτιώνει το ROI

---

