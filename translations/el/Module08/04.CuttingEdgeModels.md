<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:25:03+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "el"
}
-->
# Συνεδρία 4: Δημιουργία Εφαρμογών Συνομιλίας Παραγωγής με Chainlit

## Επισκόπηση

Αυτή η συνεδρία επικεντρώνεται στη δημιουργία εφαρμογών συνομιλίας έτοιμων για παραγωγή χρησιμοποιώντας το Chainlit και το Microsoft Foundry Local. Θα μάθετε να δημιουργείτε σύγχρονες διεπαφές ιστού για συνομιλίες AI, να υλοποιείτε αποκρίσεις ροής και να αναπτύσσετε ισχυρές εφαρμογές συνομιλίας με σωστή διαχείριση σφαλμάτων και σχεδιασμό εμπειρίας χρήστη.

**Τι θα δημιουργήσετε:**
- **Chainlit Chat App**: Σύγχρονη διεπαφή ιστού με αποκρίσεις ροής
- **WebGPU Demo**: Επεξεργασία στον περιηγητή για εφαρμογές με προτεραιότητα στην ιδιωτικότητα  
- **Open WebUI Integration**: Επαγγελματική διεπαφή συνομιλίας με Foundry Local
- **Παραγωγικά Μοτίβα**: Διαχείριση σφαλμάτων, παρακολούθηση και στρατηγικές ανάπτυξης

## Στόχοι Μάθησης

- Δημιουργία εφαρμογών συνομιλίας έτοιμων για παραγωγή με Chainlit
- Υλοποίηση αποκρίσεων ροής για βελτιωμένη εμπειρία χρήστη
- Εξοικείωση με μοτίβα ενσωμάτωσης του Foundry Local SDK
- Εφαρμογή σωστής διαχείρισης σφαλμάτων και ομαλής υποβάθμισης
- Ανάπτυξη και διαμόρφωση εφαρμογών συνομιλίας για διαφορετικά περιβάλλοντα
- Κατανόηση σύγχρονων μοτίβων διεπαφής ιστού για συνομιλητική AI

## Προαπαιτούμενα

- **Foundry Local**: Εγκατεστημένο και σε λειτουργία ([Οδηγός Εγκατάστασης](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Έκδοση 3.10 ή νεότερη με δυνατότητα εικονικού περιβάλλοντος
- **Μοντέλο**: Τουλάχιστον ένα φορτωμένο μοντέλο (`foundry model run phi-4-mini`)
- **Περιηγητής**: Σύγχρονος περιηγητής ιστού με υποστήριξη WebGPU (Chrome/Edge)
- **Docker**: Για ενσωμάτωση Open WebUI (προαιρετικό)

## Μέρος 1: Κατανόηση Σύγχρονων Εφαρμογών Συνομιλίας

### Επισκόπηση Αρχιτεκτονικής

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Βασικές Τεχνολογίες

**Μοτίβα Foundry Local SDK:**
- `FoundryLocalManager(alias)`: Αυτόματη διαχείριση υπηρεσιών
- `manager.endpoint` και `manager.api_key`: Λεπτομέρειες σύνδεσης
- `manager.get_model_info(alias).id`: Αναγνώριση μοντέλου

**Πλαίσιο Chainlit:**
- `@cl.on_chat_start`: Αρχικοποίηση συνεδριών συνομιλίας
- `@cl.on_message`: Διαχείριση εισερχόμενων μηνυμάτων χρήστη  
- `cl.Message().stream_token()`: Ροή σε πραγματικό χρόνο
- Αυτόματη δημιουργία διεπαφής και διαχείριση WebSocket

## Μέρος 2: Τοπική vs Cloud Απόφαση

### Χαρακτηριστικά Απόδοσης

| Πτυχή | Τοπικό (Foundry) | Cloud (Azure OpenAI) |
|-------|------------------|----------------------|
| **Καθυστέρηση** | 🚀 50-200ms (χωρίς δίκτυο) | ⏱️ 200-2000ms (εξαρτάται από το δίκτυο) |
| **Ιδιωτικότητα** | 🔒 Τα δεδομένα δεν φεύγουν από τη συσκευή | ⚠️ Τα δεδομένα αποστέλλονται στο cloud |
| **Κόστος** | 💰 Δωρεάν μετά το υλικό | 💸 Χρέωση ανά token |
| **Εκτός σύνδεσης** | ✅ Λειτουργεί χωρίς internet | ❌ Απαιτεί internet |
| **Μέγεθος Μοντέλου** | ⚠️ Περιορισμένο από το υλικό | ✅ Πρόσβαση στα μεγαλύτερα μοντέλα |
| **Κλιμάκωση** | ⚠️ Εξαρτάται από το υλικό | ✅ Απεριόριστη κλιμάκωση |

### Μοτίβα Υβριδικής Στρατηγικής

**Τοπικό Πρώτα με Εναλλακτική:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Δρομολόγηση Βάσει Εργασίας:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Μέρος 3: Δείγμα 04 - Εφαρμογή Συνομιλίας Chainlit

### Γρήγορη Εκκίνηση

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Η εφαρμογή ανοίγει αυτόματα στο `http://localhost:8080` με μια σύγχρονη διεπαφή συνομιλίας.

### Βασική Υλοποίηση

Η εφαρμογή Δείγμα 04 δείχνει μοτίβα έτοιμα για παραγωγή:

**Αυτόματη Ανακάλυψη Υπηρεσιών:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Διαχείριση Ροής Συνομιλίας:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Επιλογές Διαμόρφωσης

**Μεταβλητές Περιβάλλοντος:**

| Μεταβλητή | Περιγραφή | Προεπιλογή | Παράδειγμα |
|-----------|-----------|------------|------------|
| `MODEL` | Ψευδώνυμο μοντέλου προς χρήση | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint του Foundry Local | Αυτόματη ανίχνευση | `http://localhost:51211` |
| `API_KEY` | Κλειδί API (προαιρετικό για τοπικό) | `""` | `your-api-key` |

**Προχωρημένη Χρήση:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Μέρος 4: Δημιουργία και Χρήση Jupyter Notebooks

### Επισκόπηση Υποστήριξης Notebook

Το Δείγμα 04 περιλαμβάνει ένα ολοκληρωμένο Jupyter notebook (`chainlit_app.ipynb`) που παρέχει:

- **📚 Εκπαιδευτικό Υλικό**: Βήμα προς βήμα οδηγίες
- **🔬 Διαδραστική Εξερεύνηση**: Εκτέλεση και πειραματισμός με κελιά κώδικα
- **📊 Οπτικές Επιδείξεις**: Γραφήματα, διαγράμματα και οπτικοποίηση αποτελεσμάτων
- **🛠️ Εργαλεία Ανάπτυξης**: Δοκιμή και αποσφαλμάτωση

### Δημιουργία Δικών σας Notebooks

#### Βήμα 1: Ρύθμιση Περιβάλλοντος Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Βήμα 2: Δημιουργία Νέου Notebook

**Χρήση VS Code:**
1. Ανοίξτε το VS Code στον κατάλογο Module08
2. Δημιουργήστε ένα νέο αρχείο με κατάληξη `.ipynb`
3. Επιλέξτε τον πυρήνα "Foundry Local" όταν σας ζητηθεί
4. Ξεκινήστε να προσθέτετε κελιά με το περιεχόμενό σας

**Χρήση Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Βέλτιστες Πρακτικές Δομής Notebook

#### Οργάνωση Κελιών

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Διαδραστικά Παραδείγματα και Ασκήσεις

#### Άσκηση 1: Δοκιμή Διαμόρφωσης Πελάτη

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Άσκηση 2: Προσομοίωση Απόκρισης Ροής

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Μέρος 5: Επίδειξη Επεξεργασίας Περιηγητή WebGPU

### Επισκόπηση

Το WebGPU επιτρέπει την εκτέλεση μοντέλων AI απευθείας στον περιηγητή για μέγιστη ιδιωτικότητα και εμπειρίες χωρίς εγκατάσταση. Αυτό το δείγμα δείχνει την εκτέλεση ONNX Runtime Web με WebGPU.

### Βήμα 1: Έλεγχος Υποστήριξης WebGPU

**Απαιτήσεις Περιηγητή:**
- Chrome/Edge 113+ με ενεργοποιημένο WebGPU
- Έλεγχος: `chrome://gpu` → επιβεβαιώστε την κατάσταση "WebGPU"
- Προγραμματικός έλεγχος: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Βήμα 2: Δημιουργία Επίδειξης WebGPU

Δημιουργήστε κατάλογο: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Βήμα 3: Εκτέλεση Επίδειξης

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Μέρος 6: Ενσωμάτωση Open WebUI

### Επισκόπηση

Το Open WebUI παρέχει μια επαγγελματική διεπαφή τύπου ChatGPT που συνδέεται με το OpenAI-compatible API του Foundry Local.

### Βήμα 1: Προαπαιτούμενα

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Βήμα 2: Ρύθμιση Docker (Συνιστάται)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Σημείωση:** Το `host.docker.internal` επιτρέπει στα κοντέινερ Docker να έχουν πρόσβαση στον υπολογιστή υποδοχής στα Windows.

### Βήμα 3: Διαμόρφωση

1. **Άνοιγμα Περιηγητή:** Μεταβείτε στο `http://localhost:3000`
2. **Αρχική Ρύθμιση:** Δημιουργήστε λογαριασμό διαχειριστή
3. **Διαμόρφωση Μοντέλου:**
   - Ρυθμίσεις → Μοντέλα → OpenAI API  
   - Βασική Διεύθυνση URL: `http://host.docker.internal:51211/v1`
   - Κλειδί API: `foundry-local-key` (οποιαδήποτε τιμή λειτουργεί)
4. **Δοκιμή Σύνδεσης:** Τα μοντέλα θα πρέπει να εμφανίζονται στο αναπτυσσόμενο μενού

### Επίλυση Προβλημάτων

**Συνηθισμένα Θέματα:**

1. **Αρνηση Σύνδεσης:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Μοντέλα Δεν Εμφανίζονται:**
   - Επαληθεύστε ότι το μοντέλο είναι φορτωμένο: `foundry model list`
   - Ελέγξτε την απόκριση API: `curl http://localhost:51211/v1/models`
   - Επανεκκινήστε το κοντέινερ Open WebUI

## Μέρος 7: Σκέψεις Ανάπτυξης Παραγωγής

### Διαμόρφωση Περιβάλλοντος

**Ρύθμιση Ανάπτυξης:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Ανάπτυξη Παραγωγής:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Συνηθισμένα Θέματα Θυρών και Λύσεις

**Πρόληψη Σύγκρουσης Θύρας 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Παρακολούθηση Απόδοσης

**Υλοποίηση Ελέγχου Υγείας:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Περίληψη

Η Συνεδρία 4 κάλυψε τη δημιουργία εφαρμογών Chainlit έτοιμων για παραγωγή για συνομιλητική AI. Μάθατε για:

- ✅ **Πλαίσιο Chainlit**: Σύγχρονη διεπαφή και υποστήριξη ροής για εφαρμογές συνομιλίας
- ✅ **Ενσωμάτωση Foundry Local**: Χρήση SDK και μοτίβα διαμόρφωσης  
- ✅ **Επεξεργασία WebGPU**: AI στον περιηγητή για μέγιστη ιδιωτικότητα
- ✅ **Ρύθμιση Open WebUI**: Ανάπτυξη επαγγελματικής διεπαφής συνομιλίας
- ✅ **Παραγωγικά Μοτίβα**: Διαχείριση σφαλμάτων, παρακολούθηση και κλιμάκωση

Η εφαρμογή Δείγμα 04 δείχνει βέλτιστες πρακτικές για τη δημιουργία ισχυρών διεπαφών συνομιλίας που αξιοποιούν τοπικά μοντέλα AI μέσω του Microsoft Foundry Local, παρέχοντας εξαιρετικές εμπειρίες χρήστη.

## Αναφορές

- **[Δείγμα 04: Εφαρμογή Chainlit](samples/04/README.md)**: Πλήρης εφαρμογή με τεκμηρίωση
- **[Εκπαιδευτικό Notebook Chainlit](samples/04/chainlit_app.ipynb)**: Διαδραστικό εκπαιδευτικό υλικό
- **[Τεκμηρίωση Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Πλήρης τεκμηρίωση πλατφόρμας
- **[Τεκμηρίωση Chainlit](https://docs.chainlit.io/)**: Επίσημη τεκμηρίωση πλαισίου
- **[Οδηγός Ενσωμάτωσης Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Επίσημο σεμινάριο

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.