<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T08:15:46+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "el"
}
-->
# Ενότητα 4: Ανάπτυξη - Υλοποίηση Μοντέλου Έτοιμου για Παραγωγή

## Επισκόπηση

Αυτός ο αναλυτικός οδηγός θα σας καθοδηγήσει στη διαδικασία ανάπτυξης βελτιστοποιημένων μοντέλων με ποσοτικοποίηση χρησιμοποιώντας το Foundry Local. Θα καλύψουμε τη μετατροπή μοντέλου, τη βελτιστοποίηση ποσοτικοποίησης και τη διαμόρφωση ανάπτυξης από την αρχή μέχρι το τέλος.

## Προαπαιτούμενα

Πριν ξεκινήσετε, βεβαιωθείτε ότι έχετε τα εξής:

- ✅ Ένα βελτιστοποιημένο μοντέλο onnx έτοιμο για ανάπτυξη
- ✅ Υπολογιστή με Windows ή Mac
- ✅ Python 3.10 ή νεότερη έκδοση
- ✅ Τουλάχιστον 8GB διαθέσιμης RAM
- ✅ Το Foundry Local εγκατεστημένο στο σύστημά σας

## Μέρος 1: Ρύθμιση Περιβάλλοντος

### Εγκατάσταση Απαραίτητων Εργαλείων

Ανοίξτε το τερματικό σας (Command Prompt στα Windows, Terminal στο Mac) και εκτελέστε τις παρακάτω εντολές με τη σειρά:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **Σημαντική Σημείωση**: Θα χρειαστείτε επίσης την έκδοση CMake 3.31 ή νεότερη, την οποία μπορείτε να κατεβάσετε από [cmake.org](https://cmake.org/download/).

## Μέρος 2: Μετατροπή και Ποσοτικοποίηση Μοντέλου

### Επιλογή Κατάλληλης Μορφής

Για μικρά γλωσσικά μοντέλα που έχουν βελτιστοποιηθεί, προτείνουμε τη χρήση της μορφής **ONNX**, καθώς προσφέρει:

- 🚀 Καλύτερη βελτιστοποίηση απόδοσης
- 🔧 Ανάπτυξη ανεξάρτητη από το υλικό
- 🏭 Δυνατότητες έτοιμες για παραγωγή
- 📱 Συμβατότητα μεταξύ πλατφορμών

### Μέθοδος 1: Μετατροπή με Μία Εντολή (Συνιστάται)

Χρησιμοποιήστε την παρακάτω εντολή για να μετατρέψετε απευθείας το βελτιστοποιημένο μοντέλο σας:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Επεξήγηση Παραμέτρων:**
- `--model_name_or_path`: Διαδρομή προς το βελτιστοποιημένο μοντέλο σας
- `--device cpu`: Χρήση CPU για βελτιστοποίηση
- `--precision int4`: Χρήση ποσοτικοποίησης INT4 (περίπου 75% μείωση μεγέθους)
- `--output_path`: Διαδρομή εξόδου για το μετατρεπόμενο μοντέλο

### Μέθοδος 2: Προσέγγιση με Αρχείο Διαμόρφωσης (Για Προχωρημένους Χρήστες)

Δημιουργήστε ένα αρχείο διαμόρφωσης με όνομα `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Στη συνέχεια, εκτελέστε:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Σύγκριση Επιλογών Ποσοτικοποίησης

| Ακρίβεια | Μέγεθος Αρχείου | Ταχύτητα Επεξεργασίας | Ποιότητα Μοντέλου | Συνιστώμενη Χρήση |
|----------|-----------------|-----------------------|-------------------|-------------------|
| FP16     | Βασικό × 0.5    | Γρήγορη              | Καλύτερη          | Υψηλής ποιότητας υλικό |
| INT8     | Βασικό × 0.25   | Πολύ Γρήγορη         | Καλή              | Ισορροπημένη επιλογή |
| INT4     | Βασικό × 0.125  | Ταχύτερη             | Αποδεκτή          | Περιορισμένοι πόροι |

💡 **Σύσταση**: Ξεκινήστε με ποσοτικοποίηση INT4 για την πρώτη σας ανάπτυξη. Αν η ποιότητα δεν είναι ικανοποιητική, δοκιμάστε INT8 ή FP16.

## Μέρος 3: Διαμόρφωση Ανάπτυξης Foundry Local

### Δημιουργία Διαμόρφωσης Μοντέλου

Μεταβείτε στον κατάλογο μοντέλων του Foundry Local:

```bash
foundry cache cd ./models/
```

Δημιουργήστε τη δομή καταλόγου του μοντέλου σας:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Δημιουργήστε το αρχείο διαμόρφωσης `inference_model.json` στον κατάλογο του μοντέλου σας:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Πρότυπες Διαμορφώσεις για Συγκεκριμένα Μοντέλα

#### Για Μοντέλα Σειράς Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Μέρος 4: Δοκιμή και Βελτιστοποίηση Μοντέλου

### Επαλήθευση Εγκατάστασης Μοντέλου

Ελέγξτε αν το Foundry Local μπορεί να αναγνωρίσει το μοντέλο σας:

```bash
foundry cache ls
```

Θα πρέπει να δείτε το `your-finetuned-model-int4` στη λίστα.

### Έναρξη Δοκιμής Μοντέλου

```bash
foundry model run your-finetuned-model-int4
```

### Αξιολόγηση Απόδοσης

Παρακολουθήστε βασικούς δείκτες κατά τη δοκιμή:

1. **Χρόνος Απόκρισης**: Μετρήστε τον μέσο χρόνο ανά απόκριση
2. **Χρήση Μνήμης**: Παρακολουθήστε την κατανάλωση RAM
3. **Χρήση CPU**: Ελέγξτε το φορτίο του επεξεργαστή
4. **Ποιότητα Εξόδου**: Αξιολογήστε τη συνάφεια και τη συνοχή των απαντήσεων

### Λίστα Ελέγχου Επικύρωσης Ποιότητας

- ✅ Το μοντέλο ανταποκρίνεται κατάλληλα σε ερωτήματα του βελτιστοποιημένου τομέα
- ✅ Η μορφή των απαντήσεων ταιριάζει με την αναμενόμενη δομή εξόδου
- ✅ Δεν υπάρχουν διαρροές μνήμης κατά τη διάρκεια παρατεταμένης χρήσης
- ✅ Σταθερή απόδοση σε διαφορετικά μήκη εισόδου
- ✅ Σωστή διαχείριση ακραίων περιπτώσεων και μη έγκυρων εισόδων

## Περίληψη

Συγχαρητήρια! Ολοκληρώσατε με επιτυχία:

- ✅ Μετατροπή μορφής βελτιστοποιημένου μοντέλου
- ✅ Βελτιστοποίηση ποσοτικοποίησης μοντέλου
- ✅ Διαμόρφωση ανάπτυξης Foundry Local
- ✅ Ρύθμιση απόδοσης και αντιμετώπιση προβλημάτων

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.