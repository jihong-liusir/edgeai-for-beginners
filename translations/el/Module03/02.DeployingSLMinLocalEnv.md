<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-18T08:19:17+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "el"
}
-->
# Ενότητα 2: Ανάπτυξη Τοπικού Περιβάλλοντος - Λύσεις με Προτεραιότητα στην Ιδιωτικότητα

Η τοπική ανάπτυξη Μικρών Γλωσσικών Μοντέλων (SLMs) αντιπροσωπεύει μια αλλαγή παραδείγματος προς λύσεις AI που διατηρούν την ιδιωτικότητα και είναι οικονομικά αποδοτικές. Αυτός ο ολοκληρωμένος οδηγός εξερευνά δύο ισχυρά πλαίσια—Ollama και Microsoft Foundry Local—που επιτρέπουν στους προγραμματιστές να αξιοποιήσουν πλήρως τις δυνατότητες των SLMs, διατηρώντας παράλληλα τον πλήρη έλεγχο του περιβάλλοντος ανάπτυξης.

## Εισαγωγή

Σε αυτό το μάθημα, θα εξερευνήσουμε προηγμένες στρατηγικές ανάπτυξης για Μικρά Γλωσσικά Μοντέλα σε τοπικά περιβάλλοντα. Θα καλύψουμε τις βασικές έννοιες της τοπικής ανάπτυξης AI, θα εξετάσουμε δύο κορυφαίες πλατφόρμες (Ollama και Microsoft Foundry Local) και θα παρέχουμε πρακτικές οδηγίες για λύσεις έτοιμες για παραγωγή.

## Στόχοι Μάθησης

Μέχρι το τέλος αυτού του μαθήματος, θα μπορείτε να:

- Κατανοήσετε την αρχιτεκτονική και τα πλεονεκτήματα των πλαισίων τοπικής ανάπτυξης SLM.
- Υλοποιήσετε αναπτύξεις έτοιμες για παραγωγή χρησιμοποιώντας Ollama και Microsoft Foundry Local.
- Συγκρίνετε και επιλέξετε την κατάλληλη πλατφόρμα βάσει συγκεκριμένων απαιτήσεων και περιορισμών.
- Βελτιστοποιήσετε τις τοπικές αναπτύξεις για απόδοση, ασφάλεια και επεκτασιμότητα.

## Κατανόηση Αρχιτεκτονικών Τοπικής Ανάπτυξης SLM

Η τοπική ανάπτυξη SLM αντιπροσωπεύει μια θεμελιώδη αλλαγή από τις υπηρεσίες AI που εξαρτώνται από το cloud σε λύσεις που διατηρούν την ιδιωτικότητα και λειτουργούν εντός της επιχείρησης. Αυτή η προσέγγιση επιτρέπει στους οργανισμούς να διατηρούν πλήρη έλεγχο της υποδομής AI τους, εξασφαλίζοντας παράλληλα την κυριαρχία των δεδομένων και την επιχειρησιακή ανεξαρτησία.

### Κατηγορίες Πλαισίων Ανάπτυξης

Η κατανόηση διαφορετικών προσεγγίσεων ανάπτυξης βοηθά στην επιλογή της σωστής στρατηγικής για συγκεκριμένες περιπτώσεις χρήσης:

- **Εστιασμένα στην Ανάπτυξη**: Απλοποιημένη εγκατάσταση για πειραματισμό και πρωτότυπα.
- **Επιχειρησιακού Επιπέδου**: Λύσεις έτοιμες για παραγωγή με δυνατότητες ενσωμάτωσης σε επιχειρήσεις.
- **Δια-Πλατφορμικές**: Καθολική συμβατότητα σε διαφορετικά λειτουργικά συστήματα και υλικό.

### Βασικά Πλεονεκτήματα Τοπικής Ανάπτυξης SLM

Η τοπική ανάπτυξη SLM προσφέρει αρκετά θεμελιώδη πλεονεκτήματα που την καθιστούν ιδανική για εφαρμογές ευαίσθητες στην ιδιωτικότητα και επιχειρησιακές εφαρμογές:

**Ιδιωτικότητα και Ασφάλεια**: Η τοπική επεξεργασία εξασφαλίζει ότι ευαίσθητα δεδομένα δεν εγκαταλείπουν την υποδομή του οργανισμού, επιτρέποντας τη συμμόρφωση με GDPR, HIPAA και άλλες κανονιστικές απαιτήσεις. Οι απομονωμένες αναπτύξεις είναι δυνατές για απόρρητα περιβάλλοντα, ενώ τα πλήρη ίχνη ελέγχου διατηρούν την εποπτεία της ασφάλειας.

**Οικονομική Αποδοτικότητα**: Η εξάλειψη των μοντέλων τιμολόγησης ανά token μειώνει σημαντικά τα λειτουργικά κόστη. Οι χαμηλότερες απαιτήσεις εύρους ζώνης και η μειωμένη εξάρτηση από το cloud παρέχουν προβλέψιμες δομές κόστους για τον προϋπολογισμό των επιχειρήσεων.

**Απόδοση και Αξιοπιστία**: Ταχύτεροι χρόνοι συμπερασμάτων χωρίς καθυστερήσεις δικτύου επιτρέπουν εφαρμογές σε πραγματικό χρόνο. Η λειτουργικότητα εκτός σύνδεσης εξασφαλίζει συνεχή λειτουργία ανεξάρτητα από τη συνδεσιμότητα στο διαδίκτυο, ενώ η τοπική βελτιστοποίηση πόρων παρέχει σταθερή απόδοση.

## Ollama: Καθολική Πλατφόρμα Τοπικής Ανάπτυξης

### Βασική Αρχιτεκτονική και Φιλοσοφία

Το Ollama έχει σχεδιαστεί ως μια καθολική, φιλική προς τον προγραμματιστή πλατφόρμα που εκδημοκρατίζει την τοπική ανάπτυξη LLM σε διάφορες διαμορφώσεις υλικού και λειτουργικά συστήματα.

**Τεχνική Βάση**: Χτισμένο στο ισχυρό πλαίσιο llama.cpp, το Ollama χρησιμοποιεί τη βελτιστοποιημένη μορφή μοντέλου GGUF για βέλτιστη απόδοση. Η δια-πλατφορμική συμβατότητα εξασφαλίζει συνεπή συμπεριφορά σε περιβάλλοντα Windows, macOS και Linux, ενώ η έξυπνη διαχείριση πόρων βελτιστοποιεί τη χρήση CPU, GPU και μνήμης.

**Φιλοσοφία Σχεδιασμού**: Το Ollama δίνει προτεραιότητα στην απλότητα χωρίς να θυσιάζει τη λειτουργικότητα, προσφέροντας ανάπτυξη χωρίς ρυθμίσεις για άμεση παραγωγικότητα. Η πλατφόρμα διατηρεί ευρεία συμβατότητα μοντέλων, παρέχοντας συνεπείς APIs σε διαφορετικές αρχιτεκτονικές μοντέλων.

### Προηγμένα Χαρακτηριστικά και Δυνατότητες

**Εξαιρετική Διαχείριση Μοντέλων**: Το Ollama παρέχει ολοκληρωμένη διαχείριση κύκλου ζωής μοντέλων με αυτόματη λήψη, προσωρινή αποθήκευση και έκδοση. Η πλατφόρμα υποστηρίζει ένα εκτεταμένο οικοσύστημα μοντέλων, όπως Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral και εξειδικευμένα μοντέλα ενσωμάτωσης.

**Εξατομίκευση μέσω Modelfiles**: Οι προχωρημένοι χρήστες μπορούν να δημιουργήσουν προσαρμοσμένες διαμορφώσεις μοντέλων με συγκεκριμένες παραμέτρους, προτροπές συστήματος και τροποποιήσεις συμπεριφοράς. Αυτό επιτρέπει βελτιστοποιήσεις για συγκεκριμένους τομείς και εξειδικευμένες απαιτήσεις εφαρμογών.

**Βελτιστοποίηση Απόδοσης**: Το Ollama ανιχνεύει και χρησιμοποιεί αυτόματα διαθέσιμη επιτάχυνση υλικού, συμπεριλαμβανομένων των NVIDIA CUDA, Apple Metal και OpenCL. Η έξυπνη διαχείριση μνήμης εξασφαλίζει βέλτιστη χρήση πόρων σε διαφορετικές διαμορφώσεις υλικού.

### Στρατηγικές Υλοποίησης Παραγωγής

**Εγκατάσταση και Ρύθμιση**: Το Ollama παρέχει απλοποιημένη εγκατάσταση σε πλατφόρμες μέσω εγγενών εγκαταστάσεων, διαχειριστών πακέτων (WinGet, Homebrew, APT) και κοντέινερ Docker για κοντεϊνοποιημένες αναπτύξεις.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Βασικές Εντολές και Λειτουργίες**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Προηγμένη Διαμόρφωση**: Τα Modelfiles επιτρέπουν εξελιγμένη εξατομίκευση για επιχειρησιακές απαιτήσεις:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Παραδείγματα Ενσωμάτωσης Προγραμματιστών

**Ενσωμάτωση Python API**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Ενσωμάτωση JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Χρήση RESTful API με cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Βελτίωση Απόδοσης και Βελτιστοποίηση

**Διαμόρφωση Μνήμης και Νημάτων**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Επιλογή Ποσοτικοποίησης για Διαφορετικό Υλικό**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Πλατφόρμα AI για Επιχειρησιακό Edge

### Αρχιτεκτονική Επιχειρησιακού Επιπέδου

Το Microsoft Foundry Local αντιπροσωπεύει μια ολοκληρωμένη επιχειρησιακή λύση σχεδιασμένη ειδικά για αναπτύξεις edge AI παραγωγής με βαθιά ενσωμάτωση στο οικοσύστημα της Microsoft.

**Βάση ONNX**: Χτισμένο στο βιομηχανικό πρότυπο ONNX Runtime, το Foundry Local παρέχει βελτιστοποιημένη απόδοση σε διάφορες αρχιτεκτονικές υλικού. Η πλατφόρμα αξιοποιεί την ενσωμάτωση Windows ML για εγγενή βελτιστοποίηση στα Windows, διατηρώντας παράλληλα δια-πλατφορμική συμβατότητα.

**Εξαιρετική Επιτάχυνση Υλικού**: Το Foundry Local διαθέτει έξυπνη ανίχνευση και βελτιστοποίηση υλικού σε CPUs, GPUs και NPUs. Η βαθιά συνεργασία με προμηθευτές υλικού (AMD, Intel, NVIDIA, Qualcomm) εξασφαλίζει βέλτιστη απόδοση σε επιχειρησιακές διαμορφώσεις υλικού.

### Προηγμένη Εμπειρία Προγραμματιστών

**Πρόσβαση Πολλαπλών Διεπαφών**: Το Foundry Local παρέχει ολοκληρωμένες διεπαφές ανάπτυξης, συμπεριλαμβανομένου ενός ισχυρού CLI για διαχείριση και ανάπτυξη μοντέλων, SDKs πολλαπλών γλωσσών (Python, NodeJS) για εγγενή ενσωμάτωση και RESTful APIs με συμβατότητα OpenAI για απρόσκοπτη μετανάστευση.

**Ενσωμάτωση Visual Studio**: Η πλατφόρμα ενσωματώνεται απρόσκοπτα με το AI Toolkit για VS Code, παρέχοντας εργαλεία μετατροπής μοντέλων, ποσοτικοποίησης και βελτιστοποίησης εντός του περιβάλλοντος ανάπτυξης. Αυτή η ενσωμάτωση επιταχύνει τις ροές εργασίας ανάπτυξης και μειώνει την πολυπλοκότητα της ανάπτυξης.

**Προσαρμοσμένη Ροή Βελτιστοποίησης Μοντέλων**: Η ενσωμάτωση Microsoft Olive επιτρέπει εξελιγμένες ροές εργασίας βελτιστοποίησης μοντέλων, συμπεριλαμβανομένης της δυναμικής ποσοτικοποίησης, της βελτιστοποίησης γραφημάτων και της ρύθμισης ειδικής για το υλικό. Οι δυνατότητες μετατροπής μέσω cloud μέσω Azure ML παρέχουν επεκτάσιμη βελτιστοποίηση για μεγάλα μοντέλα.

### Στρατηγικές Υλοποίησης Παραγωγής

**Εγκατάσταση και Διαμόρφωση**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Λειτουργίες Διαχείρισης Μοντέλων**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Προηγμένη Διαμόρφωση Ανάπτυξης**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Ενσωμάτωση στο Επιχειρησιακό Οικοσύστημα

**Ασφάλεια και Συμμόρφωση**: Το Foundry Local παρέχει χαρακτηριστικά ασφάλειας επιχειρησιακού επιπέδου, όπως έλεγχο πρόσβασης βάσει ρόλων, καταγραφή ελέγχου, αναφορές συμμόρφωσης και κρυπτογραφημένη αποθήκευση μοντέλων. Η ενσωμάτωση με την υποδομή ασφάλειας της Microsoft εξασφαλίζει συμμόρφωση με τις πολιτικές ασφάλειας των επιχειρήσεων.

**Ενσωματωμένες Υπηρεσίες AI**: Η πλατφόρμα προσφέρει έτοιμες προς χρήση δυνατότητες AI, όπως το Phi Silica για τοπική επεξεργασία γλώσσας, το AI Imaging για βελτίωση και ανάλυση εικόνων, και εξειδικευμένα APIs για κοινές επιχειρησιακές εργασίες AI.

## Συγκριτική Ανάλυση: Ollama vs Foundry Local

### Σύγκριση Τεχνικής Αρχιτεκτονικής

| **Πτυχή** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Μορφή Μοντέλου** | GGUF (μέσω llama.cpp) | ONNX (μέσω ONNX Runtime) |
| **Εστίαση Πλατφόρμας** | Καθολική δια-πλατφορμική | Βελτιστοποίηση Windows/Επιχειρήσεων |
| **Ενσωμάτωση Υλικού** | Γενική υποστήριξη GPU/CPU | Βαθιά υποστήριξη Windows ML, NPU |
| **Βελτιστοποίηση** | Ποσοτικοποίηση llama.cpp | Microsoft Olive + ONNX Runtime |
| **Χαρακτηριστικά Επιχειρήσεων** | Κοινότητα | Επιχειρησιακού επιπέδου με SLAs |

### Χαρακτηριστικά Απόδοσης

**Πλεονεκτήματα Απόδοσης Ollama**:
- Εξαιρετική απόδοση CPU μέσω βελτιστοποίησης llama.cpp
- Συνεπής συμπεριφορά σε διαφορετικές πλατφόρμες και υλικό
- Αποτελεσματική χρήση μνήμης με έξυπνη φόρτωση μοντέλων
- Γρήγοροι χρόνοι εκκίνησης για ανάπτυξη και δοκιμές

**Πλεονεκτήματα Απόδοσης Foundry Local**:
- Ανώτερη χρήση NPU σε σύγχρονο υλικό Windows
- Βελτιστοποιημένη επιτάχυνση GPU μέσω συνεργασιών με προμηθευτές
- Παρακολούθηση και βελτιστοποίηση απόδοσης επιχειρησιακού επιπέδου
- Επεκτάσιμες δυνατότητες ανάπτυξης για περιβάλλοντα παραγωγής

### Ανάλυση Εμπειρίας Ανάπτυξης

**Εμπειρία Ανάπτυξης Ollama**:
- Ελάχιστες απαιτήσεις εγκατάστασης με άμεση παραγωγικότητα
- Διαισθητική διεπαφή γραμμής εντολών για όλες τις λειτουργίες
- Εκτεταμένη υποστήριξη κοινότητας και τεκμηρίωση
- Ευέλικτη εξατομίκευση μέσω Modelfiles

**Εμπειρία Ανάπτυξης Foundry Local**:
- Ολοκληρωμένη ενσωμάτωση IDE με το οικοσύστημα Visual Studio
- Ροές εργασίας ανάπτυξης επιχειρησιακού επιπέδου με δυνατότητες συνεργασίας ομάδας
- Επαγγελματικά κανάλια υποστήριξης με την υποστήριξη της Microsoft
- Προηγμένα εργαλεία αποσφαλμάτωσης και βελτιστοποίησης

### Βελτιστοποίηση Περιπτώσεων Χρήσης

**Επιλέξτε Ollama Όταν**:
- Αναπτύσσετε δια-πλατφορμικές εφαρμογές που απαιτούν συνεπή συμπεριφορά
- Δίνετε προτεραιότητα στη διαφάνεια ανοιχτού κώδικα και στις συνεισφορές της κοινότητας
- Εργάζεστε με περιορισμένους πόρους ή περιορισμούς προϋπολογισμού
- Δημιουργείτε πειραματικές ή ερευνητικές εφαρμογές
- Απαιτείτε ευρεία συμβατότητα μοντέλων σε διαφορετικές αρχιτεκτονικές

**Επιλέξτε Foundry Local Όταν**:
- Αναπτύσσετε επιχειρησιακές εφαρμογές με αυστηρές απαιτήσεις απόδοσης
- Αξιοποιείτε βελτιστοποιήσεις υλικού ειδικά για Windows (NPU, Windows ML)
- Απαιτείτε υποστήριξη επιχειρήσεων, SLAs και χαρακτηριστικά συμμόρφωσης
- Δημιουργείτε εφαρμογές παραγωγής με ενσωμάτωση στο οικοσύστημα της Microsoft
- Χρειάζεστε προηγμένα εργαλεία βελτιστοποίησης και επαγγελματικές ροές εργασίας ανάπτυξης

## Προηγμένες Στρατηγικές Ανάπτυξης

### Μοτίβα Ανάπτυξης με Κοντέινερ

**Κοντεϊνοποίηση Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Επιχειρησιακή Ανάπτυξη Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Τεχνικές Βελτιστοποίησης Απόδοσης

**Στρατηγικές Βελτιστοποίησης Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Βελτιστοποίηση Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Σκέψεις Ασφάλειας και Συμμόρφωσης

### Υλοποίηση Ασφάλειας Επιχειρήσεων

**Βέλτιστες Πρακτικές Ασφάλειας Ollama**

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.