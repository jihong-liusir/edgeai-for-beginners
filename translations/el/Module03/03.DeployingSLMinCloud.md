<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T08:27:01+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "el"
}
-->
# Ανάπτυξη Cloud με Containers - Λύσεις Κλίμακας Παραγωγής

Αυτός ο ολοκληρωμένος οδηγός καλύπτει τρεις βασικές προσεγγίσεις για την ανάπτυξη του μοντέλου Phi-4-mini-instruct της Microsoft σε περιβάλλοντα containers: vLLM, Ollama και SLM Engine με ONNX Runtime. Το μοντέλο αυτό, με 3.8 δισεκατομμύρια παραμέτρους, αποτελεί μια ιδανική επιλογή για εργασίες λογικής σκέψης, διατηρώντας παράλληλα την αποδοτικότητα για ανάπτυξη σε edge συσκευές.

## Πίνακας Περιεχομένων

1. [Εισαγωγή στην Ανάπτυξη Containers για το Phi-4-mini](../../../Module03)
2. [Στόχοι Μάθησης](../../../Module03)
3. [Κατανόηση της Κατηγοριοποίησης του Phi-4-mini](../../../Module03)
4. [Ανάπτυξη Container με vLLM](../../../Module03)
5. [Ανάπτυξη Container με Ollama](../../../Module03)
6. [SLM Engine με ONNX Runtime](../../../Module03)
7. [Πλαίσιο Σύγκρισης](../../../Module03)
8. [Βέλτιστες Πρακτικές](../../../Module03)

## Εισαγωγή στην Ανάπτυξη Containers για το Phi-4-mini

Τα Μικρά Μοντέλα Γλώσσας (SLMs) αποτελούν μια σημαντική πρόοδο στην EdgeAI, επιτρέποντας προηγμένες δυνατότητες επεξεργασίας φυσικής γλώσσας σε συσκευές με περιορισμένους πόρους. Αυτός ο οδηγός επικεντρώνεται στις στρατηγικές ανάπτυξης containers για το Phi-4-mini-instruct της Microsoft, ένα μοντέλο αιχμής που συνδυάζει δυνατότητες και αποδοτικότητα.

### Προτεινόμενο Μοντέλο: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 δισεκατομμύρια παράμετροι)**: Το τελευταίο ελαφρύ μοντέλο της Microsoft, προσαρμοσμένο για περιβάλλοντα με περιορισμένη μνήμη/υπολογιστική ισχύ, με εξαιρετικές δυνατότητες σε:
- **Μαθηματική λογική και σύνθετους υπολογισμούς**
- **Παραγωγή, αποσφαλμάτωση και ανάλυση κώδικα**
- **Λύση λογικών προβλημάτων και συλλογιστική βήμα προς βήμα**
- **Εκπαιδευτικές εφαρμογές που απαιτούν λεπτομερείς εξηγήσεις**
- **Κλήση λειτουργιών και ενσωμάτωση εργαλείων**

Ανήκοντας στην κατηγορία "Μικρά SLMs" (1.5B - 13.9B παράμετροι), το Phi-4-mini επιτυγχάνει την ιδανική ισορροπία μεταξύ δυνατοτήτων λογικής σκέψης και αποδοτικότητας πόρων.

### Οφέλη της Ανάπτυξης Containers για το Phi-4-mini

- **Λειτουργική Αποδοτικότητα**: Γρήγορη επεξεργασία για εργασίες λογικής σκέψης με χαμηλότερες απαιτήσεις υπολογιστικής ισχύος
- **Ευελιξία Ανάπτυξης**: Δυνατότητες AI σε συσκευές με αυξημένη ιδιωτικότητα μέσω τοπικής επεξεργασίας
- **Οικονομική Αποδοτικότητα**: Μειωμένο κόστος λειτουργίας σε σύγκριση με μεγαλύτερα μοντέλα, διατηρώντας την ποιότητα
- **Απομόνωση**: Καθαρός διαχωρισμός μεταξύ των περιπτώσεων μοντέλου και ασφαλών περιβαλλόντων εκτέλεσης
- **Κλιμακωσιμότητα**: Εύκολη οριζόντια κλιμάκωση για αυξημένη απόδοση λογικής σκέψης

## Στόχοι Μάθησης

Με την ολοκλήρωση αυτού του οδηγού, θα μπορείτε να:

- Αναπτύξετε και να βελτιστοποιήσετε το Phi-4-mini-instruct σε διάφορα περιβάλλοντα containers
- Εφαρμόσετε προηγμένες στρατηγικές ποσοτικοποίησης και συμπίεσης για διαφορετικά σενάρια ανάπτυξης
- Διαμορφώσετε containers έτοιμα για παραγωγή για εργασίες λογικής σκέψης
- Αξιολογήσετε και επιλέξετε κατάλληλα πλαίσια ανάπτυξης βάσει συγκεκριμένων απαιτήσεων χρήσης
- Εφαρμόσετε βέλτιστες πρακτικές ασφάλειας, παρακολούθησης και κλιμάκωσης για ανάπτυξη SLMs σε containers

## Κατανόηση της Κατηγοριοποίησης του Phi-4-mini

### Προδιαγραφές Μοντέλου

**Τεχνικές Λεπτομέρειες:**
- **Παράμετροι**: 3.8 δισεκατομμύρια (Κατηγορία Μικρών SLMs)
- **Αρχιτεκτονική**: Πυκνός Transformer μόνο αποκωδικοποιητής με grouped-query attention
- **Μήκος Συμφραζομένων**: 128K tokens (32K συνιστώνται για βέλτιστη απόδοση)
- **Λεξιλόγιο**: 200K tokens με πολυγλωσσική υποστήριξη
- **Δεδομένα Εκπαίδευσης**: 5T tokens υψηλής ποιότητας περιεχομένου με έμφαση στη λογική σκέψη

### Απαιτήσεις Πόρων

| Τύπος Ανάπτυξης | Ελάχιστη RAM | Συνιστώμενη RAM | VRAM (GPU) | Αποθήκευση | Τυπικές Χρήσεις |
|-----------------|-------------|-----------------|------------|------------|-----------------|
| **Ανάπτυξη** | 6GB | 8GB | - | 8GB | Τοπικές δοκιμές, πρωτότυπα |
| **Παραγωγή CPU** | 8GB | 12GB | - | 10GB | Edge servers, ανάπτυξη με χαμηλό κόστος |
| **Παραγωγή GPU** | 6GB | 8GB | 4-6GB | 8GB | Υπηρεσίες λογικής σκέψης υψηλής απόδοσης |
| **Βελτιστοποιημένο για Edge** | 4GB | 6GB | - | 6GB | Ποσοτικοποιημένη ανάπτυξη, IoT gateways |

### Δυνατότητες του Phi-4-mini

- **Μαθηματική Αριστεία**: Προηγμένη επίλυση προβλημάτων αριθμητικής, άλγεβρας και λογισμού
- **Ευφυΐα Κώδικα**: Παραγωγή κώδικα σε Python, JavaScript και πολλές γλώσσες με αποσφαλμάτωση
- **Λογική Σκέψη**: Ανάλυση προβλημάτων βήμα προς βήμα και κατασκευή λύσεων
- **Εκπαιδευτική Υποστήριξη**: Λεπτομερείς εξηγήσεις κατάλληλες για μάθηση και διδασκαλία
- **Κλήση Λειτουργιών**: Εγγενής υποστήριξη για ενσωμάτωση εργαλείων και αλληλεπιδράσεις API

## Ανάπτυξη Container με vLLM

Το vLLM προσφέρει εξαιρετική υποστήριξη για το Phi-4-mini-instruct με βελτιστοποιημένη απόδοση επεξεργασίας και APIs συμβατά με OpenAI, καθιστώντας το ιδανικό για υπηρεσίες λογικής σκέψης σε παραγωγή.

### Παραδείγματα Γρήγορης Εκκίνησης

#### Βασική Ανάπτυξη CPU (Ανάπτυξη)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Ανάπτυξη Παραγωγής με GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Διαμόρφωση Παραγωγής

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Δοκιμή Δυνατοτήτων Λογικής Σκέψης του Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ανάπτυξη Container με Ollama

Το Ollama προσφέρει εξαιρετική υποστήριξη για το Phi-4-mini-instruct με απλοποιημένη ανάπτυξη και διαχείριση, καθιστώντας το ιδανικό για ανάπτυξη και ισορροπημένες παραγωγικές εφαρμογές.

### Γρήγορη Ρύθμιση

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Διαμόρφωση Παραγωγής

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Βελτιστοποίηση Μοντέλου και Παραλλαγές

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Παραδείγματα Χρήσης API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine με ONNX Runtime

Το ONNX Runtime προσφέρει βέλτιστη απόδοση για ανάπτυξη σε edge συσκευές του Phi-4-mini-instruct με προηγμένη βελτιστοποίηση και συμβατότητα μεταξύ πλατφορμών.

### Βασική Ρύθμιση

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Απλοποιημένη Υλοποίηση Server

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Σενάριο Μετατροπής Μοντέλου

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Διαμόρφωση Παραγωγής

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Δοκιμή Ανάπτυξης ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Πλαίσιο Σύγκρισης

### Σύγκριση Πλαισίων για το Phi-4-mini

| Χαρακτηριστικό | vLLM | Ollama | ONNX Runtime |
|----------------|------|--------|--------------|
| **Πολυπλοκότητα Ρύθμισης** | Μέτρια | Εύκολη | Πολύπλοκη |
| **Απόδοση (GPU)** | Εξαιρετική (~25 tok/s) | Πολύ Καλή (~20 tok/s) | Καλή (~15 tok/s) |
| **Απόδοση (CPU)** | Καλή (~8 tok/s) | Πολύ Καλή (~12 tok/s) | Εξαιρετική (~15 tok/s) |
| **Χρήση Μνήμης** | 8-12GB | 6-10GB | 4-8GB |
| **Συμβατότητα API** | Συμβατό με OpenAI | Προσαρμοσμένο REST | Προσαρμοσμένο FastAPI |
| **Κλήση Λειτουργιών** | ✅ Εγγενής | ✅ Υποστηρίζεται | ⚠️ Προσαρμοσμένη Υλοποίηση |
| **Υποστήριξη Ποσοτικοποίησης** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Quantization |
| **Έτοιμο για Παραγωγή** | ✅ Εξαιρετικό | ✅ Πολύ Καλό | ✅ Καλό |
| **Ανάπτυξη σε Edge** | Καλή | Εξαιρετική | Εξαιρετική |

## Πρόσθετοι Πόροι

### Επίσημη Τεκμηρίωση
- **Microsoft Phi-4 Model Card**: Λεπτομερείς προδιαγραφές και οδηγίες χρήσης
- **vLLM Documentation**: Επιλογές προηγμένης διαμόρφωσης και βελτιστοποίησης
- **Ollama Model Library**: Μοντέλα κοινότητας και παραδείγματα προσαρμογής
- **ONNX Runtime Guides**: Στρατηγικές βελτιστοποίησης απόδοσης και ανάπτυξης

### Εργαλεία Ανάπτυξης
- **Hugging Face Transformers**: Για αλληλεπίδραση και προσαρμογή μοντέλων
- **OpenAI API Specification**: Για δοκιμή συμβατότητας με vLLM
- **Docker Best Practices**: Οδηγίες ασφάλειας και βελτιστοποίησης containers
- **Kubernetes Deployment**: Μοτίβα ορχήστρωσης για κλιμάκωση παραγωγής

### Πόροι Μάθησης
- **SLM Performance Benchmarking**: Μεθοδολογίες συγκριτικής ανάλυσης
- **Edge AI Deployment**: Βέλτιστες πρακτικές για περιβάλλοντα με περιορισμένους πόρους
- **Reasoning Task Optimization**: Στρατηγικές προτροπής για μαθηματικά και λογικά προβλήματα
- **Container Security**: Πρακτικές ενίσχυσης για ανάπτυξη μοντέλων AI

## Αποτελέσματα Μάθησης

Με την ολοκλήρωση αυτής της ενότητας, θα μπορείτε να:

1. Αναπτύξετε το μοντέλο Phi-4-mini-instruct σε περιβάλλοντα containers χρησιμοποιώντας πολλαπλά πλαίσια
2. Διαμορφώσετε και βελτιστοποιήσετε τις αναπτύξεις SLM για διαφορετικά περιβάλλοντα υλικού
3. Εφαρμόσετε βέλτιστες πρακτικές ασφάλειας για ανάπτυξη AI σε containers
4. Συγκρίνετε και επιλέξετε κατάλληλα πλαίσια ανάπτυξης βάσει συγκεκριμένων απαιτήσεων χρήσης
5. Εφαρμόσετε στρατηγικές παρακολούθησης και κλιμάκωσης για υπηρεσίες SLM παραγωγής

## Τι ακολουθεί

- Επιστροφή στο [Module 1](../Module01/README.md)
- Επιστροφή στο [Module 2](../Module02/README.md)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.