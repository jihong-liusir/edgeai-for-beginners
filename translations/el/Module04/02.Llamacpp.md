<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T08:00:52+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "el"
}
-->
# Ενότητα 2: Οδηγός Υλοποίησης Llama.cpp

## Πίνακας Περιεχομένων
1. [Εισαγωγή](../../../Module04)
2. [Τι είναι το Llama.cpp;](../../../Module04)
3. [Εγκατάσταση](../../../Module04)
4. [Δημιουργία από τον πηγαίο κώδικα](../../../Module04)
5. [Ποσοτικοποίηση Μοντέλου](../../../Module04)
6. [Βασική Χρήση](../../../Module04)
7. [Προχωρημένες Δυνατότητες](../../../Module04)
8. [Ενσωμάτωση με Python](../../../Module04)
9. [Αντιμετώπιση Προβλημάτων](../../../Module04)
10. [Βέλτιστες Πρακτικές](../../../Module04)

## Εισαγωγή

Αυτός ο αναλυτικός οδηγός θα σας καθοδηγήσει σε όλα όσα πρέπει να γνωρίζετε για το Llama.cpp, από τη βασική εγκατάσταση έως σενάρια προχωρημένης χρήσης. Το Llama.cpp είναι μια ισχυρή υλοποίηση σε C++ που επιτρέπει αποδοτική εξαγωγή συμπερασμάτων από Μεγάλα Μοντέλα Γλώσσας (LLMs) με ελάχιστη ρύθμιση και εξαιρετική απόδοση σε διάφορες διαμορφώσεις υλικού.

## Τι είναι το Llama.cpp;

Το Llama.cpp είναι ένα πλαίσιο εξαγωγής συμπερασμάτων LLM γραμμένο σε C/C++ που επιτρέπει την εκτέλεση μεγάλων μοντέλων γλώσσας τοπικά με ελάχιστη ρύθμιση και κορυφαία απόδοση σε ένα ευρύ φάσμα υλικού. Τα βασικά χαρακτηριστικά περιλαμβάνουν:

### Βασικά Χαρακτηριστικά
- **Υλοποίηση σε απλή C/C++** χωρίς εξαρτήσεις
- **Διαλειτουργικότητα μεταξύ πλατφορμών** (Windows, macOS, Linux)
- **Βελτιστοποίηση υλικού** για διάφορες αρχιτεκτονικές
- **Υποστήριξη ποσοτικοποίησης** (1.5-bit έως 8-bit ακέραια ποσοτικοποίηση)
- **Επιτάχυνση CPU και GPU**
- **Αποδοτικότητα μνήμης** για περιορισμένα περιβάλλοντα

### Πλεονεκτήματα
- Λειτουργεί αποδοτικά σε CPU χωρίς να απαιτεί εξειδικευμένο υλικό
- Υποστηρίζει πολλαπλά backend GPU (CUDA, Metal, OpenCL, Vulkan)
- Ελαφρύ και φορητό
- Το Apple silicon είναι πρώτης τάξης πολίτης - βελτιστοποιημένο μέσω ARM NEON, Accelerate και Metal frameworks
- Υποστηρίζει διάφορα επίπεδα ποσοτικοποίησης για μειωμένη χρήση μνήμης

## Εγκατάσταση

### Μέθοδος 1: Προκατασκευασμένα Δυαδικά Αρχεία (Συνιστάται για Αρχάριους)

#### Λήψη από το GitHub Releases
1. Επισκεφθείτε το [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Κατεβάστε το κατάλληλο δυαδικό αρχείο για το σύστημά σας:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` για Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` για macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` για Linux

3. Εξαγάγετε το αρχείο και προσθέστε τον κατάλογο στο PATH του συστήματός σας

#### Χρήση Διαχειριστών Πακέτων

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Διάφορες διανομές):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Μέθοδος 2: Πακέτο Python (llama-cpp-python)

#### Βασική Εγκατάσταση
```bash
pip install llama-cpp-python
```

#### Με Επιτάχυνση Υλικού
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Δημιουργία από τον Πηγαίο Κώδικα

### Προαπαιτούμενα

**Απαιτήσεις Συστήματος:**
- Μεταγλωττιστής C++ (GCC, Clang ή MSVC)
- CMake (έκδοση 3.14 ή νεότερη)
- Git
- Εργαλεία δημιουργίας για την πλατφόρμα σας

**Εγκατάσταση Προαπαιτούμενων:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Εγκαταστήστε το Visual Studio 2022 με εργαλεία ανάπτυξης C++
- Εγκαταστήστε το CMake από την επίσημη ιστοσελίδα
- Εγκαταστήστε το Git

### Βασική Διαδικασία Δημιουργίας

1. **Κλωνοποίηση του αποθετηρίου:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Διαμόρφωση της δημιουργίας:**
```bash
cmake -B build
```

3. **Δημιουργία του έργου:**
```bash
cmake --build build --config Release
```

Για ταχύτερη μεταγλώττιση, χρησιμοποιήστε παράλληλες εργασίες:
```bash
cmake --build build --config Release -j 8
```

### Δημιουργίες Ειδικές για Υλικό

#### Υποστήριξη CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Υποστήριξη Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Υποστήριξη OpenBLAS (Βελτιστοποίηση CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Υποστήριξη Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Προχωρημένες Επιλογές Δημιουργίας

#### Δημιουργία για Εντοπισμό Σφαλμάτων
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Με Πρόσθετες Δυνατότητες
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Ποσοτικοποίηση Μοντέλου

### Κατανόηση της Μορφής GGUF

Η μορφή GGUF (Generalized GGML Unified Format) είναι μια βελτιστοποιημένη μορφή αρχείου σχεδιασμένη για την αποδοτική εκτέλεση μεγάλων μοντέλων γλώσσας χρησιμοποιώντας το Llama.cpp και άλλα πλαίσια. Παρέχει:

- Τυποποιημένη αποθήκευση βαρών μοντέλου
- Βελτιωμένη συμβατότητα μεταξύ πλατφορμών
- Ενισχυμένη απόδοση
- Αποδοτική διαχείριση μεταδεδομένων

### Τύποι Ποσοτικοποίησης

Το Llama.cpp υποστηρίζει διάφορα επίπεδα ποσοτικοποίησης:

| Τύπος | Bits | Περιγραφή | Χρήση |
|------|------|-------------|----------|
| F16 | 16 | Μισή ακρίβεια | Υψηλή ποιότητα, μεγάλη μνήμη |
| Q8_0 | 8 | Ποσοτικοποίηση 8-bit | Καλή ισορροπία |
| Q4_0 | 4 | Ποσοτικοποίηση 4-bit | Μέτρια ποιότητα, μικρότερο μέγεθος |
| Q2_K | 2 | Ποσοτικοποίηση 2-bit | Μικρότερο μέγεθος, χαμηλότερη ποιότητα |

### Μετατροπή Μοντέλων

#### Από PyTorch σε GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Άμεση Λήψη από το Hugging Face
Πολλά μοντέλα είναι διαθέσιμα σε μορφή GGUF στο Hugging Face:
- Αναζητήστε μοντέλα με "GGUF" στο όνομα
- Κατεβάστε το κατάλληλο επίπεδο ποσοτικοποίησης
- Χρησιμοποιήστε τα απευθείας με το llama.cpp

## Βασική Χρήση

### Διεπαφή Γραμμής Εντολών

#### Απλή Δημιουργία Κειμένου
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Χρήση Μοντέλων από το Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Λειτουργία Διακομιστή
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Κοινές Παράμετροι

| Παράμετρος | Περιγραφή | Παράδειγμα |
|-----------|-------------|---------|
| `-m` | Διαδρομή αρχείου μοντέλου | `-m model.gguf` |
| `-p` | Κείμενο προτροπής | `-p "Hello world"` |
| `-n` | Αριθμός παραγόμενων tokens | `-n 100` |
| `-c` | Μέγεθος πλαισίου | `-c 4096` |
| `-t` | Αριθμός νημάτων | `-t 8` |
| `-ngl` | Επίπεδα GPU | `-ngl 32` |
| `-temp` | Θερμοκρασία | `-temp 0.7` |

### Διαδραστική Λειτουργία

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Προχωρημένες Δυνατότητες

### API Διακομιστή

#### Εκκίνηση Διακομιστή
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Χρήση API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Βελτιστοποίηση Απόδοσης

#### Διαχείριση Μνήμης
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Πολυνηματική Επεξεργασία
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Επιτάχυνση GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Ενσωμάτωση με Python

### Βασική Χρήση με llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Διεπαφή Συνομιλίας

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Ροές Απαντήσεων

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Ενσωμάτωση με LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Αντιμετώπιση Προβλημάτων

### Συνηθισμένα Προβλήματα και Λύσεις

#### Σφάλματα Δημιουργίας

**Πρόβλημα: Το CMake δεν βρέθηκε**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Πρόβλημα: Ο μεταγλωττιστής δεν βρέθηκε**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Προβλήματα Εκτέλεσης

**Πρόβλημα: Αποτυχία φόρτωσης μοντέλου**
- Επαληθεύστε τη διαδρομή του αρχείου μοντέλου
- Ελέγξτε τα δικαιώματα αρχείου
- Βεβαιωθείτε ότι υπάρχει αρκετή RAM
- Δοκιμάστε διαφορετικά επίπεδα ποσοτικοποίησης

**Πρόβλημα: Χαμηλή απόδοση**
- Ενεργοποιήστε την επιτάχυνση υλικού
- Αυξήστε τον αριθμό νημάτων
- Χρησιμοποιήστε κατάλληλη ποσοτικοποίηση
- Ελέγξτε τη χρήση μνήμης GPU

#### Προβλήματα Μνήμης

**Πρόβλημα: Έλλειψη μνήμης**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Προβλήματα Ειδικά για Πλατφόρμες

#### Windows
- Χρησιμοποιήστε MinGW ή μεταγλωττιστή Visual Studio
- Βεβαιωθείτε για σωστή διαμόρφωση PATH
- Ελέγξτε για παρεμβολές από antivirus

#### macOS
- Ενεργοποιήστε το Metal για Apple Silicon
- Χρησιμοποιήστε το Rosetta 2 για συμβατότητα αν χρειάζεται
- Ελέγξτε τα εργαλεία γραμμής εντολών Xcode

#### Linux
- Εγκαταστήστε πακέτα ανάπτυξης
- Ελέγξτε τις εκδόσεις των οδηγών GPU
- Επαληθεύστε την εγκατάσταση του CUDA toolkit

## Βέλτιστες Πρακτικές

### Επιλογή Μοντέλου
1. **Επιλέξτε κατάλληλη ποσοτικοποίηση** βάσει του υλικού σας
2. **Λάβετε υπόψη το μέγεθος του μοντέλου** σε σχέση με την ποιότητα
3. **Δοκιμάστε διαφορετικά μοντέλα** για τη συγκεκριμένη χρήση σας

### Βελτιστοποίηση Απόδοσης
1. **Χρησιμοποιήστε επιτάχυνση GPU** όταν είναι διαθέσιμη
2. **Βελτιστοποιήστε τον αριθμό νημάτων** για την CPU σας
3. **Ορίστε κατάλληλο μέγεθος πλαισίου** για τη χρήση σας
4. **Ενεργοποιήστε τη χαρτογράφηση μνήμης** για μεγάλα μοντέλα

### Ανάπτυξη σε Παραγωγή
1. **Χρησιμοποιήστε λειτουργία διακομιστή** για πρόσβαση μέσω API
2. **Εφαρμόστε σωστό χειρισμό σφαλμάτων**
3. **Παρακολουθήστε τη χρήση πόρων**
4. **Ρυθμίστε καταγραφή και παρακολούθηση**

### Ροή Ανάπτυξης
1. **Ξεκινήστε με μικρότερα μοντέλα** για δοκιμές
2. **Χρησιμοποιήστε έλεγχο εκδόσεων** για τις ρυθμίσεις μοντέλου
3. **Τεκμηριώστε τις ρυθμίσεις σας**
4. **Δοκιμάστε σε διαφορετικές πλατφόρμες**

### Θέματα Ασφαλείας
1. **Επαληθεύστε τις προτροπές εισόδου**
2. **Εφαρμόστε περιορισμό ρυθμού**
3. **Ασφαλίστε τα σημεία πρόσβασης API**
4. **Παρακολουθήστε μοτίβα κατάχρησης**

## Συμπέρασμα

Το Llama.cpp παρέχει έναν ισχυρό και αποδοτικό τρόπο εκτέλεσης μεγάλων μοντέλων γλώσσας τοπικά σε διάφορες διαμορφώσεις υλικού. Είτε αναπτύσσετε εφαρμογές AI, είτε διεξάγετε έρευνα, είτε απλώς πειραματίζεστε με LLMs, αυτό το πλαίσιο προσφέρει την ευελιξία και την απόδοση που απαιτούνται για ένα ευρύ φάσμα χρήσεων.

Κύρια σημεία:
- Επιλέξτε τη μέθοδο εγκατάστασης που ταιριάζει καλύτερα στις ανάγκες σας
- Βελτιστοποιήστε για τη συγκεκριμένη διαμόρφωση υλικού σας
- Ξεκινήστε με βασική χρήση και εξερευνήστε σταδιακά τις προχωρημένες δυνατότητες
- Εξετάστε τη χρήση των δεσμών Python για ευκολότερη ενσωμάτωση
- Ακολουθήστε βέλτιστες πρακτικές για ανάπτυξη σε παραγωγή

Για περισσότερες πληροφορίες και ενημερώσεις, επισκεφθείτε το [επίσημο αποθετήριο Llama.cpp](https://github.com/ggml-org/llama.cpp) και ανατρέξτε στην αναλυτική τεκμηρίωση και τους διαθέσιμους πόρους της κοινότητας.

## ➡️ Τι ακολουθεί

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.