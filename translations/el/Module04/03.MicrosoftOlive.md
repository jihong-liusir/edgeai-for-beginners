<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T18:54:44+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "el"
}
-->
# Ενότητα 3: Microsoft Olive Optimization Suite

## Πίνακας Περιεχομένων
1. [Εισαγωγή](../../../Module04)
2. [Τι είναι το Microsoft Olive;](../../../Module04)
3. [Εγκατάσταση](../../../Module04)
4. [Οδηγός Γρήγορης Εκκίνησης](../../../Module04)
5. [Παράδειγμα: Μετατροπή Qwen3 σε ONNX INT4](../../../Module04)
6. [Προχωρημένη Χρήση](../../../Module04)
7. [Βέλτιστες Πρακτικές](../../../Module04)
8. [Αντιμετώπιση Προβλημάτων](../../../Module04)
9. [Επιπλέον Πόροι](../../../Module04)

## Εισαγωγή

Το Microsoft Olive είναι ένα ισχυρό και εύχρηστο εργαλείο βελτιστοποίησης μοντέλων, προσαρμοσμένο στο υλικό, που απλοποιεί τη διαδικασία βελτιστοποίησης μοντέλων μηχανικής μάθησης για ανάπτυξη σε διαφορετικές πλατφόρμες υλικού. Είτε στοχεύετε CPUs, GPUs ή εξειδικευμένους AI επιταχυντές, το Olive σας βοηθά να επιτύχετε βέλτιστη απόδοση διατηρώντας την ακρίβεια του μοντέλου.

## Τι είναι το Microsoft Olive;

Το Olive είναι ένα εύχρηστο εργαλείο βελτιστοποίησης μοντέλων, προσαρμοσμένο στο υλικό, που συνδυάζει κορυφαίες τεχνικές στη συμπίεση, βελτιστοποίηση και μεταγλώττιση μοντέλων. Λειτουργεί με το ONNX Runtime ως μια ολοκληρωμένη λύση βελτιστοποίησης για την εκτέλεση.

### Βασικά Χαρακτηριστικά

- **Βελτιστοποίηση Προσαρμοσμένη στο Υλικό**: Επιλέγει αυτόματα τις καλύτερες τεχνικές βελτιστοποίησης για το υλικό σας
- **40+ Ενσωματωμένα Στοιχεία Βελτιστοποίησης**: Περιλαμβάνει συμπίεση μοντέλων, ποσοτικοποίηση, βελτιστοποίηση γραφημάτων και άλλα
- **Εύχρηστη Διεπαφή CLI**: Απλές εντολές για κοινές εργασίες βελτιστοποίησης
- **Υποστήριξη Πολλαπλών Πλαισίων**: Συνεργάζεται με PyTorch, Hugging Face μοντέλα και ONNX
- **Υποστήριξη Δημοφιλών Μοντέλων**: Το Olive μπορεί να βελτιστοποιήσει αυτόματα δημοφιλείς αρχιτεκτονικές μοντέλων όπως Llama, Phi, Qwen, Gemma κ.ά.

### Οφέλη

- **Μειωμένος Χρόνος Ανάπτυξης**: Δεν χρειάζεται να πειραματιστείτε χειροκίνητα με διαφορετικές τεχνικές βελτιστοποίησης
- **Βελτιώσεις Απόδοσης**: Σημαντικές αυξήσεις ταχύτητας (έως και 6 φορές σε ορισμένες περιπτώσεις)
- **Διαλειτουργικότητα**: Τα βελτιστοποιημένα μοντέλα λειτουργούν σε διαφορετικό υλικό και λειτουργικά συστήματα
- **Διατήρηση Ακρίβειας**: Οι βελτιστοποιήσεις διατηρούν την ποιότητα του μοντέλου ενώ βελτιώνουν την απόδοση

## Εγκατάσταση

### Προαπαιτούμενα

- Python 3.8 ή νεότερη έκδοση
- Διαχειριστής πακέτων pip
- Εικονικό περιβάλλον (συνιστάται)

### Βασική Εγκατάσταση

Δημιουργήστε και ενεργοποιήστε ένα εικονικό περιβάλλον:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Εγκαταστήστε το Olive με δυνατότητες αυτόματης βελτιστοποίησης:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Προαιρετικές Εξαρτήσεις

Το Olive προσφέρει διάφορες προαιρετικές εξαρτήσεις για επιπλέον δυνατότητες:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Επαλήθευση Εγκατάστασης

```bash
olive --help
```

Αν η εγκατάσταση είναι επιτυχής, θα πρέπει να δείτε το μήνυμα βοήθειας του Olive CLI.

## Οδηγός Γρήγορης Εκκίνησης

### Η Πρώτη σας Βελτιστοποίηση

Ας βελτιστοποιήσουμε ένα μικρό γλωσσικό μοντέλο χρησιμοποιώντας τη δυνατότητα αυτόματης βελτιστοποίησης του Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Τι Κάνει Αυτή η Εντολή

Η διαδικασία βελτιστοποίησης περιλαμβάνει: λήψη του μοντέλου από την τοπική προσωρινή μνήμη, καταγραφή του ONNX Graph και αποθήκευση των βαρών σε ένα αρχείο δεδομένων ONNX, βελτιστοποίηση του ONNX Graph και ποσοτικοποίηση του μοντέλου σε int4 χρησιμοποιώντας τη μέθοδο RTN.

### Επεξήγηση Παραμέτρων Εντολής

- `--model_name_or_path`: Αναγνωριστικό μοντέλου Hugging Face ή τοπική διαδρομή
- `--output_path`: Κατάλογος όπου θα αποθηκευτεί το βελτιστοποιημένο μοντέλο
- `--device`: Στόχος υλικού (cpu, gpu)
- `--provider`: Πάροχος εκτέλεσης (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Χρήση του ONNX Runtime Generate AI για εκτέλεση
- `--precision`: Ακρίβεια ποσοτικοποίησης (int4, int8, fp16)
- `--log_level`: Επίπεδο λεπτομέρειας καταγραφής (0=ελάχιστο, 1=λεπτομερές)

## Παράδειγμα: Μετατροπή Qwen3 σε ONNX INT4

Βάσει του παραδείγματος Hugging Face στο [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), δείτε πώς να βελτιστοποιήσετε ένα μοντέλο Qwen3:

### Βήμα 1: Λήψη Μοντέλου (Προαιρετικό)

Για να μειώσετε τον χρόνο λήψης, αποθηκεύστε μόνο τα απαραίτητα αρχεία:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Βήμα 2: Βελτιστοποίηση Μοντέλου Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Βήμα 3: Δοκιμή του Βελτιστοποιημένου Μοντέλου

Δημιουργήστε ένα απλό σενάριο Python για να δοκιμάσετε το βελτιστοποιημένο μοντέλο:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Δομή Εξόδου

Μετά τη βελτιστοποίηση, ο κατάλογος εξόδου σας θα περιέχει:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Προχωρημένη Χρήση

### Αρχεία Ρύθμισης

Για πιο σύνθετες ροές εργασίας βελτιστοποίησης, μπορείτε να χρησιμοποιήσετε αρχεία ρύθμισης JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Εκτέλεση με ρύθμιση:

```bash
olive run --config config.json
```

### Βελτιστοποίηση GPU

Για βελτιστοποίηση CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Για DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning με Olive

Το Olive υποστηρίζει επίσης fine-tuning μοντέλων:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Βέλτιστες Πρακτικές

### 1. Επιλογή Μοντέλου
- Ξεκινήστε με μικρότερα μοντέλα για δοκιμές (π.χ., 0.5B-7B παραμέτρους)
- Βεβαιωθείτε ότι η αρχιτεκτονική του στόχου σας υποστηρίζεται από το Olive

### 2. Παράγοντες Υλικού
- Ταιριάξτε τον στόχο βελτιστοποίησης με το υλικό ανάπτυξης
- Χρησιμοποιήστε βελτιστοποίηση GPU αν διαθέτετε υλικό συμβατό με CUDA
- Εξετάστε το DirectML για Windows μηχανές με ενσωματωμένα γραφικά

### 3. Επιλογή Ακρίβειας
- **INT4**: Μέγιστη συμπίεση, μικρή απώλεια ακρίβειας
- **INT8**: Καλή ισορροπία μεγέθους και ακρίβειας
- **FP16**: Ελάχιστη απώλεια ακρίβειας, μέτρια μείωση μεγέθους

### 4. Δοκιμή και Επικύρωση
- Πάντα να δοκιμάζετε τα βελτιστοποιημένα μοντέλα με τις συγκεκριμένες περιπτώσεις χρήσης σας
- Συγκρίνετε μετρικές απόδοσης (καθυστέρηση, ρυθμός, ακρίβεια)
- Χρησιμοποιήστε αντιπροσωπευτικά δεδομένα εισόδου για αξιολόγηση

### 5. Επαναληπτική Βελτιστοποίηση
- Ξεκινήστε με αυτόματη βελτιστοποίηση για γρήγορα αποτελέσματα
- Χρησιμοποιήστε αρχεία ρύθμισης για λεπτομερή έλεγχο
- Πειραματιστείτε με διαφορετικά περάσματα βελτιστοποίησης

## Αντιμετώπιση Προβλημάτων

### Συνηθισμένα Προβλήματα

#### 1. Προβλήματα Εγκατάστασης
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Προβλήματα CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Προβλήματα Μνήμης
- Χρησιμοποιήστε μικρότερα μεγέθη παρτίδας κατά τη βελτιστοποίηση
- Δοκιμάστε ποσοτικοποίηση με υψηλότερη ακρίβεια πρώτα (int8 αντί για int4)
- Βεβαιωθείτε ότι υπάρχει αρκετός χώρος στο δίσκο για προσωρινή αποθήκευση μοντέλων

#### 4. Σφάλματα Φόρτωσης Μοντέλου
- Επαληθεύστε τη διαδρομή του μοντέλου και τα δικαιώματα πρόσβασης
- Ελέγξτε αν το μοντέλο απαιτεί `trust_remote_code=True`
- Βεβαιωθείτε ότι έχουν ληφθεί όλα τα απαραίτητα αρχεία μοντέλου

### Λήψη Βοήθειας

- **Τεκμηρίωση**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Παραδείγματα**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Επιπλέον Πόροι

### Επίσημοι Σύνδεσμοι
- **Αποθετήριο GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Τεκμηρίωση ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Παράδειγμα Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Παραδείγματα Κοινότητας
- **Jupyter Notebooks**: Διαθέσιμα στο αποθετήριο GitHub του Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Επέκταση VS Code**: Επισκόπηση AI Toolkit για VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Αναρτήσεις Ιστολογίου**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Σχετικά Εργαλεία
- **ONNX Runtime**: Μηχανή εκτέλεσης υψηλής απόδοσης — https://onnxruntime.ai/
- **Hugging Face Transformers**: Πηγή πολλών συμβατών μοντέλων — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Ροές εργασίας βελτιστοποίησης στο cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Τι ακολουθεί

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

