{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af028554",
   "metadata": {},
   "source": [
    "# ŒîœÅŒøŒºŒøŒªŒøŒ≥Œ∑œÑŒÆœÇ ŒúŒøŒΩœÑŒ≠ŒªŒøœÖ ŒíŒ±œÉŒπœÉŒºŒ≠ŒΩŒøœÇ œÉœÑŒ∑ŒΩ Œ†œÅœåŒ∏ŒµœÉŒ∑ ŒºŒµ œÑŒø Foundry Local SDK\n",
    "\n",
    "**Œ£œçœÉœÑŒ∑ŒºŒ± ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ Œ†ŒøŒªŒªŒ±œÄŒªœéŒΩ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ ŒíŒµŒªœÑŒπœÉœÑŒøœÄŒøŒπŒ∑ŒºŒ≠ŒΩŒø Œ≥ŒπŒ± CPU**\n",
    "\n",
    "ŒëœÖœÑœå œÑŒø œÉŒ∑ŒºŒµŒπœâŒºŒ±œÑŒ¨œÅŒπŒø œÄŒ±œÅŒøœÖœÉŒπŒ¨Œ∂ŒµŒπ Œ≠ŒΩŒ± Œ≠ŒæœÖœÄŒΩŒø œÉœçœÉœÑŒ∑ŒºŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ œÄŒøœÖ ŒµœÄŒπŒªŒ≠Œ≥ŒµŒπ Œ±œÖœÑœåŒºŒ±œÑŒ± œÑŒø Œ∫Œ±ŒªœçœÑŒµœÅŒø ŒºŒπŒ∫œÅœå Œ≥ŒªœâœÉœÉŒπŒ∫œå ŒºŒøŒΩœÑŒ≠ŒªŒø Œ≤Œ¨œÉŒµŒπ œÑŒ∑œÇ œÄœÅœåŒ∏ŒµœÉŒ∑œÇ œÑŒøœÖ œáœÅŒÆœÉœÑŒ∑. ŒôŒ¥Œ±ŒΩŒπŒ∫œå Œ≥ŒπŒ± œÉŒµŒΩŒ¨œÅŒπŒ± Œ±ŒΩŒ¨œÄœÑœÖŒæŒ∑œÇ œÉŒµ œÄŒµœÅŒπŒ≤Œ¨ŒªŒªŒøŒΩœÑŒ± edge, œåœÄŒøœÖ Œ∏Œ≠ŒªŒµœÑŒµ ŒΩŒ± Œ±ŒæŒπŒøœÄŒøŒπŒÆœÉŒµœÑŒµ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± ŒºŒµ Œ±œÄŒøŒ¥ŒøœÑŒπŒ∫œåœÑŒ∑œÑŒ±.\n",
    "\n",
    "## üéØ Œ§Œπ ŒòŒ± ŒúŒ¨Œ∏ŒµœÑŒµ\n",
    "\n",
    "- **ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑ Œ†œÅœåŒ∏ŒµœÉŒ∑œÇ**: ŒëœÖœÑœåŒºŒ±œÑŒ∑ œÑŒ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑ œÄœÅŒøœÑœÅŒøœÄœéŒΩ (Œ∫œéŒ¥ŒπŒ∫Œ±œÇ, œÄŒµœÅŒØŒªŒ∑œàŒ∑, œÑŒ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑, Œ≥ŒµŒΩŒπŒ∫Œ¨)\n",
    "- **ŒàŒæœÖœÄŒΩŒ∑ ŒïœÄŒπŒªŒøŒ≥ŒÆ ŒúŒøŒΩœÑŒ≠ŒªŒøœÖ**: ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ œÉœÑŒø œÄŒπŒø ŒπŒ∫Œ±ŒΩœå ŒºŒøŒΩœÑŒ≠ŒªŒø Œ≥ŒπŒ± Œ∫Œ¨Œ∏Œµ ŒµœÅŒ≥Œ±œÉŒØŒ±\n",
    "- **ŒíŒµŒªœÑŒπœÉœÑŒøœÄŒøŒØŒ∑œÉŒ∑ CPU**: ŒúŒøŒΩœÑŒ≠ŒªŒ± ŒºŒµ Œ±œÄŒøŒ¥ŒøœÑŒπŒ∫ŒÆ œáœÅŒÆœÉŒ∑ ŒºŒΩŒÆŒºŒ∑œÇ œÄŒøœÖ ŒªŒµŒπœÑŒøœÖœÅŒ≥ŒøœçŒΩ œÉŒµ ŒøœÄŒøŒπŒøŒ¥ŒÆœÄŒøœÑŒµ œÖŒªŒπŒ∫œå\n",
    "- **ŒîŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ Œ†ŒøŒªŒªŒ±œÄŒªœéŒΩ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ**: ŒîŒπŒ±œÑŒÆœÅŒ∑œÉŒ∑ œÄŒøŒªŒªŒ±œÄŒªœéŒΩ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ œÜŒøœÅœÑœâŒºŒ≠ŒΩœâŒΩ ŒºŒµ `--retain true`\n",
    "- **Œ†Œ±œÅŒ±Œ≥œâŒ≥ŒπŒ∫Œ¨ Œ†œÅœåœÑœÖœÄŒ±**: ŒõŒøŒ≥ŒπŒ∫ŒÆ ŒµœÄŒ±ŒΩŒ±œÄœÅŒøœÉœÄŒ¨Œ∏ŒµŒπŒ±œÇ, Œ¥ŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ œÉœÜŒ±ŒªŒºŒ¨œÑœâŒΩ Œ∫Œ±Œπ œÄŒ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ tokens\n",
    "\n",
    "## üìã ŒïœÄŒπœÉŒ∫œåœÄŒ∑œÉŒ∑ Œ£ŒµŒΩŒ±œÅŒØŒøœÖ\n",
    "\n",
    "ŒëœÖœÑœå œÑŒø œÄœÅœåœÑœÖœÄŒø œÄŒ±œÅŒøœÖœÉŒπŒ¨Œ∂ŒµŒπ:\n",
    "\n",
    "1. **ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑ Œ†œÅœåŒ∏ŒµœÉŒ∑œÇ**: Œ§Œ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑ Œ∫Œ¨Œ∏Œµ œÄœÅŒøœÑœÅŒøœÄŒÆœÇ œáœÅŒÆœÉœÑŒ∑ (Œ∫œéŒ¥ŒπŒ∫Œ±œÇ, œÄŒµœÅŒØŒªŒ∑œàŒ∑, œÑŒ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑ ŒÆ Œ≥ŒµŒΩŒπŒ∫Œ¨)\n",
    "2. **ŒïœÄŒπŒªŒøŒ≥ŒÆ ŒúŒøŒΩœÑŒ≠ŒªŒøœÖ**: ŒëœÖœÑœåŒºŒ±œÑŒ∑ ŒµœÄŒπŒªŒøŒ≥ŒÆ œÑŒøœÖ œÄŒπŒø Œ∫Œ±œÑŒ¨ŒªŒªŒ∑ŒªŒøœÖ ŒºŒπŒ∫œÅŒøœç Œ≥ŒªœâœÉœÉŒπŒ∫Œøœç ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ Œ≤Œ¨œÉŒµŒπ Œ¥œÖŒΩŒ±œÑŒøœÑŒÆœÑœâŒΩ\n",
    "3. **Œ§ŒøœÄŒπŒ∫ŒÆ ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑**: ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ œÉŒµ ŒºŒøŒΩœÑŒ≠ŒªŒ± œÄŒøœÖ ŒµŒ∫œÑŒµŒªŒøœçŒΩœÑŒ±Œπ œÑŒøœÄŒπŒ∫Œ¨ ŒºŒ≠œÉœâ œÑŒ∑œÇ œÖœÄŒ∑œÅŒµœÉŒØŒ±œÇ Foundry Local\n",
    "4. **ŒïŒΩŒπŒ±ŒØŒ± ŒîŒπŒµœÄŒ±œÜŒÆ**: ŒïŒΩŒπŒ±ŒØŒø œÉŒ∑ŒºŒµŒØŒø ŒµŒπœÉœåŒ¥ŒøœÖ œÉœÖŒΩŒøŒºŒπŒªŒØŒ±œÇ œÄŒøœÖ Œ¥œÅŒøŒºŒøŒªŒøŒ≥ŒµŒØ œÉŒµ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ±\n",
    "\n",
    "**ŒôŒ¥Œ±ŒΩŒπŒ∫œå Œ≥ŒπŒ±**: ŒëŒΩŒ±œÄœÑœçŒæŒµŒπœÇ œÉŒµ œÄŒµœÅŒπŒ≤Œ¨ŒªŒªŒøŒΩœÑŒ± edge ŒºŒµ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ±, œåœÄŒøœÖ Œ∏Œ≠ŒªŒµœÑŒµ Œ≠ŒæœÖœÄŒΩŒ∑ Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ Œ±ŒπœÑŒ∑ŒºŒ¨œÑœâŒΩ œáœâœÅŒØœÇ œáŒµŒπœÅŒøŒ∫ŒØŒΩŒ∑œÑŒ∑ ŒµœÄŒπŒªŒøŒ≥ŒÆ ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ.\n",
    "\n",
    "## üîß Œ†œÅŒøŒ±œÄŒ±ŒπœÑŒøœçŒºŒµŒΩŒ±\n",
    "\n",
    "- ŒïŒ≥Œ∫Œ±œÑŒµœÉœÑŒ∑ŒºŒ≠ŒΩŒø Œ∫Œ±Œπ ŒµŒΩŒµœÅŒ≥œå **Foundry Local**\n",
    "- **Python 3.8+** ŒºŒµ pip\n",
    "- **8GB+ RAM** (œÉœÖŒΩŒπœÉœÑŒ¨œÑŒ±Œπ 16GB+ Œ≥ŒπŒ± œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒºŒøŒΩœÑŒ≠ŒªŒ±)\n",
    "- Œ§Œø module **workshop_utils** (œÉœÑŒø ../samples/)\n",
    "\n",
    "## üöÄ ŒìœÅŒÆŒ≥ŒøœÅŒ∑ ŒïŒ∫Œ∫ŒØŒΩŒ∑œÉŒ∑\n",
    "\n",
    "Œ§Œø œÉŒ∑ŒºŒµŒπœâŒºŒ±œÑŒ¨œÅŒπŒø Œ∏Œ±:\n",
    "1. ŒïŒΩœÑŒøœÄŒØœÉŒµŒπ œÑŒ∑ ŒºŒΩŒÆŒºŒ∑ œÑŒøœÖ œÉœÖœÉœÑŒÆŒºŒ±œÑœåœÇ œÉŒ±œÇ\n",
    "2. Œ†œÅŒøœÑŒµŒØŒΩŒµŒπ Œ∫Œ±œÑŒ¨ŒªŒªŒ∑ŒªŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± CPU\n",
    "3. Œ¶ŒøœÅœÑœéœÉŒµŒπ Œ±œÖœÑœåŒºŒ±œÑŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± ŒºŒµ `--retain true`\n",
    "4. ŒïœÄŒ±ŒªŒ∑Œ∏ŒµœçœÉŒµŒπ œåœÑŒπ œåŒªŒ± œÑŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± ŒµŒØŒΩŒ±Œπ Œ≠œÑŒøŒπŒºŒ±\n",
    "5. ŒîœÅŒøŒºŒøŒªŒøŒ≥ŒÆœÉŒµŒπ Œ¥ŒøŒ∫ŒπŒºŒ±œÉœÑŒπŒ∫Œ≠œÇ œÄœÅŒøœÑœÅŒøœÄŒ≠œÇ œÉŒµ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ±\n",
    "\n",
    "**ŒïŒ∫œÑŒπŒºœéŒºŒµŒΩŒøœÇ œáœÅœåŒΩŒøœÇ ŒµŒ≥Œ∫Œ±œÑŒ¨œÉœÑŒ±œÉŒ∑œÇ**: 5-7 ŒªŒµœÄœÑŒ¨ (œÄŒµœÅŒπŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÑŒ∑ œÜœåœÅœÑœâœÉŒ∑ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa55f0",
   "metadata": {},
   "source": [
    "## üì¶ ŒíŒÆŒºŒ± 1: ŒïŒ≥Œ∫Œ±œÑŒ¨œÉœÑŒ±œÉŒ∑ ŒïŒæŒ±œÅœÑŒÆœÉŒµœâŒΩ\n",
    "\n",
    "ŒïŒ≥Œ∫Œ±œÑŒ±œÉœÑŒÆœÉœÑŒµ œÑŒø ŒµœÄŒØœÉŒ∑ŒºŒø Foundry Local SDK Œ∫Œ±Œπ œÑŒπœÇ Œ±œÄŒ±œÅŒ±ŒØœÑŒ∑œÑŒµœÇ Œ≤ŒπŒ≤ŒªŒπŒøŒ∏ŒÆŒ∫ŒµœÇ:\n",
    "\n",
    "- **foundry-local-sdk**: ŒïœÄŒØœÉŒ∑ŒºŒø Python SDK Œ≥ŒπŒ± œÑŒ∑ Œ¥ŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ œÑŒøœÄŒπŒ∫œéŒΩ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ\n",
    "- **openai**: API œÉœÖŒºŒ≤Œ±œÑœå ŒºŒµ OpenAI Œ≥ŒπŒ± œÉœÖŒΩŒøŒºŒπŒªŒØŒµœÇ\n",
    "- **psutil**: ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑ Œ∫Œ±Œπ œÄŒ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ ŒºŒΩŒÆŒºŒ∑œÇ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2929c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q foundry-local-sdk openai psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990799e7",
   "metadata": {},
   "source": [
    "## üíª ŒíŒÆŒºŒ± 2: ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑ ŒúŒΩŒÆŒºŒ∑œÇ Œ£œÖœÉœÑŒÆŒºŒ±œÑŒøœÇ\n",
    "\n",
    "ŒëŒΩŒπœáŒΩŒµœçœÉœÑŒµ œÑŒ∑ Œ¥ŒπŒ±Œ∏Œ≠œÉŒπŒºŒ∑ ŒºŒΩŒÆŒºŒ∑ œÑŒøœÖ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ Œ≥ŒπŒ± ŒΩŒ± Œ∫Œ±Œ∏ŒøœÅŒØœÉŒµœÑŒµ œÄŒøŒπŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± CPU ŒºœÄŒøœÅŒøœçŒΩ ŒΩŒ± ŒªŒµŒπœÑŒøœÖœÅŒ≥ŒøœçŒΩ Œ±œÄŒøœÑŒµŒªŒµœÉŒºŒ±œÑŒπŒ∫Œ¨. ŒëœÖœÑœå ŒµŒæŒ±œÉœÜŒ±ŒªŒØŒ∂ŒµŒπ œÑŒ∑ŒΩ ŒπŒ¥Œ±ŒΩŒπŒ∫ŒÆ ŒµœÄŒπŒªŒøŒ≥ŒÆ ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ Œ≥ŒπŒ± œÑŒø œÖŒªŒπŒ∫œå œÉŒ±œÇ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff58f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  System Memory Information\n",
      "======================================================================\n",
      "Total Memory:     63.30 GB\n",
      "Available Memory: 16.19 GB\n",
      "\n",
      "‚úÖ High Memory System (32GB+)\n",
      "   Can run 3-4 models simultaneously\n",
      "\n",
      "üìã Recommended Model Aliases for Your System:\n",
      "   ‚Ä¢ phi-4-mini\n",
      "   ‚Ä¢ phi-3.5-mini\n",
      "   ‚Ä¢ qwen2.5-0.5b\n",
      "   ‚Ä¢ qwen2.5-coder-0.5b\n",
      "\n",
      "üí° About Model Aliases:\n",
      "   ‚úì Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)\n",
      "   ‚úì Foundry Local automatically selects CPU variant for your hardware\n",
      "   ‚úì No GPU required - optimized for CPU inference\n",
      "   ‚úì Predictable memory usage and consistent performance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory information\n",
    "total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print('üñ•Ô∏è  System Memory Information')\n",
    "print('=' * 70)\n",
    "print(f'Total Memory:     {total_memory_gb:.2f} GB')\n",
    "print(f'Available Memory: {available_memory_gb:.2f} GB')\n",
    "print()\n",
    "\n",
    "# Recommend models based on available memory\n",
    "# Using model aliases - Foundry Local will automatically select CPU variant\n",
    "model_aliases = []\n",
    "\n",
    "if total_memory_gb >= 32:\n",
    "    model_aliases = ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b', 'qwen2.5-coder-0.5b']\n",
    "    print('‚úÖ High Memory System (32GB+)')\n",
    "    print('   Can run 3-4 models simultaneously')\n",
    "elif total_memory_gb >= 16:\n",
    "    model_aliases = ['phi-4-mini', 'qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('‚úÖ Medium Memory System (16-32GB)')\n",
    "    print('   Can run 2-3 models simultaneously')\n",
    "elif total_memory_gb >= 8:\n",
    "    model_aliases = ['qwen2.5-0.5b', 'phi-3.5-mini']\n",
    "    print('‚ö†Ô∏è  Lower Memory System (8-16GB)')\n",
    "    print('   Recommended: 2 smaller models')\n",
    "else:\n",
    "    model_aliases = ['qwen2.5-0.5b']\n",
    "    print('‚ö†Ô∏è  Limited Memory System (<8GB)')\n",
    "    print('   Recommended: Use only smallest model')\n",
    "\n",
    "print()\n",
    "print('üìã Recommended Model Aliases for Your System:')\n",
    "for model in model_aliases:\n",
    "    print(f'   ‚Ä¢ {model}')\n",
    "\n",
    "print()\n",
    "print('üí° About Model Aliases:')\n",
    "print('   ‚úì Use base alias (e.g., phi-4-mini, not phi-4-mini-cpu)')\n",
    "print('   ‚úì Foundry Local automatically selects CPU variant for your hardware')\n",
    "print('   ‚úì No GPU required - optimized for CPU inference')\n",
    "print('   ‚úì Predictable memory usage and consistent performance')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69590b94",
   "metadata": {},
   "source": [
    "## ü§ñ ŒíŒÆŒºŒ± 3: ŒëœÖœÑœåŒºŒ±œÑŒ∑ Œ¶œåœÅœÑœâœÉŒ∑ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ\n",
    "\n",
    "ŒëœÖœÑœå œÑŒø Œ∫ŒµŒªŒØ Œ±œÖœÑœåŒºŒ±œÑŒ±:\n",
    "1. ŒûŒµŒ∫ŒπŒΩŒ¨ œÑŒ∑ŒΩ œÖœÄŒ∑œÅŒµœÉŒØŒ± Foundry Local (Œ±ŒΩ Œ¥ŒµŒΩ ŒªŒµŒπœÑŒøœÖœÅŒ≥ŒµŒØ)\n",
    "2. Œ¶ŒøœÅœÑœéŒΩŒµŒπ œÑŒ± œÄœÅŒøœÑŒµŒπŒΩœåŒºŒµŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± ŒºŒµ `--retain true` (Œ¥ŒπŒ±œÑŒ∑œÅŒµŒØ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒºŒøŒΩœÑŒ≠ŒªŒ± œÉœÑŒ∑ ŒºŒΩŒÆŒºŒ∑)\n",
    "3. ŒïœÄŒ±ŒªŒ∑Œ∏ŒµœçŒµŒπ œåœÑŒπ œåŒªŒ± œÑŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ± ŒµŒØŒΩŒ±Œπ Œ≠œÑŒøŒπŒºŒ± œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπœéŒΩœÑŒ±œÇ œÑŒø SDK\n",
    "\n",
    "‚è±Ô∏è **ŒëŒΩŒ±ŒºŒµŒΩœåŒºŒµŒΩŒøœÇ œáœÅœåŒΩŒøœÇ**: 3-5 ŒªŒµœÄœÑŒ¨ Œ≥ŒπŒ± œåŒªŒ± œÑŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "543fd976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Automatic Model Loading with SDK Verification\n",
      "======================================================================\n",
      "üìã Loading 3 models: ['phi-4-mini', 'phi-3.5-mini', 'qwen2.5-0.5b']\n",
      "üí° Using model aliases - Foundry will load CPU variants automatically\n",
      "\n",
      "üì° Step 1: Checking Foundry Local service...\n",
      "   ‚úÖ Service is already running\n",
      "\n",
      "ü§ñ Step 2: Loading models with retention...\n",
      "   [1/3] Starting phi-4-mini...\n",
      "       ‚úÖ phi-4-mini loading in background\n",
      "   [2/3] Starting phi-3.5-mini...\n",
      "       ‚úÖ phi-3.5-mini loading in background\n",
      "   [3/3] Starting qwen2.5-0.5b...\n",
      "       ‚úÖ qwen2.5-0.5b loading in background\n",
      "\n",
      "‚úÖ Step 3: Verifying models (this may take 2-3 minutes)...\n",
      "======================================================================\n",
      "\n",
      "   Attempt 1/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 2/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 3/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 4/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 5/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 6/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 7/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 8/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 9/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 10/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 11/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 12/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 13/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 14/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 15/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 16/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 17/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 18/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 19/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 20/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 21/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 22/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 23/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 24/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 25/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 26/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 27/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 28/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 29/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "   Attempt 30/30...\n",
      "   ‚ö†Ô∏è  phi-4-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  phi-3.5-mini error: get_client() takes 1 positional argument but 2 were given...\n",
      "   ‚ö†Ô∏è  qwen2.5-0.5b error: get_client() takes 1 positional argument but 2 were given...\n",
      "\n",
      "======================================================================\n",
      "üì¶ Final Status: 0/3 models ready\n",
      "   ‚ùå phi-4-mini - NOT READY\n",
      "   ‚ùå phi-3.5-mini - NOT READY\n",
      "   ‚ùå qwen2.5-0.5b - NOT READY\n",
      "\n",
      "‚ö†Ô∏è  Some models not ready. Check: foundry model ls\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add samples directory for workshop_utils (Foundry SDK pattern)\n",
    "sys.path.append(os.path.join('..', 'samples'))\n",
    "\n",
    "print('üöÄ Automatic Model Loading with SDK Verification')\n",
    "print('=' * 70)\n",
    "\n",
    "# Use top 3 recommended models (aliases)\n",
    "# Foundry will automatically load CPU variants\n",
    "REQUIRED_MODELS = model_aliases[:3]\n",
    "print(f'üìã Loading {len(REQUIRED_MODELS)} models: {REQUIRED_MODELS}')\n",
    "print('üí° Using model aliases - Foundry will load CPU variants automatically')\n",
    "print()\n",
    "\n",
    "# Step 1: Ensure Foundry Local service is running\n",
    "print('üì° Step 1: Checking Foundry Local service...')\n",
    "try:\n",
    "    result = subprocess.run(['foundry', 'service', 'status'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print('   ‚úÖ Service is already running')\n",
    "    else:\n",
    "        print('   ‚öôÔ∏è  Starting Foundry Local service...')\n",
    "        subprocess.run(['foundry', 'service', 'start'], \n",
    "                      capture_output=True, text=True, timeout=30)\n",
    "        time.sleep(5)\n",
    "        print('   ‚úÖ Service started')\n",
    "except Exception as e:\n",
    "    print(f'   ‚ö†Ô∏è  Could not verify service: {e}')\n",
    "    print('   üí° Try manually: foundry service start')\n",
    "\n",
    "# Step 2: Load each model with --retain true\n",
    "print(f'\\nü§ñ Step 2: Loading models with retention...')\n",
    "for i, model in enumerate(REQUIRED_MODELS, 1):\n",
    "    print(f'   [{i}/{len(REQUIRED_MODELS)}] Starting {model}...')\n",
    "    try:\n",
    "        subprocess.Popen(['foundry', 'model', 'run', model, '--retain', 'true'],\n",
    "                        stdout=subprocess.DEVNULL,\n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        print(f'       ‚úÖ {model} loading in background')\n",
    "    except Exception as e:\n",
    "        print(f'       ‚ùå Error starting {model}: {e}')\n",
    "\n",
    "# Step 3: Verify models are ready\n",
    "print(f'\\n‚úÖ Step 3: Verifying models (this may take 2-3 minutes)...')\n",
    "print('=' * 70)\n",
    "\n",
    "try:\n",
    "    from workshop_utils import get_client\n",
    "    \n",
    "    ready_models = []\n",
    "    max_attempts = 30\n",
    "    attempt = 0\n",
    "    \n",
    "    while len(ready_models) < len(REQUIRED_MODELS) and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f'\\n   Attempt {attempt}/{max_attempts}...')\n",
    "        \n",
    "        for model in REQUIRED_MODELS:\n",
    "            if model in ready_models:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                manager, client, model_id = get_client(model, None)\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_id,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    max_tokens=5,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    ready_models.append(model)\n",
    "                    print(f'   ‚úÖ {model} is READY')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if 'connection' in error_msg or 'timeout' in error_msg:\n",
    "                    print(f'   ‚è≥ {model} still loading...')\n",
    "                else:\n",
    "                    print(f'   ‚ö†Ô∏è  {model} error: {str(e)[:60]}...')\n",
    "        \n",
    "        if len(ready_models) == len(REQUIRED_MODELS):\n",
    "            break\n",
    "            \n",
    "        if len(ready_models) < len(REQUIRED_MODELS):\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Final status\n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'üì¶ Final Status: {len(ready_models)}/{len(REQUIRED_MODELS)} models ready')\n",
    "    \n",
    "    for model in REQUIRED_MODELS:\n",
    "        if model in ready_models:\n",
    "            print(f'   ‚úÖ {model} - READY (retained in memory)')\n",
    "        else:\n",
    "            print(f'   ‚ùå {model} - NOT READY')\n",
    "    \n",
    "    if len(ready_models) == len(REQUIRED_MODELS):\n",
    "        print('\\nüéâ All models loaded and verified!')\n",
    "        print('   ‚úÖ Ready for intent-based routing')\n",
    "    else:\n",
    "        print(f'\\n‚ö†Ô∏è  Some models not ready. Check: foundry model ls')\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f'\\n‚ùå Cannot import workshop_utils: {e}')\n",
    "    print('   üí° Ensure workshop_utils.py is in ../samples/')\n",
    "except Exception as e:\n",
    "    print(f'\\n‚ùå Verification error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682909b",
   "metadata": {},
   "source": [
    "## üéØ ŒíŒÆŒºŒ± 4: Œ°œçŒ∏ŒºŒπœÉŒ∑ ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑œÇ Œ†œÅŒøŒ∏Œ≠œÉŒµœâŒΩ & ŒöŒ±œÑŒ±ŒªœåŒ≥ŒøœÖ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ\n",
    "\n",
    "Œ°œÖŒ∏ŒºŒØœÉœÑŒµ œÑŒø œÉœçœÉœÑŒ∑ŒºŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ ŒºŒµ:\n",
    "- **ŒöŒ±ŒΩœåŒΩŒµœÇ Œ†œÅŒøŒ∏Œ≠œÉŒµœâŒΩ**: Œ†œÅœåœÑœÖœÄŒ± Regex Œ≥ŒπŒ± œÑŒ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑ Œ±ŒπœÑŒ∑ŒºŒ¨œÑœâŒΩ\n",
    "- **ŒöŒ±œÑŒ¨ŒªŒøŒ≥Œø ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ**: ŒëŒΩœÑŒπœÉœÑŒøŒπœáŒØŒ∂ŒµŒπ œÑŒπœÇ Œ¥œÖŒΩŒ±œÑœåœÑŒ∑œÑŒµœÇ œÑœâŒΩ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ œÉœÑŒπœÇ Œ∫Œ±œÑŒ∑Œ≥ŒøœÅŒØŒµœÇ œÄœÅŒøŒ∏Œ≠œÉŒµœâŒΩ\n",
    "- **Œ£œçœÉœÑŒ∑ŒºŒ± Œ†œÅŒøœÑŒµœÅŒ±ŒπœåœÑŒ∑œÑŒ±œÇ**: ŒöŒ±Œ∏ŒøœÅŒØŒ∂ŒµŒπ œÑŒ∑ŒΩ ŒµœÄŒπŒªŒøŒ≥ŒÆ ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ œåœÑŒ±ŒΩ œÑŒ±ŒπœÅŒπŒ¨Œ∂ŒøœÖŒΩ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒºŒøŒΩœÑŒ≠ŒªŒ±\n",
    "\n",
    "**Œ†ŒªŒµŒøŒΩŒµŒ∫œÑŒÆŒºŒ±œÑŒ± ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ CPU**:\n",
    "- ‚úÖ ŒîŒµŒΩ Œ±œÄŒ±ŒπœÑŒµŒØœÑŒ±Œπ GPU\n",
    "- ‚úÖ Œ£œÑŒ±Œ∏ŒµœÅŒÆ Œ±œÄœåŒ¥ŒøœÉŒ∑\n",
    "- ‚úÖ ŒßŒ±ŒºŒ∑ŒªœåœÑŒµœÅŒ∑ Œ∫Œ±œÑŒ±ŒΩŒ¨ŒªœâœÉŒ∑ ŒµŒΩŒ≠œÅŒ≥ŒµŒπŒ±œÇ\n",
    "- ‚úÖ Œ†œÅŒøŒ≤ŒªŒ≠œàŒπŒºŒ∑ œáœÅŒÆœÉŒ∑ ŒºŒΩŒÆŒºŒ∑œÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3620a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Active Model Catalog (Hardware-Optimized Aliases)\n",
      "======================================================================\n",
      "üí° Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   ‚Ä¢ phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   ‚Ä¢ qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   ‚Ä¢ phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   ‚Ä¢ qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "‚úÖ Intent detection and model selection configured\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üí° Using model aliases - Foundry automatically selects CPU variants\n",
      "\n",
      "   ‚Ä¢ phi-4-mini\n",
      "     Capabilities: general, summarize, reasoning\n",
      "     Priority: 3\n",
      "\n",
      "   ‚Ä¢ qwen2.5-0.5b\n",
      "     Capabilities: classification, fast, general\n",
      "     Priority: 1\n",
      "\n",
      "   ‚Ä¢ phi-3.5-mini\n",
      "     Capabilities: code, refactor, technical\n",
      "     Priority: 2\n",
      "\n",
      "   ‚Ä¢ qwen2.5-coder-0.5b\n",
      "     Capabilities: code, programming, debug\n",
      "     Priority: 1\n",
      "\n",
      "‚úÖ Intent detection and model selection configured\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Model capability catalog (maps model aliases to capabilities)\n",
    "# Use base aliases - Foundry Local will automatically select CPU variants\n",
    "CATALOG = {\n",
    "    'phi-4-mini': {\n",
    "        'capabilities': ['general', 'summarize', 'reasoning'],\n",
    "        'priority': 3\n",
    "    },\n",
    "    'qwen2.5-0.5b': {\n",
    "        'capabilities': ['classification', 'fast', 'general'],\n",
    "        'priority': 1\n",
    "    },\n",
    "    'phi-3.5-mini': {\n",
    "        'capabilities': ['code', 'refactor', 'technical'],\n",
    "        'priority': 2\n",
    "    },\n",
    "    'qwen2.5-coder-0.5b': {\n",
    "        'capabilities': ['code', 'programming', 'debug'],\n",
    "        'priority': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Filter to only include models recommended for this system\n",
    "CATALOG = {k: v for k, v in CATALOG.items() if k in model_aliases}\n",
    "\n",
    "print('üìã Active Model Catalog (Hardware-Optimized Aliases)')\n",
    "print('=' * 70)\n",
    "print('üí° Using model aliases - Foundry automatically selects CPU variants')\n",
    "print()\n",
    "for model, info in CATALOG.items():\n",
    "    caps = ', '.join(info['capabilities'])\n",
    "    print(f'   ‚Ä¢ {model}')\n",
    "    print(f'     Capabilities: {caps}')\n",
    "    print(f'     Priority: {info[\"priority\"]}')\n",
    "    print()\n",
    "\n",
    "# Intent detection rules (regex pattern -> intent label)\n",
    "INTENT_RULES = [\n",
    "    (re.compile(r'code|refactor|function|debug|program', re.I), 'code'),\n",
    "    (re.compile(r'summar|abstract|tl;?dr|brief', re.I), 'summarize'),\n",
    "    (re.compile(r'classif|categor|label|sentiment', re.I), 'classification'),\n",
    "    (re.compile(r'explain|teach|describe', re.I), 'general'),\n",
    "]\n",
    "\n",
    "def detect_intent(prompt: str) -> str:\n",
    "    \"\"\"Detect intent from prompt using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        \n",
    "    Returns:\n",
    "        Intent label: 'code', 'summarize', 'classification', or 'general'\n",
    "    \"\"\"\n",
    "    for pattern, intent in INTENT_RULES:\n",
    "        if pattern.search(prompt):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def pick_model(intent: str) -> str:\n",
    "    \"\"\"Select best model for intent based on capabilities and priority.\n",
    "    \n",
    "    Args:\n",
    "        intent: Detected intent category\n",
    "        \n",
    "    Returns:\n",
    "        Model alias string, or first available model if no match\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        (alias, info['priority']) \n",
    "        for alias, info in CATALOG.items() \n",
    "        if intent in info['capabilities']\n",
    "    ]\n",
    "    \n",
    "    if candidates:\n",
    "        # Sort by priority (higher = better)\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[0][0]\n",
    "    \n",
    "    # Fallback to first available model\n",
    "    return list(CATALOG.keys())[0] if CATALOG else None\n",
    "\n",
    "print('‚úÖ Intent detection and model selection configured')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6d09",
   "metadata": {},
   "source": [
    "## üß™ ŒíŒÆŒºŒ± 5: ŒîŒøŒ∫ŒπŒºŒÆ ŒëŒΩŒØœáŒΩŒµœÖœÉŒ∑œÇ Œ†œÅœåŒ∏ŒµœÉŒ∑œÇ\n",
    "\n",
    "ŒïœÄŒπŒ≤ŒµŒ≤Œ±ŒπœéœÉœÑŒµ œåœÑŒπ œÑŒø œÉœçœÉœÑŒ∑ŒºŒ± Œ±ŒΩŒØœáŒΩŒµœÖœÉŒ∑œÇ œÄœÅœåŒ∏ŒµœÉŒ∑œÇ œÑŒ±ŒæŒπŒΩŒøŒºŒµŒØ œÉœâœÉœÑŒ¨ Œ¥ŒπŒ±œÜŒøœÅŒµœÑŒπŒ∫ŒøœçœÇ œÑœçœÄŒøœÖœÇ ŒµœÅœâœÑŒ∑ŒºŒ¨œÑœâŒΩ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fd85468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Intent Detection\n",
      "======================================================================\n",
      "\n",
      "Prompt: Refactor this Python function for better readabili...\n",
      "   Intent: code            ‚Üí Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Summarize the key points of this article...\n",
      "   Intent: summarize       ‚Üí Model: phi-4-mini\n",
      "\n",
      "Prompt: Classify this customer feedback as positive or neg...\n",
      "   Intent: classification  ‚Üí Model: qwen2.5-0.5b\n",
      "\n",
      "Prompt: Explain how edge AI differs from cloud AI...\n",
      "   Intent: general         ‚Üí Model: phi-4-mini\n",
      "\n",
      "Prompt: Write a function to calculate fibonacci numbers...\n",
      "   Intent: code            ‚Üí Model: phi-3.5-mini\n",
      "\n",
      "Prompt: Give me a brief overview of small language models...\n",
      "   Intent: summarize       ‚Üí Model: phi-4-mini\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Intent detection working correctly\n"
     ]
    }
   ],
   "source": [
    "# Test intent detection with sample prompts\n",
    "test_prompts = [\n",
    "    'Refactor this Python function for better readability',\n",
    "    'Summarize the key points of this article',\n",
    "    'Classify this customer feedback as positive or negative',\n",
    "    'Explain how edge AI differs from cloud AI',\n",
    "    'Write a function to calculate fibonacci numbers',\n",
    "    'Give me a brief overview of small language models'\n",
    "]\n",
    "\n",
    "print('üß™ Testing Intent Detection')\n",
    "print('=' * 70)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    intent = detect_intent(prompt)\n",
    "    model = pick_model(intent)\n",
    "    print(f'\\nPrompt: {prompt[:50]}...')\n",
    "    print(f'   Intent: {intent:15s} ‚Üí Model: {model}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('‚úÖ Intent detection working correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6a08b",
   "metadata": {},
   "source": [
    "## üöÄ ŒíŒÆŒºŒ± 6: Œ•ŒªŒøœÄŒøŒØŒ∑œÉŒ∑ œÑŒ∑œÇ ŒõŒµŒπœÑŒøœÖœÅŒ≥ŒØŒ±œÇ ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ\n",
    "\n",
    "ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒÆœÉœÑŒµ œÑŒ∑ Œ≤Œ±œÉŒπŒ∫ŒÆ ŒªŒµŒπœÑŒøœÖœÅŒ≥ŒØŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ œÄŒøœÖ:\n",
    "1. ŒëŒΩŒπœáŒΩŒµœçŒµŒπ œÑŒ∑ŒΩ œÄœÅœåŒ∏ŒµœÉŒ∑ Œ±œÄœå œÑŒø prompt\n",
    "2. ŒïœÄŒπŒªŒ≠Œ≥ŒµŒπ œÑŒø Œ≤Œ≠ŒªœÑŒπœÉœÑŒø ŒºŒøŒΩœÑŒ≠ŒªŒø\n",
    "3. ŒïŒ∫œÑŒµŒªŒµŒØ œÑŒø Œ±ŒØœÑŒ∑ŒºŒ± ŒºŒ≠œÉœâ œÑŒøœÖ Foundry Local SDK\n",
    "4. Œ†Œ±œÅŒ±Œ∫ŒøŒªŒøœÖŒ∏ŒµŒØ œÑŒ∑ œáœÅŒÆœÉŒ∑ œÑœâŒΩ tokens Œ∫Œ±Œπ œÑŒ± œÉœÜŒ¨ŒªŒºŒ±œÑŒ±\n",
    "\n",
    "**ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒµŒØ œÑŒø ŒºŒøœÑŒØŒ≤Œø workshop_utils**:\n",
    "- ŒëœÖœÑœåŒºŒ±œÑŒ∑ ŒµœÄŒ±ŒΩŒ¨ŒªŒ∑œàŒ∑ ŒºŒµ ŒµŒ∫Œ∏ŒµœÑŒπŒ∫ŒÆ Œ∫Œ±Œ∏œÖœÉœÑŒ≠œÅŒ∑œÉŒ∑\n",
    "- Œ£œÖŒºŒ≤Œ±œÑœå API ŒºŒµ OpenAI\n",
    "- Œ†Œ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ tokens Œ∫Œ±Œπ Œ¥ŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ œÉœÜŒ±ŒªŒºŒ¨œÑœâŒΩ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24cc251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Routing function ready\n",
      "   Using Foundry Local SDK via workshop_utils\n",
      "   Token tracking: Enabled\n",
      "   Retry logic: Automatic with exponential backoff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workshop_utils import chat_once\n",
    "\n",
    "# Fix RETRY_BACKOFF environment variable if it has comments\n",
    "if 'RETRY_BACKOFF' in os.environ:\n",
    "    retry_val = os.environ['RETRY_BACKOFF'].strip().split()[0]\n",
    "    try:\n",
    "        float(retry_val)\n",
    "        os.environ['RETRY_BACKOFF'] = retry_val\n",
    "    except ValueError:\n",
    "        os.environ['RETRY_BACKOFF'] = '1.0'\n",
    "\n",
    "def route(prompt: str, max_tokens: int = 200, temperature: float = 0.7):\n",
    "    \"\"\"Route prompt to appropriate model based on intent.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Detect intent using regex patterns\n",
    "    2. Select best model by capability + priority\n",
    "    3. Execute via Foundry Local SDK\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input text\n",
    "        max_tokens: Maximum tokens in response\n",
    "        temperature: Sampling temperature (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with: intent, model, output, tokens, usage, error\n",
    "    \"\"\"\n",
    "    intent = detect_intent(prompt)\n",
    "    model_alias = pick_model(intent)\n",
    "    \n",
    "    if not model_alias:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': None,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': 'No suitable model found'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Call Foundry Local via workshop_utils\n",
    "        text, usage = chat_once(\n",
    "            model_alias,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Extract token information\n",
    "        usage_info = {}\n",
    "        if usage:\n",
    "            usage_info['prompt_tokens'] = getattr(usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(usage, 'total_tokens', None)\n",
    "        \n",
    "        # Estimate if not provided\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            est_prompt = len(prompt) // 4\n",
    "            est_completion = len(text or '') // 4\n",
    "            usage_info['estimated_tokens'] = est_prompt + est_completion\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': (text or '').strip(),\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'model': model_alias,\n",
    "            'output': '',\n",
    "            'tokens': None,\n",
    "            'usage': {},\n",
    "            'error': f'{type(e).__name__}: {str(e)}'\n",
    "        }\n",
    "\n",
    "print('‚úÖ Routing function ready')\n",
    "print('   Using Foundry Local SDK via workshop_utils')\n",
    "print('   Token tracking: Enabled')\n",
    "print('   Retry logic: Automatic with exponential backoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5c915",
   "metadata": {},
   "source": [
    "## üéØ ŒíŒÆŒºŒ± 7: ŒïŒ∫œÑŒ≠ŒªŒµœÉŒ∑ ŒîŒøŒ∫ŒπŒºœéŒΩ ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ\n",
    "\n",
    "ŒîŒøŒ∫ŒπŒºŒ¨œÉœÑŒµ œÑŒø œÄŒªŒÆœÅŒµœÇ œÉœçœÉœÑŒ∑ŒºŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ ŒºŒµ Œ¥ŒπŒ¨œÜŒøœÅŒµœÇ œÄœÅŒøœÑœÅŒøœÄŒ≠œÇ Œ≥ŒπŒ± ŒΩŒ± ŒµœÄŒπŒ¥ŒµŒØŒæŒµœÑŒµ:\n",
    "- ŒëœÖœÑœåŒºŒ±œÑŒ∑ Œ±ŒΩŒØœáŒΩŒµœÖœÉŒ∑ œÄœÅŒøŒ∏Œ≠œÉŒµœâŒΩ\n",
    "- ŒàŒæœÖœÄŒΩŒ∑ ŒµœÄŒπŒªŒøŒ≥ŒÆ ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ\n",
    "- ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ œÄŒøŒªŒªŒ±œÄŒªœéŒΩ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ ŒºŒµ Œ¥ŒπŒ±œÑŒÆœÅŒ∑œÉŒ∑ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ\n",
    "- Œ†Œ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ tokens Œ∫Œ±Œπ ŒºŒµœÑœÅŒÆœÉŒµŒπœÇ Œ±œÄœåŒ¥ŒøœÉŒ∑œÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Running Intent-Based Routing Tests\n",
      "================================================================================\n",
      "\n",
      "[1/6] Testing prompt...\n",
      "Prompt: Refactor this Python function to make it more efficient and readable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expected Intent: code\n",
      "   Detected Intent: code ‚úÖ\n",
      "   Selected Model:  phi-3.5-mini\n",
      "   ‚úÖ Response: To refactor a Python function for efficiency and readability, I would need to see the specific funct...\n",
      "   üìä Tokens: ~158 (estimated)\n",
      "\n",
      "[2/6] Testing prompt...\n",
      "Prompt: Summarize the key benefits of using small language models at the edge\n",
      "   Expected Intent: summarize\n",
      "   Detected Intent: summarize ‚úÖ\n",
      "   Selected Model:  phi-4-mini\n",
      "   ‚ùå Error: APIConnectionError: Connection error.\n",
      "\n",
      "[3/6] Testing prompt...\n",
      "Prompt: Classify this user feedback: The app is slow but the UI looks great\n",
      "   Expected Intent: classification\n",
      "   Detected Intent: classification ‚úÖ\n",
      "   Selected Model:  qwen2.5-0.5b\n",
      "   ‚ùå Error: APIConnectionError: Connection error.\n",
      "\n",
      "[4/6] Testing prompt...\n",
      "Prompt: Explain the difference between local and cloud inference\n",
      "   Expected Intent: general\n",
      "   Detected Intent: general ‚úÖ\n",
      "   Selected Model:  phi-4-mini\n",
      "   ‚ùå Error: APIConnectionError: Connection error.\n",
      "\n",
      "[5/6] Testing prompt...\n",
      "Prompt: Write a Python function to calculate the Fibonacci sequence\n"
     ]
    }
   ],
   "source": [
    "# Test prompts covering all intent categories\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Refactor this Python function to make it more efficient and readable',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Summarize the key benefits of using small language models at the edge',\n",
    "        'expected_intent': 'summarize'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Classify this user feedback: The app is slow but the UI looks great',\n",
    "        'expected_intent': 'classification'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Explain the difference between local and cloud inference',\n",
    "        'expected_intent': 'general'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Write a Python function to calculate the Fibonacci sequence',\n",
    "        'expected_intent': 'code'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'Give me a brief overview of the Phi model family',\n",
    "        'expected_intent': 'summarize'\n",
    "    }\n",
    "]\n",
    "\n",
    "print('üéØ Running Intent-Based Routing Tests')\n",
    "print('=' * 80)\n",
    "\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f'\\n[{i}/{len(test_cases)}] Testing prompt...')\n",
    "    print(f'Prompt: {test[\"prompt\"]}')\n",
    "    \n",
    "    result = route(test['prompt'], max_tokens=150)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f'   Expected Intent: {test[\"expected_intent\"]}')\n",
    "    print(f'   Detected Intent: {result[\"intent\"]} {\"‚úÖ\" if result[\"intent\"] == test[\"expected_intent\"] else \"‚ö†Ô∏è\"}')\n",
    "    print(f'   Selected Model:  {result[\"model\"]}')\n",
    "    \n",
    "    if result['error']:\n",
    "        print(f'   ‚ùå Error: {result[\"error\"]}')\n",
    "    else:\n",
    "        output_preview = result['output'][:100] + '...' if len(result['output']) > 100 else result['output']\n",
    "        print(f'   ‚úÖ Response: {output_preview}')\n",
    "        \n",
    "        tokens = result.get('tokens', 0)\n",
    "        if tokens:\n",
    "            usage = result.get('usage', {})\n",
    "            if 'estimated_tokens' in usage:\n",
    "                print(f'   üìä Tokens: ~{tokens} (estimated)')\n",
    "            else:\n",
    "                print(f'   üìä Tokens: {tokens}')\n",
    "\n",
    "# Summary statistics\n",
    "print('\\n' + '=' * 80)\n",
    "print('üìä ROUTING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if not r['error'])\n",
    "total_tokens = sum(r.get('tokens', 0) or 0 for r in results if not r['error'])\n",
    "intent_accuracy = sum(1 for i, r in enumerate(results) if r['intent'] == test_cases[i]['expected_intent'])\n",
    "\n",
    "print(f'Total Prompts:        {len(results)}')\n",
    "print(f'‚úÖ Successful:         {success_count}/{len(results)}')\n",
    "print(f'‚ùå Failed:             {len(results) - success_count}')\n",
    "print(f'üéØ Intent Accuracy:    {intent_accuracy}/{len(results)} ({intent_accuracy/len(results)*100:.1f}%)')\n",
    "print(f'üìä Total Tokens Used:  {total_tokens}')\n",
    "\n",
    "# Model usage distribution\n",
    "print('\\nüìã Model Usage Distribution:')\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    if r['model']:\n",
    "        model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "\n",
    "for model, count in sorted(model_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(results)) * 100\n",
    "    print(f'   ‚Ä¢ {model}: {count} requests ({percentage:.1f}%)')\n",
    "\n",
    "if success_count == len(results):\n",
    "    print('\\nüéâ All routing tests passed successfully!')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  {len(results) - success_count} test(s) failed')\n",
    "    print('   Check Foundry Local service: foundry service status')\n",
    "    print('   Verify models loaded: foundry model ls')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764811e",
   "metadata": {},
   "source": [
    "## üîß ŒíŒÆŒºŒ± 8: ŒîŒπŒ±Œ¥œÅŒ±œÉœÑŒπŒ∫ŒÆ ŒîŒøŒ∫ŒπŒºŒÆ\n",
    "\n",
    "ŒîŒøŒ∫ŒπŒºŒ¨œÉœÑŒµ œÑŒπœÇ Œ¥ŒπŒ∫Œ≠œÇ œÉŒ±œÇ œÄœÅŒøœÑœÅŒøœÄŒ≠œÇ Œ≥ŒπŒ± ŒΩŒ± Œ¥ŒµŒØœÑŒµ œÑŒø œÉœçœÉœÑŒ∑ŒºŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ œÉŒµ Œ¥œÅŒ¨œÉŒ∑!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Interactive Routing Test\n",
      "================================================================================\n",
      "Your prompt: Explain how model quantization reduces memory usage\n",
      "\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "‚úÖ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Tokens used: 292\n",
      "\n",
      "üí° Try different prompts to test routing behavior!\n",
      "Detected Intent: general\n",
      "Selected Model:  phi-4-mini\n",
      "\n",
      "‚úÖ Response:\n",
      "--------------------------------------------------------------------------------\n",
      "Model quantization is a technique used to reduce the memory footprint of a machine learning model, particularly deep learning models. It works by converting the high-precision weights of a neural network, typically represented as 32-bit floating-point numbers, into lower-precision representations, such as 8-bit integers or even binary values.\n",
      "\n",
      "\n",
      "The primary reason for quantization is to decrease the amount of memory required to store the model's parameters. Since floating-point numbers take up more space than integers, by quantizing the weights, we can significantly reduce the model's size. This reduction in size not only saves memory but also can lead to faster computation during inference, as integer operations are generally faster than floating-point operations on many hardware platforms.\n",
      "\n",
      "\n",
      "However, quantization can introduce some loss of accuracy because the lower precision representation may not capture the full range of values that the floating-point representation can. To mitigate this, techniques such as quantization-aware training can be used, where the model is trained with quantization in mind,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Tokens used: 292\n",
      "\n",
      "üí° Try different prompts to test routing behavior!\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - modify the prompt and run this cell\n",
    "custom_prompt = \"Explain how model quantization reduces memory usage\"\n",
    "\n",
    "print('üéØ Interactive Routing Test')\n",
    "print('=' * 80)\n",
    "print(f'Your prompt: {custom_prompt}')\n",
    "print()\n",
    "\n",
    "result = route(custom_prompt, max_tokens=200)\n",
    "\n",
    "print(f'Detected Intent: {result[\"intent\"]}')\n",
    "print(f'Selected Model:  {result[\"model\"]}')\n",
    "print()\n",
    "\n",
    "if result['error']:\n",
    "    print(f'‚ùå Error: {result[\"error\"]}')\n",
    "else:\n",
    "    print('‚úÖ Response:')\n",
    "    print('-' * 80)\n",
    "    print(result['output'])\n",
    "    print('-' * 80)\n",
    "    \n",
    "    if result['tokens']:\n",
    "        print(f'\\nüìä Tokens used: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\nüí° Try different prompts to test routing behavior!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17226c",
   "metadata": {},
   "source": [
    "## üìä ŒíŒÆŒºŒ± 9: ŒëŒΩŒ¨ŒªœÖœÉŒ∑ ŒëœÄœåŒ¥ŒøœÉŒ∑œÇ\n",
    "\n",
    "ŒëŒΩŒ±ŒªœçœÉœÑŒµ œÑŒ∑ŒΩ Œ±œÄœåŒ¥ŒøœÉŒ∑ œÑŒøœÖ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ Œ∫Œ±Œπ œÑŒ∑ œáœÅŒÆœÉŒ∑ œÑŒøœÖ ŒºŒøŒΩœÑŒ≠ŒªŒøœÖ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Performance Benchmark\n",
      "================================================================================\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Write a hello world function...\n",
      "   Model: phi-3.5-mini\n",
      "   Time: 3.31s\n",
      "   Tokens: 60\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Summarize: AI at the edge is powerful...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 84\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Classify: Good product...\n",
      "   Model: qwen2.5-0.5b\n",
      "   Time: 7.21s\n",
      "   Tokens: 69\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "üìä Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "üí° Note: First request may be slower due to model initialization\n",
      "================================================================================\n",
      "\n",
      "Prompt: Explain edge computing...\n",
      "   Model: phi-4-mini\n",
      "   Time: 49.67s\n",
      "   Tokens: 72\n",
      "\n",
      "================================================================================\n",
      "üìä Performance Statistics:\n",
      "   Average response time: 27.46s\n",
      "   Fastest response:      3.31s\n",
      "   Slowest response:      49.67s\n",
      "\n",
      "üí° Note: First request may be slower due to model initialization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Performance benchmark\n",
    "benchmark_prompts = [\n",
    "    'Write a hello world function',\n",
    "    'Summarize: AI at the edge is powerful',\n",
    "    'Classify: Good product',\n",
    "    'Explain edge computing'\n",
    "]\n",
    "\n",
    "print('‚ö° Performance Benchmark')\n",
    "print('=' * 80)\n",
    "\n",
    "timings = []\n",
    "for prompt in benchmark_prompts:\n",
    "    start = time.time()\n",
    "    result = route(prompt, max_tokens=50)\n",
    "    duration = time.time() - start\n",
    "    timings.append(duration)\n",
    "    \n",
    "    print(f'\\nPrompt: {prompt[:40]}...')\n",
    "    print(f'   Model: {result[\"model\"]}')\n",
    "    print(f'   Time: {duration:.2f}s')\n",
    "    if result.get('tokens'):\n",
    "        print(f'   Tokens: {result[\"tokens\"]}')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('üìä Performance Statistics:')\n",
    "print(f'   Average response time: {sum(timings)/len(timings):.2f}s')\n",
    "print(f'   Fastest response:      {min(timings):.2f}s')\n",
    "print(f'   Slowest response:      {max(timings):.2f}s')\n",
    "print('\\nüí° Note: First request may be slower due to model initialization')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db64ff",
   "metadata": {},
   "source": [
    "## üéì ŒíŒ±œÉŒπŒ∫Œ¨ Œ£œÖŒºœÄŒµœÅŒ¨œÉŒºŒ±œÑŒ± & ŒïœÄœåŒºŒµŒΩŒ± ŒíŒÆŒºŒ±œÑŒ±\n",
    "\n",
    "### ‚úÖ Œ§Œπ ŒàœáŒµœÑŒµ ŒúŒ¨Œ∏ŒµŒπ\n",
    "\n",
    "1. **ŒîœÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ ŒíŒ¨œÉŒµŒπ Œ†œÅœåŒ∏ŒµœÉŒ∑œÇ**: ŒëœÖœÑœåŒºŒ±œÑŒ∑ œÑŒ±ŒæŒπŒΩœåŒºŒ∑œÉŒ∑ œÄœÅŒøœÑœÅŒøœÄœéŒΩ Œ∫Œ±Œπ Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑ œÉŒµ ŒµŒæŒµŒπŒ¥ŒπŒ∫ŒµœÖŒºŒ≠ŒΩŒ± ŒºŒøŒΩœÑŒ≠ŒªŒ±  \n",
    "2. **ŒïœÄŒπŒªŒøŒ≥ŒÆ ŒúŒµ ŒìŒΩœéŒºŒøŒΩŒ± Œ§Œ∑ ŒúŒΩŒÆŒºŒ∑**: ŒïœÄŒπŒªŒøŒ≥ŒÆ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ CPU ŒºŒµ Œ≤Œ¨œÉŒ∑ œÑŒ∑ Œ¥ŒπŒ±Œ∏Œ≠œÉŒπŒºŒ∑ ŒºŒΩŒÆŒºŒ∑ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ  \n",
    "3. **ŒîŒπŒ±œÑŒÆœÅŒ∑œÉŒ∑ Œ†ŒøŒªŒªŒ±œÄŒªœéŒΩ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ**: ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒÆœÉœÑŒµ `--retain true` Œ≥ŒπŒ± ŒΩŒ± Œ¥ŒπŒ±œÑŒ∑œÅŒµŒØœÑŒµ œÄŒøŒªŒªŒ¨ ŒºŒøŒΩœÑŒ≠ŒªŒ± œÜŒøœÅœÑœâŒºŒ≠ŒΩŒ±  \n",
    "4. **Œ†Œ±œÅŒ±Œ≥œâŒ≥ŒπŒ∫Œ¨ ŒúŒøœÑŒØŒ≤Œ±**: ŒõŒøŒ≥ŒπŒ∫ŒÆ ŒµœÄŒ±ŒΩŒ±œÄœÅŒøœÉœÄŒ¨Œ∏ŒµŒπŒ±œÇ, Œ¥ŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ œÉœÜŒ±ŒªŒºŒ¨œÑœâŒΩ Œ∫Œ±Œπ œÄŒ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ tokens  \n",
    "5. **ŒíŒµŒªœÑŒπœÉœÑŒøœÄŒøŒØŒ∑œÉŒ∑ CPU**: ŒëœÄŒøœÑŒµŒªŒµœÉŒºŒ±œÑŒπŒ∫ŒÆ Œ±ŒΩŒ¨œÄœÑœÖŒæŒ∑ œáœâœÅŒØœÇ Œ±œÄŒ±ŒπœÑŒÆœÉŒµŒπœÇ GPU  \n",
    "\n",
    "### üöÄ ŒôŒ¥Œ≠ŒµœÇ Œ≥ŒπŒ± Œ†ŒµŒπœÅŒ±ŒºŒ±œÑŒπœÉŒºœå\n",
    "\n",
    "1. **Œ†œÅŒøœÉŒ∏ŒÆŒ∫Œ∑ Œ†œÅŒøœÉŒ±œÅŒºŒøœÉŒºŒ≠ŒΩœâŒΩ Œ†œÅŒøŒ∏Œ≠œÉŒµœâŒΩ**:  \n",
    "   ```python\n",
    "   INTENT_RULES.append(\n",
    "       (re.compile(r'translate|convert', re.I), 'translation')\n",
    "   )\n",
    "   ```\n",
    "  \n",
    "2. **Œ¶œåœÅœÑœâœÉŒ∑ ŒïœÄŒπœÄŒªŒ≠ŒøŒΩ ŒúŒøŒΩœÑŒ≠ŒªœâŒΩ**:  \n",
    "   ```bash\n",
    "   foundry model run llama-3.2-1b-cpu --retain true\n",
    "   ```\n",
    "  \n",
    "3. **Œ°œçŒ∏ŒºŒπœÉŒ∑ ŒïœÄŒπŒªŒøŒ≥ŒÆœÇ ŒúŒøŒΩœÑŒ≠ŒªŒøœÖ**:  \n",
    "   - Œ†œÅŒøœÉŒ±œÅŒºœåœÉœÑŒµ œÑŒπœÇ œÑŒπŒºŒ≠œÇ œÄœÅŒøœÑŒµœÅŒ±ŒπœåœÑŒ∑œÑŒ±œÇ œÉœÑŒøŒΩ CATALOG  \n",
    "   - Œ†œÅŒøœÉŒ∏Œ≠œÉœÑŒµ œÄŒµœÅŒπœÉœÉœåœÑŒµœÅŒµœÇ ŒµœÑŒπŒ∫Œ≠œÑŒµœÇ Œ¥œÖŒΩŒ±œÑŒøœÑŒÆœÑœâŒΩ  \n",
    "   - ŒïœÜŒ±œÅŒºœåœÉœÑŒµ œÉœÑœÅŒ±œÑŒ∑Œ≥ŒπŒ∫Œ≠œÇ ŒµœÜŒµŒ¥œÅŒµŒØŒ±œÇ  \n",
    "\n",
    "4. **Œ†Œ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ ŒëœÄœåŒ¥ŒøœÉŒ∑œÇ**:  \n",
    "   ```python\n",
    "   import psutil\n",
    "   print(f\"Memory: {psutil.virtual_memory().percent}%\")\n",
    "   ```\n",
    "  \n",
    "\n",
    "### üìö Œ†œÅœåœÉŒ∏ŒµœÑŒøŒπ Œ†œåœÅŒøŒπ\n",
    "\n",
    "- **Foundry Local SDK**: https://github.com/microsoft/Foundry-Local  \n",
    "- **ŒîŒµŒØŒ≥ŒºŒ±œÑŒ± ŒïœÅŒ≥Œ±œÉœÑŒ∑œÅŒØŒøœÖ**: ../samples/  \n",
    "- **ŒúŒ¨Œ∏Œ∑ŒºŒ± Edge AI**: ../../Module08/  \n",
    "\n",
    "### üí° ŒíŒ≠ŒªœÑŒπœÉœÑŒµœÇ Œ†œÅŒ±Œ∫œÑŒπŒ∫Œ≠œÇ\n",
    "\n",
    "‚úÖ ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒÆœÉœÑŒµ ŒºŒøŒΩœÑŒ≠ŒªŒ± CPU Œ≥ŒπŒ± œÉœÖŒΩŒµœÄŒÆ œÉœÖŒºœÄŒµœÅŒπœÜŒøœÅŒ¨ œÉŒµ œåŒªŒµœÇ œÑŒπœÇ œÄŒªŒ±œÑœÜœåœÅŒºŒµœÇ  \n",
    "‚úÖ ŒïŒªŒ≠Œ≥œáŒµœÑŒµ œÄŒ¨ŒΩœÑŒ± œÑŒ∑ ŒºŒΩŒÆŒºŒ∑ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ œÄœÅŒπŒΩ œÜŒøœÅœÑœéœÉŒµœÑŒµ œÄŒøŒªŒªŒ±œÄŒªŒ¨ ŒºŒøŒΩœÑŒ≠ŒªŒ±  \n",
    "‚úÖ ŒßœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒÆœÉœÑŒµ `--retain true` Œ≥ŒπŒ± œÉŒµŒΩŒ¨œÅŒπŒ± Œ¥œÅŒøŒºŒøŒªœåŒ≥Œ∑œÉŒ∑œÇ  \n",
    "‚úÖ ŒïœÜŒ±œÅŒºœåœÉœÑŒµ œÉœâœÉœÑŒÆ Œ¥ŒπŒ±œáŒµŒØœÅŒπœÉŒ∑ œÉœÜŒ±ŒªŒºŒ¨œÑœâŒΩ Œ∫Œ±Œπ ŒµœÄŒ±ŒΩŒ±œÄœÅŒøœÉœÄŒ¨Œ∏ŒµŒπŒµœÇ  \n",
    "‚úÖ Œ†Œ±œÅŒ±Œ∫ŒøŒªŒøœÖŒ∏ŒÆœÉœÑŒµ œÑŒ∑ œáœÅŒÆœÉŒ∑ tokens Œ≥ŒπŒ± Œ≤ŒµŒªœÑŒπœÉœÑŒøœÄŒøŒØŒ∑œÉŒ∑ Œ∫œåœÉœÑŒøœÖœÇ/Œ±œÄœåŒ¥ŒøœÉŒ∑œÇ  \n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Œ£œÖŒ≥œáŒ±œÅŒ∑œÑŒÆœÅŒπŒ±!** ŒàœáŒµœÑŒµ Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒÆœÉŒµŒπ Œ≠ŒΩŒ±ŒΩ Œ¥œÅŒøŒºŒøŒªŒøŒ≥Œ∑œÑŒÆ ŒºŒøŒΩœÑŒ≠ŒªœâŒΩ Œ≤Œ¨œÉŒµŒπ œÄœÅœåŒ∏ŒµœÉŒ∑œÇ Œ≠œÑŒøŒπŒºŒø Œ≥ŒπŒ± œÄŒ±œÅŒ±Œ≥œâŒ≥ŒÆ, œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπœéŒΩœÑŒ±œÇ œÑŒø Foundry Local SDK ŒºŒµ ŒºŒøŒΩœÑŒ≠ŒªŒ± Œ≤ŒµŒªœÑŒπœÉœÑŒøœÄŒøŒπŒ∑ŒºŒ≠ŒΩŒ± Œ≥ŒπŒ± CPU!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ŒëœÄŒøœÄŒøŒØŒ∑œÉŒ∑ ŒµœÖŒ∏œçŒΩŒ∑œÇ**:  \nŒëœÖœÑœå œÑŒø Œ≠Œ≥Œ≥œÅŒ±œÜŒø Œ≠œáŒµŒπ ŒºŒµœÑŒ±œÜœÅŒ±œÉœÑŒµŒØ œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπœéŒΩœÑŒ±œÇ œÑŒ∑ŒΩ œÖœÄŒ∑œÅŒµœÉŒØŒ± Œ±œÖœÑœåŒºŒ±œÑŒ∑œÇ ŒºŒµœÑŒ¨œÜœÅŒ±œÉŒ∑œÇ [Co-op Translator](https://github.com/Azure/co-op-translator). Œ†Œ±œÅœåŒªŒø œÄŒøœÖ Œ∫Œ±œÑŒ±Œ≤Œ¨ŒªŒªŒøœÖŒºŒµ œÄœÅŒøœÉœÄŒ¨Œ∏ŒµŒπŒµœÇ Œ≥ŒπŒ± Œ±Œ∫œÅŒØŒ≤ŒµŒπŒ±, œÄŒ±œÅŒ±Œ∫Œ±ŒªŒøœçŒºŒµ ŒΩŒ± Œ≠œáŒµœÑŒµ œÖœÄœåœàŒ∑ œåœÑŒπ ŒøŒπ Œ±œÖœÑŒøŒºŒ±œÑŒøœÄŒøŒπŒ∑ŒºŒ≠ŒΩŒµœÇ ŒºŒµœÑŒ±œÜœÅŒ¨œÉŒµŒπœÇ ŒµŒΩŒ¥Œ≠œáŒµœÑŒ±Œπ ŒΩŒ± œÄŒµœÅŒπŒ≠œáŒøœÖŒΩ ŒªŒ¨Œ∏Œ∑ ŒÆ Œ±ŒΩŒ±Œ∫œÅŒØŒ≤ŒµŒπŒµœÇ. Œ§Œø œÄœÅœâœÑœåœÑœÖœÄŒø Œ≠Œ≥Œ≥œÅŒ±œÜŒø œÉœÑŒ∑ ŒºŒ∑œÑœÅŒπŒ∫ŒÆ œÑŒøœÖ Œ≥ŒªœéœÉœÉŒ± Œ∏Œ± œÄœÅŒ≠œÄŒµŒπ ŒΩŒ± Œ∏ŒµœâœÅŒµŒØœÑŒ±Œπ Œ∑ Œ±œÖŒ∏ŒµŒΩœÑŒπŒ∫ŒÆ œÄŒ∑Œ≥ŒÆ. ŒìŒπŒ± Œ∫œÅŒØœÉŒπŒºŒµœÇ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒµœÇ, œÉœÖŒΩŒπœÉœÑŒ¨œÑŒ±Œπ ŒµœÄŒ±Œ≥Œ≥ŒµŒªŒºŒ±œÑŒπŒ∫ŒÆ Œ±ŒΩŒ∏œÅœéœÄŒπŒΩŒ∑ ŒºŒµœÑŒ¨œÜœÅŒ±œÉŒ∑. ŒîŒµŒΩ œÜŒ≠œÅŒøœÖŒºŒµ ŒµœÖŒ∏œçŒΩŒ∑ Œ≥ŒπŒ± œÑœÖœáœåŒΩ œÄŒ±œÅŒµŒæŒ∑Œ≥ŒÆœÉŒµŒπœÇ ŒÆ ŒµœÉœÜŒ±ŒªŒºŒ≠ŒΩŒµœÇ ŒµœÅŒºŒ∑ŒΩŒµŒØŒµœÇ œÄŒøœÖ œÄœÅŒøŒ∫œçœÄœÑŒøœÖŒΩ Œ±œÄœå œÑŒ∑ œáœÅŒÆœÉŒ∑ Œ±œÖœÑŒÆœÇ œÑŒ∑œÇ ŒºŒµœÑŒ¨œÜœÅŒ±œÉŒ∑œÇ.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "149e1ff0f023ecf1f4221663a7928ff5",
   "translation_date": "2025-10-09T13:26:11+00:00",
   "source_file": "Workshop/notebooks/session06_models_router.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}