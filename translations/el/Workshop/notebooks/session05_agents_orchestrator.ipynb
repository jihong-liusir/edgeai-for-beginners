{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# Συνεδρία 5 – Πολυ-Πράκτορας Ορχηστρωτής\n",
    "\n",
    "Παρουσιάζει μια απλή διαδικασία δύο πρακτόρων (Ερευνητής -> Συντάκτης) χρησιμοποιώντας το Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Εγκατάσταση Εξαρτήσεων\n",
    "Εγκαθιστά το `foundry-local-sdk` και το `openai`, που απαιτούνται για την πρόσβαση σε τοπικά μοντέλα και την ολοκλήρωση συνομιλιών. Ιδενποτέντο.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# Σενάριο\n",
    "Υλοποιεί ένα ελάχιστο μοτίβο ορχηστρωτή δύο πρακτόρων:\n",
    "- **Πράκτορας Ερευνητής** συλλέγει συνοπτικά γεγονότα σε μορφή κουκκίδων\n",
    "- **Πράκτορας Συντάκτης** ξαναγράφει για εκτελεστική σαφήνεια\n",
    "\n",
    "Δείχνει κοινή μνήμη ανά πράκτορα, διαδοχική μεταβίβαση ενδιάμεσων αποτελεσμάτων και μια απλή λειτουργία αγωγού. Επεκτάσιμο σε περισσότερους ρόλους (π.χ., Κριτικός, Επαληθευτής) ή παράλληλα παρακλάδια.\n",
    "\n",
    "**Μεταβλητές Περιβάλλοντος:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - Προεπιλεγμένο μοντέλο προς χρήση (προεπιλογή: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - Κύριο μοντέλο πράκτορα (υπερισχύει του ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - Μοντέλο πράκτορα συντάκτη (προεπιλογή: κύριο)\n",
    "\n",
    "**Αναφορά SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**Πώς λειτουργεί:**\n",
    "1. **FoundryLocalManager** ξεκινά αυτόματα την υπηρεσία Foundry Local\n",
    "2. Κατεβάζει και φορτώνει το καθορισμένο μοντέλο (ή χρησιμοποιεί την αποθηκευμένη έκδοση)\n",
    "3. Παρέχει ένα συμβατό με OpenAI endpoint για αλληλεπίδραση\n",
    "4. Κάθε πράκτορας μπορεί να χρησιμοποιεί διαφορετικό μοντέλο για εξειδικευμένες εργασίες\n",
    "5. Η ενσωματωμένη λογική επαναπροσπάθειας χειρίζεται προσωρινές αποτυχίες με ευελιξία\n",
    "\n",
    "**Βασικά Χαρακτηριστικά:**\n",
    "- ✅ Αυτόματη ανακάλυψη και αρχικοποίηση υπηρεσίας\n",
    "- ✅ Διαχείριση κύκλου ζωής μοντέλου (λήψη, αποθήκευση, φόρτωση)\n",
    "- ✅ Συμβατότητα με OpenAI SDK για οικεία API\n",
    "- ✅ Υποστήριξη πολλαπλών μοντέλων για εξειδίκευση πρακτόρων\n",
    "- ✅ Ανθεκτικός χειρισμός σφαλμάτων με λογική επαναπροσπάθειας\n",
    "- ✅ Τοπική πρόβλεψη (χωρίς ανάγκη για cloud API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Βασικές Εισαγωγές & Τύποι\n",
    "Παρουσιάζει dataclasses για την αποθήκευση μηνυμάτων του πράκτορα και υποδείξεις τύπων για σαφήνεια. Εισάγει τον Foundry Local manager + OpenAI client για επόμενες ενέργειες του πράκτορα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Αρχικοποίηση Μοντέλου (Μοτίβο SDK)\n",
    "Χρησιμοποιεί το Foundry Local Python SDK για αξιόπιστη διαχείριση μοντέλων:\n",
    "- **FoundryLocalManager(alias)** - Εκκινεί αυτόματα την υπηρεσία και φορτώνει το μοντέλο μέσω του alias\n",
    "- **get_model_info(alias)** - Μετατρέπει το alias σε συγκεκριμένο ID μοντέλου\n",
    "- **manager.endpoint** - Παρέχει το endpoint της υπηρεσίας για τον OpenAI client\n",
    "- **manager.api_key** - Παρέχει το API key (προαιρετικό για τοπική χρήση)\n",
    "- Υποστηρίζει ξεχωριστά μοντέλα για διαφορετικούς agents (κύριος vs editor)\n",
    "- Ενσωματωμένη λογική επαναπροσπάθειας με εκθετική καθυστέρηση για ανθεκτικότητα\n",
    "- Επαλήθευση σύνδεσης για να διασφαλιστεί ότι η υπηρεσία είναι έτοιμη\n",
    "\n",
    "**Βασικό Μοτίβο SDK:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**Διαχείριση Κύκλου Ζωής:**\n",
    "- Οι managers αποθηκεύονται παγκοσμίως για σωστή εκκαθάριση\n",
    "- Κάθε agent μπορεί να χρησιμοποιεί διαφορετικό μοντέλο για εξειδίκευση\n",
    "- Αυτόματη ανακάλυψη υπηρεσιών και διαχείριση σύνδεσης\n",
    "- Ομαλή επαναπροσπάθεια με εκθετική καθυστέρηση σε περιπτώσεις αποτυχίας\n",
    "\n",
    "Αυτό διασφαλίζει τη σωστή αρχικοποίηση πριν ξεκινήσει η ορχήστρωση των agents.\n",
    "\n",
    "**Αναφορά:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Κλάσεις Agent & Memory\n",
    "Ορίζει την ελαφριά `AgentMsg` για καταχωρήσεις μνήμης και την `Agent` που περιλαμβάνει:\n",
    "- **Ρόλος συστήματος** - Η προσωπικότητα και οι οδηγίες του Agent\n",
    "- **Ιστορικό μηνυμάτων** - Διατηρεί το πλαίσιο της συνομιλίας\n",
    "- **Μέθοδος act()** - Εκτελεί ενέργειες με σωστή διαχείριση σφαλμάτων\n",
    "\n",
    "Ο agent μπορεί να χρησιμοποιήσει διαφορετικά μοντέλα (κύριο vs editor) και διατηρεί απομονωμένο πλαίσιο ανά agent. Αυτό το μοτίβο επιτρέπει:\n",
    "- Επίμονη μνήμη κατά τη διάρκεια ενεργειών\n",
    "- Ευέλικτη ανάθεση μοντέλου ανά agent\n",
    "- Απομόνωση και ανάκτηση από σφάλματα\n",
    "- Εύκολη αλυσίδωση και ορχήστρωση\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Οργανωμένη Ροή Εργασιών\n",
    "Δημιουργεί δύο εξειδικευμένους πράκτορες:\n",
    "- **Ερευνητής**: Χρησιμοποιεί το κύριο μοντέλο, συλλέγει πραγματολογικές πληροφορίες\n",
    "- **Συντάκτης**: Μπορεί να χρησιμοποιήσει ξεχωριστό μοντέλο (αν έχει ρυθμιστεί), βελτιώνει και ξαναγράφει\n",
    "\n",
    "Η συνάρτηση `pipeline`:\n",
    "1. Ο Ερευνητής συλλέγει ακατέργαστες πληροφορίες\n",
    "2. Ο Συντάκτης τις βελτιώνει σε έτοιμο εκτελεστικό αποτέλεσμα\n",
    "3. Επιστρέφει τόσο τα ενδιάμεσα όσο και τα τελικά αποτελέσματα\n",
    "\n",
    "Αυτό το μοτίβο επιτρέπει:\n",
    "- Εξειδίκευση μοντέλων (διαφορετικά μοντέλα για διαφορετικούς ρόλους)\n",
    "- Βελτίωση ποιότητας μέσω επεξεργασίας πολλών σταδίων\n",
    "- Ιχνηλασιμότητα της μετατροπής πληροφοριών\n",
    "- Εύκολη επέκταση σε περισσότερους πράκτορες ή παράλληλη επεξεργασία\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### Επεξήγηση: Εκτέλεση & Αποτελέσματα Pipeline\n",
    "Εκτελεί την πολυ-πρακτορική διαδικασία σε μια ερώτηση με θέμα τη συμμόρφωση και την καθυστέρηση για να δείξει:\n",
    "- Πολυ-σταδιακή μετατροπή πληροφοριών\n",
    "- Εξειδίκευση και συνεργασία πρακτόρων\n",
    "- Βελτίωση της ποιότητας του αποτελέσματος μέσω επεξεργασίας\n",
    "- Ιχνηλασιμότητα (διατηρούνται τόσο τα ενδιάμεσα όσο και τα τελικά αποτελέσματα)\n",
    "\n",
    "**Δομή Αποτελέσματος:**\n",
    "- `question` - Αρχικό ερώτημα χρήστη\n",
    "- `research` - Ακατέργαστο ερευνητικό αποτέλεσμα (συνοπτικά γεγονότα)\n",
    "- `final` - Επεξεργασμένη εκτελεστική σύνοψη\n",
    "- `models` - Ποια μοντέλα χρησιμοποιήθηκαν σε κάθε στάδιο\n",
    "\n",
    "**Ιδέες Επέκτασης:**\n",
    "1. Προσθήκη ενός Πράκτορα Κριτικής για έλεγχο ποιότητας\n",
    "2. Εφαρμογή παράλληλων πρακτόρων έρευνας για διαφορετικές πτυχές\n",
    "3. Προσθήκη ενός Πράκτορα Επαλήθευσης για έλεγχο γεγονότων\n",
    "4. Χρήση διαφορετικών μοντέλων για διαφορετικά επίπεδα πολυπλοκότητας\n",
    "5. Εφαρμογή βρόχων ανατροφοδότησης για επαναληπτική βελτίωση\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### Προχωρημένο: Προσαρμογή Ρύθμισης Πράκτορα\n",
    "\n",
    "Δοκιμάστε να προσαρμόσετε τη συμπεριφορά του πράκτορα τροποποιώντας τις μεταβλητές περιβάλλοντος πριν εκτελέσετε το κελί αρχικοποίησης:\n",
    "\n",
    "**Διαθέσιμα Μοντέλα:**\n",
    "- Χρησιμοποιήστε `foundry model ls` στο τερματικό για να δείτε όλα τα διαθέσιμα μοντέλα\n",
    "- Παραδείγματα: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b, κ.λπ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-09T13:29:14+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}