<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-18T14:56:52+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "id"
}
-->
# Bagian 1: Pembelajaran Lanjutan SLM - Dasar dan Optimasi

Small Language Models (SLMs) merupakan kemajuan penting dalam EdgeAI, memungkinkan kemampuan pemrosesan bahasa alami yang canggih pada perangkat dengan sumber daya terbatas. Memahami cara menerapkan, mengoptimalkan, dan memanfaatkan SLM secara efektif sangat penting untuk membangun solusi AI berbasis edge yang praktis.

## Pendahuluan

Dalam pelajaran ini, kita akan mengeksplorasi Small Language Models (SLMs) dan strategi implementasi lanjutan mereka. Kita akan membahas konsep dasar SLM, batas parameter dan klasifikasinya, teknik optimasi, serta strategi penerapan praktis untuk lingkungan komputasi edge.

## Tujuan Pembelajaran

Pada akhir pelajaran ini, Anda akan dapat:

- üî¢ Memahami batas parameter dan klasifikasi Small Language Models.
- üõ†Ô∏è Mengidentifikasi teknik optimasi utama untuk penerapan SLM pada perangkat edge.
- üöÄ Mempelajari penerapan strategi kuantisasi dan kompresi lanjutan untuk SLM.

## Memahami Batas Parameter dan Klasifikasi SLM

Small Language Models (SLMs) adalah model AI yang dirancang untuk memproses, memahami, dan menghasilkan konten bahasa alami dengan jumlah parameter yang jauh lebih sedikit dibandingkan model besar. Sementara Large Language Models (LLMs) memiliki ratusan miliar hingga triliunan parameter, SLM dirancang khusus untuk efisiensi dan penerapan di perangkat edge.

Kerangka klasifikasi parameter membantu kita memahami kategori berbeda dari SLM dan kasus penggunaannya yang sesuai. Klasifikasi ini sangat penting untuk memilih model yang tepat untuk skenario komputasi edge tertentu.

### Kerangka Klasifikasi Parameter

Memahami batas parameter membantu dalam memilih model yang sesuai untuk berbagai skenario komputasi edge:

- **üî¨ Micro SLMs**: 100M - 1.4B parameter (sangat ringan untuk perangkat seluler)
- **üì± Small SLMs**: 1.5B - 13.9B parameter (keseimbangan antara performa dan efisiensi)
- **‚öñÔ∏è Medium SLMs**: 14B - 30B parameter (mendekati kemampuan LLM sambil tetap efisien)

Batasan ini tetap fleksibel dalam komunitas penelitian, tetapi sebagian besar praktisi menganggap model dengan parameter kurang dari 30 miliar sebagai "kecil," dengan beberapa sumber menetapkan ambang batas lebih rendah pada 10 miliar parameter.

### Keunggulan Utama SLM

SLM menawarkan beberapa keunggulan mendasar yang membuatnya ideal untuk aplikasi komputasi edge:

**Efisiensi Operasional**: SLM memberikan waktu inferensi yang lebih cepat karena jumlah parameter yang lebih sedikit untuk diproses, menjadikannya ideal untuk aplikasi real-time. Mereka membutuhkan sumber daya komputasi yang lebih rendah, memungkinkan penerapan pada perangkat dengan sumber daya terbatas sambil mengonsumsi lebih sedikit energi dan mengurangi jejak karbon.

**Fleksibilitas Penerapan**: Model ini memungkinkan kemampuan AI di perangkat tanpa kebutuhan konektivitas internet, meningkatkan privasi dan keamanan melalui pemrosesan lokal, dapat disesuaikan untuk aplikasi spesifik domain, dan cocok untuk berbagai lingkungan komputasi edge.

**Efektivitas Biaya**: SLM menawarkan pelatihan dan penerapan yang hemat biaya dibandingkan LLM, dengan biaya operasional yang lebih rendah dan kebutuhan bandwidth yang lebih kecil untuk aplikasi edge.

## Strategi Akuisisi Model Lanjutan

### Ekosistem Hugging Face

Hugging Face berfungsi sebagai pusat utama untuk menemukan dan mengakses SLM terkini. Platform ini menyediakan sumber daya yang komprehensif untuk penemuan dan penerapan model:

**Fitur Penemuan Model**: Platform ini menawarkan penyaringan lanjutan berdasarkan jumlah parameter, jenis lisensi, dan metrik performa. Pengguna dapat mengakses alat perbandingan model secara berdampingan, tolok ukur performa real-time dan hasil evaluasi, serta demo WebGPU untuk pengujian langsung.

**Koleksi SLM yang Dikurasi**: Model populer termasuk Phi-4-mini-3.8B untuk tugas penalaran lanjutan, seri Qwen3 (0.6B/1.7B/4B) untuk aplikasi multibahasa, Google Gemma3 untuk tugas umum yang efisien, dan model eksperimental seperti BitNET untuk penerapan presisi ultra-rendah. Platform ini juga menampilkan koleksi yang digerakkan oleh komunitas dengan model khusus untuk domain tertentu serta varian yang telah dilatih dan disesuaikan untuk berbagai kasus penggunaan.

### Katalog Model Azure AI Foundry

Katalog Model Azure AI Foundry menyediakan akses tingkat perusahaan ke SLM dengan kemampuan integrasi yang ditingkatkan:

**Integrasi Perusahaan**: Katalog ini mencakup model yang dijual langsung oleh Azure dengan dukungan tingkat perusahaan dan SLA, termasuk Phi-4-mini-3.8B untuk kemampuan penalaran lanjutan dan Llama 3-8B untuk penerapan produksi. Model lain termasuk Qwen3 8B dari model sumber terbuka pihak ketiga yang terpercaya.

**Manfaat Perusahaan**: Alat bawaan untuk penyesuaian, pengamatan, dan AI yang bertanggung jawab terintegrasi dengan Provisioned Throughput yang dapat digunakan di seluruh keluarga model. Dukungan langsung dari Microsoft dengan SLA tingkat perusahaan, fitur keamanan dan kepatuhan yang terintegrasi, serta alur kerja penerapan yang komprehensif meningkatkan pengalaman perusahaan.

## Teknik Kuantisasi dan Optimasi Lanjutan

### Kerangka Optimasi Llama.cpp

Llama.cpp menyediakan teknik kuantisasi mutakhir untuk efisiensi maksimum dalam penerapan edge:

**Metode Kuantisasi**: Kerangka ini mendukung berbagai tingkat kuantisasi termasuk Q4_0 (kuantisasi 4-bit dengan pengurangan ukuran yang sangat baik - ideal untuk penerapan Qwen3-0.6B di perangkat seluler), Q5_1 (kuantisasi 5-bit yang menyeimbangkan kualitas dan kompresi - cocok untuk inferensi edge Phi-4-mini-3.8B), dan Q8_0 (kuantisasi 8-bit untuk kualitas mendekati asli - direkomendasikan untuk penggunaan produksi Google Gemma3). BitNET mewakili teknologi mutakhir dengan kuantisasi 1-bit untuk skenario kompresi ekstrem.

**Manfaat Implementasi**: Inferensi yang dioptimalkan untuk CPU dengan akselerasi SIMD menyediakan pemuatan dan eksekusi model yang efisien memori. Kompatibilitas lintas platform di seluruh arsitektur x86, ARM, dan Apple Silicon memungkinkan kemampuan penerapan yang tidak bergantung pada perangkat keras.

**Contoh Implementasi Praktis**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Perbandingan Jejak Memori**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Suite Optimasi Microsoft Olive

Microsoft Olive menawarkan alur kerja optimasi model yang komprehensif yang dirancang untuk lingkungan produksi:

**Teknik Optimasi**: Suite ini mencakup kuantisasi dinamis untuk pemilihan presisi otomatis (terutama efektif dengan model seri Qwen3), optimasi grafik dan fusi operator (dioptimalkan untuk arsitektur Google Gemma3), optimasi spesifik perangkat keras untuk CPU, GPU, dan NPU (dengan dukungan khusus untuk Phi-4-mini-3.8B pada perangkat ARM), dan pipeline optimasi multi-tahap. Model BitNET memerlukan alur kerja kuantisasi 1-bit khusus dalam kerangka Olive.

**Otomasi Alur Kerja**: Tolok ukur otomatis di seluruh varian optimasi memastikan pelestarian metrik kualitas selama optimasi. Integrasi dengan kerangka kerja ML populer seperti PyTorch dan ONNX menyediakan kemampuan optimasi penerapan cloud dan edge.

**Contoh Implementasi Praktis**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Kerangka Apple MLX

Apple MLX menyediakan optimasi asli yang dirancang khusus untuk perangkat Apple Silicon:

**Optimasi Apple Silicon**: Kerangka ini memanfaatkan arsitektur memori terpadu dengan integrasi Metal Performance Shaders, inferensi presisi campuran otomatis (terutama efektif dengan Google Gemma3), dan pemanfaatan bandwidth memori yang dioptimalkan. Phi-4-mini-3.8B menunjukkan performa luar biasa pada chip seri M, sementara Qwen3-1.7B memberikan keseimbangan optimal untuk penerapan MacBook Air.

**Fitur Pengembangan**: Dukungan API Python dan Swift dengan operasi array yang kompatibel dengan NumPy, kemampuan diferensiasi otomatis, dan integrasi yang mulus dengan alat pengembangan Apple menyediakan lingkungan pengembangan yang komprehensif.

**Contoh Implementasi Praktis**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategi Penerapan Produksi dan Inferensi

### Ollama: Penerapan Lokal yang Disederhanakan

Ollama menyederhanakan penerapan SLM dengan fitur siap perusahaan untuk lingkungan lokal dan edge:

**Kemampuan Penerapan**: Instalasi dan eksekusi model dengan satu perintah dengan penarikan dan caching model otomatis. Dukungan untuk Phi-4-mini-3.8B, seluruh seri Qwen3 (0.6B/1.7B/4B), dan Google Gemma3 dengan REST API untuk integrasi aplikasi serta kemampuan manajemen dan pengalihan multi-model. Model BitNET memerlukan konfigurasi build eksperimental untuk dukungan kuantisasi 1-bit.

**Fitur Lanjutan**: Dukungan penyesuaian model, pembuatan Dockerfile untuk penerapan terkontainerisasi, akselerasi GPU dengan deteksi otomatis, serta opsi kuantisasi dan optimasi model memberikan fleksibilitas penerapan yang komprehensif.

### VLLM: Inferensi Berkinerja Tinggi

VLLM memberikan optimasi inferensi tingkat produksi untuk skenario throughput tinggi:

**Optimasi Performa**: PagedAttention untuk komputasi perhatian yang efisien memori (terutama bermanfaat untuk arsitektur transformer Phi-4-mini-3.8B), batching dinamis untuk optimasi throughput (dioptimalkan untuk pemrosesan paralel seri Qwen3), paralelisme tensor untuk penskalaan multi-GPU (dukungan Google Gemma3), dan decoding spekulatif untuk pengurangan latensi. Model BitNET memerlukan kernel inferensi khusus untuk operasi 1-bit.

**Integrasi Perusahaan**: Endpoint API yang kompatibel dengan OpenAI, dukungan penerapan Kubernetes, integrasi pemantauan dan pengamatan, serta kemampuan penskalaan otomatis menyediakan solusi penerapan tingkat perusahaan.

### Foundry Local: Solusi Edge dari Microsoft

Foundry Local menyediakan kemampuan penerapan edge yang komprehensif untuk lingkungan perusahaan:

**Fitur Komputasi Edge**: Desain arsitektur offline-first dengan optimasi sumber daya terbatas, manajemen registri model lokal, dan kemampuan sinkronisasi edge-to-cloud memastikan penerapan edge yang andal.

**Keamanan dan Kepatuhan**: Pemrosesan data lokal untuk pelestarian privasi, kontrol keamanan tingkat perusahaan, pencatatan audit dan pelaporan kepatuhan, serta manajemen akses berbasis peran memberikan keamanan yang komprehensif untuk penerapan edge.

## Praktik Terbaik untuk Implementasi SLM

### Panduan Pemilihan Model

Saat memilih SLM untuk penerapan edge, pertimbangkan faktor berikut:

**Pertimbangan Jumlah Parameter**: Pilih micro SLM seperti Qwen3-0.6B untuk aplikasi seluler yang sangat ringan, small SLM seperti Qwen3-1.7B atau Google Gemma3 untuk skenario performa yang seimbang, dan medium SLM seperti Phi-4-mini-3.8B atau Qwen3-4B saat mendekati kemampuan LLM sambil tetap efisien. Model BitNET menawarkan kompresi ultra eksperimental untuk aplikasi penelitian tertentu.

**Keselarasan Kasus Penggunaan**: Sesuaikan kemampuan model dengan persyaratan aplikasi spesifik, dengan mempertimbangkan faktor seperti kualitas respons, kecepatan inferensi, batasan memori, dan persyaratan operasi offline.

### Pemilihan Strategi Optimasi

**Pendekatan Kuantisasi**: Pilih tingkat kuantisasi yang sesuai berdasarkan persyaratan kualitas dan batasan perangkat keras. Pertimbangkan Q4_0 untuk kompresi maksimum (ideal untuk penerapan seluler Qwen3-0.6B), Q5_1 untuk keseimbangan kualitas-kompresi (cocok untuk Phi-4-mini-3.8B dan Google Gemma3), dan Q8_0 untuk pelestarian kualitas mendekati asli (direkomendasikan untuk lingkungan produksi Qwen3-4B). Kuantisasi 1-bit BitNET mewakili batas kompresi ekstrem untuk aplikasi khusus.

**Pemilihan Kerangka Kerja**: Pilih kerangka optimasi berdasarkan perangkat keras target dan persyaratan penerapan. Gunakan Llama.cpp untuk penerapan yang dioptimalkan untuk CPU, Microsoft Olive untuk alur kerja optimasi yang komprehensif, dan Apple MLX untuk perangkat Apple Silicon.

## Contoh Model Praktis dan Kasus Penggunaan

### Skenario Penerapan Dunia Nyata

**Aplikasi Seluler**: Qwen3-0.6B unggul dalam aplikasi chatbot smartphone dengan jejak memori minimal, sementara Google Gemma3 memberikan performa seimbang untuk alat pendidikan berbasis tablet. Phi-4-mini-3.8B menawarkan kemampuan penalaran superior untuk aplikasi produktivitas seluler.

**Komputasi Desktop dan Edge**: Qwen3-1.7B memberikan performa optimal untuk aplikasi asisten desktop, Phi-4-mini-3.8B menyediakan kemampuan pembuatan kode lanjutan untuk alat pengembang, dan Qwen3-4B memungkinkan analisis dokumen yang canggih di lingkungan workstation.

**Penelitian dan Eksperimental**: Model BitNET memungkinkan eksplorasi inferensi presisi ultra-rendah untuk penelitian akademik dan aplikasi proof-of-concept yang memerlukan batasan sumber daya ekstrem.

### Tolok Ukur Performa dan Perbandingan

**Kecepatan Inferensi**: Qwen3-0.6B mencapai waktu inferensi tercepat pada CPU seluler, Google Gemma3 memberikan rasio kecepatan-kualitas yang seimbang untuk aplikasi umum, Phi-4-mini-3.8B menawarkan kecepatan penalaran superior untuk tugas kompleks, dan BitNET memberikan throughput maksimum teoritis dengan perangkat keras khusus.

**Persyaratan Memori**: Jejak memori model berkisar dari Qwen3-0.6B (di bawah 1GB kuantisasi) hingga Phi-4-mini-3.8B (sekitar 3-4GB kuantisasi), dengan BitNET mencapai jejak di bawah 500MB dalam konfigurasi eksperimental.

## Tantangan dan Pertimbangan

### Trade-off Performa

Penerapan SLM melibatkan pertimbangan trade-off antara ukuran model, kecepatan inferensi, dan kualitas output. Misalnya, sementara Qwen3-0.6B menawarkan kecepatan dan efisiensi yang luar biasa, Phi-4-mini-3.8B memberikan kemampuan penalaran superior dengan biaya kebutuhan sumber daya yang lebih besar. Google Gemma3 memberikan keseimbangan yang cocok untuk sebagian besar aplikasi umum.

### Kompatibilitas Perangkat Keras

Perangkat edge yang berbeda memiliki kemampuan dan batasan yang bervariasi. Qwen3-0.6B berjalan efisien pada prosesor ARM dasar, Google Gemma3 memerlukan sumber daya komputasi yang moderat, dan Phi-4-mini-3.8B mendapat manfaat dari perangkat keras edge kelas atas. Model BitNET memerlukan perangkat keras atau implementasi perangkat lunak khusus untuk operasi 1-bit yang optimal.

### Keamanan dan Privasi

Meskipun SLM memungkinkan pemrosesan lokal untuk meningkatkan privasi, langkah-langkah keamanan yang tepat harus diterapkan untuk melindungi model dan data di lingkungan edge. Hal ini sangat penting saat menerapkan model seperti Phi-4-mini-3.8B di lingkungan perusahaan atau seri Qwen3 dalam aplikasi multibahasa yang menangani data sensitif.

## Tren Masa Depan dalam Pengembangan SLM

Lanskap SLM terus berkembang dengan kemajuan dalam arsitektur model, teknik optimasi, dan strategi penerapan. Perkembangan masa depan mencakup arsitektur yang lebih efisien, metode kuantisasi yang lebih baik, dan integrasi yang lebih baik dengan akselerator perangkat keras edge.

Memahami tren ini dan tetap mengikuti teknologi yang muncul akan menjadi kunci untuk tetap terkini dengan praktik terbaik pengembangan dan penerapan SLM.

## ‚û°Ô∏è Langkah Selanjutnya

- [02: Implementasi Praktis SLM](02.SLMPracticalImplementation.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.