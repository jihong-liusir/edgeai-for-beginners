<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T14:59:12+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "id"
}
-->
# Penerapan Cloud dengan Kontainer - Solusi Skala Produksi

Tutorial komprehensif ini mencakup tiga pendekatan utama untuk menerapkan model Phi-4-mini-instruct dari Microsoft dalam lingkungan kontainer: vLLM, Ollama, dan SLM Engine dengan ONNX Runtime. Model dengan 3.8 miliar parameter ini merupakan pilihan optimal untuk tugas penalaran sambil tetap efisien untuk penerapan di perangkat edge.

## Daftar Isi

1. [Pengantar Penerapan Kontainer Phi-4-mini](../../../Module03)
2. [Tujuan Pembelajaran](../../../Module03)
3. [Memahami Klasifikasi Phi-4-mini](../../../Module03)
4. [Penerapan Kontainer vLLM](../../../Module03)
5. [Penerapan Kontainer Ollama](../../../Module03)
6. [SLM Engine dengan ONNX Runtime](../../../Module03)
7. [Kerangka Perbandingan](../../../Module03)
8. [Praktik Terbaik](../../../Module03)

## Pengantar Penerapan Kontainer Phi-4-mini

Small Language Models (SLMs) merupakan kemajuan penting dalam EdgeAI, memungkinkan kemampuan pemrosesan bahasa alami yang canggih pada perangkat dengan sumber daya terbatas. Tutorial ini berfokus pada strategi penerapan kontainer untuk Phi-4-mini-instruct dari Microsoft, model penalaran mutakhir yang menggabungkan kemampuan dengan efisiensi.

### Model Unggulan: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 miliar parameter)**: Model instruksi ringan terbaru dari Microsoft yang dirancang untuk lingkungan dengan keterbatasan memori/komputasi dengan kemampuan luar biasa dalam:
- **Penalaran matematis dan perhitungan kompleks**
- **Pembuatan kode, debugging, dan analisis**
- **Pemecahan masalah logis dan penalaran langkah demi langkah**
- **Aplikasi pendidikan yang membutuhkan penjelasan mendetail**
- **Pemanggilan fungsi dan integrasi alat**

Sebagai bagian dari kategori "Small SLMs" (1.5B - 13.9B parameter), Phi-4-mini menawarkan keseimbangan optimal antara kemampuan penalaran dan efisiensi sumber daya.

### Manfaat Penerapan Kontainer Phi-4-mini

- **Efisiensi Operasional**: Inferensi cepat untuk tugas penalaran dengan kebutuhan komputasi yang lebih rendah
- **Fleksibilitas Penerapan**: Kemampuan AI di perangkat dengan privasi yang lebih baik melalui pemrosesan lokal
- **Efektivitas Biaya**: Biaya operasional yang lebih rendah dibandingkan model yang lebih besar sambil tetap menjaga kualitas
- **Isolasi**: Pemisahan yang bersih antara instance model dan lingkungan eksekusi yang aman
- **Skalabilitas**: Skalabilitas horizontal yang mudah untuk meningkatkan throughput penalaran

## Tujuan Pembelajaran

Pada akhir tutorial ini, Anda akan dapat:

- Menerapkan dan mengoptimalkan Phi-4-mini-instruct dalam berbagai lingkungan kontainer
- Mengimplementasikan strategi kuantisasi dan kompresi canggih untuk berbagai skenario penerapan
- Mengonfigurasi orkestrasi kontainer siap produksi untuk beban kerja penalaran
- Mengevaluasi dan memilih kerangka kerja penerapan yang sesuai berdasarkan kebutuhan kasus penggunaan tertentu
- Menerapkan praktik terbaik untuk keamanan, pemantauan, dan skalabilitas dalam penerapan SLM berbasis kontainer

## Memahami Klasifikasi Phi-4-mini

### Spesifikasi Model

**Detail Teknis:**
- **Parameter**: 3.8 miliar (kategori Small SLM)
- **Arsitektur**: Transformer decoder-only padat dengan grouped-query attention
- **Panjang Konteks**: 128K token (32K direkomendasikan untuk performa optimal)
- **Kosakata**: 200K token dengan dukungan multibahasa
- **Data Pelatihan**: 5T token konten berkualitas tinggi yang padat dengan penalaran

### Persyaratan Sumber Daya

| Tipe Penerapan | RAM Minimum | RAM yang Direkomendasikan | VRAM (GPU) | Penyimpanan | Kasus Penggunaan Umum |
|-----------------|-------------|---------------------------|------------|-------------|-----------------------|
| **Pengembangan** | 6GB | 8GB | - | 8GB | Pengujian lokal, pembuatan prototipe |
| **Produksi CPU** | 8GB | 12GB | - | 10GB | Server edge, penerapan yang dioptimalkan biaya |
| **Produksi GPU** | 6GB | 8GB | 4-6GB | 8GB | Layanan penalaran throughput tinggi |
| **Edge Optimized** | 4GB | 6GB | - | 6GB | Penerapan kuantisasi, gateway IoT |

### Kemampuan Phi-4-mini

- **Keunggulan Matematis**: Pemecahan masalah aritmatika, aljabar, dan kalkulus tingkat lanjut
- **Kecerdasan Kode**: Pembuatan kode Python, JavaScript, dan multi-bahasa dengan debugging
- **Penalaran Logis**: Pemecahan masalah langkah demi langkah dan konstruksi solusi
- **Dukungan Pendidikan**: Penjelasan mendetail yang cocok untuk pembelajaran dan pengajaran
- **Pemanggilan Fungsi**: Dukungan bawaan untuk integrasi alat dan interaksi API

## Penerapan Kontainer vLLM

vLLM memberikan dukungan yang sangat baik untuk Phi-4-mini-instruct dengan performa inferensi yang dioptimalkan dan API yang kompatibel dengan OpenAI, menjadikannya ideal untuk layanan penalaran produksi.

### Contoh Memulai Cepat

#### Penerapan CPU Dasar (Pengembangan)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Penerapan Produksi dengan GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Konfigurasi Produksi

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pengujian Kemampuan Penalaran Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Penerapan Kontainer Ollama

Ollama memberikan dukungan yang sangat baik untuk Phi-4-mini-instruct dengan penerapan dan pengelolaan yang disederhanakan, menjadikannya ideal untuk pengembangan dan penerapan produksi yang seimbang.

### Pengaturan Cepat

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Konfigurasi Produksi

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimasi Model dan Variasi

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Contoh Penggunaan API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine dengan ONNX Runtime

ONNX Runtime memberikan performa optimal untuk penerapan edge dari Phi-4-mini-instruct dengan optimasi canggih dan kompatibilitas lintas platform.

### Pengaturan Dasar

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Implementasi Server yang Disederhanakan

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Skrip Konversi Model

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Konfigurasi Produksi

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pengujian Penerapan ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Kerangka Perbandingan

### Perbandingan Kerangka Kerja untuk Phi-4-mini

| Fitur | vLLM | Ollama | ONNX Runtime |
|-------|------|--------|--------------|
| **Kompleksitas Pengaturan** | Sedang | Mudah | Kompleks |
| **Performa (GPU)** | Sangat Baik (~25 tok/s) | Baik (~20 tok/s) | Cukup Baik (~15 tok/s) |
| **Performa (CPU)** | Baik (~8 tok/s) | Sangat Baik (~12 tok/s) | Sangat Baik (~15 tok/s) |
| **Penggunaan Memori** | 8-12GB | 6-10GB | 4-8GB |
| **Kompatibilitas API** | Kompatibel OpenAI | REST Kustom | FastAPI Kustom |
| **Pemanggilan Fungsi** | ✅ Bawaan | ✅ Didukung | ⚠️ Implementasi Kustom |
| **Dukungan Kuantisasi** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Kuantisasi ONNX |
| **Siap Produksi** | ✅ Sangat Baik | ✅ Baik | ✅ Cukup Baik |
| **Penerapan Edge** | Baik | Sangat Baik | Luar Biasa |

## Sumber Daya Tambahan

### Dokumentasi Resmi
- **Microsoft Phi-4 Model Card**: Spesifikasi dan panduan penggunaan yang mendetail
- **Dokumentasi vLLM**: Opsi konfigurasi dan optimasi lanjutan
- **Perpustakaan Model Ollama**: Model komunitas dan contoh kustomisasi
- **Panduan ONNX Runtime**: Strategi optimasi performa dan penerapan

### Alat Pengembangan
- **Hugging Face Transformers**: Untuk interaksi dan kustomisasi model
- **Spesifikasi API OpenAI**: Untuk pengujian kompatibilitas vLLM
- **Praktik Terbaik Docker**: Panduan keamanan dan optimasi kontainer
- **Penerapan Kubernetes**: Pola orkestrasi untuk skalabilitas produksi

### Sumber Daya Pembelajaran
- **Benchmarking Performa SLM**: Metodologi analisis perbandingan
- **Penerapan Edge AI**: Praktik terbaik untuk lingkungan dengan sumber daya terbatas
- **Optimasi Tugas Penalaran**: Strategi prompting untuk masalah matematis dan logis
- **Keamanan Kontainer**: Praktik penguatan untuk penerapan model AI

## Hasil Pembelajaran

Setelah menyelesaikan modul ini, Anda akan dapat:

1. Menerapkan model Phi-4-mini-instruct dalam lingkungan kontainer menggunakan berbagai kerangka kerja
2. Mengonfigurasi dan mengoptimalkan penerapan SLM untuk berbagai lingkungan perangkat keras
3. Mengimplementasikan praktik terbaik keamanan untuk penerapan AI berbasis kontainer
4. Membandingkan dan memilih kerangka kerja penerapan yang sesuai berdasarkan kebutuhan kasus penggunaan tertentu
5. Menerapkan strategi pemantauan dan skalabilitas untuk layanan SLM skala produksi

## Langkah Selanjutnya

- Kembali ke [Modul 1](../Module01/README.md)
- Kembali ke [Modul 2](../Module02/README.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diketahui bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.