<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T14:21:41+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "id"
}
-->
# Bagian 03 - Integrasi Model Context Protocol (MCP)

## Pengantar MCP (Model Context Protocol)

Model Context Protocol (MCP) adalah kerangka kerja revolusioner yang memungkinkan model bahasa berinteraksi dengan alat dan sistem eksternal secara standar. Berbeda dengan pendekatan tradisional di mana model terisolasi, MCP menciptakan jembatan antara model AI dan dunia nyata melalui protokol yang terdefinisi dengan baik.

### Apa itu MCP?

MCP berfungsi sebagai protokol komunikasi yang memungkinkan model bahasa untuk:
- Terhubung ke sumber data eksternal
- Menjalankan alat dan fungsi
- Berinteraksi dengan API dan layanan
- Mengakses informasi secara real-time
- Melakukan operasi multi-langkah yang kompleks

Protokol ini mengubah model bahasa statis menjadi agen dinamis yang mampu melakukan tugas praktis di luar generasi teks.

## Small Language Models (SLMs) dalam MCP

Small Language Models mewakili pendekatan yang efisien untuk penerapan AI, menawarkan beberapa keuntungan:

### Keuntungan SLMs
- **Efisiensi Sumber Daya**: Membutuhkan komputasi yang lebih rendah
- **Waktu Respons Lebih Cepat**: Mengurangi latensi untuk aplikasi real-time  
- **Efektivitas Biaya**: Kebutuhan infrastruktur minimal
- **Privasi**: Dapat dijalankan secara lokal tanpa transmisi data
- **Kustomisasi**: Lebih mudah disesuaikan untuk domain tertentu

### Mengapa SLMs Cocok dengan MCP

SLMs yang dipasangkan dengan MCP menciptakan kombinasi yang kuat di mana kemampuan penalaran model ditingkatkan oleh alat eksternal, mengimbangi jumlah parameter yang lebih kecil dengan fungsionalitas yang lebih baik.

## Ikhtisar Python MCP SDK

Python MCP SDK menyediakan dasar untuk membangun aplikasi yang mendukung MCP. SDK ini mencakup:

- **Library Klien**: Untuk terhubung ke server MCP
- **Kerangka Server**: Untuk membuat server MCP khusus
- **Handler Protokol**: Untuk mengelola komunikasi
- **Integrasi Alat**: Untuk menjalankan fungsi eksternal

## Implementasi Praktis: Klien MCP Phi-4

Mari kita eksplorasi implementasi dunia nyata menggunakan model mini Phi-4 dari Microsoft yang terintegrasi dengan kemampuan MCP.

### Arsitektur Sistem

Implementasi ini mengikuti arsitektur berlapis:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Komponen Inti

#### 1. Kelas Klien MCP

**BaseMCPClient**: Fondasi abstrak yang menyediakan fungsionalitas umum
- Protokol manajer konteks asinkron
- Definisi antarmuka standar
- Pengelolaan sumber daya

**Phi4MiniMCPClient**: Implementasi berbasis STDIO
- Komunikasi proses lokal
- Penanganan input/output standar
- Pengelolaan subprocess

**Phi4MiniSSEMCPClient**: Implementasi Server-Sent Events
- Komunikasi streaming HTTP
- Penanganan peristiwa real-time
- Konektivitas server berbasis web

#### 2. Integrasi LLM

**OllamaClient**: Hosting model lokal
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Penyajian berkinerja tinggi
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Pipeline Pemrosesan Alat

Pipeline pemrosesan alat mengubah alat MCP menjadi format yang kompatibel dengan model bahasa:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Memulai: Panduan Langkah-demi-Langkah

### Langkah 1: Pengaturan Lingkungan

Instal dependensi yang diperlukan:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Langkah 2: Konfigurasi Dasar

Atur variabel lingkungan Anda:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Langkah 3: Menjalankan Klien MCP Pertama Anda

**Pengaturan Dasar Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Menggunakan Backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Koneksi Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Server MCP Khusus:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Langkah 4: Penggunaan Programatik

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Fitur Lanjutan

### Dukungan Multi-Backend

Implementasi ini mendukung backend Ollama dan vLLM, memungkinkan Anda memilih berdasarkan kebutuhan:

- **Ollama**: Lebih baik untuk pengembangan dan pengujian lokal
- **vLLM**: Dioptimalkan untuk produksi dan skenario throughput tinggi

### Protokol Koneksi yang Fleksibel

Dua mode koneksi didukung:

**Mode STDIO**: Komunikasi proses langsung
- Latensi lebih rendah
- Cocok untuk alat lokal
- Pengaturan sederhana

**Mode SSE**: Streaming berbasis HTTP
- Mendukung jaringan
- Lebih baik untuk sistem terdistribusi
- Pembaruan real-time

### Kemampuan Integrasi Alat

Sistem dapat terintegrasi dengan berbagai alat:
- Otomasi web (Playwright)
- Operasi file
- Interaksi API
- Perintah sistem
- Fungsi khusus

## Penanganan Kesalahan dan Praktik Terbaik

### Manajemen Kesalahan yang Komprehensif

Implementasi ini mencakup penanganan kesalahan yang kuat untuk:

**Kesalahan Koneksi:**
- Kegagalan server MCP
- Timeout jaringan
- Masalah konektivitas

**Kesalahan Eksekusi Alat:**
- Alat yang hilang
- Validasi parameter
- Kegagalan eksekusi

**Kesalahan Pemrosesan Respons:**
- Masalah parsing JSON
- Ketidakkonsistenan format
- Anomali respons LLM

### Praktik Terbaik

1. **Pengelolaan Sumber Daya**: Gunakan manajer konteks asinkron
2. **Penanganan Kesalahan**: Terapkan blok try-catch yang komprehensif
3. **Logging**: Aktifkan level logging yang sesuai
4. **Keamanan**: Validasi input dan sanitasi output
5. **Performa**: Gunakan pooling koneksi dan caching

## Aplikasi Dunia Nyata

### Otomasi Web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Pemrosesan Data
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrasi API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimasi Performa

### Pengelolaan Memori
- Penanganan riwayat pesan yang efisien
- Pembersihan sumber daya yang tepat
- Pooling koneksi

### Optimasi Jaringan
- Operasi HTTP asinkron
- Timeout yang dapat dikonfigurasi
- Pemulihan kesalahan yang baik

### Pemrosesan Konkuren
- I/O non-blocking
- Eksekusi alat paralel
- Pola asinkron yang efisien

## Pertimbangan Keamanan

### Perlindungan Data
- Pengelolaan kunci API yang aman
- Validasi input
- Sanitasi output

### Keamanan Jaringan
- Dukungan HTTPS
- Default endpoint lokal
- Penanganan token yang aman

### Keamanan Eksekusi
- Penyaringan alat
- Lingkungan sandboxed
- Logging audit

## Kesimpulan

SLMs yang terintegrasi dengan MCP mewakili perubahan paradigma dalam pengembangan aplikasi AI. Dengan menggabungkan efisiensi model kecil dengan kekuatan alat eksternal, pengembang dapat menciptakan sistem cerdas yang efisien sumber daya dan sangat mampu.

Implementasi klien MCP Phi-4 menunjukkan bagaimana integrasi ini dapat dicapai dalam praktik, memberikan dasar yang solid untuk membangun aplikasi bertenaga AI yang canggih.

Poin penting:
- MCP menjembatani kesenjangan antara model bahasa dan sistem eksternal
- SLMs menawarkan efisiensi tanpa mengorbankan kemampuan saat ditingkatkan dengan alat
- Arsitektur modular memungkinkan ekstensi dan kustomisasi yang mudah
- Penanganan kesalahan dan langkah keamanan yang tepat sangat penting untuk penggunaan produksi

Tutorial ini memberikan dasar untuk membangun aplikasi MCP yang didukung SLM Anda sendiri, membuka kemungkinan untuk otomatisasi, pemrosesan data, dan integrasi sistem cerdas.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.