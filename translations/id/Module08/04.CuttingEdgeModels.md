<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T22:39:30+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "id"
}
-->
# Sesi 4: Model Terkini – LLM, SLM, dan Inferensi di Perangkat

## Gambaran Umum

Bandingkan LLM dan SLM, evaluasi trade-off inferensi lokal vs cloud, dan implementasikan demo yang menampilkan skenario EdgeAI menggunakan Phi dan ONNX Runtime. Kami juga akan menyoroti Chainlit RAG, opsi inferensi WebGPU, dan integrasi Open WebUI.

Referensi:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Panduan Open WebUI (aplikasi chat dengan Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Tujuan Pembelajaran
- Memahami trade-off LLM vs SLM terkait biaya, latensi, dan akurasi
- Memilih antara inferensi lokal dan cloud untuk kebutuhan bisnis tertentu
- Mengimplementasikan demo RAG kecil dengan Chainlit
- Mengeksplorasi WebGPU untuk akselerasi di browser
- Menghubungkan Open WebUI ke Foundry Local

## Bagian 1: LLM vs SLM – Matriks Keputusan

Pertimbangkan:
- Latensi: SLM di perangkat sering memberikan respons di bawah satu detik
- Biaya: inferensi lokal mengurangi biaya cloud
- Privasi: data sensitif tetap berada di perangkat
- Kemampuan: LLM mungkin lebih unggul dalam tugas yang kompleks
- Keandalan: strategi hibrid mengurangi risiko downtime

## Bagian 2: Lokal vs Cloud – Pola Hibrid

- Lokal terlebih dahulu dengan fallback cloud untuk prompt besar/kompleks
- Cloud terlebih dahulu dengan lokal untuk skenario yang sensitif terhadap privasi atau offline
- Rute berdasarkan jenis tugas (code-gen ke DeepSeek, chat umum ke Phi/Qwen)

## Bagian 3: Aplikasi Chat RAG dengan Chainlit (Minimal)

Instal dependensi:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Jalankan:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Perluas: tambahkan retriever sederhana (file lokal) dan tambahkan konteks yang diambil ke prompt pengguna.

## Bagian 4: Inferensi WebGPU (Heads-up)

Jalankan model kecil langsung di browser menggunakan WebGPU. Ini ideal untuk demo yang mengutamakan privasi dan pengalaman tanpa instalasi. Di bawah ini adalah contoh langkah-langkah minimal menggunakan ONNX Runtime Web dengan penyedia eksekusi WebGPU.

1) Periksa dukungan WebGPU
- Browser Chromium: chrome://gpu → pastikan “WebGPU” diaktifkan
- Pemeriksaan programatik (kami juga akan memeriksa di kode): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) Buat proyek minimal
Buat folder dan dua file: `index.html` dan `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Jalankan secara lokal (Windows cmd.exe)
Gunakan server statis sederhana agar browser dapat mengambil model.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Buka http://localhost:5173 di browser Anda. Anda akan melihat log inisialisasi, pembuatan sesi dengan WebGPU, dan prediksi argmax.

4) Pemecahan Masalah
- Jika WebGPU tidak tersedia: perbarui Chrome/Edge dan pastikan driver GPU terkini, lalu periksa chrome://flags untuk “Enable WebGPU”.
- Jika terjadi kesalahan CORS atau fetch: pastikan Anda menyajikan file melalui http:// (bukan file://) dan URL model memungkinkan permintaan lintas asal.
- Fallback ke CPU: ubah `executionProviders: ['wasm']` untuk memverifikasi perilaku dasar.

5) Langkah Selanjutnya
- Ganti dengan model ONNX khusus domain (misalnya, klasifikasi gambar atau model teks kecil).
- Tambahkan logika preprocessing/postprocessing untuk input nyata.
- Untuk model yang lebih besar atau latensi produksi, gunakan Foundry Local atau ONNX Runtime Server.

## Bagian 5: Open WebUI + Foundry Local (Langkah-langkah)

Ini menghubungkan Open WebUI ke endpoint kompatibel OpenAI milik Foundry Local untuk UI chat lokal.

1) Prasyarat
- Foundry Local terinstal dan berfungsi (`foundry --version`)
- Satu model siap dijalankan secara lokal (misalnya, `phi-4-mini`)
- Docker Desktop terinstal (disarankan untuk Open WebUI)

2) Jalankan model dengan Foundry Local
```powershell
foundry model run phi-4-mini
```
Ini membuka API kompatibel OpenAI di `http://localhost:8000`.

3) Jalankan Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Catatan:
- Di Windows, `host.docker.internal` memungkinkan container mengakses host Anda di `localhost`.
- Kami menetapkan `OPENAI_API_BASE_URL` ke endpoint Foundry Local dan `OPENAI_API_KEY` dummy.

4) Konfigurasi dari UI Open WebUI (alternatif)
- Buka http://localhost:3000
- Selesaikan pengaturan awal (pengguna admin)
- Pergi ke Pengaturan → Model/Penyedia
- Tetapkan Base URL: `http://host.docker.internal:8000/v1`
- Tetapkan API Key: `local-key` (placeholder)
- Simpan

5) Jalankan prompt uji
- Di chat Open WebUI, pilih atau masukkan nama model `phi-4-mini`
- Prompt: “Sebutkan lima manfaat inferensi AI di perangkat.”
- Anda akan melihat respons yang dialirkan dari model lokal Anda

6) Pemecahan Masalah
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Opsional: Persistensi data Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Daftar Periksa Praktis
- [ ] Bandingkan respons/latensi antara SLM dan LLM secara lokal
- [ ] Jalankan demo Chainlit dengan setidaknya dua model
- [ ] Hubungkan Open WebUI ke endpoint lokal Anda dan uji

## Langkah Selanjutnya
- Persiapkan alur kerja agen di Sesi 5
- Identifikasi skenario di mana hibrid lokal/cloud meningkatkan ROI

---

