<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:58:09+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "id"
}
-->
# Sesi 4: Membangun Aplikasi Chat Produksi dengan Chainlit

## Ikhtisar

Sesi ini berfokus pada pembuatan aplikasi chat siap produksi menggunakan Chainlit dan Microsoft Foundry Local. Anda akan belajar membuat antarmuka web modern untuk percakapan AI, menerapkan respons streaming, dan mengembangkan aplikasi chat yang tangguh dengan penanganan kesalahan yang tepat serta desain pengalaman pengguna yang baik.

**Apa yang Akan Anda Bangun:**
- **Aplikasi Chat Chainlit**: Antarmuka web modern dengan respons streaming
- **Demo WebGPU**: Inferensi berbasis browser untuk aplikasi yang mengutamakan privasi  
- **Integrasi Open WebUI**: Antarmuka chat profesional dengan Foundry Local
- **Pola Produksi**: Penanganan kesalahan, pemantauan, dan strategi penerapan

## Tujuan Pembelajaran

- Membangun aplikasi chat siap produksi dengan Chainlit
- Menerapkan respons streaming untuk meningkatkan pengalaman pengguna
- Menguasai pola integrasi SDK Foundry Local
- Menerapkan penanganan kesalahan yang tepat dan degradasi yang mulus
- Menerapkan dan mengonfigurasi aplikasi chat untuk berbagai lingkungan
- Memahami pola antarmuka web modern untuk AI percakapan

## Prasyarat

- **Foundry Local**: Terinstal dan berjalan ([Panduan Instalasi](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Versi 3.10 atau lebih baru dengan kemampuan virtual environment
- **Model**: Setidaknya satu model dimuat (`foundry model run phi-4-mini`)
- **Browser**: Browser web modern dengan dukungan WebGPU (Chrome/Edge)
- **Docker**: Untuk integrasi Open WebUI (opsional)

## Bagian 1: Memahami Aplikasi Chat Modern

### Ikhtisar Arsitektur

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Teknologi Utama

**Pola SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Manajemen layanan otomatis
- `manager.endpoint` dan `manager.api_key`: Detail koneksi
- `manager.get_model_info(alias).id`: Identifikasi model

**Framework Chainlit:**
- `@cl.on_chat_start`: Inisialisasi sesi chat
- `@cl.on_message`: Menangani pesan pengguna yang masuk  
- `cl.Message().stream_token()`: Streaming real-time
- Pembuatan UI otomatis dan manajemen WebSocket

## Bagian 2: Matriks Keputusan Lokal vs Cloud

### Karakteristik Performa

| Aspek | Lokal (Foundry) | Cloud (Azure OpenAI) |
|-------|-----------------|---------------------|
| **Latensi** | 🚀 50-200ms (tanpa jaringan) | ⏱️ 200-2000ms (tergantung jaringan) |
| **Privasi** | 🔒 Data tidak pernah meninggalkan perangkat | ⚠️ Data dikirim ke cloud |
| **Biaya** | 💰 Gratis setelah perangkat keras | 💸 Bayar per token |
| **Offline** | ✅ Berfungsi tanpa internet | ❌ Membutuhkan internet |
| **Ukuran Model** | ⚠️ Terbatas oleh perangkat keras | ✅ Akses ke model terbesar |
| **Skalabilitas** | ⚠️ Bergantung pada perangkat keras | ✅ Skalabilitas tanpa batas |

### Pola Strategi Hybrid

**Lokal-Pertama dengan Cadangan:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Routing Berdasarkan Tugas:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Bagian 3: Contoh 04 - Aplikasi Chat Chainlit

### Memulai dengan Cepat

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Aplikasi secara otomatis terbuka di `http://localhost:8080` dengan antarmuka chat modern.

### Implementasi Inti

Aplikasi Contoh 04 menunjukkan pola siap produksi:

**Penemuan Layanan Otomatis:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Handler Chat Streaming:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Opsi Konfigurasi

**Variabel Lingkungan:**

| Variabel | Deskripsi | Default | Contoh |
|----------|-----------|---------|--------|
| `MODEL` | Alias model yang digunakan | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint Foundry Local | Terdeteksi otomatis | `http://localhost:51211` |
| `API_KEY` | API key (opsional untuk lokal) | `""` | `your-api-key` |

**Penggunaan Lanjutan:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Bagian 4: Membuat dan Menggunakan Jupyter Notebook

### Ikhtisar Dukungan Notebook

Contoh 04 mencakup notebook Jupyter yang komprehensif (`chainlit_app.ipynb`) yang menyediakan:

- **📚 Konten Edukasi**: Materi pembelajaran langkah demi langkah
- **🔬 Eksplorasi Interaktif**: Menjalankan dan bereksperimen dengan sel kode
- **📊 Demonstrasi Visual**: Grafik, diagram, dan visualisasi output
- **🛠️ Alat Pengembangan**: Pengujian dan debugging

### Membuat Notebook Anda Sendiri

#### Langkah 1: Menyiapkan Lingkungan Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Langkah 2: Membuat Notebook Baru

**Menggunakan VS Code:**
1. Buka VS Code di direktori Module08
2. Buat file baru dengan ekstensi `.ipynb`
3. Pilih kernel "Foundry Local" saat diminta
4. Mulai tambahkan sel dengan konten Anda

**Menggunakan Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Praktik Terbaik Struktur Notebook

#### Organisasi Sel

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Contoh Interaktif dan Latihan

#### Latihan 1: Pengujian Konfigurasi Klien

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Latihan 2: Simulasi Respons Streaming

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Bagian 5: Demo Inferensi Browser WebGPU

### Ikhtisar

WebGPU memungkinkan menjalankan model AI langsung di browser untuk privasi maksimal dan pengalaman tanpa instalasi. Contoh ini menunjukkan eksekusi ONNX Runtime Web dengan WebGPU.

### Langkah 1: Periksa Dukungan WebGPU

**Persyaratan Browser:**
- Chrome/Edge 113+ dengan WebGPU diaktifkan
- Periksa: `chrome://gpu` → konfirmasi status "WebGPU"
- Pemeriksaan programatik: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Langkah 2: Membuat Demo WebGPU

Buat direktori: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Langkah 3: Menjalankan Demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Bagian 6: Integrasi Open WebUI

### Ikhtisar

Open WebUI menyediakan antarmuka profesional mirip ChatGPT yang terhubung ke API kompatibel OpenAI Foundry Local.

### Langkah 1: Prasyarat

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Langkah 2: Pengaturan Docker (Direkomendasikan)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Catatan:** `host.docker.internal` memungkinkan kontainer Docker mengakses mesin host di Windows.

### Langkah 3: Konfigurasi

1. **Buka Browser:** Navigasikan ke `http://localhost:3000`
2. **Pengaturan Awal:** Buat akun admin
3. **Konfigurasi Model:**
   - Pengaturan → Model → API OpenAI  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (nilai apa pun dapat digunakan)
4. **Uji Koneksi:** Model harus muncul di dropdown

### Pemecahan Masalah

**Masalah Umum:**

1. **Koneksi Ditolak:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Model Tidak Muncul:**
   - Verifikasi model dimuat: `foundry model list`
   - Periksa respons API: `curl http://localhost:51211/v1/models`
   - Restart kontainer Open WebUI

## Bagian 7: Pertimbangan Penerapan Produksi

### Konfigurasi Lingkungan

**Pengaturan Pengembangan:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Penerapan Produksi:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Masalah Port Umum dan Solusi

**Pencegahan Konflik Port 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Pemantauan Performa

**Implementasi Pemeriksaan Kesehatan:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Ringkasan

Sesi 4 membahas pembuatan aplikasi Chainlit siap produksi untuk AI percakapan. Anda telah mempelajari tentang:

- ✅ **Framework Chainlit**: UI modern dan dukungan streaming untuk aplikasi chat
- ✅ **Integrasi Foundry Local**: Penggunaan SDK dan pola konfigurasi  
- ✅ **Inferensi WebGPU**: AI berbasis browser untuk privasi maksimal
- ✅ **Pengaturan Open WebUI**: Penerapan antarmuka chat profesional
- ✅ **Pola Produksi**: Penanganan kesalahan, pemantauan, dan skalabilitas

Aplikasi Contoh 04 menunjukkan praktik terbaik untuk membangun antarmuka chat yang tangguh yang memanfaatkan model AI lokal melalui Microsoft Foundry Local sambil memberikan pengalaman pengguna yang luar biasa.

## Referensi

- **[Contoh 04: Aplikasi Chainlit](samples/04/README.md)**: Aplikasi lengkap dengan dokumentasi
- **[Notebook Edukasi Chainlit](samples/04/chainlit_app.ipynb)**: Materi pembelajaran interaktif
- **[Dokumentasi Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Dokumentasi platform lengkap
- **[Dokumentasi Chainlit](https://docs.chainlit.io/)**: Dokumentasi resmi framework
- **[Panduan Integrasi Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Tutorial resmi

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau interpretasi yang keliru yang timbul dari penggunaan terjemahan ini.