<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T22:38:50+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "id"
}
-->
# Sesi 6: Foundry Local â€“ Model sebagai Alat

## Ikhtisar

Perlakukan model AI sebagai alat modular yang dapat disesuaikan dan dijalankan langsung di perangkat dengan Foundry Local. Sesi ini menekankan alur kerja praktis untuk inferensi yang menjaga privasi dan berlatensi rendah, serta cara mengintegrasikan alat ini melalui SDK, API, atau CLI. Anda juga akan belajar cara melakukan skala ke Azure AI Foundry jika diperlukan.

Referensi:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integrasi dengan inference SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Kompilasi model Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Tujuan Pembelajaran
- Merancang pola model sebagai alat di perangkat
- Mengintegrasikan melalui REST API yang kompatibel dengan OpenAI atau SDK
- Menyesuaikan model untuk kasus penggunaan spesifik domain
- Merencanakan skala hibrid ke Azure AI Foundry

## Bagian 1: Abstraksi Alat (Langkah-langkah)

Tujuan: Mewakili model sebagai alat dengan kontrak yang jelas dan router sederhana.

Langkah 1) Definisikan antarmuka alat dan registri
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```

Langkah 2) Implementasikan dua alat yang didukung oleh Foundry Local
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```

Langkah 3) Router berdasarkan tugas
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```

## Bagian 2: Integrasi SDK dan API (Langkah-langkah)

Tujuan: Menggunakan OpenAI Python SDK terhadap endpoint Foundry Local.

Langkah 1) Instal
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```

Langkah 2) Konfigurasi variabel lingkungan
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```

Langkah 3) Panggil API chat
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```

## Bagian 3: Penyesuaian Domain (Langkah-langkah)

Tujuan: Menyesuaikan output untuk domain menggunakan template prompt dan skema JSON.

Langkah 1) Buat template prompt domain
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```

Langkah 2) Terapkan output JSON
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```

## Bagian 4: Mode Offline dan Keamanan (Langkah-langkah)

Tujuan: Memastikan privasi dan ketahanan saat menjalankan model sebagai alat secara lokal.

Langkah 1) Panaskan dan validasi endpoint lokal
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```

Langkah 2) Sanitasi input
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

Langkah 3) Aktifkan flag lokal saja dan logging
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```

## Bagian 5: Skalabilitas ke Azure AI Foundry (Langkah-langkah)

Tujuan: Mencerminkan model lokal dengan endpoint Azure untuk kapasitas tambahan.

Langkah 1) Tentukan strategi routing
- Prioritaskan lokal untuk privasi/latensi, fallback ke Azure jika terjadi error atau prompt besar

Langkah 2) Implementasikan stub router sederhana
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```

## Daftar Tugas Praktis
- [ ] Registrasi setidaknya dua alat dan rute permintaan
- [ ] Panggil Foundry Local melalui OpenAI SDK dan REST mentah
- [ ] Terapkan output JSON untuk template domain
- [ ] Sanitasi dan log panggilan secara lokal
- [ ] Implementasikan router hibrid sederhana dengan fallback ke Azure

## Penutup

Foundry Local memungkinkan AI yang tangguh di perangkat, di mana model menjadi alat yang dapat dikomposisi. Dengan antarmuka yang jelas, tata kelola, dan skala hibrid, tim dapat menghadirkan aplikasi AI real-time yang aman, menghormati privasi pengguna, dan tetap siap untuk kebutuhan perusahaan.

---

