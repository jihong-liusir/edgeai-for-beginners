<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:57:45+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "id"
}
-->
# Sesi 3: Penemuan dan Pengelolaan Model Open-Source

## Ikhtisar

Sesi ini berfokus pada penemuan dan pengelolaan model secara praktis menggunakan Foundry Local. Anda akan belajar cara mencantumkan model yang tersedia, menguji berbagai opsi, dan memahami karakteristik kinerja dasar. Pendekatan ini menekankan eksplorasi langsung dengan CLI Foundry untuk membantu Anda memilih model yang tepat sesuai kebutuhan Anda.

## Tujuan Pembelajaran

- Menguasai perintah CLI Foundry untuk penemuan dan pengelolaan model
- Memahami pola cache model dan penyimpanan lokal
- Belajar menguji dan membandingkan model dengan cepat
- Membangun alur kerja praktis untuk pemilihan dan pengujian model
- Mengeksplorasi ekosistem model yang terus berkembang melalui Foundry Local

## Prasyarat

- Menyelesaikan Sesi 1: Memulai dengan Foundry Local
- CLI Foundry Local terinstal dan dapat diakses
- Ruang penyimpanan yang cukup untuk mengunduh model (ukuran model berkisar antara 1GB hingga 20GB+)
- Pemahaman dasar tentang jenis model dan kasus penggunaan

## Ikhtisar

Sesi ini mengeksplorasi cara membawa model open-source ke Foundry Local.

## Bagian 6: Latihan Langsung

### Latihan: Penemuan dan Perbandingan Model

Buat skrip evaluasi model Anda sendiri berdasarkan Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### Tugas Anda

1. **Jalankan skrip Sample 03**: `samples\03\list_and_bench.cmd`
2. **Coba model yang berbeda**: Uji setidaknya 3 model yang berbeda
3. **Bandingkan kinerja**: Catat perbedaan dalam kecepatan dan kualitas respons
4. **Dokumentasikan temuan**: Buat diagram perbandingan sederhana

### Format Perbandingan Contoh

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## Bagian 7: Pemecahan Masalah dan Praktik Terbaik

### Masalah Umum dan Solusi

**Model Tidak Mau Berjalan:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**Memori Tidak Cukup:**
- Mulai dengan model yang lebih kecil (`phi-4-mini`)
- Tutup aplikasi lain
- Tingkatkan RAM jika sering mengalami batasan

**Kinerja Lambat:**
- Pastikan model telah sepenuhnya dimuat (periksa output verbose)
- Tutup aplikasi latar belakang yang tidak diperlukan
- Pertimbangkan penyimpanan yang lebih cepat (SSD)

### Praktik Terbaik

1. **Mulai dari yang Kecil**: Mulailah dengan `phi-4-mini` untuk memvalidasi pengaturan
2. **Satu Model pada Satu Waktu**: Hentikan model sebelumnya sebelum memulai yang baru
3. **Pantau Sumber Daya**: Perhatikan penggunaan memori
4. **Uji Secara Konsisten**: Gunakan prompt yang sama untuk perbandingan yang adil
5. **Dokumentasikan Hasil**: Catat kinerja model untuk kasus penggunaan Anda

## Bagian 8: Langkah Selanjutnya dan Referensi

### Persiapan untuk Sesi 4

- **Fokus Sesi 4**: Alat dan teknik optimasi
- **Prasyarat**: Nyaman dengan pergantian model dan pengujian kinerja dasar
- **Rekomendasi**: Identifikasi 2-3 model favorit dari sesi ini

### Sumber Daya Tambahan

- **[Dokumentasi Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Dokumentasi resmi
- **[Referensi CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Referensi perintah lengkap
- **[Model Mondays](https://aka.ms/model-mondays)**: Sorotan model mingguan
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Komunitas dan masalah
- **[Sample 03: Penemuan Model](samples/03/README.md)**: Skrip contoh langsung

### Poin Penting

✅ **Penemuan Model**: Gunakan `foundry model list` untuk mengeksplorasi model yang tersedia  
✅ **Pengujian Cepat**: Pola `list_and_bench.cmd` untuk evaluasi cepat  
✅ **Pemantauan Kinerja**: Pengukuran penggunaan sumber daya dan waktu respons dasar  
✅ **Pemilihan Model**: Panduan praktis untuk memilih model berdasarkan kasus penggunaan  
✅ **Manajemen Cache**: Memahami prosedur penyimpanan dan pembersihan  

Anda sekarang memiliki keterampilan praktis untuk menemukan, menguji, dan memilih model yang sesuai untuk aplikasi AI Anda menggunakan pendekatan CLI Foundry Local yang sederhana.

## Tujuan Pembelajaran

- Menemukan dan mengevaluasi model open-source untuk inferensi lokal
- Mengompilasi dan menjalankan model Hugging Face tertentu dalam Foundry Local
- Menerapkan strategi pemilihan model untuk akurasi, latensi, dan kebutuhan sumber daya
- Mengelola model secara lokal dengan cache dan versi

## Bagian 1: Penemuan Model dengan Foundry CLI

### Perintah Pengelolaan Model Dasar

CLI Foundry menyediakan perintah sederhana untuk penemuan dan pengelolaan model:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### Menjalankan Model Pertama Anda

Mulailah dengan model populer yang telah teruji untuk memahami karakteristik kinerja:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**Catatan:** Flag `--verbose` memberikan informasi startup yang rinci, termasuk:
- Progres unduhan model (pada saat pertama kali dijalankan)
- Detail alokasi memori
- Informasi pengikatan layanan
- Metrik inisialisasi kinerja

### Memahami Kategori Model

**Small Language Models (SLMs):**
- `phi-4-mini`: Cepat, efisien, cocok untuk obrolan umum
- `phi-4`: Versi yang lebih mampu dengan penalaran yang lebih baik

**Model Medium:**
- `qwen2.5-7b`: Penalaran yang sangat baik dan konteks yang lebih panjang
- `deepseek-r1-7b`: Dioptimalkan untuk pembuatan kode

**Model Besar:**
- `llama-3.2`: Model open-source terbaru dari Meta
- `qwen2.5-14b`: Penalaran tingkat perusahaan

## Bagian 2: Pengujian dan Perbandingan Model Cepat

### Pendekatan Sample 03: Daftar dan Uji Sederhana

Berdasarkan pola Sample 03, berikut adalah alur kerja minimal:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### Pengujian Kinerja Model

Setelah model berjalan, uji dengan prompt yang konsisten:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### Alternatif Pengujian PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## Bagian 3: Manajemen Cache dan Penyimpanan Model

### Memahami Cache Model

Foundry Local secara otomatis mengelola unduhan dan cache model:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### Pertimbangan Penyimpanan Model

**Ukuran Model Tipikal:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Praktik Penyimpanan Terbaik:**
- Simpan 2-3 model dalam cache untuk pergantian cepat
- Hapus model yang tidak digunakan untuk mengosongkan ruang: `foundry cache clean`
- Pantau penggunaan disk, terutama pada SSD yang lebih kecil
- Pertimbangkan trade-off antara ukuran model dan kemampuan

### Pemantauan Kinerja Model

Saat model berjalan, pantau sumber daya sistem:

**Windows Task Manager:**
- Perhatikan penggunaan memori (model tetap dimuat di RAM)
- Pantau penggunaan CPU selama inferensi
- Periksa I/O disk selama pemuatan model awal

**Pemantauan Command Line:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## Bagian 4: Panduan Pemilihan Model Praktis

### Memilih Model Berdasarkan Kasus Penggunaan

**Untuk Obrolan Umum dan Tanya Jawab:**
- Mulai dengan: `phi-4-mini` (cepat, efisien)
- Tingkatkan ke: `phi-4` (penalaran lebih baik)
- Lanjutan: `qwen2.5-7b` (konteks lebih panjang)

**Untuk Pembuatan Kode:**
- Direkomendasikan: `deepseek-r1-7b`
- Alternatif: `qwen2.5-7b` (juga bagus untuk kode)

**Untuk Penalaran Kompleks:**
- Terbaik: `qwen2.5-7b` atau `qwen2.5-14b`
- Opsi hemat: `phi-4`

### Panduan Persyaratan Perangkat Keras

**Persyaratan Sistem Minimum:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**Direkomendasikan untuk Kinerja Terbaik:**
- RAM 32GB+ untuk pergantian multi-model yang nyaman
- Penyimpanan SSD untuk pemuatan model yang lebih cepat
- CPU modern dengan kinerja single-thread yang baik
- Dukungan NPU (PC Windows 11 Copilot+) untuk akselerasi

### Alur Kerja Pergantian Model

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## Bagian 5: Pengujian Model Sederhana

### Pengujian Kinerja Dasar

Berikut adalah pendekatan sederhana untuk membandingkan kinerja model:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### Penilaian Kualitas Manual

Untuk setiap model, uji dengan prompt yang konsisten dan evaluasi secara manual:

**Prompt Uji:**
1. "Jelaskan komputasi kuantum dengan istilah sederhana."
2. "Tulis fungsi Python untuk mengurutkan daftar."
3. "Apa kelebihan dan kekurangan kerja jarak jauh?"
4. "Ringkas manfaat AI edge."

**Kriteria Evaluasi:**
- **Akurasi**: Apakah informasi benar?
- **Kejelasan**: Apakah penjelasan mudah dipahami?
- **Kelengkapan**: Apakah menjawab pertanyaan secara penuh?
- **Kecepatan**: Seberapa cepat responsnya?

### Pemantauan Penggunaan Sumber Daya

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## Bagian 6: Langkah Selanjutnya

- Berlangganan Model Mondays untuk model dan tips baru: https://aka.ms/model-mondays
- Kontribusikan temuan ke `models.json` tim Anda
- Persiapkan untuk Sesi 4: membandingkan LLM vs SLM, inferensi lokal vs cloud, dan demo langsung

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diketahui bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau interpretasi yang keliru yang timbul dari penggunaan terjemahan ini.