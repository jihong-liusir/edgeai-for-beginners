<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T22:37:21+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "id"
}
-->
# Sesi 3: Model Open-Source dengan Foundry Local

## Ikhtisar

Sesi ini membahas cara membawa model open-source ke Foundry Local: memilih model dari komunitas, mengintegrasikan konten Hugging Face, dan menerapkan strategi "bawa model sendiri" (BYOM). Anda juga akan mengenal seri Model Mondays untuk pembelajaran berkelanjutan dan penemuan model.

Referensi:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilasi model Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Tujuan Pembelajaran
- Menemukan dan mengevaluasi model open-source untuk inferensi lokal
- Mengompilasi dan menjalankan model Hugging Face tertentu di Foundry Local
- Menerapkan strategi pemilihan model berdasarkan akurasi, latensi, dan kebutuhan sumber daya
- Mengelola model secara lokal dengan cache dan versi

## Bagian 1: Penemuan dan Pemilihan Model (Langkah-langkah)

Langkah 1) Daftar model yang tersedia di katalog lokal  
```cmd
foundry model list
```
  
Langkah 2) Coba cepat dua kandidat (unduhan otomatis saat pertama kali dijalankan)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Langkah 3) Catat metrik dasar  
- Perhatikan latensi (subjektif) dan kualitas untuk prompt tetap  
- Pantau penggunaan memori melalui Task Manager saat setiap model dijalankan  

## Bagian 2: Menjalankan Model Katalog melalui CLI (Langkah-langkah)

Langkah 1) Mulai model  
```cmd
foundry model run llama-3.2
```
  
Langkah 2) Kirim prompt uji melalui endpoint yang kompatibel dengan OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Bagian 3: BYOM – Kompilasi Model Hugging Face (Langkah-langkah)

Ikuti panduan resmi untuk mengompilasi model. Alur tingkat tinggi di bawah ini—lihat artikel Microsoft Learn untuk perintah dan konfigurasi yang didukung secara spesifik.

Langkah 1) Siapkan direktori kerja  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Langkah 2) Kompilasi model HF yang didukung  
- Gunakan langkah-langkah dari dokumen Learn untuk mengonversi dan menempatkan model ONNX yang telah dikompilasi di direktori `models` Anda  
- Konfirmasi dengan:  
```cmd
foundry cache ls
```
  
Anda seharusnya melihat nama model yang telah dikompilasi (misalnya, `llama-3.2`).  

Langkah 3) Jalankan model yang telah dikompilasi  
```cmd
foundry model run llama-3.2 --verbose
```
  
Catatan:  
- Pastikan disk dan RAM cukup untuk proses kompilasi dan menjalankan model  
- Mulailah dengan model yang lebih kecil untuk memvalidasi alur, lalu tingkatkan skala  

## Bagian 4: Kurasi Model Praktis (Langkah-langkah)

Langkah 1) Buat registri `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Langkah 2) Skrip pemilih kecil  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Bagian 5: Benchmark Praktis (Langkah-langkah)

Langkah 1) Benchmark latensi sederhana  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Langkah 2) Pemeriksaan kualitas  
- Gunakan set prompt tetap, tangkap output ke CSV/JSON  
- Nilai secara manual kelancaran, relevansi, dan ketepatan (1–5)  

## Bagian 6: Langkah Selanjutnya
- Berlangganan Model Mondays untuk model dan tips baru: https://aka.ms/model-mondays  
- Kontribusikan temuan ke `models.json` tim Anda  
- Persiapkan untuk Sesi 4: membandingkan LLM vs SLM, inferensi lokal vs cloud, dan demo langsung  

---

