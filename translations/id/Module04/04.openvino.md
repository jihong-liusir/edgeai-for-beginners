<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T14:34:16+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "id"
}
-->
# Bagian 4: OpenVINO Toolkit Optimization Suite

## Daftar Isi
1. [Pendahuluan](../../../Module04)
2. [Apa itu OpenVINO?](../../../Module04)
3. [Instalasi](../../../Module04)
4. [Panduan Memulai Cepat](../../../Module04)
5. [Contoh: Konversi dan Optimasi Model dengan OpenVINO](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Praktik Terbaik](../../../Module04)
8. [Pemecahan Masalah](../../../Module04)
9. [Sumber Daya Tambahan](../../../Module04)

## Pendahuluan

OpenVINO (Open Visual Inference and Neural Network Optimization) adalah toolkit open-source dari Intel untuk menerapkan solusi AI yang berkinerja tinggi di lingkungan cloud, on-premises, dan edge. Baik Anda menargetkan CPU, GPU, VPU, atau akselerator AI khusus, OpenVINO menyediakan kemampuan optimasi yang komprehensif sambil mempertahankan akurasi model dan memungkinkan penerapan lintas platform.

## Apa itu OpenVINO?

OpenVINO adalah toolkit open-source yang memungkinkan pengembang untuk mengoptimalkan, mengonversi, dan menerapkan model AI secara efisien di berbagai platform perangkat keras. Toolkit ini terdiri dari tiga komponen utama: OpenVINO Runtime untuk inferensi, Neural Network Compression Framework (NNCF) untuk optimasi model, dan OpenVINO Model Server untuk penerapan yang skalabel.

### Fitur Utama

- **Penerapan Lintas Platform**: Mendukung Linux, Windows, dan macOS dengan API Python, C++, dan C
- **Akselerasi Perangkat Keras**: Penemuan perangkat otomatis dan optimasi untuk CPU, GPU, VPU, dan akselerator AI
- **Kerangka Kompresi Model**: Teknik kuantisasi, pruning, dan optimasi canggih melalui NNCF
- **Kompatibilitas Framework**: Dukungan langsung untuk model TensorFlow, ONNX, PaddlePaddle, dan PyTorch
- **Dukungan AI Generatif**: OpenVINO GenAI khusus untuk menerapkan model bahasa besar dan aplikasi AI generatif

### Manfaat

- **Optimasi Kinerja**: Peningkatan kecepatan yang signifikan dengan kehilangan akurasi minimal
- **Jejak Penerapan yang Lebih Kecil**: Ketergantungan eksternal minimal menyederhanakan instalasi dan penerapan
- **Waktu Startup yang Lebih Cepat**: Pemuatan dan caching model yang dioptimalkan untuk inisialisasi aplikasi yang lebih cepat
- **Penerapan yang Skalabel**: Dari perangkat edge hingga infrastruktur cloud dengan API yang konsisten
- **Siap Produksi**: Keandalan tingkat enterprise dengan dokumentasi yang komprehensif dan dukungan komunitas

## Instalasi

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengelola paket pip
- Lingkungan virtual (disarankan)
- Perangkat keras yang kompatibel (CPU Intel disarankan, tetapi mendukung berbagai arsitektur)

### Instalasi Dasar

Buat dan aktifkan lingkungan virtual:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Instal OpenVINO Runtime:

```bash
pip install openvino
```

Instal NNCF untuk optimasi model:

```bash
pip install nncf
```

### Instalasi OpenVINO GenAI

Untuk aplikasi AI generatif:

```bash
pip install openvino-genai
```

### Ketergantungan Opsional

Paket tambahan untuk kasus penggunaan tertentu:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verifikasi Instalasi

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Jika berhasil, Anda akan melihat informasi versi OpenVINO.

## Panduan Memulai Cepat

### Optimasi Model Pertama Anda

Mari kita konversi dan optimasi model Hugging Face menggunakan OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Apa yang Dilakukan Proses Ini

Alur kerja optimasi melibatkan: memuat model asli dari Hugging Face, mengonversi ke format Intermediate Representation (IR) OpenVINO, menerapkan optimasi default, dan mengompilasi untuk perangkat keras target.

### Penjelasan Parameter Utama

- `export=True`: Mengonversi model ke format IR OpenVINO
- `compile=False`: Menunda kompilasi hingga runtime untuk fleksibilitas
- `device`: Perangkat keras target ("CPU", "GPU", "AUTO" untuk pemilihan otomatis)
- `save_pretrained()`: Menyimpan model yang dioptimalkan untuk digunakan kembali

## Contoh: Konversi dan Optimasi Model dengan OpenVINO

### Langkah 1: Konversi Model dengan Kuantisasi NNCF

Berikut cara menerapkan kuantisasi pasca-pelatihan menggunakan NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Langkah 2: Optimasi Lanjutan dengan Kompresi Bobot

Untuk model berbasis transformer, terapkan kompresi bobot:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Langkah 3: Inferensi dengan Model yang Dioptimalkan

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struktur Output

Setelah optimasi, direktori model Anda akan berisi:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Penggunaan Lanjutan

### Konfigurasi dengan NNCF YAML

Untuk alur kerja optimasi yang kompleks, gunakan file konfigurasi NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Terapkan konfigurasi:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimasi GPU

Untuk akselerasi GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimasi Pemrosesan Batch

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Penerapan Model Server

Terapkan model yang dioptimalkan dengan OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Kode klien untuk model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Praktik Terbaik

### 1. Pemilihan dan Persiapan Model
- Gunakan model dari framework yang didukung (PyTorch, TensorFlow, ONNX)
- Pastikan input model memiliki bentuk tetap atau bentuk dinamis yang diketahui
- Uji dengan dataset representatif untuk kalibrasi

### 2. Pemilihan Strategi Optimasi
- **Kuantisasi Pasca-Pelatihan**: Mulai dari sini untuk optimasi cepat
- **Kompresi Bobot**: Ideal untuk model bahasa besar dan transformer
- **Pelatihan Sadar Kuantisasi**: Gunakan saat akurasi sangat penting

### 3. Optimasi Spesifik Perangkat Keras
- **CPU**: Gunakan kuantisasi INT8 untuk kinerja seimbang
- **GPU**: Manfaatkan presisi FP16 dan pemrosesan batch
- **VPU**: Fokus pada penyederhanaan model dan fusi lapisan

### 4. Penyempurnaan Kinerja
- **Mode Throughput**: Untuk pemrosesan batch volume tinggi
- **Mode Latensi**: Untuk aplikasi interaktif real-time
- **Perangkat AUTO**: Biarkan OpenVINO memilih perangkat keras yang optimal

### 5. Manajemen Memori
- Gunakan bentuk dinamis dengan bijak untuk menghindari overhead memori
- Terapkan caching model untuk pemuatan yang lebih cepat
- Pantau penggunaan memori selama optimasi

### 6. Validasi Akurasi
- Selalu validasi model yang dioptimalkan terhadap kinerja asli
- Gunakan dataset uji representatif untuk evaluasi
- Pertimbangkan optimasi bertahap (mulai dengan pengaturan konservatif)

## Pemecahan Masalah

### Masalah Umum

#### 1. Masalah Instalasi
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Kesalahan Konversi Model
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Masalah Kinerja
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Masalah Memori
- Kurangi ukuran batch model selama optimasi
- Gunakan streaming untuk dataset besar
- Aktifkan caching model: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Penurunan Akurasi
- Gunakan presisi lebih tinggi (INT8 daripada INT4)
- Tingkatkan ukuran dataset kalibrasi
- Terapkan optimasi presisi campuran

### Pemantauan Kinerja

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Mendapatkan Bantuan

- **Dokumentasi**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum Komunitas**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Sumber Daya Tambahan

### Tautan Resmi
- **Halaman Utama OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Repositori GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Repositori NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Sumber Belajar
- **Notebook OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Panduan Memulai Cepat**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Panduan Optimasi**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Alat Integrasi
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmark Kinerja
- **Benchmark Resmi**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Contoh Komunitas
- **Notebook Jupyter**: [Repositori Notebook OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Tutorial lengkap tersedia di repositori notebook OpenVINO
- **Aplikasi Contoh**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Contoh dunia nyata untuk berbagai domain (visi komputer, NLP, audio)
- **Posting Blog**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Posting blog Intel AI dan komunitas dengan kasus penggunaan yang mendetail

### Alat Terkait
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Teknik optimasi tambahan untuk perangkat keras Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Untuk perbandingan penerapan di perangkat mobile dan edge
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternatif mesin inferensi lintas platform

## ➡️ Langkah Selanjutnya

- [05: Penjelasan Mendalam Apple MLX Framework](./05.AppleMLX.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.