<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T14:52:09+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "id"
}
-->
# Bagian 4: Penerapan - Implementasi Model Siap Produksi

## Ikhtisar

Tutorial lengkap ini akan membimbing Anda melalui proses penerapan model terkuantisasi yang telah disesuaikan menggunakan Foundry Local. Kita akan membahas konversi model, optimasi kuantisasi, dan konfigurasi penerapan dari awal hingga akhir.

## Prasyarat

Sebelum memulai, pastikan Anda memiliki hal-hal berikut:

- ‚úÖ Model onnx yang telah disesuaikan dan siap untuk diterapkan
- ‚úÖ Komputer Windows atau Mac
- ‚úÖ Python 3.10 atau lebih tinggi
- ‚úÖ RAM tersedia minimal 8GB
- ‚úÖ Foundry Local terinstal di sistem Anda

## Bagian 1: Pengaturan Lingkungan

### Menginstal Alat yang Dibutuhkan

Buka terminal Anda (Command Prompt di Windows, Terminal di Mac) dan jalankan perintah berikut secara berurutan:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Catatan Penting**: Anda juga memerlukan CMake versi 3.31 atau lebih baru, yang dapat diunduh dari [cmake.org](https://cmake.org/download/).

## Bagian 2: Konversi dan Kuantisasi Model

### Memilih Format yang Tepat

Untuk model bahasa kecil yang telah disesuaikan, kami merekomendasikan menggunakan **format ONNX** karena menawarkan:

- üöÄ Optimasi kinerja yang lebih baik
- üîß Penerapan yang tidak bergantung pada perangkat keras
- üè≠ Kemampuan siap produksi
- üì± Kompatibilitas lintas platform

### Metode 1: Konversi Satu Perintah (Direkomendasikan)

Gunakan perintah berikut untuk langsung mengonversi model yang telah disesuaikan:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Penjelasan Parameter:**
- `--model_name_or_path`: Jalur ke model yang telah disesuaikan
- `--device cpu`: Gunakan CPU untuk optimasi
- `--precision int4`: Gunakan kuantisasi INT4 (pengurangan ukuran sekitar 75%)
- `--output_path`: Jalur keluaran untuk model yang telah dikonversi

### Metode 2: Pendekatan File Konfigurasi (Pengguna Lanjutan)

Buat file konfigurasi bernama `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Kemudian jalankan:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Perbandingan Opsi Kuantisasi

| Presisi | Ukuran File | Kecepatan Inferensi | Kualitas Model | Penggunaan yang Direkomendasikan |
|---------|-------------|---------------------|----------------|----------------------------------|
| FP16    | Baseline √ó 0.5 | Cepat | Terbaik | Perangkat keras kelas atas |
| INT8    | Baseline √ó 0.25 | Sangat Cepat | Baik | Pilihan seimbang |
| INT4    | Baseline √ó 0.125 | Tercepat | Dapat diterima | Sumber daya terbatas |

üí° **Rekomendasi**: Mulailah dengan kuantisasi INT4 untuk penerapan pertama Anda. Jika kualitas tidak memuaskan, coba INT8 atau FP16.

## Bagian 3: Konfigurasi Penerapan Foundry Local

### Membuat Konfigurasi Model

Navigasikan ke direktori model Foundry Local:

```bash
foundry cache cd ./models/
```

Buat struktur direktori model Anda:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Buat file konfigurasi `inference_model.json` di direktori model Anda:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Template Konfigurasi Khusus Model

#### Untuk Model Seri Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Bagian 4: Pengujian dan Optimasi Model

### Memverifikasi Instalasi Model

Periksa apakah Foundry Local dapat mengenali model Anda:

```bash
foundry cache ls
```

Anda seharusnya melihat `your-finetuned-model-int4` dalam daftar.

### Memulai Pengujian Model

```bash
foundry model run your-finetuned-model-int4
```

### Pengukuran Kinerja

Pantau metrik utama selama pengujian:

1. **Waktu Respons**: Ukur waktu rata-rata per respons
2. **Penggunaan Memori**: Pantau konsumsi RAM
3. **Pemanfaatan CPU**: Periksa beban prosesor
4. **Kualitas Output**: Evaluasi relevansi dan koherensi respons

### Daftar Periksa Validasi Kualitas

- ‚úÖ Model merespons dengan tepat untuk kueri domain yang telah disesuaikan
- ‚úÖ Format respons sesuai dengan struktur output yang diharapkan
- ‚úÖ Tidak ada kebocoran memori selama penggunaan yang lama
- ‚úÖ Kinerja konsisten di berbagai panjang input
- ‚úÖ Penanganan yang tepat untuk kasus tepi dan input yang tidak valid

## Ringkasan

Selamat! Anda telah berhasil menyelesaikan:

- ‚úÖ Konversi format model yang telah disesuaikan
- ‚úÖ Optimasi kuantisasi model
- ‚úÖ Konfigurasi penerapan Foundry Local
- ‚úÖ Penyetelan kinerja dan pemecahan masalah

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diingat bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.