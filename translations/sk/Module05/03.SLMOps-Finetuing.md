<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T19:01:00+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "sk"
}
-->
# Sekcia 3: Jemné doladenie - Prispôsobenie modelov na špecifické úlohy

## Obsah
1. [Úvod do jemného doladenia](../../../Module05)
2. [Prečo je jemné doladenie dôležité](../../../Module05)
3. [Typy jemného doladenia](../../../Module05)
4. [Jemné doladenie s Microsoft Olive](../../../Module05)
5. [Praktické príklady](../../../Module05)
6. [Najlepšie postupy a odporúčania](../../../Module05)
7. [Pokročilé techniky](../../../Module05)
8. [Hodnotenie a monitorovanie](../../../Module05)
9. [Bežné výzvy a riešenia](../../../Module05)
10. [Záver](../../../Module05)

## Úvod do jemného doladenia

**Jemné doladenie** je výkonná technika strojového učenia, ktorá zahŕňa prispôsobenie predtrénovaného modelu na vykonávanie špecifických úloh alebo prácu so špecializovanými dátovými súbormi. Namiesto trénovania modelu od začiatku jemné doladenie využíva už naučené znalosti predtrénovaného modelu a upravuje ich pre váš konkrétny prípad použitia.

### Čo je jemné doladenie?

Jemné doladenie je forma **transferového učenia**, pri ktorej:
- Začínate s predtrénovaným modelom, ktorý sa naučil všeobecné vzory z veľkých dátových súborov
- Upravujete vnútorné parametre modelu pomocou vášho špecifického dátového súboru
- Zachovávate cenné znalosti a zároveň špecializujete model na vašu úlohu

Predstavte si to ako učenie skúseného šéfkuchára variť novú kuchyňu - už rozumie základom varenia, ale potrebuje sa naučiť špecifické techniky a chute pre nový štýl.

### Hlavné výhody

- **Úspora času**: Výrazne rýchlejšie ako trénovanie od začiatku
- **Efektívnosť dát**: Vyžaduje menšie dátové súbory na dosiahnutie dobrého výkonu
- **Nákladová efektívnosť**: Nižšie požiadavky na výpočtové zdroje
- **Lepší výkon**: Často dosahuje lepšie výsledky v porovnaní s trénovaním od začiatku
- **Optimalizácia zdrojov**: Sprístupňuje výkonnú AI menším tímom a organizáciám

## Prečo je jemné doladenie dôležité

### Aplikácie v reálnom svete

Jemné doladenie je nevyhnutné v mnohých scenároch:

**1. Adaptácia na doménu**
- Medicínska AI: Prispôsobenie všeobecných jazykových modelov na medicínsku terminológiu a klinické poznámky
- Právne technológie: Špecializácia modelov na analýzu právnych dokumentov a revíziu zmlúv
- Finančné služby: Prispôsobenie modelov na analýzu finančných správ a hodnotenie rizík

**2. Špecializácia úloh**
- Generovanie obsahu: Jemné doladenie na špecifické štýly alebo tóny písania
- Generovanie kódu: Prispôsobenie modelov na konkrétne programovacie jazyky alebo rámce
- Preklad: Zlepšenie výkonu pre konkrétne jazykové páry alebo technické oblasti

**3. Firemné aplikácie**
- Zákaznícky servis: Vytváranie chatbotov, ktoré rozumejú špecifickej terminológii spoločnosti
- Interná dokumentácia: Budovanie AI asistentov oboznámených s procesmi organizácie
- Riešenia pre konkrétne odvetvia: Vývoj modelov, ktoré rozumejú špecifickému žargónu a pracovným postupom v danom sektore

## Typy jemného doladenia

### 1. Plné jemné doladenie (Inštrukčné jemné doladenie)

Pri plnom jemnom doladení sa počas tréningu aktualizujú všetky parametre modelu. Tento prístup:
- Poskytuje maximálnu flexibilitu a potenciál výkonu
- Vyžaduje značné výpočtové zdroje
- Výsledkom je úplne nová verzia modelu
- Najlepšie pre scenáre, kde máte dostatok tréningových dát a výpočtových zdrojov

### 2. Jemné doladenie efektívne na parametre (PEFT)

Metódy PEFT aktualizujú iba malú podmnožinu parametrov, čím je proces efektívnejší:

#### Low-Rank Adaptation (LoRA)
- Pridáva malé trénovateľné matice rozkladu hodnosti k existujúcim váham
- Výrazne znižuje počet trénovateľných parametrov
- Udržiava výkon blízky plnému jemnému doladeniu
- Umožňuje jednoduché prepínanie medzi rôznymi adaptáciami

#### QLoRA (Kvantizované LoRA)
- Kombinuje LoRA s kvantizačnými technikami
- Ďalej znižuje požiadavky na pamäť
- Umožňuje jemné doladenie väčších modelov na spotrebiteľskom hardvéri
- Vyvažuje efektívnosť s výkonom

#### Adaptéry
- Vkladajú malé neurónové siete medzi existujúce vrstvy
- Umožňujú cielené jemné doladenie pri zachovaní základného modelu nezmeneného
- Umožňujú modulárny prístup k prispôsobeniu modelu

### 3. Jemné doladenie špecifické pre úlohu

Zameriava sa na prispôsobenie modelov na konkrétne úlohy:
- **Klasifikácia**: Úprava modelov na kategorizáciu
- **Generovanie**: Optimalizácia na tvorbu obsahu a generovanie textu
- **Extrahovanie**: Jemné doladenie na extrakciu informácií a rozpoznávanie pomenovaných entít
- **Sumarizácia**: Špecializácia modelov na sumarizáciu dokumentov

## Jemné doladenie s Microsoft Olive

Microsoft Olive je komplexná sada nástrojov na optimalizáciu modelov, ktorá zjednodušuje proces jemného doladenia a poskytuje funkcie na úrovni podnikov.

### Čo je Microsoft Olive?

Microsoft Olive je open-source nástroj na optimalizáciu modelov, ktorý:
- Zjednodušuje pracovné postupy jemného doladenia pre rôzne hardvérové ciele
- Poskytuje vstavanú podporu pre populárne architektúry modelov (Llama, Phi, Qwen, Gemma)
- Ponúka možnosti nasadenia v cloude aj lokálne
- Integruje sa bezproblémovo s Azure ML a ďalšími službami Microsoft AI
- Podporuje automatickú optimalizáciu a kvantizáciu

### Hlavné funkcie

- **Optimalizácia podľa hardvéru**: Automaticky optimalizuje modely pre konkrétny hardvér (CPU, GPU, NPU)
- **Podpora viacerých formátov**: Funguje s modelmi PyTorch, Hugging Face a ONNX
- **Automatizované pracovné postupy**: Znižuje manuálnu konfiguráciu a pokusy-omyl
- **Integrácia do podnikov**: Vstavaná podpora pre Azure ML a nasadenie v cloude
- **Rozšíriteľná architektúra**: Umožňuje vlastné optimalizačné techniky

### Inštalácia a nastavenie

#### Základná inštalácia

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Voliteľné závislosti

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Overenie inštalácie

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktické príklady

### Príklad 1: Základné jemné doladenie pomocou Olive CLI

Tento príklad demonštruje jemné doladenie malého jazykového modelu na klasifikáciu fráz:

#### Krok 1: Príprava prostredia

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Krok 2: Jemné doladenie modelu

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Krok 3: Optimalizácia na nasadenie

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Príklad 2: Pokročilá konfigurácia s vlastným dátovým súborom

#### Krok 1: Príprava vlastného dátového súboru

Vytvorte JSON súbor s vašimi tréningovými dátami:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Krok 2: Vytvorenie konfiguračného súboru

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Krok 3: Spustenie jemného doladenia

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Príklad 3: Jemné doladenie QLoRA pre efektívnosť pamäte

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Najlepšie postupy a odporúčania

### Príprava dát

**1. Kvalita dát nad kvantitou**
- Uprednostnite kvalitné, rozmanité príklady pred veľkými objemami nekvalitných dát
- Zabezpečte, aby dáta reprezentovali váš cieľový prípad použitia
- Dôsledne čistite a predspracovávajte dáta

**2. Formát dát a šablóny**
- Používajte konzistentné formátovanie vo všetkých tréningových príkladoch
- Vytvorte jasné šablóny vstupov a výstupov, ktoré zodpovedajú vášmu prípadu použitia
- Zahrňte vhodné formátovanie inštrukcií pre modely jemne doladené na inštrukcie

**3. Rozdelenie dátového súboru**
- Rezervujte 10-20 % dát na validáciu
- Udržujte podobné rozdelenie medzi tréningovými a validačnými dátami
- Zvážte stratifikované vzorkovanie pre úlohy klasifikácie

### Konfigurácia tréningu

**1. Výber učebnej rýchlosti**
- Začnite s menšími učebnými rýchlosťami (1e-5 až 1e-4) pre jemné doladenie
- Používajte plánovanie učebnej rýchlosti pre lepšiu konvergenciu
- Sledujte krivky straty na úpravu rýchlosti

**2. Optimalizácia veľkosti dávky**
- Vyvážte veľkosť dávky s dostupnou pamäťou
- Používajte akumuláciu gradientov pre väčšie efektívne veľkosti dávok
- Zvážte vzťah medzi veľkosťou dávky a učebnou rýchlosťou

**3. Trvanie tréningu**
- Sledujte validačné metriky, aby ste sa vyhli pretrénovaniu
- Používajte skoré zastavenie, keď sa výkon validácie ustáli
- Pravidelne ukladajte kontrolné body na obnovu a analýzu

### Výber modelu

**1. Výber základného modelu**
- Vyberte modely predtrénované na podobných doménach, ak je to možné
- Zvážte veľkosť modelu vzhľadom na vaše výpočtové obmedzenia
- Posúďte licenčné požiadavky na komerčné použitie

**2. Výber metódy jemného doladenia**
- Používajte LoRA/QLoRA pre prostredia s obmedzenými zdrojmi
- Vyberte plné jemné doladenie, keď je kritický maximálny výkon
- Zvážte prístupy založené na adaptéroch pre scenáre s viacerými úlohami

### Správa zdrojov

**1. Optimalizácia hardvéru**
- Vyberte vhodný hardvér pre veľkosť modelu a metódu
- Efektívne využívajte pamäť GPU pomocou kontrolovania gradientov
- Zvážte cloudové riešenia pre väčšie modely

**2. Správa pamäte**
- Používajte tréning s miešanou presnosťou, ak je dostupný
- Implementujte akumuláciu gradientov pre obmedzenia pamäte
- Sledujte využitie pamäte GPU počas tréningu

## Pokročilé techniky

### Tréning viacerých adaptérov

Trénujte viacero adaptérov pre rôzne úlohy pri zdieľaní základného modelu:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Optimalizácia hyperparametrov

Implementujte systematické ladenie hyperparametrov:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Vlastné funkcie straty

Implementujte funkcie straty špecifické pre doménu:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Hodnotenie a monitorovanie

### Metriky a hodnotenie

**1. Štandardné metriky**
- **Presnosť**: Celková správnosť pre úlohy klasifikácie
- **Perplexita**: Miera kvality jazykového modelovania
- **BLEU/ROUGE**: Kvalita generovania textu a sumarizácie
- **F1 skóre**: Vyvážená presnosť a citlivosť pre klasifikáciu

**2. Metriky špecifické pre doménu**
- **Referenčné hodnoty úloh**: Používajte zavedené referenčné hodnoty pre vašu doménu
- **Hodnotenie ľuďmi**: Zahrňte hodnotenie ľuďmi pre subjektívne úlohy
- **Obchodné metriky**: Zlaďte s reálnymi obchodnými cieľmi

**3. Nastavenie hodnotenia**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Monitorovanie priebehu tréningu

**1. Sledovanie straty**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Monitorovanie validácie**
- Sledujte validačnú stratu spolu so stratou tréningu
- Sledujte príznaky pretrénovania (zvyšovanie validačnej straty pri znižovaní tréningovej straty)
- Používajte skoré zastavenie na základe validačných metrík

**3. Monitorovanie zdrojov**
- Sledujte využitie GPU/CPU
- Sledujte vzory využitia pamäte
- Sledujte rýchlosť tréningu a priepustnosť

## Bežné výzvy a riešenia

### Výzva 1: Pretrénovanie

**Príznaky:**
- Tréningová strata pokračuje v znižovaní, zatiaľ čo validačná strata rastie
- Veľký rozdiel medzi tréningovým a validačným výkonom
- Slabá generalizácia na nové dáta

**Riešenia:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Výzva 2: Obmedzenia pamäte

**Riešenia:**
- Používajte kontrolovanie gradientov
- Implementujte akumuláciu gradientov
- Vyberte metódy efektívne na parametre (LoRA, QLoRA)
- Využívajte paralelizmus modelu pre veľké modely

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Výzva 3: Pomalé tréningy

**Riešenia:**
- Optimalizujte pipeline načítania dát
- Používajte tréning s miešanou presnosťou
- Implementujte efektívne stratégie dávkovania
- Zvážte distribuované tréningy pre veľké dátové súbory

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Výzva 4: Slabý výkon

**Kroky diagnostiky:**
1. Overte kvalitu a formátovanie dát
2. Skontrolujte učebnú rýchlosť a trvanie tréningu
3. Posúďte výber základného modelu
4. Preskúmajte predspracovanie a tokenizáciu

**Riešenia:**
- Zvýšte rozmanitosť tréningových dát
- Upravte plán učebnej rýchlosti
- Vyskúšajte rôzne základné modely
- Implementujte techniky augmentácie dát

## Záver

Jemné doladenie je výkonná technika, ktorá demokratizuje prístup k najmodernejším schopnostiam AI. Využitím nástrojov ako Microsoft Olive môžu organizácie efektívne prispôsobiť predtrénované modely svojim špecifickým potrebám a zároveň optimalizovať výkon a obmedzenia zdrojov.

### Hlavné poznatky

1. **Vyberte správny prístup**: Zvoľte metódy jemného doladenia na základe vašich výpočtových zdrojov a požiadaviek na výkon
2. **Kvalita dát je dôležitá**: Investujte do kvalitných, reprezentatívnych tréningových dát
3. **Monitorujte a iterujte**: Neustále hodnotte a zlepšujte svoje modely
4. **Využívajte nástroje**: Používajte rámce ako Olive na zjednodušenie a optimalizáciu procesu
5. **Zvážte nasadenie**: Plánujte optimalizáciu a nasadenie modelu od začiatku

##

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.