<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-18T18:59:37+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "sk"
}
-->
# Sekcia 2: Destilácia modelov - Od teórie k praxi

## Obsah
1. [Úvod do destilácie modelov](../../../Module05)
2. [Prečo je destilácia dôležitá](../../../Module05)
3. [Proces destilácie](../../../Module05)
4. [Praktická implementácia](../../../Module05)
5. [Príklad destilácie v Azure ML](../../../Module05)
6. [Najlepšie postupy a optimalizácia](../../../Module05)
7. [Aplikácie v reálnom svete](../../../Module05)
8. [Záver](../../../Module05)

## Úvod do destilácie modelov {#introduction}

Destilácia modelov je výkonná technika, ktorá nám umožňuje vytvárať menšie a efektívnejšie modely, pričom si zachovávajú veľkú časť výkonu väčších a komplexnejších modelov. Tento proces zahŕňa trénovanie kompaktného "študentského" modelu, aby napodobňoval správanie väčšieho "učiteľského" modelu.

**Hlavné výhody:**
- **Znížené požiadavky na výpočtový výkon** pri inferencii
- **Nižšia spotreba pamäte** a potreby úložiska
- **Rýchlejšie časy inferencie** pri zachovaní primeranej presnosti
- **Nákladovo efektívne nasadenie** v prostrediach s obmedzenými zdrojmi

## Prečo je destilácia dôležitá {#why-distillation-matters}

Veľké jazykové modely (LLM) sú čoraz výkonnejšie, ale zároveň čoraz náročnejšie na zdroje. Model s miliardami parametrov môže poskytovať vynikajúce výsledky, ale nemusí byť praktický pre mnohé aplikácie v reálnom svete z dôvodu:

### Obmedzenia zdrojov
- **Výpočtová náročnosť**: Veľké modely vyžadujú značnú pamäť GPU a výpočtový výkon
- **Latencia inferencie**: Komplexné modely potrebujú viac času na generovanie odpovedí
- **Spotreba energie**: Väčšie modely spotrebujú viac energie, čo zvyšuje prevádzkové náklady
- **Náklady na infraštruktúru**: Hostovanie veľkých modelov vyžaduje drahý hardvér

### Praktické obmedzenia
- **Nasadenie na mobilných zariadeniach**: Veľké modely nemôžu efektívne fungovať na mobilných zariadeniach
- **Aplikácie v reálnom čase**: Aplikácie vyžadujúce nízku latenciu nemôžu tolerovať pomalú inferenciu
- **Edge computing**: IoT a edge zariadenia majú obmedzené výpočtové zdroje
- **Nákladové obmedzenia**: Mnohé organizácie si nemôžu dovoliť infraštruktúru na nasadenie veľkých modelov

## Proces destilácie {#the-distillation-process}

Destilácia modelov prebieha v dvoch etapách, ktoré prenášajú znalosti z učiteľského modelu na študentský model:

### Etapa 1: Generovanie syntetických dát

Učiteľský model generuje odpovede pre váš tréningový dataset, čím vytvára kvalitné syntetické dáta, ktoré zachytávajú znalosti a vzory uvažovania učiteľa.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Kľúčové aspekty tejto etapy:**
- Učiteľský model spracováva každý tréningový príklad
- Generované odpovede sa stávajú "pravdou" pre tréning študenta
- Tento proces zachytáva rozhodovacie vzory učiteľa
- Kvalita syntetických dát priamo ovplyvňuje výkon študentského modelu

### Etapa 2: Doladenie študentského modelu

Študentský model je trénovaný na syntetickom datasete, aby sa naučil replikovať správanie a odpovede učiteľa.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Ciele tréningu:**
- Minimalizovať rozdiel medzi výstupmi študenta a učiteľa
- Zachovať znalosti učiteľa v menšom priestore parametrov
- Udržať výkon pri znížení komplexnosti modelu

## Praktická implementácia {#practical-implementation}

### Výber učiteľského a študentského modelu

**Výber učiteľského modelu:**
- Vyberte veľké LLM (100B+ parametrov) s osvedčeným výkonom pre vašu konkrétnu úlohu
- Populárne učiteľské modely zahŕňajú:
  - **DeepSeek V3** (671B parametrov) - vynikajúci pre uvažovanie a generovanie kódu
  - **Meta Llama 3.1 405B Instruct** - komplexné všeobecné schopnosti
  - **GPT-4** - silný výkon naprieč rôznymi úlohami
  - **Claude 3.5 Sonnet** - vynikajúci pre komplexné úlohy uvažovania
- Uistite sa, že učiteľský model dobre funguje na vašich doménovo špecifických dátach

**Výber študentského modelu:**
- Nájdite rovnováhu medzi veľkosťou modelu a požiadavkami na výkon
- Zamerajte sa na efektívne, menšie modely ako:
  - **Microsoft Phi-4-mini** - najnovší efektívny model so silnými schopnosťami uvažovania
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K a 128K varianty)
  - Microsoft Phi-3.5 Mini Instruct

### Kroky implementácie

1. **Príprava dát**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Nastavenie učiteľského modelu**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Generovanie syntetických dát**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Tréning študentského modelu**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Príklad destilácie v Azure ML {#azure-ml-example}

Azure Machine Learning poskytuje komplexnú platformu na implementáciu destilácie modelov. Tu je postup, ako využiť Azure ML pre váš destilačný workflow:

### Predpoklady

1. **Azure ML Workspace**: Nastavte si pracovný priestor v príslušnom regióne
   - Zabezpečte prístup k veľkým učiteľským modelom (DeepSeek V3, Llama 405B)
   - Konfigurujte regióny na základe dostupnosti modelov

2. **Výpočtové zdroje**: Konfigurujte vhodné výpočtové inštancie pre tréning
   - Inštancie s vysokou pamäťou pre inferenciu učiteľského modelu
   - GPU-enabled výpočty pre doladenie študentského modelu

### Podporované typy úloh

Azure ML podporuje destiláciu pre rôzne úlohy:

- **Interpretácia prirodzeného jazyka (NLI)**
- **Konverzačná AI**
- **Otázky a odpovede (QA)**
- **Matematické uvažovanie**
- **Sumarizácia textu**

### Ukážková implementácia

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Monitorovanie a hodnotenie

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Najlepšie postupy a optimalizácia {#best-practices}

### Kvalita dát

**Kvalitné tréningové dáta sú kľúčové:**
- Zabezpečte rozmanité a reprezentatívne tréningové príklady
- Používajte doménovo špecifické dáta, ak je to možné
- Validujte výstupy učiteľského modelu pred ich použitím na tréning študenta
- Vyvážte dataset, aby ste sa vyhli zaujatosti pri učení študentského modelu

### Doladenie hyperparametrov

**Kľúčové parametre na optimalizáciu:**
- **Learning rate**: Začnite s menšími hodnotami (1e-5 až 5e-5) pre doladenie
- **Batch size**: Nájdite rovnováhu medzi obmedzeniami pamäte a stabilitou tréningu
- **Počet epoch**: Sledujte overfitting; zvyčajne postačujú 2-5 epochy
- **Teplotná škála**: Nastavte mäkkosť výstupov učiteľa pre lepší prenos znalostí

### Úvahy o architektúre modelu

**Kompatibilita učiteľ-študent:**
- Zabezpečte architektonickú kompatibilitu medzi učiteľským a študentským modelom
- Zvážte zhodu medzi medzivrstvami pre lepší prenos znalostí
- Používajte techniky prenosu pozornosti, ak je to možné

### Hodnotiace stratégie

**Komplexný hodnotiaci prístup:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Aplikácie v reálnom svete {#real-world-applications}

### Nasadenie na mobilných a edge zariadeniach

Destilované modely umožňujú AI schopnosti na zariadeniach s obmedzenými zdrojmi:
- **Aplikácie na smartfónoch** s textovým spracovaním v reálnom čase
- **IoT zariadenia** vykonávajúce lokálnu inferenciu
- **Vstavané systémy** s obmedzenými výpočtovými zdrojmi

### Nákladovo efektívne produkčné systémy

Organizácie využívajú destiláciu na zníženie prevádzkových nákladov:
- **Chatboty zákazníckej podpory** s rýchlejšími odpoveďami
- **Systémy moderovania obsahu** efektívne spracovávajúce veľké objemy
- **Služby prekladu v reálnom čase** s nižšími požiadavkami na latenciu

### Doménovo špecifické aplikácie

Destilácia pomáha vytvárať špecializované modely:
- **Pomoc pri medicínskej diagnostike** s ochranou súkromia pri lokálnej inferencii
- **Analýza právnych dokumentov** optimalizovaná pre konkrétne právne oblasti
- **Hodnotenie finančných rizík** s rýchlym rozhodovaním

### Prípadová štúdia: Zákaznícka podpora s DeepSeek V3 → Phi-4-mini

Technologická spoločnosť implementovala destiláciu pre svoj systém zákazníckej podpory:

**Detaily implementácie:**
- **Učiteľský model**: DeepSeek V3 (671B parametrov) - vynikajúce uvažovanie pre komplexné zákaznícke otázky
- **Študentský model**: Phi-4-mini - optimalizovaný pre rýchlu inferenciu a nasadenie
- **Tréningové dáta**: 50 000 konverzácií zákazníckej podpory
- **Úloha**: Viacotáčková konverzačná podpora s technickým riešením problémov

**Dosiahnuté výsledky:**
- **85% zníženie** času inferencie (z 3,2s na 0,48s na odpoveď)
- **95% zníženie** požiadaviek na pamäť (z 1,2TB na 60GB)
- **92% zachovanie** presnosti pôvodného modelu pri úlohách podpory
- **60% zníženie** prevádzkových nákladov
- **Zlepšená škálovateľnosť** - teraz zvládne 10x viac súčasných používateľov

**Rozpis výkonu:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Záver {#conclusion}

Destilácia modelov predstavuje kľúčovú techniku na demokratizáciu prístupu k pokročilým AI schopnostiam. Umožňuje vytváranie menších, efektívnejších modelov, ktoré si zachovávajú veľkú časť výkonu svojich väčších náprotivkov, čím rieši rastúcu potrebu praktického nasadenia AI.

### Hlavné poznatky

1. **Destilácia prekonáva bariéru** medzi výkonom modelu a praktickými obmedzeniami
2. **Dvojstupňový proces** zabezpečuje efektívny prenos znalostí z učiteľa na študenta
3. **Azure ML poskytuje robustnú infraštruktúru** na implementáciu destilačných workflowov
4. **Správne hodnotenie a optimalizácia** sú nevyhnutné pre úspešnú destiláciu
5. **Aplikácie v reálnom svete** ukazujú významné výhody v nákladoch, rýchlosti a dostupnosti

### Budúce smerovanie

Ako sa oblasť ďalej vyvíja, môžeme očakávať:
- **Pokročilé techniky destilácie** s lepšími metódami prenosu znalostí
- **Destiláciu s viacerými učiteľmi** pre rozšírené schopnosti študentských modelov
- **Automatizovanú optimalizáciu** destilačného procesu
- **Širšiu podporu modelov** naprieč rôznymi architektúrami a doménami

Destilácia modelov umožňuje organizáciám využívať najmodernejšie AI schopnosti pri zachovaní praktických obmedzení nasadenia, čím sprístupňuje pokročilé jazykové modely v širokej škále aplikácií a prostredí.

## ➡️ Čo ďalej

- [03: Doladenie - Prispôsobenie modelov pre konkrétne úlohy](./03.SLMOps-Finetuing.md)

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho pôvodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.