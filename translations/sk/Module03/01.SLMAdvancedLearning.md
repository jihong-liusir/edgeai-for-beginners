<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-08T15:55:42+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "sk"
}
-->
# Sekcia 1: Pokroƒçil√© uƒçenie SLM - Z√°klady a optimaliz√°cia

Mal√© jazykov√© modely (SLM) predstavuj√∫ z√°sadn√Ω pokrok v EdgeAI, umo≈æ≈àuj√∫ci sofistikovan√© spracovanie prirodzen√©ho jazyka na zariadeniach s obmedzen√Ωmi zdrojmi. Porozumenie tomu, ako efekt√≠vne nasadi≈•, optimalizova≈• a vyu≈æ√≠va≈• SLM, je nevyhnutn√© pre vytv√°ranie praktick√Ωch AI rie≈°en√≠ na okraji siete.

## √övod

V tejto lekcii presk√∫mame mal√© jazykov√© modely (SLM) a ich pokroƒçil√© implementaƒçn√© strat√©gie. Pokryjeme z√°kladn√© koncepty SLM, ich hranice parametrov a klasifik√°cie, techniky optimaliz√°cie a praktick√© strat√©gie nasadenia pre prostredia edge computingu.

## Ciele uƒçenia

Na konci tejto lekcie budete schopn√≠:

- üî¢ Porozumie≈• hraniciam parametrov a klasifik√°ci√°m mal√Ωch jazykov√Ωch modelov.
- üõ†Ô∏è Identifikova≈• kƒæ√∫ƒçov√© techniky optimaliz√°cie pre nasadenie SLM na edge zariadeniach.
- üöÄ Nauƒçi≈• sa implementova≈• pokroƒçil√© strat√©gie kvantiz√°cie a kompresie pre SLM.

## Porozumenie hraniciam parametrov a klasifik√°ci√°m SLM

Mal√© jazykov√© modely (SLM) s√∫ AI modely navrhnut√© na spracovanie, porozumenie a generovanie obsahu prirodzen√©ho jazyka s v√Ωrazne men≈°√≠m poƒçtom parametrov ako ich veƒæk√© n√°protivky. Zatiaƒæ ƒço veƒæk√© jazykov√© modely (LLM) obsahuj√∫ stovky mili√°rd a≈æ bili√≥ny parametrov, SLM s√∫ ≈°peci√°lne navrhnut√© pre efekt√≠vnos≈• a nasadenie na okraji siete.

R√°mec klasifik√°cie parametrov n√°m pom√°ha pochopi≈• r√¥zne kateg√≥rie SLM a ich vhodn√© pou≈æitie. T√°to klasifik√°cia je kƒæ√∫ƒçov√° pre v√Ωber spr√°vneho modelu pre konkr√©tne scen√°re edge computingu.

### R√°mec klasifik√°cie parametrov

Porozumenie hraniciam parametrov pom√°ha pri v√Ωbere vhodn√Ωch modelov pre r√¥zne scen√°re edge computingu:

- **üî¨ Mikro SLM**: 100M - 1,4B parametrov (ultraƒæahk√© pre mobiln√© zariadenia)
- **üì± Mal√© SLM**: 1,5B - 13,9B parametrov (vyv√°≈æen√Ω v√Ωkon a efekt√≠vnos≈•)
- **‚öñÔ∏è Stredn√© SLM**: 14B - 30B parametrov (bl√≠≈æiace sa schopnostiam LLM pri zachovan√≠ efekt√≠vnosti)

Presn√° hranica zost√°va v r√°mci v√Ωskumnej komunity flexibiln√°, ale v√§ƒç≈°ina odborn√≠kov pova≈æuje modely s menej ako 30 miliardami parametrov za "mal√©", priƒçom niektor√© zdroje nastavuj√∫ prah e≈°te ni≈æ≈°ie na 10 mili√°rd parametrov.

### Kƒæ√∫ƒçov√© v√Ωhody SLM

SLM pon√∫kaj√∫ niekoƒæko z√°kladn√Ωch v√Ωhod, ktor√© ich robia ide√°lnymi pre aplik√°cie edge computingu:

**Prev√°dzkov√° efekt√≠vnos≈•**: SLM poskytuj√∫ r√Ωchlej≈°ie ƒçasy inferencie vƒèaka men≈°iemu poƒçtu parametrov na spracovanie, ƒço ich rob√≠ ide√°lnymi pre aplik√°cie v re√°lnom ƒçase. Vy≈æaduj√∫ ni≈æ≈°ie v√Ωpoƒçtov√© zdroje, umo≈æ≈àuj√∫ nasadenie na zariadeniach s obmedzen√Ωmi zdrojmi, spotrebuj√∫ menej energie a udr≈æuj√∫ zn√≠≈æen√∫ uhl√≠kov√∫ stopu.

**Flexibilita nasadenia**: Tieto modely umo≈æ≈àuj√∫ AI schopnosti priamo na zariaden√≠ bez potreby internetov√©ho pripojenia, zvy≈°uj√∫ s√∫kromie a bezpeƒçnos≈• prostredn√≠ctvom lok√°lneho spracovania, m√¥≈æu by≈• prisp√¥soben√© pre aplik√°cie ≈°pecifick√© pre dan√∫ oblas≈• a s√∫ vhodn√© pre r√¥zne prostredia edge computingu.

**N√°kladov√° efekt√≠vnos≈•**: SLM pon√∫kaj√∫ n√°kladovo efekt√≠vne ≈°kolenie a nasadenie v porovnan√≠ s LLM, s ni≈æ≈°√≠mi prev√°dzkov√Ωmi n√°kladmi a ni≈æ≈°√≠mi po≈æiadavkami na ≈°√≠rku p√°sma pre aplik√°cie na okraji siete.

## Pokroƒçil√© strat√©gie z√≠skavania modelov

### Ekosyst√©m Hugging Face

Hugging Face sl√∫≈æi ako hlavn√© centrum na objavovanie a pr√≠stup k najmodernej≈°√≠m SLM. Platforma poskytuje komplexn√© zdroje na objavovanie a nasadenie modelov:

**Funkcie objavovania modelov**: Platforma pon√∫ka pokroƒçil√© filtrovanie podƒæa poƒçtu parametrov, typu licencie a v√Ωkonnostn√Ωch metr√≠k. Pou≈æ√≠vatelia m√¥≈æu pristupova≈• k n√°strojom na porovn√°vanie modelov vedƒæa seba, k v√Ωkonnostn√Ωm benchmarkom v re√°lnom ƒçase a k hodnotiacim v√Ωsledkom, ako aj k WebGPU dem√°m na okam≈æit√© testovanie.

**Kur√°torsk√© kolekcie SLM**: Popul√°rne modely zah≈ï≈àaj√∫ Phi-4-mini-3.8B pre pokroƒçil√© √∫lohy uva≈æovania, s√©riu Qwen3 (0.6B/1.7B/4B) pre viacjazyƒçn√© aplik√°cie, Google Gemma3 pre efekt√≠vne √∫lohy v≈°eobecn√©ho pou≈æitia a experiment√°lne modely ako BitNET pre ultra-n√≠zku presnos≈• nasadenia. Platforma tie≈æ obsahuje kolekcie riaden√© komunitou so ≈°pecializovan√Ωmi modelmi pre konkr√©tne oblasti a pred≈°kolen√© a in≈°trukƒçne laden√© varianty optimalizovan√© pre r√¥zne pou≈æitia.

### Katal√≥g modelov Azure AI Foundry

Katal√≥g modelov Azure AI Foundry poskytuje pr√≠stup k SLM na podnikovej √∫rovni s roz≈°√≠ren√Ωmi integraƒçn√Ωmi schopnos≈•ami:

**Podnikov√° integr√°cia**: Katal√≥g obsahuje modely pred√°van√© priamo spoloƒçnos≈•ou Azure s podporou na podnikovej √∫rovni a SLA, vr√°tane Phi-4-mini-3.8B pre pokroƒçil√© schopnosti uva≈æovania a Llama 3-8B pre produkƒçn√© nasadenie. Obsahuje tie≈æ modely ako Qwen3 8B od d√¥veryhodn√Ωch tret√≠ch str√°n otvoren√©ho zdroja.

**V√Ωhody pre podniky**: Integrovan√© n√°stroje na doladenie, pozorovateƒænos≈• a zodpovedn√© AI s√∫ integrovan√© s flexibiln√Ωm Provisioned Throughput naprieƒç rodinami modelov. Priama podpora od Microsoftu s podnikov√Ωmi SLA, integrovan√© bezpeƒçnostn√© a s√∫ladov√© funkcie a komplexn√© pracovn√© postupy nasadenia zlep≈°uj√∫ podnikov√© sk√∫senosti.

## Pokroƒçil√© techniky kvantiz√°cie a optimaliz√°cie

### Optimalizaƒçn√Ω r√°mec Llama.cpp

Llama.cpp poskytuje ≈°piƒçkov√© techniky kvantiz√°cie pre maxim√°lnu efekt√≠vnos≈• pri nasaden√≠ na okraji siete:

**Met√≥dy kvantiz√°cie**: R√°mec podporuje r√¥zne √∫rovne kvantiz√°cie vr√°tane Q4_0 (4-bitov√° kvantiz√°cia s vynikaj√∫cim zn√≠≈æen√≠m veƒækosti - ide√°lne pre mobiln√© nasadenie Qwen3-0.6B), Q5_1 (5-bitov√° kvantiz√°cia vyva≈æuj√∫ca kvalitu a kompresiu - vhodn√° pre inferenciu Phi-4-mini-3.8B na okraji siete) a Q8_0 (8-bitov√° kvantiz√°cia pre takmer p√¥vodn√∫ kvalitu - odpor√∫ƒçan√° pre produkƒçn√© pou≈æitie Google Gemma3). BitNET predstavuje ≈°piƒçku s 1-bitovou kvantiz√°ciou pre extr√©mne kompresn√© scen√°re.

**V√Ωhody implement√°cie**: Inferencia optimalizovan√° pre CPU so zr√Ωchlen√≠m SIMD poskytuje pam√§≈•ovo efekt√≠vne naƒç√≠tanie a vykon√°vanie modelov. Kompatibilita naprieƒç platformami na architekt√∫rach x86, ARM a Apple Silicon umo≈æ≈àuje hardv√©rovo agnostick√© schopnosti nasadenia.

**Praktick√Ω pr√≠klad implement√°cie**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Porovnanie pam√§≈•ovej stopy**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Optimalizaƒçn√Ω bal√≠k Microsoft Olive

Microsoft Olive pon√∫ka komplexn√© pracovn√© postupy optimaliz√°cie modelov navrhnut√© pre produkƒçn√© prostredia:

**Techniky optimaliz√°cie**: Bal√≠k zah≈ï≈àa dynamick√∫ kvantiz√°ciu pre automatick√Ω v√Ωber presnosti (obzvl√°≈°≈• efekt√≠vne pri modeloch s√©rie Qwen3), optimaliz√°ciu grafov a f√∫ziu oper√°torov (optimalizovan√© pre architekt√∫ru Google Gemma3), optimaliz√°cie ≈°pecifick√© pre hardv√©r pre CPU, GPU a NPU (so ≈°peci√°lnou podporou pre Phi-4-mini-3.8B na ARM zariadeniach) a viacstup≈àov√© optimalizaƒçn√© pipeline. Modely BitNET vy≈æaduj√∫ ≈°pecializovan√© pracovn√© postupy 1-bitovej kvantiz√°cie v r√°mci Olive frameworku.

**Automatiz√°cia pracovn√Ωch postupov**: Automatizovan√© benchmarky naprieƒç variantmi optimaliz√°cie zabezpeƒçuj√∫ zachovanie kvalitat√≠vnych metr√≠k poƒças optimaliz√°cie. Integr√°cia s popul√°rnymi ML frameworkmi ako PyTorch a ONNX poskytuje optimalizaƒçn√© schopnosti pre cloud a edge nasadenie.

**Praktick√Ω pr√≠klad implement√°cie**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### R√°mec Apple MLX

Apple MLX poskytuje nat√≠vnu optimaliz√°ciu ≈°peci√°lne navrhnut√∫ pre zariadenia Apple Silicon:

**Optimaliz√°cia pre Apple Silicon**: R√°mec vyu≈æ√≠va jednotn√∫ pam√§≈•ov√∫ architekt√∫ru s integr√°ciou Metal Performance Shaders, automatick√∫ inferenciu zmie≈°anej presnosti (obzvl√°≈°≈• efekt√≠vne pri Google Gemma3) a optimalizovan√© vyu≈æitie pam√§≈•ovej ≈°√≠rky p√°sma. Phi-4-mini-3.8B vykazuje v√Ωnimoƒçn√Ω v√Ωkon na ƒçipoch s√©rie M, zatiaƒæ ƒço Qwen3-1.7B poskytuje optim√°lnu rovnov√°hu pre nasadenie na MacBook Air.

**V√Ωvojov√© funkcie**: Podpora API pre Python a Swift s oper√°ciami kompatibiln√Ωmi s NumPy, schopnosti automatickej diferenci√°cie a bezprobl√©mov√° integr√°cia s v√Ωvojov√Ωmi n√°strojmi Apple poskytuj√∫ komplexn√© v√Ωvojov√© prostredie.

**Praktick√Ω pr√≠klad implement√°cie**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Produkƒçn√© nasadenie a strat√©gie inferencie

### Ollama: Zjednodu≈°en√© lok√°lne nasadenie

Ollama zjednodu≈°uje nasadenie SLM s funkciami pripraven√Ωmi na podnikov√© prostredia pre lok√°lne a edge prostredia:

**Schopnosti nasadenia**: In≈°tal√°cia a vykon√°vanie modelov jedn√Ωm pr√≠kazom s automatick√Ωm s≈•ahovan√≠m a cachovan√≠m modelov. Podpora pre Phi-4-mini-3.8B, cel√∫ s√©riu Qwen3 (0.6B/1.7B/4B) a Google Gemma3 s REST API pre integr√°ciu aplik√°ci√≠ a schopnosti spr√°vy a prep√≠nania viacer√Ωch modelov. Modely BitNET vy≈æaduj√∫ experiment√°lne konfigur√°cie buildov pre podporu 1-bitovej kvantiz√°cie.

**Pokroƒçil√© funkcie**: Podpora doladenia vlastn√Ωch modelov, generovanie Dockerfile pre kontajnerizovan√© nasadenie, akceler√°cia GPU s automatickou detekciou a mo≈ænosti kvantiz√°cie a optimaliz√°cie modelov poskytuj√∫ komplexn√∫ flexibilitu nasadenia.

### VLLM: Inferencia s vysok√Ωm v√Ωkonom

VLLM poskytuje optimaliz√°ciu inferencie na produkƒçnej √∫rovni pre scen√°re s vysokou priepustnos≈•ou:

**Optimaliz√°cie v√Ωkonu**: PagedAttention pre pam√§≈•ovo efekt√≠vne v√Ωpoƒçty pozornosti (obzvl√°≈°≈• prospe≈°n√© pre transform√°torov√∫ architekt√∫ru Phi-4-mini-3.8B), dynamick√© d√°vkovanie pre optimaliz√°ciu priepustnosti (optimalizovan√© pre paraleln√© spracovanie s√©rie Qwen3), paralelizmus tensorov pre ≈°k√°lovanie na viacer√Ωch GPU (podpora Google Gemma3) a ≈°pekulat√≠vne dek√≥dovanie pre zn√≠≈æenie latencie. Modely BitNET vy≈æaduj√∫ ≈°pecializovan√© inferenƒçn√© jadr√° pre oper√°cie s 1-bitovou presnos≈•ou.

**Podnikov√° integr√°cia**: API kompatibiln√© s OpenAI, podpora nasadenia Kubernetes, integr√°cia monitorovania a pozorovateƒænosti a schopnosti automatick√©ho ≈°k√°lovania poskytuj√∫ rie≈°enia nasadenia na podnikovej √∫rovni.

### Foundry Local: Microsoftovo rie≈°enie pre edge

Foundry Local poskytuje komplexn√© schopnosti nasadenia na okraji siete pre podnikov√© prostredia:

**Funkcie edge computingu**: Dizajn architekt√∫ry "offline-first" s optimaliz√°ciou obmedzen√Ωch zdrojov, spr√°va lok√°lneho registra modelov a schopnosti synchroniz√°cie edge-to-cloud zabezpeƒçuj√∫ spoƒæahliv√© nasadenie na okraji siete.

**Bezpeƒçnos≈• a s√∫lad**: Lok√°lne spracovanie √∫dajov na zachovanie s√∫kromia, podnikov√© bezpeƒçnostn√© kontroly, auditovanie a reportovanie s√∫ladu a spr√°va pr√≠stupu na z√°klade rol√≠ poskytuj√∫ komplexn√∫ bezpeƒçnos≈• pre nasadenia na okraji siete.

## Najlep≈°ie postupy pri implement√°cii SLM

### Usmernenia pre v√Ωber modelu

Pri v√Ωbere SLM pre nasadenie na okraji siete zv√°≈æte nasleduj√∫ce faktory:

**√övahy o poƒçte parametrov**: Vyberte mikro SLM ako Qwen3-0.6B pre ultraƒæahk√© mobiln√© aplik√°cie, mal√© SLM ako Qwen3-1.7B alebo Google Gemma3 pre scen√°re vyv√°≈æen√©ho v√Ωkonu a stredn√© SLM ako Phi-4-mini-3.8B alebo Qwen3-4B pri pribli≈æovan√≠ sa schopnostiam LLM pri zachovan√≠ efekt√≠vnosti. Modely BitNET pon√∫kaj√∫ experiment√°lnu ultra-kompresiu pre ≈°pecifick√© v√Ωskumn√© aplik√°cie.

**Zladenie s pr√≠padom pou≈æitia**: Prisp√¥sobte schopnosti modelu konkr√©tnym po≈æiadavk√°m aplik√°cie, priƒçom zohƒæadnite faktory ako kvalita odpovede, r√Ωchlos≈• inferencie, pam√§≈•ov√© obmedzenia a po≈æiadavky na offline prev√°dzku.

### V√Ωber strat√©gie optimaliz√°cie

**Pr√≠stup ku kvantiz√°cii**: Vyberte vhodn√© √∫rovne kvantiz√°cie na z√°klade po≈æiadaviek na kvalitu a hardv√©rov√Ωch obmedzen√≠. Zv√°≈æte Q4_0 pre maxim√°lnu kompresiu (ide√°lne pre mobiln√© nasadenie Qwen3-0.6B), Q5_1 pre vyv√°≈æen√Ω kompromis medzi kvalitou a kompresiou (vhodn√© pre Phi-4-mini-3.8B a Google Gemma3) a Q8_0 pre zachovanie takmer p√¥vodnej kvality (odpor√∫ƒçan√© pre produkƒçn√© prostredia Qwen3-4B). BitNET s 1-bitovou kvantiz√°ciou predstavuje extr√©mnu kompresn√∫ hranicu pre ≈°pecializovan√© aplik√°cie.

**V√Ωber r√°mca**: Vyberte optimalizaƒçn√© r√°mce na z√°klade cieƒæov√©ho hardv√©ru a po≈æiadaviek na nasadenie. Pou≈æite Llama.cpp pre nasadenie optimalizovan√© pre CPU, Microsoft Olive pre komplexn√© pracovn√© postupy optimaliz√°cie a Apple MLX pre zariadenia Apple Silicon.

## Praktick√© pr√≠klady modelov a pr√≠pady pou≈æitia

### Scen√°re nasadenia v re√°lnom svete

**Mobiln√© aplik√°cie**: Qwen3-0.6B vynik√° v aplik√°ci√°ch chatbotov na smartf√≥noch s minim√°lnou pam√§≈•ovou stopou, zatiaƒæ ƒço Google Gemma3 poskytuje vyv√°≈æen√Ω v√Ωkon pre vzdel√°vacie n√°stroje na tabletoch. Phi-4-mini-3.8B pon√∫ka vynikaj√∫ce schopnosti uva≈æovania pre mobiln√© produkt√≠vne aplik√°cie.

**Desktop a edge computing**: Qwen3-1.7B poskytuje optim√°lny v√Ωkon pre aplik√°cie desktopov√Ωch asistentov, Phi-4-mini-3.8B poskytuje pokroƒçil√© schopnosti generovania k√≥du pre v√Ωvoj√°rske n√°stroje a Qwen3-4B umo≈æ≈àuje sofistikovan√∫ anal√Ωzu dokumentov na pracovn√Ωch staniciach.

**V√Ωskum a experiment√°lne**: Modely BitNET umo≈æ≈àuj√∫ sk√∫manie inferencie s ultra-n√≠zkou presnos≈•ou pre akademick√Ω v√Ωskum a aplik√°cie proof-of-concept vy≈æaduj√∫ce extr√©mne obmedzenia zdrojov.

### V√Ωkonnostn√© benchmarky a porovnania

**R√Ωchlos≈• inferencie**: Qwen3-0.6B dosahuje najr√Ωchlej≈°ie ƒçasy inferencie na mobiln√Ωch CPU, Google Gemma3 poskytuje vyv√°≈æen√Ω pomer r√Ωchlosti a kvality pre v≈°eobecn√© aplik√°cie, Phi-4-mini-3.8B pon√∫ka vynikaj√∫cu r√Ωchlos≈• uva≈æovania pre komplexn√© √∫lohy a BitNET poskytuje teoreticky maxim√°lnu priepustnos≈• so ≈°pecializovan√Ωm hardv√©rom.

**Po≈æiadavky na pam√§≈•**: Pam√§≈•ov√© stopy modelov sa pohybuj√∫ od Qwen3-0.6B (pod 1GB

---

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, upozor≈àujeme, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nenesieme zodpovednos≈• za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.