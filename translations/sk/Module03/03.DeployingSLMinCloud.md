<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T19:08:28+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "sk"
}
-->
# Kontajnerové nasadenie v cloude - riešenia v produkčnom meradle

Tento komplexný návod pokrýva tri hlavné prístupy k nasadeniu modelu Phi-4-mini-instruct od Microsoftu v kontajnerových prostrediach: vLLM, Ollama a SLM Engine s ONNX Runtime. Tento model s 3,8 miliardami parametrov predstavuje optimálnu voľbu pre úlohy vyžadujúce logické uvažovanie, pričom si zachováva efektivitu pre nasadenie na okraji siete.

## Obsah

1. [Úvod do kontajnerového nasadenia Phi-4-mini](../../../Module03)
2. [Ciele učenia](../../../Module03)
3. [Porozumenie klasifikácii Phi-4-mini](../../../Module03)
4. [Nasadenie kontajnera vLLM](../../../Module03)
5. [Nasadenie kontajnera Ollama](../../../Module03)
6. [SLM Engine s ONNX Runtime](../../../Module03)
7. [Porovnávací rámec](../../../Module03)
8. [Najlepšie postupy](../../../Module03)

## Úvod do kontajnerového nasadenia Phi-4-mini

Malé jazykové modely (SLM) predstavujú zásadný pokrok v oblasti EdgeAI, umožňujúci sofistikované spracovanie prirodzeného jazyka na zariadeniach s obmedzenými zdrojmi. Tento návod sa zameriava na stratégie kontajnerového nasadenia modelu Phi-4-mini-instruct od Microsoftu, špičkového modelu pre logické uvažovanie, ktorý vyvažuje schopnosti a efektivitu.

### Predstavený model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 miliardy parametrov)**: Najnovší ľahký model od Microsoftu, optimalizovaný pre prostredia s obmedzenou pamäťou/výpočtovým výkonom, s výnimočnými schopnosťami v:
- **Matematickom uvažovaní a komplexných výpočtoch**
- **Generovaní, ladení a analýze kódu**
- **Logickom riešení problémov a krokovom uvažovaní**
- **Vzdelávacích aplikáciách vyžadujúcich podrobné vysvetlenia**
- **Volaní funkcií a integrácii nástrojov**

Ako súčasť kategórie "Malé SLM" (1,5B - 13,9B parametrov) Phi-4-mini dosahuje optimálnu rovnováhu medzi schopnosťou uvažovania a efektivitou zdrojov.

### Výhody kontajnerového nasadenia Phi-4-mini

- **Prevádzková efektivita**: Rýchle inferencie pre úlohy uvažovania s nižšími výpočtovými požiadavkami
- **Flexibilita nasadenia**: AI na zariadení s vylepšeným súkromím prostredníctvom lokálneho spracovania
- **Nákladová efektívnosť**: Znížené prevádzkové náklady v porovnaní s väčšími modelmi pri zachovaní kvality
- **Izolácia**: Čisté oddelenie medzi inštanciami modelu a bezpečné prostredia vykonávania
- **Škálovateľnosť**: Jednoduché horizontálne škálovanie pre zvýšenie priepustnosti uvažovania

## Ciele učenia

Na konci tohto návodu budete schopní:

- Nasadiť a optimalizovať Phi-4-mini-instruct v rôznych kontajnerových prostrediach
- Implementovať pokročilé stratégie kvantizácie a kompresie pre rôzne scenáre nasadenia
- Konfigurovať produkčne pripravenú kontajnerovú orchestráciu pre úlohy uvažovania
- Vyhodnotiť a vybrať vhodné rámce nasadenia na základe konkrétnych požiadaviek
- Aplikovať bezpečnostné, monitorovacie a škálovacie najlepšie postupy pre kontajnerové nasadenia SLM

## Porozumenie klasifikácii Phi-4-mini

### Špecifikácie modelu

**Technické detaily:**
- **Parametre**: 3,8 miliardy (kategória Malé SLM)
- **Architektúra**: Hustý dekodér-only Transformer so skupinovou dotazovou pozornosťou
- **Dĺžka kontextu**: 128K tokenov (32K odporúčaných pre optimálny výkon)
- **Slovník**: 200K tokenov s podporou viacerých jazykov
- **Tréningové dáta**: 5T tokenov vysokokvalitného obsahu zameraného na uvažovanie

### Požiadavky na zdroje

| Typ nasadenia | Min RAM | Odporúčaná RAM | VRAM (GPU) | Úložisko | Typické použitie |
|---------------|---------|----------------|------------|---------|------------------|
| **Vývoj** | 6GB | 8GB | - | 8GB | Lokálne testovanie, prototypovanie |
| **Produkcia CPU** | 8GB | 12GB | - | 10GB | Edge servery, nákladovo optimalizované nasadenie |
| **Produkcia GPU** | 6GB | 8GB | 4-6GB | 8GB | Služby s vysokou priepustnosťou uvažovania |
| **Optimalizované pre okraj** | 4GB | 6GB | - | 6GB | Kvantizované nasadenie, IoT brány |

### Schopnosti Phi-4-mini

- **Matematická excelentnosť**: Pokročilé riešenie problémov z aritmetiky, algebry a kalkulu
- **Inteligencia kódu**: Generovanie kódu v Python, JavaScript a viacerých jazykoch s ladením
- **Logické uvažovanie**: Krokové rozkladanie problémov a konštrukcia riešení
- **Vzdelávacia podpora**: Podrobné vysvetlenia vhodné pre učenie a výučbu
- **Volanie funkcií**: Natívna podpora pre integráciu nástrojov a interakcie s API

## Nasadenie kontajnera vLLM

vLLM poskytuje vynikajúcu podporu pre Phi-4-mini-instruct s optimalizovaným výkonom inferencie a OpenAI-kompatibilnými API, čo ho robí ideálnym pre produkčné služby uvažovania.

### Príklady rýchleho štartu

#### Základné nasadenie na CPU (vývoj)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Produkčné nasadenie s akceleráciou GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Konfigurácia pre produkciu

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testovanie schopností uvažovania Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Nasadenie kontajnera Ollama

Ollama poskytuje vynikajúcu podporu pre Phi-4-mini-instruct so zjednodušeným nasadením a správou, čo ho robí ideálnym pre vývoj a vyvážené produkčné nasadenia.

### Rýchle nastavenie

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Konfigurácia pre produkciu

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimalizácia modelu a varianty

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Príklady použitia API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine s ONNX Runtime

ONNX Runtime poskytuje optimálny výkon pre nasadenie Phi-4-mini-instruct na okraji siete s pokročilou optimalizáciou a kompatibilitou naprieč platformami.

### Základné nastavenie

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Zjednodušená implementácia servera

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Skript na konverziu modelu

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Konfigurácia pre produkciu

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testovanie nasadenia ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Porovnávací rámec

### Porovnanie rámcov pre Phi-4-mini

| Funkcia | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Komplexnosť nastavenia** | Stredná | Jednoduchá | Komplexná |
| **Výkon (GPU)** | Vynikajúci (~25 tok/s) | Veľmi dobrý (~20 tok/s) | Dobrý (~15 tok/s) |
| **Výkon (CPU)** | Dobrý (~8 tok/s) | Veľmi dobrý (~12 tok/s) | Vynikajúci (~15 tok/s) |
| **Použitie pamäte** | 8-12GB | 6-10GB | 4-8GB |
| **Kompatibilita API** | OpenAI kompatibilné | Vlastné REST | Vlastné FastAPI |
| **Volanie funkcií** | ✅ Natívne | ✅ Podporované | ⚠️ Vlastná implementácia |
| **Podpora kvantizácie** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX kvantizácia |
| **Pripravené na produkciu** | ✅ Vynikajúce | ✅ Veľmi dobré | ✅ Dobré |
| **Nasadenie na okraji** | Dobrý | Vynikajúci | Výnimočný |

## Dodatočné zdroje

### Oficiálna dokumentácia
- **Microsoft Phi-4 Model Card**: Podrobné špecifikácie a pokyny na použitie
- **vLLM Dokumentácia**: Pokročilé možnosti konfigurácie a optimalizácie
- **Ollama Model Library**: Komunitné modely a príklady prispôsobenia
- **ONNX Runtime Guides**: Optimalizácia výkonu a stratégie nasadenia

### Nástroje pre vývoj
- **Hugging Face Transformers**: Pre interakciu a prispôsobenie modelu
- **OpenAI API Špecifikácia**: Pre testovanie kompatibility vLLM
- **Najlepšie postupy pre Docker**: Bezpečnosť kontajnerov a optimalizácia
- **Kubernetes Nasadenie**: Vzory orchestrácie pre škálovanie v produkcii

### Zdroje na učenie
- **Benchmarking výkonu SLM**: Metodológie porovnávacej analýzy
- **Nasadenie Edge AI**: Najlepšie postupy pre prostredia s obmedzenými zdrojmi
- **Optimalizácia úloh uvažovania**: Stratégie pre zadávanie otázok pri matematických a logických problémoch
- **Bezpečnosť kontajnerov**: Praktiky na zabezpečenie nasadenia AI modelov

## Výsledky učenia

Po dokončení tohto modulu budete schopní:

1. Nasadiť model Phi-4-mini-instruct v kontajnerových prostrediach pomocou viacerých rámcov
2. Konfigurovať a optimalizovať nasadenia SLM pre rôzne hardvérové prostredia
3. Implementovať najlepšie postupy bezpečnosti pre kontajnerové nasadenia AI
4. Porovnať a vybrať vhodné rámce nasadenia na základe konkrétnych požiadaviek
5. Aplikovať stratégie monitorovania a škálovania pre služby SLM v produkčnej kvalite

## Čo ďalej

- Vrátiť sa k [Modulu 1](../Module01/README.md)
- Vrátiť sa k [Modulu 2](../Module02/README.md)

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.