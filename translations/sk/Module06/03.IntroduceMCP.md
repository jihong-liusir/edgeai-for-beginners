<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T18:40:00+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "sk"
}
-->
# Sekcia 03 - Integrácia Model Context Protocol (MCP)

## Úvod do MCP (Model Context Protocol)

Model Context Protocol (MCP) je revolučný rámec, ktorý umožňuje jazykovým modelom interagovať s externými nástrojmi a systémami štandardizovaným spôsobom. Na rozdiel od tradičných prístupov, kde sú modely izolované, MCP vytvára most medzi AI modelmi a reálnym svetom prostredníctvom dobre definovaného protokolu.

### Čo je MCP?

MCP slúži ako komunikačný protokol, ktorý umožňuje jazykovým modelom:
- Pripojiť sa k externým zdrojom dát
- Vykonávať nástroje a funkcie
- Interagovať s API a službami
- Pristupovať k informáciám v reálnom čase
- Realizovať komplexné operácie v niekoľkých krokoch

Tento protokol transformuje statické jazykové modely na dynamických agentov schopných vykonávať praktické úlohy nad rámec generovania textu.

## Malé jazykové modely (SLMs) v MCP

Malé jazykové modely predstavujú efektívny prístup k nasadeniu AI, ponúkajúc niekoľko výhod:

### Výhody SLMs
- **Efektívnosť zdrojov**: Nižšie požiadavky na výpočtový výkon
- **Rýchlejšie odozvy**: Znížená latencia pre aplikácie v reálnom čase  
- **Nákladová efektívnosť**: Minimálne infraštruktúrne potreby
- **Ochrana súkromia**: Možnosť lokálneho behu bez prenosu dát
- **Prispôsobenie**: Jednoduchšie doladenie pre konkrétne oblasti

### Prečo SLMs dobre fungujú s MCP

SLMs v kombinácii s MCP vytvárajú silnú kombináciu, kde schopnosti modelu v oblasti uvažovania sú rozšírené o externé nástroje, čím kompenzujú ich menší počet parametrov prostredníctvom rozšírenej funkcionality.

## Prehľad Python MCP SDK

Python MCP SDK poskytuje základ pre budovanie aplikácií podporujúcich MCP. SDK zahŕňa:

- **Klientské knižnice**: Na pripojenie k MCP serverom
- **Serverový rámec**: Na vytváranie vlastných MCP serverov
- **Spracovanie protokolu**: Na riadenie komunikácie
- **Integráciu nástrojov**: Na vykonávanie externých funkcií

## Praktická implementácia: Phi-4 MCP klient

Pozrime sa na implementáciu v reálnom svete pomocou mini modelu Phi-4 od Microsoftu integrovaného s MCP schopnosťami.

### Architektúra systému

Implementácia nasleduje vrstvenú architektúru:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Hlavné komponenty

#### 1. MCP klientské triedy

**BaseMCPClient**: Abstraktný základ poskytujúci spoločnú funkcionalitu
- Asynchrónny protokol správcu kontextu
- Definícia štandardného rozhrania
- Správa zdrojov

**Phi4MiniMCPClient**: Implementácia založená na STDIO
- Lokálna komunikácia procesov
- Spracovanie štandardného vstupu/výstupu
- Riadenie podprocesov

**Phi4MiniSSEMCPClient**: Implementácia Server-Sent Events
- HTTP streamová komunikácia
- Spracovanie udalostí v reálnom čase
- Pripojenie k webovým serverom

#### 2. Integrácia LLM

**OllamaClient**: Lokálne hosťovanie modelu
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Vysoko výkonné podávanie
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Pipeline spracovania nástrojov

Pipeline spracovania nástrojov transformuje MCP nástroje do formátov kompatibilných s jazykovými modelmi:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Začíname: Návod krok za krokom

### Krok 1: Nastavenie prostredia

Nainštalujte požadované závislosti:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Krok 2: Základná konfigurácia

Nastavte svoje environmentálne premenné:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Krok 3: Spustenie prvého MCP klienta

**Základné nastavenie Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Použitie vLLM backendu:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Pripojenie Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Vlastný MCP server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Krok 4: Programové použitie

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Pokročilé funkcie

### Podpora viacerých backendov

Implementácia podporuje backendy Ollama aj vLLM, čo vám umožňuje vybrať si podľa vašich požiadaviek:

- **Ollama**: Lepšie pre lokálny vývoj a testovanie
- **vLLM**: Optimalizované pre produkciu a scenáre s vysokou priepustnosťou

### Flexibilné komunikačné protokoly

Podporované sú dva režimy pripojenia:

**STDIO režim**: Priama komunikácia procesov
- Nižšia latencia
- Vhodné pre lokálne nástroje
- Jednoduché nastavenie

**SSE režim**: Streamovanie založené na HTTP
- Schopnosť práce v sieti
- Lepšie pre distribuované systémy
- Aktualizácie v reálnom čase

### Schopnosti integrácie nástrojov

Systém dokáže integrovať rôzne nástroje:
- Webová automatizácia (Playwright)
- Operácie so súbormi
- Interakcie s API
- Systémové príkazy
- Vlastné funkcie

## Spracovanie chýb a osvedčené postupy

### Komplexné spracovanie chýb

Implementácia zahŕňa robustné spracovanie chýb pre:

**Chyby pripojenia:**
- Zlyhania MCP servera
- Časové limity siete
- Problémy s pripojením

**Chyby vykonávania nástrojov:**
- Chýbajúce nástroje
- Validácia parametrov
- Zlyhania vykonávania

**Chyby spracovania odpovedí:**
- Problémy s parsovaním JSON
- Nekonzistencie formátu
- Anomálie odpovedí LLM

### Osvedčené postupy

1. **Správa zdrojov**: Používajte asynchrónnych správcov kontextu
2. **Spracovanie chýb**: Implementujte komplexné bloky try-catch
3. **Logovanie**: Aktivujte vhodné úrovne logovania
4. **Bezpečnosť**: Validujte vstupy a sanitizujte výstupy
5. **Výkon**: Používajte pooling pripojení a caching

## Aplikácie v reálnom svete

### Webová automatizácia
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Spracovanie dát
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrácia API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimalizácia výkonu

### Správa pamäte
- Efektívne spracovanie histórie správ
- Správne čistenie zdrojov
- Pooling pripojení

### Optimalizácia siete
- Asynchrónne HTTP operácie
- Konfigurovateľné časové limity
- Plynulé zotavenie po chybách

### Súbežné spracovanie
- Nezablokované I/O
- Paralelné vykonávanie nástrojov
- Efektívne asynchrónne vzory

## Bezpečnostné úvahy

### Ochrana dát
- Bezpečná správa API kľúčov
- Validácia vstupov
- Sanitizácia výstupov

### Sieťová bezpečnosť
- Podpora HTTPS
- Predvolené lokálne endpointy
- Bezpečné spracovanie tokenov

### Bezpečnosť vykonávania
- Filtrovanie nástrojov
- Sandboxované prostredia
- Audit logovanie

## Záver

SLMs integrované s MCP predstavujú posun paradigmy vo vývoji AI aplikácií. Kombináciou efektívnosti malých modelov s výkonom externých nástrojov môžu vývojári vytvárať inteligentné systémy, ktoré sú zároveň efektívne z hľadiska zdrojov a vysoko schopné.

Implementácia Phi-4 MCP klienta demonštruje, ako možno túto integráciu dosiahnuť v praxi, poskytujúc pevný základ pre budovanie sofistikovaných aplikácií poháňaných AI.

Kľúčové poznatky:
- MCP prekonáva priepasť medzi jazykovými modelmi a externými systémami
- SLMs ponúkajú efektívnosť bez obetovania schopností, keď sú rozšírené o nástroje
- Modulárna architektúra umožňuje jednoduché rozšírenie a prispôsobenie
- Správne spracovanie chýb a bezpečnostné opatrenia sú nevyhnutné pre produkčné použitie

Tento návod poskytuje základ pre budovanie vlastných aplikácií podporujúcich SLM a MCP, otvárajúc možnosti pre automatizáciu, spracovanie dát a integráciu inteligentných systémov.

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za žiadne nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.