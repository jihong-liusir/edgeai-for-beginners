<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T23:38:31+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sk"
}
-->
# Sekcia 3: Microsoft Olive Optimization Suite

## Obsah
1. [Úvod](../../../Module04)
2. [Čo je Microsoft Olive?](../../../Module04)
3. [Inštalácia](../../../Module04)
4. [Rýchly sprievodca](../../../Module04)
5. [Príklad: Konverzia Qwen3 na ONNX INT4](../../../Module04)
6. [Pokročilé použitie](../../../Module04)
7. [Najlepšie postupy](../../../Module04)
8. [Riešenie problémov](../../../Module04)
9. [Dodatočné zdroje](../../../Module04)

## Úvod

Microsoft Olive je výkonný a jednoduchý nástroj na optimalizáciu modelov, ktorý je prispôsobený hardvéru. Uľahčuje proces optimalizácie modelov strojového učenia pre nasadenie na rôznych hardvérových platformách. Či už cielite na CPU, GPU alebo špecializované AI akcelerátory, Olive vám pomôže dosiahnuť optimálny výkon pri zachovaní presnosti modelu.

## Čo je Microsoft Olive?

Olive je jednoduchý nástroj na optimalizáciu modelov prispôsobený hardvéru, ktorý kombinuje špičkové techniky v oblasti kompresie, optimalizácie a kompilácie modelov. Funguje s ONNX Runtime ako komplexné riešenie pre optimalizáciu inferencie.

### Kľúčové vlastnosti

- **Optimalizácia prispôsobená hardvéru**: Automaticky vyberá najlepšie techniky optimalizácie pre váš cieľový hardvér
- **Viac ako 40 zabudovaných optimalizačných komponentov**: Zahŕňa kompresiu modelov, kvantizáciu, optimalizáciu grafov a ďalšie
- **Jednoduché CLI rozhranie**: Jednoduché príkazy pre bežné úlohy optimalizácie
- **Podpora viacerých rámcov**: Funguje s PyTorch, Hugging Face modelmi a ONNX
- **Podpora populárnych modelov**: Olive dokáže automaticky optimalizovať populárne architektúry modelov ako Llama, Phi, Qwen, Gemma a ďalšie

### Výhody

- **Zníženie času vývoja**: Nie je potrebné manuálne experimentovať s rôznymi technikami optimalizácie
- **Zvýšenie výkonu**: Výrazné zlepšenie rýchlosti (až 6x v niektorých prípadoch)
- **Nasadenie na rôznych platformách**: Optimalizované modely fungujú na rôznych hardvéroch a operačných systémoch
- **Zachovanie presnosti**: Optimalizácie zachovávajú kvalitu modelu pri zlepšení výkonu

## Inštalácia

### Predpoklady

- Python 3.8 alebo vyšší
- Správca balíkov pip
- Virtuálne prostredie (odporúčané)

### Základná inštalácia

Vytvorte a aktivujte virtuálne prostredie:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Nainštalujte Olive s funkciami automatickej optimalizácie:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Voliteľné závislosti

Olive ponúka rôzne voliteľné závislosti pre dodatočné funkcie:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Overenie inštalácie

```bash
olive --help
```

Ak je inštalácia úspešná, mali by ste vidieť pomocnú správu Olive CLI.

## Rýchly sprievodca

### Vaša prvá optimalizácia

Optimalizujme malý jazykový model pomocou funkcie automatickej optimalizácie Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Čo tento príkaz robí

Proces optimalizácie zahŕňa: získanie modelu z lokálnej cache, zachytenie ONNX grafu a uloženie váh do ONNX dátového súboru, optimalizáciu ONNX grafu a kvantizáciu modelu na int4 pomocou metódy RTN.

### Vysvetlenie parametrov príkazu

- `--model_name_or_path`: Identifikátor modelu Hugging Face alebo lokálna cesta
- `--output_path`: Adresár, kde bude uložený optimalizovaný model
- `--device`: Cieľové zariadenie (cpu, gpu)
- `--provider`: Poskytovateľ vykonávania (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Použitie ONNX Runtime Generate AI pre inferenciu
- `--precision`: Presnosť kvantizácie (int4, int8, fp16)
- `--log_level`: Verbóznosť logovania (0=minimálna, 1=podrobná)

## Príklad: Konverzia Qwen3 na ONNX INT4

Na základe poskytnutého príkladu Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), tu je postup optimalizácie modelu Qwen3:

### Krok 1: Stiahnutie modelu (voliteľné)

Na minimalizáciu času sťahovania cacheujte iba potrebné súbory:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Optimalizácia modelu Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Testovanie optimalizovaného modelu

Vytvorte jednoduchý Python skript na testovanie vášho optimalizovaného modelu:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Štruktúra výstupu

Po optimalizácii bude váš výstupný adresár obsahovať:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Pokročilé použitie

### Konfiguračné súbory

Pre komplexnejšie pracovné postupy optimalizácie môžete použiť JSON konfiguračné súbory:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Spustenie s konfiguráciou:

```bash
olive run --config config.json
```

### Optimalizácia pre GPU

Pre optimalizáciu CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pre DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Jemné doladenie s Olive

Olive podporuje aj jemné doladenie modelov:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najlepšie postupy

### 1. Výber modelu
- Začnite s menšími modelmi na testovanie (napr. 0.5B-7B parametrov)
- Uistite sa, že architektúra vášho cieľového modelu je podporovaná Olive

### 2. Hardvérové úvahy
- Prispôsobte cieľ optimalizácie vášmu nasadzovaciemu hardvéru
- Použite optimalizáciu pre GPU, ak máte hardvér kompatibilný s CUDA
- Zvážte DirectML pre Windows zariadenia s integrovanou grafikou

### 3. Výber presnosti
- **INT4**: Maximálna kompresia, mierna strata presnosti
- **INT8**: Dobrá rovnováha medzi veľkosťou a presnosťou
- **FP16**: Minimálna strata presnosti, stredné zníženie veľkosti

### 4. Testovanie a validácia
- Vždy testujte optimalizované modely s vašimi konkrétnymi prípadmi použitia
- Porovnajte výkonnostné metriky (latencia, priepustnosť, presnosť)
- Použite reprezentatívne vstupné dáta na hodnotenie

### 5. Iteratívna optimalizácia
- Začnite s automatickou optimalizáciou pre rýchle výsledky
- Použite konfiguračné súbory na jemné ovládanie
- Experimentujte s rôznymi optimalizačnými krokmi

## Riešenie problémov

### Bežné problémy

#### 1. Problémy s inštaláciou
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problémy s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problémy s pamäťou
- Použite menšie veľkosti batchov počas optimalizácie
- Skúste kvantizáciu s vyššou presnosťou najskôr (int8 namiesto int4)
- Uistite sa, že máte dostatok miesta na disku pre cache modelov

#### 4. Chyby pri načítaní modelu
- Overte cestu k modelu a prístupové oprávnenia
- Skontrolujte, či model vyžaduje `trust_remote_code=True`
- Uistite sa, že všetky potrebné súbory modelu sú stiahnuté

### Získanie pomoci

- **Dokumentácia**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Príklady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Dodatočné zdroje

### Oficiálne odkazy
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokumentácia**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Príklad**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Príklady od komunity
- **Jupyter Notebooks**: Dostupné v Olive GitHub repozitári — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Rozšírenie**: Prehľad AI Toolkit pre VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogové príspevky**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Súvisiace nástroje
- **ONNX Runtime**: Vysoko výkonný inferenčný engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Zdroj mnohých kompatibilných modelov — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloudové pracovné postupy optimalizácie — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Čo ďalej

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

