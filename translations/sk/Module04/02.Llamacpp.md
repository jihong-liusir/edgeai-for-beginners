<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T18:55:22+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "sk"
}
-->
# Sekcia 2: Príručka implementácie Llama.cpp

## Obsah
1. [Úvod](../../../Module04)
2. [Čo je Llama.cpp?](../../../Module04)
3. [Inštalácia](../../../Module04)
4. [Kompilácia zo zdroja](../../../Module04)
5. [Kvantizácia modelu](../../../Module04)
6. [Základné použitie](../../../Module04)
7. [Pokročilé funkcie](../../../Module04)
8. [Integrácia s Pythonom](../../../Module04)
9. [Riešenie problémov](../../../Module04)
10. [Najlepšie postupy](../../../Module04)

## Úvod

Táto komplexná príručka vás prevedie všetkým, čo potrebujete vedieť o Llama.cpp, od základnej inštalácie až po pokročilé scenáre použitia. Llama.cpp je výkonná implementácia v C++, ktorá umožňuje efektívne inferencie veľkých jazykových modelov (LLM) s minimálnym nastavením a vynikajúcim výkonom na rôznych hardvérových konfiguráciách.

## Čo je Llama.cpp?

Llama.cpp je framework na inferenciu LLM napísaný v C/C++, ktorý umožňuje spúšťanie veľkých jazykových modelov lokálne s minimálnym nastavením a špičkovým výkonom na širokej škále hardvéru. Kľúčové vlastnosti zahŕňajú:

### Hlavné vlastnosti
- **Čistá implementácia v C/C++** bez závislostí
- **Kompatibilita naprieč platformami** (Windows, macOS, Linux)
- **Optimalizácia hardvéru** pre rôzne architektúry
- **Podpora kvantizácie** (1,5-bitová až 8-bitová kvantizácia celých čísel)
- **Podpora akcelerácie na CPU a GPU**
- **Efektívne využitie pamäte** pre obmedzené prostredia

### Výhody
- Efektívne funguje na CPU bez potreby špecializovaného hardvéru
- Podporuje viacero GPU backendov (CUDA, Metal, OpenCL, Vulkan)
- Ľahký a prenosný
- Apple Silicon je prioritou - optimalizovaný cez ARM NEON, Accelerate a Metal frameworky
- Podporuje rôzne úrovne kvantizácie na zníženie využitia pamäte

## Inštalácia

### Metóda 1: Predkompilované binárne súbory (odporúčané pre začiatočníkov)

#### Stiahnutie z GitHub Releases
1. Navštívte [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Stiahnite si vhodný binárny súbor pre váš systém:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` pre Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` pre macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` pre Linux

3. Rozbaľte archív a pridajte adresár do systémovej PATH

#### Použitie správcov balíkov

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (rôzne distribúcie):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metóda 2: Python balík (llama-cpp-python)

#### Základná inštalácia
```bash
pip install llama-cpp-python
```

#### S hardvérovou akceleráciou
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Kompilácia zo zdroja

### Predpoklady

**Systémové požiadavky:**
- Kompilátor C++ (GCC, Clang alebo MSVC)
- CMake (verzia 3.14 alebo vyššia)
- Git
- Nástroje na kompiláciu pre vašu platformu

**Inštalácia predpokladov:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Nainštalujte Visual Studio 2022 s nástrojmi na vývoj v C++
- Nainštalujte CMake z oficiálnej webovej stránky
- Nainštalujte Git

### Základný proces kompilácie

1. **Naklonujte repozitár:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurujte kompiláciu:**
```bash
cmake -B build
```

3. **Skopírujte projekt:**
```bash
cmake --build build --config Release
```

Pre rýchlejšiu kompiláciu použite paralelné úlohy:
```bash
cmake --build build --config Release -j 8
```

### Kompilácie špecifické pre hardvér

#### Podpora CUDA (NVIDIA GPU)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Podpora Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Podpora OpenBLAS (optimalizácia CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Podpora Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Pokročilé možnosti kompilácie

#### Debug Build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### S ďalšími funkciami
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kvantizácia modelu

### Pochopenie formátu GGUF

GGUF (Generalized GGML Unified Format) je optimalizovaný formát súborov navrhnutý na efektívne spúšťanie veľkých jazykových modelov pomocou Llama.cpp a iných frameworkov. Poskytuje:

- Štandardizované ukladanie váh modelu
- Zlepšenú kompatibilitu naprieč platformami
- Zvýšený výkon
- Efektívne spracovanie metadát

### Typy kvantizácie

Llama.cpp podporuje rôzne úrovne kvantizácie:

| Typ | Bity | Popis | Použitie |
|-----|------|-------|----------|
| F16 | 16 | Polovičná presnosť | Vysoká kvalita, veľká pamäť |
| Q8_0 | 8 | 8-bitová kvantizácia | Dobrá rovnováha |
| Q4_0 | 4 | 4-bitová kvantizácia | Stredná kvalita, menšia veľkosť |
| Q2_K | 2 | 2-bitová kvantizácia | Najmenšia veľkosť, nižšia kvalita |

### Konverzia modelov

#### Z PyTorch na GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Priame stiahnutie z Hugging Face
Mnoho modelov je dostupných vo formáte GGUF na Hugging Face:
- Vyhľadajte modely s názvom "GGUF"
- Stiahnite si vhodnú úroveň kvantizácie
- Použite priamo s Llama.cpp

## Základné použitie

### Rozhranie príkazového riadku

#### Jednoduchá generácia textu
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Použitie modelov z Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Serverový režim
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Bežné parametre

| Parameter | Popis | Príklad |
|-----------|-------|---------|
| `-m` | Cesta k súboru modelu | `-m model.gguf` |
| `-p` | Text promptu | `-p "Hello world"` |
| `-n` | Počet generovaných tokenov | `-n 100` |
| `-c` | Veľkosť kontextu | `-c 4096` |
| `-t` | Počet vlákien | `-t 8` |
| `-ngl` | GPU vrstvy | `-ngl 32` |
| `-temp` | Teplota | `-temp 0.7` |

### Interaktívny režim

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Pokročilé funkcie

### Serverové API

#### Spustenie servera
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Použitie API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optimalizácia výkonu

#### Správa pamäte
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multithreading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Akcelerácia GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integrácia s Pythonom

### Základné použitie s llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chat rozhranie

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streamovanie odpovedí

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integrácia s LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Riešenie problémov

### Bežné problémy a riešenia

#### Chyby pri kompilácii

**Problém: CMake nebol nájdený**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problém: Kompilátor nebol nájdený**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Problémy pri spustení

**Problém: Načítanie modelu zlyhalo**
- Skontrolujte cestu k súboru modelu
- Overte oprávnenia súboru
- Uistite sa, že máte dostatok RAM
- Vyskúšajte rôzne úrovne kvantizácie

**Problém: Slabý výkon**
- Aktivujte hardvérovú akceleráciu
- Zvýšte počet vlákien
- Použite vhodnú kvantizáciu
- Skontrolujte využitie pamäte GPU

#### Problémy s pamäťou

**Problém: Nedostatok pamäte**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problémy špecifické pre platformu

#### Windows
- Použite kompilátor MinGW alebo Visual Studio
- Uistite sa, že PATH je správne nastavený
- Skontrolujte, či antivírus nezasahuje

#### macOS
- Aktivujte Metal pre Apple Silicon
- Použite Rosetta 2 pre kompatibilitu, ak je to potrebné
- Skontrolujte nástroje príkazového riadku Xcode

#### Linux
- Nainštalujte vývojové balíky
- Skontrolujte verzie ovládačov GPU
- Overte inštaláciu CUDA toolkit

## Najlepšie postupy

### Výber modelu
1. **Vyberte vhodnú kvantizáciu** podľa vášho hardvéru
2. **Zvážte veľkosť modelu** vs. kompromis kvality
3. **Testujte rôzne modely** pre váš konkrétny prípad použitia

### Optimalizácia výkonu
1. **Použite akceleráciu GPU**, ak je dostupná
2. **Optimalizujte počet vlákien** pre váš CPU
3. **Nastavte vhodnú veľkosť kontextu** pre váš prípad použitia
4. **Aktivujte mapovanie pamäte** pre veľké modely

### Nasadenie do produkcie
1. **Použite serverový režim** pre prístup cez API
2. **Implementujte správu chýb**
3. **Monitorujte využitie zdrojov**
4. **Nastavte logovanie a monitoring**

### Vývojový proces
1. **Začnite s menšími modelmi** na testovanie
2. **Použite verzovanie** pre konfigurácie modelov
3. **Dokumentujte svoje konfigurácie**
4. **Testujte na rôznych platformách**

### Bezpečnostné aspekty
1. **Validujte vstupné prompty**
2. **Implementujte obmedzenie rýchlosti**
3. **Zabezpečte API endpointy**
4. **Monitorujte vzory zneužívania**

## Záver

Llama.cpp poskytuje výkonný a efektívny spôsob, ako spúšťať veľké jazykové modely lokálne na rôznych hardvérových konfiguráciách. Či už vyvíjate AI aplikácie, robíte výskum alebo len experimentujete s LLM, tento framework ponúka flexibilitu a výkon potrebný pre širokú škálu prípadov použitia.

Kľúčové poznatky:
- Vyberte metódu inštalácie, ktorá najlepšie vyhovuje vašim potrebám
- Optimalizujte pre vašu konkrétnu hardvérovú konfiguráciu
- Začnite so základným použitím a postupne preskúmajte pokročilé funkcie
- Zvážte použitie Python bindingov pre jednoduchšiu integráciu
- Dodržiavajte najlepšie postupy pri nasadení do produkcie

Pre viac informácií a aktualizácií navštívte [oficiálny repozitár Llama.cpp](https://github.com/ggml-org/llama.cpp) a pozrite si komplexnú dokumentáciu a dostupné zdroje komunity.

## ➡️ Čo ďalej

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za žiadne nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.