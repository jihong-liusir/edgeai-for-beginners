<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-18T18:41:29+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "sk"
}
-->
# Sekcia 3: Praktická príručka implementácie

## Prehľad

Táto komplexná príručka vám pomôže pripraviť sa na kurz EdgeAI, ktorý sa zameriava na vytváranie praktických AI riešení, ktoré efektívne fungujú na edge zariadeniach. Kurz kladie dôraz na praktický vývoj pomocou moderných frameworkov a najmodernejších modelov optimalizovaných pre nasadenie na edge.

## 1. Nastavenie vývojového prostredia

### Programovacie jazyky a frameworky

**Python prostredie**
- **Verzia**: Python 3.10 alebo vyššia (odporúčané: Python 3.11)
- **Správca balíkov**: pip alebo conda
- **Virtuálne prostredie**: Používajte venv alebo conda prostredia na izoláciu
- **Kľúčové knižnice**: Počas kurzu nainštalujeme špecifické EdgeAI knižnice

**Microsoft .NET prostredie**
- **Verzia**: .NET 8 alebo vyššia
- **IDE**: Visual Studio 2022, Visual Studio Code alebo JetBrains Rider
- **SDK**: Uistite sa, že máte nainštalovaný .NET SDK pre vývoj na rôznych platformách

### Vývojové nástroje

**Editory kódu a IDE**
- Visual Studio Code (odporúčané pre vývoj na rôznych platformách)
- PyCharm alebo Visual Studio (pre jazykovo špecifický vývoj)
- Jupyter Notebooks na interaktívny vývoj a prototypovanie

**Verzovanie kódu**
- Git (najnovšia verzia)
- GitHub účet na prístup k repozitárom a spoluprácu

## 2. Hardvérové požiadavky a odporúčania

### Minimálne systémové požiadavky
- **CPU**: Viacjadrový procesor (Intel i5/AMD Ryzen 5 alebo ekvivalent)
- **RAM**: Minimálne 8GB, odporúčané 16GB
- **Úložisko**: 50GB voľného miesta na modely a vývojové nástroje
- **OS**: Windows 10/11, macOS 10.15+ alebo Linux (Ubuntu 20.04+)

### Stratégia výpočtových zdrojov
Kurz je navrhnutý tak, aby bol prístupný na rôznych hardvérových konfiguráciách:

**Lokálny vývoj (zameranie na CPU/NPU)**
- Primárny vývoj bude využívať CPU a NPU akceleráciu
- Vhodné pre väčšinu moderných notebookov a stolných počítačov
- Dôraz na efektívnosť a praktické scenáre nasadenia

**Cloudové GPU zdroje (voliteľné)**
- **Azure Machine Learning**: Na intenzívne tréningy a experimentovanie
- **Google Colab**: Bezplatná verzia dostupná na vzdelávacie účely
- **Kaggle Notebooks**: Alternatívna cloudová platforma na výpočty

### Zváženie edge zariadení
- Porozumenie procesorom založeným na ARM
- Znalosť obmedzení mobilného a IoT hardvéru
- Oboznámenie sa s optimalizáciou spotreby energie

## 3. Hlavné modelové rodiny a zdroje

### Primárne modelové rodiny

**Microsoft Phi-4 Family**
- **Popis**: Kompaktné, efektívne modely navrhnuté pre nasadenie na edge
- **Silné stránky**: Výborný pomer výkonu k veľkosti, optimalizované na úlohy uvažovania
- **Zdroj**: [Phi-4 Collection na Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Použitie**: Generovanie kódu, matematické uvažovanie, všeobecná konverzácia

**Qwen-3 Family**
- **Popis**: Najnovšia generácia multijazyčných modelov od Alibaba
- **Silné stránky**: Silné multijazyčné schopnosti, efektívna architektúra
- **Zdroj**: [Qwen-3 Collection na Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Použitie**: Multijazyčné aplikácie, AI riešenia pre rôzne kultúry

**Google Gemma-3n Family**
- **Popis**: Ľahké modely od Google optimalizované pre nasadenie na edge
- **Silné stránky**: Rýchla inferencia, architektúra priateľská k mobilným zariadeniam
- **Zdroj**: [Gemma-3n Collection na Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Použitie**: Mobilné aplikácie, spracovanie v reálnom čase

### Kritériá výberu modelov
- **Kompromis výkonu a veľkosti**: Porozumenie, kedy zvoliť menšie vs. väčšie modely
- **Optimalizácia pre konkrétne úlohy**: Prispôsobenie modelov konkrétnym prípadom použitia
- **Obmedzenia nasadenia**: Pamäť, latencia a spotreba energie

## 4. Nástroje na kvantizáciu a optimalizáciu

### Llama.cpp Framework
- **Repozitár**: [Llama.cpp na GitHub](https://github.com/ggml-org/llama.cpp)
- **Účel**: Vysoko výkonný inferenčný engine pre LLMs
- **Kľúčové vlastnosti**:
  - Optimalizovaná inferencia na CPU
  - Viacero formátov kvantizácie (Q4, Q5, Q8)
  - Kompatibilita na rôznych platformách
  - Pamäťovo efektívna exekúcia
- **Inštalácia a základné použitie**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repozitár**: [Microsoft Olive na GitHub](https://github.com/microsoft/olive)
- **Účel**: Nástroj na optimalizáciu modelov pre nasadenie na edge
- **Kľúčové vlastnosti**:
  - Automatizované pracovné postupy optimalizácie modelov
  - Optimalizácia zohľadňujúca hardvér
  - Integrácia s ONNX Runtime
  - Nástroje na benchmarking výkonu
- **Inštalácia a základné použitie**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Definovanie modelu a konfigurácie optimalizácie
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Spustenie pracovného postupu optimalizácie
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Uloženie optimalizovaného modelu
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # Inštalácia MLX
  pip install mlx
  
  # Príklad Python skriptu na načítanie a optimalizáciu modelu
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repozitár**: [ONNX Runtime na GitHub](https://github.com/microsoft/onnxruntime)
- **Účel**: Akcelerácia inferencie pre ONNX modely na rôznych platformách
- **Kľúčové vlastnosti**:
  - Optimalizácie špecifické pre hardvér (CPU, GPU, NPU)
  - Grafové optimalizácie pre inferenciu
  - Podpora kvantizácie
  - Podpora rôznych jazykov (Python, C++, C#, JavaScript)
- **Inštalácia a základné použitie**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. Odporúčaná literatúra a zdroje

### Základná dokumentácia
- **ONNX Runtime Dokumentácia**: Porozumenie inferencii na rôznych platformách
- **Hugging Face Transformers Guide**: Načítanie modelov a inferencia
- **Edge AI Design Patterns**: Najlepšie postupy pre nasadenie na edge

### Technické články
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Komunitné zdroje
- **EdgeAI Slack/Discord komunity**: Podpora od kolegov a diskusie
- **GitHub repozitáre**: Príklady implementácií a tutoriály
- **YouTube kanály**: Technické analýzy a tutoriály

## 6. Hodnotenie a overenie

### Predkurzový kontrolný zoznam
- [ ] Nainštalovaný a overený Python 3.10+
- [ ] Nainštalovaný a overený .NET 8+
- [ ] Konfigurované vývojové prostredie
- [ ] Vytvorený Hugging Face účet
- [ ] Základná znalosť cieľových modelových rodín
- [ ] Nainštalované a otestované nástroje na kvantizáciu
- [ ] Splnené hardvérové požiadavky
- [ ] Nastavené cloudové účty (ak sú potrebné)

## Kľúčové vzdelávacie ciele

Na konci tejto príručky budete schopní:

1. Nastaviť kompletné vývojové prostredie pre vývoj EdgeAI aplikácií
2. Nainštalovať a konfigurovať potrebné nástroje a frameworky na optimalizáciu modelov
3. Vybrať vhodné hardvérové a softvérové konfigurácie pre vaše EdgeAI projekty
4. Porozumieť kľúčovým aspektom nasadenia AI modelov na edge zariadeniach
5. Pripraviť váš systém na praktické cvičenia v kurze

## Dodatočné zdroje

### Oficiálna dokumentácia
- **Python Dokumentácia**: Oficiálna dokumentácia jazyka Python
- **Microsoft .NET Dokumentácia**: Oficiálne zdroje pre vývoj v .NET
- **ONNX Runtime Dokumentácia**: Komplexný sprievodca ONNX Runtime
- **TensorFlow Lite Dokumentácia**: Oficiálna dokumentácia TensorFlow Lite

### Vývojové nástroje
- **Visual Studio Code**: Ľahký editor kódu s rozšíreniami pre AI vývoj
- **Jupyter Notebooks**: Interaktívne prostredie na experimentovanie s ML
- **Docker**: Platforma na kontajnerizáciu pre konzistentné vývojové prostredia
- **Git**: Systém na správu verzií kódu

### Vzdelávacie zdroje
- **EdgeAI Výskumné články**: Najnovší akademický výskum o efektívnych modeloch
- **Online kurzy**: Doplnkové vzdelávacie materiály o AI optimalizácii
- **Komunitné fóra**: Platformy na otázky a odpovede o výzvach EdgeAI vývoja
- **Benchmark Datasets**: Štandardné datasety na hodnotenie výkonu modelov

## Výsledky vzdelávania

Po dokončení tejto prípravnej príručky budete:

1. Mať plne konfigurované vývojové prostredie pripravené na vývoj EdgeAI
2. Porozumieť hardvérovým a softvérovým požiadavkám pre rôzne scenáre nasadenia
3. Byť oboznámení s kľúčovými frameworkmi a nástrojmi používanými počas kurzu
4. Vedieť vybrať vhodné modely na základe obmedzení zariadení a požiadaviek
5. Mať základné znalosti o optimalizačných technikách pre nasadenie na edge

## ➡️ Čo ďalej

- [04: EdgeAI Hardvér a nasadenie](04.EdgeDeployment.md)

---

**Upozornenie**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za žiadne nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.