<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:25:10+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "hk"
}
-->
# 第三節：開源模型的探索與管理

## 概覽

本節課程專注於使用 Foundry Local 進行模型探索與管理的實踐操作。您將學習如何列出可用模型、測試不同選項，以及了解基本性能特徵。課程重點在於通過 Foundry CLI 的實際操作幫助您選擇適合您使用場景的模型。

## 學習目標

- 掌握使用 Foundry CLI 進行模型探索與管理的指令
- 理解模型緩存和本地存儲模式
- 學習快速測試和比較不同模型
- 建立模型選擇和基準測試的實用工作流程
- 探索 Foundry Local 提供的日益增長的模型生態系統

## 先決條件

- 完成第一節課程：Foundry Local 入門
- 已安裝並可使用 Foundry Local CLI
- 擁有足夠的存儲空間以下載模型（模型大小範圍從 1GB 到 20GB+）
- 基本了解模型類型及其使用場景

## 概覽

本節課程探討如何將開源模型引入 Foundry Local，並採用“自帶模型”（BYOM）策略。您還將了解 Model Mondays 系列，持續學習和探索模型。

## 第六部分：實作練習

### 練習：模型探索與比較

基於範例 03，創建您自己的模型評估腳本：

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### 您的任務

1. **運行範例 03 腳本**：`samples\03\list_and_bench.cmd`
2. **嘗試不同模型**：至少測試 3 個不同的模型
3. **比較性能**：記錄速度和響應質量的差異
4. **記錄結果**：創建簡單的比較圖表

### 示例比較格式

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## 第七部分：故障排除與最佳實踐

### 常見問題及解決方案

**模型無法啟動：**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**內存不足：**
- 從較小的模型開始（如 `phi-4-mini`）
- 關閉其他應用程式
- 如果經常遇到內存限制，請升級 RAM

**性能緩慢：**
- 確保模型已完全加載（檢查詳細輸出）
- 關閉不必要的背景應用程式
- 考慮使用更快的存儲設備（如 SSD）

### 最佳實踐

1. **從小開始**：使用 `phi-4-mini` 驗證設置
2. **一次運行一個模型**：在啟動新模型前停止之前的模型
3. **監控資源**：密切關注內存使用情況
4. **一致測試**：使用相同的提示進行公平比較
5. **記錄結果**：記錄模型性能以便於您的使用場景

## 第八部分：後續步驟與參考資料

### 為第四節課程做準備

- **第四節課程重點**：優化工具與技術
- **先決條件**：熟悉模型切換和基本性能測試
- **建議**：從本節課程中選出 2-3 個最喜歡的模型

### 附加資源

- **[Foundry Local 文件](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**：官方文件
- **[CLI 參考](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**：完整指令參考
- **[Model Mondays](https://aka.ms/model-mondays)**：每週模型聚焦
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**：社群與問題追蹤
- **[範例 03：模型探索](samples/03/README.md)**：實作範例腳本

### 關鍵要點

✅ **模型探索**：使用 `foundry model list` 探索可用模型  
✅ **快速測試**：使用 `list_and_bench.cmd` 模式進行快速評估  
✅ **性能監控**：基本資源使用和響應時間測量  
✅ **模型選擇**：根據使用場景選擇模型的實用指南  
✅ **緩存管理**：理解存儲和清理程序  

您現在擁有使用 Foundry Local 的簡單 CLI 方法探索、測試和選擇適合 AI 應用的模型的實用技能。

參考資料：
- Foundry Local 文件：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- 編譯 Hugging Face 模型：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays：https://aka.ms/model-mondays
- Foundry Local GitHub：https://github.com/microsoft/Foundry-Local

## 學習目標

- 探索並評估用於本地推理的開源模型
- 在 Foundry Local 中編譯並運行選定的 Hugging Face 模型
- 根據準確性、延遲和資源需求應用模型選擇策略
- 使用緩存和版本管理本地管理模型

## 第一部分：使用 Foundry CLI 進行模型探索

### 基本模型管理指令

Foundry CLI 提供了簡單的指令進行模型探索與管理：

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### 運行您的第一個模型

從流行且經過良好測試的模型開始，以了解性能特徵：

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**注意：** `--verbose` 標誌提供詳細的啟動信息，包括：
- 模型下載進度（首次運行時）
- 內存分配詳情
- 服務綁定信息
- 性能初始化指標

### 理解模型類別

**小型語言模型（SLMs）：**
- `phi-4-mini`：快速、高效，適合一般聊天
- `phi-4`：更強大的版本，推理能力更佳

**中型模型：**
- `qwen2.5-7b`：卓越的推理能力和更長的上下文
- `deepseek-r1-7b`：針對代碼生成進行優化

**大型模型：**
- `llama-3.2`：Meta 最新的開源模型
- `qwen2.5-14b`：企業級推理能力

## 第二部分：快速模型測試與比較

### 範例 03 方法：簡單列出與基準測試

基於範例 03 模式，以下是最小化工作流程：

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### 測試模型性能

模型運行後，使用一致的提示進行測試：

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell 測試替代方案

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## 第三部分：模型緩存與存儲管理

### 理解模型緩存

Foundry Local 自動管理模型下載和緩存：

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### 模型存儲考量

**典型模型大小：**
- `phi-4-mini`：約 2.5 GB
- `qwen2.5-7b`：約 4.1 GB  
- `deepseek-r1-7b`：約 4.3 GB
- `llama-3.2`：約 4.9 GB
- `qwen2.5-14b`：約 8.2 GB

**存儲最佳實踐：**
- 保留 2-3 個模型緩存以便快速切換
- 移除未使用的模型以釋放空間：`foundry cache clean`
- 監控磁碟使用情況，尤其是較小的 SSD
- 考慮模型大小與能力的權衡

### 模型性能監控

模型運行期間，監控系統資源：

**Windows 任務管理器：**
- 觀察內存使用情況（模型保持加載於 RAM 中）
- 監控推理期間的 CPU 使用率
- 檢查初始模型加載期間的磁碟 I/O

**命令行監控：**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## 第四部分：實用模型選擇指南

### 根據使用場景選擇模型

**一般聊天與問答：**
- 起步：`phi-4-mini`（快速、高效）
- 升級：`phi-4`（推理能力更佳）
- 高級：`qwen2.5-7b`（更長的上下文）

**代碼生成：**
- 推薦：`deepseek-r1-7b`
- 替代：`qwen2.5-7b`（同樣適合代碼）

**複雜推理：**
- 最佳：`qwen2.5-7b` 或 `qwen2.5-14b`
- 經濟選擇：`phi-4`

### 硬件需求指南

**最低系統需求：**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**最佳性能推薦：**
- 32GB+ RAM，方便多模型切換
- SSD 存儲，提升模型加載速度
- 現代 CPU，具備良好的單線程性能
- 支援 NPU（Windows 11 Copilot+ PC）加速

### 模型切換工作流程

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## 第五部分：簡單模型基準測試

### 基本性能測試

以下是比較模型性能的簡單方法：

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### 手動質量評估

對每個模型進行一致的提示測試並手動評估：

**測試提示：**
1. "用簡單的語言解釋量子計算。"
2. "寫一個 Python 函數來排序列表。"
3. "遠程工作的優缺點是什麼？"
4. "總結邊緣 AI 的好處。"

**評估標準：**
- **準確性**：信息是否正確？
- **清晰度**：解釋是否易於理解？
- **完整性**：是否全面回答問題？
- **速度**：響應速度如何？

### 資源使用監控

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## 第六部分：後續步驟

- 訂閱 Model Mondays，獲取新模型和技巧：https://aka.ms/model-mondays
- 將發現結果貢獻到您的團隊 `models.json`
- 為第四節課程做準備：比較 LLMs 與 SLMs、本地與雲端推理，以及實作演示

---

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。