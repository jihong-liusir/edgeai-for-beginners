<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2b01d2da38267efa55b48a4a89b5fe3",
  "translation_date": "2025-07-22T05:10:03+00:00",
  "source_file": "Module04/README.md",
  "language_code": "hk"
}
-->
# 第四章：模型格式轉換與量化 - 章節概述

EdgeAI 的興起使得模型格式轉換與量化成為在資源有限的設備上部署高級機器學習功能的必要技術。本章提供完整指南，幫助讀者理解、實施及優化模型以適應邊緣部署場景。

## 📚 章節結構與學習路徑

本章分為四個循序漸進的部分，每部分都建立在前一部分的基礎上，旨在全面理解邊緣計算的模型優化：

---

## [第一部分：模型格式轉換與量化基礎](./01.Introduce.md)

### 🎯 概述
此基礎部分建立了邊緣計算環境中模型優化的理論框架，涵蓋從 1-bit 到 8-bit 精度的量化範圍以及主要的格式轉換策略。

**主要主題：**
- 精度分類框架（超低、低、中等精度）
- GGUF 和 ONNX 格式的優勢及使用案例
- 量化對運行效率和部署靈活性的好處
- 性能基準測試及內存佔用比較

**學習成果：**
- 理解量化範圍及分類
- 識別適合的格式轉換技術
- 學習邊緣部署的高級優化策略

---

## [第二部分：Llama.cpp 實施指南](./02.Llamacpp.md)

### 🎯 概述
全面教程，介紹如何實施 Llama.cpp，一個強大的 C++ 框架，能夠在多種硬件配置上以最少的設置高效進行大型語言模型推理。

**主要主題：**
- 在 Windows、macOS 和 Linux 平台上的安裝
- GGUF 格式轉換及多種量化級別（Q2_K 至 Q8_0）
- 使用 CUDA、Metal、OpenCL 和 Vulkan 進行硬件加速
- Python 集成及生產部署策略

**學習成果：**
- 掌握跨平台安裝及從源碼構建
- 實施模型量化及優化技術
- 使用 REST API 集成部署模型於伺服器模式

---

## [第三部分：Microsoft Olive 優化套件](./03.MicrosoftOlive.md)

### 🎯 概述
探索 Microsoft Olive，一個硬件感知的模型優化工具包，內置超過 40 種優化組件，專為企業級模型部署於多種硬件平台而設計。

**主要主題：**
- 自動優化功能，支持動態及靜態量化
- 硬件感知智能，適用於 CPU、GPU 和 NPU 部署
- 支持流行模型（Llama、Phi、Qwen、Gemma）即時使用
- 與 Azure ML 及生產工作流的企業集成

**學習成果：**
- 利用自動化優化處理多種模型架構
- 實施跨平台部署策略
- 建立企業級優化管道

---

## [第四部分：Apple MLX 框架深入探討](./04.AppleMLX.md)

### 🎯 概述
全面介紹 Apple MLX，一個專為 Apple Silicon 設計的革命性框架，重點在於大型語言模型功能及本地部署的高效性。

**主要主題：**
- 統一內存架構的優勢及 Metal Performance Shaders
- 支持 LLaMA、Mistral、Phi-3、Qwen 和 Code Llama 模型
- LoRA 微調以高效進行模型定制
- Hugging Face 集成及量化支持（4-bit 和 8-bit）

**學習成果：**
- 掌握 Apple Silicon 的優化技術以部署 LLM
- 實施微調及模型定制技術
- 構建具增強隱私功能的企業 AI 應用

---

## 🎯 章節學習成果

完成本章後，讀者將能夠：

### **技術精通**
- 深入理解量化範圍及其實際應用
- 實際操作多種優化框架
- 掌握邊緣計算環境的生產部署技能

### **策略性理解**
- 選擇硬件感知的優化方案
- 在性能取捨上做出明智決策
- 建立企業級部署及監控策略

### **性能基準**

| 框架       | 量化方式 | 內存使用量 | 速度提升 | 使用案例               |
|------------|----------|------------|----------|------------------------|
| Llama.cpp  | Q4_K_M   | ~4GB       | 2-3倍    | 跨平台部署             |
| Olive      | INT4     | 減少 60-75% | 2-6倍    | 企業工作流             |
| MLX        | 4-bit    | ~4GB       | 2-4倍    | Apple Silicon 優化     |

## 🚀 下一步及高級應用

本章提供完整基礎，適用於：
- 為特定領域開發定制模型
- 邊緣 AI 優化研究
- 商業 AI 應用開發
- 大規模企業邊緣 AI 部署

這四個部分的知識為讀者提供全面工具包，幫助其在快速演變的邊緣 AI 模型優化及部署領域中游刃有餘。

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始語言的文件應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。