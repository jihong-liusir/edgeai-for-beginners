<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-07-22T05:19:55+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "hk"
}
-->
# 第 2 節：Llama.cpp 實現指南

## 目錄
1. [簡介](../../../Module04)
2. [什麼是 Llama.cpp？](../../../Module04)
3. [安裝](../../../Module04)
4. [從源碼構建](../../../Module04)
5. [模型量化](../../../Module04)
6. [基本用法](../../../Module04)
7. [進階功能](../../../Module04)
8. [Python 集成](../../../Module04)
9. [故障排除](../../../Module04)
10. [最佳實踐](../../../Module04)

## 簡介

這份全面的教程將帶領你了解有關 Llama.cpp 的所有內容，從基本安裝到進階使用場景。Llama.cpp 是一個強大的 C++ 實現，能夠以最小的設置和卓越的性能在各種硬件配置上高效地推理大型語言模型（LLMs）。

## 什麼是 Llama.cpp？

Llama.cpp 是一個用 C/C++ 編寫的 LLM 推理框架，能夠在本地運行大型語言模型，設置簡單，並在多種硬件上提供最先進的性能。其主要特點包括：

### 核心功能
- **純 C/C++ 實現**，無需依賴其他庫
- **跨平台兼容性**（Windows、macOS、Linux）
- **針對多種架構的硬件優化**
- **支持量化**（1.5 位到 8 位整數量化）
- **支持 CPU 和 GPU 加速**
- **適用於資源受限環境的內存效率**

### 優勢
- 在無需專用硬件的情況下高效運行於 CPU
- 支持多種 GPU 後端（CUDA、Metal、OpenCL、Vulkan）
- 輕量且可攜帶
- Apple Silicon 是一等公民——通過 ARM NEON、Accelerate 和 Metal 框架進行優化
- 支持多種量化級別以減少內存使用

## 安裝

### 方法 1：預編譯二進制文件（建議初學者使用）

#### 從 GitHub Releases 下載
1. 訪問 [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. 下載適合你系統的二進制文件：
   - Windows：`llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS：`llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux：`llama-<version>-bin-linux-<feature>-<arch>.zip`

3. 解壓文件並將目錄添加到系統的 PATH 中

#### 使用包管理器

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (多種發行版):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### 方法 2：Python 包 (llama-cpp-python)

#### 基本安裝
```bash
pip install llama-cpp-python
```

#### 啟用硬件加速
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## 從源碼構建

### 先決條件

**系統要求：**
- C++ 編譯器（GCC、Clang 或 MSVC）
- CMake（版本 3.14 或更高）
- Git
- 適用於你平台的構建工具

**安裝先決條件：**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- 安裝 Visual Studio 2022 並啟用 C++ 開發工具
- 從官方網站安裝 CMake
- 安裝 Git

### 基本構建過程

1. **克隆代碼庫：**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **配置構建：**
```bash
cmake -B build
```

3. **構建項目：**
```bash
cmake --build build --config Release
```

為了加快編譯速度，可以使用並行作業：
```bash
cmake --build build --config Release -j 8
```

### 硬件特定構建

#### 支持 CUDA（NVIDIA GPU）
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### 支持 Metal（Apple Silicon）
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### 支持 OpenBLAS（CPU 優化）
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### 支持 Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### 進階構建選項

#### 調試構建
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### 啟用額外功能
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## 模型量化

### 了解 GGUF 格式

GGUF（通用 GGML 統一格式）是一種優化的文件格式，專為使用 Llama.cpp 和其他框架高效運行大型語言模型而設計。其優勢包括：

- 標準化的模型權重存儲
- 提高跨平台兼容性
- 增強性能
- 高效的元數據處理

### 量化類型

Llama.cpp 支持多種量化級別：

| 類型 | 位數 | 描述 | 使用場景 |
|------|------|------|----------|
| F16 | 16 | 半精度 | 高質量，大內存 |
| Q8_0 | 8 | 8 位量化 | 良好平衡 |
| Q4_0 | 4 | 4 位量化 | 中等質量，較小體積 |
| Q2_K | 2 | 2 位量化 | 最小體積，較低質量 |

### 轉換模型

#### 從 PyTorch 轉換為 GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### 從 Hugging Face 直接下載
許多模型已以 GGUF 格式提供於 Hugging Face：
- 搜索名稱中帶有 "GGUF" 的模型
- 下載適合的量化級別
- 可直接與 Llama.cpp 一起使用

## 基本用法

### 命令行界面

#### 簡單文本生成
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### 使用 Hugging Face 的模型
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### 服務器模式
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### 常用參數

| 參數 | 描述 | 示例 |
|------|------|------|
| `-m` | 模型文件路徑 | `-m model.gguf` |
| `-p` | 提示文本 | `-p "Hello world"` |
| `-n` | 生成的 token 數量 | `-n 100` |
| `-c` | 上下文大小 | `-c 4096` |
| `-t` | 線程數量 | `-t 8` |
| `-ngl` | GPU 層數 | `-ngl 32` |
| `-temp` | 溫度 | `-temp 0.7` |

### 交互模式

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## 進階功能

### 服務器 API

#### 啟動服務器
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API 用法
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### 性能優化

#### 內存管理
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### 多線程
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU 加速
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python 集成

### 使用 llama-cpp-python 的基本用法

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### 聊天界面

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### 流式響應

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### 與 LangChain 集成

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## 故障排除

### 常見問題及解決方案

#### 構建錯誤

**問題：找不到 CMake**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**問題：找不到編譯器**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### 運行時問題

**問題：模型加載失敗**
- 驗證模型文件路徑
- 檢查文件權限
- 確保有足夠的 RAM
- 嘗試不同的量化級別

**問題：性能不佳**
- 啟用硬件加速
- 增加線程數量
- 使用適當的量化
- 檢查 GPU 內存使用情況

#### 內存問題

**問題：內存不足**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### 平台特定問題

#### Windows
- 使用 MinGW 或 Visual Studio 編譯器
- 確保正確配置 PATH
- 檢查防病毒軟件是否干擾

#### macOS
- 為 Apple Silicon 啟用 Metal
- 如有需要，使用 Rosetta 2 提高兼容性
- 檢查 Xcode 命令行工具

#### Linux
- 安裝開發包
- 檢查 GPU 驅動版本
- 驗證 CUDA 工具包安裝

## 最佳實踐

### 模型選擇
1. **根據硬件選擇適當的量化級別**
2. **考慮模型大小與質量的權衡**
3. **針對具體用例測試不同模型**

### 性能優化
1. **在可用時使用 GPU 加速**
2. **為 CPU 優化線程數量**
3. **根據用例設置適當的上下文大小**
4. **為大型模型啟用內存映射**

### 生產部署
1. **使用服務器模式提供 API 訪問**
2. **實現適當的錯誤處理**
3. **監控資源使用情況**
4. **設置日誌記錄和監控**

### 開發工作流程
1. **從較小的模型開始測試**
2. **使用版本控制管理模型配置**
3. **記錄你的配置**
4. **在不同平台上進行測試**

### 安全考量
1. **驗證輸入提示**
2. **實施速率限制**
3. **保護 API 端點**
4. **監控濫用模式**

## 結論

Llama.cpp 提供了一種強大且高效的方式，能夠在多種硬件配置上本地運行大型語言模型。無論你是在開發 AI 應用、進行研究，還是僅僅對 LLMs 進行實驗，這個框架都能為各種用例提供所需的靈活性和性能。

關鍵要點：
- 選擇最適合你需求的安裝方法
- 根據你的硬件配置進行優化
- 從基本用法開始，逐步探索進階功能
- 考慮使用 Python 綁定以簡化集成
- 遵循生產部署的最佳實踐

欲了解更多信息和更新，請訪問 [官方 Llama.cpp 資源庫](https://github.com/ggml-org/llama.cpp)，並參考全面的文檔和社區資源。

## ➡️ 下一步

- [03: Microsoft Olive 優化套件](./03.MicrosoftOlive.md)

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始語言的文件應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。