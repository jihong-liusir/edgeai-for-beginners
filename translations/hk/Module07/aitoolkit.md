<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "65a22ed38b95f334dd8a893bf2c55806",
  "translation_date": "2025-10-02T11:29:39+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "hk"
}
-->
# AI Toolkit for Visual Studio Code - Edge AI 開發指南

## 簡介

歡迎使用 AI Toolkit for Visual Studio Code 的全面指南，專為邊緣 AI 開發而設計。隨著人工智能從集中式雲端計算轉向分散式邊緣設備，開發者需要強大的整合工具來應對邊緣部署的獨特挑戰——包括資源限制和離線操作需求。

AI Toolkit for Visual Studio Code 彌合了這一差距，提供了一個完整的開發環境，專門用於構建、測試和優化能在邊緣設備上高效運行的 AI 應用程序。無論您是為 IoT 感測器、移動設備、嵌入式系統還是邊緣伺服器開發，這款工具包都能在熟悉的 VS Code 環境中簡化您的整個開發工作流程。

本指南將帶您了解如何在邊緣 AI 項目中使用 AI Toolkit 的基本概念、工具和最佳實踐，從初始模型選擇到生產部署。

## 概述

AI Toolkit for Visual Studio Code 是一款強大的擴展工具，能簡化代理開發和 AI 應用程序創建。該工具包提供了全面的功能，支持探索、評估和部署來自多個提供商（包括 Anthropic、OpenAI、GitHub、Google）的 AI 模型，同時支持使用 ONNX 和 Ollama 進行本地模型執行。

AI Toolkit 的獨特之處在於其涵蓋整個 AI 開發生命周期的全面方法。與傳統的 AI 開發工具僅專注於單一方面不同，AI Toolkit 提供了一個整合環境，涵蓋模型發現、實驗、代理開發、評估和部署——全部在熟悉的 VS Code 環境中完成。

該平台專為快速原型設計和生產部署而設計，擁有提示生成、快速入門、無縫 MCP（Model Context Protocol）工具整合以及廣泛的評估功能等特性。對於邊緣 AI 開發而言，這意味著您可以高效地開發、測試和優化 AI 應用程序，以適應邊緣部署場景，同時保持完整的開發工作流程。

## 學習目標

完成本指南後，您將能夠：

### 核心能力
- **安裝和配置** AI Toolkit for Visual Studio Code，以支持邊緣 AI 開發工作流程
- **導航和使用** AI Toolkit 界面，包括模型目錄、Playground 和代理構建器
- **選擇和評估** 適合邊緣部署的 AI 模型，基於性能和資源限制
- **轉換和優化** 模型，使用 ONNX 格式和量化技術以適應邊緣設備

### 邊緣 AI 開發技能
- **設計和實現** 使用整合開發環境的邊緣 AI 應用程序
- **在邊緣條件下進行模型測試**，使用本地推理和資源監控
- **創建和定制** 為邊緣部署場景優化的 AI 代理
- **使用與邊緣計算相關的指標**（延遲、內存使用、準確性）評估模型性能

### 優化和部署
- **應用量化和剪枝技術**，在保持可接受性能的同時減少模型大小
- **優化模型**，以適應特定的邊緣硬件平台，包括 CPU、GPU 和 NPU 加速
- **實施邊緣 AI 開發的最佳實踐**，包括資源管理和備援策略
- **準備模型和應用程序**，以便在邊緣設備上進行生產部署

### 高級邊緣 AI 概念
- **整合邊緣 AI 框架**，包括 ONNX Runtime、Windows ML 和 TensorFlow Lite
- **實現多模型架構**和聯邦學習場景以適應邊緣環境
- **排查常見邊緣 AI 問題**，包括內存限制、推理速度和硬件兼容性
- **設計監控和日誌策略**，以支持生產中的邊緣 AI 應用程序

### 實際應用
- **構建端到端邊緣 AI 解決方案**，從模型選擇到部署
- **展示熟練掌握**邊緣特定的開發工作流程和優化技術
- **將所學概念應用於**真實世界的邊緣 AI 用例，包括 IoT、移動和嵌入式應用
- **評估和比較**不同的邊緣 AI 部署策略及其權衡

## 邊緣 AI 開發的主要功能

### 1. 模型目錄和探索
- **多提供商支持**：瀏覽並訪問來自 Anthropic、OpenAI、GitHub、Google 等提供商的 AI 模型
- **本地模型整合**：簡化 ONNX 和 Ollama 模型的探索，以支持邊緣部署
- **GitHub 模型**：直接整合 GitHub 的模型托管，方便訪問
- **模型比較**：並排比較模型，以找到適合邊緣設備限制的最佳平衡

### 2. 互動式 Playground
- **互動測試環境**：在受控環境中快速試驗模型功能
- **多模態支持**：使用圖像、文本和其他典型邊緣場景的輸入進行測試
- **即時試驗**：即時反饋模型響應和性能
- **參數優化**：根據邊緣部署需求微調模型參數

### 3. 提示（代理）構建器
- **自然語言生成**：使用自然語言描述生成入門提示
- **迭代改進**：根據模型響應和性能改進提示
- **任務分解**：通過提示鏈接和結構化輸出分解複雜任務
- **變數支持**：在提示中使用變數以實現動態代理行為
- **生產代碼生成**：生成生產就緒代碼以快速開發應用程序

### 4. 批量運行和評估
- **多模型測試**：同時在選定模型上執行多個提示
- **高效大規模測試**：高效測試各種輸入和配置
- **自定義測試案例**：使用測試案例運行代理以驗證功能
- **性能比較**：比較不同模型和配置的結果

### 5. 使用數據集進行模型評估
- **標準指標**：使用內置評估器（F1 分數、相關性、相似性、一致性）測試 AI 模型
- **自定義評估器**：為特定用例創建自己的評估指標
- **數據集整合**：使用全面的數據集測試模型
- **性能測量**：量化模型性能以支持邊緣部署決策

### 6. 微調功能
- **模型定制**：根據特定用例和領域定制模型
- **專業化適配**：使模型適配專業領域和需求
- **邊緣優化**：專門為邊緣部署限制微調模型
- **領域特定訓練**：創建針對特定邊緣用例的模型

### 7. MCP 工具整合
- **外部工具連接**：通過 Model Context Protocol 伺服器連接代理到外部工具
- **實際操作**：使代理能查詢數據庫、訪問 API 或執行自定義邏輯
- **現有 MCP 伺服器**：使用命令（stdio）或 HTTP（伺服器推送事件）協議的工具
- **自定義 MCP 開發**：使用代理構建器測試新 MCP 伺服器的構建和搭建

### 8. 代理開發和測試
- **函數調用支持**：使代理能動態調用外部函數
- **即時整合測試**：通過即時運行和工具使用測試整合
- **代理版本控制**：代理的版本控制，並能比較評估結果
- **調試和追蹤**：本地追蹤和調試功能支持代理開發

## 邊緣 AI 開發工作流程

### 第一階段：模型探索和選擇
1. **探索模型目錄**：使用模型目錄尋找適合邊緣部署的模型
2. **比較性能**：根據大小、準確性和推理速度評估模型
3. **本地測試**：使用 Ollama 或 ONNX 模型在本地測試，準備邊緣部署
4. **評估資源需求**：確定目標邊緣設備的內存和計算需求

### 第二階段：模型優化
1. **轉換為 ONNX**：將選定模型轉換為 ONNX 格式以支持邊緣兼容性
2. **應用量化**：通過 INT8 或 INT4 量化減少模型大小
3. **硬件優化**：針對目標邊緣硬件（ARM、x86、專用加速器）進行優化
4. **性能驗證**：驗證優化後的模型是否保持可接受的準確性

### 第三階段：應用程序開發
1. **代理設計**：使用代理構建器創建邊緣優化的 AI 代理
2. **提示工程**：開發能有效與較小邊緣模型配合的提示
3. **整合測試**：在模擬邊緣條件下測試代理
4. **代碼生成**：生成針對邊緣部署優化的生產代碼

### 第四階段：評估和測試
1. **批量評估**：測試多種配置以找到最佳邊緣設置
2. **性能分析**：分析推理速度、內存使用和準確性
3. **邊緣模擬**：在類似目標邊緣部署環境的條件下測試
4. **壓力測試**：在各種負載條件下評估性能

### 第五階段：部署準備
1. **最終優化**：根據測試結果進行最終優化
2. **部署打包**：打包模型和代碼以支持邊緣部署
3. **文檔編寫**：記錄部署需求和配置
4. **監控設置**：準備邊緣部署的監控和日誌功能

## 邊緣 AI 開發的目標受眾

### 邊緣 AI 開發者
- 構建 AI 驅動的邊緣設備和 IoT 解決方案的應用程序開發者
- 將 AI 功能整合到資源受限設備中的嵌入式系統開發者
- 為智能手機和平板電腦創建設備端 AI 應用程序的移動開發者

### 邊緣 AI 工程師
- 優化模型以支持邊緣部署並管理推理管道的 AI 工程師
- 部署和管理分散式邊緣基礎設施上的 AI 模型的 DevOps 工程師
- 優化 AI 工作負載以適應邊緣硬件限制的性能工程師

### 研究人員和教育工作者
- 開發高效模型和算法以支持邊緣計算的 AI 研究人員
- 教授邊緣 AI 概念並演示優化技術的教育工作者
- 學習邊緣 AI 部署挑戰和解決方案的學生

## 邊緣 AI 用例

### 智能 IoT 設備
- **即時圖像識別**：在 IoT 攝像頭和感測器上部署計算機視覺模型
- **語音處理**：在智能音箱上實現語音識別和自然語言處理
- **預測性維護**：在工業邊緣設備上運行異常檢測模型
- **環境監測**：部署感測器數據分析模型以支持環境應用

### 移動和嵌入式應用
- **設備端翻譯**：實現能離線工作的語言翻譯模型
- **增強現實**：部署即時物體識別和追蹤以支持 AR 應用
- **健康監測**：在可穿戴設備和醫療設備上運行健康分析模型
- **自主系統**：為無人機、機器人和車輛實現決策模型

### 邊緣計算基礎設施
- **邊緣數據中心**：在邊緣數據中心部署 AI 模型以支持低延遲應用
- **CDN 整合**：將 AI 處理能力整合到內容分發網絡中
- **5G 邊緣**：利用 5G 邊緣計算支持 AI 驅動的應用
- **霧計算**：在霧計算環境中實現 AI 處理

## 安裝和設置

### 擴展安裝
直接從 Visual Studio Code Marketplace 安裝 AI Toolkit 擴展：

**擴展 ID**：`ms-windows-ai-studio.windows-ai-studio`

**安裝方法**：
1. **VS Code Marketplace**：在擴展視圖中搜索 "AI Toolkit"
2. **命令行**：`code --install-extension ms-windows-ai-studio.windows-ai-studio`
3. **直接安裝**：從 [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) 下載

### 邊緣 AI 開發的先決條件
- **Visual Studio Code**：建議使用最新版本
- **Python 環境**：Python 3.8+，並安裝所需的 AI 庫
- **ONNX Runtime**（可選）：支持 ONNX 模型推理
- **Ollama**（可選）：支持本地模型服務
- **硬件加速工具**：CUDA、OpenVINO 或平台特定加速器

### 初始配置
1. **擴展激活**：打開 VS Code，確認 AI Toolkit 出現在活動欄中
2. **模型提供商設置**：配置訪問 GitHub、OpenAI、Anthropic 或其他模型提供商
3. **本地環境**：設置 Python 環境並安裝所需的軟件包
4. **硬件加速**：配置 GPU/NPU 加速（如果可用）
5. **MCP 整合**：根據需要設置 Model Context Protocol 伺服器

### 首次設置檢查清單
- [ ] 已安裝並激活 AI Toolkit 擴展
- [ ] 模型目錄可訪問，模型可探索
- [ ] Playground 可用於模型測試
- [ ] 代理構建器可用於提示開發
- [ ] 本地開發環境已配置
- [ ] 硬件加速（如果可用）已正確配置

## 使用 AI Toolkit 入門

### 快速入門指南

我們建議從 GitHub 托管的模型開始，以獲得最流暢的體驗：

1. **安裝**：按照 [安裝指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup) 設置您的設備上的 AI Toolkit
2. **模型探索**：在擴展樹視圖中選擇 **CATALOG > Models**，探索可用模型
3. **GitHub 模型**：從 GitHub 托管的模型開始，以獲得最佳整合
4. **Playground 測試**：從任意模型卡中選擇 **Try in Playground**，開始試驗模型功能

### 邊緣 AI 開發的逐步指南

#### 第一步：模型探索和選擇
1. 在 VS Code 活動欄中打開 AI Toolkit 視圖
2. 瀏覽模型目錄，尋找適合邊緣部署的模型
3. 根據邊緣需求篩選提供商（GitHub、ONNX、Ollama）
4. 使用 **Try in Playground** 即時測試模型功能

#### 第二步：代理開發
1. 使用 **提示（代理）構建器** 創建邊緣優化的 AI 代理
2. 使用自然語言描述生成初始提示
3. 根據模型回應進行迭代和完善提示
4. 整合 MCP 工具以增強代理功能

#### 第三步：測試與評估
1. 使用 **Bulk Run** 測試多個提示在選定模型上的表現
2. 使用測試案例運行代理以驗證功能
3. 使用內建或自定義指標評估準確性和性能
4. 比較不同模型和配置

#### 第四步：微調與優化
1. 為特定邊緣使用案例定制模型
2. 應用領域專屬的微調
3. 優化以滿足邊緣部署限制
4. 對不同代理配置進行版本管理和比較

#### 第五步：部署準備
1. 使用 Agent Builder 生成生產就緒的代碼
2. 設置 MCP 伺服器連接以供生產使用
3. 為邊緣設備準備部署包
4. 配置監控和評估指標

## 邊緣 AI 開發最佳實踐

### 模型選擇
- **大小限制**：選擇適合目標設備內存限制的模型
- **推理速度**：優先選擇推理速度快的模型以支持實時應用
- **準確性權衡**：在模型準確性與資源限制之間取得平衡
- **格式兼容性**：優先選擇 ONNX 或硬件優化格式以支持邊緣部署

### 優化技術
- **量化**：使用 INT8 或 INT4 量化以減小模型大小並提高速度
- **剪枝**：移除不必要的模型參數以減少計算需求
- **知識蒸餾**：創建較小的模型，同時保持大型模型的性能
- **硬件加速**：在可用時利用 NPU、GPU 或專用加速器

### 開發工作流程
- **迭代測試**：在開發過程中頻繁在類似邊緣的條件下測試
- **性能監控**：持續監控資源使用和推理速度
- **版本控制**：跟蹤模型版本和優化設置
- **文檔記錄**：記錄所有優化決策和性能權衡

### 部署考量
- **資源監控**：在生產環境中監控內存、CPU 和功耗使用
- **回退策略**：為模型故障實現回退機制
- **更新機制**：規劃模型更新和版本管理
- **安全性**：為邊緣 AI 應用實施適當的安全措施

## 與邊緣 AI 框架的整合

### ONNX Runtime
- **跨平台部署**：在不同邊緣平台上部署 ONNX 模型
- **硬件優化**：利用 ONNX Runtime 的硬件專屬優化
- **移動支持**：使用 ONNX Runtime Mobile 支持智能手機和平板應用
- **物聯網整合**：使用 ONNX Runtime 的輕量級分發在物聯網設備上部署

### Windows ML
- **Windows 設備**：優化 Windows 基於邊緣設備和 PC 的應用
- **NPU 加速**：利用 Windows 設備上的神經處理單元
- **DirectML**：在 Windows 平台上使用 DirectML 進行 GPU 加速
- **UWP 整合**：與通用 Windows 平台應用整合

### TensorFlow Lite
- **移動優化**：在移動和嵌入式設備上部署 TensorFlow Lite 模型
- **硬件代理**：使用專用硬件代理進行加速
- **微控制器**：使用 TensorFlow Lite Micro 部署到微控制器
- **跨平台支持**：在 Android、iOS 和嵌入式 Linux 系統上部署

### Azure IoT Edge
- **雲-邊混合**：結合雲端訓練與邊緣推理
- **模組部署**：將 AI 模型作為 IoT Edge 模組部署
- **設備管理**：遠程管理邊緣設備和模型更新
- **遙測**：收集邊緣部署的性能數據和模型指標

## 高級邊緣 AI 場景

### 多模型部署
- **模型集成**：部署多個模型以提高準確性或冗餘性
- **A/B 測試**：在邊緣設備上同時測試不同模型
- **動態選擇**：根據當前設備條件選擇模型
- **資源共享**：優化多個部署模型的資源使用

### 聯邦學習
- **分佈式訓練**：在多個邊緣設備上訓練模型
- **隱私保護**：保持訓練數據本地化，同時共享模型改進
- **協作學習**：使設備能從集體經驗中學習
- **邊緣-雲協調**：協調邊緣設備與雲基礎設施之間的學習

### 實時處理
- **流處理**：在邊緣設備上處理連續數據流
- **低延遲推理**：優化以實現最小推理延遲
- **批量處理**：在邊緣設備上高效處理數據批次
- **自適應處理**：根據當前設備能力調整處理

## 邊緣 AI 開發故障排除

### 常見問題
- **內存限制**：模型過大無法適應目標設備內存
- **推理速度**：模型推理速度無法滿足實時需求
- **準確性下降**：優化導致模型準確性無法接受地降低
- **硬件兼容性**：模型與目標硬件不兼容

### 調試策略
- **性能分析**：使用 AI Toolkit 的追蹤功能識別瓶頸
- **資源監控**：在開發過程中監控內存和 CPU 使用
- **增量測試**：逐步測試優化以隔離問題
- **硬件模擬**：使用開發工具模擬目標硬件

### 優化解決方案
- **進一步量化**：應用更激進的量化技術
- **模型架構**：考慮針對邊緣優化的不同模型架構
- **預處理優化**：優化數據預處理以適應邊緣限制
- **推理優化**：使用硬件專屬的推理優化

## 資源與下一步

### 官方文檔
- [AI Toolkit 開發者文檔](https://aka.ms/AIToolkit/doc)
- [安裝與設置指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)
- [VS Code 智能應用文檔](https://code.visualstudio.com/docs/intelligentapps)
- [Model Context Protocol (MCP) 文檔](https://modelcontextprotocol.io/)

### 社群與支持
- [AI Toolkit GitHub 儲存庫](https://github.com/microsoft/vscode-ai-toolkit)
- [GitHub 問題與功能請求](https://aka.ms/AIToolkit/feedback)
- [Azure AI Foundry Discord 社群](https://aka.ms/azureaifoundry/discord)
- [VS Code 擴展市場](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 技術資源
- [ONNX Runtime 文檔](https://onnxruntime.ai/)
- [Ollama 文檔](https://ollama.ai/)
- [Windows ML 文檔](https://docs.microsoft.com/en-us/windows/ai/)
- [Azure AI Foundry 文檔](https://learn.microsoft.com/en-us/azure/ai-foundry/)

### 學習路徑
- [邊緣 AI 基礎課程](../Module01/README.md)
- [小型語言模型指南](../Module02/README.md)
- [邊緣部署策略](../Module03/README.md)
- [Windows 邊緣 AI 開發](./windowdeveloper.md)

### 其他資源
- **儲存庫統計**：1.8k+ 星標，150+ 分支，18+ 貢獻者
- **許可證**：MIT 許可證
- **安全性**：適用於 Microsoft 安全政策
- **遙測**：遵守 VS Code 遙測設置

## 結論

Visual Studio Code 的 AI Toolkit 是一個現代化 AI 開發的綜合平台，提供了特別適合邊緣 AI 應用的流線型代理開發能力。其廣泛的模型目錄支持如 Anthropic、OpenAI、GitHub 和 Google 等提供商，並結合通過 ONNX 和 Ollama 的本地執行，為多樣化的邊緣部署場景提供了所需的靈活性。

該工具包的優勢在於其整合式方法——從 Playground 中的模型發現和實驗，到使用 Prompt Builder 進行的高級代理開發、全面的評估能力，以及無縫的 MCP 工具整合。對於邊緣 AI 開發者來說，這意味著在邊緣部署之前快速原型設計和測試 AI 代理，並能快速迭代和優化以適應資源受限的環境。

邊緣 AI 開發的主要優勢包括：
- **快速實驗**：在邊緣部署之前快速測試模型和代理
- **多提供商靈活性**：從多個來源訪問模型以尋找最佳邊緣解決方案
- **本地開發**：使用 ONNX 和 Ollama 進行離線和隱私保護的開發
- **生產就緒**：生成生產就緒代碼並通過 MCP 整合外部工具
- **全面評估**：使用內建和自定義指標驗證邊緣 AI 性能

隨著 AI 持續向邊緣部署場景發展，VS Code 的 AI Toolkit 提供了所需的開發環境和工作流程，幫助構建、測試和優化適用於資源受限環境的智能應用。無論您是在開發物聯網解決方案、移動 AI 應用還是嵌入式智能系統，該工具包的全面功能集和整合式工作流程支持整個邊緣 AI 開發生命周期。

隨著持續的開發和活躍的社群（1.8k+ GitHub 星標），AI Toolkit 始終處於 AI 開發工具的前沿，不斷演進以滿足現代 AI 開發者在邊緣部署場景中的需求。

[下一步 Foundry Local](./foundrylocal.md)

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。