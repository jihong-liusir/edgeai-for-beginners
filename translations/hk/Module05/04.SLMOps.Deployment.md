<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:13:09+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "hk"
}
-->
# 第四部分：部署 - 生產級模型實施

## 概述

這份全面的教學將引導你完成使用 Foundry Local 部署微調量化模型的完整過程。我們將涵蓋模型轉換、量化優化以及部署配置的所有步驟。

## 先決條件

在開始之前，請確保你已準備好以下項目：

- ✅ 一個已微調的 ONNX 模型，準備部署
- ✅ Windows 或 Mac 電腦
- ✅ Python 3.10 或更高版本
- ✅ 至少 8GB 可用 RAM
- ✅ 已在系統上安裝 Foundry Local

## 第一部分：環境設置

### 安裝所需工具

打開你的終端（Windows 上的命令提示符，Mac 上的終端），並按順序執行以下命令：

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **重要提示**：你還需要 CMake 3.31 或更新版本，可從 [cmake.org](https://cmake.org/download/) 下載。

## 第二部分：模型轉換與量化

### 選擇合適的格式

對於微調的小型語言模型，我們建議使用 **ONNX 格式**，因為它提供：

- 🚀 更佳的性能優化
- 🔧 硬件無關的部署
- 🏭 生產級能力
- 📱 跨平台兼容性

### 方法一：一鍵轉換（推薦）

使用以下命令直接轉換你的微調模型：

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**參數說明：**
- `--model_name_or_path`：微調模型的路徑
- `--device cpu`：使用 CPU 進行優化
- `--precision int4`：使用 INT4 量化（大約減少 75% 的大小）
- `--output_path`：轉換後模型的輸出路徑

### 方法二：配置文件方式（進階用戶）

創建一個名為 `finetuned_conversion_config.json` 的配置文件：

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

然後執行：

```bash
olive run --config ./finetuned_conversion_config.json
```

### 量化選項比較

| 精度       | 文件大小       | 推理速度       | 模型質量       | 推薦用途         |
|------------|----------------|----------------|----------------|------------------|
| FP16       | 基線 × 0.5    | 快速           | 最佳           | 高端硬件         |
| INT8       | 基線 × 0.25   | 非常快速       | 良好           | 平衡選擇         |
| INT4       | 基線 × 0.125  | 最快           | 可接受         | 資源有限         |

💡 **建議**：首次部署時，從 INT4 量化開始。如果質量不滿意，可嘗試 INT8 或 FP16。

## 第三部分：Foundry Local 部署配置

### 創建模型配置

導航到 Foundry Local 的模型目錄：

```bash
foundry cache cd ./models/
```

創建你的模型目錄結構：

```bash
mkdir -p ./models/custom/your-finetuned-model
```

在模型目錄中創建 `inference_model.json` 配置文件：

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### 特定模型的模板配置

#### 對於 Qwen 系列模型：

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## 第四部分：模型測試與優化

### 驗證模型安裝

檢查 Foundry Local 是否能識別你的模型：

```bash
foundry cache ls
```

你應該能在列表中看到 `your-finetuned-model-int4`。

### 開始模型測試

```bash
foundry model run your-finetuned-model-int4
```

### 性能基準測試

在測試期間監控以下關鍵指標：

1. **響應時間**：測量每次響應的平均時間
2. **內存使用**：監控 RAM 消耗
3. **CPU 使用率**：檢查處理器負載
4. **輸出質量**：評估響應的相關性和連貫性

### 質量驗證清單

- ✅ 模型能正確響應微調領域的查詢
- ✅ 響應格式符合預期的輸出結構
- ✅ 長時間使用期間無內存洩漏
- ✅ 在不同輸入長度下保持一致性能
- ✅ 正確處理邊界情況和無效輸入

## 總結

恭喜！你已成功完成：

- ✅ 微調模型格式轉換
- ✅ 模型量化優化
- ✅ Foundry Local 部署配置
- ✅ 性能調整與故障排除

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。