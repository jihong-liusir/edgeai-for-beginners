<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-07-22T04:22:51+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "hk"
}
-->
# 第三章：微調 - 為特定任務定制模型

## 目錄
1. [微調簡介](../../../Module05)
2. [微調的重要性](../../../Module05)
3. [微調的類型](../../../Module05)
4. [使用 Microsoft Olive 進行微調](../../../Module05)
5. [實操範例](../../../Module05)
6. [最佳實踐與指導方針](../../../Module05)
7. [進階技術](../../../Module05)
8. [評估與監控](../../../Module05)
9. [常見挑戰與解決方案](../../../Module05)
10. [結論](../../../Module05)

## 微調簡介

**微調**是一種強大的機器學習技術，通過調整預訓練模型，使其能執行特定任務或處理專門的數據集。與從零開始訓練模型相比，微調利用預訓練模型已經學到的知識，並根據您的特定需求進行調整。

### 什麼是微調？

微調是一種**遷移學習**形式，具體包括：
- 使用已從大型數據集中學習一般模式的預訓練模型
- 使用您的特定數據集調整模型的內部參數
- 保留已有的寶貴知識，同時使模型專注於您的任務

可以將其比作教一位熟練的廚師學習新的菜系——他們已經掌握了烹飪的基本原理，但需要學習新的技術和風味。

### 主要優勢

- **時間效率**：比從零開始訓練快得多
- **數據效率**：需要較小的數據集即可達到良好性能
- **成本效益**：計算需求較低
- **更佳性能**：通常比從零開始訓練的結果更好
- **資源優化**：使強大的 AI 技術對小型團隊和組織更具可行性

## 微調的重要性

### 現實應用

微調在許多場景中至關重要：

**1. 領域適配**
- 醫療 AI：使一般語言模型適配醫學術語和臨床筆記
- 法律科技：專注於法律文件分析和合同審查
- 金融服務：定制模型以分析財務報告和進行風險評估

**2. 任務專精**
- 內容生成：微調以適配特定的寫作風格或語氣
- 代碼生成：使模型適配特定的編程語言或框架
- 翻譯：提升特定語言對或技術領域的翻譯性能

**3. 企業應用**
- 客戶服務：創建能理解公司特定術語的聊天機器人
- 內部文檔：構建熟悉組織流程的 AI 助手
- 行業專屬解決方案：開發能理解行業特定術語和工作流程的模型

## 微調的類型

### 1. 全參數微調（指令微調）

在全參數微調中，所有模型參數都會在訓練過程中更新。此方法：
- 提供最大靈活性和性能潛力
- 需要大量計算資源
- 生成完全新版本的模型
- 適合擁有大量訓練數據和計算資源的場景

### 2. 參數高效微調（PEFT）

PEFT 方法僅更新一小部分參數，使過程更高效：

#### 低秩適配（LoRA）
- 在現有權重中添加小型可訓練的秩分解矩陣
- 大幅減少可訓練參數數量
- 性能接近全參數微調
- 支持在不同適配之間輕鬆切換

#### QLoRA（量化 LoRA）
- 將 LoRA 與量化技術結合
- 進一步降低內存需求
- 支持在消費級硬件上微調更大的模型
- 在效率與性能之間取得平衡

#### Adapters
- 在現有層之間插入小型神經網絡
- 在保持基礎模型不變的情況下進行針對性微調
- 支持模塊化的模型定制方法

### 3. 任務專屬微調

專注於使模型適配特定的下游任務：
- **分類**：調整模型以執行分類任務
- **生成**：優化模型以進行內容創建和文本生成
- **提取**：微調模型以進行信息提取和命名實體識別
- **摘要**：使模型專注於文檔摘要生成

## 使用 Microsoft Olive 進行微調

Microsoft Olive 是一款全面的模型優化工具包，簡化了微調過程，並提供企業級功能。

### 什麼是 Microsoft Olive？

Microsoft Olive 是一款開源的模型優化工具，具備以下功能：
- 簡化針對各種硬件目標的微調工作流程
- 提供對流行模型架構（Llama、Phi、Qwen、Gemma）的內置支持
- 支持雲端和本地部署選項
- 與 Azure ML 和其他 Microsoft AI 服務無縫集成
- 支持自動優化和量化

### 主要特點

- **硬件感知優化**：自動針對特定硬件（CPU、GPU、NPU）進行模型優化
- **多格式支持**：兼容 PyTorch、Hugging Face 和 ONNX 模型
- **自動化工作流程**：減少手動配置和反覆試驗
- **企業集成**：內置支持 Azure ML 和雲端部署
- **可擴展架構**：支持自定義優化技術

### 安裝與設置

#### 基本安裝

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### 可選依賴項

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### 驗證安裝

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## 實操範例

### 範例 1：使用 Olive CLI 進行基本微調

此範例展示如何微調一個小型語言模型以進行短語分類：

#### 第一步：準備環境

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### 第二步：微調模型

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### 第三步：優化部署

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### 範例 2：使用自定義數據集進行高級配置

#### 第一步：準備自定義數據集

創建包含訓練數據的 JSON 文件：

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### 第二步：創建配置文件

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### 第三步：執行微調

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### 範例 3：使用 QLoRA 進行內存高效微調

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## 最佳實踐與指導方針

### 數據準備

**1. 質量優先於數量**
- 優先選擇高質量、多樣化的示例，而非大量低質量數據
- 確保數據能代表您的目標使用場景
- 一致地清理和預處理數據

**2. 數據格式與模板**
- 在所有訓練示例中使用一致的格式
- 創建與您的使用場景匹配的清晰輸入-輸出模板
- 為指令微調模型包含適當的指令格式

**3. 數據集劃分**
- 保留 10-20% 的數據用於驗證
- 確保訓練/驗證劃分具有相似的分佈
- 對於分類任務，考慮分層抽樣

### 訓練配置

**1. 學習率選擇**
- 微調時使用較小的學習率（1e-5 至 1e-4）
- 使用學習率調度以改善收斂效果
- 監控損失曲線以調整學習率

**2. 批量大小優化**
- 根據可用內存平衡批量大小
- 使用梯度累積以實現更大的有效批量大小
- 考慮批量大小與學習率的關係

**3. 訓練時長**
- 監控驗證指標以避免過擬合
- 當驗證性能停滯時使用早停
- 定期保存檢查點以便恢復和分析

### 模型選擇

**1. 基礎模型選擇**
- 儘量選擇已在相似領域預訓練的模型
- 根據計算限制選擇模型大小
- 評估商業用途的許可要求

**2. 微調方法選擇**
- 在資源有限的環境中使用 LoRA/QLoRA
- 當性能至關重要時選擇全參數微調
- 對於多任務場景考慮基於 Adapter 的方法

### 資源管理

**1. 硬件優化**
- 根據模型大小和方法選擇合適的硬件
- 使用梯度檢查點高效利用 GPU 內存
- 對於大型模型考慮雲端解決方案

**2. 內存管理**
- 使用混合精度訓練（如果可用）
- 在內存受限的情況下實施梯度累積
- 在訓練過程中監控 GPU 內存使用情況

## 進階技術

### 多適配器訓練

為不同任務訓練多個適配器，同時共享基礎模型：

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### 超參數優化

實施系統化的超參數調整：

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### 自定義損失函數

實施領域專屬的損失函數：

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## 評估與監控

### 指標與評估

**1. 標準指標**
- **準確率**：分類任務的整體正確性
- **困惑度**：語言建模質量的衡量指標
- **BLEU/ROUGE**：文本生成和摘要質量的衡量指標
- **F1 分數**：分類任務中平衡精確率與召回率的指標

**2. 領域專屬指標**
- **任務專屬基準**：使用領域內已建立的基準
- **人工評估**：對主觀任務進行人工評估
- **業務指標**：與實際業務目標保持一致

**3. 評估設置**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### 監控訓練進度

**1. 損失跟蹤**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. 驗證監控**
- 同時跟蹤驗證損失與訓練損失
- 監控過擬合跡象（驗證損失增加而訓練損失減少）
- 根據驗證指標使用早停

**3. 資源監控**
- 監控 GPU/CPU 使用率
- 跟蹤內存使用模式
- 監控訓練速度和吞吐量

## 常見挑戰與解決方案

### 挑戰 1：過擬合

**症狀：**
- 訓練損失持續下降而驗證損失增加
- 訓練與驗證性能之間存在較大差距
- 對新數據的泛化能力差

**解決方案：**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### 挑戰 2：內存限制

**解決方案：**
- 使用梯度檢查點
- 實施梯度累積
- 選擇參數高效方法（LoRA、QLoRA）
- 對大型模型使用模型並行化

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### 挑戰 3：訓練速度慢

**解決方案：**
- 優化數據加載管道
- 使用混合精度訓練
- 實施高效的批量策略
- 對大型數據集考慮分布式訓練

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### 挑戰 4：性能不佳

**診斷步驟：**
1. 驗證數據質量和格式
2. 檢查學習率和訓練時長
3. 評估基礎模型選擇
4. 審查預處理和分詞

**解決方案：**
- 增加訓練數據的多樣性
- 調整學習率調度
- 嘗試不同的基礎模型
- 實施數據增強技術

## 結論

微調是一種強大的技術，使得最先進的 AI 能力更加普及。通過使用 Microsoft Olive 等工具，組織可以高效地將預訓練模型適配到其特定需求，同時優化性能和資源限制。

### 核心要點

1. **選擇合適的方法**：根據計算資源和性能需求選擇微調方法
2. **數據質量至關重要**：投資於高質量、具有代表性的訓練數據
3. **監控與迭代**：持續評估並改進模型
4. **利用工具**：使用像 Olive 這樣的框架簡化和優化過程
5. **考慮部署**：從一開始就計劃模型的優化和部署

## ➡️ 下一步

- [第四章：部署 - 生產就緒模型實施](./04.SLMOps.Deployment.md)

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。