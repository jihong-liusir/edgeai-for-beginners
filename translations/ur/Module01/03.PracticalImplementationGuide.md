<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:14:42+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ur"
}
-->
# سیکشن 3: عملی نفاذ کے رہنما اصول

## جائزہ

یہ جامع رہنما آپ کو EdgeAI کورس کے لیے تیار کرے گا، جو ایسے عملی AI حل بنانے پر مرکوز ہے جو مؤثر طریقے سے ایج ڈیوائسز پر چلتے ہیں۔ کورس جدید فریم ورکز اور ایج ڈپلائمنٹ کے لیے بہتر ماڈلز کے استعمال کے ساتھ عملی ترقی پر زور دیتا ہے۔

## 1. ترقیاتی ماحول کی ترتیب

### پروگرامنگ زبانیں اور فریم ورکز

**پائتھون ماحول**
- **ورژن**: پائتھون 3.10 یا اس سے زیادہ (تجویز کردہ: پائتھون 3.11)
- **پیکیج مینیجر**: pip یا conda
- **ورچوئل ماحول**: تنہائی کے لیے venv یا conda ماحول استعمال کریں
- **اہم لائبریریاں**: کورس کے دوران مخصوص EdgeAI لائبریریاں انسٹال کریں گے

**Microsoft .NET ماحول**
- **ورژن**: .NET 8 یا اس سے زیادہ
- **IDE**: Visual Studio 2022، Visual Studio Code، یا JetBrains Rider
- **SDK**: کراس پلیٹ فارم ترقی کے لیے .NET SDK انسٹال کریں

### ترقیاتی ٹولز

**کوڈ ایڈیٹرز اور IDEs**
- Visual Studio Code (کراس پلیٹ فارم ترقی کے لیے تجویز کردہ)
- PyCharm یا Visual Studio (زبان کے لحاظ سے ترقی کے لیے)
- Jupyter Notebooks انٹرایکٹو ترقی اور پروٹوٹائپنگ کے لیے

**ورژن کنٹرول**
- Git (تازہ ترین ورژن)
- GitHub اکاؤنٹ ریپوزیٹریز تک رسائی اور تعاون کے لیے

## 2. ہارڈویئر کی ضروریات اور سفارشات

### کم از کم سسٹم کی ضروریات
- **CPU**: ملٹی کور پروسیسر (Intel i5/AMD Ryzen 5 یا مساوی)
- **RAM**: کم از کم 8GB، تجویز کردہ 16GB
- **اسٹوریج**: ماڈلز اور ترقیاتی ٹولز کے لیے 50GB دستیاب جگہ
- **OS**: Windows 10/11، macOS 10.15+، یا Linux (Ubuntu 20.04+)

### کمپیوٹ وسائل کی حکمت عملی
کورس مختلف ہارڈویئر کنفیگریشنز کے لیے قابل رسائی بنایا گیا ہے:

**مقامی ترقی (CPU/NPU فوکس)**
- بنیادی ترقی CPU اور NPU ایکسیلیریشن کا استعمال کرے گی
- زیادہ تر جدید لیپ ٹاپ اور ڈیسک ٹاپس کے لیے موزوں
- مؤثر اور عملی ڈپلائمنٹ منظرناموں پر توجہ

**کلاؤڈ GPU وسائل (اختیاری)**
- **Azure Machine Learning**: شدید تربیت اور تجربات کے لیے
- **Google Colab**: تعلیمی مقاصد کے لیے مفت ٹائر دستیاب
- **Kaggle Notebooks**: کلاؤڈ کمپیوٹنگ پلیٹ فارم کا متبادل

### ایج ڈیوائس کے تحفظات
- ARM پر مبنی پروسیسرز کی سمجھ
- موبائل اور IoT ہارڈویئر کی حدود کا علم
- توانائی کی کھپت کی اصلاح سے واقفیت

## 3. بنیادی ماڈل خاندان اور وسائل

### بنیادی ماڈل خاندان

**Microsoft Phi-4 خاندان**
- **تفصیل**: کمپیکٹ، مؤثر ماڈلز جو ایج ڈپلائمنٹ کے لیے ڈیزائن کیے گئے ہیں
- **خصوصیات**: سائز کے مقابلے میں بہترین کارکردگی، استدلال کے کاموں کے لیے بہتر
- **وسائل**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **استعمال کے کیسز**: کوڈ جنریشن، ریاضیاتی استدلال، عمومی گفتگو

**Qwen-3 خاندان**
- **تفصیل**: Alibaba کے تازہ ترین نسل کے کثیر لسانی ماڈلز
- **خصوصیات**: مضبوط کثیر لسانی صلاحیتیں، مؤثر آرکیٹیکچر
- **وسائل**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **استعمال کے کیسز**: کثیر لسانی ایپلیکیشنز، ثقافتی AI حل

**Google Gemma-3n خاندان**
- **تفصیل**: گوگل کے ہلکے ماڈلز جو ایج ڈپلائمنٹ کے لیے بہتر ہیں
- **خصوصیات**: تیز انفرنس، موبائل دوستانہ آرکیٹیکچر
- **وسائل**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **استعمال کے کیسز**: موبائل ایپلیکیشنز، حقیقی وقت کی پروسیسنگ

### ماڈل انتخاب کے معیار
- **کارکردگی بمقابلہ سائز کے سمجھوتے**: چھوٹے یا بڑے ماڈلز کا انتخاب کب کرنا ہے
- **کام کے لحاظ سے اصلاح**: ماڈلز کو مخصوص استعمال کے کیسز سے ملانا
- **ڈپلائمنٹ کی حدود**: میموری، لیٹنسی، اور توانائی کی کھپت کے تحفظات

## 4. کوانٹائزیشن اور اصلاحی ٹولز

### Llama.cpp فریم ورک
- **ریپوزیٹری**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **مقصد**: LLMs کے لیے اعلیٰ کارکردگی کا انفرنس انجن
- **اہم خصوصیات**:
  - CPU کے لیے بہتر انفرنس
  - متعدد کوانٹائزیشن فارمیٹس (Q4، Q5، Q8)
  - کراس پلیٹ فارم مطابقت
  - میموری مؤثر عملدرآمد
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **ریپوزیٹری**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **مقصد**: ایج ڈپلائمنٹ کے لیے ماڈل اصلاحی ٹول کٹ
- **اہم خصوصیات**:
  - خودکار ماڈل اصلاحی ورک فلو
  - ہارڈویئر کے لحاظ سے اصلاح
  - ONNX Runtime کے ساتھ انضمام
  - کارکردگی کے بینچ مارکنگ ٹولز
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # ماڈل اصلاح کے لیے مثال پائتھون اسکرپٹ
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS صارفین)
- **ریپوزیٹری**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **مقصد**: Apple Silicon کے لیے مشین لرننگ فریم ورک
- **اہم خصوصیات**:
  - Apple Silicon کے لیے مقامی اصلاح
  - میموری مؤثر آپریشنز
  - PyTorch جیسا API
  - متحد میموری آرکیٹیکچر سپورٹ
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **ریپوزیٹری**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **مقصد**: ONNX ماڈلز کے لیے کراس پلیٹ فارم انفرنس ایکسیلیریشن
- **اہم خصوصیات**:
  - ہارڈویئر کے لحاظ سے اصلاحات (CPU، GPU، NPU)
  - انفرنس کے لیے گراف اصلاحات
  - کوانٹائزیشن سپورٹ
  - کراس زبان سپورٹ (پائتھون، C++، C#، جاوا اسکرپٹ)
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. تجویز کردہ مطالعہ اور وسائل

### ضروری دستاویزات
- **ONNX Runtime Documentation**: کراس پلیٹ فارم انفرنس کو سمجھنا
- **Hugging Face Transformers Guide**: ماڈل لوڈنگ اور انفرنس
- **Edge AI Design Patterns**: ایج ڈپلائمنٹ کے لیے بہترین طریقے

### تکنیکی مقالے
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### کمیونٹی وسائل
- **EdgeAI Slack/Discord Communities**: ہم مرتبہ مدد اور گفتگو
- **GitHub Repositories**: مثال نفاذ اور ٹیوٹوریلز
- **YouTube Channels**: تکنیکی گہرائی اور ٹیوٹوریلز

## 6. تشخیص اور تصدیق

### کورس سے پہلے چیک لسٹ
- [ ] پائتھون 3.10+ انسٹال اور تصدیق شدہ
- [ ] .NET 8+ انسٹال اور تصدیق شدہ
- [ ] ترقیاتی ماحول ترتیب دیا گیا
- [ ] Hugging Face اکاؤنٹ بنایا گیا
- [ ] ہدف ماڈل خاندانوں سے بنیادی واقفیت
- [ ] کوانٹائزیشن ٹولز انسٹال اور ٹیسٹ کیے گئے
- [ ] ہارڈویئر کی ضروریات پوری کی گئیں
- [ ] کلاؤڈ کمپیوٹنگ اکاؤنٹس ترتیب دیے گئے (اگر ضرورت ہو)

## کلیدی سیکھنے کے مقاصد

اس رہنما کے اختتام تک، آپ قابل ہوں گے:

1. EdgeAI ایپلیکیشن ترقی کے لیے مکمل ترقیاتی ماحول ترتیب دینا
2. ماڈل اصلاح کے لیے ضروری ٹولز اور فریم ورکز انسٹال اور ترتیب دینا
3. اپنے EdgeAI پروجیکٹس کے لیے مناسب ہارڈویئر اور سافٹ ویئر کنفیگریشنز کا انتخاب کرنا
4. ایج ڈیوائسز پر AI ماڈلز کی تعیناتی کے کلیدی تحفظات کو سمجھنا
5. کورس کے عملی مشقوں کے لیے اپنے سسٹم کو تیار کرنا

## اضافی وسائل

### سرکاری دستاویزات
- **Python Documentation**: پائتھون زبان کی سرکاری دستاویزات
- **Microsoft .NET Documentation**: .NET ترقیاتی وسائل
- **ONNX Runtime Documentation**: ONNX Runtime کے لیے جامع رہنما
- **TensorFlow Lite Documentation**: TensorFlow Lite کی سرکاری دستاویزات

### ترقیاتی ٹولز
- **Visual Studio Code**: ہلکا پھلکا کوڈ ایڈیٹر AI ترقیاتی ایکسٹینشنز کے ساتھ
- **Jupyter Notebooks**: ML تجربات کے لیے انٹرایکٹو کمپیوٹنگ ماحول
- **Docker**: مستقل ترقیاتی ماحول کے لیے کنٹینرائزیشن پلیٹ فارم
- **Git**: کوڈ مینجمنٹ کے لیے ورژن کنٹرول سسٹم

### سیکھنے کے وسائل
- **EdgeAI Research Papers**: مؤثر ماڈلز پر تازہ ترین علمی تحقیق
- **Online Courses**: AI اصلاح پر اضافی سیکھنے کے مواد
- **Community Forums**: EdgeAI ترقیاتی چیلنجز کے لیے Q&A پلیٹ فارمز
- **Benchmark Datasets**: ماڈل کارکردگی کا جائزہ لینے کے لیے معیاری ڈیٹا سیٹس

## سیکھنے کے نتائج

اس تیاری کے رہنما کو مکمل کرنے کے بعد، آپ:

1. EdgeAI ترقی کے لیے مکمل طور پر ترتیب دیا گیا ترقیاتی ماحول رکھیں گے
2. مختلف ڈپلائمنٹ منظرناموں کے لیے ہارڈویئر اور سافٹ ویئر کی ضروریات کو سمجھیں گے
3. کورس کے دوران استعمال ہونے والے کلیدی فریم ورکز اور ٹولز سے واقف ہوں گے
4. ڈیوائس کی حدود اور ضروریات کی بنیاد پر مناسب ماڈلز کا انتخاب کرنے کے قابل ہوں گے
5. ایج ڈپلائمنٹ کے لیے اصلاحی تکنیکوں کا بنیادی علم حاصل کریں گے

## ➡️ آگے کیا ہے

- [04: EdgeAI ہارڈویئر اور ڈپلائمنٹ](04.EdgeDeployment.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔