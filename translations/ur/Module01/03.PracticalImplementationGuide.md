<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-17T17:38:59+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ur"
}
-->
# سیکشن 3: عملی رہنمائی

## جائزہ

یہ جامع رہنمائی آپ کو EdgeAI کورس کے لیے تیار کرے گی، جو ایسے عملی AI حل بنانے پر مرکوز ہے جو edge devices پر مؤثر طریقے سے چل سکیں۔ کورس جدید فریم ورکز اور جدید ترین ماڈلز کے استعمال پر زور دیتا ہے جو edge deployment کے لیے بہتر بنائے گئے ہیں۔

## 1. ترقیاتی ماحول کی ترتیب

### پروگرامنگ زبانیں اور فریم ورکز

**پائتھون ماحول**
- **ورژن**: پائتھون 3.10 یا اس سے زیادہ (تجویز کردہ: پائتھون 3.11)
- **پیکیج مینیجر**: pip یا conda
- **ورچوئل ماحول**: venv یا conda ماحول استعمال کریں تاکہ علیحدگی برقرار رہے
- **اہم لائبریریاں**: کورس کے دوران مخصوص EdgeAI لائبریریاں انسٹال کی جائیں گی

**مائیکروسافٹ .NET ماحول**
- **ورژن**: .NET 8 یا اس سے زیادہ
- **IDE**: Visual Studio 2022، Visual Studio Code، یا JetBrains Rider
- **SDK**: کراس پلیٹ فارم ترقی کے لیے .NET SDK انسٹال کریں

### ترقیاتی ٹولز

**کوڈ ایڈیٹرز اور IDEs**
- Visual Studio Code (کراس پلیٹ فارم ترقی کے لیے تجویز کردہ)
- PyCharm یا Visual Studio (زبان کے لحاظ سے ترقی کے لیے)
- Jupyter Notebooks انٹرایکٹو ترقی اور پروٹوٹائپنگ کے لیے

**ورژن کنٹرول**
- Git (تازہ ترین ورژن)
- GitHub اکاؤنٹ ریپوزیٹریز تک رسائی اور تعاون کے لیے

## 2. ہارڈویئر کی ضروریات اور سفارشات

### کم از کم سسٹم کی ضروریات
- **CPU**: ملٹی کور پروسیسر (Intel i5/AMD Ryzen 5 یا مساوی)
- **RAM**: کم از کم 8GB، تجویز کردہ 16GB
- **اسٹوریج**: ماڈلز اور ترقیاتی ٹولز کے لیے 50GB دستیاب جگہ
- **OS**: Windows 10/11، macOS 10.15+، یا Linux (Ubuntu 20.04+)

### کمپیوٹ وسائل کی حکمت عملی
کورس مختلف ہارڈویئر کنفیگریشنز پر قابل رسائی ہونے کے لیے ڈیزائن کیا گیا ہے:

**لوکل ترقی (CPU/NPU فوکس)**
- بنیادی ترقی CPU اور NPU ایکسیلیریشن کا استعمال کرے گی
- زیادہ تر جدید لیپ ٹاپ اور ڈیسک ٹاپس کے لیے موزوں
- مؤثر اور عملی deployment کے منظرناموں پر توجہ

**کلاؤڈ GPU وسائل (اختیاری)**
- **Azure Machine Learning**: شدید تربیت اور تجربات کے لیے
- **Google Colab**: تعلیمی مقاصد کے لیے مفت ٹائر دستیاب
- **Kaggle Notebooks**: کلاؤڈ کمپیوٹنگ پلیٹ فارم کا متبادل

### Edge ڈیوائس کے پہلو
- ARM-based پروسیسرز کی سمجھ
- موبائل اور IoT ہارڈویئر کی حدود کا علم
- پاور کنزمپشن کی اصلاح سے واقفیت

## 3. بنیادی ماڈل خاندان اور وسائل

### بنیادی ماڈل خاندان

**Microsoft Phi-4 خاندان**
- **تفصیل**: کمپیکٹ، مؤثر ماڈلز جو edge deployment کے لیے ڈیزائن کیے گئے ہیں
- **خصوصیات**: کارکردگی اور سائز کا بہترین تناسب، reasoning tasks کے لیے بہتر
- **وسائل**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **استعمال کے کیسز**: کوڈ جنریشن، ریاضیاتی reasoning، عمومی گفتگو

**Qwen-3 خاندان**
- **تفصیل**: Alibaba کے جدید ترین multilingual ماڈلز
- **خصوصیات**: مضبوط multilingual صلاحیتیں، مؤثر آرکیٹیکچر
- **وسائل**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **استعمال کے کیسز**: multilingual ایپلیکیشنز، cross-cultural AI حل

**Google Gemma-3n خاندان**
- **تفصیل**: Google کے lightweight ماڈلز جو edge deployment کے لیے بہتر بنائے گئے ہیں
- **خصوصیات**: تیز inference، موبائل فرینڈلی آرکیٹیکچر
- **وسائل**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **استعمال کے کیسز**: موبائل ایپلیکیشنز، real-time پروسیسنگ

### ماڈل انتخاب کے معیار
- **کارکردگی بمقابلہ سائز کے سمجھوتے**: چھوٹے یا بڑے ماڈلز کا انتخاب کب کرنا ہے
- **ٹاسک مخصوص اصلاح**: ماڈلز کو مخصوص استعمال کے کیسز سے ہم آہنگ کرنا
- **deployment کی حدود**: میموری، لیٹینسی، اور پاور کنزمپشن کے پہلو

## 4. کوانٹائزیشن اور اصلاحی ٹولز

### Llama.cpp فریم ورک
- **ریپوزیٹری**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **مقصد**: LLMs کے لیے اعلیٰ کارکردگی inference انجن
- **اہم خصوصیات**:
  - CPU-optimized inference
  - متعدد کوانٹائزیشن فارمیٹس (Q4، Q5، Q8)
  - کراس پلیٹ فارم مطابقت
  - میموری مؤثر عملدرآمد
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **ریپوزیٹری**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **مقصد**: edge deployment کے لیے ماڈل اصلاحی ٹول کٹ
- **اہم خصوصیات**:
  - خودکار ماڈل اصلاحی ورک فلو
  - ہارڈویئر سے آگاہ اصلاح
  - ONNX Runtime کے ساتھ انضمام
  - کارکردگی بینچ مارکنگ ٹولز
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # ماڈل اور اصلاحی کنفیگریشن کی تعریف کریں
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # اصلاحی ورک فلو چلائیں
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # اصلاح شدہ ماڈل محفوظ کریں
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # MLX انسٹال کریں
  pip install mlx
  
  # ماڈل لوڈ کرنے اور اصلاح کرنے کے لیے مثال Python اسکرپٹ
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **ریپوزیٹری**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **مقصد**: ONNX ماڈلز کے لیے کراس پلیٹ فارم inference ایکسیلیریشن
- **اہم خصوصیات**:
  - ہارڈویئر مخصوص اصلاحات (CPU، GPU، NPU)
  - inference کے لیے گراف اصلاحات
  - کوانٹائزیشن سپورٹ
  - کراس زبان سپورٹ (Python، C++، C#، JavaScript)
- **انسٹالیشن اور بنیادی استعمال**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. تجویز کردہ مطالعہ اور وسائل

### ضروری دستاویزات
- **ONNX Runtime Documentation**: کراس پلیٹ فارم inference کو سمجھنا
- **Hugging Face Transformers Guide**: ماڈل لوڈنگ اور inference
- **Edge AI Design Patterns**: edge deployment کے لیے بہترین طریقے

### تکنیکی مقالے
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### کمیونٹی وسائل
- **EdgeAI Slack/Discord Communities**: ہم عمر مدد اور گفتگو
- **GitHub ریپوزیٹریز**: مثال implementations اور tutorials
- **YouTube چینلز**: تکنیکی گہرائی اور tutorials

## 6. تشخیص اور تصدیق

### کورس سے پہلے چیک لسٹ
- [ ] پائتھون 3.10+ انسٹال اور تصدیق شدہ
- [ ] .NET 8+ انسٹال اور تصدیق شدہ
- [ ] ترقیاتی ماحول ترتیب دیا گیا
- [ ] Hugging Face اکاؤنٹ بنایا گیا
- [ ] ہدف ماڈل خاندانوں سے بنیادی واقفیت
- [ ] کوانٹائزیشن ٹولز انسٹال اور ٹیسٹ کیے گئے
- [ ] ہارڈویئر کی ضروریات پوری کی گئیں
- [ ] کلاؤڈ کمپیوٹنگ اکاؤنٹس ترتیب دیے گئے (اگر ضرورت ہو)

## اہم سیکھنے کے مقاصد

اس رہنمائی کے اختتام تک، آپ:

1. EdgeAI ایپلیکیشن ترقی کے لیے مکمل ترقیاتی ماحول ترتیب دے سکیں گے
2. ماڈل اصلاح کے لیے ضروری ٹولز اور فریم ورکز انسٹال اور ترتیب دے سکیں گے
3. اپنے EdgeAI پروجیکٹس کے لیے مناسب ہارڈویئر اور سافٹ ویئر کنفیگریشنز منتخب کر سکیں گے
4. edge devices پر AI ماڈلز کو deploy کرنے کے کلیدی پہلو سمجھ سکیں گے
5. کورس کے عملی مشقوں کے لیے اپنے سسٹم کو تیار کر سکیں گے

## اضافی وسائل

### سرکاری دستاویزات
- **Python Documentation**: پائتھون زبان کی سرکاری دستاویزات
- **Microsoft .NET Documentation**: .NET ترقیاتی وسائل
- **ONNX Runtime Documentation**: ONNX Runtime کے لیے جامع گائیڈ
- **TensorFlow Lite Documentation**: TensorFlow Lite کی سرکاری دستاویزات

### ترقیاتی ٹولز
- **Visual Studio Code**: AI ترقیاتی ایکسٹینشنز کے ساتھ ہلکا پھلکا کوڈ ایڈیٹر
- **Jupyter Notebooks**: ML تجربات کے لیے انٹرایکٹو کمپیوٹنگ ماحول
- **Docker**: مستقل ترقیاتی ماحول کے لیے کنٹینرائزیشن پلیٹ فارم
- **Git**: کوڈ مینجمنٹ کے لیے ورژن کنٹرول سسٹم

### سیکھنے کے وسائل
- **EdgeAI تحقیقی مقالے**: مؤثر ماڈلز پر تازہ ترین علمی تحقیق
- **آن لائن کورسز**: AI اصلاح پر اضافی تعلیمی مواد
- **کمیونٹی فورمز**: EdgeAI ترقیاتی چیلنجز کے لیے Q&A پلیٹ فارمز
- **بینچ مارک ڈیٹاسیٹس**: ماڈل کارکردگی کا جائزہ لینے کے لیے معیاری ڈیٹاسیٹس

## سیکھنے کے نتائج

اس تیاری کی رہنمائی مکمل کرنے کے بعد، آپ:

1. EdgeAI ترقی کے لیے مکمل طور پر ترتیب دیا گیا ترقیاتی ماحول رکھیں گے
2. مختلف deployment منظرناموں کے لیے ہارڈویئر اور سافٹ ویئر کی ضروریات کو سمجھیں گے
3. کورس میں استعمال ہونے والے کلیدی فریم ورکز اور ٹولز سے واقف ہوں گے
4. ڈیوائس کی حدود اور ضروریات کے مطابق مناسب ماڈلز منتخب کر سکیں گے
5. edge deployment کے لیے اصلاحی تکنیکوں کا بنیادی علم حاصل کریں گے

## ➡️ آگے کیا ہے

- [04: EdgeAI ہارڈویئر اور ڈیپلائمنٹ](04.EdgeDeployment.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔