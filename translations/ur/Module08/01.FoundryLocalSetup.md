<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T14:22:20+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ur"
}
-->
# سیشن 1: فاؤنڈری لوکل کے ساتھ شروعات

## جائزہ

مائیکروسافٹ فاؤنڈری لوکل Azure AI فاؤنڈری کی صلاحیتوں کو براہ راست آپ کے Windows 11 ڈیولپمنٹ ماحول میں لاتا ہے، جو پرائیویسی کو محفوظ رکھنے، کم تاخیر والے AI ڈیولپمنٹ کو انٹرپرائز گریڈ ٹولز کے ساتھ ممکن بناتا ہے۔ اس سیشن میں انسٹالیشن، کنفیگریشن، اور مشہور ماڈلز جیسے phi، qwen، deepseek، اور GPT-OSS-20B کی عملی تعیناتی شامل ہے۔

## سیکھنے کے مقاصد

اس سیشن کے اختتام تک، آپ:
- Windows 11 پر فاؤنڈری لوکل انسٹال اور کنفیگر کریں گے
- CLI کمانڈز اور کنفیگریشن آپشنز میں مہارت حاصل کریں گے
- بہترین کارکردگی کے لیے ماڈل کیشنگ حکمت عملی کو سمجھیں گے
- phi، qwen، deepseek، اور GPT-OSS-20B ماڈلز کو کامیابی سے چلائیں گے
- فاؤنڈری لوکل کا استعمال کرتے ہوئے اپنی پہلی AI ایپلیکیشن بنائیں گے

## ضروریات

### سسٹم کی ضروریات
- **Windows 11**: ورژن 22H2 یا اس سے جدید
- **RAM**: کم از کم 16GB، 32GB تجویز کردہ
- **اسٹوریج**: ماڈلز اور کیش کے لیے 50GB خالی جگہ
- **ہارڈویئر**: NPU- یا GPU-فعال ڈیوائس ترجیحی (Copilot+ PC یا NVIDIA GPU)
- **نیٹ ورک**: ماڈل ڈاؤنلوڈ کے لیے تیز رفتار انٹرنیٹ

### ڈیولپمنٹ ماحول
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## حصہ 1: انسٹالیشن اور سیٹ اپ

### مرحلہ 1: فاؤنڈری لوکل انسٹال کریں

Winget کا استعمال کرتے ہوئے فاؤنڈری لوکل انسٹال کریں یا GitHub سے انسٹالر ڈاؤنلوڈ کریں:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### مرحلہ 2: انسٹالیشن کی تصدیق کریں

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## حصہ 2: CLI کو سمجھنا

### بنیادی کمانڈز کا ڈھانچہ

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## حصہ 3: ماڈل کیشنگ اور مینجمنٹ

فاؤنڈری لوکل کارکردگی اور اسٹوریج کو بہتر بنانے کے لیے ذہین ماڈل کیشنگ نافذ کرتا ہے:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## حصہ 4: ماڈل کی عملی تعیناتی

### مائیکروسافٹ Phi ماڈلز چلانا

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen ماڈلز کے ساتھ کام کرنا

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek ماڈلز چلانا

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B چلانا

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## حصہ 5: اپنی پہلی ایپلیکیشن بنانا

### سادہ چیٹ انٹرفیس (OpenAI-مطابقت پذیر API)

فاؤنڈری لوکل کے OpenAI-مطابقت پذیر REST API کا استعمال کرتے ہوئے ایک بنیادی چیٹ ایپلیکیشن بنائیں۔ یقینی بنائیں کہ ایک ماڈل دوسرے ٹرمینل میں چل رہا ہو۔

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### چیٹ ایپلیکیشن چلائیں

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## حصہ 6: مسائل کا حل اور بہترین طریقے

### عام مسائل اور ان کے حل

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### سسٹم وسائل کی نگرانی (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### بہترین طریقے

- `foundry model ...`, `foundry cache ...`, اور `foundry service ...` کمانڈز کو ترجیح دیں (CLI حوالہ دیکھیں)
- نئے ماڈلز اور فکسز تک رسائی کے لیے باقاعدگی سے اپ گریڈ کریں
- چھوٹے ماڈلز (Phi mini، Qwen 7B) سے شروع کریں اور بتدریج بڑھائیں
- پرامپٹس اور سیٹنگز کو ٹیون کرتے وقت CPU/GPU/میموری کی نگرانی کریں

## حصہ 7: عملی مشقیں

### مشق 1: فوری ملٹی ماڈل رنز

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### مشق 2: بنیادی تاخیر کا بینچ مارک

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## حوالہ جات

- فاؤنڈری لوکل کے ساتھ شروعات کریں: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- CLI حوالہ اور کمانڈز کا جائزہ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- فاؤنڈری لوکل کے لیے Hugging Face ماڈلز کو کمپائل کریں: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- مائیکروسافٹ فاؤنڈری لوکل GitHub: https://github.com/microsoft/Foundry-Local

---

