<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T14:21:39+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ur"
}
-->
# سیشن 3: اوپن سورس ماڈلز کے ساتھ Foundry Local

## جائزہ

اس سیشن میں یہ سیکھا جائے گا کہ اوپن سورس ماڈلز کو Foundry Local میں کیسے لایا جائے: کمیونٹی ماڈلز کا انتخاب، Hugging Face مواد کو شامل کرنا، اور "اپنا ماڈل لائیں" (BYOM) حکمت عملی اپنانا۔ آپ Model Mondays سیریز کے بارے میں بھی جانیں گے تاکہ مسلسل سیکھنے اور ماڈلز کی دریافت جاری رہے۔

حوالہ جات:
- Foundry Local دستاویزات: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face ماڈلز کو کمپائل کریں: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## سیکھنے کے مقاصد
- مقامی انفرنس کے لیے اوپن سورس ماڈلز دریافت کریں اور ان کا جائزہ لیں
- Foundry Local میں منتخب Hugging Face ماڈلز کو کمپائل اور چلائیں
- درستگی، تاخیر، اور وسائل کی ضروریات کے لیے ماڈل انتخاب کی حکمت عملی اپلائی کریں
- کیش اور ورژننگ کے ساتھ ماڈلز کو مقامی طور پر منظم کریں

## حصہ 1: ماڈل کی دریافت اور انتخاب (مرحلہ وار)

مرحلہ 1) مقامی کیٹلاگ میں دستیاب ماڈلز کی فہرست بنائیں  
```cmd
foundry model list
```
  
مرحلہ 2) دو امیدواروں کو جلدی آزمائیں (پہلی بار چلانے پر خودکار ڈاؤنلوڈ)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
مرحلہ 3) بنیادی میٹرکس نوٹ کریں  
- ایک مقررہ پرامپٹ کے لیے تاخیر (ذاتی مشاہدہ) اور معیار دیکھیں  
- ہر ماڈل کے چلنے کے دوران Task Manager کے ذریعے میموری کے استعمال کا مشاہدہ کریں  

## حصہ 2: CLI کے ذریعے کیٹلاگ ماڈلز چلانا (مرحلہ وار)

مرحلہ 1) ایک ماڈل شروع کریں  
```cmd
foundry model run llama-3.2
```
  
مرحلہ 2) OpenAI-compatible endpoint کے ذریعے ایک ٹیسٹ پرامپٹ بھیجیں  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## حصہ 3: BYOM – Hugging Face ماڈلز کو کمپائل کریں (مرحلہ وار)

ماڈلز کو کمپائل کرنے کے لیے آفیشل ہاؤ ٹو گائیڈ پر عمل کریں۔ نیچے دیے گئے ہائی لیول فلو کو دیکھیں—Microsoft Learn آرٹیکل میں بالکل درست کمانڈز اور سپورٹڈ کنفیگریشنز موجود ہیں۔

مرحلہ 1) ایک ورکنگ ڈائریکٹری تیار کریں  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
مرحلہ 2) ایک سپورٹڈ HF ماڈل کمپائل کریں  
- Learn دستاویز کے مراحل استعمال کریں تاکہ کمپائل شدہ ONNX ماڈل کو اپنی `models` ڈائریکٹری میں رکھیں  
- تصدیق کریں:  
```cmd
foundry cache ls
```
  
آپ کو اپنے کمپائل شدہ ماڈل کا نام دیکھنا چاہیے (مثال کے طور پر، `llama-3.2`)۔

مرحلہ 3) کمپائل شدہ ماڈل چلائیں  
```cmd
foundry model run llama-3.2 --verbose
```
  
نوٹس:  
- کمپائل اور چلانے کے لیے کافی ڈسک اور RAM یقینی بنائیں  
- فلو کی تصدیق کے لیے چھوٹے ماڈلز سے شروع کریں، پھر اسکیل اپ کریں  

## حصہ 4: عملی ماڈل کیوریٹیشن (مرحلہ وار)

مرحلہ 1) ایک `models.json` رجسٹری بنائیں  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
مرحلہ 2) چھوٹا سلیکٹر اسکرپٹ  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## حصہ 5: ہینڈز آن بینچ مارکس (مرحلہ وار)

مرحلہ 1) سادہ تاخیر بینچ مارک  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
مرحلہ 2) معیار کی فوری جانچ  
- ایک مقررہ پرامپٹ سیٹ استعمال کریں، آؤٹ پٹس کو CSV/JSON میں کیپچر کریں  
- روانی، مطابقت، اور درستگی کو دستی طور پر ریٹ کریں (1–5)  

## حصہ 6: اگلے مراحل
- نئے ماڈلز اور تجاویز کے لیے Model Mondays کو سبسکرائب کریں: https://aka.ms/model-mondays  
- اپنی ٹیم کے `models.json` میں نتائج شامل کریں  
- سیشن 4 کے لیے تیاری کریں: LLMs بمقابلہ SLMs، مقامی بمقابلہ کلاؤڈ انفرنس، اور ہینڈز آن ڈیموز

---

