<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:14:40+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ur"
}
-->
# سیشن 3: اوپن سورس ماڈل کی دریافت اور انتظام

## جائزہ

یہ سیشن Foundry Local کے ساتھ عملی ماڈل دریافت اور انتظام پر مرکوز ہے۔ آپ سیکھیں گے کہ دستیاب ماڈلز کی فہرست کیسے بنائیں، مختلف اختیارات کو آزمائیں، اور بنیادی کارکردگی کی خصوصیات کو سمجھیں۔ یہ طریقہ کار Foundry CLI کے ساتھ عملی تجربے پر زور دیتا ہے تاکہ آپ اپنے استعمال کے کیسز کے لیے صحیح ماڈلز کا انتخاب کر سکیں۔

## سیکھنے کے مقاصد

- ماڈل دریافت اور انتظام کے لیے Foundry CLI کمانڈز میں مہارت حاصل کریں  
- ماڈل کیش اور مقامی اسٹوریج کے نمونوں کو سمجھیں  
- مختلف ماڈلز کو جلدی سے آزمائیں اور موازنہ کریں  
- ماڈل کے انتخاب اور بینچ مارکنگ کے لیے عملی ورک فلو قائم کریں  
- Foundry Local کے ذریعے دستیاب ماڈلز کے بڑھتے ہوئے ماحولیاتی نظام کو دریافت کریں  

## ضروریات

- سیشن 1: Foundry Local کے ساتھ شروعات مکمل کر لیا ہو  
- Foundry Local CLI انسٹال اور قابل رسائی ہو  
- ماڈل ڈاؤن لوڈ کے لیے کافی اسٹوریج اسپیس (ماڈلز 1GB سے 20GB+ تک ہو سکتے ہیں)  
- ماڈل کی اقسام اور استعمال کے کیسز کی بنیادی سمجھ  

## حصہ 6: عملی مشق

### مشق: ماڈل دریافت اور موازنہ

نمونہ 03 کی بنیاد پر اپنا ماڈل تشخیص اسکرپٹ بنائیں:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### آپ کا کام

1. **نمونہ 03 اسکرپٹ چلائیں**: `samples\03\list_and_bench.cmd`  
2. **مختلف ماڈلز آزمائیں**: کم از کم 3 مختلف ماڈلز کا تجربہ کریں  
3. **کارکردگی کا موازنہ کریں**: رفتار اور جواب کے معیار میں فرق نوٹ کریں  
4. **نتائج دستاویز کریں**: ایک سادہ موازنہ چارٹ بنائیں  

### مثال موازنہ فارمیٹ

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## حصہ 7: خرابیوں کا پتہ لگانا اور بہترین طریقے

### عام مسائل اور حل

**ماڈل شروع نہیں ہو رہا:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**ناکافی میموری:**
- چھوٹے ماڈلز سے شروع کریں (`phi-4-mini`)  
- دیگر ایپلیکیشنز بند کریں  
- اگر بار بار حدود پر پہنچ رہے ہیں تو RAM اپ گریڈ کریں  

**سست کارکردگی:**
- یقینی بنائیں کہ ماڈل مکمل طور پر لوڈ ہو چکا ہے (تفصیلی آؤٹ پٹ چیک کریں)  
- غیر ضروری پس منظر کی ایپلیکیشنز بند کریں  
- تیز اسٹوریج (SSD) پر غور کریں  

### بہترین طریقے

1. **چھوٹے سے شروع کریں**: سیٹ اپ کی توثیق کے لیے `phi-4-mini` سے آغاز کریں  
2. **ایک وقت میں ایک ماڈل**: نئے ماڈل شروع کرنے سے پہلے پچھلے ماڈلز بند کریں  
3. **وسائل کی نگرانی کریں**: میموری کے استعمال پر نظر رکھیں  
4. **مسلسل آزمائش کریں**: منصفانہ موازنہ کے لیے ایک جیسے پرامپٹس استعمال کریں  
5. **نتائج دستاویز کریں**: اپنے استعمال کے کیسز کے لیے ماڈل کی کارکردگی پر نوٹس رکھیں  

## حصہ 8: اگلے اقدامات اور حوالہ جات

### سیشن 4 کی تیاری

- **سیشن 4 کا فوکس**: اصلاح کے اوزار اور تکنیک  
- **ضروریات**: ماڈل سوئچنگ اور بنیادی کارکردگی کی جانچ میں مہارت  
- **تجویز کردہ**: اس سیشن سے 2-3 پسندیدہ ماڈلز کی شناخت کریں  

### اضافی وسائل

- **[Foundry Local دستاویزات](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: آفیشل دستاویزات  
- **[CLI حوالہ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: مکمل کمانڈ حوالہ  
- **[ماڈل منڈیز](https://aka.ms/model-mondays)**: ہفتہ وار ماڈل کی جھلکیاں  
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: کمیونٹی اور مسائل  
- **[نمونہ 03: ماڈل دریافت](samples/03/README.md)**: عملی مثال اسکرپٹ  

### اہم نکات

✅ **ماڈل دریافت**: دستیاب ماڈلز کو دریافت کرنے کے لیے `foundry model list` استعمال کریں  
✅ **جلدی آزمائش**: تیز تشخیص کے لیے `list_and_bench.cmd` پیٹرن  
✅ **کارکردگی کی نگرانی**: بنیادی وسائل کے استعمال اور جواب کے وقت کی پیمائش  
✅ **ماڈل کا انتخاب**: استعمال کے کیس کے مطابق ماڈلز منتخب کرنے کے لیے عملی رہنما اصول  
✅ **کیش مینجمنٹ**: اسٹوریج اور صفائی کے طریقہ کار کو سمجھنا  

اب آپ کے پاس Foundry Local کے آسان CLI طریقہ کار کا استعمال کرتے ہوئے اپنے AI ایپلیکیشنز کے لیے مناسب ماڈلز دریافت، آزمائش، اور منتخب کرنے کی عملی مہارتیں ہیں۔

---

## سیکھنے کے مقاصد

- مقامی انفرینس کے لیے اوپن سورس ماڈلز دریافت کریں اور ان کا جائزہ لیں  
- Foundry Local میں منتخب Hugging Face ماڈلز کو کمپائل اور چلائیں  
- درستگی، تاخیر، اور وسائل کی ضروریات کے لیے ماڈل انتخاب کی حکمت عملی اپنائیں  
- کیش اور ورژننگ کے ساتھ ماڈلز کو مقامی طور پر منظم کریں  

## حصہ 1: Foundry CLI کے ساتھ ماڈل دریافت

### بنیادی ماڈل انتظامی کمانڈز

Foundry CLI ماڈل دریافت اور انتظام کے لیے آسان کمانڈز فراہم کرتا ہے:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### اپنے پہلے ماڈلز چلانا

کارکردگی کی خصوصیات کو سمجھنے کے لیے مشہور، اچھی طرح سے آزمائے گئے ماڈلز سے آغاز کریں:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**نوٹ:** `--verbose` فلیگ تفصیلی اسٹارٹ اپ معلومات فراہم کرتا ہے، بشمول:  
- ماڈل ڈاؤن لوڈ کی پیش رفت (پہلی بار چلانے پر)  
- میموری مختص کی تفصیلات  
- سروس بائنڈنگ کی معلومات  
- کارکردگی کی ابتدائی میٹرکس  

### ماڈل کی اقسام کو سمجھنا

**چھوٹے لینگویج ماڈلز (SLMs):**
- `phi-4-mini`: تیز، موثر، عمومی چیٹ کے لیے بہترین  
- `phi-4`: بہتر استدلال کے ساتھ زیادہ قابل ورژن  

**درمیانے ماڈلز:**
- `qwen2.5-7b`: بہترین استدلال اور طویل سیاق و سباق  
- `deepseek-r1-7b`: کوڈ جنریشن کے لیے بہتر بنایا گیا  

**بڑے ماڈلز:**
- `llama-3.2`: Meta کا تازہ ترین اوپن سورس ماڈل  
- `qwen2.5-14b`: انٹرپرائز گریڈ استدلال  

## حصہ 2: جلدی ماڈل آزمائش اور موازنہ

### نمونہ 03 طریقہ: سادہ فہرست اور بینچ

ہمارے نمونہ 03 پیٹرن کی بنیاد پر، یہاں کم سے کم ورک فلو ہے:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### ماڈل کی کارکردگی کی جانچ

ایک ماڈل چلنے کے بعد، مستقل پرامپٹس کے ساتھ اس کا تجربہ کریں:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### پاور شیل آزمائش کا متبادل

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## حصہ 3: ماڈل کیش اور اسٹوریج کا انتظام

### ماڈل کیش کو سمجھنا

Foundry Local خود بخود ماڈل ڈاؤن لوڈ اور کیش کا انتظام کرتا ہے:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### ماڈل اسٹوریج کے تحفظات

**عام ماڈل سائزز:**
- `phi-4-mini`: ~2.5 GB  
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB  
- `llama-3.2`: ~4.9 GB  
- `qwen2.5-14b`: ~8.2 GB  

**اسٹوریج کے بہترین طریقے:**
- جلدی سوئچنگ کے لیے 2-3 ماڈلز کیش رکھیں  
- غیر استعمال شدہ ماڈلز کو جگہ خالی کرنے کے لیے ہٹائیں: `foundry cache clean`  
- ڈسک کے استعمال کی نگرانی کریں، خاص طور پر چھوٹے SSDs پر  
- ماڈل کے سائز اور صلاحیت کے درمیان توازن پر غور کریں  

### ماڈل کی کارکردگی کی نگرانی

ماڈلز چلتے وقت، سسٹم وسائل کی نگرانی کریں:

**ونڈوز ٹاسک مینیجر:**
- میموری کے استعمال پر نظر رکھیں (ماڈلز RAM میں لوڈ رہتے ہیں)  
- انفرینس کے دوران CPU کے استعمال کی نگرانی کریں  
- ابتدائی ماڈل لوڈنگ کے دوران ڈسک I/O چیک کریں  

**کمانڈ لائن نگرانی:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## حصہ 4: عملی ماڈل انتخاب کے رہنما اصول

### استعمال کے کیس کے مطابق ماڈلز کا انتخاب

**عمومی چیٹ اور سوال و جواب کے لیے:**
- آغاز کریں: `phi-4-mini` (تیز، موثر)  
- اپ گریڈ کریں: `phi-4` (بہتر استدلال)  
- ایڈوانسڈ: `qwen2.5-7b` (طویل سیاق و سباق)  

**کوڈ جنریشن کے لیے:**
- تجویز کردہ: `deepseek-r1-7b`  
- متبادل: `qwen2.5-7b` (کوڈ کے لیے بھی اچھا)  

**پیچیدہ استدلال کے لیے:**
- بہترین: `qwen2.5-7b` یا `qwen2.5-14b`  
- بجٹ آپشن: `phi-4`  

### ہارڈویئر ضروریات کی رہنمائی

**کم از کم سسٹم کی ضروریات:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**بہترین کارکردگی کے لیے تجویز کردہ:**
- 32GB+ RAM آرام دہ ملٹی ماڈل سوئچنگ کے لیے  
- تیز ماڈل لوڈنگ کے لیے SSD اسٹوریج  
- جدید CPU اچھے سنگل تھریڈ کی کارکردگی کے ساتھ  
- NPU سپورٹ (Windows 11 Copilot+ PCs) تیز رفتاری کے لیے  

### ماڈل سوئچنگ ورک فلو

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## حصہ 5: سادہ ماڈل بینچ مارکنگ

### بنیادی کارکردگی کی جانچ

ماڈل کی کارکردگی کا موازنہ کرنے کے لیے یہاں ایک سیدھا طریقہ ہے:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### دستی معیار کی تشخیص

ہر ماڈل کے لیے، مستقل پرامپٹس کے ساتھ تجربہ کریں اور دستی طور پر جائزہ لیں:

**ٹیسٹ پرامپٹس:**
1. "کوانٹم کمپیوٹنگ کو آسان الفاظ میں سمجھائیں۔"  
2. "ایک Python فنکشن لکھیں جو ایک فہرست کو ترتیب دے۔"  
3. "ریموٹ ورک کے فوائد اور نقصانات کیا ہیں؟"  
4. "ایج AI کے فوائد کا خلاصہ کریں۔"  

**تشخیص کے معیار:**
- **درستگی**: کیا معلومات درست ہیں؟  
- **وضاحت**: کیا وضاحت آسانی سے سمجھنے والی ہے؟  
- **مکملیت**: کیا یہ پورے سوال کو حل کرتا ہے؟  
- **رفتار**: جواب کتنی جلدی آتا ہے؟  

### وسائل کے استعمال کی نگرانی

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## حصہ 6: اگلے اقدامات

- نئے ماڈلز اور تجاویز کے لیے ماڈل منڈیز کو سبسکرائب کریں: https://aka.ms/model-mondays  
- اپنی ٹیم کے `models.json` میں نتائج شامل کریں  
- سیشن 4 کی تیاری کریں: LLMs بمقابلہ SLMs، مقامی بمقابلہ کلاؤڈ انفرینس، اور عملی ڈیمو

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔