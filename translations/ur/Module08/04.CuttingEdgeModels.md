<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T14:24:26+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ur"
}
-->
# سیشن 4: جدید ماڈلز – LLMs، SLMs، اور ڈیوائس پر انفرنس

## جائزہ

LLMs اور SLMs کا موازنہ کریں، مقامی اور کلاؤڈ انفرنس کے فوائد و نقصانات کا جائزہ لیں، اور EdgeAI کے منظرنامے کو ظاہر کرنے کے لیے Phi اور ONNX Runtime کے ساتھ ڈیمو بنائیں۔ ہم Chainlit RAG، WebGPU انفرنس آپشنز، اور Open WebUI انضمام کو بھی نمایاں کریں گے۔

حوالہ جات:
- Foundry Local دستاویزات: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI ہاؤ ٹو (چیٹ ایپ Open WebUI کے ساتھ): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## سیکھنے کے مقاصد
- LLM اور SLM کے درمیان لاگت، تاخیر، اور درستگی کے فوائد و نقصانات کو سمجھیں
- مخصوص کاروباری ضروریات کے لیے مقامی اور کلاؤڈ انفرنس کا انتخاب کریں
- Chainlit کے ساتھ ایک چھوٹا RAG ڈیمو نافذ کریں
- براؤزر میں تیز رفتار کے لیے WebGPU کو دریافت کریں
- Open WebUI کو Foundry Local سے جوڑیں

## حصہ 1: LLM بمقابلہ SLM – فیصلہ سازی میٹرکس

غور کریں:
- تاخیر: SLMs ڈیوائس پر اکثر سیکنڈ سے کم وقت میں جواب دیتے ہیں
- لاگت: مقامی انفرنس کلاؤڈ کے اخراجات کو کم کرتا ہے
- رازداری: حساس ڈیٹا ڈیوائس پر ہی رہتا ہے
- صلاحیت: LLMs پیچیدہ کاموں میں SLMs سے بہتر ہو سکتے ہیں
- قابل اعتماد: ہائبرڈ حکمت عملی ڈاؤن ٹائم کے خطرے کو کم کرتی ہے

## حصہ 2: مقامی بمقابلہ کلاؤڈ – ہائبرڈ پیٹرنز

- مقامی ترجیح کے ساتھ کلاؤڈ بیک اپ بڑے/پیچیدہ پرامپٹس کے لیے
- کلاؤڈ ترجیح کے ساتھ مقامی استعمال رازداری یا آف لائن منظرناموں کے لیے
- کام کی قسم کے لحاظ سے راستہ طے کریں (کوڈ جنریشن DeepSeek کو، عمومی چیٹ Phi/Qwen کو)

## حصہ 3: Chainlit کے ساتھ RAG چیٹ ایپ (کم سے کم)

ضروریات انسٹال کریں:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

چلائیں:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

توسیع کریں: ایک سادہ ریٹریور (مقامی فائلیں) شامل کریں اور صارف کے پرامپٹ سے پہلے حاصل کردہ سیاق و سباق کو شامل کریں۔

## حصہ 4: WebGPU انفرنس (خبردار)

WebGPU کا استعمال کرتے ہوئے چھوٹے ماڈلز کو براہ راست براؤزر میں چلائیں۔ یہ رازداری پر مبنی ڈیمو اور زیرو انسٹال تجربات کے لیے مثالی ہے۔ نیچے ONNX Runtime Web کے ساتھ WebGPU ایگزیکیوشن پرووائیڈر کا استعمال کرتے ہوئے ایک کم سے کم، مرحلہ وار مثال دی گئی ہے۔

1) WebGPU سپورٹ چیک کریں
- کرومیم براؤزرز: chrome://gpu → تصدیق کریں کہ “WebGPU” فعال ہے
- پروگراماتی چیک (ہم کوڈ میں بھی چیک کریں گے): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) ایک کم سے کم پروجیکٹ بنائیں
ایک فولڈر اور دو فائلیں بنائیں: `index.html` اور `main.js`۔

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) مقامی طور پر سرور کریں (Windows cmd.exe)
ایک سادہ جامد سرور استعمال کریں تاکہ براؤزر ماڈل کو حاصل کر سکے۔

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

اپنے براؤزر میں http://localhost:5173 کھولیں۔ آپ کو ابتدائی لاگز، WebGPU کے ساتھ سیشن تخلیق، اور ایک argmax پیش گوئی نظر آنی چاہیے۔

4) خرابیوں کا پتہ لگانا
- اگر WebGPU دستیاب نہیں ہے: Chrome/Edge کو اپ ڈیٹ کریں اور یقینی بنائیں کہ GPU ڈرائیورز موجودہ ہیں، پھر chrome://flags میں “Enable WebGPU” چیک کریں۔
- اگر CORS یا fetch کی غلطیاں ہوں: یقینی بنائیں کہ آپ فائلوں کو http:// (file:// نہیں) پر سرور کرتے ہیں اور ماڈل URL کراس اوریجن درخواستوں کی اجازت دیتا ہے۔
- CPU پر واپس جائیں: `executionProviders: ['wasm']` کو تبدیل کریں تاکہ بنیادی رویے کی تصدیق ہو۔

5) اگلے مراحل
- ایک ڈومین مخصوص ONNX ماڈل شامل کریں (مثلاً، امیج کلاسیفیکیشن یا ایک چھوٹا ٹیکسٹ ماڈل)۔
- حقیقی ان پٹس کے لیے پری پروسیسنگ/پوسٹ پروسیسنگ منطق شامل کریں۔
- بڑے ماڈلز یا پروڈکشن تاخیر کے لیے، Foundry Local یا ONNX Runtime Server کو ترجیح دیں۔

## حصہ 5: Open WebUI + Foundry Local (مرحلہ وار)

یہ Open WebUI کو Foundry Local کے OpenAI-compatible endpoint سے جوڑتا ہے تاکہ ایک مقامی چیٹ UI بنایا جا سکے۔

1) ضروریات
- Foundry Local انسٹال اور کام کر رہا ہو (`foundry --version`)
- ایک ماڈل مقامی طور پر چلانے کے لیے تیار ہو (مثلاً، `phi-4-mini`)
- Docker Desktop انسٹال ہو (Open WebUI کے لیے تجویز کردہ)

2) Foundry Local کے ساتھ ایک ماڈل شروع کریں
```powershell
foundry model run phi-4-mini
```
یہ ایک OpenAI-compatible API کو `http://localhost:8000` پر ظاہر کرتا ہے۔

3) Open WebUI شروع کریں (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
نوٹس:
- Windows پر، `host.docker.internal` کنٹینر کو آپ کے میزبان تک `localhost` پر پہنچنے دیتا ہے۔
- ہم نے `OPENAI_API_BASE_URL` کو Foundry Local کے endpoint اور ایک dummy `OPENAI_API_KEY` پر سیٹ کیا۔

4) Open WebUI UI سے ترتیب دیں (متبادل)
- http://localhost:3000 پر جائیں
- ابتدائی سیٹ اپ مکمل کریں (ایڈمن صارف)
- Settings → Models/Providers پر جائیں
- Base URL سیٹ کریں: `http://host.docker.internal:8000/v1`
- API Key سیٹ کریں: `local-key` (placeholder)
- محفوظ کریں

5) ایک ٹیسٹ پرامپٹ چلائیں
- Open WebUI چیٹ میں، ماڈل کا نام منتخب کریں یا درج کریں `phi-4-mini`
- پرامپٹ: “ڈیوائس پر AI انفرنس کے پانچ فوائد کی فہرست بنائیں۔”
- آپ کو اپنے مقامی ماڈل سے ایک جواب اسٹریم ہوتا ہوا نظر آنا چاہیے

6) خرابیوں کا پتہ لگانا
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) اختیاری: Open WebUI ڈیٹا کو برقرار رکھیں
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## عملی چیک لسٹ
- [ ] SLM اور LLM کے درمیان مقامی طور پر جوابات/تاخیر کا موازنہ کریں
- [ ] Chainlit ڈیمو کو کم از کم دو ماڈلز کے خلاف چلائیں
- [ ] Open WebUI کو اپنے مقامی endpoint سے جوڑیں اور ٹیسٹ کریں

## اگلے مراحل
- سیشن 5 میں ایجنٹ ورک فلو کے لیے تیاری کریں
- ایسے منظرنامے کی شناخت کریں جہاں ہائبرڈ مقامی/کلاؤڈ ROI کو بہتر بناتا ہے

---

