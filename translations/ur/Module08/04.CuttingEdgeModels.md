<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:15:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ur"
}
-->
# سیشن 4: چینلٹ کے ساتھ پروڈکشن چیٹ ایپلیکیشنز بنانا

## جائزہ

یہ سیشن چینلٹ اور مائیکروسافٹ فاؤنڈری لوکل کے ذریعے پروڈکشن کے لیے تیار چیٹ ایپلیکیشنز بنانے پر مرکوز ہے۔ آپ جدید ویب انٹرفیسز بنانا سیکھیں گے جو AI گفتگو کے لیے موزوں ہوں، اسٹریمنگ جوابات نافذ کریں گے، اور مضبوط چیٹ ایپلیکیشنز کو بہتر ایرر ہینڈلنگ اور صارف کے تجربے کے ڈیزائن کے ساتھ تعینات کریں گے۔

**آپ کیا بنائیں گے:**
- **چینلٹ چیٹ ایپ**: جدید ویب UI کے ساتھ اسٹریمنگ جوابات
- **ویب جی پی یو ڈیمو**: براؤزر پر مبنی انفرینس پرائیویسی فرسٹ ایپلیکیشنز کے لیے  
- **اوپن ویب UI انٹیگریشن**: فاؤنڈری لوکل کے ساتھ پروفیشنل چیٹ انٹرفیس
- **پروڈکشن پیٹرنز**: ایرر ہینڈلنگ، مانیٹرنگ، اور تعیناتی کی حکمت عملی

## سیکھنے کے مقاصد

- چینلٹ کے ساتھ پروڈکشن کے لیے تیار چیٹ ایپلیکیشنز بنائیں
- صارف کے تجربے کو بہتر بنانے کے لیے اسٹریمنگ جوابات نافذ کریں
- فاؤنڈری لوکل SDK انٹیگریشن پیٹرنز میں مہارت حاصل کریں
- مناسب ایرر ہینڈلنگ اور گریسفل ڈیگریڈیشن کا اطلاق کریں
- مختلف ماحول کے لیے چیٹ ایپلیکیشنز کو تعینات اور ترتیب دیں
- گفتگو AI کے لیے جدید ویب UI پیٹرنز کو سمجھیں

## ضروریات

- **فاؤنڈری لوکل**: انسٹال اور چل رہا ہو ([انسٹالیشن گائیڈ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **پائتھون**: 3.10 یا اس سے جدید ورژن کے ساتھ ورچوئل ماحول کی صلاحیت
- **ماڈل**: کم از کم ایک ماڈل لوڈ کیا ہوا (`foundry model run phi-4-mini`)
- **براؤزر**: جدید ویب براؤزر جس میں ویب جی پی یو سپورٹ ہو (کروم/ایج)
- **ڈوکر**: اوپن ویب UI انٹیگریشن کے لیے (اختیاری)

## حصہ 1: جدید چیٹ ایپلیکیشنز کو سمجھنا

### آرکیٹیکچر کا جائزہ

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### کلیدی ٹیکنالوجیز

**فاؤنڈری لوکل SDK پیٹرنز:**
- `FoundryLocalManager(alias)`: خودکار سروس مینجمنٹ
- `manager.endpoint` اور `manager.api_key`: کنکشن کی تفصیلات
- `manager.get_model_info(alias).id`: ماڈل کی شناخت

**چینلٹ فریم ورک:**
- `@cl.on_chat_start`: چیٹ سیشنز کو شروع کریں
- `@cl.on_message`: صارف کے آنے والے پیغامات کو ہینڈل کریں  
- `cl.Message().stream_token()`: حقیقی وقت میں اسٹریمنگ
- خودکار UI جنریشن اور ویب ساکٹ مینجمنٹ

## حصہ 2: لوکل بمقابلہ کلاؤڈ فیصلہ میٹرکس

### کارکردگی کی خصوصیات

| پہلو | لوکل (فاؤنڈری) | کلاؤڈ (Azure OpenAI) |
|--------|-----------------|---------------------|
| **لیٹنسی** | 🚀 50-200ms (کوئی نیٹ ورک نہیں) | ⏱️ 200-2000ms (نیٹ ورک پر منحصر) |
| **پرائیویسی** | 🔒 ڈیٹا کبھی بھی ڈیوائس سے باہر نہیں جاتا | ⚠️ ڈیٹا کلاؤڈ کو بھیجا جاتا ہے |
| **لاگت** | 💰 ہارڈویئر کے بعد مفت | 💸 ہر ٹوکن پر ادائیگی |
| **آف لائن** | ✅ انٹرنیٹ کے بغیر کام کرتا ہے | ❌ انٹرنیٹ کی ضرورت ہے |
| **ماڈل سائز** | ⚠️ ہارڈویئر کی حد تک محدود | ✅ سب سے بڑے ماڈلز تک رسائی |
| **اسکیلنگ** | ⚠️ ہارڈویئر پر منحصر | ✅ لامحدود اسکیلنگ |

### ہائبرڈ حکمت عملی کے پیٹرنز

**لوکل-فرسٹ کے ساتھ فال بیک:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**ٹاسک پر مبنی روٹنگ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## حصہ 3: سیمپل 04 - چینلٹ چیٹ ایپلیکیشن

### فوری آغاز

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

ایپلیکیشن خود بخود `http://localhost:8080` پر ایک جدید چیٹ انٹرفیس کے ساتھ کھلتی ہے۔

### بنیادی نفاذ

سیمپل 04 ایپلیکیشن پروڈکشن کے لیے تیار پیٹرنز کو ظاہر کرتی ہے:

**خودکار سروس دریافت:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**اسٹریمنگ چیٹ ہینڈلر:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### کنفیگریشن کے اختیارات

**ماحولیاتی متغیرات:**

| متغیر | وضاحت | ڈیفالٹ | مثال |
|----------|-------------|---------|----------|
| `MODEL` | استعمال کرنے کے لیے ماڈل کا عرف | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | فاؤنڈری لوکل اینڈ پوائنٹ | خودکار طور پر پتہ لگایا گیا | `http://localhost:51211` |
| `API_KEY` | API کلید (لوکل کے لیے اختیاری) | `""` | `your-api-key` |

**ایڈوانسڈ استعمال:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## حصہ 4: جیوپیٹر نوٹ بکس بنانا اور استعمال کرنا

### نوٹ بک سپورٹ کا جائزہ

سیمپل 04 میں ایک جامع جیوپیٹر نوٹ بک (`chainlit_app.ipynb`) شامل ہے جو فراہم کرتی ہے:

- **📚 تعلیمی مواد**: مرحلہ وار سیکھنے کے مواد
- **🔬 انٹرایکٹو ایکسپلوریشن**: کوڈ سیلز کو چلائیں اور تجربہ کریں
- **📊 بصری مظاہرے**: چارٹس، ڈایاگرامز، اور آؤٹ پٹ کی بصری نمائندگی
- **🛠️ ترقیاتی ٹولز**: ٹیسٹنگ اور ڈیبگنگ کی صلاحیتیں

### اپنی نوٹ بکس بنانا

#### مرحلہ 1: جیوپیٹر ماحول ترتیب دیں

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### مرحلہ 2: نئی نوٹ بک بنائیں

**VS کوڈ استعمال کرتے ہوئے:**
1. VS کوڈ کو Module08 ڈائریکٹری میں کھولیں
2. `.ipynb` ایکسٹینشن کے ساتھ نئی فائل بنائیں
3. "فاؤنڈری لوکل" کرنل منتخب کریں جب پوچھا جائے
4. اپنے مواد کے ساتھ سیلز شامل کرنا شروع کریں

**جیوپیٹر لیب استعمال کرتے ہوئے:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### نوٹ بک کی ساخت کے بہترین طریقے

#### سیل آرگنائزیشن

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### انٹرایکٹو مثالیں اور مشقیں

#### مشق 1: کلائنٹ کنفیگریشن ٹیسٹنگ

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### مشق 2: اسٹریمنگ رسپانس سیمولیشن

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## حصہ 5: ویب جی پی یو براؤزر انفرینس ڈیمو

### جائزہ

ویب جی پی یو AI ماڈلز کو براہ راست براؤزر میں چلانے کی اجازت دیتا ہے تاکہ زیادہ سے زیادہ پرائیویسی اور زیرو انسٹال تجربات فراہم کیے جا سکیں۔ یہ سیمپل ONNX رن ٹائم ویب کے ساتھ ویب جی پی یو ایکزیکیوشن کو ظاہر کرتا ہے۔

### مرحلہ 1: ویب جی پی یو سپورٹ چیک کریں

**براؤزر کی ضروریات:**
- کروم/ایج 113+ ویب جی پی یو کے ساتھ فعال
- چیک کریں: `chrome://gpu` → "WebGPU" اسٹیٹس کی تصدیق کریں
- پروگراماتی چیک: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### مرحلہ 2: ویب جی پی یو ڈیمو بنائیں

ڈائریکٹری بنائیں: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### مرحلہ 3: ڈیمو چلائیں

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## حصہ 6: اوپن ویب UI انٹیگریشن

### جائزہ

اوپن ویب UI ایک پروفیشنل ChatGPT جیسا انٹرفیس فراہم کرتا ہے جو فاؤنڈری لوکل کے اوپن AI-کمپیٹیبل API سے جڑتا ہے۔

### مرحلہ 1: ضروریات

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### مرحلہ 2: ڈوکر سیٹ اپ (تجویز کردہ)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**نوٹ:** `host.docker.internal` ونڈوز پر ڈوکر کنٹینرز کو میزبان مشین تک رسائی کی اجازت دیتا ہے۔

### مرحلہ 3: کنفیگریشن

1. **براؤزر کھولیں:** `http://localhost:3000` پر جائیں
2. **ابتدائی سیٹ اپ:** ایڈمن اکاؤنٹ بنائیں
3. **ماڈل کنفیگریشن:**
   - سیٹنگز → ماڈلز → اوپن AI API  
   - بیس URL: `http://host.docker.internal:51211/v1`
   - API کلید: `foundry-local-key` (کوئی بھی ویلیو کام کرے گی)
4. **کنکشن ٹیسٹ کریں:** ماڈلز ڈراپ ڈاؤن میں ظاہر ہونے چاہئیں

### خرابیوں کا سراغ لگانا

**عام مسائل:**

1. **کنکشن ریفیوزڈ:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **ماڈلز ظاہر نہیں ہو رہے:**
   - تصدیق کریں کہ ماڈل لوڈ ہے: `foundry model list`
   - API رسپانس چیک کریں: `curl http://localhost:51211/v1/models`
   - اوپن ویب UI کنٹینر کو دوبارہ شروع کریں

## حصہ 7: پروڈکشن تعیناتی کے خیالات

### ماحولیاتی کنفیگریشن

**ترقیاتی سیٹ اپ:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**پروڈکشن تعیناتی:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### عام پورٹ مسائل اور حل

**پورٹ 51211 تنازعہ کی روک تھام:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### کارکردگی کی نگرانی

**ہیلتھ چیک نفاذ:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## خلاصہ

سیشن 4 میں چینلٹ ایپلیکیشنز کے لیے پروڈکشن کے لیے تیار چیٹ ایپلیکیشنز بنانے کا احاطہ کیا گیا۔ آپ نے سیکھا:

- ✅ **چینلٹ فریم ورک**: چیٹ ایپلیکیشنز کے لیے جدید UI اور اسٹریمنگ سپورٹ
- ✅ **فاؤنڈری لوکل انٹیگریشن**: SDK استعمال اور کنفیگریشن پیٹرنز  
- ✅ **ویب جی پی یو انفرینس**: زیادہ سے زیادہ پرائیویسی کے لیے براؤزر پر مبنی AI
- ✅ **اوپن ویب UI سیٹ اپ**: پروفیشنل چیٹ انٹرفیس تعیناتی
- ✅ **پروڈکشن پیٹرنز**: ایرر ہینڈلنگ، مانیٹرنگ، اور اسکیلنگ

سیمپل 04 ایپلیکیشن بہترین طریقوں کو ظاہر کرتی ہے جو مضبوط چیٹ انٹرفیسز بنانے کے لیے مقامی AI ماڈلز کو مائیکروسافٹ فاؤنڈری لوکل کے ذریعے استعمال کرتی ہے اور بہترین صارف تجربہ فراہم کرتی ہے۔

## حوالہ جات

- **[سیمپل 04: چینلٹ ایپلیکیشن](samples/04/README.md)**: مکمل ایپلیکیشن کے ساتھ دستاویزات
- **[چینلٹ تعلیمی نوٹ بک](samples/04/chainlit_app.ipynb)**: انٹرایکٹو سیکھنے کے مواد
- **[فاؤنڈری لوکل دستاویزات](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: مکمل پلیٹ فارم دستاویزات
- **[چینلٹ دستاویزات](https://docs.chainlit.io/)**: آفیشل فریم ورک دستاویزات
- **[اوپن ویب UI انٹیگریشن گائیڈ](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: آفیشل ٹیوٹوریل

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔