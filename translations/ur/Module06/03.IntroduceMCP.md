<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T17:36:59+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ur"
}
-->
# سیکشن 03 - ماڈل کانٹیکسٹ پروٹوکول (MCP) انضمام

## MCP (ماڈل کانٹیکسٹ پروٹوکول) کا تعارف

ماڈل کانٹیکسٹ پروٹوکول (MCP) ایک انقلابی فریم ورک ہے جو زبان کے ماڈلز کو بیرونی ٹولز اور سسٹمز کے ساتھ معیاری طریقے سے تعامل کرنے کی اجازت دیتا ہے۔ روایتی طریقوں کے برعکس جہاں ماڈلز الگ تھلگ ہوتے ہیں، MCP ایک واضح پروٹوکول کے ذریعے AI ماڈلز اور حقیقی دنیا کے درمیان پل بناتا ہے۔

### MCP کیا ہے؟

MCP ایک مواصلاتی پروٹوکول کے طور پر کام کرتا ہے جو زبان کے ماڈلز کو درج ذیل کام کرنے کی اجازت دیتا ہے:
- بیرونی ڈیٹا ذرائع سے جڑنا
- ٹولز اور فنکشنز کو چلانا
- APIs اور سروسز کے ساتھ تعامل کرنا
- حقیقی وقت کی معلومات تک رسائی حاصل کرنا
- پیچیدہ کثیر مرحلہ آپریشنز انجام دینا

یہ پروٹوکول جامد زبان کے ماڈلز کو متحرک ایجنٹس میں تبدیل کرتا ہے جو متن کی تخلیق سے آگے عملی کام انجام دے سکتے ہیں۔

## چھوٹے زبان کے ماڈلز (SLMs) MCP میں

چھوٹے زبان کے ماڈلز AI کی تعیناتی کے لیے ایک مؤثر طریقہ پیش کرتے ہیں، جو کئی فوائد فراہم کرتے ہیں:

### SLMs کے فوائد
- **وسائل کی بچت**: کم کمپیوٹیشنل ضروریات  
- **تیز ردعمل کا وقت**: حقیقی وقت کی ایپلیکیشنز کے لیے کم تاخیر  
- **کم خرچ**: کم سے کم انفراسٹرکچر کی ضرورت  
- **پرائیویسی**: مقامی طور پر چل سکتے ہیں بغیر ڈیٹا ٹرانسمیشن کے  
- **حسب ضرورت**: مخصوص شعبوں کے لیے آسانی سے بہتر بنایا جا سکتا ہے  

### MCP کے ساتھ SLMs کیوں اچھے کام کرتے ہیں؟

SLMs اور MCP کا امتزاج ایک طاقتور امتزاج بناتا ہے جہاں ماڈل کی استدلال کی صلاحیتیں بیرونی ٹولز کے ذریعے بڑھائی جاتی ہیں، چھوٹے پیرامیٹرز کی تعداد کو بہتر فعالیت کے ذریعے پورا کیا جاتا ہے۔

## Python MCP SDK کا جائزہ

Python MCP SDK MCP-فعال ایپلیکیشنز بنانے کے لیے بنیاد فراہم کرتا ہے۔ SDK میں شامل ہیں:

- **کلائنٹ لائبریریاں**: MCP سرورز سے جڑنے کے لیے  
- **سرور فریم ورک**: حسب ضرورت MCP سرورز بنانے کے لیے  
- **پروٹوکول ہینڈلرز**: مواصلات کا انتظام کرنے کے لیے  
- **ٹول انضمام**: بیرونی فنکشنز کو چلانے کے لیے  

## عملی نفاذ: Phi-4 MCP کلائنٹ

آئیے Microsoft کے Phi-4 منی ماڈل کے ساتھ MCP صلاحیتوں کے انضمام کا ایک حقیقی دنیا کا نفاذ دیکھتے ہیں۔

### سسٹم آرکیٹیکچر

نفاذ ایک پرت دار آرکیٹیکچر کی پیروی کرتا ہے:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### بنیادی اجزاء

#### 1. MCP کلائنٹ کلاسز

**BaseMCPClient**: عام فعالیت فراہم کرنے کی تجریدی بنیاد  
- غیر متزامن کانٹیکسٹ مینیجر پروٹوکول  
- معیاری انٹرفیس کی تعریف  
- وسائل کا انتظام  

**Phi4MiniMCPClient**: STDIO پر مبنی نفاذ  
- مقامی عمل مواصلات  
- معیاری ان پٹ/آؤٹ پٹ ہینڈلنگ  
- سب پروسیس کا انتظام  

**Phi4MiniSSEMCPClient**: سرور-سینٹ ایونٹس نفاذ  
- HTTP اسٹریمنگ مواصلات  
- حقیقی وقت کے ایونٹس کا انتظام  
- ویب پر مبنی سرور کنیکٹیویٹی  

#### 2. LLM انضمام

**OllamaClient**: مقامی ماڈل ہوسٹنگ  
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: اعلی کارکردگی سرونگ  
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. ٹول پروسیسنگ پائپ لائن

ٹول پروسیسنگ پائپ لائن MCP ٹولز کو زبان کے ماڈلز کے ساتھ مطابقت پذیر فارمیٹس میں تبدیل کرتی ہے:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## شروعات: مرحلہ وار گائیڈ

### مرحلہ 1: ماحول کی ترتیب

ضروری انحصارات انسٹال کریں:  
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### مرحلہ 2: بنیادی ترتیب

اپنے ماحول کے متغیرات ترتیب دیں:  
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### مرحلہ 3: اپنا پہلا MCP کلائنٹ چلائیں

**بنیادی Ollama ترتیب:**  
```bash
python ghmodel_mcp_demo.py
```

**vLLM بیک اینڈ استعمال کرنا:**  
```bash
python ghmodel_mcp_demo.py --env vllm
```

**سرور-سینٹ ایونٹس کنکشن:**  
```bash
python ghmodel_mcp_demo.py --run sse
```

**حسب ضرورت MCP سرور:**  
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### مرحلہ 4: پروگراماتی استعمال

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## جدید خصوصیات

### ملٹی-بیک اینڈ سپورٹ

نفاذ Ollama اور vLLM دونوں بیک اینڈز کی حمایت کرتا ہے، جو آپ کی ضروریات کے مطابق انتخاب کی اجازت دیتا ہے:

- **Ollama**: مقامی ترقی اور ٹیسٹنگ کے لیے بہتر  
- **vLLM**: پروڈکشن اور اعلی throughput کے منظرناموں کے لیے بہتر  

### لچکدار کنکشن پروٹوکولز

دو کنکشن موڈز کی حمایت کی جاتی ہے:

**STDIO موڈ**: براہ راست عمل مواصلات  
- کم تاخیر  
- مقامی ٹولز کے لیے موزوں  
- سادہ ترتیب  

**SSE موڈ**: HTTP پر مبنی اسٹریمنگ  
- نیٹ ورک کے قابل  
- تقسیم شدہ سسٹمز کے لیے بہتر  
- حقیقی وقت کی اپ ڈیٹس  

### ٹول انضمام کی صلاحیتیں

سسٹم مختلف ٹولز کے ساتھ انضمام کر سکتا ہے:
- ویب آٹومیشن (Playwright)  
- فائل آپریشنز  
- API تعاملات  
- سسٹم کمانڈز  
- حسب ضرورت فنکشنز  

## خرابیوں کا انتظام اور بہترین طریقے

### جامع خرابی کا انتظام

نفاذ میں درج ذیل کے لیے مضبوط خرابی کا انتظام شامل ہے:

**کنکشن کی خرابیاں:**  
- MCP سرور کی ناکامیاں  
- نیٹ ورک ٹائم آؤٹس  
- کنیکٹیویٹی کے مسائل  

**ٹول نفاذ کی خرابیاں:**  
- گمشدہ ٹولز  
- پیرامیٹر کی توثیق  
- نفاذ کی ناکامیاں  

**جواب پروسیسنگ کی خرابیاں:**  
- JSON پارسنگ کے مسائل  
- فارمیٹ کی بے ضابطگیاں  
- LLM جواب کی غیر معمولیات  

### بہترین طریقے

1. **وسائل کا انتظام:** غیر متزامن کانٹیکسٹ مینیجرز استعمال کریں  
2. **خرابی کا انتظام:** جامع try-catch بلاکس نافذ کریں  
3. **لاگنگ:** مناسب لاگنگ لیولز فعال کریں  
4. **سیکیورٹی:** ان پٹ کی توثیق کریں اور آؤٹ پٹ کو صاف کریں  
5. **کارکردگی:** کنکشن پولنگ اور کیشنگ استعمال کریں  

## حقیقی دنیا کی ایپلیکیشنز

### ویب آٹومیشن  
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### ڈیٹا پروسیسنگ  
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API انضمام  
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## کارکردگی کی اصلاح

### میموری کا انتظام  
- مؤثر پیغام کی تاریخ کا انتظام  
- مناسب وسائل کی صفائی  
- کنکشن پولنگ  

### نیٹ ورک کی اصلاح  
- غیر متزامن HTTP آپریشنز  
- قابل ترتیب ٹائم آؤٹس  
- خرابی کی شائستہ بازیابی  

### ہم وقتی پروسیسنگ  
- غیر مسدود I/O  
- متوازی ٹول نفاذ  
- مؤثر غیر متزامن پیٹرنز  

## سیکیورٹی کے تحفظات

### ڈیٹا کا تحفظ  
- محفوظ API کلید کا انتظام  
- ان پٹ کی توثیق  
- آؤٹ پٹ کی صفائی  

### نیٹ ورک سیکیورٹی  
- HTTPS کی حمایت  
- مقامی اینڈ پوائنٹ ڈیفالٹس  
- محفوظ ٹوکن کا انتظام  

### نفاذ کی حفاظت  
- ٹول فلٹرنگ  
- سینڈ باکسڈ ماحول  
- آڈٹ لاگنگ  

## نتیجہ

MCP کے ساتھ SLMs AI ایپلیکیشن ڈویلپمنٹ میں ایک نیا رجحان پیش کرتے ہیں۔ چھوٹے ماڈلز کی کارکردگی کو بیرونی ٹولز کی طاقت کے ساتھ جوڑ کر، ڈویلپرز ایسے ذہین سسٹمز بنا سکتے ہیں جو وسائل کے لحاظ سے مؤثر اور انتہائی قابل ہوں۔

Phi-4 MCP کلائنٹ نفاذ یہ ظاہر کرتا ہے کہ یہ انضمام عملی طور پر کیسے حاصل کیا جا سکتا ہے، ایک مضبوط بنیاد فراہم کرتے ہوئے ذہین AI-فعال ایپلیکیشنز بنانے کے لیے۔

اہم نکات:
- MCP زبان کے ماڈلز اور بیرونی سسٹمز کے درمیان خلا کو پُر کرتا ہے  
- SLMs ٹولز کے ساتھ بڑھائے جانے پر صلاحیت کو قربان کیے بغیر مؤثر کارکردگی پیش کرتے ہیں  
- ماڈیولر آرکیٹیکچر آسان توسیع اور حسب ضرورت کو ممکن بناتا ہے  
- پروڈکشن کے استعمال کے لیے مناسب خرابی کا انتظام اور سیکیورٹی اقدامات ضروری ہیں  

یہ ٹیوٹوریل آپ کو اپنی SLM-فعال MCP ایپلیکیشنز بنانے کی بنیاد فراہم کرتا ہے، آٹومیشن، ڈیٹا پروسیسنگ، اور ذہین سسٹم انضمام کے امکانات کو کھولتا ہے۔

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔