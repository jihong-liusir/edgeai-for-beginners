<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T18:14:57+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "ur"
}
-->
# کنٹینرائزڈ کلاؤڈ ڈیپلائمنٹ - پروڈکشن اسکیل حل

یہ جامع ٹیوٹوریل مائیکروسافٹ کے Phi-4-mini-instruct ماڈل کو کنٹینرائزڈ ماحول میں ڈیپلائمنٹ کے تین اہم طریقوں پر مشتمل ہے: vLLM، Ollama، اور SLM Engine with ONNX Runtime۔ یہ 3.8B پیرامیٹر ماڈل منطقی کاموں کے لیے ایک بہترین انتخاب ہے جبکہ ایج ڈیپلائمنٹ کے لیے کارکردگی کو برقرار رکھتا ہے۔

## فہرست مضامین

1. [Phi-4-mini کنٹینر ڈیپلائمنٹ کا تعارف](../../../Module03)
2. [سیکھنے کے مقاصد](../../../Module03)
3. [Phi-4-mini کی درجہ بندی کو سمجھنا](../../../Module03)
4. [vLLM کنٹینر ڈیپلائمنٹ](../../../Module03)
5. [Ollama کنٹینر ڈیپلائمنٹ](../../../Module03)
6. [SLM Engine with ONNX Runtime](../../../Module03)
7. [موازنہ فریم ورک](../../../Module03)
8. [بہترین طریقے](../../../Module03)

## Phi-4-mini کنٹینر ڈیپلائمنٹ کا تعارف

چھوٹے لینگویج ماڈلز (SLMs) EdgeAI میں ایک اہم پیش رفت کی نمائندگی کرتے ہیں، جو محدود وسائل والے آلات پر جدید قدرتی زبان پروسیسنگ کی صلاحیتیں فراہم کرتے ہیں۔ یہ ٹیوٹوریل مائیکروسافٹ کے Phi-4-mini-instruct کے کنٹینرائزڈ ڈیپلائمنٹ حکمت عملیوں پر مرکوز ہے، جو ایک جدید ترین منطقی ماڈل ہے جو صلاحیت اور کارکردگی کے درمیان توازن برقرار رکھتا ہے۔

### نمایاں ماڈل: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B پیرامیٹرز)**: مائیکروسافٹ کا تازہ ترین ہلکا پھلکا انسٹرکشن ٹیونڈ ماڈل، جو میموری/کمپیوٹ محدود ماحول کے لیے ڈیزائن کیا گیا ہے اور درج ذیل میں غیر معمولی صلاحیتیں فراہم کرتا ہے:
- **ریاضیاتی منطق اور پیچیدہ حسابات**
- **کوڈ جنریشن، ڈیبگنگ، اور تجزیہ**
- **منطقی مسئلہ حل کرنا اور مرحلہ وار استدلال**
- **تعلیمی ایپلیکیشنز جن میں تفصیلی وضاحتیں درکار ہیں**
- **فنکشن کالنگ اور ٹول انٹیگریشن**

"چھوٹے SLMs" کیٹیگری (1.5B - 13.9B پیرامیٹرز) کا حصہ، Phi-4-mini منطق کی صلاحیت اور وسائل کی کارکردگی کے درمیان ایک بہترین توازن فراہم کرتا ہے۔

### کنٹینرائزڈ Phi-4-mini ڈیپلائمنٹ کے فوائد

- **آپریشنل کارکردگی**: کم کمپیوٹیشنل ضروریات کے ساتھ منطقی کاموں کے لیے تیز انفرنس
- **ڈیپلائمنٹ کی لچک**: لوکل پروسیسنگ کے ذریعے بہتر پرائیویسی کے ساتھ آن ڈیوائس AI صلاحیتیں
- **لاگت کی بچت**: بڑے ماڈلز کے مقابلے میں کم آپریشنل اخراجات جبکہ معیار کو برقرار رکھتے ہوئے
- **تنہائی**: ماڈل انسٹینسز کے درمیان صاف علیحدگی اور محفوظ ایگزیکیوشن ماحول
- **اسکیل ایبلٹی**: منطقی کاموں کے لیے بڑھتی ہوئی تھروپٹ کے لیے آسان افقی اسکیلنگ

## سیکھنے کے مقاصد

اس ٹیوٹوریل کے اختتام تک، آپ درج ذیل کرنے کے قابل ہوں گے:

- Phi-4-mini-instruct کو مختلف کنٹینرائزڈ ماحول میں ڈیپلائمنٹ اور بہتر بنانا
- مختلف ڈیپلائمنٹ منظرناموں کے لیے جدید کوانٹائزیشن اور کمپریشن حکمت عملیوں کو نافذ کرنا
- منطقی کاموں کے لیے پروڈکشن ریڈی کنٹینر آرکیسٹریشن کو ترتیب دینا
- مخصوص استعمال کے کیس کی ضروریات کی بنیاد پر مناسب ڈیپلائمنٹ فریم ورک کا انتخاب اور جائزہ لینا
- کنٹینرائزڈ SLM ڈیپلائمنٹ کے لیے سیکیورٹی، مانیٹرنگ، اور اسکیلنگ کے بہترین طریقے اپنانا

## Phi-4-mini کی درجہ بندی کو سمجھنا

### ماڈل کی تفصیلات

**تکنیکی تفصیلات:**
- **پیرامیٹرز**: 3.8 بلین (چھوٹے SLM کیٹیگری)
- **آرکیٹیکچر**: ڈینس ڈیکوڈر-اونلی ٹرانسفارمر گروپڈ-کوئری اٹینشن کے ساتھ
- **کانٹیکسٹ لینتھ**: 128K ٹوکنز (32K بہترین کارکردگی کے لیے تجویز کردہ)
- **ووکیبلری**: 200K ٹوکنز ملٹی لنگوئل سپورٹ کے ساتھ
- **ٹریننگ ڈیٹا**: 5T ٹوکنز اعلیٰ معیار کے منطقی مواد پر مشتمل

### وسائل کی ضروریات

| ڈیپلائمنٹ کی قسم | کم از کم RAM | تجویز کردہ RAM | VRAM (GPU) | اسٹوریج | عام استعمال کے کیسز |
|------------------|-------------|----------------|------------|---------|---------------------|
| **ڈیولپمنٹ** | 6GB | 8GB | - | 8GB | لوکل ٹیسٹنگ، پروٹوٹائپنگ |
| **پروڈکشن CPU** | 8GB | 12GB | - | 10GB | ایج سرورز، لاگت کے لحاظ سے بہتر ڈیپلائمنٹ |
| **پروڈکشن GPU** | 6GB | 8GB | 4-6GB | 8GB | ہائی تھروپٹ منطقی خدمات |
| **ایج آپٹمائزڈ** | 4GB | 6GB | - | 6GB | کوانٹائزڈ ڈیپلائمنٹ، IoT گیٹ ویز |

### Phi-4-mini کی صلاحیتیں

- **ریاضیاتی مہارت**: ایڈوانسڈ اریتھمیٹک، الجبرا، اور کیلکولس مسئلہ حل کرنا
- **کوڈ انٹیلیجنس**: Python، JavaScript، اور ملٹی لینگویج کوڈ جنریشن ڈیبگنگ کے ساتھ
- **منطقی استدلال**: مرحلہ وار مسئلہ کی تقسیم اور حل کی تعمیر
- **تعلیمی معاونت**: تفصیلی وضاحتیں جو سیکھنے اور تدریس کے لیے موزوں ہیں
- **فنکشن کالنگ**: ٹول انٹیگریشن اور API انٹریکشن کے لیے نیٹو سپورٹ

## vLLM کنٹینر ڈیپلائمنٹ

vLLM Phi-4-mini-instruct کے لیے بہترین سپورٹ فراہم کرتا ہے، جس میں بہتر انفرنس کارکردگی اور OpenAI کے موافق APIs شامل ہیں، جو پروڈکشن منطقی خدمات کے لیے مثالی ہیں۔

### فوری آغاز کی مثالیں

#### بنیادی CPU ڈیپلائمنٹ (ڈیولپمنٹ)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-تیز پروڈکشن ڈیپلائمنٹ
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### پروڈکشن کنفیگریشن

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Phi-4-mini کی منطقی صلاحیتوں کی جانچ

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama کنٹینر ڈیپلائمنٹ

Ollama Phi-4-mini-instruct کے لیے بہترین سپورٹ فراہم کرتا ہے، جس میں سادہ ڈیپلائمنٹ اور مینجمنٹ شامل ہے، جو ڈیولپمنٹ اور متوازن پروڈکشن ڈیپلائمنٹ کے لیے موزوں ہے۔

### فوری سیٹ اپ

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### پروڈکشن کنفیگریشن

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### ماڈل کی اصلاح اور ویریئنٹس

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API استعمال کی مثالیں

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine with ONNX Runtime

ONNX Runtime Phi-4-mini-instruct کے ایج ڈیپلائمنٹ کے لیے بہترین کارکردگی فراہم کرتا ہے، جس میں جدید اصلاح اور کراس پلیٹ فارم مطابقت شامل ہے۔

### بنیادی سیٹ اپ

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### سادہ سرور نفاذ

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### ماڈل کنورژن اسکرپٹ

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### پروڈکشن کنفیگریشن

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ONNX ڈیپلائمنٹ کی جانچ

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## موازنہ فریم ورک

### Phi-4-mini کے لیے فریم ورک کا موازنہ

| خصوصیت | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **سیٹ اپ کی پیچیدگی** | درمیانی | آسان | پیچیدہ |
| **کارکردگی (GPU)** | بہترین (~25 ٹوکنز/سیکنڈ) | بہت اچھی (~20 ٹوکنز/سیکنڈ) | اچھی (~15 ٹوکنز/سیکنڈ) |
| **کارکردگی (CPU)** | اچھی (~8 ٹوکنز/سیکنڈ) | بہت اچھی (~12 ٹوکنز/سیکنڈ) | بہترین (~15 ٹوکنز/سیکنڈ) |
| **میموری استعمال** | 8-12GB | 6-10GB | 4-8GB |
| **API مطابقت** | OpenAI موافق | کسٹم REST | کسٹم FastAPI |
| **فنکشن کالنگ** | ✅ نیٹو | ✅ سپورٹڈ | ⚠️ کسٹم نفاذ |
| **کوانٹائزیشن سپورٹ** | AWQ، GPTQ | Q4_0، Q5_1، Q8_0 | ONNX کوانٹائزیشن |
| **پروڈکشن ریڈی** | ✅ بہترین | ✅ بہت اچھی | ✅ اچھی |
| **ایج ڈیپلائمنٹ** | اچھی | بہترین | شاندار |

## اضافی وسائل

### آفیشل دستاویزات
- **Microsoft Phi-4 ماڈل کارڈ**: تفصیلی وضاحتیں اور استعمال کے رہنما
- **vLLM دستاویزات**: جدید کنفیگریشن اور اصلاح کے اختیارات
- **Ollama ماڈل لائبریری**: کمیونٹی ماڈلز اور حسب ضرورت مثالیں
- **ONNX Runtime گائیڈز**: کارکردگی کی اصلاح اور ڈیپلائمنٹ حکمت عملی

### ڈیولپمنٹ ٹولز
- **Hugging Face Transformers**: ماڈل انٹریکشن اور حسب ضرورت کے لیے
- **OpenAI API وضاحت**: vLLM مطابقت کی جانچ کے لیے
- **Docker بہترین طریقے**: کنٹینر سیکیورٹی اور اصلاح کے رہنما
- **Kubernetes ڈیپلائمنٹ**: پروڈکشن اسکیلنگ کے لیے آرکیسٹریشن پیٹرنز

### سیکھنے کے وسائل
- **SLM کارکردگی بینچ مارکنگ**: تقابلی تجزیہ کے طریقے
- **ایج AI ڈیپلائمنٹ**: محدود وسائل والے ماحول کے لیے بہترین طریقے
- **منطقی کاموں کی اصلاح**: ریاضیاتی اور منطقی مسائل کے لیے پرومپٹنگ حکمت عملی
- **کنٹینر سیکیورٹی**: AI ماڈل ڈیپلائمنٹ کے لیے سختی کے طریقے

## سیکھنے کے نتائج

اس ماڈیول کو مکمل کرنے کے بعد، آپ درج ذیل کرنے کے قابل ہوں گے:

1. Phi-4-mini-instruct ماڈل کو کنٹینرائزڈ ماحول میں مختلف فریم ورک کے ذریعے ڈیپلائمنٹ کرنا
2. مختلف ہارڈویئر ماحول کے لیے SLM ڈیپلائمنٹ کو ترتیب دینا اور بہتر بنانا
3. کنٹینرائزڈ AI ڈیپلائمنٹ کے لیے سیکیورٹی کے بہترین طریقے نافذ کرنا
4. مخصوص استعمال کے کیس کی ضروریات کی بنیاد پر مناسب ڈیپلائمنٹ فریم ورک کا موازنہ اور انتخاب کرنا
5. پروڈکشن گریڈ SLM خدمات کے لیے مانیٹرنگ اور اسکیلنگ حکمت عملیوں کو اپنانا

## آگے کیا ہے

- [ماڈیول 1](../Module01/README.md) پر واپس جائیں
- [ماڈیول 2](../Module02/README.md) پر واپس جائیں

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔