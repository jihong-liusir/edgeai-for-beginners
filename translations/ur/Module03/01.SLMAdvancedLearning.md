<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-17T18:13:10+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "ur"
}
-->
# سیکشن 1: SLM ایڈوانسڈ لرننگ - بنیادیں اور اصلاح

چھوٹے لینگویج ماڈلز (SLMs) EdgeAI میں ایک اہم پیش رفت کی نمائندگی کرتے ہیں، جو محدود وسائل والے آلات پر پیچیدہ قدرتی زبان کی پروسیسنگ کی صلاحیتوں کو ممکن بناتے ہیں۔ SLMs کو مؤثر طریقے سے تعینات، بہتر اور استعمال کرنے کا طریقہ سمجھنا عملی ایج پر مبنی AI حل بنانے کے لیے ضروری ہے۔

## تعارف

اس سبق میں، ہم چھوٹے لینگویج ماڈلز (SLMs) اور ان کے ایڈوانسڈ نفاذ کی حکمت عملیوں کا جائزہ لیں گے۔ ہم SLMs کے بنیادی تصورات، ان کے پیرامیٹر کی حدود اور درجہ بندی، اصلاح کی تکنیک، اور ایج کمپیوٹنگ ماحول کے لیے عملی تعیناتی کی حکمت عملیوں کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس سبق کے اختتام تک، آپ:

- 🔢 چھوٹے لینگویج ماڈلز کے پیرامیٹر کی حدود اور درجہ بندی کو سمجھ سکیں گے۔
- 🛠️ ایج ڈیوائسز پر SLM کی تعیناتی کے لیے کلیدی اصلاحی تکنیکوں کی شناخت کر سکیں گے۔
- 🚀 SLMs کے لیے ایڈوانسڈ کوانٹائزیشن اور کمپریشن کی حکمت عملیوں کو نافذ کرنا سیکھ سکیں گے۔

## SLM پیرامیٹر کی حدود اور درجہ بندی کو سمجھنا

چھوٹے لینگویج ماڈلز (SLMs) AI ماڈلز ہیں جو قدرتی زبان کے مواد کو پروسیس، سمجھنے اور پیدا کرنے کے لیے ڈیزائن کیے گئے ہیں، اور ان کے بڑے ہم منصبوں کے مقابلے میں نمایاں طور پر کم پیرامیٹرز رکھتے ہیں۔ جہاں بڑے لینگویج ماڈلز (LLMs) میں سینکڑوں ارب سے کھربوں پیرامیٹرز ہوتے ہیں، SLMs خاص طور پر کارکردگی اور ایج تعیناتی کے لیے ڈیزائن کیے گئے ہیں۔

پیرامیٹر کی درجہ بندی کا فریم ورک ہمیں SLMs کے مختلف زمروں اور ان کے مناسب استعمال کے معاملات کو سمجھنے میں مدد دیتا ہے۔ یہ درجہ بندی ایج کمپیوٹنگ کے مخصوص منظرناموں کے لیے صحیح ماڈل کا انتخاب کرنے کے لیے اہم ہے۔

### پیرامیٹر کی درجہ بندی کا فریم ورک

پیرامیٹر کی حدود کو سمجھنا مختلف ایج کمپیوٹنگ منظرناموں کے لیے مناسب ماڈلز کے انتخاب میں مدد کرتا ہے:

- **🔬 مائیکرو SLMs**: 100M - 1.4B پیرامیٹرز (موبائل ڈیوائسز کے لیے انتہائی ہلکے وزن والے)
- **📱 چھوٹے SLMs**: 1.5B - 13.9B پیرامیٹرز (کارکردگی اور کارکردگی میں توازن)
- **⚖️ درمیانے SLMs**: 14B - 30B پیرامیٹرز (LLM کی صلاحیتوں کے قریب پہنچتے ہوئے کارکردگی برقرار رکھتے ہیں)

تحقیقاتی کمیونٹی میں بالکل حد متغیر رہتی ہے، لیکن زیادہ تر ماہرین 30 ارب پیرامیٹرز سے کم ماڈلز کو "چھوٹا" سمجھتے ہیں، جبکہ کچھ ذرائع اس حد کو 10 ارب پیرامیٹرز تک بھی کم کرتے ہیں۔

### SLMs کے کلیدی فوائد

SLMs کئی بنیادی فوائد پیش کرتے ہیں جو انہیں ایج کمپیوٹنگ ایپلیکیشنز کے لیے مثالی بناتے ہیں:

**آپریشنل کارکردگی**: SLMs کم پیرامیٹرز کی پروسیسنگ کی وجہ سے تیز تر انفرنس کے اوقات فراہم کرتے ہیں، جو انہیں حقیقی وقت کی ایپلیکیشنز کے لیے مثالی بناتے ہیں۔ یہ کم کمپیوٹیشنل وسائل کی ضرورت رکھتے ہیں، محدود وسائل والے آلات پر تعیناتی کو ممکن بناتے ہیں، کم توانائی استعمال کرتے ہیں، اور کاربن کے اثرات کو کم رکھتے ہیں۔

**تعیناتی کی لچک**: یہ ماڈلز انٹرنیٹ کنیکٹیویٹی کی ضرورت کے بغیر آن ڈیوائس AI صلاحیتوں کو فعال کرتے ہیں، مقامی پروسیسنگ کے ذریعے پرائیویسی اور سیکیورٹی کو بڑھاتے ہیں، ڈومین مخصوص ایپلیکیشنز کے لیے حسب ضرورت بنائے جا سکتے ہیں، اور مختلف ایج کمپیوٹنگ ماحول کے لیے موزوں ہیں۔

**لاگت کی تاثیر**: SLMs تربیت اور تعیناتی کے لیے لاگت کے لحاظ سے مؤثر ہیں، آپریشنل اخراجات کو کم کرتے ہیں، اور ایج ایپلیکیشنز کے لیے کم بینڈوڈتھ کی ضروریات رکھتے ہیں۔

## ایڈوانسڈ ماڈل حاصل کرنے کی حکمت عملی

### ہگنگ فیس ایکو سسٹم

ہگنگ فیس جدید ترین SLMs کو دریافت کرنے اور ان تک رسائی حاصل کرنے کے لیے بنیادی مرکز کے طور پر کام کرتا ہے۔ یہ پلیٹ فارم ماڈل دریافت اور تعیناتی کے لیے جامع وسائل فراہم کرتا ہے:

**ماڈل دریافت کی خصوصیات**: پلیٹ فارم پیرامیٹر کی تعداد، لائسنس کی قسم، اور کارکردگی کے میٹرکس کے ذریعے ایڈوانسڈ فلٹرنگ پیش کرتا ہے۔ صارفین سائیڈ بائی سائیڈ ماڈل موازنہ کے ٹولز، حقیقی وقت کی کارکردگی کے بینچ مارکس اور تشخیصی نتائج، اور فوری جانچ کے لیے WebGPU ڈیمو تک رسائی حاصل کر سکتے ہیں۔

**منتخب SLM مجموعے**: مشہور ماڈلز میں شامل ہیں Phi-4-mini-3.8B ایڈوانسڈ ریزننگ ٹاسکس کے لیے، Qwen3 سیریز (0.6B/1.7B/4B) کثیر لسانی ایپلیکیشنز کے لیے، Google Gemma3 موثر عمومی مقصد کے کاموں کے لیے، اور تجرباتی ماڈلز جیسے BitNET انتہائی کم پریسجن تعیناتی کے لیے۔ پلیٹ فارم کمیونٹی سے چلنے والے مجموعے بھی پیش کرتا ہے جن میں مخصوص ڈومینز کے لیے خصوصی ماڈلز اور مختلف استعمال کے معاملات کے لیے بہتر پری ٹرینڈ اور انسٹرکشن ٹونڈ ویریئنٹس شامل ہیں۔

### Azure AI Foundry ماڈل کیٹلاگ

Azure AI Foundry ماڈل کیٹلاگ انٹرپرائز گریڈ SLMs تک رسائی فراہم کرتا ہے جس میں بہتر انضمام کی صلاحیتیں شامل ہیں:

**انٹرپرائز انضمام**: کیٹلاگ میں Azure کے ذریعے براہ راست فروخت کیے جانے والے ماڈلز شامل ہیں جن میں انٹرپرائز گریڈ سپورٹ اور SLAs شامل ہیں، جیسے Phi-4-mini-3.8B ایڈوانسڈ ریزننگ صلاحیتوں کے لیے اور Llama 3-8B پروڈکشن تعیناتی کے لیے۔ اس میں Qwen3 8B جیسے ماڈلز بھی شامل ہیں جو قابل اعتماد تھرڈ پارٹی اوپن سورس ماڈل سے ہیں۔

**انٹرپرائز فوائد**: فائن ٹیوننگ، مشاہدہ، اور ذمہ دار AI کے لیے بلٹ ان ٹولز ماڈل فیملیز کے درمیان قابل تبادلہ Provisioned Throughput کے ساتھ مربوط ہیں۔ انٹرپرائز SLAs کے ساتھ براہ راست Microsoft سپورٹ، مربوط سیکیورٹی اور تعمیل کی خصوصیات، اور جامع تعیناتی ورک فلو انٹرپرائز تجربے کو بڑھاتے ہیں۔

## ایڈوانسڈ کوانٹائزیشن اور اصلاحی تکنیکیں

### Llama.cpp اصلاحی فریم ورک

Llama.cpp ایج تعیناتی میں زیادہ سے زیادہ کارکردگی کے لیے جدید کوانٹائزیشن تکنیکیں فراہم کرتا ہے:

**کوانٹائزیشن کے طریقے**: فریم ورک مختلف کوانٹائزیشن لیولز کی حمایت کرتا ہے، جن میں شامل ہیں Q4_0 (4-بٹ کوانٹائزیشن بہترین سائز میں کمی کے ساتھ - Qwen3-0.6B موبائل تعیناتی کے لیے مثالی)، Q5_1 (5-بٹ کوانٹائزیشن معیار اور کمپریشن میں توازن - Phi-4-mini-3.8B ایج انفرنس کے لیے موزوں)، اور Q8_0 (اصل معیار کے قریب - Google Gemma3 پروڈکشن استعمال کے لیے تجویز کردہ)۔ BitNET انتہائی کمپریشن کے منظرناموں کے لیے 1-بٹ کوانٹائزیشن کے ساتھ جدید ترین ہے۔

**نفاذ کے فوائد**: SIMD ایکسیلیریشن کے ساتھ CPU-آپٹمائزڈ انفرنس میموری کے موثر ماڈل لوڈنگ اور عملدرآمد فراہم کرتا ہے۔ x86، ARM، اور Apple Silicon آرکیٹیکچرز کے درمیان کراس پلیٹ فارم مطابقت ہارڈویئر-اگناسٹک تعیناتی کی صلاحیتوں کو فعال کرتی ہے۔

**عملی نفاذ کی مثال**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**میموری فٹ پرنٹ کا موازنہ**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive اصلاحی سوٹ

Microsoft Olive پروڈکشن ماحول کے لیے جامع ماڈل اصلاحی ورک فلو فراہم کرتا ہے:

**اصلاحی تکنیکیں**: سوٹ میں شامل ہیں ڈائنامک کوانٹائزیشن خودکار پریسجن سلیکشن کے لیے (خاص طور پر Qwen3 سیریز ماڈلز کے ساتھ مؤثر)، گراف اصلاح اور آپریٹر فیوژن (Google Gemma3 آرکیٹیکچر کے لیے بہتر)، CPU، GPU، اور NPU کے لیے ہارڈویئر مخصوص اصلاحات (ARM ڈیوائسز پر Phi-4-mini-3.8B کے لیے خصوصی سپورٹ کے ساتھ)، اور ملٹی اسٹیج اصلاحی پائپ لائنز۔ BitNET ماڈلز کو Olive فریم ورک کے اندر خصوصی 1-بٹ کوانٹائزیشن ورک فلو کی ضرورت ہوتی ہے۔

**ورک فلو آٹومیشن**: اصلاحی ویریئنٹس کے درمیان خودکار بینچ مارکنگ معیار کے میٹرکس کو محفوظ رکھتے ہوئے اصلاح کو یقینی بناتی ہے۔ PyTorch اور ONNX جیسے مشہور ML فریم ورک کے ساتھ انضمام کلاؤڈ اور ایج تعیناتی کی اصلاحی صلاحیتیں فراہم کرتا ہے۔

**عملی نفاذ کی مثال**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX فریم ورک

Apple MLX خاص طور پر Apple Silicon ڈیوائسز کے لیے ڈیزائن کردہ مقامی اصلاحی صلاحیتیں فراہم کرتا ہے:

**Apple Silicon اصلاح**: فریم ورک متحد میموری آرکیٹیکچر کے ساتھ Metal Performance Shaders انضمام، خودکار مکسڈ پریسجن انفرنس (خاص طور پر Google Gemma3 کے ساتھ مؤثر)، اور بہتر میموری بینڈوڈتھ استعمال فراہم کرتا ہے۔ Phi-4-mini-3.8B M-سیریز چپس پر غیر معمولی کارکردگی دکھاتا ہے، جبکہ Qwen3-1.7B MacBook Air تعیناتی کے لیے بہترین توازن فراہم کرتا ہے۔

**ترقیاتی خصوصیات**: Python اور Swift API سپورٹ کے ساتھ NumPy-مطابقت پذیر ارے آپریشنز، خودکار تفریق کی صلاحیتیں، اور Apple ترقیاتی ٹولز کے ساتھ ہموار انضمام جامع ترقیاتی ماحول فراہم کرتا ہے۔

**عملی نفاذ کی مثال**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## پروڈکشن تعیناتی اور انفرنس کی حکمت عملی

### Ollama: سادہ مقامی تعیناتی

Ollama ایج اور مقامی ماحول کے لیے انٹرپرائز ریڈی خصوصیات کے ساتھ SLM تعیناتی کو آسان بناتا ہے:

**تعیناتی کی صلاحیتیں**: ایک کمانڈ ماڈل انسٹالیشن اور عملدرآمد کے ساتھ خودکار ماڈل پلنگ اور کیشنگ۔ Phi-4-mini-3.8B، مکمل Qwen3 سیریز (0.6B/1.7B/4B)، اور Google Gemma3 کے لیے سپورٹ کے ساتھ REST API ایپلیکیشن انضمام اور ملٹی ماڈل مینجمنٹ اور سوئچنگ کی صلاحیتیں۔ BitNET ماڈلز کو 1-بٹ کوانٹائزیشن سپورٹ کے لیے تجرباتی بلڈ کنفیگریشنز کی ضرورت ہوتی ہے۔

**ایڈوانسڈ خصوصیات**: کسٹم ماڈل فائن ٹیوننگ سپورٹ، کنٹینرائزڈ تعیناتی کے لیے Dockerfile جنریشن، GPU ایکسیلیریشن کے ساتھ خودکار ڈیٹیکشن، اور ماڈل کوانٹائزیشن اور اصلاحی اختیارات جامع تعیناتی کی لچک فراہم کرتے ہیں۔

### VLLM: اعلی کارکردگی انفرنس

VLLM اعلی throughput منظرناموں کے لیے پروڈکشن گریڈ انفرنس اصلاح فراہم کرتا ہے:

**کارکردگی کی اصلاحات**: PagedAttention میموری کے موثر توجہ کی گنتی کے لیے (خاص طور پر Phi-4-mini-3.8B کے ٹرانسفارمر آرکیٹیکچر کے لیے فائدہ مند)، throughput کی اصلاح کے لیے ڈائنامک بیچنگ (Qwen3 سیریز کے متوازی پروسیسنگ کے لیے بہتر)، ملٹی-GPU اسکیلنگ کے لیے ٹینسر پیراللزم (Google Gemma3 سپورٹ)، اور لیٹنسی کو کم کرنے کے لیے speculative decoding۔ BitNET ماڈلز کو 1-بٹ آپریشنز کے لیے خصوصی انفرنس کرنلز کی ضرورت ہوتی ہے۔

**انٹرپرائز انضمام**: OpenAI-مطابقت پذیر API اینڈپوائنٹس، Kubernetes تعیناتی سپورٹ، مانیٹرنگ اور مشاہدہ انضمام، اور خودکار اسکیلنگ کی صلاحیتیں انٹرپرائز گریڈ تعیناتی کے حل فراہم کرتی ہیں۔

### Foundry Local: Microsoft کا ایج حل

Foundry Local انٹرپرائز ماحول کے لیے جامع ایج تعیناتی کی صلاحیتیں فراہم کرتا ہے:

**ایج کمپیوٹنگ خصوصیات**: آف لائن-فرسٹ آرکیٹیکچر ڈیزائن کے ساتھ وسائل کی پابندی کی اصلاح، مقامی ماڈل رجسٹری مینجمنٹ، اور ایج-ٹو-کلاؤڈ ہم آہنگی کی صلاحیتیں قابل اعتماد ایج تعیناتی کو یقینی بناتی ہیں۔

**سیکیورٹی اور تعمیل**: پرائیویسی کے تحفظ کے لیے مقامی ڈیٹا پروسیسنگ، انٹرپرائز سیکیورٹی کنٹرولز، آڈٹ لاگنگ اور تعمیل کی رپورٹنگ، اور رول-بیسڈ ایکسیس مینجمنٹ ایج تعیناتیوں کے لیے جامع سیکیورٹی فراہم کرتے ہیں۔

## SLM نفاذ کے بہترین طریقے

### ماڈل انتخاب کے رہنما اصول

ایج تعیناتی کے لیے SLMs کا انتخاب کرتے وقت درج ذیل عوامل پر غور کریں:

**پیرامیٹر کی تعداد کے تحفظات**: مائیکرو SLMs جیسے Qwen3-0.6B انتہائی ہلکے وزن والے موبائل ایپلیکیشنز کے لیے منتخب کریں، چھوٹے SLMs جیسے Qwen3-1.7B یا Google Gemma3 متوازن کارکردگی کے منظرناموں کے لیے، اور درمیانے SLMs جیسے Phi-4-mini-3.8B یا Qwen3-4B LLM کی صلاحیتوں کے قریب پہنچتے ہوئے کارکردگی برقرار رکھنے کے لیے۔ BitNET ماڈلز مخصوص تحقیقی ایپلیکیشنز کے لیے تجرباتی انتہائی کمپریشن پیش کرتے ہیں۔

**استعمال کے معاملے کی مطابقت**: ماڈل کی صلاحیتوں کو مخصوص ایپلیکیشن کی ضروریات کے مطابق بنائیں، جیسے جواب کے معیار، انفرنس کی رفتار، میموری کی پابندیاں، اور آف لائن آپریشن کی ضروریات۔

### اصلاحی حکمت عملی کا انتخاب

**کوانٹائزیشن کا طریقہ**: معیار کی ضروریات اور ہارڈویئر کی پابندیوں کی بنیاد پر مناسب کوانٹائزیشن لیولز کا انتخاب کریں۔ زیادہ سے زیادہ کمپریشن کے لیے Q4_0 منتخب کریں (Qwen3-0.6B موبائل تعیناتی کے لیے مثالی)، Q5_1 معیار-کمپریشن کے توازن کے لیے (Phi-4-mini-3.8B اور Google Gemma3 کے لیے موزوں)، اور Q8_0 اصل معیار کے تحفظ کے لیے (Qwen3-4B پروڈکشن ماحول کے لیے تجویز کردہ)۔ BitNET کا 1-بٹ کوانٹائزیشن مخصوص ایپلیکیشنز کے لیے انتہائی کمپریشن کی حد کی نمائندگی کرتا ہے۔

**فریم ورک کا انتخاب**: ہدف ہارڈویئر اور تعیناتی کی ضروریات کی بنیاد پر اصلاحی فریم ورک کا انتخاب کریں۔ CPU-آپٹمائزڈ تعیناتی کے لیے Llama.cpp استعمال کریں، جامع اصلاحی ورک فلو کے لیے Microsoft Olive، اور Apple Silicon ڈیوائسز کے لیے Apple MLX۔

## عملی ماڈل کی مثالیں اور استعمال کے معاملات

### حقیقی دنیا کے تعیناتی کے منظرنامے

**موبائل ایپلیکیشنز**: Qwen3-0.6B اسمارٹ فون چیٹ بوٹ ایپلیکیشنز میں کم سے کم میموری فٹ پرنٹ کے ساتھ بہترین کارکردگی دکھاتا ہے، جبکہ Google Gemma3 ٹیبلٹ پر مبنی تعلیمی ٹولز کے لیے متوازن کارکردگی فراہم کرتا ہے۔ Phi-4-mini-3.8B موبائل پروڈکٹیویٹی ایپلیکیشنز کے لیے اعلیٰ ریزننگ صلاحیتیں پیش کرتا ہے۔

**ڈیسک ٹاپ اور ایج کمپیوٹنگ**: Qwen3-1.7B ڈیسک ٹاپ اسسٹنٹ ایپلیکیشنز کے لیے بہترین کارکردگی فراہم کرتا ہے، Phi-4-mini-3.8B ڈویلپر ٹولز کے لیے ایڈوانسڈ کوڈ جنریشن کی صلاحیتیں فراہم کرتا ہے، اور Qwen3-4B ورک سٹیشن ماحول میں پیچیدہ دستاویز تجزیہ کو فعال کرتا ہے۔

**تحقیق اور تجرباتی**: BitNET ماڈلز انتہائی کم پریسجن انفرنس کی تلاش کے لیے تعلیمی تحقیق اور پروف آف کانسیپٹ ایپلیکیشنز کے لیے انتہائی وسائل کی پابندیوں کے ساتھ قابل بناتے ہیں۔

### کارکردگی کے بینچ مارکس اور موازنہ

**انفرنس کی رفتار**: Qwen3-0.6B

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔