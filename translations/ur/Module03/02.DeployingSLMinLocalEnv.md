<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-17T18:10:55+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "ur"
}
-->
# سیکشن 2: مقامی ماحول میں تعیناتی - پرائیویسی کو ترجیح دینے والے حل

چھوٹے زبان کے ماڈلز (SLMs) کی مقامی تعیناتی پرائیویسی کو محفوظ رکھنے اور کم خرچ AI حل کی طرف ایک نیا رجحان ہے۔ یہ جامع رہنما دو طاقتور فریم ورک—Ollama اور Microsoft Foundry Local—کا جائزہ لیتا ہے جو ڈویلپرز کو SLMs کی مکمل صلاحیت کو استعمال کرنے کے قابل بناتے ہیں، جبکہ تعیناتی ماحول پر مکمل کنٹرول برقرار رکھتے ہیں۔

## تعارف

اس سبق میں، ہم مقامی ماحول میں چھوٹے زبان کے ماڈلز کی تعیناتی کے جدید طریقے دریافت کریں گے۔ ہم مقامی AI تعیناتی کے بنیادی تصورات کا جائزہ لیں گے، دو اہم پلیٹ فارمز (Ollama اور Microsoft Foundry Local) کا مطالعہ کریں گے، اور پیداوار کے لیے تیار حل کے عملی نفاذ کی رہنمائی فراہم کریں گے۔

## سیکھنے کے مقاصد

اس سبق کے اختتام تک، آپ:

- مقامی SLM تعیناتی فریم ورک کی ساخت اور فوائد کو سمجھ سکیں گے۔
- Ollama اور Microsoft Foundry Local کا استعمال کرتے ہوئے پیداوار کے لیے تیار تعیناتیوں کو نافذ کر سکیں گے۔
- مخصوص ضروریات اور پابندیوں کی بنیاد پر مناسب پلیٹ فارم کا انتخاب اور موازنہ کر سکیں گے۔
- کارکردگی، سیکیورٹی، اور توسیع پذیری کے لیے مقامی تعیناتیوں کو بہتر بنا سکیں گے۔

## مقامی SLM تعیناتی کی ساخت کو سمجھنا

مقامی SLM تعیناتی کلاؤڈ پر منحصر AI خدمات سے آن-پریمیس، پرائیویسی کو محفوظ رکھنے والے حل کی طرف ایک بنیادی تبدیلی کی نمائندگی کرتی ہے۔ یہ طریقہ تنظیموں کو اپنے AI انفراسٹرکچر پر مکمل کنٹرول برقرار رکھنے کے قابل بناتا ہے، جبکہ ڈیٹا کی خودمختاری اور آپریشنل آزادی کو یقینی بناتا ہے۔

### تعیناتی فریم ورک کی درجہ بندی

مختلف تعیناتی طریقوں کو سمجھنا مخصوص استعمال کے معاملات کے لیے صحیح حکمت عملی منتخب کرنے میں مدد کرتا ہے:

- **ترقی پر مرکوز**: تجربات اور پروٹوٹائپنگ کے لیے آسان سیٹ اپ  
- **انٹرپرائز گریڈ**: پیداوار کے لیے تیار حل کے ساتھ انٹرپرائز انضمام کی صلاحیتیں  
- **کراس پلیٹ فارم**: مختلف آپریٹنگ سسٹمز اور ہارڈویئر پر عالمگیر مطابقت  

### مقامی SLM تعیناتی کے اہم فوائد

مقامی SLM تعیناتی کئی بنیادی فوائد پیش کرتی ہے جو اسے انٹرپرائز اور پرائیویسی حساس ایپلیکیشنز کے لیے مثالی بناتے ہیں:

**پرائیویسی اور سیکیورٹی**: مقامی پروسیسنگ اس بات کو یقینی بناتی ہے کہ حساس ڈیٹا تنظیم کے انفراسٹرکچر سے باہر نہ جائے، GDPR، HIPAA، اور دیگر ضوابط کی تعمیل کو ممکن بناتی ہے۔ ایئر گیپڈ تعیناتیوں کو خفیہ ماحول کے لیے ممکن بنایا جا سکتا ہے، جبکہ مکمل آڈٹ ٹریلز سیکیورٹی کی نگرانی برقرار رکھتے ہیں۔

**کم خرچ**: فی ٹوکن قیمت کے ماڈلز کو ختم کرنے سے آپریٹنگ اخراجات میں نمایاں کمی آتی ہے۔ کم بینڈوڈتھ کی ضروریات اور کلاؤڈ پر انحصار میں کمی انٹرپرائز بجٹ کے لیے پیش گوئی کے قابل قیمت ڈھانچے فراہم کرتی ہے۔

**کارکردگی اور قابل اعتماد**: نیٹ ورک کی تاخیر کے بغیر تیز تر انفرنس اوقات حقیقی وقت کی ایپلیکیشنز کو ممکن بناتے ہیں۔ آف لائن فعالیت انٹرنیٹ کنیکٹیویٹی کے بغیر مسلسل آپریشن کو یقینی بناتی ہے، جبکہ مقامی وسائل کی اصلاح مستقل کارکردگی فراہم کرتی ہے۔

## Ollama: عالمگیر مقامی تعیناتی پلیٹ فارم

### بنیادی ساخت اور فلسفہ

Ollama ایک عالمگیر، ڈویلپر دوستانہ پلیٹ فارم کے طور پر تیار کیا گیا ہے جو مختلف ہارڈویئر کنفیگریشنز اور آپریٹنگ سسٹمز میں مقامی LLM تعیناتی کو جمہوری بناتا ہے۔

**تکنیکی بنیاد**: مضبوط llama.cpp فریم ورک پر مبنی، Ollama موثر GGUF ماڈل فارمیٹ کا استعمال کرتا ہے تاکہ بہترین کارکردگی حاصل کی جا سکے۔ کراس پلیٹ فارم مطابقت Windows، macOS، اور Linux ماحول میں مستقل رویے کو یقینی بناتی ہے، جبکہ ذہین وسائل کا انتظام CPU، GPU، اور میموری کے استعمال کو بہتر بناتا ہے۔

**ڈیزائن فلسفہ**: Ollama سادگی کو ترجیح دیتا ہے بغیر فعالیت کو قربان کیے، فوری پیداواریت کے لیے زیرو کنفیگریشن تعیناتی پیش کرتا ہے۔ پلیٹ فارم وسیع ماڈل مطابقت برقرار رکھتا ہے جبکہ مختلف ماڈل ساختوں کے درمیان مستقل APIs فراہم کرتا ہے۔

### جدید خصوصیات اور صلاحیتیں

**ماڈل مینجمنٹ کی مہارت**: Ollama جامع ماڈل لائف سائیکل مینجمنٹ فراہم کرتا ہے، جس میں خودکار پلنگ، کیشنگ، اور ورژننگ شامل ہیں۔ پلیٹ فارم ایک وسیع ماڈل ایکو سسٹم کی حمایت کرتا ہے، جس میں Llama 3.2، Google Gemma 2، Microsoft Phi-4، Qwen 2.5، DeepSeek، Mistral، اور خصوصی ایمبیڈنگ ماڈلز شامل ہیں۔

**ماڈل فائلز کے ذریعے حسب ضرورت**: جدید صارفین مخصوص پیرامیٹرز، سسٹم پرامپٹس، اور رویے میں ترمیم کے ساتھ حسب ضرورت ماڈل کنفیگریشنز بنا سکتے ہیں۔ یہ ڈومین مخصوص اصلاحات اور خصوصی ایپلیکیشن کی ضروریات کو ممکن بناتا ہے۔

**کارکردگی کی اصلاح**: Ollama دستیاب ہارڈویئر ایکسیلیریشن کا پتہ لگاتا ہے اور استعمال کرتا ہے، جس میں NVIDIA CUDA، Apple Metal، اور OpenCL شامل ہیں۔ ذہین میموری مینجمنٹ مختلف ہارڈویئر کنفیگریشنز میں وسائل کے بہترین استعمال کو یقینی بناتا ہے۔

### پیداوار کے نفاذ کی حکمت عملی

**انسٹالیشن اور سیٹ اپ**: Ollama مختلف پلیٹ فارمز پر مقامی انسٹالرز، پیکیج مینیجرز (WinGet، Homebrew، APT)، اور کنٹینرائزڈ تعیناتیوں کے لیے Docker کنٹینرز کے ذریعے آسان انسٹالیشن فراہم کرتا ہے۔

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**ضروری کمانڈز اور آپریشنز**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**جدید کنفیگریشن**: ماڈل فائلز انٹرپرائز کی ضروریات کے لیے نفیس حسب ضرورت کو ممکن بناتی ہیں:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### ڈویلپر انضمام کی مثالیں

**Python API انضمام**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript انضمام (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API کا استعمال cURL کے ساتھ**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### کارکردگی کی ترتیب اور اصلاح

**میموری اور تھریڈ کنفیگریشن**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**مختلف ہارڈویئر کے لیے کوانٹائزیشن کا انتخاب**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: انٹرپرائز ایج AI پلیٹ فارم

### انٹرپرائز گریڈ ساخت

Microsoft Foundry Local ایک جامع انٹرپرائز حل کی نمائندگی کرتا ہے جو پیداوار ایج AI تعیناتیوں کے لیے خاص طور پر ڈیزائن کیا گیا ہے، جس میں Microsoft ایکو سسٹم کے ساتھ گہرا انضمام شامل ہے۔

**ONNX پر مبنی بنیاد**: صنعت کے معیار ONNX Runtime پر مبنی، Foundry Local مختلف ہارڈویئر ساختوں میں بہتر کارکردگی فراہم کرتا ہے۔ پلیٹ فارم Windows ML انضمام کے ذریعے مقامی Windows اصلاحات کا فائدہ اٹھاتا ہے، جبکہ کراس پلیٹ فارم مطابقت برقرار رکھتا ہے۔

**ہارڈویئر ایکسیلیریشن کی مہارت**: Foundry Local CPUs، GPUs، اور NPUs میں ذہین ہارڈویئر کا پتہ لگانے اور اصلاح کی خصوصیات رکھتا ہے۔ ہارڈویئر فروشوں (AMD، Intel، NVIDIA، Qualcomm) کے ساتھ گہرا تعاون انٹرپرائز ہارڈویئر کنفیگریشنز پر بہترین کارکردگی کو یقینی بناتا ہے۔

### جدید ڈویلپر تجربہ

**ملٹی انٹرفیس رسائی**: Foundry Local جامع ترقیاتی انٹرفیس فراہم کرتا ہے، جس میں ماڈل مینجمنٹ اور تعیناتی کے لیے ایک طاقتور CLI، مقامی انضمام کے لیے ملٹی لینگویج SDKs (Python، NodeJS)، اور OpenAI مطابقت کے ساتھ RESTful APIs شامل ہیں۔

**Visual Studio انضمام**: پلیٹ فارم AI Toolkit کے ساتھ VS Code میں بغیر کسی رکاوٹ کے انضمام فراہم کرتا ہے، جس میں ماڈل کنورژن، کوانٹائزیشن، اور اصلاح کے ٹولز شامل ہیں۔ یہ انضمام ترقیاتی ورک فلو کو تیز کرتا ہے اور تعیناتی کی پیچیدگی کو کم کرتا ہے۔

**ماڈل اصلاح پائپ لائن**: Microsoft Olive انضمام متحرک کوانٹائزیشن، گراف اصلاح، اور ہارڈویئر مخصوص ٹیوننگ سمیت نفیس ماڈل اصلاح ورک فلو کو ممکن بناتا ہے۔ Azure ML کے ذریعے کلاؤڈ پر مبنی کنورژن صلاحیتیں بڑے ماڈلز کے لیے توسیع پذیر اصلاح فراہم کرتی ہیں۔

### پیداوار کے نفاذ کی حکمت عملی

**انسٹالیشن اور کنفیگریشن**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**ماڈل مینجمنٹ آپریشنز**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**جدید تعیناتی کنفیگریشن**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### انٹرپرائز ایکو سسٹم انضمام

**سیکیورٹی اور تعمیل**: Foundry Local انٹرپرائز گریڈ سیکیورٹی خصوصیات فراہم کرتا ہے، جس میں رول پر مبنی رسائی کنٹرول، آڈٹ لاگنگ، تعمیل رپورٹنگ، اور انکرپٹڈ ماڈل اسٹوریج شامل ہیں۔ Microsoft سیکیورٹی انفراسٹرکچر کے ساتھ انضمام انٹرپرائز سیکیورٹی پالیسیوں کی تعمیل کو یقینی بناتا ہے۔

**بلٹ ان AI خدمات**: پلیٹ فارم مقامی زبان کی پروسیسنگ کے لیے Phi Silica، تصویر کی بہتری اور تجزیہ کے لیے AI Imaging، اور عام انٹرپرائز AI کاموں کے لیے خصوصی APIs سمیت تیار AI صلاحیتیں پیش کرتا ہے۔

## Ollama بمقابلہ Foundry Local: تقابلی تجزیہ

### تکنیکی ساخت کا موازنہ

| **پہلو** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **ماڈل فارمیٹ** | GGUF (llama.cpp کے ذریعے) | ONNX (ONNX Runtime کے ذریعے) |
| **پلیٹ فارم فوکس** | کراس پلیٹ فارم | Windows/انٹرپرائز اصلاح |
| **ہارڈویئر انضمام** | عمومی GPU/CPU سپورٹ | Windows ML، NPU سپورٹ |
| **اصلاح** | llama.cpp کوانٹائزیشن | Microsoft Olive + ONNX Runtime |
| **انٹرپرائز خصوصیات** | کمیونٹی پر مبنی | انٹرپرائز گریڈ SLAs کے ساتھ |

### کارکردگی کی خصوصیات

**Ollama کی کارکردگی کی طاقتیں**:
- CPU کی غیر معمولی کارکردگی llama.cpp اصلاح کے ذریعے  
- مختلف پلیٹ فارمز اور ہارڈویئر پر مستقل رویہ  
- ذہین ماڈل لوڈنگ کے ساتھ موثر میموری کا استعمال  
- ترقی اور جانچ کے منظرناموں کے لیے تیز کولڈ اسٹارٹ اوقات  

**Foundry Local کی کارکردگی کے فوائد**:
- جدید Windows ہارڈویئر پر NPU کے بہترین استعمال  
- ہارڈویئر فروشوں کے ساتھ شراکت داری کے ذریعے بہتر GPU ایکسیلیریشن  
- انٹرپرائز گریڈ کارکردگی کی نگرانی اور اصلاح  
- پیداوار کے ماحول کے لیے توسیع پذیر تعیناتی صلاحیتیں  

### ترقیاتی تجربے کا تجزیہ

**Ollama کا ڈویلپر تجربہ**:
- فوری پیداواریت کے ساتھ کم سے کم سیٹ اپ کی ضروریات  
- تمام آپریشنز کے لیے بدیہی کمانڈ لائن انٹرفیس  
- وسیع کمیونٹی سپورٹ اور دستاویزات  
- ماڈل فائلز کے ذریعے لچکدار حسب ضرورت  

**Foundry Local کا ڈویلپر تجربہ**:
- Visual Studio ایکو سسٹم کے ساتھ جامع IDE انضمام  
- ٹیم تعاون کی خصوصیات کے ساتھ انٹرپرائز ترقیاتی ورک فلو  
- Microsoft کی حمایت کے ساتھ پیشہ ورانہ سپورٹ چینلز  
- جدید ڈیبگنگ اور اصلاح کے ٹولز  

### استعمال کے معاملے کی اصلاح

**Ollama کا انتخاب کریں جب**:
- کراس پلیٹ فارم ایپلیکیشنز تیار کر رہے ہوں جن میں مستقل رویہ کی ضرورت ہو  
- اوپن سورس شفافیت اور کمیونٹی تعاون کو ترجیح دے رہے ہوں  
- محدود وسائل یا بجٹ کی پابندیوں کے ساتھ کام کر رہے ہوں  
- تجرباتی یا تحقیق پر مبنی ایپلیکیشنز بنا رہے ہوں  
- مختلف ساختوں کے درمیان وسیع ماڈل مطابقت کی ضرورت ہو  

**Foundry Local کا انتخاب کریں جب**:
- انٹرپرائز ایپلیکیشنز تعینات کر رہے ہوں جن میں سخت کارکردگی کی ضروریات ہوں  
- Windows مخصوص ہارڈویئر اصلاحات (NPU، Windows ML) کا فائدہ اٹھا رہے ہوں  
- انٹرپرائز سپورٹ، SLAs، اور تعمیل خصوصیات کی ضرورت ہو  
- Microsoft ایکو سسٹم انضمام کے ساتھ پیداوار ایپلیکیشنز بنا رہے ہوں  
- جدید اصلاح کے ٹولز اور پیشہ ورانہ ترقیاتی ورک فلو کی ضرورت ہو  

## جدید تعیناتی کی حکمت عملی

### کنٹینرائزڈ تعیناتی کے نمونے

**Ollama کنٹینرائزیشن**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local انٹرپرائز تعیناتی**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### کارکردگی کی اصلاح کی تکنیکیں

**Ollama اصلاح کی حکمت عملی**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local اصلاح**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## سیکیورٹی اور تعمیل کے تحفظات

### انٹرپرائز سیکیورٹی نفاذ

**Ollama سیکیورٹی بہترین طریقے**:
- فائر وال قواعد اور VPN رسائی کے ساتھ نیٹ ورک کی تنہائی  
- ریورس پراکسی انضمام کے ذریعے تصدیق  
- ماڈل کی سالمیت کی تصدیق اور محفوظ ماڈل تقسیم  
- API رسائی اور ماڈل آپریشنز کے لیے آڈٹ لاگنگ  

**Foundry Local انٹرپرائز سیکیورٹی**:
- Active Directory انضمام کے ساتھ بلٹ ان رول پر مبنی رسائی کنٹرول  
- تعمیل رپورٹنگ کے ساتھ جامع آڈٹ ٹریلز  
- انکرپٹڈ ماڈل اسٹوریج اور محفوظ ماڈل تعیناتی  
- Microsoft سیکیورٹی انفراسٹرکچر کے ساتھ انضمام  

### تعمیل اور ضوابط کی ضروریات

دونوں پلیٹ فارمز تعمیل کی حمایت کرتے ہیں:
- مقامی پروسیسنگ کو یقینی بنانے کے لیے ڈیٹا رہائش کنٹرول  
- ضوابط کی رپورٹنگ کی ضروریات کے لیے آڈٹ لاگنگ  
- حساس ڈیٹا ہینڈلنگ کے لیے رسائی کنٹرول  
- ڈیٹا کی حفاظت کے لیے آرام اور ٹرانزٹ میں انکرپشن  

## پیداوار کی تعیناتی کے بہترین طریقے

### نگرانی اور مشاہدہ

**نگرانی کے لیے کلیدی میٹرکس**:
- ماڈل انفرنس کی تاخیر اور تھروپٹ  
- وسائل کا استعمال (CPU، GPU، میموری)  
- API کے ردعمل کے اوقات اور غلطی کی شرح  
- ماڈل کی درستگی اور کارکردگی میں کمی  

**نگرانی کا نفاذ**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### مسلسل انضمام اور تعیناتی

**CI/CD پائپ لائن انضمام**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## مستقبل کے رجحانات اور تحفظات

### ابھرتی ہوئی ٹیکنالوجیز

مقامی SLM تعیناتی کا منظر نامہ کئی کلیدی رجحانات کے ساتھ ترقی کرتا رہتا ہے:

**جدید ماڈل ساختیں**: بہتر کارکردگی اور صلاحیت کے تناسب کے ساتھ اگلی نسل کے SLMs ابھر رہے ہیں، جن میں متحرک اسکیلنگ کے لیے ماہرین کے ماڈلز اور ایج تعیناتی کے لیے خصوصی ساختیں شامل ہیں۔

**ہارڈویئر انضمام**: خصوصی AI ہارڈویئر جیسے NPUs، کسٹم سلیکون، اور ایج کمپیوٹنگ ایکسیلیریٹرز کے ساتھ گہرا انضمام بہتر کارکردگی کی صلاحیتیں فراہم کرے گا۔

**ایکو سسٹم ارتقاء**: تعیناتی پلیٹ فارمز کے درمیان معیاری کوششیں اور مختلف فریم ورک کے درمیان بہتر انٹرآپریبلٹی ملٹی پلیٹ فارم تعیناتیوں کو آسان بنائے گی۔

### صنعت کے اپنانے کے نمونے

**انٹرپرائز اپنانا**: پرائیویسی کی ضروریات، قیمت کی اصلاح، اور ضوابط کی تعمیل کی ضروریات کے ذریعے بڑھتا ہوا انٹرپرائز اپنانا۔ حکومت اور دفاعی شعبے خاص طور پر ایئر گیپڈ تعیناتیوں پر توجہ مرکوز کر رہے ہیں۔

**عالمی تحفظات**: سخت ڈیٹا تحفظ کے ضوابط والے علاقوں میں مقامی تعیناتی اپنانے کو بڑھا رہے ہیں، خاص طور پر بین الاقوامی ڈیٹا خودمختاری کی ضروریات کے ذریعے۔

## چیلنجز اور تحفظات

### تکنیکی چیلنج

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔