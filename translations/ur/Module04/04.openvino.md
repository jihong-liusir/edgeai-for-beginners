<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T17:49:59+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ur"
}
-->
# سیکشن 4: اوپن وی آئی این او ٹول کٹ آپٹیمائزیشن سوٹ

## فہرست مواد
1. [تعارف](../../../Module04)
2. [اوپن وی آئی این او کیا ہے؟](../../../Module04)
3. [انسٹالیشن](../../../Module04)
4. [جلدی شروع کرنے کی گائیڈ](../../../Module04)
5. [مثال: ماڈلز کو تبدیل اور بہتر بنانا اوپن وی آئی این او کے ساتھ](../../../Module04)
6. [ایڈوانسڈ استعمال](../../../Module04)
7. [بہترین طریقے](../../../Module04)
8. [مسائل کا حل](../../../Module04)
9. [اضافی وسائل](../../../Module04)

## تعارف

اوپن وی آئی این او (اوپن ویژول انفرنس اور نیورل نیٹ ورک آپٹیمائزیشن) انٹیل کا اوپن سورس ٹول کٹ ہے جو کلاؤڈ، آن پریمیسز، اور ایج ماحول میں بہترین AI حل فراہم کرنے کے لیے بنایا گیا ہے۔ چاہے آپ CPUs، GPUs، VPUs، یا خاص AI ایکسیلیریٹرز کو ہدف بنا رہے ہوں، اوپن وی آئی این او ماڈل کی درستگی کو برقرار رکھتے ہوئے جامع آپٹیمائزیشن صلاحیتیں فراہم کرتا ہے اور کراس پلیٹ فارم ڈیپلائمنٹ کو ممکن بناتا ہے۔

## اوپن وی آئی این او کیا ہے؟

اوپن وی آئی این او ایک اوپن سورس ٹول کٹ ہے جو ڈیولپرز کو مختلف ہارڈویئر پلیٹ فارمز پر AI ماڈلز کو مؤثر طریقے سے آپٹیمائز، تبدیل، اور ڈیپلائے کرنے کی سہولت دیتا ہے۔ یہ تین اہم اجزاء پر مشتمل ہے: انفرنس کے لیے اوپن وی آئی این او رن ٹائم، ماڈل آپٹیمائزیشن کے لیے نیورل نیٹ ورک کمپریشن فریم ورک (NNCF)، اور اسکیل ایبل ڈیپلائمنٹ کے لیے اوپن وی آئی این او ماڈل سرور۔

### اہم خصوصیات

- **کراس پلیٹ فارم ڈیپلائمنٹ**: لینکس، ونڈوز، اور میک او ایس کے ساتھ Python، C++، اور C APIs کی سپورٹ
- **ہارڈویئر ایکسیلیریشن**: CPU، GPU، VPU، اور AI ایکسیلیریٹرز کے لیے خودکار ڈیوائس دریافت اور آپٹیمائزیشن
- **ماڈل کمپریشن فریم ورک**: NNCF کے ذریعے جدید کوانٹائزیشن، پروننگ، اور آپٹیمائزیشن تکنیکیں
- **فریم ورک مطابقت**: TensorFlow، ONNX، PaddlePaddle، اور PyTorch ماڈلز کے لیے براہ راست سپورٹ
- **جنریٹو AI سپورٹ**: بڑے لینگویج ماڈلز اور جنریٹو AI ایپلیکیشنز کے لیے خاص اوپن وی آئی این او GenAI

### فوائد

- **پرفارمنس آپٹیمائزیشن**: کم سے کم درستگی کے نقصان کے ساتھ نمایاں رفتار میں بہتری
- **کم ڈیپلائمنٹ فٹ پرنٹ**: انسٹالیشن اور ڈیپلائمنٹ کو آسان بنانے کے لیے کم بیرونی ڈیپینڈنسز
- **بہتر اسٹارٹ اپ وقت**: تیز ایپلیکیشن انیشیالائزیشن کے لیے ماڈل لوڈنگ اور کیشنگ کو آپٹیمائز کیا گیا
- **اسکیل ایبل ڈیپلائمنٹ**: ایج ڈیوائسز سے کلاؤڈ انفراسٹرکچر تک مستقل APIs کے ساتھ
- **پروڈکشن ریڈی**: انٹرپرائز گریڈ قابل اعتمادیت کے ساتھ جامع دستاویزات اور کمیونٹی سپورٹ

## انسٹالیشن

### ضروریات

- Python 3.8 یا اس سے زیادہ
- pip پیکیج مینیجر
- ورچوئل ماحول (تجویز کردہ)
- مطابقت رکھنے والا ہارڈویئر (انٹیل CPUs تجویز کردہ، لیکن مختلف آرکیٹیکچرز کی سپورٹ موجود ہے)

### بنیادی انسٹالیشن

ورچوئل ماحول بنائیں اور فعال کریں:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

اوپن وی آئی این او رن ٹائم انسٹال کریں:

```bash
pip install openvino
```

ماڈل آپٹیمائزیشن کے لیے NNCF انسٹال کریں:

```bash
pip install nncf
```

### اوپن وی آئی این او GenAI انسٹالیشن

جنریٹو AI ایپلیکیشنز کے لیے:

```bash
pip install openvino-genai
```

### اختیاری ڈیپینڈنسز

خاص استعمال کے کیسز کے لیے اضافی پیکیجز:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### انسٹالیشن کی تصدیق کریں

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

اگر کامیاب ہو، تو آپ کو اوپن وی آئی این او ورژن کی معلومات نظر آئے گی۔

## جلدی شروع کرنے کی گائیڈ

### آپ کا پہلا ماڈل آپٹیمائزیشن

آئیے اوپن وی آئی این او کے ذریعے ایک Hugging Face ماڈل کو تبدیل اور آپٹیمائز کرتے ہیں:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### یہ عمل کیا کرتا ہے؟

آپٹیمائزیشن ورک فلو میں شامل ہیں: Hugging Face سے اصل ماڈل لوڈ کرنا، اوپن وی آئی این او انٹرمیڈیٹ ریپریزنٹیشن (IR) فارمیٹ میں تبدیل کرنا، ڈیفالٹ آپٹیمائزیشنز کا اطلاق، اور ہدف ہارڈویئر کے لیے کمپائل کرنا۔

### اہم پیرامیٹرز کی وضاحت

- `export=True`: ماڈل کو اوپن وی آئی این او IR فارمیٹ میں تبدیل کرتا ہے
- `compile=False`: رن ٹائم تک کمپائلنگ میں تاخیر کرتا ہے تاکہ لچک فراہم کی جا سکے
- `device`: ہدف ہارڈویئر ("CPU"، "GPU"، "AUTO" خودکار انتخاب کے لیے)
- `save_pretrained()`: آپٹیمائزڈ ماڈل کو دوبارہ استعمال کے لیے محفوظ کرتا ہے

## مثال: ماڈلز کو تبدیل اور بہتر بنانا اوپن وی آئی این او کے ساتھ

### مرحلہ 1: NNCF کوانٹائزیشن کے ساتھ ماڈل کنورژن

یہاں پوسٹ ٹریننگ کوانٹائزیشن کا اطلاق کرنے کا طریقہ ہے:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### مرحلہ 2: وزن کمپریشن کے ساتھ ایڈوانسڈ آپٹیمائزیشن

ٹرانسفارمر پر مبنی ماڈلز کے لیے وزن کمپریشن کا اطلاق کریں:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### مرحلہ 3: آپٹیمائزڈ ماڈل کے ساتھ انفرنس

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### آؤٹ پٹ اسٹرکچر

آپٹیمائزیشن کے بعد، آپ کے ماڈل ڈائریکٹری میں شامل ہوں گے:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## ایڈوانسڈ استعمال

### NNCF YAML کے ساتھ کنفیگریشن

پیچیدہ آپٹیمائزیشن ورک فلو کے لیے NNCF کنفیگریشن فائلز استعمال کریں:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

کنفیگریشن کا اطلاق کریں:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU آپٹیمائزیشن

GPU ایکسیلیریشن کے لیے:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### بیچ پروسیسنگ آپٹیمائزیشن

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### ماڈل سرور ڈیپلائمنٹ

اوپن وی آئی این او ماڈل سرور کے ساتھ آپٹیمائزڈ ماڈلز کو ڈیپلائے کریں:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

ماڈل سرور کے لیے کلائنٹ کوڈ:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## بہترین طریقے

### 1. ماڈل کا انتخاب اور تیاری
- سپورٹڈ فریم ورک سے ماڈلز استعمال کریں (PyTorch، TensorFlow، ONNX)
- یقینی بنائیں کہ ماڈل انپٹس کے فکسڈ یا معلوم ڈائنامک شیپس ہوں
- کیلیبریشن کے لیے نمائندہ ڈیٹا سیٹس کے ساتھ ٹیسٹ کریں

### 2. آپٹیمائزیشن حکمت عملی کا انتخاب
- **پوسٹ ٹریننگ کوانٹائزیشن**: جلدی آپٹیمائزیشن کے لیے یہاں سے شروع کریں
- **وزن کمپریشن**: بڑے لینگویج ماڈلز اور ٹرانسفارمرز کے لیے مثالی
- **کوانٹائزیشن آگاہ ٹریننگ**: جب درستگی اہم ہو

### 3. ہارڈویئر مخصوص آپٹیمائزیشن
- **CPU**: متوازن پرفارمنس کے لیے INT8 کوانٹائزیشن استعمال کریں
- **GPU**: FP16 پریسیشن اور بیچ پروسیسنگ کا فائدہ اٹھائیں
- **VPU**: ماڈل کو سادہ بنانے اور لیئر فیوژن پر توجہ دیں

### 4. پرفارمنس ٹیوننگ
- **تھروپٹ موڈ**: زیادہ حجم بیچ پروسیسنگ کے لیے
- **لیٹنسی موڈ**: ریئل ٹائم انٹرایکٹو ایپلیکیشنز کے لیے
- **AUTO ڈیوائس**: اوپن وی آئی این او کو بہترین ہارڈویئر منتخب کرنے دیں

### 5. میموری مینجمنٹ
- میموری اوور ہیڈ سے بچنے کے لیے ڈائنامک شیپس کا دانشمندانہ استعمال کریں
- تیز لوڈنگ کے لیے ماڈل کیشنگ نافذ کریں
- آپٹیمائزیشن کے دوران میموری استعمال کی نگرانی کریں

### 6. درستگی کی توثیق
- ہمیشہ آپٹیمائزڈ ماڈلز کو اصل پرفارمنس کے خلاف توثیق کریں
- نمائندہ ٹیسٹ ڈیٹا سیٹس کے ساتھ تشخیص کریں
- بتدریج آپٹیمائزیشن پر غور کریں (محفوظ سیٹنگز سے شروع کریں)

## مسائل کا حل

### عام مسائل

#### 1. انسٹالیشن کے مسائل
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. ماڈل کنورژن کی غلطیاں
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. پرفارمنس کے مسائل
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. میموری کے مسائل
- آپٹیمائزیشن کے دوران ماڈل بیچ سائز کو کم کریں
- بڑے ڈیٹا سیٹس کے لیے اسٹریمنگ استعمال کریں
- ماڈل کیشنگ فعال کریں: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. درستگی میں کمی
- زیادہ پریسیشن استعمال کریں (INT8 کے بجائے INT4)
- کیلیبریشن ڈیٹا سیٹ کا سائز بڑھائیں
- مکسڈ پریسیشن آپٹیمائزیشن کا اطلاق کریں

### پرفارمنس مانیٹرنگ

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### مدد حاصل کریں

- **دستاویزات**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub مسائل**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **کمیونٹی فورم**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## اضافی وسائل

### آفیشل لنکس
- **اوپن وی آئی این او ہوم پیج**: [openvino.ai](https://openvino.ai/)
- **GitHub ریپوزٹری**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF ریپوزٹری**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **ماڈل زو**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### سیکھنے کے وسائل
- **اوپن وی آئی این او نوٹ بکس**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **جلدی شروع کرنے کی گائیڈ**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **آپٹیمائزیشن گائیڈ**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### انٹیگریشن ٹولز
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **اوپن وی آئی این او ماڈل سرور**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **اوپن وی آئی این او GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### پرفارمنس بینچ مارکس
- **آفیشل بینچ مارکس**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF ماڈل زو**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### کمیونٹی مثالیں
- **Jupyter نوٹ بکس**: [اوپن وی آئی این او نوٹ بکس ریپوزٹری](https://github.com/openvinotoolkit/openvino_notebooks) - جامع ٹیوٹوریلز اوپن وی آئی این او نوٹ بکس ریپوزٹری میں دستیاب ہیں
- **نمونہ ایپلیکیشنز**: [اوپن وی آئی این او اوپن ماڈل زو](https://github.com/openvinotoolkit/open_model_zoo) - مختلف ڈومینز (کمپیوٹر ویژن، NLP، آڈیو) کے لیے حقیقی دنیا کی مثالیں
- **بلاگ پوسٹس**: [Intel AI بلاگ](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - انٹیل AI اور کمیونٹی بلاگ پوسٹس کے ساتھ تفصیلی استعمال کے کیسز

### متعلقہ ٹولز
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - انٹیل ہارڈویئر کے لیے اضافی آپٹیمائزیشن تکنیکیں
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - موبائل اور ایج ڈیپلائمنٹ کے موازنہ کے لیے
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - کراس پلیٹ فارم انفرنس انجن کے متبادل

## ➡️ آگے کیا ہے؟

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔