<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-17T17:56:14+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "ur"
}
-->
# سیکشن 4: ایپل MLX فریم ورک کی گہرائی میں جائزہ

## فہرست
1. [ایپل MLX کا تعارف](../../../Module04)
2. [LLM ڈیولپمنٹ کے لیے اہم خصوصیات](../../../Module04)
3. [انسٹالیشن گائیڈ](../../../Module04)
4. [MLX کے ساتھ شروعات](../../../Module04)
5. [MLX-LM: لینگویج ماڈلز](../../../Module04)
6. [بڑے لینگویج ماڈلز کے ساتھ کام کرنا](../../../Module04)
7. [ہگنگ فیس انٹیگریشن](../../../Module04)
8. [ماڈل کنورژن اور کوانٹائزیشن](../../../Module04)
9. [لینگویج ماڈلز کی فائن ٹیوننگ](../../../Module04)
10. [LLM کی ایڈوانس خصوصیات](../../../Module04)
11. [LLMs کے لیے بہترین طریقے](../../../Module04)
12. [مسائل کا حل](../../../Module04)
13. [اضافی وسائل](../../../Module04)

## ایپل MLX کا تعارف

ایپل MLX ایک فریم ورک ہے جو خاص طور پر ایپل سلیکون پر مشین لرننگ کو مؤثر اور لچکدار بنانے کے لیے ڈیزائن کیا گیا ہے، اور یہ ایپل مشین لرننگ ریسرچ کے ذریعے تیار کیا گیا ہے۔ دسمبر 2023 میں جاری ہونے والا MLX، PyTorch اور TensorFlow جیسے فریم ورکس کا ایپل کا جواب ہے، جس میں میک کمپیوٹرز پر بڑے لینگویج ماڈلز کو چلانے کی صلاحیت پر خاص توجہ دی گئی ہے۔

### LLMs کے لیے MLX کو خاص کیا بناتا ہے؟

MLX ایپل سلیکون کی یونیفائیڈ میموری آرکیٹیکچر کا مکمل فائدہ اٹھانے کے لیے ڈیزائن کیا گیا ہے، جو میک کمپیوٹرز پر بڑے لینگویج ماڈلز کو مقامی طور پر چلانے اور فائن ٹیوننگ کے لیے خاص طور پر موزوں ہے۔ یہ فریم ورک ان بہت سے مطابقت کے مسائل کو ختم کرتا ہے جن کا میک صارفین کو روایتی طور پر LLMs کے ساتھ سامنا کرنا پڑتا تھا۔

### کون MLX کو LLMs کے لیے استعمال کرے؟

- **میک صارفین** جو LLMs کو مقامی طور پر کلاؤڈ پر انحصار کیے بغیر چلانا چاہتے ہیں
- **ریسرچرز** جو لینگویج ماڈلز کی فائن ٹیوننگ اور حسب ضرورت پر تجربہ کر رہے ہیں
- **ڈیولپرز** جو لینگویج ماڈل کی صلاحیتوں کے ساتھ AI ایپلیکیشنز بنا رہے ہیں
- **کوئی بھی** جو ایپل سلیکون کو ٹیکسٹ جنریشن، چیٹ، اور لینگویج ٹاسکس کے لیے استعمال کرنا چاہتا ہے

## LLM ڈیولپمنٹ کے لیے اہم خصوصیات

### 1. یونیفائیڈ میموری آرکیٹیکچر
ایپل سلیکون کی یونیفائیڈ میموری MLX کو بڑے لینگویج ماڈلز کو مؤثر طریقے سے ہینڈل کرنے کی اجازت دیتی ہے، بغیر میموری کاپی کرنے کے اوور ہیڈ کے جو دوسرے فریم ورکس میں عام ہے۔ اس کا مطلب ہے کہ آپ ایک ہی ہارڈویئر پر بڑے ماڈلز کے ساتھ کام کر سکتے ہیں۔

### 2. نیٹو ایپل سلیکون آپٹیمائزیشن
MLX ایپل کے M سیریز چپس کے لیے شروع سے بنایا گیا ہے، جو لینگویج ماڈلز میں عام طور پر استعمال ہونے والی ٹرانسفارمر آرکیٹیکچرز کے لیے بہترین کارکردگی فراہم کرتا ہے۔

### 3. کوانٹائزیشن سپورٹ
4-بٹ اور 8-بٹ کوانٹائزیشن کے لیے بلٹ ان سپورٹ میموری کی ضروریات کو کم کرتا ہے جبکہ ماڈل کے معیار کو برقرار رکھتا ہے، جس سے بڑے ماڈلز کو کنزیومر ہارڈویئر پر چلانا ممکن ہوتا ہے۔

### 4. ہگنگ فیس انٹیگریشن
ہگنگ فیس ایکو سسٹم کے ساتھ بغیر کسی رکاوٹ کے انٹیگریشن ہزاروں پری ٹرینڈ لینگویج ماڈلز تک رسائی فراہم کرتا ہے، جس میں آسان کنورژن ٹولز شامل ہیں۔

### 5. LoRA فائن ٹیوننگ
لو رینک ایڈاپٹیشن (LoRA) کے لیے سپورٹ بڑے ماڈلز کی مؤثر فائن ٹیوننگ کو کم سے کم کمپیوٹیشنل وسائل کے ساتھ ممکن بناتا ہے۔

## انسٹالیشن گائیڈ

### سسٹم کی ضروریات
- **macOS 13.0+** (ایپل سلیکون آپٹیمائزیشن کے لیے)
- **Python 3.8+**
- **ایپل سلیکون** (M1، M2، M3، M4 سیریز)
- **نیٹو ARM ماحول** (Rosetta کے تحت نہ چل رہا ہو)
- **8GB+ RAM** (بڑے ماڈلز کے لیے 16GB+ تجویز کردہ)

### LLMs کے لیے فوری انسٹالیشن

لینگویج ماڈلز کے ساتھ شروعات کرنے کا سب سے آسان طریقہ MLX-LM انسٹال کرنا ہے:

```bash
pip install mlx-lm
```

یہ واحد کمانڈ MLX فریم ورک اور لینگویج ماڈل یوٹیلیٹیز دونوں کو انسٹال کرتی ہے۔

### ورچوئل ماحول ترتیب دینا (تجویز کردہ)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### آڈیو ماڈلز کے لیے اضافی ڈپینڈنسیز

اگر آپ اسپیک ماڈلز جیسے Whisper کے ساتھ کام کرنے کا ارادہ رکھتے ہیں:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## MLX کے ساتھ شروعات

### آپ کا پہلا لینگویج ماڈل

آئیے ایک سادہ ٹیکسٹ جنریشن مثال چلا کر شروع کریں:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Python API مثال

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### ماڈل لوڈنگ کو سمجھنا

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: لینگویج ماڈلز

### سپورٹڈ ماڈل آرکیٹیکچرز

MLX-LM کئی مشہور لینگویج ماڈل آرکیٹیکچرز کو سپورٹ کرتا ہے:

- **LLaMA اور LLaMA 2** - میٹا کے بنیادی ماڈلز
- **Mistral اور Mixtral** - مؤثر اور طاقتور ماڈلز
- **Phi-3** - مائیکروسافٹ کے کمپیکٹ لینگویج ماڈلز
- **Qwen** - علی بابا کے ملٹی لنگوئل ماڈلز
- **Code Llama** - کوڈ جنریشن کے لیے خاص
- **Gemma** - گوگل کے اوپن لینگویج ماڈلز

### کمانڈ لائن انٹرفیس

MLX-LM کمانڈ لائن انٹرفیس لینگویج ماڈلز کے ساتھ کام کرنے کے لیے طاقتور ٹولز فراہم کرتا ہے:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### ایڈوانس کیسز کے لیے Python API

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## بڑے لینگویج ماڈلز کے ساتھ کام کرنا

### ٹیکسٹ جنریشن پیٹرنز

#### سنگل ٹرن جنریشن
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### انسٹرکشن فالو کرنا
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### تخلیقی تحریر
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### ملٹی ٹرن گفتگو

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## ہگنگ فیس انٹیگریشن

### MLX کے ساتھ مطابقت رکھنے والے ماڈلز تلاش کرنا

MLX ہگنگ فیس ایکو سسٹم کے ساتھ بغیر کسی رکاوٹ کے کام کرتا ہے:

- **MLX ماڈلز براؤز کریں**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX کمیونٹی**: https://huggingface.co/mlx-community (پری کنورٹڈ ماڈلز)
- **اصل ماڈلز**: زیادہ تر LLaMA، Mistral، Phi، اور Qwen ماڈلز کنورژن کے ساتھ کام کرتے ہیں

### ہگنگ فیس سے ماڈلز لوڈ کرنا

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### آف لائن استعمال کے لیے ماڈلز ڈاؤن لوڈ کرنا

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## ماڈل کنورژن اور کوانٹائزیشن

### ہگنگ فیس ماڈلز کو MLX میں کنورٹ کرنا

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### کوانٹائزیشن کو سمجھنا

کوانٹائزیشن ماڈل کے سائز اور میموری کے استعمال کو کم کرتی ہے، معیار میں کم سے کم نقصان کے ساتھ:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### کسٹم کوانٹائزیشن

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## لینگویج ماڈلز کی فائن ٹیوننگ

### LoRA (لو رینک ایڈاپٹیشن) فائن ٹیوننگ

MLX مؤثر فائن ٹیوننگ کو LoRA کے ذریعے سپورٹ کرتا ہے، جو بڑے ماڈلز کو کم سے کم کمپیوٹیشنل وسائل کے ساتھ ایڈاپٹ کرنے کی اجازت دیتا ہے:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### ٹریننگ ڈیٹا تیار کرنا

اپنے ٹریننگ مثالوں کے ساتھ ایک JSON فائل بنائیں:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### فائن ٹیوننگ کمانڈ

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### فائن ٹیونڈ ماڈلز کا استعمال

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## LLM کی ایڈوانس خصوصیات

### مؤثریت کے لیے پرامپٹ کیشنگ

ایک ہی کانٹیکسٹ کے بار بار استعمال کے لیے، MLX پرامپٹ کیشنگ کو سپورٹ کرتا ہے تاکہ کارکردگی کو بہتر بنایا جا سکے:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### اسٹریمنگ ٹیکسٹ جنریشن

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### کوڈ جنریشن ماڈلز کے ساتھ کام کرنا

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### چیٹ ماڈلز کے ساتھ کام کرنا

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## LLMs کے لیے بہترین طریقے

### میموری مینجمنٹ

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### ماڈل سلیکشن کے رہنما اصول

**تجربہ اور سیکھنے کے لیے:**
- 4-بٹ کوانٹائزڈ ماڈلز استعمال کریں (جیسے `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- چھوٹے ماڈلز جیسے Phi-3-mini سے شروع کریں

**پروڈکشن ایپلیکیشنز کے لیے:**
- ماڈل کے سائز اور معیار کے درمیان توازن پر غور کریں
- کوانٹائزڈ اور فل پریسیشن ماڈلز دونوں کو ٹیسٹ کریں
- اپنے مخصوص استعمال کے کیسز پر بینچ مارک کریں

**مخصوص ٹاسکس کے لیے:**
- **کوڈ جنریشن**: CodeLlama، Code Llama Instruct
- **جنرل چیٹ**: Mistral-7B-Instruct، Phi-3
- **ملٹی لنگوئل**: Qwen ماڈلز
- **تخلیقی تحریر**: Mistral یا LLaMA کے ساتھ زیادہ درجہ حرارت کی ترتیبات

### پرامپٹ انجینئرنگ کے بہترین طریقے

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### کارکردگی کی اصلاح

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## مسائل کا حل

### عام مسائل اور ان کے حل

#### انسٹالیشن کے مسائل

**مسئلہ**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**حل**: نیٹو ARM Python یا Miniconda استعمال کریں:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### میموری کے مسائل

**مسئلہ**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### ماڈل لوڈنگ کے مسائل

**مسئلہ**: ماڈل لوڈ ہونے میں ناکام یا خراب آؤٹ پٹ پیدا کرتا ہے
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### کارکردگی کے مسائل

**مسئلہ**: جنریشن کی رفتار سست ہے
- دیگر میموری کے زیادہ استعمال والے ایپلیکیشنز بند کریں
- ممکن ہو تو کوانٹائزڈ ماڈلز استعمال کریں
- یقینی بنائیں کہ آپ Rosetta کے تحت نہیں چل رہے ہیں
- ماڈلز لوڈ کرنے سے پہلے دستیاب میموری چیک کریں

### ڈیبگنگ کے نکات

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## اضافی وسائل

### آفیشل دستاویزات اور ریپوزیٹریز

- **MLX GitHub ریپوزیٹری**: https://github.com/ml-explore/mlx
- **MLX-LM مثالیں**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX دستاویزات**: https://ml-explore.github.io/mlx/
- **ہگنگ فیس MLX انٹیگریشن**: https://huggingface.co/docs/hub/en/mlx

### ماڈل کلیکشنز

- **MLX کمیونٹی ماڈلز**: https://huggingface.co/mlx-community
- **ٹرینڈنگ MLX ماڈلز**: https://huggingface.co/models?library=mlx&sort=trending

### مثال ایپلیکیشنز

1. **ذاتی AI اسسٹنٹ**: گفتگو کی یادداشت کے ساتھ ایک مقامی چیٹ بوٹ بنائیں
2. **کوڈ ہیلپر**: اپنے ڈیولپمنٹ ورک فلو کے لیے ایک کوڈنگ اسسٹنٹ بنائیں
3. **مواد جنریٹر**: تحریر، خلاصہ، اور مواد تخلیق کے لیے ٹولز تیار کریں
4. **کسٹم فائن ٹیونڈ ماڈلز**: ڈومین مخصوص ٹاسکس کے لیے ماڈلز کو ایڈاپٹ کریں
5. **ملٹی موڈل ایپلیکیشنز**: ٹیکسٹ جنریشن کو دیگر MLX صلاحیتوں کے ساتھ جوڑیں

### کمیونٹی اور سیکھنا

- **MLX کمیونٹی ڈسکشنز**: GitHub Issues اور Discussions
- **ہگنگ فیس فورمز**: کمیونٹی سپورٹ اور ماڈل شیئرنگ
- **ایپل ڈیولپر دستاویزات**: آفیشل ایپل ML وسائل

### حوالہ

اگر آپ اپنی ریسرچ میں MLX استعمال کرتے ہیں تو براہ کرم حوالہ دیں:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## نتیجہ

ایپل MLX نے میک کمپیوٹرز پر بڑے لینگویج ماڈلز چلانے کے منظرنامے کو بدل دیا ہے۔ نیٹو ایپل سلیکون آپٹیمائزیشن، ہگنگ فیس انٹیگریشن، اور کوانٹائزیشن اور LoRA فائن ٹیوننگ جیسی طاقتور خصوصیات فراہم کر کے، MLX مقامی طور پر پیچیدہ لینگویج ماڈلز کو بہترین کارکردگی کے ساتھ چلانا ممکن بناتا ہے۔

چاہے آپ چیٹ بوٹس، کوڈ اسسٹنٹس، مواد جنریٹرز، یا کسٹم فائن ٹیونڈ ماڈلز بنا رہے ہوں، MLX آپ کے ایپل سلیکون میک کی مکمل صلاحیت کو لینگویج ماڈل ایپلیکیشنز کے لیے استعمال کرنے کے لیے ضروری ٹولز اور کارکردگی فراہم کرتا ہے۔ فریم ورک کی مؤثریت اور استعمال میں آسانی اسے ریسرچ اور پروڈکشن ایپلیکیشنز دونوں کے لیے ایک بہترین انتخاب بناتی ہے۔

اس ٹیوٹوریل میں بنیادی مثالوں سے شروعات کریں، ہگنگ فیس پر پری کنورٹڈ ماڈلز کے بھرپور ایکو سسٹم کو دریافت کریں، اور آہستہ آہستہ فائن ٹیوننگ اور کسٹم ماڈل ڈیولپمنٹ جیسی ایڈوانس خصوصیات تک پہنچیں۔ جیسے جیسے MLX ایکو سسٹم بڑھتا جا رہا ہے، یہ ایپل ہارڈویئر پر لینگویج ماڈل ڈیولپمنٹ کے لیے ایک طاقتور پلیٹ فارم بنتا جا رہا ہے۔

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔