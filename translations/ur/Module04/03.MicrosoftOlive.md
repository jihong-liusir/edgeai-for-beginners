<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T17:54:33+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ur"
}
-->
# سیکشن 3: مائیکروسافٹ اولیو آپٹیمائزیشن سوٹ

## فہرست مضامین
1. [تعارف](../../../Module04)
2. [مائیکروسافٹ اولیو کیا ہے؟](../../../Module04)
3. [انسٹالیشن](../../../Module04)
4. [جلدی شروع کرنے کی گائیڈ](../../../Module04)
5. [مثال: Qwen3 کو ONNX INT4 میں تبدیل کرنا](../../../Module04)
6. [ایڈوانسڈ استعمال](../../../Module04)
7. [بہترین طریقے](../../../Module04)
8. [مسائل کا حل](../../../Module04)
9. [اضافی وسائل](../../../Module04)

## تعارف

مائیکروسافٹ اولیو ایک طاقتور اور آسان استعمال کرنے والا ہارڈویئر-آگاہ ماڈل آپٹیمائزیشن ٹول کٹ ہے جو مشین لرننگ ماڈلز کو مختلف ہارڈویئر پلیٹ فارمز پر ڈپلائمنٹ کے لیے آپٹیمائز کرنے کے عمل کو آسان بناتا ہے۔ چاہے آپ CPUs، GPUs، یا خاص AI ایکسیلیریٹرز کو ہدف بنا رہے ہوں، اولیو آپ کو بہترین کارکردگی حاصل کرنے میں مدد دیتا ہے جبکہ ماڈل کی درستگی کو برقرار رکھتا ہے۔

## مائیکروسافٹ اولیو کیا ہے؟

اولیو ایک آسان استعمال کرنے والا ہارڈویئر-آگاہ ماڈل آپٹیمائزیشن ٹول ہے جو ماڈل کمپریشن، آپٹیمائزیشن، اور کمپائلیشن کے شعبے میں صنعت کے بہترین طریقوں کو یکجا کرتا ہے۔ یہ ONNX Runtime کے ساتھ ایک مکمل انفرنس آپٹیمائزیشن حل کے طور پر کام کرتا ہے۔

### اہم خصوصیات

- **ہارڈویئر-آگاہ آپٹیمائزیشن**: آپ کے ہدف ہارڈویئر کے لیے بہترین آپٹیمائزیشن تکنیک خودکار طور پر منتخب کرتا ہے  
- **40+ بلٹ ان آپٹیمائزیشن کمپوننٹس**: ماڈل کمپریشن، کوانٹائزیشن، گراف آپٹیمائزیشن، اور مزید شامل ہیں  
- **آسان CLI انٹرفیس**: عام آپٹیمائزیشن کاموں کے لیے سادہ کمانڈز  
- **ملٹی-فریم ورک سپورٹ**: PyTorch، Hugging Face ماڈلز، اور ONNX کے ساتھ کام کرتا ہے  
- **مشہور ماڈل سپورٹ**: اولیو خودکار طور پر مشہور ماڈل آرکیٹیکچرز جیسے Llama، Phi، Qwen، Gemma وغیرہ کو آپٹیمائز کر سکتا ہے  

### فوائد

- **ترقی کا وقت کم کریں**: مختلف آپٹیمائزیشن تکنیکوں کے ساتھ دستی تجربہ کرنے کی ضرورت نہیں  
- **کارکردگی میں اضافہ**: نمایاں رفتار میں بہتری (کچھ کیسز میں 6x تک)  
- **کراس-پلیٹ فارم ڈپلائمنٹ**: آپٹیمائزڈ ماڈلز مختلف ہارڈویئر اور آپریٹنگ سسٹمز پر کام کرتے ہیں  
- **درستگی برقرار رکھنا**: آپٹیمائزیشنز ماڈل کی کوالٹی کو برقرار رکھتے ہوئے کارکردگی کو بہتر بناتے ہیں  

## انسٹالیشن

### ضروریات

- Python 3.8 یا اس سے زیادہ  
- pip پیکیج مینیجر  
- ورچوئل ماحول (تجویز کردہ)  

### بنیادی انسٹالیشن

ورچوئل ماحول بنائیں اور فعال کریں:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

اولیو کو آٹو آپٹیمائزیشن فیچرز کے ساتھ انسٹال کریں:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### اختیاری ڈیپینڈنسیز

اولیو اضافی فیچرز کے لیے مختلف اختیاری ڈیپینڈنسیز پیش کرتا ہے:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### انسٹالیشن کی تصدیق کریں

```bash
olive --help
```

اگر کامیاب ہو، تو آپ کو اولیو CLI ہیلپ میسج نظر آنا چاہیے۔

## جلدی شروع کرنے کی گائیڈ

### آپ کی پہلی آپٹیمائزیشن

آئیے اولیو کے آٹو آپٹیمائزیشن فیچر کا استعمال کرتے ہوئے ایک چھوٹے لینگویج ماڈل کو آپٹیمائز کریں:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### یہ کمانڈ کیا کرتی ہے؟

آپٹیمائزیشن کا عمل شامل ہے: ماڈل کو لوکل کیش سے حاصل کرنا، ONNX گراف کو کیپچر کرنا اور ویٹس کو ONNX ڈیٹا فائل میں اسٹور کرنا، ONNX گراف کو آپٹیمائز کرنا، اور ماڈل کو RTN طریقہ استعمال کرتے ہوئے int4 میں کوانٹائز کرنا۔

### کمانڈ پیرامیٹرز کی وضاحت

- `--model_name_or_path`: Hugging Face ماڈل کی شناخت یا لوکل راستہ  
- `--output_path`: وہ ڈائریکٹری جہاں آپٹیمائزڈ ماڈل محفوظ ہوگا  
- `--device`: ہدف ڈیوائس (cpu, gpu)  
- `--provider`: ایکزیکیوشن پرووائیڈر (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)  
- `--use_ort_genai`: انفرنس کے لیے ONNX Runtime Generate AI استعمال کریں  
- `--precision`: کوانٹائزیشن کی درستگی (int4, int8, fp16)  
- `--log_level`: لاگنگ کی تفصیل (0=minimal, 1=verbose)  

## مثال: Qwen3 کو ONNX INT4 میں تبدیل کرنا

Hugging Face کی فراہم کردہ مثال [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) کی بنیاد پر، یہاں Qwen3 ماڈل کو آپٹیمائز کرنے کا طریقہ ہے:

### مرحلہ 1: ماڈل ڈاؤنلوڈ کریں (اختیاری)

ڈاؤنلوڈ کے وقت کو کم کرنے کے لیے صرف ضروری فائلیں کیش کریں:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### مرحلہ 2: Qwen3 ماڈل کو آپٹیمائز کریں

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### مرحلہ 3: آپٹیمائزڈ ماڈل کو ٹیسٹ کریں

آپٹیمائزڈ ماڈل کو ٹیسٹ کرنے کے لیے ایک سادہ Python اسکرپٹ بنائیں:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### آؤٹ پٹ کی ساخت

آپٹیمائزیشن کے بعد، آپ کی آؤٹ پٹ ڈائریکٹری میں شامل ہوگا:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## ایڈوانسڈ استعمال

### کنفیگریشن فائلز

زیادہ پیچیدہ آپٹیمائزیشن ورک فلو کے لیے، آپ JSON کنفیگریشن فائلز استعمال کر سکتے ہیں:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

کنفیگریشن کے ساتھ چلائیں:

```bash
olive run --config config.json
```

### GPU آپٹیمائزیشن

CUDA GPU آپٹیمائزیشن کے لیے:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) کے لیے:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### اولیو کے ساتھ فائن ٹیوننگ

اولیو ماڈلز کو فائن ٹیوننگ کی بھی سپورٹ کرتا ہے:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## بہترین طریقے

### 1. ماڈل کا انتخاب
- ٹیسٹنگ کے لیے چھوٹے ماڈلز سے شروع کریں (مثلاً، 0.5B-7B پیرامیٹرز)  
- یقینی بنائیں کہ آپ کا ہدف ماڈل آرکیٹیکچر اولیو کے ذریعے سپورٹڈ ہے  

### 2. ہارڈویئر کے پہلو
- آپٹیمائزیشن ہدف کو اپنے ڈپلائمنٹ ہارڈویئر سے ہم آہنگ کریں  
- اگر آپ کے پاس CUDA-کمپیٹیبل ہارڈویئر ہے تو GPU آپٹیمائزیشن استعمال کریں  
- Windows مشینز کے لیے DirectML پر غور کریں  

### 3. درستگی کا انتخاب
- **INT4**: زیادہ سے زیادہ کمپریشن، معمولی درستگی کا نقصان  
- **INT8**: سائز اور درستگی کا اچھا توازن  
- **FP16**: کم سے کم درستگی کا نقصان، درمیانی سائز میں کمی  

### 4. ٹیسٹنگ اور ویلیڈیشن
- ہمیشہ آپٹیمائزڈ ماڈلز کو اپنے مخصوص استعمال کے کیسز کے ساتھ ٹیسٹ کریں  
- کارکردگی کے میٹرکس کا موازنہ کریں (لیٹنسی، تھروپٹ، درستگی)  
- ویلیویشن کے لیے نمائندہ ان پٹ ڈیٹا استعمال کریں  

### 5. تکراری آپٹیمائزیشن
- جلدی نتائج کے لیے آٹو آپٹیمائزیشن سے شروع کریں  
- تفصیلی کنٹرول کے لیے کنفیگریشن فائلز استعمال کریں  
- مختلف آپٹیمائزیشن پاسز کے ساتھ تجربہ کریں  

## مسائل کا حل

### عام مسائل

#### 1. انسٹالیشن کے مسائل
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU کے مسائل
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. میموری کے مسائل
- آپٹیمائزیشن کے دوران چھوٹے بیچ سائز استعمال کریں  
- پہلے زیادہ درستگی کے ساتھ کوانٹائزیشن آزمائیں (int8 بجائے int4)  
- ماڈل کیشنگ کے لیے کافی ڈسک اسپیس یقینی بنائیں  

#### 4. ماڈل لوڈنگ کی غلطیاں
- ماڈل کے راستے اور رسائی کی اجازتوں کی تصدیق کریں  
- چیک کریں کہ آیا ماڈل کو `trust_remote_code=True` کی ضرورت ہے  
- یقینی بنائیں کہ تمام مطلوبہ ماڈل فائلز ڈاؤنلوڈ ہو چکی ہیں  

### مدد حاصل کرنا

- **دستاویزات**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)  
- **GitHub مسائل**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)  
- **مثالیں**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)  

## اضافی وسائل

### آفیشل لنکس
- **GitHub ریپوزٹری**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)  
- **ONNX Runtime دستاویزات**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)  
- **Hugging Face مثال**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)  

### کمیونٹی مثالیں
- **Jupyter نوٹ بکس**: اولیو GitHub ریپوزٹری میں دستیاب ہیں  
- **VS کوڈ ایکسٹینشن**: AI Toolkit ایکسٹینشن اولیو کو ماڈل آپٹیمائزیشن کے لیے استعمال کرتی ہے  
- **بلاگ پوسٹس**: مائیکروسافٹ اوپن سورس بلاگ میں اولیو کے تفصیلی ٹیوٹوریلز موجود ہیں  

### متعلقہ ٹولز
- **ONNX Runtime**: ہائی پرفارمنس انفرنس انجن  
- **Hugging Face Transformers**: بہت سے مطابقت پذیر ماڈلز کا ذریعہ  
- **Azure Machine Learning**: کلاؤڈ بیسڈ آپٹیمائزیشن ورک فلو  

## ➡️ آگے کیا کریں

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)  

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔