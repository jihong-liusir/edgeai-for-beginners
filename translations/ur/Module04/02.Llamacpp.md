<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T17:57:58+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "ur"
}
-->
# سیکشن 2: Llama.cpp عملدرآمد گائیڈ

## فہرست مواد
1. [تعارف](../../../Module04)
2. [Llama.cpp کیا ہے؟](../../../Module04)
3. [انسٹالیشن](../../../Module04)
4. [سورس سے بلڈنگ](../../../Module04)
5. [ماڈل کوانٹائزیشن](../../../Module04)
6. [بنیادی استعمال](../../../Module04)
7. [جدید خصوصیات](../../../Module04)
8. [پائتھون انٹیگریشن](../../../Module04)
9. [مسائل کا حل](../../../Module04)
10. [بہترین طریقے](../../../Module04)

## تعارف

یہ جامع ٹیوٹوریل آپ کو Llama.cpp کے بارے میں ہر چیز سکھائے گا، بنیادی انسٹالیشن سے لے کر جدید استعمال کے منظرناموں تک۔ Llama.cpp ایک طاقتور C++ عملدرآمد ہے جو بڑے زبان کے ماڈلز (LLMs) کو مؤثر طریقے سے چلانے کی صلاحیت فراہم کرتا ہے، کم سے کم سیٹ اپ کے ساتھ اور مختلف ہارڈویئر کنفیگریشنز پر بہترین کارکردگی کے ساتھ۔

## Llama.cpp کیا ہے؟

Llama.cpp ایک LLM انفرنس فریم ورک ہے جو C/C++ میں لکھا گیا ہے اور بڑے زبان کے ماڈلز کو مقامی طور پر چلانے کی صلاحیت فراہم کرتا ہے، کم سے کم سیٹ اپ کے ساتھ اور مختلف ہارڈویئر پر جدید کارکردگی کے ساتھ۔ اہم خصوصیات میں شامل ہیں:

### بنیادی خصوصیات
- **سادہ C/C++ عملدرآمد** بغیر کسی اضافی ڈپینڈنسی کے
- **کراس پلیٹ فارم مطابقت** (ونڈوز، میک او ایس، لینکس)
- **ہارڈویئر کی اصلاح** مختلف آرکیٹیکچرز کے لیے
- **کوانٹائزیشن سپورٹ** (1.5-bit سے 8-bit انٹیجر کوانٹائزیشن)
- **CPU اور GPU ایکسیلیریشن** سپورٹ
- **میموری کی مؤثریت** محدود ماحول کے لیے

### فوائد
- CPU پر مؤثر طریقے سے چلتا ہے، خصوصی ہارڈویئر کی ضرورت نہیں
- متعدد GPU بیک اینڈز کی سپورٹ (CUDA، Metal، OpenCL، Vulkan)
- ہلکا پھلکا اور پورٹیبل
- ایپل سلیکون کو ترجیح دی گئی ہے - ARM NEON، Accelerate اور Metal فریم ورک کے ذریعے بہتر بنایا گیا
- میموری کے استعمال کو کم کرنے کے لیے مختلف کوانٹائزیشن لیولز کی سپورٹ

## انسٹالیشن

### طریقہ 1: پہلے سے تیار کردہ بائنریز (نوآموزوں کے لیے تجویز کردہ)

#### GitHub ریلیزز سے ڈاؤنلوڈ کریں
1. [Llama.cpp GitHub ریلیزز](https://github.com/ggml-org/llama.cpp/releases) پر جائیں
2. اپنے سسٹم کے لیے مناسب بائنری ڈاؤنلوڈ کریں:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` ونڈوز کے لیے
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` میک او ایس کے لیے
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` لینکس کے لیے

3. آرکائیو کو نکالیں اور ڈائریکٹری کو اپنے سسٹم کے PATH میں شامل کریں

#### پیکیج مینیجرز کا استعمال

**میک او ایس (Homebrew):**
```bash
brew install llama.cpp
```

**لینکس (مختلف ڈسٹریبیوشنز):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### طریقہ 2: پائتھون پیکیج (llama-cpp-python)

#### بنیادی انسٹالیشن
```bash
pip install llama-cpp-python
```

#### ہارڈویئر ایکسیلیریشن کے ساتھ
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## سورس سے بلڈنگ

### ضروریات

**سسٹم کی ضروریات:**
- C++ کمپائلر (GCC، Clang، یا MSVC)
- CMake (ورژن 3.14 یا اس سے زیادہ)
- Git
- اپنے پلیٹ فارم کے لیے بلڈ ٹولز

**ضروریات کی انسٹالیشن:**

**میک او ایس:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**ونڈوز:**
- Visual Studio 2022 انسٹال کریں، C++ ڈیولپمنٹ ٹولز کے ساتھ
- CMake کو آفیشل ویب سائٹ سے انسٹال کریں
- Git انسٹال کریں

### بنیادی بلڈ پروسیس

1. **ریپوزٹری کلون کریں:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **بلڈ کو کنفیگر کریں:**
```bash
cmake -B build
```

3. **پروجیکٹ کو بلڈ کریں:**
```bash
cmake --build build --config Release
```

تیز کمپائلنگ کے لیے، پیرالل جابز استعمال کریں:
```bash
cmake --build build --config Release -j 8
```

### ہارڈویئر مخصوص بلڈز

#### CUDA سپورٹ (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal سپورٹ (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS سپورٹ (CPU آپٹیمائزیشن)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan سپورٹ
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### جدید بلڈ آپشنز

#### Debug بلڈ
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### اضافی خصوصیات کے ساتھ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## ماڈل کوانٹائزیشن

### GGUF فارمیٹ کو سمجھنا

GGUF (Generalized GGML Unified Format) ایک بہتر فائل فارمیٹ ہے جو بڑے زبان کے ماڈلز کو مؤثر طریقے سے چلانے کے لیے Llama.cpp اور دیگر فریم ورک کے ساتھ ڈیزائن کیا گیا ہے۔ یہ فراہم کرتا ہے:

- ماڈل ویٹ اسٹوریج کو معیاری بنانا
- پلیٹ فارمز کے درمیان بہتر مطابقت
- کارکردگی میں اضافہ
- مؤثر میٹا ڈیٹا ہینڈلنگ

### کوانٹائزیشن کی اقسام

Llama.cpp مختلف کوانٹائزیشن لیولز کی سپورٹ فراہم کرتا ہے:

| قسم | بٹس | تفصیل | استعمال کا کیس |
|------|------|-------------|----------|
| F16 | 16 | ہاف پریسجن | اعلیٰ معیار، بڑی میموری |
| Q8_0 | 8 | 8-bit کوانٹائزیشن | اچھا توازن |
| Q4_0 | 4 | 4-bit کوانٹائزیشن | درمیانی معیار، چھوٹا سائز |
| Q2_K | 2 | 2-bit کوانٹائزیشن | سب سے چھوٹا سائز، کم معیار |

### ماڈلز کو تبدیل کرنا

#### PyTorch سے GGUF میں
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face سے براہ راست ڈاؤنلوڈ
بہت سے ماڈلز GGUF فارمیٹ میں Hugging Face پر دستیاب ہیں:
- ماڈلز کو "GGUF" نام میں تلاش کریں
- مناسب کوانٹائزیشن لیول ڈاؤنلوڈ کریں
- Llama.cpp کے ساتھ براہ راست استعمال کریں

## بنیادی استعمال

### کمانڈ لائن انٹرفیس

#### سادہ ٹیکسٹ جنریشن
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face سے ماڈلز کا استعمال
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### سرور موڈ
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### عام پیرامیٹرز

| پیرامیٹر | تفصیل | مثال |
|-----------|-------------|---------|
| `-m` | ماڈل فائل کا راستہ | `-m model.gguf` |
| `-p` | پرامپٹ ٹیکسٹ | `-p "Hello world"` |
| `-n` | جنریٹ کرنے کے لیے ٹوکنز کی تعداد | `-n 100` |
| `-c` | کانٹیکسٹ سائز | `-c 4096` |
| `-t` | تھریڈز کی تعداد | `-t 8` |
| `-ngl` | GPU لیئرز | `-ngl 32` |
| `-temp` | ٹیمپریچر | `-temp 0.7` |

### انٹرایکٹو موڈ

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## جدید خصوصیات

### سرور API

#### سرور شروع کرنا
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API کا استعمال
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### کارکردگی کی اصلاح

#### میموری مینجمنٹ
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### ملٹی تھریڈنگ
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU ایکسیلیریشن
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## پائتھون انٹیگریشن

### llama-cpp-python کے ساتھ بنیادی استعمال

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### چیٹ انٹرفیس

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### اسٹریمنگ رسپانسز

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain کے ساتھ انٹیگریشن

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## مسائل کا حل

### عام مسائل اور حل

#### بلڈ ایررز

**مسئلہ: CMake نہیں ملا**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**مسئلہ: کمپائلر نہیں ملا**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### رن ٹائم مسائل

**مسئلہ: ماڈل لوڈنگ ناکام**
- ماڈل فائل کا راستہ چیک کریں
- فائل پرمیشنز چیک کریں
- کافی RAM یقینی بنائیں
- مختلف کوانٹائزیشن لیولز آزمائیں

**مسئلہ: ناقص کارکردگی**
- ہارڈویئر ایکسیلیریشن فعال کریں
- تھریڈز کی تعداد بڑھائیں
- مناسب کوانٹائزیشن استعمال کریں
- GPU میموری کا استعمال چیک کریں

#### میموری مسائل

**مسئلہ: میموری ختم**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### پلیٹ فارم مخصوص مسائل

#### ونڈوز
- MinGW یا Visual Studio کمپائلر استعمال کریں
- PATH کنفیگریشن درست کریں
- اینٹی وائرس مداخلت چیک کریں

#### میک او ایس
- Apple Silicon کے لیے Metal فعال کریں
- مطابقت کے لیے Rosetta 2 استعمال کریں اگر ضرورت ہو
- Xcode کمانڈ لائن ٹولز چیک کریں

#### لینکس
- ڈیولپمنٹ پیکیجز انسٹال کریں
- GPU ڈرائیور ورژنز چیک کریں
- CUDA ٹول کٹ انسٹالیشن کی تصدیق کریں

## بہترین طریقے

### ماڈل کا انتخاب
1. **اپنے ہارڈویئر کے مطابق مناسب کوانٹائزیشن منتخب کریں**
2. **ماڈل کے سائز اور معیار کے درمیان توازن پر غور کریں**
3. **اپنے مخصوص استعمال کے لیے مختلف ماڈلز آزمائیں**

### کارکردگی کی اصلاح
1. **GPU ایکسیلیریشن استعمال کریں** جب دستیاب ہو
2. **اپنے CPU کے لیے تھریڈز کی تعداد بہتر بنائیں**
3. **اپنے استعمال کے لیے مناسب کانٹیکسٹ سائز سیٹ کریں**
4. **بڑے ماڈلز کے لیے میموری میپنگ فعال کریں**

### پروڈکشن ڈیپلائمنٹ
1. **API رسائی کے لیے سرور موڈ استعمال کریں**
2. **مناسب ایرر ہینڈلنگ نافذ کریں**
3. **وسائل کے استعمال کی نگرانی کریں**
4. **لاگنگ اور مانیٹرنگ سیٹ اپ کریں**

### ڈیولپمنٹ ورک فلو
1. **ٹیسٹنگ کے لیے چھوٹے ماڈلز سے شروع کریں**
2. **ماڈل کنفیگریشنز کے لیے ورژن کنٹرول استعمال کریں**
3. **اپنی کنفیگریشنز کو دستاویزی بنائیں**
4. **مختلف پلیٹ فارمز پر ٹیسٹ کریں**

### سیکیورٹی کے تحفظات
1. **ان پٹ پرامپٹس کی تصدیق کریں**
2. **ریٹ لمیٹنگ نافذ کریں**
3. **API اینڈپوائنٹس کو محفوظ کریں**
4. **غلط استعمال کے پیٹرنز کی نگرانی کریں**

## نتیجہ

Llama.cpp ایک طاقتور اور مؤثر طریقہ فراہم کرتا ہے بڑے زبان کے ماڈلز کو مقامی طور پر مختلف ہارڈویئر کنفیگریشنز پر چلانے کے لیے۔ چاہے آپ AI ایپلیکیشنز تیار کر رہے ہوں، تحقیق کر رہے ہوں، یا صرف LLMs کے ساتھ تجربہ کر رہے ہوں، یہ فریم ورک آپ کے مختلف استعمال کے لیے لچک اور کارکردگی فراہم کرتا ہے۔

اہم نکات:
- وہ انسٹالیشن طریقہ منتخب کریں جو آپ کی ضروریات کے مطابق ہو
- اپنے مخصوص ہارڈویئر کنفیگریشن کے لیے اصلاح کریں
- بنیادی استعمال سے شروع کریں اور آہستہ آہستہ جدید خصوصیات کو دریافت کریں
- آسان انٹیگریشن کے لیے پائتھون بائنڈنگز پر غور کریں
- پروڈکشن ڈیپلائمنٹ کے لیے بہترین طریقے اپنائیں

مزید معلومات اور اپڈیٹس کے لیے، [Llama.cpp کی آفیشل ریپوزٹری](https://github.com/ggml-org/llama.cpp) پر جائیں اور دستیاب جامع دستاویزات اور کمیونٹی وسائل کا حوالہ دیں۔

## ➡️ آگے کیا ہے؟

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔