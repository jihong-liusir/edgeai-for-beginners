<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T18:06:57+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ur"
}
-->
# سیکشن 4: تعیناتی - پروڈکشن کے لیے تیار ماڈل کا نفاذ

## جائزہ

یہ جامع ٹیوٹوریل آپ کو Foundry Local کے ذریعے فائن ٹیونڈ کوانٹائزڈ ماڈلز کی تعیناتی کے مکمل عمل سے گزرنے میں رہنمائی کرے گا۔ ہم ماڈل کنورژن، کوانٹائزیشن آپٹیمائزیشن، اور تعیناتی کی ترتیب کو ابتدا سے آخر تک کور کریں گے۔

## ضروریات

شروع کرنے سے پہلے، یقینی بنائیں کہ آپ کے پاس درج ذیل چیزیں موجود ہیں:

- ✅ تعیناتی کے لیے تیار ایک فائن ٹیونڈ onnx ماڈل
- ✅ ونڈوز یا میک کمپیوٹر
- ✅ Python 3.10 یا اس سے جدید ورژن
- ✅ کم از کم 8GB دستیاب RAM
- ✅ آپ کے سسٹم پر انسٹال شدہ Foundry Local

## حصہ 1: ماحول کی ترتیب

### مطلوبہ ٹولز انسٹال کرنا

اپنے ٹرمینل (ونڈوز پر کمانڈ پرامپٹ، میک پر ٹرمینل) کو کھولیں اور درج ذیل کمانڈز کو ترتیب وار چلائیں:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **اہم نوٹ**: آپ کو CMake ورژن 3.31 یا جدید تر کی بھی ضرورت ہوگی، جسے [cmake.org](https://cmake.org/download/) سے ڈاؤنلوڈ کیا جا سکتا ہے۔

## حصہ 2: ماڈل کنورژن اور کوانٹائزیشن

### صحیح فارمیٹ کا انتخاب

چھوٹے فائن ٹیونڈ لینگویج ماڈلز کے لیے، ہم **ONNX فارمیٹ** استعمال کرنے کی تجویز دیتے ہیں کیونکہ یہ فراہم کرتا ہے:

- 🚀 بہتر کارکردگی کی آپٹیمائزیشن
- 🔧 ہارڈویئر سے آزاد تعیناتی
- 🏭 پروڈکشن کے لیے تیار صلاحیتیں
- 📱 کراس پلیٹ فارم مطابقت

### طریقہ 1: ایک کمانڈ کنورژن (تجویز کردہ)

اپنے فائن ٹیونڈ ماڈل کو براہ راست کنورٹ کرنے کے لیے درج ذیل کمانڈ استعمال کریں:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**پیرامیٹر کی وضاحت:**
- `--model_name_or_path`: آپ کے فائن ٹیونڈ ماڈل کا راستہ
- `--device cpu`: آپٹیمائزیشن کے لیے CPU استعمال کریں
- `--precision int4`: INT4 کوانٹائزیشن استعمال کریں (تقریباً 75% سائز کی کمی)
- `--output_path`: کنورٹڈ ماڈل کے لیے آؤٹ پٹ راستہ

### طریقہ 2: کنفیگریشن فائل کا طریقہ (ایڈوانسڈ یوزرز)

`finetuned_conversion_config.json` نامی ایک کنفیگریشن فائل بنائیں:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

پھر چلائیں:

```bash
olive run --config ./finetuned_conversion_config.json
```

### کوانٹائزیشن آپشنز کا موازنہ

| پریسیژن | فائل سائز | انفیرنس کی رفتار | ماڈل کی کوالٹی | تجویز کردہ استعمال |
|-----------|-----------|-----------------|---------------|-----------------|
| FP16      | بیس لائن × 0.5 | تیز | بہترین | ہائی اینڈ ہارڈویئر |
| INT8      | بیس لائن × 0.25 | بہت تیز | اچھی | متوازن انتخاب |
| INT4      | بیس لائن × 0.125 | سب سے تیز | قابل قبول | محدود وسائل |

💡 **تجویز**: اپنی پہلی تعیناتی کے لیے INT4 کوانٹائزیشن سے شروع کریں۔ اگر کوالٹی تسلی بخش نہ ہو، تو INT8 یا FP16 آزمائیں۔

## حصہ 3: Foundry Local تعیناتی کی ترتیب

### ماڈل کنفیگریشن بنانا

Foundry Local ماڈلز ڈائریکٹری پر جائیں:

```bash
foundry cache cd ./models/
```

اپنے ماڈل کی ڈائریکٹری کا ڈھانچہ بنائیں:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

اپنی ماڈل ڈائریکٹری میں `inference_model.json` کنفیگریشن فائل بنائیں:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### ماڈل کے لیے مخصوص ٹیمپلیٹ کنفیگریشنز

#### Qwen سیریز ماڈلز کے لیے:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## حصہ 4: ماڈل کی جانچ اور آپٹیمائزیشن

### ماڈل انسٹالیشن کی تصدیق

چیک کریں کہ آیا Foundry Local آپ کے ماڈل کو پہچان سکتا ہے:

```bash
foundry cache ls
```

آپ کو `your-finetuned-model-int4` فہرست میں نظر آنا چاہیے۔

### ماڈل کی جانچ شروع کرنا

```bash
foundry model run your-finetuned-model-int4
```

### کارکردگی کی بینچ مارکنگ

جانچ کے دوران کلیدی میٹرکس کی نگرانی کریں:

1. **جواب کا وقت**: ہر جواب کے لیے اوسط وقت کی پیمائش کریں
2. **میموری کا استعمال**: RAM کی کھپت کی نگرانی کریں
3. **CPU کا استعمال**: پروسیسر کے لوڈ کو چیک کریں
4. **آؤٹ پٹ کوالٹی**: جواب کی مطابقت اور ہم آہنگی کا جائزہ لیں

### کوالٹی ویلیڈیشن چیک لسٹ

- ✅ ماڈل فائن ٹیونڈ ڈومین کے سوالات کا مناسب جواب دیتا ہے
- ✅ جواب کا فارمیٹ متوقع آؤٹ پٹ ڈھانچے سے میل کھاتا ہے
- ✅ طویل استعمال کے دوران کوئی میموری لیک نہیں
- ✅ مختلف ان پٹ لمبائیوں پر مستقل کارکردگی
- ✅ کنارے کے کیسز اور غلط ان پٹس کو مناسب طریقے سے ہینڈل کرنا

## خلاصہ

مبارک ہو! آپ نے کامیابی سے مکمل کیا:

- ✅ فائن ٹیونڈ ماڈل فارمیٹ کنورژن
- ✅ ماڈل کوانٹائزیشن آپٹیمائزیشن
- ✅ Foundry Local تعیناتی کی ترتیب
- ✅ کارکردگی کی ٹیوننگ اور خرابیوں کا پتہ لگانا

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔