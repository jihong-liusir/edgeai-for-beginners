<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T20:27:27+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ur"
}
-->
# سیکشن 2: قیوین فیملی کے بنیادی اصول

قیوین ماڈل فیملی علی بابا کلاؤڈ کی بڑی زبان ماڈلز اور ملٹی موڈل AI کے لیے جامع نقطہ نظر کی نمائندگی کرتی ہے، جو یہ ظاہر کرتی ہے کہ اوپن سورس ماڈلز مختلف تعیناتی منظرناموں میں قابل رسائی ہونے کے ساتھ ساتھ شاندار کارکردگی حاصل کر سکتے ہیں۔ یہ سمجھنا ضروری ہے کہ قیوین فیملی کس طرح طاقتور AI صلاحیتوں کو لچکدار تعیناتی کے اختیارات کے ساتھ فعال کرتی ہے جبکہ مختلف کاموں میں مسابقتی کارکردگی کو برقرار رکھتی ہے۔

## ڈویلپرز کے لیے وسائل

### ہگنگ فیس ماڈل ریپوزٹری
قیوین فیملی کے منتخب ماڈلز [ہگنگ فیس](https://huggingface.co/models?search=qwen) کے ذریعے دستیاب ہیں، جو ان ماڈلز کے کچھ ویریئنٹس تک رسائی فراہم کرتے ہیں۔ آپ دستیاب ویریئنٹس کو دریافت کر سکتے ہیں، انہیں اپنے مخصوص استعمال کے معاملات کے لیے فائن ٹیون کر سکتے ہیں، اور مختلف فریم ورک کے ذریعے تعینات کر سکتے ہیں۔

### لوکل ڈیولپمنٹ ٹولز
لوکل ڈیولپمنٹ اور ٹیسٹنگ کے لیے، آپ [مائیکروسافٹ فاؤنڈری لوکل](https://github.com/microsoft/foundry-local) استعمال کر سکتے ہیں تاکہ دستیاب قیوین ماڈلز کو اپنی ڈیولپمنٹ مشین پر بہتر کارکردگی کے ساتھ چلایا جا سکے۔

### دستاویزات کے وسائل
- [قیوین ماڈل دستاویزات](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ایج ڈیپلائمنٹ کے لیے قیوین ماڈلز کو بہتر بنانا](https://github.com/microsoft/olive)

## تعارف

اس ٹیوٹوریل میں، ہم علی بابا کے قیوین ماڈل فیملی اور اس کے بنیادی تصورات کو دریافت کریں گے۔ ہم قیوین فیملی کے ارتقاء، جدید تربیتی طریقوں، اہم ویریئنٹس، اور مختلف منظرناموں میں عملی اطلاق کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس ٹیوٹوریل کے اختتام تک، آپ:

- علی بابا کے قیوین ماڈل فیملی کے ڈیزائن فلسفہ اور ارتقاء کو سمجھ سکیں گے۔
- ان اہم جدتوں کی شناخت کر سکیں گے جو قیوین ماڈلز کو مختلف پیرامیٹر سائزز میں اعلیٰ کارکردگی حاصل کرنے کے قابل بناتی ہیں۔
- مختلف قیوین ماڈل ویریئنٹس کے فوائد اور حدود کو پہچان سکیں گے۔
- حقیقی دنیا کے منظرناموں کے لیے مناسب ویریئنٹس کا انتخاب کرنے کے لیے قیوین ماڈلز کے علم کو لاگو کر سکیں گے۔

## جدید AI ماڈل منظرنامے کو سمجھنا

AI کا منظرنامہ نمایاں طور پر ترقی کر چکا ہے، مختلف تنظیمیں زبان ماڈل کی ترقی کے لیے مختلف طریقے اختیار کر رہی ہیں۔ کچھ تنظیمیں ملکیتی بند سورس ماڈلز پر توجہ مرکوز کرتی ہیں، جبکہ دیگر اوپن سورس رسائی اور شفافیت کو ترجیح دیتی ہیں۔ روایتی طریقہ یا تو بڑے ملکیتی ماڈلز پر مشتمل ہوتا ہے جو صرف APIs کے ذریعے قابل رسائی ہوتے ہیں یا اوپن سورس ماڈلز جو صلاحیتوں میں پیچھے رہ سکتے ہیں۔

یہ نقطہ نظر ان تنظیموں کے لیے چیلنجز پیدا کرتا ہے جو طاقتور AI صلاحیتوں کی تلاش میں ہیں جبکہ اپنے ڈیٹا، اخراجات، اور تعیناتی کی لچک پر کنٹرول برقرار رکھنا چاہتے ہیں۔ روایتی طریقہ اکثر جدید کارکردگی اور عملی تعیناتی کے تحفظات کے درمیان انتخاب کی ضرورت ہوتی ہے۔

## قابل رسائی AI کی عمدگی کا چیلنج

اعلیٰ معیار کے، قابل رسائی AI کی ضرورت مختلف منظرناموں میں بڑھتی جا رہی ہے۔ ان ایپلیکیشنز پر غور کریں جنہیں مختلف تنظیمی ضروریات کے لیے لچکدار تعیناتی کے اختیارات، لاگت سے موثر نفاذ، جہاں API کے اخراجات اہم ہو سکتے ہیں، عالمی ایپلیکیشنز کے لیے کثیر لسانی صلاحیتیں، یا کوڈنگ اور ریاضی جیسے شعبوں میں خصوصی مہارت کی ضرورت ہوتی ہے۔

### کلیدی تعیناتی کی ضروریات

جدید AI تعیناتیوں کو کئی بنیادی ضروریات کا سامنا کرنا پڑتا ہے جو عملی اطلاق کو محدود کرتی ہیں:

- **رسائی**: شفافیت اور حسب ضرورت کے لیے اوپن سورس دستیابی
- **لاگت کی تاثیر**: مختلف بجٹ کے لیے معقول کمپیوٹیشنل ضروریات
- **لچک**: مختلف تعیناتی منظرناموں کے لیے متعدد ماڈل سائزز
- **عالمی رسائی**: مضبوط کثیر لسانی اور ثقافتی صلاحیتیں
- **خصوصیت**: مخصوص استعمال کے معاملات کے لیے ڈومین مخصوص ویریئنٹس

## قیوین ماڈل فلسفہ

قیوین ماڈل فیملی AI ماڈل کی ترقی کے لیے ایک جامع نقطہ نظر کی نمائندگی کرتی ہے، جو اوپن سورس رسائی، کثیر لسانی صلاحیتوں، اور عملی تعیناتی کو ترجیح دیتی ہے جبکہ مسابقتی کارکردگی کی خصوصیات کو برقرار رکھتی ہے۔ قیوین ماڈلز مختلف ماڈل سائزز، اعلیٰ معیار کی تربیتی طریقوں، اور مختلف ڈومینز کے لیے خصوصی ویریئنٹس کے ذریعے یہ حاصل کرتے ہیں۔

قیوین فیملی مختلف نقطہ نظر کو شامل کرتی ہے جو کارکردگی-افادیت کے اسپیکٹرم میں اختیارات فراہم کرتی ہے، موبائل ڈیوائسز سے لے کر انٹرپرائز سرورز تک تعیناتی کو فعال کرتی ہے جبکہ بامعنی AI صلاحیتیں فراہم کرتی ہے۔ مقصد اعلیٰ معیار کے AI تک رسائی کو جمہوری بنانا ہے جبکہ تعیناتی کے انتخاب میں لچک فراہم کرنا ہے۔

### قیوین کے بنیادی ڈیزائن اصول

قیوین ماڈلز کئی بنیادی اصولوں پر مبنی ہیں جو انہیں دیگر زبان ماڈل فیملیز سے ممتاز کرتے ہیں:

- **اوپن سورس پہلے**: تحقیق اور تجارتی استعمال کے لیے مکمل شفافیت اور رسائی
- **جامع تربیت**: متعدد زبانوں اور ڈومینز کا احاطہ کرنے والے بڑے، متنوع ڈیٹا سیٹس پر تربیت
- **قابل توسیع آرکیٹیکچر**: مختلف کمپیوٹیشنل ضروریات سے مطابقت رکھنے کے لیے متعدد ماڈل سائزز
- **خصوصی عمدگی**: مخصوص کاموں کے لیے بہتر ڈومین مخصوص ویریئنٹس

## قیوین فیملی کو فعال کرنے والی کلیدی ٹیکنالوجیز

### بڑے پیمانے پر تربیت

قیوین فیملی کی ایک نمایاں خصوصیت تربیتی ڈیٹا اور ماڈل کی ترقی میں سرمایہ کاری کیے گئے کمپیوٹیشنل وسائل کا بڑا پیمانہ ہے۔ قیوین ماڈلز احتیاط سے تیار کردہ، کثیر لسانی ڈیٹا سیٹس کا فائدہ اٹھاتے ہیں جو کھربوں ٹوکنز پر مشتمل ہیں، جو جامع عالمی علم اور استدلال کی صلاحیتیں فراہم کرنے کے لیے ڈیزائن کیے گئے ہیں۔

یہ طریقہ اعلیٰ معیار کے ویب مواد، علمی ادب، کوڈ ریپوزٹریز، اور کثیر لسانی وسائل کو یکجا کر کے کام کرتا ہے۔ تربیتی طریقہ کار مختلف ڈومینز اور زبانوں میں علم کی وسعت اور تفہیم کی گہرائی دونوں پر زور دیتا ہے۔

### جدید استدلال اور سوچ

حالیہ قیوین ماڈلز پیچیدہ ملٹی اسٹیپ مسئلہ حل کرنے کو فعال کرنے والی نفیس استدلال کی صلاحیتوں کو شامل کرتے ہیں:

**سوچنے کا موڈ (قیوین3)**: ماڈلز تفصیلی مرحلہ وار استدلال میں مشغول ہو سکتے ہیں اس سے پہلے کہ حتمی جوابات فراہم کریں، انسانی مسئلہ حل کرنے کے طریقوں کی طرح۔

**ڈوئل موڈ آپریشن**: سادہ سوالات کے لیے فوری جواب موڈ اور پیچیدہ مسائل کے لیے گہری سوچ کے موڈ کے درمیان سوئچ کرنے کی صلاحیت۔

**چین آف تھوٹ انٹیگریشن**: استدلال کے مراحل کا قدرتی طور پر شامل ہونا جو پیچیدہ کاموں میں شفافیت اور درستگی کو بہتر بناتا ہے۔

### آرکیٹیکچرل جدتیں

قیوین فیملی کارکردگی اور افادیت دونوں کے لیے کئی آرکیٹیکچرل اصلاحات کو شامل کرتی ہے:

**قابل توسیع ڈیزائن**: ماڈل سائزز کے درمیان مستقل آرکیٹیکچر جو آسان توسیع اور موازنہ کو فعال کرتا ہے۔

**ملٹی موڈل انٹیگریشن**: متن، وژن، اور آڈیو پروسیسنگ کی صلاحیتوں کا ایک متحد آرکیٹیکچر میں ہموار انضمام۔

**تعیناتی کی اصلاح**: مختلف ہارڈویئر کنفیگریشنز کے لیے متعدد کوانٹائزیشن آپشنز اور تعیناتی فارمیٹس۔

## ماڈل سائز اور تعیناتی کے اختیارات

جدید تعیناتی کے ماحول قیوین ماڈلز کی مختلف کمپیوٹیشنل ضروریات کے لیے لچک سے فائدہ اٹھاتے ہیں:

### چھوٹے ماڈلز (0.5B-3B)

قیوین چھوٹے ماڈلز فراہم کرتا ہے جو ایج ڈیپلائمنٹ، موبائل ایپلیکیشنز، اور محدود وسائل والے ماحول کے لیے موزوں ہیں جبکہ متاثر کن صلاحیتوں کو برقرار رکھتے ہیں۔

### درمیانے ماڈلز (7B-32B)

درمیانے درجے کے ماڈلز پیشہ ورانہ ایپلیکیشنز کے لیے بہتر صلاحیتیں فراہم کرتے ہیں، کارکردگی اور کمپیوٹیشنل ضروریات کے درمیان بہترین توازن فراہم کرتے ہیں۔

### بڑے ماڈلز (72B+)

فل اسکیل ماڈلز جدید ایپلیکیشنز، تحقیق، اور انٹرپرائز تعیناتیوں کے لیے زیادہ سے زیادہ صلاحیت کی ضرورت کے لیے جدید کارکردگی فراہم کرتے ہیں۔

## قیوین ماڈل فیملی کے فوائد

### اوپن سورس رسائی

قیوین ماڈلز مکمل شفافیت اور حسب ضرورت صلاحیتیں فراہم کرتے ہیں، تنظیموں کو ماڈلز کو سمجھنے، ترمیم کرنے، اور اپنی مخصوص ضروریات کے مطابق ڈھالنے کے قابل بناتے ہیں بغیر وینڈر لاک ان کے۔

### تعیناتی کی لچک

ماڈل سائزز کی رینج مختلف ہارڈویئر کنفیگریشنز پر تعیناتی کو فعال کرتی ہے، موبائل ڈیوائسز سے لے کر ہائی اینڈ سرورز تک، تنظیموں کو ان کے AI انفراسٹرکچر کے انتخاب میں لچک فراہم کرتی ہے۔

### کثیر لسانی عمدگی

قیوین ماڈلز کثیر لسانی تفہیم اور جنریشن میں مہارت رکھتے ہیں، درجنوں زبانوں کو سپورٹ کرتے ہیں، خاص طور پر انگریزی اور چینی میں مضبوط، جو انہیں عالمی ایپلیکیشنز کے لیے موزوں بناتا ہے۔

### مسابقتی کارکردگی

قیوین ماڈلز مسلسل بینچ مارکس پر مسابقتی نتائج حاصل کرتے ہیں جبکہ اوپن سورس رسائی فراہم کرتے ہیں، یہ ظاہر کرتے ہوئے کہ اوپن ماڈلز ملکیتی متبادلات سے میل کھا سکتے ہیں۔

### خصوصی صلاحیتیں

قیوین-کوڈر اور قیوین-میتھ جیسے ڈومین مخصوص ویریئنٹس عمومی زبان کی تفہیم کی صلاحیتوں کو برقرار رکھتے ہوئے خصوصی مہارت فراہم کرتے ہیں۔

## عملی مثالیں اور استعمال کے معاملات

تکنیکی تفصیلات میں جانے سے پہلے، آئیے کچھ ٹھوس مثالوں کو دریافت کریں کہ قیوین ماڈلز کیا حاصل کر سکتے ہیں:

### ریاضیاتی استدلال کی مثال

قیوین-میتھ مرحلہ وار ریاضیاتی مسئلہ حل کرنے میں مہارت رکھتا ہے۔ مثال کے طور پر، جب پیچیدہ کیلکولس مسئلہ حل کرنے کے لیے کہا جائے:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### کثیر لسانی سپورٹ کی مثال

قیوین ماڈلز مختلف زبانوں میں مضبوط کثیر لسانی صلاحیتوں کا مظاہرہ کرتے ہیں:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### ملٹی موڈل صلاحیتوں کی مثال

قیوین-VL متن اور تصاویر کو بیک وقت پروسیس کر سکتا ہے:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### کوڈ جنریشن کی مثال

قیوین-کوڈر متعدد پروگرامنگ زبانوں میں کوڈ جنریٹ کرنے اور وضاحت کرنے میں مہارت رکھتا ہے:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

یہ نفاذ بہترین طریقوں پر عمل کرتا ہے، واضح ویریبل ناموں، جامع دستاویزات، اور موثر منطق کے ساتھ۔
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# موبائل ڈیوائس پر کوانٹائزیشن کے ساتھ مثال تعیناتی
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# موبائل تعیناتی کے لیے کوانٹائزڈ ماڈل لوڈ کریں

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## قیوین فیملی کا ارتقاء

### قیوین 1.0 اور 1.5: بنیادی ماڈلز

ابتدائی قیوین ماڈلز نے جامع تربیت اور اوپن سورس رسائی کے بنیادی اصول قائم کیے:

- **قیوین-7B (7B پیرامیٹرز)**: ابتدائی ریلیز جو چینی اور انگریزی زبان کی تفہیم پر مرکوز ہے۔
- **قیوین-14B (14B پیرامیٹرز)**: بہتر استدلال اور علم کے ساتھ صلاحیتوں میں اضافہ۔
- **قیوین-72B (72B پیرامیٹرز)**: بڑے پیمانے پر ماڈل جو جدید کارکردگی فراہم کرتا ہے۔
- **قیوین1.5 سیریز**: بہتر طویل سیاق و سباق کے ساتھ متعدد سائزز (0.5B سے 110B) تک توسیع۔

### قیوین2 فیملی: ملٹی موڈل توسیع

قیوین2 سیریز نے زبان اور ملٹی موڈل صلاحیتوں میں نمایاں ترقی کی نشاندہی کی:

- **قیوین2-0.5B سے 72B**: مختلف تعیناتی کی ضروریات کے لیے زبان کے ماڈلز کی جامع رینج۔
- **قیوین2-57B-A14B (MoE)**: پیرامیٹر کے موثر استعمال کے لیے ماہرین کے مرکب کی آرکیٹیکچر۔
- **قیوین2-VL**: امیج کی تفہیم کے لیے جدید وژن-زبان کی صلاحیتیں۔
- **قیوین2-آڈیو**: آڈیو پروسیسنگ اور تفہیم کی صلاحیتیں۔
- **قیوین2-میتھ**: ریاضیاتی استدلال اور مسئلہ حل کرنے میں مہارت۔

### قیوین2.5 فیملی: بہتر کارکردگی

قیوین2.5 سیریز نے تمام جہتوں میں نمایاں بہتری لائی:

- **توسیع شدہ تربیت**: 18 کھرب ٹوکنز کے تربیتی ڈیٹا کے ساتھ بہتر صلاحیتیں۔
- **بڑھا ہوا سیاق**: 128K ٹوکنز سیاق کی لمبائی، ٹربو ویریئنٹ کے ساتھ 1M ٹوکنز کی حمایت۔
- **بہتر خصوصیت**: بہتر قیوین2.5-کوڈر اور قیوین2.5-میتھ ویریئنٹس۔
- **بہتر کثیر لسانی سپورٹ**: 27+ زبانوں میں بہتر کارکردگی۔

### قیوین3 فیملی: جدید استدلال

تازہ ترین نسل استدلال اور سوچنے کی صلاحیتوں کی حدود کو آگے بڑھاتی ہے:

- **قیوین3-235B-A22B**: 235B کل پیرامیٹرز کے ساتھ فلیگ شپ ماہرین کے مرکب ماڈل۔
- **قیوین3-30B-A3B**: فعال پیرامیٹرز فی مضبوط کارکردگی کے ساتھ موثر MoE ماڈل۔
- **ڈینس ماڈلز**: مختلف تعیناتی منظرناموں کے لیے قیوین3-32B، 14B، 8B، 4B، 1.7B، 0.6B۔
- **سوچنے کا موڈ**: فوری جوابات اور گہری سوچ دونوں کی حمایت کرنے والا ہائبرڈ استدلال کا طریقہ۔
- **کثیر لسانی عمدگی**: 119 زبانوں اور بولیوں کے لیے سپورٹ۔
- **بہتر تربیت**: 36 کھرب ٹوکنز کے متنوع، اعلیٰ معیار کے تربیتی ڈیٹا۔

## قیوین ماڈلز کے اطلاقات

### انٹرپرائز ایپلیکیشنز

تنظیمیں قیوین ماڈلز کو دستاویز کے تجزیے، کسٹمر سروس آٹومیشن، کوڈ جنریشن اسسٹنس، اور بزنس انٹیلیجنس ایپلیکیشنز کے لیے استعمال کرتی ہیں۔ اوپن سورس نوعیت مخصوص کاروباری ضروریات کے لیے حسب ضرورت کو فعال کرتی ہے جبکہ ڈیٹا کی رازداری اور کنٹرول کو برقرار رکھتی ہے۔

### موبائل اور ایج کمپیوٹنگ

موبائل ایپلیکیشنز قیوین ماڈلز کو حقیقی وقت کے ترجمے، ذہین معاونین، مواد کی تخلیق، اور ذاتی سفارشات کے لیے استعمال کرتی ہیں۔ ماڈل سائزز کی رینج موبائل ڈیوائسز سے ایج سرورز تک تعیناتی کو فعال کرتی ہے۔

### تعلیمی ٹیکنالوجی

تعلیمی پلیٹ فارمز قیوین ماڈلز کو ذاتی ٹیوٹرنگ، خودکار مواد کی تخلیق، زبان سیکھنے کی مدد، اور انٹرایکٹو تعلیمی تجربات کے لیے استعمال کرتے ہیں۔ قیوین-میتھ جیسے خصوصی ماڈلز ڈومین مخصوص مہارت فراہم کرتے ہیں۔

### عالمی ایپلیکیشنز

بین الاقوامی ایپلیکیشنز ق
- Qwen3-235B-A22B نے کوڈنگ، ریاضی، اور عمومی صلاحیتوں کے بینچ مارک میں دیگر اعلیٰ ماڈلز جیسے DeepSeek-R1، o1، o3-mini، Grok-3، اور Gemini-2.5-Pro کے مقابلے میں شاندار نتائج حاصل کیے ہیں۔
- Qwen3-30B-A3B نے QwQ-32B کو 10 گنا زیادہ فعال پیرامیٹرز کے ساتھ پیچھے چھوڑ دیا۔
- Qwen3-4B کی کارکردگی Qwen2.5-72B-Instruct کے برابر ہو سکتی ہے۔

**کارکردگی کی کامیابیاں:**
- Qwen3-MoE بنیادی ماڈلز نے Qwen2.5 کے ڈینس بنیادی ماڈلز کے برابر کارکردگی حاصل کی ہے، جبکہ صرف 10% فعال پیرامیٹرز استعمال کیے ہیں۔
- تربیت اور انفرنس میں ڈینس ماڈلز کے مقابلے میں نمایاں لاگت کی بچت۔

**کثیر لسانی صلاحیتیں:**
- Qwen3 ماڈلز 119 زبانوں اور بولیوں کی حمایت کرتے ہیں۔
- مختلف لسانی اور ثقافتی سیاق و سباق میں مضبوط کارکردگی۔

**تربیتی پیمانہ:**
- Qwen3 تقریباً 36 ٹریلین ٹوکنز استعمال کرتا ہے، جو 119 زبانوں اور بولیوں کا احاطہ کرتا ہے، جبکہ Qwen2.5 کے 18 ٹریلین ٹوکنز کے مقابلے میں تقریباً دوگنا ہے۔

### ماڈل موازنہ میٹرکس

| ماڈل سیریز | پیرامیٹرز کی حد | سیاق کی لمبائی | کلیدی طاقتیں | بہترین استعمال کے کیسز |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | متوازن کارکردگی، کثیر لسانی | عمومی ایپلیکیشنز، پروڈکشن ڈیپلائمنٹ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | کوڈ جنریشن، پروگرامنگ | سافٹ ویئر ڈیولپمنٹ، کوڈنگ اسسٹنس |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ریاضیاتی استدلال | تعلیمی پلیٹ فارمز، STEM ایپلیکیشنز |
| **Qwen2.5-VL** | مختلف | متغیر | وژن-زبان کی سمجھ | ملٹی موڈل ایپلیکیشنز، امیج تجزیہ |
| **Qwen3** | 0.6B-235B | متغیر | اعلیٰ استدلال، سوچنے کا موڈ | پیچیدہ استدلال، تحقیقی ایپلیکیشنز |
| **Qwen3 MoE** | 30B-235B کل | متغیر | موثر بڑے پیمانے کی کارکردگی | انٹرپرائز ایپلیکیشنز، اعلیٰ کارکردگی کی ضروریات |

## ماڈل انتخاب گائیڈ

### بنیادی ایپلیکیشنز کے لیے
- **Qwen2.5-0.5B/1.5B**: موبائل ایپس، ایج ڈیوائسز، ریئل ٹائم ایپلیکیشنز
- **Qwen2.5-3B/7B**: عمومی چیٹ بوٹس، مواد کی تخلیق، سوال و جواب کے نظام

### ریاضیاتی اور استدلالی کاموں کے لیے
- **Qwen2.5-Math**: ریاضیاتی مسئلہ حل کرنے اور STEM تعلیم
- **Qwen3 سوچنے کے موڈ کے ساتھ**: پیچیدہ استدلال جو قدم بہ قدم تجزیہ کی ضرورت ہو

### پروگرامنگ اور ڈیولپمنٹ کے لیے
- **Qwen2.5-Coder**: کوڈ جنریشن، ڈیبگنگ، پروگرامنگ اسسٹنس
- **Qwen3**: استدلالی صلاحیتوں کے ساتھ اعلیٰ پروگرامنگ کام

### ملٹی موڈل ایپلیکیشنز کے لیے
- **Qwen2.5-VL**: امیج کی سمجھ، بصری سوال و جواب
- **Qwen-Audio**: آڈیو پروسیسنگ اور تقریر کی سمجھ

### انٹرپرائز ڈیپلائمنٹ کے لیے
- **Qwen2.5-32B/72B**: اعلیٰ کارکردگی کی زبان کی سمجھ
- **Qwen3-235B-A22B**: زیادہ سے زیادہ صلاحیت کے لیے سخت ایپلیکیشنز

## ڈیپلائمنٹ پلیٹ فارمز اور رسائی
### کلاؤڈ پلیٹ فارمز
- **Hugging Face Hub**: کمیونٹی سپورٹ کے ساتھ جامع ماڈل ریپوزٹری
- **ModelScope**: علی بابا کا ماڈل پلیٹ فارم، آپٹیمائزیشن ٹولز کے ساتھ
- **مختلف کلاؤڈ فراہم کنندگان**: معیاری ML پلیٹ فارمز کے ذریعے سپورٹ

### مقامی ترقیاتی فریم ورک
- **Transformers**: آسان ڈیپلائمنٹ کے لیے معیاری Hugging Face انٹیگریشن
- **vLLM**: پروڈکشن ماحول کے لیے اعلیٰ کارکردگی کی سروسنگ
- **Ollama**: مقامی ڈیپلائمنٹ اور مینجمنٹ کو آسان بناتا ہے
- **ONNX Runtime**: مختلف ہارڈویئر کے لیے کراس پلیٹ فارم آپٹیمائزیشن
- **llama.cpp**: مختلف پلیٹ فارمز کے لیے موثر C++ امپلیمنٹیشن

### سیکھنے کے وسائل
- **Qwen دستاویزات**: آفیشل دستاویزات اور ماڈل کارڈز
- **Hugging Face Model Hub**: انٹرایکٹو ڈیموز اور کمیونٹی مثالیں
- **ریسرچ پیپرز**: arxiv پر تکنیکی پیپرز، گہرائی سے سمجھنے کے لیے
- **کمیونٹی فورمز**: فعال کمیونٹی سپورٹ اور مباحثے

### Qwen ماڈلز کے ساتھ شروعات

#### ترقیاتی پلیٹ فارمز
1. **Hugging Face Transformers**: معیاری Python انٹیگریشن کے ساتھ شروع کریں
2. **ModelScope**: علی بابا کے آپٹیمائزڈ ڈیپلائمنٹ ٹولز کو دریافت کریں
3. **مقامی ڈیپلائمنٹ**: Ollama یا ڈائریکٹ Transformers استعمال کریں مقامی ٹیسٹنگ کے لیے

#### سیکھنے کا راستہ
1. **بنیادی تصورات کو سمجھیں**: Qwen فیملی کی آرکیٹیکچر اور صلاحیتوں کا مطالعہ کریں
2. **ویریئنٹس کے ساتھ تجربہ کریں**: مختلف ماڈل سائز آزمائیں تاکہ کارکردگی کے فرق کو سمجھ سکیں
3. **عملی نفاذ کریں**: ترقیاتی ماحول میں ماڈلز کو ڈیپلائے کریں
4. **ڈیپلائمنٹ کو بہتر بنائیں**: پروڈکشن کے استعمال کے کیسز کے لیے فائن ٹیون کریں

#### بہترین طریقے
- **چھوٹے سے شروع کریں**: ابتدائی ترقی کے لیے چھوٹے ماڈلز (1.5B-7B) سے آغاز کریں
- **چیٹ ٹیمپلیٹس استعمال کریں**: بہترین نتائج کے لیے مناسب فارمیٹنگ اپلائی کریں
- **وسائل کی نگرانی کریں**: میموری کے استعمال اور انفرنس کی رفتار کو ٹریک کریں
- **خصوصی کاری پر غور کریں**: جب مناسب ہو تو ڈومین مخصوص ویریئنٹس کا انتخاب کریں

## اعلیٰ استعمال کے پیٹرنز

### فائن ٹیوننگ کی مثالیں

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### خصوصی پرامپٹ انجینئرنگ

**پیچیدہ استدلالی کاموں کے لیے:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**سیاق کے ساتھ کوڈ جنریشن کے لیے:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### کثیر لسانی ایپلیکیشنز

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 پروڈکشن ڈیپلائمنٹ پیٹرنز

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## کارکردگی کی آپٹیمائزیشن کی حکمت عملی

### میموری آپٹیمائزیشن

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### انفرنس آپٹیمائزیشن

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## بہترین طریقے اور رہنما اصول

### سیکیورٹی اور پرائیویسی

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### نگرانی اور تشخیص

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## نتیجہ

Qwen ماڈل فیملی AI ٹیکنالوجی کو جمہوری بنانے کے لیے ایک جامع نقطہ نظر کی نمائندگی کرتا ہے، جبکہ مختلف ایپلیکیشنز میں مسابقتی کارکردگی کو برقرار رکھتا ہے۔ اپنی اوپن سورس رسائی، کثیر لسانی صلاحیتوں، اور لچکدار ڈیپلائمنٹ آپشنز کے ذریعے، Qwen تنظیموں اور ڈیولپرز کو طاقتور AI صلاحیتوں سے فائدہ اٹھانے کے قابل بناتا ہے، چاہے ان کے وسائل یا مخصوص ضروریات کچھ بھی ہوں۔

### کلیدی نکات

**اوپن سورس کی برتری**: Qwen نے ثابت کیا کہ اوپن سورس ماڈلز ملکیتی متبادل کے ساتھ مسابقتی کارکردگی حاصل کر سکتے ہیں، جبکہ شفافیت، حسب ضرورت، اور کنٹرول فراہم کرتے ہیں۔

**قابل توسیع آرکیٹیکچر**: 0.5B سے 235B پیرامیٹرز تک کی رینج کمپیوٹیشنل ماحول کے پورے اسپیکٹرم میں ڈیپلائمنٹ کو ممکن بناتی ہے، موبائل ڈیوائسز سے لے کر انٹرپرائز کلسٹرز تک۔

**خصوصی صلاحیتیں**: ڈومین مخصوص ویریئنٹس جیسے Qwen-Coder، Qwen-Math، اور Qwen-VL عمومی زبان کی سمجھ کو برقرار رکھتے ہوئے خصوصی مہارت فراہم کرتے ہیں۔

**عالمی رسائی**: 119+ زبانوں میں مضبوط کثیر لسانی سپورٹ Qwen کو بین الاقوامی ایپلیکیشنز اور متنوع صارفین کے لیے موزوں بناتی ہے۔

**مسلسل جدت**: Qwen 1.0 سے Qwen3 تک کا ارتقاء صلاحیتوں، کارکردگی، اور ڈیپلائمنٹ آپشنز میں مسلسل بہتری کو ظاہر کرتا ہے۔

### مستقبل کا نقطہ نظر

جیسا کہ Qwen فیملی ترقی کرتی رہتی ہے، ہم توقع کر سکتے ہیں:

- **بہتر کارکردگی**: پیرامیٹرز کے تناسب کے لیے بہتر کارکردگی کے لیے مسلسل آپٹیمائزیشن
- **ملٹی موڈل صلاحیتوں میں توسیع**: زیادہ نفیس وژن، آڈیو، اور ٹیکسٹ پروسیسنگ کا انضمام
- **بہتر استدلال**: اعلیٰ سوچنے کے میکانزم اور کثیر قدمی مسئلہ حل کرنے کی صلاحیتیں
- **بہتر ڈیپلائمنٹ ٹولز**: مختلف ڈیپلائمنٹ منظرناموں کے لیے بہتر فریم ورک اور آپٹیمائزیشن ٹولز
- **کمیونٹی کی ترقی**: ٹولز، ایپلیکیشنز، اور کمیونٹی تعاون کے ایک وسیع ماحولیاتی نظام کا پھیلاؤ

### اگلے اقدامات

چاہے آپ چیٹ بوٹ بنا رہے ہوں، تعلیمی ٹولز تیار کر رہے ہوں، کوڈنگ اسسٹنس تخلیق کر رہے ہوں، یا کثیر لسانی ایپلیکیشنز پر کام کر رہے ہوں، Qwen فیملی مضبوط کمیونٹی سپورٹ اور جامع دستاویزات کے ساتھ قابل توسیع حل فراہم کرتی ہے۔

تازہ ترین اپ ڈیٹس، ماڈل ریلیزز، اور تفصیلی تکنیکی دستاویزات کے لیے، Hugging Face پر آفیشل Qwen ریپوزٹریز ملاحظہ کریں اور فعال کمیونٹی مباحثے اور مثالوں کو دریافت کریں۔

AI ڈیولپمنٹ کا مستقبل قابل رسائی، شفاف، اور طاقتور ٹولز میں ہے جو تمام شعبوں اور پیمانوں میں جدت کو ممکن بناتے ہیں۔ Qwen فیملی اس وژن کی مثال پیش کرتی ہے، تنظیموں اور ڈیولپرز کو اگلی نسل کی AI سے چلنے والی ایپلیکیشنز بنانے کے لیے بنیاد فراہم کرتی ہے۔

## اضافی وسائل

- **آفیشل دستاویزات**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **ماڈل ہب**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **تکنیکی پیپرز**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **کمیونٹی**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope پلیٹ فارم**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## سیکھنے کے نتائج

اس ماڈیول کو مکمل کرنے کے بعد، آپ قابل ہوں گے:

1. Qwen ماڈل فیملی کے آرکیٹیکچرل فوائد اور اس کے اوپن سورس نقطہ نظر کی وضاحت کریں
2. مخصوص ایپلیکیشن کی ضروریات اور وسائل کی حدود کی بنیاد پر مناسب Qwen ویریئنٹ کا انتخاب کریں
3. مختلف ڈیپلائمنٹ منظرناموں میں Qwen ماڈلز کو نافذ کریں، آپٹیمائزڈ کنفیگریشنز کے ساتھ
4. Qwen ماڈل کی کارکردگی کو بہتر بنانے کے لیے کوانٹائزیشن اور آپٹیمائزیشن تکنیکوں کا اطلاق کریں
5. Qwen فیملی کے ماڈل سائز، کارکردگی، اور صلاحیتوں کے درمیان تجارتی تعلقات کا جائزہ لیں

## آگے کیا ہے

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔