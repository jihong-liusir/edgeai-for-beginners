<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T17:05:41+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ur"
}
-->
# سیکشن 2: قیوین فیملی کی بنیادی باتیں

قیوین ماڈل فیملی علی بابا کلاؤڈ کا جامع طریقہ ہے جو بڑے زبان ماڈلز اور ملٹی موڈل اے آئی کے لیے ہے، یہ ظاہر کرتا ہے کہ اوپن سورس ماڈلز قابل ذکر کارکردگی حاصل کر سکتے ہیں جبکہ مختلف تعیناتی منظرناموں میں قابل رسائی رہتے ہیں۔ یہ سمجھنا ضروری ہے کہ قیوین فیملی طاقتور اے آئی صلاحیتوں کو لچکدار تعیناتی کے اختیارات کے ساتھ کیسے فعال کرتی ہے جبکہ مختلف کاموں میں مسابقتی کارکردگی کو برقرار رکھتی ہے۔

## ڈویلپرز کے لیے وسائل

### ہگنگ فیس ماڈل ریپوزٹری
قیوین فیملی کے منتخب ماڈلز [ہگنگ فیس](https://huggingface.co/models?search=qwen) کے ذریعے دستیاب ہیں، جو ان ماڈلز کے کچھ ورژنز تک رسائی فراہم کرتے ہیں۔ آپ دستیاب ورژنز کو دریافت کر سکتے ہیں، انہیں اپنے مخصوص استعمال کے لیے بہتر بنا سکتے ہیں، اور مختلف فریم ورک کے ذریعے تعینات کر سکتے ہیں۔

### مقامی ترقیاتی ٹولز
مقامی ترقی اور ٹیسٹنگ کے لیے، آپ [مائیکروسافٹ فاؤنڈری لوکل](https://github.com/microsoft/foundry-local) استعمال کر سکتے ہیں تاکہ دستیاب قیوین ماڈلز کو اپنی ترقیاتی مشین پر بہتر کارکردگی کے ساتھ چلایا جا سکے۔

### دستاویزات کے وسائل
- [قیوین ماڈل دستاویزات](https://huggingface.co/docs/transformers/model_doc/qwen)
- [ایج تعیناتی کے لیے قیوین ماڈلز کو بہتر بنانا](https://github.com/microsoft/olive)

## تعارف

اس ٹیوٹوریل میں، ہم علی بابا کے قیوین ماڈل فیملی اور اس کے بنیادی تصورات کو دریافت کریں گے۔ ہم قیوین فیملی کے ارتقاء، جدید تربیتی طریقوں، جو قیوین ماڈلز کو مؤثر بناتے ہیں، فیملی میں کلیدی ورژنز، اور مختلف منظرناموں میں عملی اطلاقات کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس ٹیوٹوریل کے اختتام تک، آپ قابل ہوں گے:

- علی بابا کے قیوین ماڈل فیملی کے ڈیزائن فلسفہ اور ارتقاء کو سمجھنا
- کلیدی جدتوں کی شناخت کرنا جو قیوین ماڈلز کو مختلف پیرامیٹر سائز میں اعلیٰ کارکردگی حاصل کرنے کے قابل بناتی ہیں
- مختلف قیوین ماڈل ورژنز کے فوائد اور حدود کو پہچاننا
- حقیقی دنیا کے منظرناموں کے لیے مناسب ورژنز منتخب کرنے کے لیے قیوین ماڈلز کے علم کو لاگو کرنا

## جدید اے آئی ماڈل منظرنامے کو سمجھنا

اے آئی منظرنامہ نمایاں طور پر ترقی کر چکا ہے، مختلف تنظیمیں زبان ماڈل کی ترقی کے لیے مختلف طریقے اختیار کر رہی ہیں۔ کچھ ملکیتی بند سورس ماڈلز پر توجہ مرکوز کرتے ہیں، جبکہ دیگر اوپن سورس رسائی اور شفافیت پر زور دیتے ہیں۔ روایتی طریقہ یا تو بڑے ملکیتی ماڈلز پر مشتمل ہوتا ہے جو صرف APIs کے ذریعے قابل رسائی ہوتے ہیں یا اوپن سورس ماڈلز جو صلاحیتوں میں پیچھے رہ سکتے ہیں۔

یہ نقطہ نظر ان تنظیموں کے لیے چیلنجز پیدا کرتا ہے جو طاقتور اے آئی صلاحیتوں کی تلاش میں ہیں جبکہ اپنے ڈیٹا، اخراجات، اور تعیناتی کی لچک پر کنٹرول برقرار رکھتے ہیں۔ روایتی طریقہ اکثر جدید کارکردگی اور عملی تعیناتی کے تحفظات کے درمیان انتخاب کی ضرورت ہوتی ہے۔

## قابل رسائی اے آئی کی عمدگی کا چیلنج

اعلیٰ معیار، قابل رسائی اے آئی کی ضرورت مختلف منظرناموں میں بڑھتی جا رہی ہے۔ ان اطلاقات پر غور کریں جن کے لیے مختلف تنظیمی ضروریات کے لیے لچکدار تعیناتی کے اختیارات، لاگت مؤثر نفاذ جہاں API کے اخراجات اہم ہو سکتے ہیں، عالمی اطلاقات کے لیے کثیر لسانی صلاحیتیں، یا کوڈنگ اور ریاضی جیسے شعبوں میں خصوصی مہارت کی ضرورت ہوتی ہے۔

### کلیدی تعیناتی ضروریات

جدید اے آئی تعیناتیوں کو کئی بنیادی ضروریات کا سامنا کرنا پڑتا ہے جو عملی اطلاق کو محدود کرتی ہیں:

- **رسائی**: شفافیت اور حسب ضرورت کے لیے اوپن سورس دستیابی
- **لاگت مؤثر**: مختلف بجٹ کے لیے معقول کمپیوٹیشنل ضروریات
- **لچک**: مختلف تعیناتی منظرناموں کے لیے متعدد ماڈل سائز
- **عالمی رسائی**: مضبوط کثیر لسانی اور بین الثقافتی صلاحیتیں
- **خصوصیت**: مخصوص استعمال کے معاملات کے لیے ڈومین مخصوص ورژنز

## قیوین ماڈل فلسفہ

قیوین ماڈل فیملی اے آئی ماڈل کی ترقی کے لیے ایک جامع نقطہ نظر کی نمائندگی کرتی ہے، جو اوپن سورس رسائی، کثیر لسانی صلاحیتوں، اور عملی تعیناتی کو ترجیح دیتی ہے جبکہ مسابقتی کارکردگی کی خصوصیات کو برقرار رکھتی ہے۔ قیوین ماڈلز مختلف ماڈل سائز، اعلیٰ معیار کی تربیتی طریقوں، اور مختلف ڈومینز کے لیے خصوصی ورژنز کے ذریعے یہ حاصل کرتے ہیں۔

قیوین فیملی مختلف نقطہ نظر کو شامل کرتی ہے جو کارکردگی-افادیت کے اسپیکٹرم میں اختیارات فراہم کرتی ہے، موبائل ڈیوائسز سے لے کر انٹرپرائز سرورز تک تعیناتی کو فعال کرتی ہے جبکہ بامعنی اے آئی صلاحیتیں فراہم کرتی ہے۔ مقصد اعلیٰ معیار کے اے آئی تک رسائی کو جمہوری بنانا ہے جبکہ تعیناتی کے انتخاب میں لچک فراہم کرنا ہے۔

### قیوین کے بنیادی ڈیزائن اصول

قیوین ماڈلز کئی بنیادی اصولوں پر مبنی ہیں جو انہیں دیگر زبان ماڈل فیملیز سے ممتاز کرتے ہیں:

- **اوپن سورس پہلے**: تحقیق اور تجارتی استعمال کے لیے مکمل شفافیت اور رسائی
- **جامع تربیت**: متعدد زبانوں اور ڈومینز کا احاطہ کرنے والے بڑے، متنوع ڈیٹا سیٹس پر تربیت
- **قابل توسیع آرکیٹیکچر**: مختلف کمپیوٹیشنل ضروریات سے مطابقت رکھنے کے لیے متعدد ماڈل سائز
- **خصوصی عمدگی**: مخصوص کاموں کے لیے بہتر ڈومین مخصوص ورژنز

## قیوین فیملی کو فعال کرنے والی کلیدی ٹیکنالوجیز

### بڑے پیمانے پر تربیت

قیوین فیملی کی ایک نمایاں خصوصیت تربیتی ڈیٹا اور کمپیوٹیشنل وسائل کا بڑا پیمانہ ہے جو ماڈل کی ترقی میں لگایا گیا ہے۔ قیوین ماڈلز احتیاط سے تیار کردہ، کثیر لسانی ڈیٹا سیٹس کا فائدہ اٹھاتے ہیں جو کھربوں ٹوکنز پر مشتمل ہیں، جو جامع عالمی علم اور استدلال کی صلاحیتیں فراہم کرنے کے لیے ڈیزائن کیے گئے ہیں۔

یہ طریقہ اعلیٰ معیار کے ویب مواد، تعلیمی ادب، کوڈ ریپوزٹریز، اور کثیر لسانی وسائل کو یکجا کر کے کام کرتا ہے۔ تربیتی طریقہ کار مختلف ڈومینز اور زبانوں میں علم کی وسعت اور تفہیم کی گہرائی دونوں پر زور دیتا ہے۔

### جدید استدلال اور سوچ

حالیہ قیوین ماڈلز پیچیدہ کثیر مرحلہ مسئلہ حل کرنے کو فعال کرنے والی نفیس استدلال کی صلاحیتوں کو شامل کرتے ہیں:

**سوچنے کا موڈ (قیوین3)**: ماڈلز تفصیلی مرحلہ وار استدلال میں مشغول ہو سکتے ہیں اس سے پہلے کہ حتمی جوابات فراہم کریں، انسانی مسئلہ حل کرنے کے طریقوں کی طرح۔

**دوہری موڈ آپریشن**: سادہ سوالات کے لیے فوری جواب موڈ اور پیچیدہ مسائل کے لیے گہری سوچ کے موڈ کے درمیان سوئچ کرنے کی صلاحیت۔

**چین آف تھوٹ انضمام**: استدلال کے مراحل کا قدرتی طور پر انضمام جو پیچیدہ کاموں میں شفافیت اور درستگی کو بہتر بناتا ہے۔

### آرکیٹیکچرل جدتیں

قیوین فیملی کئی آرکیٹیکچرل اصلاحات کو شامل کرتی ہے جو کارکردگی اور افادیت دونوں کے لیے ڈیزائن کی گئی ہیں:

**قابل توسیع ڈیزائن**: ماڈل سائزز کے درمیان مستقل آرکیٹیکچر جو آسان توسیع اور موازنہ کو فعال کرتا ہے۔

**ملٹی موڈل انضمام**: متن، وژن، اور آڈیو پروسیسنگ صلاحیتوں کا ایک متحد آرکیٹیکچر میں ہموار انضمام۔

**تعیناتی کی اصلاح**: مختلف ہارڈویئر کنفیگریشنز کے لیے متعدد کوانٹائزیشن اختیارات اور تعیناتی فارمیٹس۔

## ماڈل سائز اور تعیناتی کے اختیارات

جدید تعیناتی ماحول قیوین ماڈلز کی لچک سے فائدہ اٹھاتے ہیں جو مختلف کمپیوٹیشنل ضروریات کے مطابق ہیں:

### چھوٹے ماڈلز (0.5B-3B)

قیوین چھوٹے ماڈلز فراہم کرتا ہے جو ایج تعیناتی، موبائل ایپلیکیشنز، اور وسائل کی محدود ماحول کے لیے موزوں ہیں جبکہ متاثر کن صلاحیتوں کو برقرار رکھتے ہیں۔

### درمیانے ماڈلز (7B-32B)

درمیانے درجے کے ماڈلز پیشہ ورانہ ایپلیکیشنز کے لیے بہتر صلاحیتیں فراہم کرتے ہیں، کارکردگی اور کمپیوٹیشنل ضروریات کے درمیان بہترین توازن فراہم کرتے ہیں۔

### بڑے ماڈلز (72B+)

فل اسکیل ماڈلز جدید کارکردگی فراہم کرتے ہیں جو زیادہ مطالبہ کرنے والے ایپلیکیشنز، تحقیق، اور انٹرپرائز تعیناتیوں کے لیے زیادہ سے زیادہ صلاحیت کی ضرورت ہوتی ہے۔

## قیوین ماڈل فیملی کے فوائد

### اوپن سورس رسائی

قیوین ماڈلز مکمل شفافیت اور حسب ضرورت صلاحیتیں فراہم کرتے ہیں، تنظیموں کو ان کے مخصوص ضروریات کے مطابق ماڈلز کو سمجھنے، ترمیم کرنے، اور اپنانے کے قابل بناتے ہیں بغیر وینڈر لاک ان کے۔

### تعیناتی کی لچک

ماڈل سائزز کی رینج مختلف ہارڈویئر کنفیگریشنز پر تعیناتی کو فعال کرتی ہے، موبائل ڈیوائسز سے لے کر ہائی اینڈ سرورز تک، تنظیموں کو ان کے اے آئی انفراسٹرکچر کے انتخاب میں لچک فراہم کرتی ہے۔

### کثیر لسانی عمدگی

قیوین ماڈلز کثیر لسانی تفہیم اور جنریشن میں بہترین ہیں، درجنوں زبانوں کو سپورٹ کرتے ہیں، خاص طور پر انگریزی اور چینی میں مضبوط، جو انہیں عالمی ایپلیکیشنز کے لیے موزوں بناتے ہیں۔

### مسابقتی کارکردگی

قیوین ماڈلز مسلسل بینچ مارکس پر مسابقتی نتائج حاصل کرتے ہیں جبکہ اوپن سورس رسائی فراہم کرتے ہیں، یہ ظاہر کرتے ہیں کہ اوپن ماڈلز ملکیتی متبادلات سے میل کھا سکتے ہیں۔

### خصوصی صلاحیتیں

قیوین-کوڈر اور قیوین-میٹھ جیسے ڈومین مخصوص ورژنز عمومی زبان کی تفہیم کی صلاحیتوں کو برقرار رکھتے ہوئے خصوصی مہارت فراہم کرتے ہیں۔

## عملی مثالیں اور استعمال کے معاملات

تکنیکی تفصیلات میں جانے سے پہلے، آئیے کچھ ٹھوس مثالوں کو دریافت کریں کہ قیوین ماڈلز کیا حاصل کر سکتے ہیں:

### ریاضیاتی استدلال کی مثال

قیوین-میٹھ مرحلہ وار ریاضیاتی مسئلہ حل کرنے میں بہترین ہے۔ مثال کے طور پر، جب پیچیدہ کیلکولس مسئلہ حل کرنے کے لیے کہا گیا:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### کثیر لسانی سپورٹ کی مثال

قیوین ماڈلز مختلف زبانوں میں مضبوط کثیر لسانی صلاحیتوں کا مظاہرہ کرتے ہیں:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### ملٹی موڈل صلاحیتوں کی مثال

قیوین-VL متن اور تصاویر دونوں کو بیک وقت پروسیس کر سکتا ہے:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### کوڈ جنریشن کی مثال

قیوین-کوڈر متعدد پروگرامنگ زبانوں میں کوڈ جنریٹ کرنے اور وضاحت کرنے میں بہترین ہے:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### ایج تعیناتی کی مثال

قیوین ماڈلز مختلف ایج ڈیوائسز پر بہتر کنفیگریشنز کے ساتھ تعینات کیے جا سکتے ہیں:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## قیوین فیملی کا ارتقاء

### قیوین 1.0 اور 1.5: بنیادی ماڈلز

ابتدائی قیوین ماڈلز نے جامع تربیت اور اوپن سورس رسائی کے بنیادی اصولوں کو قائم کیا:

- **قیوین-7B (7B پیرامیٹرز)**: ابتدائی ریلیز جو چینی اور انگریزی زبان کی تفہیم پر مرکوز ہے
- **قیوین-14B (14B پیرامیٹرز)**: بہتر استدلال اور علم کے ساتھ صلاحیتوں میں اضافہ
- **قیوین-72B (72B پیرامیٹرز)**: بڑے پیمانے پر ماڈل جو جدید کارکردگی فراہم کرتا ہے
- **قیوین1.5 سیریز**: بہتر طویل سیاق و سباق ہینڈلنگ کے ساتھ متعدد سائز (0.5B سے 110B) تک توسیع

### قیوین2 فیملی: ملٹی موڈل توسیع

قیوین2 سیریز نے زبان اور ملٹی موڈل صلاحیتوں میں نمایاں پیش رفت کی:

- **قیوین2-0.5B سے 72B**: مختلف تعیناتی ضروریات کے لیے جامع زبان ماڈلز کی رینج
- **قیوین2-57B-A14B (MoE)**: مؤثر پیرامیٹر استعمال کے لیے ماہرین کے مرکب کی آرکیٹیکچر
- **قیوین2-VL**: امیج تفہیم کے لیے جدید وژن-زبان کی صلاحیتیں
- **قیوین2-آڈیو**: آڈیو پروسیسنگ اور تفہیم کی صلاحیتیں
- **قیوین2-میٹھ**: خصوصی ریاضیاتی استدلال اور مسئلہ حل کرنے کی صلاحیتیں

### قیوین2.5 فیملی: بہتر کارکردگی

قیوین2.5 سیریز نے تمام جہتوں میں نمایاں بہتری لائی:

- **توسیع شدہ تربیت**: 18 کھرب ٹوکنز کے تربیتی ڈیٹا کے ساتھ بہتر صلاحیتیں
- **وسیع سیاق و سباق**: 128K ٹوکنز سیاق کی لمبائی، ٹربو ورژن 1M ٹوکنز کو سپورٹ کرتا ہے
- **بہتر خصوصیت**: بہتر قیوین2.5-کوڈر اور قیوین2.5-میٹھ ورژنز
- **بہتر کثیر لسانی سپورٹ**: 27+ زبانوں میں بہتر کارکردگی

### قیوین3 فیملی: جدید استدلال

تازہ ترین نسل استدلال اور سوچنے کی صلاحیتوں کی حدود کو آگے بڑھاتی ہے:

- **قیوین3-235B-A22B**: 235B کل پیرامیٹرز کے ساتھ فلیگ شپ ماہرین کے مرکب ماڈل
- **قیوین3-30B-A3B**: مؤثر MoE ماڈل جو فعال پیرامیٹرز کے مطابق مضبوط کارکردگی فراہم کرتا ہے
- **ڈینس ماڈلز**: مختلف تعیناتی منظرناموں کے لیے قیوین3-32B، 14B، 8B، 4B، 1.7B، 0.6B
- **سوچنے کا موڈ**: فوری جوابات اور گہری سوچ دونوں کو سپورٹ کرنے والا ہائبرڈ استدلال کا طریقہ
- **کثیر لسانی عمدگی**: 119 زبانوں اور بولیوں کے لیے سپورٹ
- **بہتر تربیت**: 36 کھرب ٹوکنز کے متنوع، اعلیٰ معیار کے تربیتی ڈیٹا

## قیوین ماڈلز کے اطلاقات

### انٹرپرائز اطلاقات

تنظیمیں قیوین ماڈلز کو دستاویز تجزیہ، کسٹمر سروس آٹومیشن، کوڈ جنریشن اسسٹنس، اور بزنس ان
یہاں یہ بتایا گیا ہے کہ Hugging Face Transformers لائبریری کے ذریعے Qwen ماڈلز کے ساتھ کیسے شروعات کریں:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 ماڈلز کا استعمال

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### خصوصی ماڈلز کا استعمال

**Qwen-Coder کے ذریعے کوڈ جنریشن:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**ریاضی کے مسائل حل کرنا:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**ویژن-لینگویج ٹاسکس:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### تھنکنگ موڈ (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 موبائل اور ایج ڈپلائمنٹ

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API ڈپلائمنٹ کی مثال

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## کارکردگی کے بینچ مارکس اور کامیابیاں

Qwen ماڈل فیملی نے مختلف بینچ مارکس پر شاندار کارکردگی حاصل کی ہے، جبکہ اوپن سورس رسائی کو برقرار رکھا ہے:

### اہم کارکردگی کی جھلکیاں

**استدلال میں مہارت:**
- Qwen3-235B-A22B کوڈنگ، ریاضی، اور عمومی صلاحیتوں کے بینچ مارک میں دیگر اعلیٰ ماڈلز جیسے DeepSeek-R1، o1، o3-mini، Grok-3، اور Gemini-2.5-Pro کے مقابلے میں مسابقتی نتائج حاصل کرتا ہے۔
- Qwen3-30B-A3B نے QwQ-32B کو 10 گنا زیادہ فعال پیرامیٹرز کے ساتھ پیچھے چھوڑ دیا۔
- Qwen3-4B کی کارکردگی Qwen2.5-72B-Instruct کے برابر ہو سکتی ہے۔

**موثریت کی کامیابیاں:**
- Qwen3-MoE بیس ماڈلز نے Qwen2.5 کے ڈینس بیس ماڈلز کے برابر کارکردگی حاصل کی، جبکہ صرف 10% فعال پیرامیٹرز استعمال کیے۔
- ڈینس ماڈلز کے مقابلے میں تربیت اور انفرنس میں نمایاں لاگت کی بچت۔

**کثیر لسانی صلاحیتیں:**
- Qwen3 ماڈلز 119 زبانوں اور بولیوں کی حمایت کرتے ہیں۔
- مختلف لسانی اور ثقافتی سیاق و سباق میں مضبوط کارکردگی۔

**تربیتی پیمانہ:**
- Qwen3 تقریباً 36 ٹریلین ٹوکنز کے ساتھ 119 زبانوں اور بولیوں کا احاطہ کرتا ہے، جو Qwen2.5 کے 18 ٹریلین ٹوکنز سے تقریباً دوگنا ہے۔

### ماڈل موازنہ میٹرکس

| ماڈل سیریز | پیرامیٹرز کی حد | کانٹیکسٹ کی لمبائی | اہم طاقتیں | بہترین استعمال کے کیسز |
|--------------|------------------|----------------|---------------|----------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | متوازن کارکردگی، کثیر لسانی | عمومی ایپلیکیشنز، پروڈکشن ڈپلائمنٹ |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | کوڈ جنریشن، پروگرامنگ | سافٹ ویئر ڈیولپمنٹ، کوڈنگ اسسٹنس |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | ریاضیاتی استدلال | تعلیمی پلیٹ فارمز، STEM ایپلیکیشنز |
| **Qwen2.5-VL** | مختلف | متغیر | ویژن-لینگویج سمجھداری | ملٹی موڈل ایپلیکیشنز، امیج تجزیہ |
| **Qwen3** | 0.6B-235B | متغیر | اعلیٰ استدلال، تھنکنگ موڈ | پیچیدہ استدلال، تحقیقی ایپلیکیشنز |
| **Qwen3 MoE** | 30B-235B کل | متغیر | موثر بڑے پیمانے پر کارکردگی | انٹرپرائز ایپلیکیشنز، اعلیٰ کارکردگی کی ضروریات |

## ماڈل انتخاب گائیڈ

### بنیادی ایپلیکیشنز کے لیے
- **Qwen2.5-0.5B/1.5B**: موبائل ایپس، ایج ڈیوائسز، ریئل ٹائم ایپلیکیشنز
- **Qwen2.5-3B/7B**: عمومی چیٹ بوٹس، مواد کی تخلیق، سوال و جواب کے نظام

### ریاضیاتی اور استدلالی ٹاسکس کے لیے
- **Qwen2.5-Math**: ریاضیاتی مسائل حل کرنا اور STEM تعلیم
- **Qwen3 تھنکنگ موڈ کے ساتھ**: پیچیدہ استدلال جس میں مرحلہ وار تجزیہ کی ضرورت ہو

### پروگرامنگ اور ڈیولپمنٹ کے لیے
- **Qwen2.5-Coder**: کوڈ جنریشن، ڈیبگنگ، پروگرامنگ اسسٹنس
- **Qwen3**: استدلالی صلاحیتوں کے ساتھ اعلیٰ پروگرامنگ ٹاسکس

### ملٹی موڈل ایپلیکیشنز کے لیے
- **Qwen2.5-VL**: امیج سمجھداری، بصری سوال و جواب
- **Qwen-Audio**: آڈیو پروسیسنگ اور تقریر کی سمجھداری

### انٹرپرائز ڈپلائمنٹ کے لیے
- **Qwen2.5-32B/72B**: اعلیٰ کارکردگی کی زبان سمجھداری
- **Qwen3-235B-A22B**: زیادہ سے زیادہ صلاحیت کے لیے سخت ایپلیکیشنز

## ڈپلائمنٹ پلیٹ فارمز اور رسائی

### کلاؤڈ پلیٹ فارمز
- **Hugging Face Hub**: کمیونٹی سپورٹ کے ساتھ جامع ماڈل ریپوزٹری
- **ModelScope**: علی بابا کا ماڈل پلیٹ فارم جس میں آپٹیمائزیشن ٹولز شامل ہیں
- **مختلف کلاؤڈ فراہم کنندگان**: معیاری ML پلیٹ فارمز کے ذریعے سپورٹ

### لوکل ڈیولپمنٹ فریم ورکس
- **Transformers**: آسان ڈپلائمنٹ کے لیے معیاری Hugging Face انٹیگریشن
- **vLLM**: پروڈکشن ماحول کے لیے اعلیٰ کارکردگی کی سروسنگ
- **Ollama**: لوکل ڈپلائمنٹ اور مینجمنٹ کو آسان بناتا ہے
- **ONNX Runtime**: مختلف ہارڈویئر کے لیے کراس پلیٹ فارم آپٹیمائزیشن
- **llama.cpp**: مختلف پلیٹ فارمز کے لیے موثر C++ امپلیمنٹیشن

### سیکھنے کے وسائل
- **Qwen دستاویزات**: آفیشل دستاویزات اور ماڈل کارڈز
- **Hugging Face Model Hub**: انٹرایکٹو ڈیموز اور کمیونٹی کی مثالیں
- **ریسرچ پیپرز**: arxiv پر تکنیکی پیپرز گہرائی سے سمجھنے کے لیے
- **کمیونٹی فورمز**: فعال کمیونٹی سپورٹ اور مباحثے

### Qwen ماڈلز کے ساتھ شروعات

#### ڈیولپمنٹ پلیٹ فارمز
1. **Hugging Face Transformers**: معیاری Python انٹیگریشن کے ساتھ شروعات کریں
2. **ModelScope**: علی بابا کے آپٹیمائزڈ ڈپلائمنٹ ٹولز کو دریافت کریں
3. **لوکل ڈپلائمنٹ**: Ollama یا ڈائریکٹ Transformers کا استعمال کریں لوکل ٹیسٹنگ کے لیے

#### سیکھنے کا راستہ
1. **بنیادی تصورات کو سمجھیں**: Qwen فیملی کی آرکیٹیکچر اور صلاحیتوں کا مطالعہ کریں
2. **ویریئنٹس کے ساتھ تجربہ کریں**: مختلف ماڈل سائز آزمائیں تاکہ کارکردگی کے فرق کو سمجھ سکیں
3. **عملی نفاذ کریں**: ڈیولپمنٹ ماحول میں ماڈلز کو ڈپلائے کریں
4. **ڈپلائمنٹ کو بہتر بنائیں**: پروڈکشن کے استعمال کے کیسز کے لیے فائن ٹیون کریں

#### بہترین طریقے
- **چھوٹے سے شروع کریں**: ابتدائی ڈیولپمنٹ کے لیے چھوٹے ماڈلز (1.5B-7B) سے آغاز کریں
- **چیٹ ٹیمپلیٹس استعمال کریں**: بہترین نتائج کے لیے مناسب فارمیٹنگ اپلائی کریں
- **وسائل کی نگرانی کریں**: میموری کے استعمال اور انفرنس کی رفتار کو ٹریک کریں
- **خصوصیت پر غور کریں**: جب مناسب ہو تو ڈومین مخصوص ویریئنٹس کا انتخاب کریں

## اعلیٰ استعمال کے پیٹرنز

### فائن ٹیوننگ کی مثالیں

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### خصوصی پرامپٹ انجینئرنگ

**پیچیدہ استدلالی ٹاسکس کے لیے:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**کانٹیکسٹ کے ساتھ کوڈ جنریشن کے لیے:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### کثیر لسانی ایپلیکیشنز

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 پروڈکشن ڈپلائمنٹ پیٹرنز

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## کارکردگی کی آپٹیمائزیشن کی حکمت عملی

### میموری آپٹیمائزیشن

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### انفرنس آپٹیمائزیشن

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## بہترین طریقے اور رہنما اصول

### سیکیورٹی اور پرائیویسی

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### نگرانی اور تشخیص

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## نتیجہ

Qwen ماڈل فیملی ایک جامع طریقہ پیش کرتی ہے AI ٹیکنالوجی کو جمہوری بنانے کے لیے، جبکہ مختلف ایپلیکیشنز میں مسابقتی کارکردگی کو برقرار رکھتی ہے۔ اوپن سورس رسائی، کثیر لسانی صلاحیتوں، اور لچکدار ڈپلائمنٹ آپشنز کے ذریعے، Qwen تنظیموں اور ڈیولپرز کو طاقتور AI صلاحیتوں سے فائدہ اٹھانے کی اجازت دیتا ہے، چاہے ان کے وسائل یا مخصوص ضروریات کچھ بھی ہوں۔

### اہم نکات

**اوپن سورس مہارت**: Qwen یہ ظاہر کرتا ہے کہ اوپن سورس ماڈلز ملکیتی متبادلات کے ساتھ مسابقتی کارکردگی حاصل کر سکتے ہیں، جبکہ شفافیت، حسب ضرورت، اور کنٹرول فراہم کرتے ہیں۔

**قابل توسیع آرکیٹیکچر**: 0.5B سے 235B پیرامیٹرز کی حد مختلف کمپیوٹیشنل ماحول میں ڈپلائمنٹ کو ممکن بناتی ہے، موبائل ڈیوائسز سے لے کر انٹرپرائز کلسٹرز تک۔

**خصوصی صلاحیتیں**: ڈومین مخصوص ویریئنٹس جیسے Qwen-Coder، Qwen-Math، اور Qwen-VL عمومی زبان کی سمجھداری کو برقرار رکھتے ہوئے خصوصی مہارت فراہم کرتے ہیں۔

**عالمی رسائی**: 119+ زبانوں میں مضبوط کثیر لسانی سپورٹ Qwen کو بین الاقوامی ایپلیکیشنز اور متنوع صارفین کے لیے موزوں بناتی ہے۔

**مسلسل جدت**: Qwen 1.0 سے Qwen3 تک ارتقاء صلاحیتوں، موثریت، اور ڈپلائمنٹ آپشنز میں مسلسل بہتری کو ظاہر کرتا ہے۔

### مستقبل کا منظر

جیسے جیسے Qwen فیملی ترقی کرتی ہے، ہم توقع کر سکتے ہیں:

- **موثریت میں اضافہ**: بہتر کارکردگی-پیرامیٹر تناسب کے لیے مسلسل آپٹیمائزیشن
- **ملٹی موڈل صلاحیتوں میں توسیع**: زیادہ نفیس ویژن، آڈیو، اور ٹیکسٹ پروسیسنگ کا انضمام
- **استدلال میں بہتری**: اعلیٰ تھنکنگ میکانزم اور ملٹی-اسٹیپ مسئلہ حل کرنے کی صلاحیتیں
- **بہتر ڈپلائمنٹ ٹولز**: مختلف ڈپلائمنٹ منظرناموں کے لیے بہتر فریم ورکس اور آپٹیمائزیشن ٹولز
- **کمیونٹی کی ترقی**: ٹولز، ایپلیکیشنز، اور کمیونٹی کی شراکتوں کے وسیع تر ماحولیاتی نظام

### اگلے اقدامات

چاہے آپ چیٹ بوٹ بنا رہے ہوں، تعلیمی ٹولز تیار کر رہے ہوں، کوڈنگ اسسٹنس تخلیق کر رہے ہوں، یا کثیر لسانی ایپلیکیشنز پر کام کر رہے ہوں، Qwen فیملی مضبوط کمیونٹی سپورٹ اور جامع دستاویزات کے ساتھ قابل توسیع حل فراہم کرتی ہے۔

تازہ ترین اپ ڈیٹس، ماڈل ریلیزز، اور تفصیلی تکنیکی دستاویزات کے لیے، Hugging Face پر آفیشل Qwen ریپوزٹریز ملاحظہ کریں اور فعال کمیونٹی مباحثے اور مثالوں کو دریافت کریں۔

AI ڈیولپمنٹ کا مستقبل قابل رسائی، شفاف، اور طاقتور ٹولز میں ہے جو تمام شعبوں اور پیمانوں پر جدت کو ممکن بناتے ہیں۔ Qwen فیملی اس وژن کی مثال پیش کرتی ہے، تنظیموں اور ڈیولپرز کو اگلی نسل کی AI سے چلنے والی ایپلیکیشنز بنانے کے لیے بنیاد فراہم کرتی ہے۔

## اضافی وسائل

- **آفیشل دستاویزات**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **ماڈل ہب**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **تکنیکی پیپرز**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **کمیونٹی**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope پلیٹ فارم**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## سیکھنے کے نتائج

اس ماڈیول کو مکمل کرنے کے بعد، آپ قابل ہوں گے:

1. Qwen ماڈل فیملی کے آرکیٹیکچرل فوائد اور اس کے اوپن سورس طریقہ کار کی وضاحت کریں
2. مخصوص ایپلیکیشن کی ضروریات اور وسائل کی حدود کی بنیاد پر مناسب Qwen ویریئنٹ کا انتخاب کریں
3. مختلف ڈپلائمنٹ منظرناموں میں Qwen ماڈلز کو نافذ کریں اور آپٹیمائزڈ کنفیگریشنز کے ساتھ
4. Qwen ماڈل کی کارکردگی کو بہتر بنانے کے لیے کوانٹائزیشن اور آپٹیمائزیشن تکنیکوں کا اطلاق کریں
5. Qwen فیملی میں ماڈل سائز، کارکردگی، اور صلاحیتوں کے درمیان تجارتی تعلقات کا جائزہ لیں

## آگے کیا ہے

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔