<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-17T17:20:11+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "ur"
}
-->
# سیکشن 3: جیمما فیملی کے بنیادی اصول

جیمما ماڈل فیملی گوگل کے اوپن سورس بڑے زبان ماڈلز اور ملٹی موڈل AI کے جامع نقطہ نظر کی نمائندگی کرتی ہے، یہ ظاہر کرتی ہے کہ قابل رسائی ماڈلز شاندار کارکردگی حاصل کر سکتے ہیں اور مختلف منظرناموں میں موبائل ڈیوائسز سے لے کر انٹرپرائز ورک سٹیشنز تک تعینات کیے جا سکتے ہیں۔ یہ سمجھنا ضروری ہے کہ جیمما فیملی کس طرح طاقتور AI صلاحیتوں کو لچکدار تعیناتی کے اختیارات کے ساتھ فعال کرتی ہے، جبکہ مسابقتی کارکردگی اور ذمہ دار AI طریقوں کو برقرار رکھتی ہے۔

## تعارف

اس ٹیوٹوریل میں، ہم گوگل کے جیمما ماڈل فیملی اور اس کے بنیادی تصورات کا جائزہ لیں گے۔ ہم جیمما فیملی کے ارتقاء، انقلابی تربیتی طریقوں، جو جیمما ماڈلز کو مؤثر بناتے ہیں، فیملی میں کلیدی اقسام، اور مختلف تعیناتی منظرناموں میں عملی اطلاقات کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس ٹیوٹوریل کے اختتام تک، آپ قابل ہوں گے:

- گوگل کے جیمما ماڈل فیملی کے ڈیزائن فلسفہ اور ارتقاء کو سمجھنا
- کلیدی جدتوں کی شناخت کرنا جو جیمما ماڈلز کو مختلف پیرامیٹر سائزز میں اعلیٰ کارکردگی حاصل کرنے کے قابل بناتی ہیں
- جیمما ماڈل کی مختلف اقسام کے فوائد اور حدود کو پہچاننا
- حقیقی دنیا کے منظرناموں کے لیے مناسب اقسام کا انتخاب کرنے کے لیے جیمما ماڈلز کے علم کا اطلاق کرنا

## جدید AI ماڈل منظرنامے کو سمجھنا

AI کا منظرنامہ نمایاں طور پر ترقی کر چکا ہے، مختلف تنظیمیں زبان ماڈل کی ترقی کے لیے مختلف طریقے اپنا رہی ہیں۔ کچھ صرف APIs کے ذریعے قابل رسائی ملکیتی بند سورس ماڈلز پر توجہ مرکوز کرتے ہیں، جبکہ دیگر اوپن سورس رسائی اور شفافیت پر زور دیتے ہیں۔ روایتی طریقہ یا تو بڑے ملکیتی ماڈلز کے ساتھ جاری اخراجات یا اوپن سورس ماڈلز پر مشتمل ہوتا ہے، جنہیں تعیناتی کے لیے اہم تکنیکی مہارت کی ضرورت ہو سکتی ہے۔

یہ نقطہ نظر ان تنظیموں کے لیے چیلنجز پیدا کرتا ہے جو طاقتور AI صلاحیتوں کی تلاش میں ہیں، جبکہ اپنے ڈیٹا، اخراجات، اور تعیناتی کی لچک پر کنٹرول برقرار رکھنا چاہتے ہیں۔ روایتی طریقہ اکثر جدید کارکردگی اور عملی تعیناتی کے تحفظات کے درمیان انتخاب کی ضرورت ہوتی ہے۔

## قابل رسائی AI کی عمدگی کا چیلنج

اعلیٰ معیار، قابل رسائی AI کی ضرورت مختلف منظرناموں میں بڑھتی جا رہی ہے۔ ان اطلاقات پر غور کریں جنہیں مختلف تنظیمی ضروریات کے لیے لچکدار تعیناتی کے اختیارات، لاگت مؤثر نفاذ جہاں API کے اخراجات اہم ہو سکتے ہیں، جامع تفہیم کے لیے ملٹی موڈل صلاحیتوں، یا موبائل اور ایج ڈیوائسز پر خصوصی تعیناتی کی ضرورت ہوتی ہے۔

### کلیدی تعیناتی کی ضروریات

جدید AI تعیناتیوں کو کئی بنیادی ضروریات کا سامنا کرنا پڑتا ہے جو عملی اطلاق کو محدود کرتی ہیں:

- **رسائی**: شفافیت اور حسب ضرورت کے لیے اوپن سورس دستیابی
- **لاگت مؤثر**: مختلف بجٹ کے لیے معقول کمپیوٹیشنل ضروریات
- **لچک**: مختلف تعیناتی منظرناموں کے لیے متعدد ماڈل سائزز
- **ملٹی موڈل تفہیم**: وژن، متن، اور آڈیو پروسیسنگ کی صلاحیتیں
- **ایج تعیناتی**: موبائل اور محدود وسائل والے آلات پر بہتر کارکردگی

## جیمما ماڈل فلسفہ

جیمما ماڈل فیملی گوگل کے AI ماڈل کی ترقی کے جامع نقطہ نظر کی نمائندگی کرتی ہے، جو اوپن سورس رسائی، ملٹی موڈل صلاحیتوں، اور عملی تعیناتی کو ترجیح دیتی ہے، جبکہ مسابقتی کارکردگی کی خصوصیات کو برقرار رکھتی ہے۔ جیمما ماڈلز مختلف ماڈل سائزز، اعلیٰ معیار کی تربیتی طریقوں، اور مختلف ڈومینز اور تعیناتی منظرناموں کے لیے خصوصی اقسام کے ذریعے یہ حاصل کرتے ہیں۔

جیمما فیملی کارکردگی-افادیت کے اسپیکٹرم میں اختیارات فراہم کرنے کے لیے مختلف طریقوں کو شامل کرتی ہے، موبائل ڈیوائسز سے لے کر انٹرپرائز سرورز تک تعیناتی کو فعال کرتی ہے، جبکہ معنی خیز AI صلاحیتیں فراہم کرتی ہے۔ مقصد اعلیٰ معیار کی AI ٹیکنالوجی تک رسائی کو جمہوری بنانا ہے، جبکہ تعیناتی کے انتخاب میں لچک فراہم کرنا ہے۔

### جیمما کے بنیادی ڈیزائن اصول

جیمما ماڈلز کئی بنیادی اصولوں پر مبنی ہیں جو انہیں دیگر زبان ماڈل فیملیز سے ممتاز کرتے ہیں:

- **اوپن سورس پہلے**: تحقیق اور تجارتی استعمال کے لیے مکمل شفافیت اور رسائی
- **تحقیق پر مبنی ترقی**: وہی تحقیق اور ٹیکنالوجی استعمال کرتے ہوئے بنائے گئے جو جیمینی ماڈلز کو طاقت فراہم کرتی ہے
- **قابل توسیع آرکیٹیکچر**: مختلف کمپیوٹیشنل ضروریات سے مطابقت رکھنے کے لیے متعدد ماڈل سائزز
- **ذمہ دار AI**: حفاظتی اقدامات اور ذمہ دار ترقیاتی طریقوں کا انضمام

## جیمما فیملی کو فعال کرنے والی کلیدی ٹیکنالوجیز

### جدید تربیتی طریقے

جیمما فیملی کی ایک نمایاں خصوصیت گوگل کی جیمینی تحقیق سے حاصل کردہ نفیس تربیتی طریقہ ہے۔ جیمما ماڈلز بڑے ماڈلز سے ڈسٹلیشن، انسانی تاثرات سے تقویت یافتہ سیکھنے (RLHF)، اور ماڈل مرجنگ تکنیکوں کا فائدہ اٹھاتے ہیں تاکہ ریاضی، کوڈنگ، اور ہدایات کی پیروی میں بہتر کارکردگی حاصل کی جا سکے۔

تربیتی عمل میں بڑے انسٹرکٹ ماڈلز سے ڈسٹلیشن، انسانی تاثرات سے تقویت یافتہ سیکھنے (RLHF) انسانی ترجیحات کے ساتھ ہم آہنگی کے لیے، مشین تاثرات سے تقویت یافتہ سیکھنے (RLMF) ریاضیاتی استدلال کے لیے، اور عملدرآمد تاثرات سے تقویت یافتہ سیکھنے (RLEF) کوڈنگ صلاحیتوں کے لیے شامل ہیں۔

### ملٹی موڈل انضمام اور تفہیم

حالیہ جیمما ماڈلز نفیس ملٹی موڈل صلاحیتوں کو شامل کرتے ہیں جو مختلف ان پٹ اقسام کے درمیان جامع تفہیم کو فعال کرتے ہیں:

**وژن-زبان انضمام (جیمما 3)**: جیمما 3 متن اور تصاویر کو بیک وقت پروسیس کر سکتا ہے، جس سے یہ تصاویر کا تجزیہ کرنے، بصری مواد کے بارے میں سوالات کے جوابات دینے، تصاویر سے متن نکالنے، اور پیچیدہ بصری ڈیٹا کو سمجھنے کے قابل ہوتا ہے۔

**آڈیو پروسیسنگ (جیمما 3n)**: جیمما 3n میں جدید آڈیو صلاحیتیں شامل ہیں، جن میں خودکار تقریر کی شناخت (ASR) اور خودکار تقریر کا ترجمہ (AST) شامل ہیں، خاص طور پر انگریزی اور ہسپانوی، فرانسیسی، اطالوی، اور پرتگالی کے درمیان ترجمہ کے لیے مضبوط کارکردگی کے ساتھ۔

**انٹرلیوڈ ان پٹ پروسیسنگ**: جیمما ماڈلز ملٹی موڈل تعاملات کی پیچیدگی کو سمجھنے کے لیے انٹرلیوڈ ان پٹس کی حمایت کرتے ہیں، جہاں متن، تصاویر، اور آڈیو کو ایک ساتھ پروسیس کیا جا سکتا ہے۔

### آرکیٹیکچرل جدتیں

جیمما فیملی کارکردگی اور افادیت کے لیے کئی آرکیٹیکچرل اصلاحات کو شامل کرتی ہے:

**کانٹیکسٹ ونڈو کی توسیع**: جیمما 3 ماڈلز میں 128K-ٹوکین کانٹیکسٹ ونڈو شامل ہے، جو پچھلے جیمما ماڈلز سے 16 گنا بڑی ہے، جس سے متعدد دستاویزات یا سینکڑوں تصاویر سمیت وسیع معلومات کو پروسیس کرنا ممکن ہوتا ہے۔

**موبائل-پہلا آرکیٹیکچر (جیمما 3n)**: جیمما 3n پر-لیئر ایمبیڈنگز (PLE) ٹیکنالوجی اور میٹفارمر آرکیٹیکچر کا فائدہ اٹھاتا ہے، جس سے بڑے ماڈلز کو چھوٹے روایتی ماڈلز کے برابر میموری فٹ پرنٹس کے ساتھ چلانے کی اجازت ملتی ہے۔

**فنکشن کالنگ صلاحیتیں**: جیمما 3 فنکشن کالنگ کی حمایت کرتا ہے، جس سے ڈویلپرز کو پروگرامنگ انٹرفیس کے لیے قدرتی زبان کے انٹرفیس بنانے اور ذہین آٹومیشن سسٹمز تخلیق کرنے کی اجازت ملتی ہے۔

## ماڈل سائز اور تعیناتی کے اختیارات

جدید تعیناتی ماحولیات جیمما ماڈلز کی مختلف کمپیوٹیشنل ضروریات کے لیے لچک سے فائدہ اٹھاتے ہیں:

### چھوٹے ماڈلز (0.6B-4B)

جیمما چھوٹے ماڈلز فراہم کرتا ہے جو ایج تعیناتی، موبائل ایپلیکیشنز، اور محدود وسائل والے ماحول کے لیے موزوں ہیں، جبکہ متاثر کن صلاحیتوں کو برقرار رکھتے ہیں۔ 1B ماڈل چھوٹے اطلاقات کے لیے مثالی ہے، جبکہ 4B ماڈل ملٹی موڈل سپورٹ کے ساتھ متوازن کارکردگی اور لچک فراہم کرتا ہے۔

### درمیانے ماڈلز (8B-14B)

درمیانے درجے کے ماڈلز پیشہ ورانہ اطلاقات کے لیے بہتر صلاحیتیں فراہم کرتے ہیں، ورک سٹیشن اور سرور تعیناتی کے لیے کارکردگی اور کمپیوٹیشنل ضروریات کے درمیان بہترین توازن فراہم کرتے ہیں۔

### بڑے ماڈلز (27B+)

مکمل پیمانے کے ماڈلز جدید اطلاقات، تحقیق، اور انٹرپرائز تعیناتیوں کے لیے زیادہ سے زیادہ صلاحیت کی ضرورت کے لیے جدید کارکردگی فراہم کرتے ہیں۔ 27B ماڈل سب سے زیادہ قابل آپشن کی نمائندگی کرتا ہے جو اب بھی ایک واحد GPU پر چل سکتا ہے۔

### موبائل کے لیے بہتر ماڈلز (جیمما 3n)

جیمما 3n E2B اور E4B ماڈلز خاص طور پر موبائل اور ایج تعیناتی کے لیے تیار کیے گئے ہیں، مؤثر پیرامیٹر گنتی 2B اور 4B بالترتیب، جبکہ میموری فٹ پرنٹ کو کم سے کم 2GB کے لیے E2B اور 3GB کے لیے E4B تک کم کرنے کے لیے جدید آرکیٹیکچر کا استعمال کرتے ہیں۔

## جیمما ماڈل فیملی کے فوائد

### اوپن سورس رسائی

جیمما ماڈلز مکمل شفافیت اور حسب ضرورت صلاحیتیں فراہم کرتے ہیں، اوپن ویٹس کے ساتھ جو ذمہ دار تجارتی استعمال کی اجازت دیتے ہیں، تنظیموں کو اپنے منصوبوں اور اطلاقات میں انہیں ٹیون اور تعینات کرنے کے قابل بناتے ہیں۔

### تعیناتی کی لچک

ماڈل سائزز کی رینج مختلف ہارڈویئر کنفیگریشنز پر تعیناتی کو فعال کرتی ہے، موبائل ڈیوائسز سے لے کر ہائی اینڈ سرورز تک، مختلف پلیٹ فارمز کے لیے اصلاح کے ساتھ، بشمول گوگل کلاؤڈ TPUs، NVIDIA GPUs، AMD GPUs ROCm کے ذریعے، اور CPU پر عملدرآمد جیمما.cpp کے ذریعے۔

### کثیر لسانی عمدگی

جیمما ماڈلز کثیر لسانی تفہیم اور جنریشن میں مہارت رکھتے ہیں، 140 سے زائد زبانوں کی حمایت کرتے ہیں، بے مثال کثیر لسانی صلاحیتوں کے ساتھ، انہیں عالمی اطلاقات کے لیے موزوں بناتے ہیں۔

### مسابقتی کارکردگی

جیمما ماڈلز مسلسل بینچ مارکس پر مسابقتی نتائج حاصل کرتے ہیں، جیمما 3 صارف کی ترجیحی تشخیصات میں مقبول ملکیتی اور اوپن ماڈلز دونوں کے درمیان اعلیٰ درجہ حاصل کرتا ہے۔

### خصوصی صلاحیتیں

ڈومین مخصوص اطلاقات جیمما کی ملٹی موڈل تفہیم، فنکشن کالنگ صلاحیتوں، اور مختلف ہارڈویئر پلیٹ فارمز پر بہتر کارکردگی سے فائدہ اٹھاتے ہیں۔

## عملی مثالیں اور استعمال کے کیسز

تکنیکی تفصیلات میں جانے سے پہلے، آئیے کچھ ٹھوس مثالوں کا جائزہ لیتے ہیں کہ جیمما ماڈلز کیا کر سکتے ہیں:

### ملٹی موڈل تفہیم کی مثال

جیمما 3 متن اور تصاویر کو بیک وقت پروسیس کر سکتا ہے، جامع تجزیہ کے لیے:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### کثیر لسانی سپورٹ کی مثال

جیمما ماڈلز 140+ زبانوں میں مضبوط کثیر لسانی صلاحیتوں کا مظاہرہ کرتے ہیں:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### فنکشن کالنگ کی مثال

جیمما 3 فنکشن کالنگ کی حمایت کرتا ہے، قدرتی زبان کے انٹرفیس بنانے کے لیے:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### موبائل تعیناتی کی مثال (جیمما 3n)

جیمما 3n موبائل اور ایج تعیناتی کے لیے بہتر ہے، مؤثر میموری استعمال کے ساتھ:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### آڈیو پروسیسنگ کی مثال (جیمما 3n)

جیمما 3n تقریر کی شناخت اور ترجمہ کے لیے جدید آڈیو صلاحیتیں شامل کرتا ہے:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## جیمما فیملی کا ارتقاء

### جیمما 1.0 اور 2.0: بنیادی ماڈلز

ابتدائی جیمما ماڈلز نے اوپن سورس رسائی اور عملی تعیناتی کے بنیادی اصول قائم کیے:

- **جیمما-2B اور 7B**: مؤثر زبان کی تفہیم پر توجہ مرکوز کرتے ہوئے ابتدائی ریلیز
- **جیمما 1.5 سیریز**: کانٹیکسٹ ہینڈلنگ اور بہتر کارکردگی کو وسعت دی گئی
- **جیمما 2 فیملی**: ملٹی موڈل صلاحیتوں اور ماڈل سائزز کی توسیع کا تعارف

### جیمما 3: ملٹی موڈل عمدگی

جیمما 3 سیریز نے ملٹی موڈل صلاحیتوں اور کارکردگی میں نمایاں پیش رفت کی۔ جیمینی 2.0 ماڈلز کو طاقت فراہم کرنے والی تحقیق اور ٹیکنالوجی سے تیار کردہ، جیمما 3 نے وژن-زبان کی تفہیم، 128K-ٹوکین کانٹیکسٹ ونڈوز، فنکشن کالنگ، اور 140 سے زائد زبانوں کی حمایت متعارف کرائی۔

کلیدی جیمما 3 خصوصیات میں شامل ہیں:
- **جیمما 3-1B سے 27B**: مختلف تعیناتی ضروریات کے لیے جامع رینج
- **ملٹی موڈل تفہیم**: جدید متن اور بصری استدلال کی صلاحیتیں
- **وسیع کانٹیکسٹ**: 128K-ٹوکین پروسیسنگ کی صلاحیت
- **فنکشن کالنگ**: قدرتی زبان کے انٹرفیس کی تعمیر
- **بہتر تربیت**: ڈسٹلیشن اور تقویت یافتہ سیکھنے کا استعمال کرتے ہوئے بہتر بنایا گیا

### جیمما 3n: موبائل-پہلا جدت

جیمما 3n موبائل-پہلا AI آرکیٹیکچر میں ایک پیش رفت کی نمائندگی کرتا ہے، جس میں جدید پر-لیئر ایمبیڈنگز (PLE) ٹیکنالوجی، میٹفارمر آرکیٹیکچر کمپیوٹ لچک کے لیے، اور جامع ملٹی موڈل صلاحیتیں شامل ہیں، جن میں آڈیو پروسیسنگ شامل ہے۔

جیمما 3n جدتوں میں شامل ہیں:
- **E2B اور E4B ماڈلز**: مؤثر 2B اور 4B پیرامیٹر کارکردگی، کم میموری فٹ پرنٹ کے ساتھ
- **آڈیو صلاحیتیں**: اعلیٰ معیار کی ASR اور تقریر کا ترجمہ
- **ویڈیو تفہیم**: ویڈیو پروسیسنگ کی صلاحیتوں میں نمایاں اضافہ
- **موبائل اصلاح**: فونز اور ٹیبلٹس پر حقیقی وقت AI کے لیے تیار کیا گیا

## جیمما ماڈلز کے اطلاقات

### انٹرپرائز اطلاقات

تنظیمیں جیمما ماڈلز کا استعمال بصری مواد کے ساتھ دستاویز تجزیہ، ملٹی موڈل سپورٹ کے ساتھ کسٹمر سروس آٹومیشن، ذہین کوڈنگ معاونت، اور بزنس انٹیلیجنس اطلاقات کے لیے کرتی ہیں۔ اوپن سورس نوعیت مخصوص کاروباری ضروریات کے لیے حسب ضرورت کو فعال کرتی ہے، جبکہ ڈیٹا کی رازداری اور کنٹرول کو برقرار رکھتی ہے۔

### موبائل اور ایج کمپیوٹنگ

موبائل اطلاقات جیمما 3n کا فائدہ اٹھاتے ہیں، حقیقی وقت AI کو براہ راست آلات پر چلانے کے لیے، ذاتی اور نجی تجربات کو فعال کرتے ہیں، ملٹی موڈل AI صلاحیتوں کے ساتھ تیز رفتار۔ اطلاقات میں حقیقی وقت ترجمہ، ذہین معاونین، مواد کی تخلیق، اور ذاتی سفارشات شامل ہیں۔

### تعلیمی ٹیکنالوجی

تعلیمی پلیٹ
- جیما 3 ڈویلپرز کے لیے طاقتور صلاحیتیں فراہم کرتا ہے، جدید متن اور بصری استدلال کی صلاحیتوں کے ساتھ، جو ملٹی موڈل سمجھ کے لیے تصویر اور متن کی ان پٹ کو سپورٹ کرتا ہے۔
- جیما 3n چیٹ بوٹ ایرینا ایلو اسکورز میں مقبول ملکیتی اور اوپن ماڈلز کے درمیان اعلیٰ درجہ رکھتا ہے، جو صارفین کی مضبوط ترجیح کو ظاہر کرتا ہے۔

**کارکردگی کی کامیابیاں:**
- جیما 3 ماڈلز 128K ٹوکن تک پرامپٹ ان پٹ کو سنبھال سکتے ہیں، جو پچھلے جیما ماڈلز کے مقابلے میں 16 گنا بڑا کانٹیکسٹ ونڈو ہے۔
- جیما 3n پر لیئر ایمبیڈنگز (PLE) کا فائدہ اٹھاتا ہے، جو بڑی ماڈل صلاحیتوں کو برقرار رکھتے ہوئے RAM کے استعمال میں نمایاں کمی فراہم کرتا ہے۔

**موبائل آپٹیمائزیشن:**
- جیما 3n E2B صرف 2GB میموری کے ساتھ کام کرتا ہے جبکہ E4B کو صرف 3GB کی ضرورت ہوتی ہے، حالانکہ ان کے خام پیرامیٹرز کی تعداد بالترتیب 5B اور 8B ہے۔
- موبائل ڈیوائسز پر پرائیویسی فرسٹ، آف لائن ریڈی آپریشن کے ساتھ حقیقی وقت کی AI صلاحیتیں۔

**ٹریننگ اسکیل:**
- جیما 3 کو گوگل TPUs اور JAX فریم ورک کا استعمال کرتے ہوئے 1B کے لیے 2T ٹوکن، 4B کے لیے 4T، 12B کے لیے 12T، اور 27B ماڈلز کے لیے 14T ٹوکن پر تربیت دی گئی۔

### ماڈل موازنہ میٹرکس

| ماڈل سیریز | پیرامیٹرز کی حد | کانٹیکسٹ کی لمبائی | کلیدی طاقتیں | بہترین استعمال کے کیسز |
|--------------|------------------|----------------|---------------|----------------|
| **جیما 3** | 1B-27B | 128K | ملٹی موڈل سمجھ، فنکشن کالنگ | عمومی ایپلیکیشنز، وژن-لینگویج ٹاسکس |
| **جیما 3n** | E2B (5B), E4B (8B) | متغیر | موبائل آپٹیمائزیشن، آڈیو پروسیسنگ | موبائل ایپس، ایج کمپیوٹنگ، حقیقی وقت کی AI |
| **جیما 2.5** | 0.5B-72B | 32K-128K | متوازن کارکردگی، کثیر لسانی | پروڈکشن ڈیپلائمنٹ، موجودہ ورک فلو |
| **جیما-VL** | مختلف | متغیر | وژن-لینگویج اسپیشلائزیشن | تصویر کا تجزیہ، بصری سوالات کے جوابات |

## ماڈل انتخاب گائیڈ

### بنیادی ایپلیکیشنز کے لیے
- **جیما 3-1B**: ہلکے متن کے کام، سادہ موبائل ایپلیکیشنز
- **جیما 3-4B**: عمومی استعمال کے لیے متوازن کارکردگی کے ساتھ ملٹی موڈل سپورٹ

### ملٹی موڈل ایپلیکیشنز کے لیے
- **جیما 3-4B/12B**: تصویر کی سمجھ، بصری سوالات کے جوابات
- **جیما 3n**: آڈیو پروسیسنگ صلاحیتوں کے ساتھ موبائل ملٹی موڈل ایپس

### موبائل اور ایج ڈیپلائمنٹ کے لیے
- **جیما 3n E2B**: وسائل کی کمی والے آلات، حقیقی وقت کی موبائل AI
- **جیما 3n E4B**: آڈیو صلاحیتوں کے ساتھ بہتر موبائل کارکردگی

### انٹرپرائز ڈیپلائمنٹ کے لیے
- **جیما 3-12B/27B**: اعلیٰ کارکردگی کی زبان اور وژن کی سمجھ
- **فنکشن کالنگ صلاحیتیں**: ذہین آٹومیشن سسٹمز کی تعمیر

### عالمی ایپلیکیشنز کے لیے
- **کسی بھی جیما 3 قسم**: 140+ زبانوں کی سپورٹ کے ساتھ ثقافتی سمجھ
- **جیما 3n**: آڈیو ترجمے کے ساتھ موبائل فرسٹ عالمی ایپلیکیشنز

## ڈیپلائمنٹ پلیٹ فارمز اور رسائی

### کلاؤڈ پلیٹ فارمز
- **Vertex AI**: سرور لیس تجربے کے ساتھ اینڈ ٹو اینڈ MLOps صلاحیتیں
- **Google Kubernetes Engine (GKE)**: پیچیدہ ورک لوڈز کے لیے قابل توسیع کنٹینر ڈیپلائمنٹ
- **Google GenAI API**: تیز پروٹوٹائپنگ کے لیے براہ راست API رسائی
- **NVIDIA API Catalog**: NVIDIA GPUs پر بہتر کارکردگی

### مقامی ترقیاتی فریم ورک
- **Hugging Face Transformers**: ترقی کے لیے معیاری انضمام
- **Ollama**: مقامی ڈیپلائمنٹ اور انتظام کو آسان بناتا ہے
- **vLLM**: پروڈکشن کے لیے اعلیٰ کارکردگی کی خدمت
- **Gemma.cpp**: CPU کے لیے بہتر عملدرآمد
- **Google AI Edge**: موبائل اور ایج ڈیپلائمنٹ کی اصلاح

### سیکھنے کے وسائل
- **Google AI Studio**: جیما ماڈلز کو چند کلکس میں آزمائیں
- **Kaggle اور Hugging Face**: ماڈل ویٹس اور کمیونٹی کی مثالیں ڈاؤن لوڈ کریں
- **تکنیکی رپورٹس**: جامع دستاویزات اور تحقیقی مقالے
- **کمیونٹی فورمز**: فعال کمیونٹی سپورٹ اور مباحثے

### جیما ماڈلز کے ساتھ شروعات

#### ترقیاتی پلیٹ فارمز
1. **Google AI Studio**: ویب پر مبنی تجربات کے ساتھ شروع کریں
2. **Hugging Face Hub**: ماڈلز اور کمیونٹی کے نفاذ کو دریافت کریں
3. **مقامی ڈیپلائمنٹ**: ترقی کے لیے Ollama یا Transformers استعمال کریں

#### سیکھنے کا راستہ
1. **بنیادی تصورات کو سمجھیں**: ملٹی موڈل صلاحیتوں اور ڈیپلائمنٹ کے اختیارات کا مطالعہ کریں
2. **مختلف اقسام کے ساتھ تجربہ کریں**: مختلف ماڈل سائز اور خصوصی ورژنز آزمائیں
3. **نفاذ کی مشق کریں**: ترقیاتی ماحول میں ماڈلز کو ڈیپلائے کریں
4. **پروڈکشن کے لیے بہتر بنائیں**: مخصوص استعمال کے کیسز اور پلیٹ فارمز کے لیے فائن ٹیون کریں

#### بہترین طریقے
- **چھوٹے سے شروع کریں**: ابتدائی ترقی اور جانچ کے لیے جیما 3-4B کے ساتھ شروع کریں
- **سرکاری ٹیمپلیٹس استعمال کریں**: بہترین نتائج کے لیے مناسب چیٹ ٹیمپلیٹس کا اطلاق کریں
- **وسائل کی نگرانی کریں**: میموری کے استعمال اور انفرنس کی کارکردگی کو ٹریک کریں
- **خصوصی کاری پر غور کریں**: ملٹی موڈل یا موبائل ضروریات کے لیے مناسب اقسام کا انتخاب کریں

## جدید استعمال کے نمونے

### فائن ٹیوننگ کی مثالیں

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```

### خصوصی پرامپٹ انجینئرنگ

**ملٹی موڈل کاموں کے لیے:**
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```

**کانٹیکسٹ کے ساتھ فنکشن کالنگ کے لیے:**
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```

### ثقافتی سیاق و سباق کے ساتھ کثیر لسانی ایپلیکیشنز

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```

### پروڈکشن ڈیپلائمنٹ کے نمونے

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```

## کارکردگی کی اصلاح کی حکمت عملی

### میموری کی اصلاح

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```

### انفرنس کی اصلاح

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```

## بہترین طریقے اور رہنما اصول

### سیکیورٹی اور پرائیویسی

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```

### نگرانی اور تشخیص

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```

## نتیجہ

جیما ماڈل فیملی گوگل کے جامع نقطہ نظر کی نمائندگی کرتا ہے جو AI ٹیکنالوجی کو جمہوری بنانے کے ساتھ ساتھ مختلف ایپلیکیشنز اور ڈیپلائمنٹ کے منظرناموں میں مسابقتی کارکردگی کو برقرار رکھتا ہے۔ اوپن سورس رسائی، ملٹی موڈل صلاحیتوں، اور جدید آرکیٹیکچرل ڈیزائنز کے لیے اپنی وابستگی کے ذریعے، جیما تنظیموں اور ڈویلپرز کو طاقتور AI صلاحیتوں کو فائدہ اٹھانے کے قابل بناتا ہے، چاہے ان کے وسائل یا مخصوص ضروریات کچھ بھی ہوں۔

### کلیدی نکات

**اوپن سورس کی برتری**: جیما ظاہر کرتا ہے کہ اوپن سورس ماڈلز ملکیتی متبادلات کے ساتھ مسابقتی کارکردگی حاصل کر سکتے ہیں، جبکہ AI ڈیپلائمنٹ پر شفافیت، حسب ضرورت، اور کنٹرول فراہم کرتے ہیں۔

**ملٹی موڈل جدت**: جیما 3 اور جیما 3n میں متن، وژن، اور آڈیو صلاحیتوں کا انضمام قابل رسائی ملٹی موڈل AI میں ایک اہم پیش رفت کی نمائندگی کرتا ہے، جو مختلف ان پٹ اقسام کے جامع سمجھ کو قابل بناتا ہے۔

**موبائل فرسٹ آرکیٹیکچر**: جیما 3n کی بریک تھرو پر لیئر ایمبیڈنگز (PLE) ٹیکنالوجی اور موبائل آپٹیمائزیشن ظاہر کرتا ہے کہ طاقتور AI وسائل کی کمی والے آلات پر مؤثر طریقے سے کام کر سکتا ہے، بغیر صلاحیتوں کی قربانی دیے۔

**قابل توسیع ڈیپلائمنٹ**: 1B سے 27B پیرامیٹرز کی حد، خصوصی موبائل اقسام کے ساتھ، کمپیوٹیشنل ماحول کے مکمل اسپیکٹرم میں مستقل معیار اور کارکردگی کو برقرار رکھتے ہوئے ڈیپلائمنٹ کو قابل بناتا ہے۔

**ذمہ دار AI انضمام**: ShieldGemma 2 کے ذریعے بلٹ ان حفاظتی اقدامات اور ذمہ دار ترقیاتی طریقے اس بات کو یقینی بناتے ہیں کہ طاقتور AI صلاحیتوں کو محفوظ اور اخلاقی طور پر ڈیپلائے کیا جا سکتا ہے۔

### مستقبل کا نقطہ نظر

جیسا کہ جیما فیملی ترقی کرتا رہتا ہے، ہم توقع کر سکتے ہیں:

**بہتر موبائل صلاحیتیں**: موبائل اور ایج ڈیپلائمنٹ کے لیے مزید اصلاحات، جیما 3n آرکیٹیکچر کو اینڈرائیڈ اور کروم جیسے بڑے پلیٹ فارمز میں ضم کرنا۔

**ملٹی موڈل سمجھ میں توسیع**: وژن-لینگویج-آڈیو انضمام میں مسلسل پیش رفت، زیادہ جامع AI تجربات کے لیے۔

**بہتر کارکردگی**: بہتر کارکردگی-پیرامیٹر تناسب اور کم کمپیوٹیشنل ضروریات فراہم کرنے کے لیے جاری آرکیٹیکچرل جدت۔

**وسیع تر ماحولیاتی نظام کا انضمام**: موجودہ ورک فلو میں ہموار انضمام کے لیے ترقیاتی فریم ورک، کلاؤڈ پلیٹ فارمز، اور ڈیپلائمنٹ ٹولز کے لیے بہتر سپورٹ۔

**کمیونٹی کی ترقی**: کمیونٹی کے ذریعے تخلیق کردہ ماڈلز، ٹولز، اور ایپلیکیشنز کے ساتھ جیما ورس کی مسلسل توسیع، جو بنیادی صلاحیتوں کو بڑھاتے ہیں۔

### اگلے اقدامات

چاہے آپ حقیقی وقت کی AI صلاحیتوں کے ساتھ موبائل ایپلیکیشنز بنا رہے ہوں، ملٹی موڈل تعلیمی ٹولز تیار کر رہے ہوں، ذہین آٹومیشن سسٹمز بنا رہے ہوں، یا کثیر لسانی سپورٹ کی ضرورت والے عالمی ایپلیکیشنز پر کام کر رہے ہوں، جیما فیملی مضبوط کمیونٹی سپورٹ اور جامع دستاویزات کے ساتھ قابل توسیع حل فراہم کرتا ہے۔

**شروعات کی سفارشات:**
1. **Google AI Studio کے ساتھ تجربہ کریں** فوری عملی تجربے کے لیے
2. **Hugging Face سے ماڈلز ڈاؤن لوڈ کریں** مقامی ترقی اور حسب ضرورت کے لیے
3. **خصوصی اقسام کو دریافت کریں** جیسے جیما 3n موبائل ایپلیکیشنز کے لیے
4. **ملٹی موڈل صلاحیتوں کو نافذ کریں** جامع AI تجربات کے لیے
5. **پروڈکشن ڈیپلائمنٹ کے لیے سیکیورٹی کے بہترین طریقے اپنائیں**

**موبائل ترقی کے لیے**: آڈیو اور وژن صلاحیتوں کے ساتھ وسائل کی کمی والے ڈیپلائمنٹ کے لیے جیما 3n E2B کے ساتھ شروع کریں۔

**انٹرپرائز ایپلیکیشنز کے لیے**: فنکشن کالنگ اور جدید استدلال کے ساتھ زیادہ سے زیادہ صلاحیت کے لیے جیما 3-12B یا 27B ماڈلز پر غور کریں۔

**عالمی ایپلیکیشنز کے لیے**: ثقافتی طور پر آگاہ پرامپٹ انجینئرنگ کے ساتھ جیما کی 140+ زبانوں کی سپورٹ کا فائدہ اٹھائیں۔

**خصوصی استعمال کے کیسز کے لیے**: فائن ٹیوننگ کے طریقے اور ڈومین مخصوص اصلاحی تکنیکوں کو دریافت کریں۔

### 🔮 AI کی جمہوریت

جیما فیملی AI ترقی کے مستقبل کی مثال دیتا ہے جہاں طاقتور، قابل ماڈلز ہر کسی کے لیے قابل رسائی ہیں، انفرادی ڈویلپرز سے لے کر بڑے انٹرپرائزز تک۔ جدید تحقیق کو اوپن سورس رسائی کے ساتھ جوڑ کر، گوگل نے ایک بنیاد رکھی ہے جو تمام شعبوں اور پیمانوں میں جدت کو قابل بناتی ہے۔

جیما کی کامیابی، 100 ملین سے زیادہ ڈاؤن لوڈز اور 60,000+ کمیونٹی اقسام کے ساتھ، AI ٹیکنالوجی کو آگے بڑھانے میں اوپن تعاون کی طاقت کو ظاہر کرتی ہے۔ جیسے جیسے ہم آگے بڑھتے ہیں، جیما فیملی AI جدت کے لیے ایک محرک کے طور پر کام کرتا رہے گا، ایسی ایپلیکیشنز کی ترقی کو قابل بنائے گا جو پہلے صرف ملکیتی، مہنگے ماڈلز کے ساتھ ممکن تھیں۔

AI کا مستقبل کھلا، قابل رسائی، اور طاقتور ہے – اور جیما فیملی اس وژن کو حقیقت بنانے میں پیش پیش ہے۔

## اضافی وسائل

**سرکاری دستاویزات اور ماڈلز:**
- **Google AI Studio**: [جیما ماڈلز کو براہ راست آزمائیں](https://aistudio.google.com)
- **Hugging Face Collections**: 
  - [جیما 3 ریلیز](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
  - [جیما 3n پریویو](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Google AI Developer Documentation**: [جامع جیما گائیڈز](https://ai.google.dev/gemma)
- **Vertex AI Documentation**: [انٹرپرائز ڈیپلائمنٹ گائیڈز](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)

**تکنیکی وسائل:**
- **تحقیقی مقالے اور تکنیکی رپورٹس**: [گوگل ڈیپ مائنڈ پبلیکیشنز](https://deepmind.google/models/gemma/)
- **ڈویلپر بلاگ پوسٹس**: [تازہ ترین اعلانات اور ٹیوٹوریلز](https://developers.googleblog.com)
- **ماڈل کارڈز**: تفصیلی تکنیکی وضاحتیں اور کارکردگی کے بینچ مارکس

**کمیونٹی اور سپورٹ:**
- **Hugging Face کمیونٹی**: فعال مباحثے اور کمیونٹی کی مثالیں
- **GitHub ریپوزیٹریز**: اوپن سورس نفاذ اور ٹولز
- **ڈویلپر فورمز**: گوگل AI ڈویلپر کمیونٹی سپورٹ
- **Stack Overflow**: ٹیگ کردہ سوالات اور کمیونٹی کے حل

**ترقیاتی ٹولز:**
- **Ollama**: [سادہ مقامی ڈیپلائمنٹ](https://ollama.ai)
- **vLLM**: [اعلیٰ کارکردگی کی خدمت](https://github.com/vllm-project/vllm)
- **Transformers Library**: [Hugging Face انضمام](https://huggingface.co/docs/transformers)
- **Google AI Edge**: موبائل اور ایج ڈیپلائمنٹ کی اصلاح

**سیکھنے کے راستے:**
- **ابتدائی**: Google AI Studio → Hugging Face مثالیں → مقامی ڈیپلائمنٹ کے ساتھ شروع کریں
- **ڈویلپر**: Transformers انضمام → حسب ضرورت ایپلیکیشنز → پروڈکشن ڈیپلائمنٹ
- **محقق**: تکنیکی مقالے → فائن ٹیوننگ → نئے ایپلیکیشنز
- **انٹرپرائز**: Vertex AI ڈیپلائمنٹ → سیکیورٹی نفاذ → پیمانے کی اصلاح

جیما ماڈل فیملی نہ صرف AI ماڈلز کا مجموعہ ہے بلکہ قابل رسائی، طاقتور، اور ذمہ دار AI ایپلیکیشنز کی تعمیر کے لیے ایک مکمل ماحولیاتی نظام ہے۔ آج ہی دریافت کرنا شروع کریں اور اوپن سورس AI کے ساتھ ممکنات کی حدود کو آگے بڑھانے والے ڈویلپرز اور محققین کی بڑھتی ہوئی کمیونٹی میں شامل ہوں۔

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔