{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392a8a8",
   "metadata": {},
   "source": [
    "# سیشن 2 – RAG کی تشخیص راگاس کے ساتھ\n",
    "\n",
    "راگاس میٹرکس استعمال کرتے ہوئے کم سے کم RAG پائپ لائن کی تشخیص کریں: جواب کی مطابقت، درستگی، سیاق و سباق کی درستگی۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34473b",
   "metadata": {},
   "source": [
    "# منظر نامہ\n",
    "یہ منظر نامہ ایک مختصر Retrieval Augmented Generation (RAG) پائپ لائن کو مقامی طور پر جانچتا ہے۔ ہم:\n",
    "- ایک چھوٹا مصنوعی دستاویزاتی ذخیرہ تشکیل دیتے ہیں۔\n",
    "- دستاویزات کو ایمبیڈ کرتے ہیں اور ایک سادہ مماثلت تلاش کرنے والا نافذ کرتے ہیں۔\n",
    "- ایک مقامی ماڈل (Foundry Local / OpenAI-compatible) کا استعمال کرتے ہوئے مستند جوابات تیار کرتے ہیں۔\n",
    "- ragas میٹرکس (`answer_relevancy`, `faithfulness`, `context_precision`) کا حساب لگاتے ہیں۔\n",
    "- ایک تیز رفتار موڈ (env `RAG_FAST=1`) کی حمایت کرتے ہیں تاکہ جلدی سے جواب کی مطابقت کا حساب لگایا جا سکے۔\n",
    "\n",
    "اس نوٹ بک کو استعمال کریں تاکہ تصدیق کی جا سکے کہ آپ کا مقامی ماڈل + ایمبیڈنگز اسٹیک بڑے ذخیرے پر جانے سے پہلے حقیقت پر مبنی جوابات پیدا کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb1aa2",
   "metadata": {},
   "source": [
    "### وضاحت: ڈیپینڈنسی انسٹالیشن\n",
    "ضروری لائبریریاں انسٹال کرتا ہے:\n",
    "- `foundry-local-sdk` مقامی ماڈل مینجمنٹ کے لیے۔\n",
    "- `openai` کلائنٹ انٹرفیس کے لیے۔\n",
    "- `sentence-transformers` گھنے ایمبیڈنگز کے لیے۔\n",
    "- `ragas` + `datasets` تشخیص اور میٹرک حساب کے لیے۔\n",
    "- `langchain-openai` ایڈاپٹر ragas LLM انٹرفیس کے لیے۔\n",
    "\n",
    "دوبارہ چلانا محفوظ ہے؛ اگر ماحول پہلے سے تیار ہے تو چھوڑ دیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff641221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (ragas pulls datasets, evaluate, etc.)\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers ragas datasets numpy langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e82678",
   "metadata": {},
   "source": [
    "### وضاحت: کور امپورٹس اور میٹرکس\n",
    "کور لائبریریاں اور راگس میٹرکس لوڈ کرتا ہے۔ اہم اجزاء:\n",
    "- ایمبیڈنگز کے لیے SentenceTransformer۔\n",
    "- `evaluate` + منتخب کردہ راگس میٹرکس۔\n",
    "- تشخیصی کارپس بنانے کے لیے `Dataset`۔\n",
    "یہ امپورٹس ریموٹ کالز کو متحرک نہیں کرتے (سوائے ایمبیڈنگز کے لیے ممکنہ ماڈل کیش لوڈ کے)۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01938",
   "metadata": {},
   "source": [
    "### وضاحت: کھلونا کارپس اور QA گراؤنڈ ٹروتھ  \n",
    "ایک چھوٹے سے ان-میموری کارپس (`DOCS`)، صارف کے سوالات کا ایک سیٹ، اور متوقع درست جوابات کی وضاحت کرتا ہے۔ یہ بیرونی ڈیٹا حاصل کیے بغیر تیز اور متعین میٹرک حساب کو ممکن بناتے ہیں۔ حقیقی حالات میں، آپ پروڈکشن سوالات اور منتخب کردہ جوابات کا نمونہ لیں گے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27307d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    " 'Foundry Local exposes a local OpenAI-compatible endpoint.',\n",
    " 'RAG retrieves relevant context snippets before generation.',\n",
    " 'Local inference improves privacy and reduces latency.',\n",
    "]\n",
    "QUESTIONS = [\n",
    " 'What advantage does local inference offer?',\n",
    " 'How does RAG improve grounding?',\n",
    "]\n",
    "GROUND_TRUTH = [\n",
    " 'It reduces latency and preserves privacy.',\n",
    " 'It adds retrieved context snippets for factual grounding.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b2ec",
   "metadata": {},
   "source": [
    "### وضاحت: سروس انیشیئٹ، ایمبیڈنگز اور سیفٹی پیچ\n",
    "Foundry Local مینیجر کو انیشیئلائز کرتا ہے، `promptTemplate` کے لیے اسکیمہ ڈرفٹ سیفٹی پیچ لگاتا ہے، ماڈل آئی ڈی کو ریزولو کرتا ہے، OpenAI کے ساتھ مطابقت رکھنے والا کلائنٹ بناتا ہے، اور دستاویزات کے مجموعے کے لیے ڈینس ایمبیڈنگز کو پہلے سے کمپیوٹ کرتا ہے۔ یہ بازیافت اور جنریشن کے لیے قابل استعمال حالت تیار کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service running: True | Endpoint: http://127.0.0.1:57127/v1\n",
      "Cached models: [FoundryModelInfo(alias=gpt-oss-20b, id=gpt-oss-20b-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=9882 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu:4, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0)]\n",
      "Using model id: Phi-4-mini-instruct-cuda-gpu:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leestott\\AppData\\Local\\miniforge\\envs\\demo\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from foundry_local.models import FoundryModelInfo\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Safe monkeypatch for potential null promptTemplate field (schema drift guard) ---\n",
    "_original_from_list_response = FoundryModelInfo.from_list_response\n",
    "\n",
    "def _safe_from_list_response(response):  # type: ignore\n",
    "    try:\n",
    "        if isinstance(response, dict) and response.get(\"promptTemplate\") is None:\n",
    "            response[\"promptTemplate\"] = {}\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        print(f\"Warning normalizing promptTemplate: {e}\")\n",
    "    return _original_from_list_response(response)\n",
    "\n",
    "if getattr(FoundryModelInfo.from_list_response, \"__name__\", \"\") != \"_safe_from_list_response\":\n",
    "    FoundryModelInfo.from_list_response = staticmethod(_safe_from_list_response)  # type: ignore\n",
    "# --- End monkeypatch ---\n",
    "\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS','phi-3.5-mini')\n",
    "manager = FoundryLocalManager(alias)\n",
    "print(f\"Service running: {manager.is_service_running()} | Endpoint: {manager.endpoint}\")\n",
    "print('Cached models:', manager.list_cached_models())\n",
    "model_info = manager.get_model_info(alias)\n",
    "model_id = model_info.id\n",
    "print(f\"Using model id: {model_id}\")\n",
    "\n",
    "# OpenAI-compatible client\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import numpy as np\n",
    "doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d24a8",
   "metadata": {},
   "source": [
    "### وضاحت: ریٹریور فنکشن\n",
    "ایک سادہ ویکٹر مماثلت ریٹریور کو وضاحت کرتا ہے جو نارملائزڈ ایمبیڈنگز پر ڈاٹ پروڈکٹ استعمال کرتا ہے۔ ٹاپ-k ڈاکس واپس کرتا ہے (k=2 ڈیفالٹ ہے)۔ پروڈکشن میں اس کو ANN انڈیکس (FAISS، Chroma، Milvus) کے ساتھ تبدیل کریں تاکہ اسکیل اور لیٹینسی بہتر ہو۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af32d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return [DOCS[i] for i in sims.argsort()[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f284e",
   "metadata": {},
   "source": [
    "### وضاحت: جنریشن فنکشن\n",
    "`generate` ایک محدود پرامپٹ تیار کرتا ہے (سسٹم ہدایت دیتا ہے کہ صرف سیاق و سباق استعمال کریں) اور لوکل ماڈل کو کال کرتا ہے۔ کم درجہ حرارت (0.1) تخلیقی صلاحیت کے بجائے درست استخراج کو ترجیح دیتا ہے۔ مختصر جواب کا متن واپس کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7798ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, contexts):\n",
    "    ctx = \"\\n\".join(contexts)\n",
    "    messages = [\n",
    "        {'role':'system','content':'Answer using ONLY the provided context.'},\n",
    "        {'role':'user','content':f\"Context:\\n{ctx}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=model_id, messages=messages, max_tokens=120, temperature=0.1)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde788",
   "metadata": {},
   "source": [
    "### وضاحت: فال بیک کلائنٹ کی ابتدائی تشکیل\n",
    "یقینی بناتا ہے کہ `client` موجود ہو، چاہے پہلے کی ابتدائی تشکیل کا سیل چھوڑ دیا گیا ہو یا ناکام ہو گیا ہو—بعد کے جائزہ مراحل کے دوران NameError کو روکتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback client initialization (added after patch failure)\n",
    "try:\n",
    "    client  # type: ignore\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "    print('Initialized OpenAI-compatible client (late init).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17386ee",
   "metadata": {},
   "source": [
    "### وضاحت: تشخیصی لوپ اور میٹرکس\n",
    "تشخیصی ڈیٹا سیٹ تیار کرتا ہے (ضروری کالمز: سوال، جواب، سیاق و سباق، درست جوابات، حوالہ) اور منتخب کردہ راگاس میٹرکس پر عمل کرتا ہے۔\n",
    "\n",
    "آپٹیمائزیشن:\n",
    "- FAST_MODE صرف جواب کی مطابقت تک محدود ہے تاکہ جلدی ابتدائی ٹیسٹ کیے جا سکیں۔\n",
    "- ہر میٹرک کے لیے الگ لوپ مکمل دوبارہ حساب کتاب سے بچاتا ہے جب کوئی میٹرک ناکام ہو۔\n",
    "\n",
    "ایک dict پیدا کرتا ہے جس میں میٹرک -> اسکور (ناکامی کی صورت میں NaN) شامل ہوتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset columns: ['question', 'answer', 'contexts', 'ground_truths', 'reference']\n",
      "Metrics to compute: ['answer_relevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy finished in 78.1s -> 0.6975427764759168\n",
      "RAG evaluation results: {'answer_relevancy': 0.6975427764759168}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6975427764759168}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build evaluation dataset with required columns (including 'reference' for context_precision)\n",
    "records = []\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTH):\n",
    "    ctxs = retrieve(q)\n",
    "    ans = generate(q, ctxs)\n",
    "    records.append({\n",
    "        'question': q,\n",
    "        'answer': ans,\n",
    "        'contexts': ctxs,\n",
    "        'ground_truths': [gt],\n",
    "        'reference': gt\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.run_config import RunConfig\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=model_id, base_url=manager.endpoint, api_key=manager.api_key or 'not-needed', temperature=0.0, timeout=60)\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "# Fast mode: only answer_relevancy unless RAG_FAST=0\n",
    "FAST_MODE = os.getenv('RAG_FAST','1') == '1'\n",
    "metrics = [answer_relevancy] if FAST_MODE else [answer_relevancy, faithfulness, context_precision]\n",
    "\n",
    "base_timeout = 45 if FAST_MODE else 120\n",
    "\n",
    "ds = Dataset.from_list(records)\n",
    "print('Evaluation dataset columns:', ds.column_names)\n",
    "print('Metrics to compute:', [m.name for m in metrics])\n",
    "\n",
    "results_dict = {}\n",
    "for metric in metrics:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        cfg = RunConfig(timeout=base_timeout, max_workers=1)\n",
    "        partial = evaluate(ds, metrics=[metric], llm=ragas_llm, embeddings=LocalEmbeddings(), run_config=cfg, show_progress=False)\n",
    "        raw_val = partial[metric.name]\n",
    "        if isinstance(raw_val, list):\n",
    "            numeric = [v for v in raw_val if isinstance(v, (int, float))]\n",
    "            score = float(np.nanmean(numeric)) if numeric else math.nan\n",
    "        else:\n",
    "            score = float(raw_val)\n",
    "        results_dict[metric.name] = score\n",
    "    except Exception as e:\n",
    "        results_dict[metric.name] = math.nan\n",
    "        print(f\"Metric {metric.name} failed: {e}\")\n",
    "    finally:\n",
    "        print(f\"{metric.name} finished in {time.time()-t0:.1f}s -> {results_dict[metric.name]}\")\n",
    "\n",
    "print('RAG evaluation results:', results_dict)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "4895a2e01d85b98643a177c89ff7757f",
   "translation_date": "2025-10-09T07:15:55+00:00",
   "source_file": "Workshop/notebooks/session02_rag_eval_ragas.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}