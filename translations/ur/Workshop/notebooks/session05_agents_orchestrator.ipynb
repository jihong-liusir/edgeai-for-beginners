{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# سیشن 5 – ملٹی ایجنٹ آرکسٹریٹر\n",
    "\n",
    "Foundry Local کا استعمال کرتے ہوئے ایک سادہ دو ایجنٹ پائپ لائن (ریسرچر -> ایڈیٹر) کا مظاہرہ کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### وضاحت: ڈیپینڈنسی انسٹالیشن\n",
    "`foundry-local-sdk` اور `openai` انسٹال کرتا ہے جو مقامی ماڈل تک رسائی اور چیٹ مکمل کرنے کے لیے ضروری ہیں۔ یہ عمل بار بار کرنے پر بھی کوئی مسئلہ پیدا نہیں کرتا۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# منظر نامہ\n",
    "ایک کم سے کم دو ایجنٹ آرکسٹریٹر پیٹرن نافذ کرتا ہے:\n",
    "- **ریسرچر ایجنٹ** مختصر اور حقائق پر مبنی نکات جمع کرتا ہے\n",
    "- **ایڈیٹر ایجنٹ** ان نکات کو ایگزیکٹو وضاحت کے لیے دوبارہ لکھتا ہے\n",
    "\n",
    "مشترکہ میموری فی ایجنٹ، درمیانی نتائج کا ترتیب وار تبادلہ، اور ایک سادہ پائپ لائن فنکشن کا مظاہرہ کرتا ہے۔ مزید کرداروں (جیسے نقاد، تصدیق کنندہ) یا متوازی شاخوں کے لیے توسیع پذیر۔\n",
    "\n",
    "**ماحولیاتی متغیرات:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - استعمال کرنے کے لیے ڈیفالٹ ماڈل (ڈیفالٹ: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - بنیادی ایجنٹ ماڈل (ALIAS کو اووررائیڈ کرتا ہے)\n",
    "- `AGENT_MODEL_EDITOR` - ایڈیٹر ایجنٹ ماڈل (بنیادی ماڈل پر ڈیفالٹ)\n",
    "\n",
    "**SDK حوالہ:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**یہ کیسے کام کرتا ہے:**\n",
    "1. **FoundryLocalManager** خود بخود Foundry Local سروس شروع کرتا ہے\n",
    "2. مخصوص ماڈل ڈاؤن لوڈ اور لوڈ کرتا ہے (یا کیش شدہ ورژن استعمال کرتا ہے)\n",
    "3. تعامل کے لیے OpenAI کے موافق اینڈ پوائنٹ فراہم کرتا ہے\n",
    "4. ہر ایجنٹ مختلف ماڈل استعمال کر سکتا ہے خصوصی کاموں کے لیے\n",
    "5. بلٹ ان ریٹری لاجک عارضی ناکامیوں کو خوش اسلوبی سے ہینڈل کرتا ہے\n",
    "\n",
    "**اہم خصوصیات:**\n",
    "- ✅ خودکار سروس دریافت اور ابتدائیہ\n",
    "- ✅ ماڈل لائف سائیکل مینجمنٹ (ڈاؤن لوڈ، کیش، لوڈ)\n",
    "- ✅ OpenAI SDK مطابقت ایک مانوس API کے لیے\n",
    "- ✅ ایجنٹ کی مہارت کے لیے ملٹی ماڈل سپورٹ\n",
    "- ✅ مضبوط خرابی ہینڈلنگ ریٹری لاجک کے ساتھ\n",
    "- ✅ مقامی انفرنس (کلاؤڈ API کی ضرورت نہیں)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### وضاحت: کور امپورٹس اور ٹائپنگ  \n",
    "ایجنٹ پیغام ذخیرہ کرنے کے لیے ڈیٹاکلاسز اور وضاحت کے لیے ٹائپنگ ہنٹس متعارف کراتا ہے۔ بعد کے ایجنٹ کے اعمال کے لیے فاؤنڈری لوکل مینیجر اور اوپن اے آئی کلائنٹ کو امپورٹ کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### وضاحت: ماڈل کی ابتدا (SDK پیٹرن)\n",
    "Foundry Local Python SDK کا استعمال مضبوط ماڈل مینجمنٹ کے لیے:\n",
    "- **FoundryLocalManager(alias)** - سروس کو خودکار طور پر شروع کرتا ہے اور ماڈل کو عرفیت کے ذریعے لوڈ کرتا ہے\n",
    "- **get_model_info(alias)** - عرفیت کو ٹھوس ماڈل ID میں تبدیل کرتا ہے\n",
    "- **manager.endpoint** - OpenAI کلائنٹ کے لیے سروس کا اینڈ پوائنٹ فراہم کرتا ہے\n",
    "- **manager.api_key** - API کلید فراہم کرتا ہے (مقامی استعمال کے لیے اختیاری)\n",
    "- مختلف ایجنٹس (پرائمری بمقابلہ ایڈیٹر) کے لیے الگ ماڈلز کی حمایت کرتا ہے\n",
    "- مضبوطی کے لیے بلٹ ان ریٹری لاجک کے ساتھ ایکسپونینشل بیک آف\n",
    "- سروس کے تیار ہونے کو یقینی بنانے کے لیے کنکشن کی تصدیق\n",
    "\n",
    "**اہم SDK پیٹرن:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**لائف سائیکل مینجمنٹ:**\n",
    "- مینیجرز کو عالمی طور پر ذخیرہ کیا جاتا ہے تاکہ مناسب صفائی ہو سکے\n",
    "- ہر ایجنٹ ماہرین کے لیے مختلف ماڈل استعمال کر سکتا ہے\n",
    "- خودکار سروس دریافت اور کنکشن ہینڈلنگ\n",
    "- ناکامیوں پر ایکسپونینشل بیک آف کے ساتھ شائستہ ریٹری\n",
    "\n",
    "یہ ایجنٹ آرکسٹریشن شروع ہونے سے پہلے مناسب ابتدا کو یقینی بناتا ہے۔\n",
    "\n",
    "**حوالہ:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### وضاحت: ایجنٹ اور میموری کلاسز\n",
    "ہلکی پھلکی `AgentMsg` میموری اندراجات کے لیے اور `Agent` کو شامل کرتی ہے:\n",
    "- **سسٹم کردار** - ایجنٹ کی شخصیت اور ہدایات\n",
    "- **پیغام کی تاریخ** - گفتگو کے سیاق و سباق کو برقرار رکھتی ہے\n",
    "- **act() طریقہ** - مناسب غلطی ہینڈلنگ کے ساتھ اعمال انجام دیتا ہے\n",
    "\n",
    "ایجنٹ مختلف ماڈلز (پرائمری بمقابلہ ایڈیٹر) استعمال کر سکتا ہے اور ہر ایجنٹ کے لیے الگ تھلگ سیاق و سباق برقرار رکھتا ہے۔ یہ پیٹرن ممکن بناتا ہے:\n",
    "- اعمال کے دوران میموری کا تسلسل\n",
    "- ہر ایجنٹ کے لیے ماڈل کی لچکدار تفویض\n",
    "- غلطیوں کی تنہائی اور بحالی\n",
    "- آسان چیننگ اور آرکیسٹریشن\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### وضاحت: مربوط پائپ لائن  \n",
    "دو خاص ایجنٹس تخلیق کرتا ہے:  \n",
    "- **ریسرچر**: بنیادی ماڈل استعمال کرتا ہے، حقائق پر مبنی معلومات جمع کرتا ہے  \n",
    "- **ایڈیٹر**: الگ ماڈل استعمال کر سکتا ہے (اگر ترتیب دیا گیا ہو)، معلومات کو بہتر بناتا اور دوبارہ لکھتا ہے  \n",
    "\n",
    "`pipeline` فنکشن:  \n",
    "1. ریسرچر خام معلومات جمع کرتا ہے  \n",
    "2. ایڈیٹر اسے ایگزیکٹو کے لیے تیار آؤٹ پٹ میں بہتر بناتا ہے  \n",
    "3. درمیانی اور حتمی نتائج دونوں واپس کرتا ہے  \n",
    "\n",
    "یہ طریقہ کار فراہم کرتا ہے:  \n",
    "- ماڈل کی تخصیص (مختلف کرداروں کے لیے مختلف ماڈلز)  \n",
    "- معیار میں بہتری، کئی مراحل کی پروسیسنگ کے ذریعے  \n",
    "- معلومات کی تبدیلی کی قابلِ سراغی  \n",
    "- مزید ایجنٹس یا متوازی پروسیسنگ کے لیے آسان توسیع  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### وضاحت: پائپ لائن کی عملدرآمد اور نتائج\n",
    "ایک کمپلائنس + لیٹنسی سے متعلق سوال پر ملٹی ایجنٹ پائپ لائن کو چلانے کا مظاہرہ:\n",
    "- معلومات کی ملٹی اسٹیج تبدیلی\n",
    "- ایجنٹ کی مہارت اور تعاون\n",
    "- نتائج کے معیار میں بہتری کے ذریعے اصلاح\n",
    "- ٹریس ایبلٹی (دونوں درمیانی اور حتمی نتائج محفوظ)\n",
    "\n",
    "**نتائج کی ساخت:**\n",
    "- `question` - صارف کا اصل سوال\n",
    "- `research` - خام تحقیق کا نتیجہ (حقائق پر مبنی نکات)\n",
    "- `final` - بہتر کردہ ایگزیکٹو خلاصہ\n",
    "- `models` - ہر مرحلے کے لیے استعمال کیے گئے ماڈلز\n",
    "\n",
    "**توسیعی خیالات:**\n",
    "1. معیار کے جائزے کے لیے ایک Critic ایجنٹ شامل کریں\n",
    "2. مختلف پہلوؤں کے لیے متوازی تحقیقاتی ایجنٹس نافذ کریں\n",
    "3. حقائق کی تصدیق کے لیے ایک Verifier ایجنٹ شامل کریں\n",
    "4. مختلف پیچیدگی کی سطحوں کے لیے مختلف ماڈلز استعمال کریں\n",
    "5. تکراری بہتری کے لیے فیڈبیک لوپس نافذ کریں\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### ایڈوانسڈ: کسٹم ایجنٹ کنفیگریشن\n",
    "\n",
    "ایجنٹ کے رویے کو حسب ضرورت بنانے کی کوشش کریں، ابتدائی سیل چلانے سے پہلے ماحول کے متغیرات میں ترمیم کریں:\n",
    "\n",
    "**دستیاب ماڈلز:**\n",
    "- تمام دستیاب ماڈلز دیکھنے کے لیے ٹرمینل میں `foundry model ls` استعمال کریں\n",
    "- مثالیں: phi-4-mini، phi-3.5-mini، qwen2.5-7b، llama-3.2-3b، وغیرہ۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈس کلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-09T07:17:21+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}