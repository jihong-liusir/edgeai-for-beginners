<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a534c7d108d29f908a8f9f693d694664",
  "translation_date": "2025-09-24T09:43:21+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "tw"
}
-->
# 第五節：使用 Foundry Local 快速構建 AI 驅動代理

注意：Foundry Local 中的代理功能不斷演進——在實施高級模式之前，請確認最新版本說明中的支持情況。

## 概述

使用 Foundry Local 快速原型化代理應用：系統提示、基礎設置和編排模式。當代理支持存在時，您可以標準化使用 OpenAI 兼容的函數調用，或在混合設計中使用 Azure AI Agents 的雲端支持。

> **🔄 已更新至現代 SDK**：本模組已與最新的 Microsoft Foundry-Local 存儲庫模式對齊，並匹配 `samples/05/` 中的全面實現。示例現在使用現代的 `foundry-local-sdk` 和 `OpenAI` 客戶端，而非手動請求。

**🏗️ 架構亮點：**
- **專家代理**：檢索、推理和執行代理，具備不同能力
- **協調模式**：通過反饋循環編排多代理工作流
- **現代 SDK 集成**：使用 `FoundryLocalManager` 和 OpenAI 客戶端
- **生產就緒**：包括錯誤處理、性能監控和健康檢查
- **全面示例**：帶有高級功能的交互式 Jupyter notebook

**📁 本地實現：**
- `samples/05/multi_agent_orchestration.ipynb` - 交互式示例和基準測試
- `samples/05/agents/specialists.py` - 代理實現
- `samples/05/agents/coordinator.py` - 編排邏輯

參考資料：
- Foundry Local 文檔：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Azure AI Foundry Agents：https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- 函數調用示例（Foundry Local 示例）：https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## 學習目標
- 設計系統提示和基礎設置策略以確保可靠行為
- 實現函數調用（工具使用）模式
- 編排多代理工作流（本地和混合）
- 規劃可觀測性和安全性

## 第一部分：系統提示和基礎設置

- 定義嚴格的角色、約束和輸出結構
- 使用本地或企業數據作為基礎設置
- 強制 JSON 輸出以便下游自動化

## 第二部分：函數調用（現代 SDK 方法）

```python
# tools.py
import json
from typing import List, Dict, Any

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

# Modern tools format for OpenAI API
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "City name"}
                },
                "required": ["city"]
            }
        }
    }
]
```

```python
# agent.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import json
from tools import TOOLS, get_weather

# Initialize Foundry Local Manager
alias = "phi-4-mini"
manager = FoundryLocalManager(alias)

# Create OpenAI client using Foundry Local endpoint
client = OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key
)

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def process_function_call(messages: List[Dict], tools: List[Dict]) -> str:
    """Process function calling with modern OpenAI API."""
    try:
        response = client.chat.completions.create(
            model=manager.get_model_info(alias).id,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        message = response.choices[0].message
        
        if message.tool_calls:
            # Handle function calls
            messages.append(message)
            
            for tool_call in message.tool_calls:
                if tool_call.function.name == "get_weather":
                    args = json.loads(tool_call.function.arguments)
                    result = get_weather(args["city"])
                    
                    # Add function result to messages
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
            
            # Get final response
            final_response = client.chat.completions.create(
                model=manager.get_model_info(alias).id,
                messages=messages
            )
            return final_response.choices[0].message.content
        else:
            return message.content
            
    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": "What's the weather in Paris?"}
]

result = process_function_call(messages, TOOLS)
print(result)
```

運行：
```powershell
# Ensure Foundry Local is running with a model
foundry model run phi-4-mini
python agent.py
```


## 第三部分：多代理編排（模式）

設計一個協調器，使用 Foundry Local 的 OpenAI 兼容端點將任務路由到專家代理（檢索、推理、執行）。

步驟 1）使用現代 SDK 定義專家代理（參見 `samples/05/agents/specialists.py`）
```python
# agents/specialists.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import List, Dict, Any

class FoundryClient:
    """Shared client for all specialist agents."""
    
    def __init__(self, model_alias: str = "phi-4-mini"):
        self.client = None
        self.model_name = None
        self.model_alias = model_alias
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local."""
        try:
            manager = FoundryLocalManager(self.model_alias)
            model_info = manager.get_model_info(self.model_alias)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.model_name = model_info.id
            print(f"✅ Foundry Local initialized with model: {self.model_name}")
        except Exception as e:
            print(f"❌ Error initializing Foundry Local: {e}")
            raise
    
    def chat(self, messages: List[Dict[str, str]], max_tokens: int = 300, temperature: float = 0.4) -> str:
        """Send chat completion request to the model."""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response: {str(e)}"

# Global client instance
_client = FoundryClient()

class RetrievalAgent:
    """Agent specialized in retrieving relevant information from knowledge sources."""
    
    SYSTEM = """You are a specialized retrieval agent. Your job is to extract and retrieve 
    the most relevant information from knowledge sources based on a given query. Focus on key facts, 
    data points, and contextual information that would be useful for decision-making."""
    
    def run(self, query: str) -> str:
        """Retrieve relevant information based on the query."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Query: {query}\n\nRetrieve the most relevant key facts, data points, and contextual information that would help answer this query or support decision-making around it."}
        ]
        return _client.chat(messages)

class ReasoningAgent:
    """Agent specialized in step-by-step analysis and reasoning."""
    
    SYSTEM = """You are a specialized reasoning agent. Your job is to analyze inputs 
    step-by-step and produce structured, logical conclusions. Break down complex problems 
    into manageable parts and provide clear reasoning for your conclusions."""
    
    def run(self, context: str, question: str) -> str:
        """Analyze context and question to produce structured conclusions."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\n\nAnalyze this step-by-step and provide a structured, logical conclusion with clear reasoning."}
        ]
        return _client.chat(messages, max_tokens=400)

class ExecutionAgent:
    """Agent specialized in creating actionable execution plans."""
    
    SYSTEM = """You are a specialized execution agent. Your job is to transform decisions 
    and conclusions into concrete, actionable steps. Always format your response as valid JSON 
    with an array of action items. Each action should be specific, measurable, and achievable."""
    
    def run(self, decision: str) -> str:
        """Transform decision into actionable steps in JSON format."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Decision/Conclusion:\n{decision}\n\nCreate 3-5 specific, actionable steps to implement this decision. Format as JSON with this structure:\n{{\"actions\": [{{\"step\": 1, \"description\": \"...\", \"priority\": \"high/medium/low\", \"timeline\": \"...\"}}]}}"}
        ]
        return _client.chat(messages, max_tokens=400, temperature=0.3)
```

步驟 2）構建具有高級功能的協調器
```python
# agents/coordinator.py
from .specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent
from typing import Dict, Any
import time
import json

class Coordinator:
    """Multi-agent coordinator that orchestrates specialist agents to handle complex tasks."""
    
    def __init__(self):
        """Initialize the coordinator with specialist agents."""
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()
    
    def handle(self, user_goal: str) -> Dict[str, Any]:
        """
        Orchestrate multiple agents to handle a complex user goal.
        
        Args:
            user_goal: The user's high-level goal or request
            
        Returns:
            Dictionary containing the goal, context, decision, and actions
        """
        print(f"🎯 **Coordinator:** Processing goal: {user_goal}")
        print("=" * 60)
        
        start_time = time.time()
        
        # Step 1: Retrieve relevant context
        print("📚 **Step 1:** Retrieving context...")
        context = self.retrieval.run(user_goal)
        print(f"   ✅ Context retrieved ({len(context)} chars)")
        
        # Step 2: Analyze and reason about the context
        print("🧠 **Step 2:** Analyzing and reasoning...")
        decision = self.reasoning.run(context, user_goal)
        print(f"   ✅ Analysis completed ({len(decision)} chars)")
        
        # Step 3: Create actionable execution plan
        print("⚡ **Step 3:** Creating execution plan...")
        actions = self.execution.run(decision)
        print(f"   ✅ Execution plan created ({len(actions)} chars)")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions,
            "agent_flow": ["retrieval", "reasoning", "execution"],
            "processing_time": processing_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"✅ **Coordination Complete** (⏱️ {processing_time:.2f}s)")
        return result
    
    def handle_with_feedback(self, user_goal: str, feedback_rounds: int = 1) -> Dict[str, Any]:
        """
        Handle a goal with multiple feedback rounds for refinement.
        
        Args:
            user_goal: The user's high-level goal or request
            feedback_rounds: Number of feedback rounds to perform
            
        Returns:
            Dictionary containing the refined result
        """
        result = self.handle(user_goal)
        
        for round_num in range(feedback_rounds):
            print(f"\n🔄 **Feedback Round {round_num + 1}:**")
            print("-" * 40)
            
            # Use reasoning agent to refine the execution plan
            refinement_prompt = f"""
            Original Goal: {user_goal}
            Current Decision: {result['decision']}
            Current Actions: {result['actions']}
            
            Review the above and suggest improvements or refinements to make the execution plan more effective.
            """
            
            refined_decision = self.reasoning.run(result['context'], refinement_prompt)
            refined_actions = self.execution.run(refined_decision)
            
            result['decision'] = refined_decision
            result['actions'] = refined_actions
            result['refinement_rounds'] = round_num + 1
            
            print(f"   ✅ Round {round_num + 1} refinement completed")
        
        return result

def main():
    """Main function demonstrating the multi-agent coordinator."""
    print("🤖 **Multi-Agent Coordinator Demo**")
    print("=" * 50)
    
    # Create coordinator
    coord = Coordinator()
    
    # Example goals
    example_goals = [
        "Create a plan to onboard 5 new customers this month",
        "Develop a strategy to improve team productivity by 20%",
        "Design a customer feedback collection system"
    ]
    
    # Process example with feedback
    goal = example_goals[0]
    print(f"🎯 **Processing Goal:** {goal}")
    print("-" * 50)
    
    try:
        # Basic processing
        result = coord.handle(goal)
        
        # With feedback refinement
        refined_result = coord.handle_with_feedback(goal, feedback_rounds=1)
        
        print("\n📊 **Final Result:**")
        print("=" * 50)
        print(f"**Goal:** {refined_result['goal']}")
        print(f"**Processing Time:** {refined_result['processing_time']:.2f}s")
        
        # Try to parse actions as JSON
        try:
            actions_json = json.loads(refined_result['actions'])
            print(f"\n**Formatted Actions:**")
            print(json.dumps(actions_json, indent=2))
        except (json.JSONDecodeError, TypeError):
            print(f"\n**Actions:** {refined_result['actions']}")
            
    except Exception as e:
        print(f"❌ **Error:** {e}")
        print("\nPlease ensure Foundry Local is running with a model loaded.")

if __name__ == "__main__":
    main()
```

步驟 3）驗證 Foundry Local 並運行示例
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator from Module08 directory
cd Module08
python -m samples.05.agents.coordinator

REM Or explore the comprehensive Jupyter notebook
jupyter notebook samples/05/multi_agent_orchestration.ipynb
```


> **📚 本地示例參考：**
> - **主要實現**：`samples/05/agents/specialists.py` 和 `samples/05/agents/coordinator.py`
> - **全面示例**：`samples/05/multi_agent_orchestration.ipynb`
> - **設置說明**：`samples/05/README.md`
> 
> **🔗 相關 Foundry Local 示例：**
> - [函數調用示例](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
> - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)

指南：
- 在代理之間實施重試和超時
- 添加一個小型內存存儲（字典）以保存對話/線程狀態
- 在鏈接多次調用時引入速率限制

## 第四部分：可觀測性和安全性

本地跟蹤提示、響應和錯誤，同時在代理堆棧中強制數據清理。

步驟 1）輕量級請求日誌（可選）

注意：以下助手未默認包含。如果需要本地 JSON 日誌以進行實驗，請創建 `infra/obs.py`。
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```

集成日誌到代理（可選）：
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```

步驟 2）通過 CLI 驗證可用性和基本健康狀況
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```

步驟 3）敏感信息清理和 PII 衛生
- 在向模型發送消息之前，剝離或哈希敏感字段（電子郵件、電話號碼、ID）
- 將原始源數據保留在設備上，只傳遞必要的上下文字符串

示例清理助手：
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

在代理中使用：
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```

步驟 4）斷路器和錯誤處理
- 使用 try/except 和指數回退包裹每次代理調用
- 在多次失敗時短路管道

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```

步驟 5）本地審計跟蹤和導出
- 將 JSON 日誌存儲在 `./agent_logs` 下
- 定期壓縮和輪換日誌
- 導出摘要以進行審查（計數、平均延遲、錯誤率）

步驟 6）與 Microsoft Learn 文檔交叉檢查
- Foundry Local 提供 OpenAI 兼容 API（使用 `curl /v1/models` 驗證）
- 使用 `foundry model run <name>` 確認模型可用性
- 遵循官方指南進行客戶端集成和示例應用（Open WebUI/操作指南）

參考資料：
- **Foundry Local 文檔**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- **Azure AI Agents**: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- **本地示例**：
  - 多代理編排：`Module08/samples/05/multi_agent_orchestration.ipynb`
  - 代理實現：`Module08/samples/05/agents/`
  - 示例 README：`Module08/samples/05/README.md`
- **官方 Microsoft 示例**：
  - [函數調用](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
  - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
  - [Foundry Local Python SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **集成示例**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## 下一步
- 探索 Azure AI Agents 用於雲端托管編排
- 添加企業連接器（Microsoft Graph、搜索、數據庫）

---

