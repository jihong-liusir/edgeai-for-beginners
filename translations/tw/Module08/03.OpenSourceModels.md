<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T11:44:50+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "tw"
}
-->
# 第三節：使用 Foundry Local 的開源模型

## 概述

本節課程探討如何將開源模型引入 Foundry Local：選擇社群模型、整合 Hugging Face 內容，以及採用「自帶模型」（BYOM）策略。您還將了解 Model Mondays 系列，持續學習並發掘模型。

參考資料：
- Foundry Local 文件：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- 編譯 Hugging Face 模型：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays：https://aka.ms/model-mondays
- Foundry Local GitHub：https://github.com/microsoft/Foundry-Local

## 學習目標
- 發掘並評估適用於本地推理的開源模型
- 在 Foundry Local 中編譯並運行選定的 Hugging Face 模型
- 根據準確性、延遲和資源需求應用模型選擇策略
- 使用快取和版本管理在本地管理模型

## 第一部分：模型發掘與選擇（逐步指南）

步驟 1）列出本地目錄中的可用模型  
```cmd
foundry model list
```
  
步驟 2）快速嘗試兩個候選模型（首次運行時自動下載）  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
步驟 3）記錄基本指標  
- 觀察延遲（主觀）和固定提示的質量  
- 在模型運行時通過任務管理器查看記憶體使用情況  

## 第二部分：通過 CLI 運行目錄模型（逐步指南）

步驟 1）啟動模型  
```cmd
foundry model run llama-3.2
```
  
步驟 2）通過 OpenAI 兼容端點發送測試提示  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## 第三部分：BYOM – 編譯 Hugging Face 模型（逐步指南）

按照官方指南編譯模型。以下是高層次流程——具體命令和支持的配置請參閱 Microsoft Learn 文章。

步驟 1）準備工作目錄  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
步驟 2）編譯支持的 HF 模型  
- 使用 Learn 文件中的步驟轉換並將編譯的 ONNX 模型放置在您的 `models` 目錄中  
- 確認：  
```cmd
foundry cache ls
```
  
您應該看到編譯的模型名稱（例如，`llama-3.2`）。  

步驟 3）運行編譯的模型  
```cmd
foundry model run llama-3.2 --verbose
```
  
注意事項：  
- 確保有足夠的磁碟空間和 RAM 以進行編譯和運行  
- 從較小的模型開始驗證流程，然後逐步擴展  

## 第四部分：實用模型策劃（逐步指南）

步驟 1）創建 `models.json` 註冊表  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
步驟 2）小型選擇器腳本  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## 第五部分：實作基準測試（逐步指南）

步驟 1）簡單延遲基準測試  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
步驟 2）質量抽查  
- 使用固定提示集，將輸出捕獲到 CSV/JSON  
- 手動評分流暢性、相關性和正確性（1–5 分）  

## 第六部分：後續步驟
- 訂閱 Model Mondays，獲取新模型和技巧：https://aka.ms/model-mondays  
- 將發現結果貢獻到您團隊的 `models.json`  
- 為第四節做好準備：比較 LLMs 與 SLMs、本地與雲端推理，以及實作演示  

---

