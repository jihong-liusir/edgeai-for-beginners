<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T13:13:01+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "nl"
}
-->
# Containerized Cloud Deployment - Productieschaaloplossingen

Deze uitgebreide tutorial behandelt drie belangrijke benaderingen voor het implementeren van Microsoft's Phi-4-mini-instruct model in containeromgevingen: vLLM, Ollama en SLM Engine met ONNX Runtime. Dit model met 3,8 miljard parameters is een optimale keuze voor redeneertaken en behoudt tegelijkertijd efficiëntie voor edge-implementatie.

## Inhoudsopgave

1. [Introductie tot Phi-4-mini Containerimplementatie](../../../Module03)
2. [Leerdoelen](../../../Module03)
3. [Begrip van Phi-4-mini Classificatie](../../../Module03)
4. [vLLM Containerimplementatie](../../../Module03)
5. [Ollama Containerimplementatie](../../../Module03)
6. [SLM Engine met ONNX Runtime](../../../Module03)
7. [Vergelijkingskader](../../../Module03)
8. [Best Practices](../../../Module03)

## Introductie tot Phi-4-mini Containerimplementatie

Small Language Models (SLMs) vormen een belangrijke vooruitgang in EdgeAI en maken geavanceerde natuurlijke taalverwerkingsmogelijkheden mogelijk op apparaten met beperkte middelen. Deze tutorial richt zich op containerimplementatiestrategieën voor Microsoft's Phi-4-mini-instruct, een geavanceerd redeneer-model dat capaciteit en efficiëntie in balans brengt.

### Uitgelicht model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 miljard parameters)**: Microsoft's nieuwste lichtgewicht instructie-afgestemde model, ontworpen voor omgevingen met beperkte geheugen- en rekenkracht, met uitzonderlijke mogelijkheden in:
- **Wiskundig redeneren en complexe berekeningen**
- **Codegeneratie, debugging en analyse**
- **Logische probleemoplossing en stapsgewijs redeneren**
- **Educatieve toepassingen die gedetailleerde uitleg vereisen**
- **Functie-aanroepen en toolintegratie**

Als onderdeel van de categorie "Small SLMs" (1,5 miljard - 13,9 miljard parameters) biedt Phi-4-mini een optimale balans tussen redeneercapaciteit en resource-efficiëntie.

### Voordelen van containerimplementatie van Phi-4-mini

- **Operationele efficiëntie**: Snelle inferentie voor redeneertaken met lagere rekenvereisten
- **Implementatieflexibiliteit**: AI-mogelijkheden op het apparaat met verbeterde privacy door lokale verwerking
- **Kostenbesparing**: Lagere operationele kosten in vergelijking met grotere modellen, terwijl de kwaliteit behouden blijft
- **Isolatie**: Schone scheiding tussen modelinstanties en veilige uitvoeringsomgevingen
- **Schaalbaarheid**: Eenvoudige horizontale schaalvergroting voor verhoogde redeneerdoorvoer

## Leerdoelen

Aan het einde van deze tutorial kun je:

- Phi-4-mini-instruct implementeren en optimaliseren in verschillende containeromgevingen
- Geavanceerde kwantisatie- en compressiestrategieën toepassen voor verschillende implementatiescenario's
- Productieklaar containerorkestratie configureren voor redeneerworkloads
- Geschikte implementatiekaders evalueren en selecteren op basis van specifieke gebruiksvereisten
- Beveiligings-, monitoring- en schaalstrategieën toepassen voor containerimplementaties van SLMs

## Begrip van Phi-4-mini Classificatie

### Modelspecificaties

**Technische details:**
- **Parameters**: 3,8 miljard (Small SLM categorie)
- **Architectuur**: Dense decoder-only Transformer met grouped-query attention
- **Contextlengte**: 128K tokens (32K aanbevolen voor optimale prestaties)
- **Vocabulaire**: 200K tokens met meertalige ondersteuning
- **Trainingsdata**: 5T tokens van hoogwaardige redeneerintensieve inhoud

### Resourcevereisten

| Implementatietype | Min RAM | Aanbevolen RAM | VRAM (GPU) | Opslag | Typische gebruiksscenario's |
|-------------------|---------|----------------|------------|-------|-----------------------------|
| **Ontwikkeling** | 6GB | 8GB | - | 8GB | Lokale tests, prototyping |
| **Productie CPU** | 8GB | 12GB | - | 10GB | Edge-servers, kosten-geoptimaliseerde implementatie |
| **Productie GPU** | 6GB | 8GB | 4-6GB | 8GB | Hoogdoorvoer redeneerdiensten |
| **Edge-geoptimaliseerd** | 4GB | 6GB | - | 6GB | Gekwantisatie implementatie, IoT-gateways |

### Phi-4-mini Capaciteiten

- **Wiskundige uitmuntendheid**: Geavanceerde rekenkunde, algebra en calculus probleemoplossing
- **Code-intelligentie**: Python, JavaScript en meertalige codegeneratie met debugging
- **Logisch redeneren**: Stapsgewijze probleemdecompositie en oplossingsconstructie
- **Educatieve ondersteuning**: Gedetailleerde uitleg geschikt voor leer- en onderwijsscenario's
- **Functie-aanroepen**: Natuurlijke ondersteuning voor toolintegratie en API-interacties

## vLLM Containerimplementatie

vLLM biedt uitstekende ondersteuning voor Phi-4-mini-instruct met geoptimaliseerde inferentieprestaties en OpenAI-compatibele API's, waardoor het ideaal is voor productie-redeneerdiensten.

### Snelle startvoorbeelden

#### Basis CPU-implementatie (Ontwikkeling)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-versnelde productie-implementatie
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Productieconfiguratie

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testen van Phi-4-mini redeneercapaciteiten

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama Containerimplementatie

Ollama biedt uitstekende ondersteuning voor Phi-4-mini-instruct met vereenvoudigde implementatie en beheer, waardoor het ideaal is voor ontwikkeling en gebalanceerde productie-implementaties.

### Snelle setup

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Productieconfiguratie

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Modeloptimalisatie en varianten

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API-gebruik voorbeelden

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine met ONNX Runtime

ONNX Runtime biedt optimale prestaties voor edge-implementatie van Phi-4-mini-instruct met geavanceerde optimalisatie en cross-platform compatibiliteit.

### Basis setup

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Vereenvoudigde serverimplementatie

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Modelconversiescript

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Productieconfiguratie

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testen van ONNX-implementatie

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Vergelijkingskader

### Vergelijking van kaders voor Phi-4-mini

| Kenmerk | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Setup-complexiteit** | Gemiddeld | Eenvoudig | Complex |
| **Prestaties (GPU)** | Uitstekend (~25 tok/s) | Zeer goed (~20 tok/s) | Goed (~15 tok/s) |
| **Prestaties (CPU)** | Goed (~8 tok/s) | Zeer goed (~12 tok/s) | Uitstekend (~15 tok/s) |
| **Geheugengebruik** | 8-12GB | 6-10GB | 4-8GB |
| **API-compatibiliteit** | OpenAI-compatibel | Aangepaste REST | Aangepaste FastAPI |
| **Functie-aanroepen** | ✅ Native | ✅ Ondersteund | ⚠️ Aangepaste implementatie |
| **Kwantisatie-ondersteuning** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Kwantisatie |
| **Productieklaar** | ✅ Uitstekend | ✅ Zeer goed | ✅ Goed |
| **Edge-implementatie** | Goed | Uitstekend | Uitzonderlijk |

## Aanvullende bronnen

### Officiële documentatie
- **Microsoft Phi-4 Modelkaart**: Gedetailleerde specificaties en gebruiksrichtlijnen
- **vLLM Documentatie**: Geavanceerde configuratie- en optimalisatieopties
- **Ollama Modelbibliotheek**: Communitymodellen en aanpassing voorbeelden
- **ONNX Runtime Gidsen**: Prestatieoptimalisatie en implementatiestrategieën

### Ontwikkeltools
- **Hugging Face Transformers**: Voor modelinteractie en aanpassing
- **OpenAI API-specificatie**: Voor vLLM-compatibiliteitstests
- **Docker Best Practices**: Containerbeveiliging en optimalisatierichtlijnen
- **Kubernetes Implementatie**: Orkestratiepatronen voor productieschaalvergroting

### Leerbronnen
- **SLM Prestatiebenchmarking**: Vergelijkende analysemethodologieën
- **Edge AI Implementatie**: Best practices voor omgevingen met beperkte middelen
- **Optimalisatie van redeneertaken**: Prompting-strategieën voor wiskundige en logische problemen
- **Containerbeveiliging**: Verhardingspraktijken voor AI-modelimplementaties

## Leerresultaten

Na het voltooien van deze module kun je:

1. Het Phi-4-mini-instruct model implementeren in containeromgevingen met meerdere kaders
2. SLM-implementaties configureren en optimaliseren voor verschillende hardwareomgevingen
3. Beveiligingsbest practices implementeren voor container-AI-implementaties
4. Geschikte implementatiekaders vergelijken en selecteren op basis van specifieke gebruiksvereisten
5. Monitoring- en schaalstrategieën toepassen voor productieklare SLM-diensten

## Wat is de volgende stap

- Ga terug naar [Module 1](../Module01/README.md)
- Ga terug naar [Module 2](../Module02/README.md)

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we ons best doen om nauwkeurigheid te garanderen, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.