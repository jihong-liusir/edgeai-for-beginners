<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T12:53:59+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "nl"
}
-->
# Sectie 2: Llama.cpp Implementatiehandleiding

## Inhoudsopgave
1. [Introductie](../../../Module04)
2. [Wat is Llama.cpp?](../../../Module04)
3. [Installatie](../../../Module04)
4. [Bouwen vanuit broncode](../../../Module04)
5. [Modelkwantisatie](../../../Module04)
6. [Basisgebruik](../../../Module04)
7. [Geavanceerde functies](../../../Module04)
8. [Python-integratie](../../../Module04)
9. [Probleemoplossing](../../../Module04)
10. [Best practices](../../../Module04)

## Introductie

Deze uitgebreide handleiding leidt je door alles wat je moet weten over Llama.cpp, van basisinstallatie tot geavanceerde gebruiksscenario's. Llama.cpp is een krachtige C++-implementatie die efficiënte inferentie van Large Language Models (LLM's) mogelijk maakt met minimale setup en uitstekende prestaties op verschillende hardwareconfiguraties.

## Wat is Llama.cpp?

Llama.cpp is een LLM-inferentiekader geschreven in C/C++ waarmee grote taalmodellen lokaal kunnen worden uitgevoerd met minimale setup en state-of-the-art prestaties op een breed scala aan hardware. Belangrijke kenmerken zijn:

### Kernfuncties
- **Eenvoudige C/C++-implementatie** zonder afhankelijkheden
- **Compatibiliteit op meerdere platforms** (Windows, macOS, Linux)
- **Hardware-optimalisatie** voor verschillende architecturen
- **Ondersteuning voor kwantisatie** (1,5-bit tot 8-bit integer kwantisatie)
- **CPU- en GPU-versnelling** ondersteuning
- **Geheugenefficiëntie** voor beperkte omgevingen

### Voordelen
- Werkt efficiënt op CPU zonder gespecialiseerde hardware
- Ondersteunt meerdere GPU-backends (CUDA, Metal, OpenCL, Vulkan)
- Lichtgewicht en draagbaar
- Apple Silicon is een eersteklas platform - geoptimaliseerd via ARM NEON, Accelerate en Metal-frameworks
- Ondersteunt verschillende kwantisatieniveaus voor verminderd geheugengebruik

## Installatie

### Methode 1: Vooraf gebouwde binaries (Aanbevolen voor beginners)

#### Download van GitHub Releases
1. Bezoek de [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Download de juiste binary voor jouw systeem:
   - `llama-<versie>-bin-win-<functie>-<arch>.zip` voor Windows
   - `llama-<versie>-bin-macos-<functie>-<arch>.zip` voor macOS
   - `llama-<versie>-bin-linux-<functie>-<arch>.zip` voor Linux

3. Pak het archief uit en voeg de map toe aan het PATH van je systeem.

#### Gebruik van pakketbeheerders

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Verschillende distributies):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Methode 2: Python-pakket (llama-cpp-python)

#### Basisinstallatie
```bash
pip install llama-cpp-python
```

#### Met hardwareversnelling
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Bouwen vanuit broncode

### Vereisten

**Systeemeisen:**
- C++-compiler (GCC, Clang of MSVC)
- CMake (versie 3.14 of hoger)
- Git
- Buildtools voor jouw platform

**Installeren van vereisten:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Installeer Visual Studio 2022 met C++-ontwikkeltools
- Installeer CMake van de officiële website
- Installeer Git

### Basisbouwproces

1. **Kloon de repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Configureer de build:**
```bash
cmake -B build
```

3. **Bouw het project:**
```bash
cmake --build build --config Release
```

Voor snellere compilatie, gebruik parallelle taken:
```bash
cmake --build build --config Release -j 8
```

### Hardware-specifieke builds

#### CUDA-ondersteuning (NVIDIA GPU's)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal-ondersteuning (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS-ondersteuning (CPU-optimalisatie)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan-ondersteuning
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Geavanceerde bouwopties

#### Debug-build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Met extra functies
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Modelkwantisatie

### Begrijpen van het GGUF-formaat

GGUF (Generalized GGML Unified Format) is een geoptimaliseerd bestandsformaat dat is ontworpen voor het efficiënt uitvoeren van grote taalmodellen met Llama.cpp en andere frameworks. Het biedt:

- Gestandaardiseerde opslag van modelgewichten
- Verbeterde compatibiliteit tussen platforms
- Verbeterde prestaties
- Efficiënt metadatabeheer

### Kwantisatietypen

Llama.cpp ondersteunt verschillende kwantisatieniveaus:

| Type | Bits | Beschrijving | Gebruiksscenario |
|------|------|--------------|------------------|
| F16 | 16 | Half precisie | Hoge kwaliteit, groot geheugen |
| Q8_0 | 8 | 8-bit kwantisatie | Goede balans |
| Q4_0 | 4 | 4-bit kwantisatie | Gemiddelde kwaliteit, kleinere grootte |
| Q2_K | 2 | 2-bit kwantisatie | Kleinste grootte, lagere kwaliteit |

### Modellen converteren

#### Van PyTorch naar GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Direct downloaden van Hugging Face
Veel modellen zijn beschikbaar in GGUF-formaat op Hugging Face:
- Zoek naar modellen met "GGUF" in de naam
- Download het gewenste kwantisatieniveau
- Gebruik direct met Llama.cpp

## Basisgebruik

### Command-line interface

#### Eenvoudige tekstgeneratie
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Modellen gebruiken van Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Servermodus
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Veelgebruikte parameters

| Parameter | Beschrijving | Voorbeeld |
|-----------|--------------|-----------|
| `-m` | Pad naar modelbestand | `-m model.gguf` |
| `-p` | Prompttekst | `-p "Hallo wereld"` |
| `-n` | Aantal te genereren tokens | `-n 100` |
| `-c` | Contextgrootte | `-c 4096` |
| `-t` | Aantal threads | `-t 8` |
| `-ngl` | GPU-lagen | `-ngl 32` |
| `-temp` | Temperatuur | `-temp 0.7` |

### Interactieve modus

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Geavanceerde functies

### Server-API

#### Server starten
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API-gebruik
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Prestatie-optimalisatie

#### Geheugenbeheer
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multithreading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU-versnelling
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python-integratie

### Basisgebruik met llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chatinterface

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streaming-antwoorden

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integratie met LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Probleemoplossing

### Veelvoorkomende problemen en oplossingen

#### Buildfouten

**Probleem: CMake niet gevonden**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Probleem: Compiler niet gevonden**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Runtime-problemen

**Probleem: Laden van model mislukt**
- Controleer het pad naar het modelbestand
- Controleer bestandsrechten
- Zorg voor voldoende RAM
- Probeer andere kwantisatieniveaus

**Probleem: Slechte prestaties**
- Schakel hardwareversnelling in
- Verhoog het aantal threads
- Gebruik een geschikt kwantisatieniveau
- Controleer GPU-geheugengebruik

#### Geheugenproblemen

**Probleem: Onvoldoende geheugen**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platformspecifieke problemen

#### Windows
- Gebruik MinGW of Visual Studio-compiler
- Zorg voor een correcte PATH-configuratie
- Controleer op interferentie van antivirussoftware

#### macOS
- Schakel Metal in voor Apple Silicon
- Gebruik Rosetta 2 voor compatibiliteit indien nodig
- Controleer Xcode command-line tools

#### Linux
- Installeer ontwikkelpackages
- Controleer GPU-stuurprogrammaversies
- Verifieer installatie van CUDA-toolkit

## Best practices

### Modelselectie
1. **Kies een geschikt kwantisatieniveau** op basis van je hardware
2. **Overweeg de balans tussen modelgrootte** en kwaliteit
3. **Test verschillende modellen** voor jouw specifieke toepassing

### Prestatie-optimalisatie
1. **Gebruik GPU-versnelling** indien beschikbaar
2. **Optimaliseer het aantal threads** voor je CPU
3. **Stel een geschikte contextgrootte in** voor jouw toepassing
4. **Schakel geheugenmapping in** voor grote modellen

### Productiedeployment
1. **Gebruik servermodus** voor API-toegang
2. **Implementeer foutafhandeling**
3. **Monitor resourcegebruik**
4. **Stel logging en monitoring in**

### Ontwikkelworkflow
1. **Begin met kleinere modellen** voor tests
2. **Gebruik versiebeheer** voor modelconfiguraties
3. **Documenteer je configuraties**
4. **Test op verschillende platforms**

### Beveiligingsoverwegingen
1. **Valideer invoerprompts**
2. **Implementeer rate limiting**
3. **Beveilig API-eindpunten**
4. **Monitor misbruikpatronen**

## Conclusie

Llama.cpp biedt een krachtige en efficiënte manier om grote taalmodellen lokaal uit te voeren op verschillende hardwareconfiguraties. Of je nu AI-toepassingen ontwikkelt, onderzoek doet of experimenteert met LLM's, dit framework biedt de flexibiliteit en prestaties die nodig zijn voor een breed scala aan toepassingen.

Belangrijke punten:
- Kies de installatiemethode die het beste bij je past
- Optimaliseer voor jouw specifieke hardwareconfiguratie
- Begin met basisgebruik en verken geleidelijk geavanceerde functies
- Overweeg het gebruik van de Python-bindings voor eenvoudigere integratie
- Volg best practices voor productie-implementaties

Voor meer informatie en updates, bezoek de [officiële Llama.cpp-repository](https://github.com/ggml-org/llama.cpp) en raadpleeg de uitgebreide documentatie en beschikbare communitybronnen.

## ➡️ Wat nu?

- [03: Microsoft Olive Optimalisatie Suite](./03.MicrosoftOlive.md)

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.