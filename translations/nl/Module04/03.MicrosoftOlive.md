<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T21:33:04+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "nl"
}
-->
# Sectie 3: Microsoft Olive Optimalisatie Suite

## Inhoudsopgave
1. [Introductie](../../../Module04)
2. [Wat is Microsoft Olive?](../../../Module04)
3. [Installatie](../../../Module04)
4. [Snelstartgids](../../../Module04)
5. [Voorbeeld: Qwen3 converteren naar ONNX INT4](../../../Module04)
6. [Geavanceerd gebruik](../../../Module04)
7. [Best practices](../../../Module04)
8. [Problemen oplossen](../../../Module04)
9. [Aanvullende bronnen](../../../Module04)

## Introductie

Microsoft Olive is een krachtige, gebruiksvriendelijke hardware-bewuste toolkit voor modeloptimalisatie die het proces vereenvoudigt om machine learning-modellen te optimaliseren voor implementatie op verschillende hardwareplatforms. Of je nu richt op CPU's, GPU's of gespecialiseerde AI-versnellers, Olive helpt je om optimale prestaties te bereiken terwijl de nauwkeurigheid van het model behouden blijft.

## Wat is Microsoft Olive?

Olive is een gebruiksvriendelijke hardware-bewuste tool voor modeloptimalisatie die toonaangevende technieken combineert op het gebied van modelcompressie, optimalisatie en compilatie. Het werkt samen met ONNX Runtime als een end-to-end oplossing voor inferentieoptimalisatie.

### Belangrijkste kenmerken

- **Hardware-bewuste optimalisatie**: Selecteert automatisch de beste optimalisatietechnieken voor jouw doelhardware
- **40+ ingebouwde optimalisatiecomponenten**: Omvat modelcompressie, kwantisatie, grafiekoptimalisatie en meer
- **Eenvoudige CLI-interface**: Simpele commando's voor veelvoorkomende optimalisatietaken
- **Ondersteuning voor meerdere frameworks**: Werkt met PyTorch, Hugging Face-modellen en ONNX
- **Ondersteuning voor populaire modellen**: Olive kan populaire modelarchitecturen zoals Llama, Phi, Qwen, Gemma, enz. automatisch optimaliseren

### Voordelen

- **Kortere ontwikkeltijd**: Geen noodzaak om handmatig te experimenteren met verschillende optimalisatietechnieken
- **Prestatieverbeteringen**: Significante snelheidsverbeteringen (tot 6x in sommige gevallen)
- **Cross-platform implementatie**: Geoptimaliseerde modellen werken op verschillende hardware en besturingssystemen
- **Behoud van nauwkeurigheid**: Optimalisaties behouden de kwaliteit van het model terwijl de prestaties worden verbeterd

## Installatie

### Vereisten

- Python 3.8 of hoger
- pip package manager
- Virtuele omgeving (aanbevolen)

### Basisinstallatie

Maak en activeer een virtuele omgeving:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
Installeer Olive met automatische optimalisatiefuncties:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  

### Optionele afhankelijkheden

Olive biedt verschillende optionele afhankelijkheden voor extra functies:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  

### Installatie verifiëren

```bash
olive --help
```
  
Als de installatie succesvol is, zou je het Olive CLI-helpbericht moeten zien.

## Snelstartgids

### Je eerste optimalisatie

Laten we een klein taalmodel optimaliseren met Olive's automatische optimalisatiefunctie:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### Wat doet dit commando?

Het optimalisatieproces omvat: het ophalen van het model uit de lokale cache, het vastleggen van de ONNX-grafiek en het opslaan van de gewichten in een ONNX-databestand, het optimaliseren van de ONNX-grafiek en het kwantiseren van het model naar int4 met behulp van de RTN-methode.

### Uitleg van de commandoparameters

- `--model_name_or_path`: Hugging Face-modelidentificator of lokaal pad
- `--output_path`: Map waar het geoptimaliseerde model wordt opgeslagen
- `--device`: Doelapparaat (cpu, gpu)
- `--provider`: Uitvoeringsprovider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gebruik ONNX Runtime Generate AI voor inferentie
- `--precision`: Kwantisatienauwkeurigheid (int4, int8, fp16)
- `--log_level`: Logniveau (0=minimaal, 1=uitgebreid)

## Voorbeeld: Qwen3 converteren naar ONNX INT4

Gebaseerd op het gegeven Hugging Face-voorbeeld op [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), hier is hoe je een Qwen3-model kunt optimaliseren:

### Stap 1: Model downloaden (optioneel)

Om downloadtijd te minimaliseren, cache alleen essentiële bestanden:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  

### Stap 2: Qwen3-model optimaliseren

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### Stap 3: Het geoptimaliseerde model testen

Maak een eenvoudige Python-script om je geoptimaliseerde model te testen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  

### Outputstructuur

Na optimalisatie bevat je uitvoermap:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## Geavanceerd gebruik

### Configuratiebestanden

Voor complexere optimalisatieworkflows kun je JSON-configuratiebestanden gebruiken:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
Uitvoeren met configuratie:

```bash
olive run --config config.json
```
  

### GPU-optimalisatie

Voor CUDA GPU-optimalisatie:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
Voor DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### Fijn-afstemming met Olive

Olive ondersteunt ook het fijn-afstemmen van modellen:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## Best practices

### 1. Modelselectie
- Begin met kleinere modellen voor testen (bijv. 0.5B-7B parameters)
- Zorg ervoor dat je doelmodelarchitectuur wordt ondersteund door Olive

### 2. Hardwareoverwegingen
- Stem je optimalisatiedoel af op je implementatiehardware
- Gebruik GPU-optimalisatie als je CUDA-compatibele hardware hebt
- Overweeg DirectML voor Windows-machines met geïntegreerde grafische kaarten

### 3. Nauwkeurigheidsselectie
- **INT4**: Maximale compressie, lichte nauwkeurigheidsverlies
- **INT8**: Goede balans tussen grootte en nauwkeurigheid
- **FP16**: Minimale nauwkeurigheidsverlies, matige groottevermindering

### 4. Testen en valideren
- Test geoptimaliseerde modellen altijd met je specifieke use cases
- Vergelijk prestatiewaarden (latentie, doorvoer, nauwkeurigheid)
- Gebruik representatieve invoergegevens voor evaluatie

### 5. Iteratieve optimalisatie
- Begin met automatische optimalisatie voor snelle resultaten
- Gebruik configuratiebestanden voor gedetailleerde controle
- Experimenteer met verschillende optimalisatiepasses

## Problemen oplossen

### Veelvoorkomende problemen

#### 1. Installatieproblemen
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  

#### 2. CUDA/GPU-problemen
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  

#### 3. Geheugenproblemen
- Gebruik kleinere batchgroottes tijdens optimalisatie
- Probeer kwantisatie met hogere nauwkeurigheid eerst (int8 in plaats van int4)
- Zorg voor voldoende schijfruimte voor modelcaching

#### 4. Model laadfouten
- Controleer modelpad en toegangsrechten
- Controleer of het model `trust_remote_code=True` vereist
- Zorg ervoor dat alle benodigde modelbestanden zijn gedownload

### Hulp krijgen

- **Documentatie**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Voorbeelden**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Aanvullende bronnen

### Officiële links
- **GitHub-repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime-documentatie**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-voorbeeld**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Communityvoorbeelden
- **Jupyter Notebooks**: Beschikbaar in de Olive GitHub-repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code-extensie**: AI Toolkit voor VS Code overzicht — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogposts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Gerelateerde tools
- **ONNX Runtime**: High-performance inferentie-engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Bron van veel compatibele modellen — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-gebaseerde optimalisatieworkflows — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Wat nu?

- [04: OpenVINO Toolkit Optimalisatie Suite](./04.openvino.md)

---

