<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T12:45:25+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "nl"
}
-->
# Sectie 4: OpenVINO Toolkit Optimalisatie Suite

## Inhoudsopgave
1. [Introductie](../../../Module04)
2. [Wat is OpenVINO?](../../../Module04)
3. [Installatie](../../../Module04)
4. [Snelstartgids](../../../Module04)
5. [Voorbeeld: Modellen converteren en optimaliseren met OpenVINO](../../../Module04)
6. [Geavanceerd gebruik](../../../Module04)
7. [Best practices](../../../Module04)
8. [Probleemoplossing](../../../Module04)
9. [Aanvullende bronnen](../../../Module04)

## Introductie

OpenVINO (Open Visual Inference and Neural Network Optimization) is Intel's open-source toolkit voor het implementeren van krachtige AI-oplossingen in de cloud, on-premises en edge-omgevingen. Of je nu werkt met CPU's, GPU's, VPU's of gespecialiseerde AI-versnellers, OpenVINO biedt uitgebreide optimalisatiemogelijkheden terwijl de modelnauwkeurigheid behouden blijft en cross-platform implementatie mogelijk wordt gemaakt.

## Wat is OpenVINO?

OpenVINO is een open-source toolkit waarmee ontwikkelaars AI-modellen efficiënt kunnen optimaliseren, converteren en implementeren op diverse hardwareplatforms. Het bestaat uit drie hoofdcomponenten: OpenVINO Runtime voor inferentie, Neural Network Compression Framework (NNCF) voor modeloptimalisatie en OpenVINO Model Server voor schaalbare implementatie.

### Belangrijkste kenmerken

- **Cross-platform implementatie**: Ondersteunt Linux, Windows en macOS met Python-, C++- en C-API's
- **Hardwareversnelling**: Automatische apparaatdetectie en optimalisatie voor CPU, GPU, VPU en AI-versnellers
- **Modelcompressiekader**: Geavanceerde technieken voor kwantisatie, pruning en optimalisatie via NNCF
- **Compatibiliteit met frameworks**: Directe ondersteuning voor TensorFlow-, ONNX-, PaddlePaddle- en PyTorch-modellen
- **Generatieve AI-ondersteuning**: Gespecialiseerde OpenVINO GenAI voor het implementeren van grote taalmodellen en generatieve AI-toepassingen

### Voordelen

- **Prestatieoptimalisatie**: Significante snelheidsverbeteringen met minimale nauwkeurigheidsverlies
- **Verminderde implementatievoetafdruk**: Minimale externe afhankelijkheden vereenvoudigen installatie en implementatie
- **Verbeterde opstarttijd**: Geoptimaliseerde modellading en caching voor snellere applicatie-initialisatie
- **Schaalbare implementatie**: Van edge-apparaten tot cloudinfrastructuur met consistente API's
- **Productieklaar**: Betrouwbaarheid op ondernemingsniveau met uitgebreide documentatie en communityondersteuning

## Installatie

### Vereisten

- Python 3.8 of hoger
- pip package manager
- Virtuele omgeving (aanbevolen)
- Compatibele hardware (Intel CPU's aanbevolen, maar ondersteunt verschillende architecturen)

### Basisinstallatie

Maak en activeer een virtuele omgeving:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installeer OpenVINO Runtime:

```bash
pip install openvino
```

Installeer NNCF voor modeloptimalisatie:

```bash
pip install nncf
```

### OpenVINO GenAI-installatie

Voor generatieve AI-toepassingen:

```bash
pip install openvino-genai
```

### Optionele afhankelijkheden

Extra pakketten voor specifieke gebruiksscenario's:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Installatie verifiëren

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Als de installatie succesvol is, zou je de OpenVINO-versie-informatie moeten zien.

## Snelstartgids

### Je eerste modeloptimalisatie

Laten we een Hugging Face-model converteren en optimaliseren met OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Wat doet dit proces?

De optimalisatieworkflow omvat: het laden van het originele model van Hugging Face, converteren naar OpenVINO Intermediate Representation (IR)-formaat, toepassen van standaardoptimalisaties en compileren voor doelhardware.

### Uitleg van belangrijke parameters

- `export=True`: Converteert het model naar OpenVINO IR-formaat
- `compile=False`: Stelt compilatie uit tot runtime voor flexibiliteit
- `device`: Doelhardware ("CPU", "GPU", "AUTO" voor automatische selectie)
- `save_pretrained()`: Slaat het geoptimaliseerde model op voor hergebruik

## Voorbeeld: Modellen converteren en optimaliseren met OpenVINO

### Stap 1: Modelconversie met NNCF-kwantisatie

Zo pas je post-training kwantisatie toe met NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Stap 2: Geavanceerde optimalisatie met gewichtscompressie

Voor transformer-gebaseerde modellen, pas gewichtscompressie toe:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Stap 3: Inferentie met geoptimaliseerd model

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Outputstructuur

Na optimalisatie bevat je modelmap:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Geavanceerd gebruik

### Configuratie met NNCF YAML

Voor complexe optimalisatieworkflows gebruik je NNCF-configuratiebestanden:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Pas configuratie toe:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-optimalisatie

Voor GPU-versnelling:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Batchverwerkingsoptimalisatie

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modelserverimplementatie

Implementeer geoptimaliseerde modellen met OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Clientcode voor modelserver:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Best practices

### 1. Modelselectie en voorbereiding
- Gebruik modellen van ondersteunde frameworks (PyTorch, TensorFlow, ONNX)
- Zorg ervoor dat modelinputs vaste of bekende dynamische vormen hebben
- Test met representatieve datasets voor kalibratie

### 2. Optimalisatiestrategie kiezen
- **Post-training kwantisatie**: Begin hier voor snelle optimalisatie
- **Gewichtscompressie**: Ideaal voor grote taalmodellen en transformers
- **Kwantisatie-bewuste training**: Gebruik dit wanneer nauwkeurigheid cruciaal is

### 3. Hardware-specifieke optimalisatie
- **CPU**: Gebruik INT8-kwantisatie voor gebalanceerde prestaties
- **GPU**: Maak gebruik van FP16-precisie en batchverwerking
- **VPU**: Focus op modelsimplificatie en laagfusie

### 4. Prestatieafstemming
- **Throughput-modus**: Voor batchverwerking met hoog volume
- **Latency-modus**: Voor interactieve toepassingen in real-time
- **AUTO-apparaat**: Laat OpenVINO de optimale hardware selecteren

### 5. Geheugenbeheer
- Gebruik dynamische vormen spaarzaam om geheugenoverhead te vermijden
- Implementeer modelcaching voor snellere herladingen
- Monitor geheugengebruik tijdens optimalisatie

### 6. Nauwkeurigheidsvalidatie
- Valideer geoptimaliseerde modellen altijd tegen originele prestaties
- Gebruik representatieve testdatasets voor evaluatie
- Overweeg geleidelijke optimalisatie (begin met conservatieve instellingen)

## Probleemoplossing

### Veelvoorkomende problemen

#### 1. Installatieproblemen
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modelconversiefouten
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Prestatieproblemen
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Geheugenproblemen
- Verminder de batchgrootte van het model tijdens optimalisatie
- Gebruik streaming voor grote datasets
- Schakel modelcaching in: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Nauwkeurigheidsverlies
- Gebruik hogere precisie (INT8 in plaats van INT4)
- Vergroot de kalibratiedataset
- Pas gemengde precisie-optimalisatie toe

### Prestatiemonitoring

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Hulp krijgen

- **Documentatie**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community Forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Aanvullende bronnen

### Officiële links
- **OpenVINO Homepage**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Leerbronnen
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Snelstartgids**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimalisatiegids**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integratietools
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Prestatiebenchmarks
- **Officiële benchmarks**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Communityvoorbeelden
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Uitgebreide tutorials beschikbaar in de OpenVINO-notebooksrepository
- **Voorbeeldtoepassingen**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Praktijkvoorbeelden voor verschillende domeinen (computer vision, NLP, audio)
- **Blogposts**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI en communityblogposts met gedetailleerde use cases

### Gerelateerde tools
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Aanvullende optimalisatietechnieken voor Intel-hardware
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Voor mobiele en edge-implementatievergelijkingen
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternatieven voor cross-platform inferentie-engines

## ➡️ Wat is de volgende stap

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.