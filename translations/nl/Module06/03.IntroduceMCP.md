<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T12:31:21+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "nl"
}
-->
# Sectie 03 - Model Context Protocol (MCP) Integratie

## Introductie tot MCP (Model Context Protocol)

Het Model Context Protocol (MCP) is een revolutionair framework dat taalmodellen in staat stelt om op een gestandaardiseerde manier te communiceren met externe tools en systemen. In tegenstelling tot traditionele benaderingen waarbij modellen geïsoleerd zijn, creëert MCP een brug tussen AI-modellen en de echte wereld via een goed gedefinieerd protocol.

### Wat is MCP?

MCP fungeert als een communicatieprotocol waarmee taalmodellen kunnen:
- Verbinden met externe databronnen
- Tools en functies uitvoeren
- Interageren met API's en diensten
- Toegang krijgen tot realtime informatie
- Complexe meerstapsoperaties uitvoeren

Dit protocol transformeert statische taalmodellen in dynamische agenten die praktische taken kunnen uitvoeren, verder dan alleen tekstgeneratie.

## Kleine Taalmodellen (SLMs) in MCP

Kleine Taalmodellen bieden een efficiënte benadering van AI-implementatie en hebben verschillende voordelen:

### Voordelen van SLMs
- **Efficiënt gebruik van middelen**: Lagere computationale vereisten
- **Snellere reactietijden**: Verminderde latentie voor realtime toepassingen  
- **Kostenbesparing**: Minimale infrastructuurbehoeften
- **Privacy**: Kan lokaal draaien zonder datatransmissie
- **Aanpasbaarheid**: Gemakkelijker af te stemmen op specifieke domeinen

### Waarom SLMs goed werken met MCP

SLMs in combinatie met MCP vormen een krachtige combinatie waarbij de redeneercapaciteiten van het model worden versterkt door externe tools, waardoor hun kleinere aantal parameters wordt gecompenseerd met verbeterde functionaliteit.

## Python MCP SDK Overzicht

De Python MCP SDK biedt de basis voor het bouwen van MCP-compatibele applicaties. De SDK bevat:

- **Clientbibliotheken**: Voor verbinding met MCP-servers
- **Serverframework**: Voor het maken van aangepaste MCP-servers
- **Protocolhandlers**: Voor het beheren van communicatie
- **Toolintegratie**: Voor het uitvoeren van externe functies

## Praktische Implementatie: Phi-4 MCP Client

Laten we een praktijkvoorbeeld bekijken met Microsoft's Phi-4 mini-model geïntegreerd met MCP-functionaliteiten.

### Systeemarchitectuur

De implementatie volgt een gelaagde architectuur:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Kerncomponenten

#### 1. MCP Client Klassen

**BaseMCPClient**: Abstracte basis die gemeenschappelijke functionaliteit biedt
- Async contextmanagerprotocol
- Standaard interface definitie
- Resourcebeheer

**Phi4MiniMCPClient**: STDIO-gebaseerde implementatie
- Lokale procescommunicatie
- Standaard input/output verwerking
- Subprocessbeheer

**Phi4MiniSSEMCPClient**: Server-Sent Events implementatie
- HTTP-streamingcommunicatie
- Realtime eventverwerking
- Webgebaseerde serverconnectiviteit

#### 2. LLM Integratie

**OllamaClient**: Lokale modelhosting  
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Hoogpresterende server  
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Toolverwerkingspipeline

De toolverwerkingspipeline transformeert MCP-tools naar formaten die compatibel zijn met taalmodellen:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Aan de slag: Stapsgewijze handleiding

### Stap 1: Omgevingsinstellingen

Installeer vereiste afhankelijkheden:  
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Stap 2: Basisconfiguratie

Stel je omgevingsvariabelen in:  
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Stap 3: Je eerste MCP Client uitvoeren

**Basis Ollama Setup:**  
```bash
python ghmodel_mcp_demo.py
```

**Gebruik van vLLM Backend:**  
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events Verbinding:**  
```bash
python ghmodel_mcp_demo.py --run sse
```

**Aangepaste MCP Server:**  
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Stap 4: Programmeerbaar gebruik

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Geavanceerde functies

### Ondersteuning voor meerdere backends

De implementatie ondersteunt zowel Ollama als vLLM backends, zodat je kunt kiezen op basis van je behoeften:

- **Ollama**: Geschikt voor lokale ontwikkeling en testen
- **vLLM**: Geoptimaliseerd voor productie en toepassingen met hoge doorvoer

### Flexibele verbindingsprotocollen

Er worden twee verbindingsmodi ondersteund:

**STDIO Mode**: Directe procescommunicatie
- Lagere latentie
- Geschikt voor lokale tools
- Eenvoudige setup

**SSE Mode**: HTTP-gebaseerde streaming
- Netwerkgeschikt
- Beter voor gedistribueerde systemen
- Realtime updates

### Toolintegratiemogelijkheden

Het systeem kan integreren met verschillende tools:
- Webautomatisering (Playwright)
- Bestandsbewerkingen
- API-interacties
- Systeemcommando's
- Aangepaste functies

## Foutafhandeling en beste praktijken

### Uitgebreid foutbeheer

De implementatie bevat robuuste foutafhandeling voor:

**Verbindingsfouten:**
- MCP serverstoringen
- Netwerktime-outs
- Connectiviteitsproblemen

**Tooluitvoeringsfouten:**
- Ontbrekende tools
- Parametervalidatie
- Uitvoeringsstoringen

**Responsverwerkingsfouten:**
- JSON-parsingproblemen
- Formaatinconsistenties
- LLM-responsafwijkingen

### Beste praktijken

1. **Resourcebeheer**: Gebruik async contextmanagers
2. **Foutafhandeling**: Implementeer uitgebreide try-catch blokken
3. **Logging**: Schakel passende logniveaus in
4. **Beveiliging**: Valideer invoer en zuiver uitvoer
5. **Prestaties**: Gebruik connectiepooling en caching

## Toepassingen in de praktijk

### Webautomatisering  
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Gegevensverwerking  
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API-integratie  
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Prestatieoptimalisatie

### Geheugenbeheer
- Efficiënt beheer van berichtgeschiedenis
- Correct opruimen van resources
- Connectiepooling

### Netwerkoptimalisatie
- Async HTTP-operaties
- Configureerbare time-outs
- Gracieus herstel van fouten

### Gelijktijdige verwerking
- Niet-blokkerende I/O
- Parallelle tooluitvoering
- Efficiënte async patronen

## Beveiligingsoverwegingen

### Gegevensbescherming
- Beveiligd beheer van API-sleutels
- Validatie van invoer
- Zuivering van uitvoer

### Netwerkbeveiliging
- Ondersteuning voor HTTPS
- Standaard lokale eindpunten
- Beveiligd tokenbeheer

### Veilige uitvoering
- Toolfiltering
- Gesandboxte omgevingen
- Auditlogging

## Conclusie

SLMs geïntegreerd met MCP vertegenwoordigen een paradigmaverschuiving in de ontwikkeling van AI-toepassingen. Door de efficiëntie van kleine modellen te combineren met de kracht van externe tools, kunnen ontwikkelaars intelligente systemen creëren die zowel middelenbesparend als zeer capabel zijn.

De Phi-4 MCP client-implementatie laat zien hoe deze integratie in de praktijk kan worden gerealiseerd en biedt een solide basis voor het bouwen van geavanceerde AI-aangedreven applicaties.

Belangrijke punten:
- MCP overbrugt de kloof tussen taalmodellen en externe systemen
- SLMs bieden efficiëntie zonder in te leveren op functionaliteit wanneer ze worden aangevuld met tools
- De modulaire architectuur maakt eenvoudige uitbreiding en aanpassing mogelijk
- Goede foutafhandeling en beveiligingsmaatregelen zijn essentieel voor productiegebruik

Deze tutorial biedt de basis voor het bouwen van je eigen SLM-aangedreven MCP-toepassingen, waarmee mogelijkheden worden geopend voor automatisering, gegevensverwerking en intelligente systeemintegratie.

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.