<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T13:05:00+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "nl"
}
-->
# Sectie 4: Implementatie van een Productieklaar Model

## Overzicht

Deze uitgebreide handleiding begeleidt je door het volledige proces van het implementeren van fijn-afgestelde gequantiseerde modellen met Foundry Local. We behandelen modelconversie, optimalisatie door quantisatie en configuratie van de implementatie van begin tot eind.

## Vereisten

Voordat je begint, zorg ervoor dat je het volgende hebt:

- ‚úÖ Een fijn-afgesteld onnx-model dat klaar is voor implementatie
- ‚úÖ Windows- of Mac-computer
- ‚úÖ Python 3.10 of hoger
- ‚úÖ Minimaal 8GB beschikbare RAM
- ‚úÖ Foundry Local ge√Ønstalleerd op je systeem

## Deel 1: Omgevingsinstellingen

### Vereiste Tools Installeren

Open je terminal (Command Prompt op Windows, Terminal op Mac) en voer de volgende commando's in volgorde uit:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Belangrijke Opmerking**: Je hebt ook CMake versie 3.31 of nieuwer nodig, die je kunt downloaden van [cmake.org](https://cmake.org/download/).

## Deel 2: Modelconversie en Quantisatie

### Het Juiste Formaat Kiezen

Voor fijn-afgestelde kleine taalmodellen raden we aan om **ONNX-formaat** te gebruiken, omdat het biedt:

- üöÄ Betere prestatieoptimalisatie
- üîß Hardware-onafhankelijke implementatie
- üè≠ Productieklaar vermogen
- üì± Cross-platform compatibiliteit

### Methode 1: E√©n-Commandoconversie (Aanbevolen)

Gebruik het volgende commando om je fijn-afgestelde model direct te converteren:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Uitleg van Parameters:**
- `--model_name_or_path`: Pad naar je fijn-afgestelde model
- `--device cpu`: Gebruik CPU voor optimalisatie
- `--precision int4`: Gebruik INT4 quantisatie (ongeveer 75% groottevermindering)
- `--output_path`: Uitvoermap voor het geconverteerde model

### Methode 2: Configuratiebestandbenadering (Voor Gevorderde Gebruikers)

Maak een configuratiebestand genaamd `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Voer vervolgens uit:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Vergelijking van Quantisatieopties

| Precisie | Bestandsgrootte | Inferencesnelheid | Modelkwaliteit | Aanbevolen Gebruik |
|----------|-----------------|-------------------|----------------|--------------------|
| FP16     | Baseline √ó 0.5 | Snel | Beste | High-end hardware |
| INT8     | Baseline √ó 0.25 | Zeer Snel | Goed | Gebalanceerde keuze |
| INT4     | Baseline √ó 0.125 | Snelst | Acceptabel | Beperkte middelen |

üí° **Aanbeveling**: Begin met INT4 quantisatie voor je eerste implementatie. Als de kwaliteit niet bevredigend is, probeer dan INT8 of FP16.

## Deel 3: Foundry Local Implementatieconfiguratie

### Modelconfiguratie Maken

Navigeer naar de Foundry Local modelmap:

```bash
foundry cache cd ./models/
```

Maak je modelmapstructuur:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Maak het configuratiebestand `inference_model.json` in je modelmap:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Templatespecifieke Configuraties

#### Voor Qwen Series Modellen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Deel 4: Modeltesten en Optimalisatie

### Verifi√´ren van Modelinstallatie

Controleer of Foundry Local je model kan herkennen:

```bash
foundry cache ls
```

Je zou `your-finetuned-model-int4` in de lijst moeten zien.

### Starten met Modeltesten

```bash
foundry model run your-finetuned-model-int4
```

### Prestatiebenchmarking

Monitor belangrijke statistieken tijdens het testen:

1. **Reactietijd**: Meet de gemiddelde tijd per reactie
2. **Geheugengebruik**: Houd RAM-verbruik in de gaten
3. **CPU-gebruik**: Controleer de processorbelasting
4. **Uitvoeringskwaliteit**: Evalueer de relevantie en samenhang van reacties

### Checklist voor Kwaliteitsvalidatie

- ‚úÖ Model reageert correct op fijn-afgestelde domeinvragen
- ‚úÖ Reactieformaat komt overeen met de verwachte outputstructuur
- ‚úÖ Geen geheugenlekken tijdens langdurig gebruik
- ‚úÖ Consistente prestaties bij verschillende invoerlengtes
- ‚úÖ Correcte afhandeling van randgevallen en ongeldige invoer

## Samenvatting

Gefeliciteerd! Je hebt succesvol voltooid:

- ‚úÖ Conversie van fijn-afgesteld modelformaat
- ‚úÖ Optimalisatie door quantisatie
- ‚úÖ Configuratie van Foundry Local implementatie
- ‚úÖ Prestatieafstemming en probleemoplossing

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.