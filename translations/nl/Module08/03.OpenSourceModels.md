<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:47:37+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "nl"
}
-->
# Sessie 3: Ontdekking en Beheer van Open-Source Modellen

## Overzicht

Deze sessie richt zich op praktische modelontdekking en -beheer met Foundry Local. Je leert hoe je beschikbare modellen kunt opsommen, verschillende opties kunt testen en basiskenmerken van prestaties kunt begrijpen. De aanpak benadrukt praktische verkenning met de Foundry CLI om je te helpen de juiste modellen voor jouw toepassingen te selecteren.

## Leerdoelen

- Beheers Foundry CLI-commando's voor modelontdekking en -beheer
- Begrijp modelcache en lokale opslagpatronen
- Leer snel verschillende modellen te testen en te vergelijken
- Stel praktische workflows op voor modelselectie en benchmarking
- Verken het groeiende ecosysteem van modellen beschikbaar via Foundry Local

## Vereisten

- Sessie 1: Aan de slag met Foundry Local voltooid
- Foundry Local CLI geïnstalleerd en toegankelijk
- Voldoende opslagruimte voor modeldownloads (modellen kunnen variëren van 1GB tot 20GB+)
- Basiskennis van modeltypes en toepassingen

## Overzicht

Deze sessie onderzoekt hoe je open-source modellen naar Foundry Local kunt brengen.

## Deel 6: Praktische Oefening

### Oefening: Modelontdekking en Vergelijking

Maak je eigen script voor modelevaluatie op basis van Voorbeeld 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### Jouw Taak

1. **Voer het script Voorbeeld 03 uit**: `samples\03\list_and_bench.cmd`
2. **Probeer verschillende modellen**: Test minstens 3 verschillende modellen
3. **Vergelijk prestaties**: Noteer verschillen in snelheid en responskwaliteit
4. **Documenteer bevindingen**: Maak een eenvoudige vergelijkingsgrafiek

### Voorbeeldformaat voor Vergelijking

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## Deel 7: Problemen oplossen en Beste Praktijken

### Veelvoorkomende Problemen en Oplossingen

**Model Start Niet:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**Onvoldoende Geheugen:**
- Begin met kleinere modellen (`phi-4-mini`)
- Sluit andere applicaties
- Upgrade RAM als je vaak tegen limieten aanloopt

**Trage Prestaties:**
- Zorg ervoor dat het model volledig is geladen (controleer gedetailleerde uitvoer)
- Sluit onnodige achtergrondapplicaties
- Overweeg snellere opslag (SSD)

### Beste Praktijken

1. **Begin Klein**: Start met `phi-4-mini` om de setup te valideren
2. **Eén Model Tegelijk**: Stop eerdere modellen voordat je nieuwe start
3. **Houd Resources in de Gaten**: Let op geheugengebruik
4. **Test Consistent**: Gebruik dezelfde prompts voor eerlijke vergelijkingen
5. **Documenteer Resultaten**: Houd notities bij over modelprestaties voor jouw toepassingen

## Deel 8: Volgende Stappen en Referenties

### Voorbereiding op Sessie 4

- **Focus van Sessie 4**: Optimalisatietools en -technieken
- **Vereisten**: Vertrouwd zijn met modelwisseling en basisprestatietests
- **Aanbevolen**: Identificeer 2-3 favoriete modellen uit deze sessie

### Aanvullende Bronnen

- **[Foundry Local Documentatie](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Officiële documentatie
- **[CLI Referentie](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Complete commandoreferentie
- **[Model Mondays](https://aka.ms/model-mondays)**: Wekelijkse modelspotlights
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Community en problemen
- **[Voorbeeld 03: Modelontdekking](samples/03/README.md)**: Praktisch voorbeeldscript

### Belangrijkste Inzichten

✅ **Modelontdekking**: Gebruik `foundry model list` om beschikbare modellen te verkennen  
✅ **Snel Testen**: Het `list_and_bench.cmd` patroon voor snelle evaluatie  
✅ **Prestatiemonitoring**: Basisgebruik van resources en responstijdmeting  
✅ **Modelselectie**: Praktische richtlijnen voor het kiezen van modellen per toepassing  
✅ **Cachebeheer**: Begrip van opslag- en opruimprocedures  

Je hebt nu de praktische vaardigheden om geschikte modellen te ontdekken, testen en selecteren voor jouw AI-toepassingen met Foundry Local's eenvoudige CLI-aanpak: communitymodellen selecteren, Hugging Face-content integreren en "breng je eigen model" (BYOM) strategieën adopteren. Je zult ook de Model Mondays-serie ontdekken voor doorlopende leer- en modelontdekking.

Referenties:
- Foundry Local documentatie: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face modellen compileren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Leerdoelen

- Ontdek en evalueer open-source modellen voor lokale inferentie
- Compileer en voer geselecteerde Hugging Face modellen uit binnen Foundry Local
- Pas strategieën voor modelselectie toe op basis van nauwkeurigheid, latentie en resourcebehoeften
- Beheer modellen lokaal met cache en versiebeheer

## Deel 1: Modelontdekking met Foundry CLI

### Basiscommando's voor Modelbeheer

De Foundry CLI biedt eenvoudige commando's voor modelontdekking en -beheer:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### Je Eerste Modellen Uitvoeren

Begin met populaire, goed geteste modellen om prestatiekenmerken te begrijpen:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**Opmerking:** De `--verbose` vlag geeft gedetailleerde startinformatie, waaronder:
- Voortgang van modeldownload (bij eerste uitvoering)
- Details over geheugentoewijzing
- Informatie over servicebinding
- Prestatie-initialisatiemetrics

### Begrip van Modelcategorieën

**Kleine Taalmodellen (SLMs):**
- `phi-4-mini`: Snel, efficiënt, ideaal voor algemene chat
- `phi-4`: Meer capabele versie met betere redeneervaardigheden

**Middelgrote Modellen:**
- `qwen2.5-7b`: Uitstekend in redeneren en langere context
- `deepseek-r1-7b`: Geoptimaliseerd voor codegeneratie

**Grote Modellen:**
- `llama-3.2`: Meta's nieuwste open-source model
- `qwen2.5-14b`: Redeneren op ondernemingsniveau

## Deel 2: Snel Modellen Testen en Vergelijken

### Voorbeeld 03 Aanpak: Eenvoudig Opsommen en Benchmarken

Gebaseerd op ons Voorbeeld 03 patroon, hier is de minimale workflow:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### Modelprestaties Testen

Zodra een model draait, test het met consistente prompts:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### PowerShell Testalternatief

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## Deel 3: Modelcache en Opslagbeheer

### Begrip van de Modelcache

Foundry Local beheert automatisch modeldownloads en caching:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### Overwegingen voor Modelopslag

**Typische Modelgroottes:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Beste Praktijken voor Opslag:**
- Houd 2-3 modellen in cache voor snelle wisseling
- Verwijder ongebruikte modellen om ruimte vrij te maken: `foundry cache clean`
- Houd schijfgebruik in de gaten, vooral op kleinere SSD's
- Overweeg de afweging tussen modelgrootte en capaciteit

### Prestatiemonitoring van Modellen

Terwijl modellen draaien, houd systeemresources in de gaten:

**Windows Taakbeheer:**
- Controleer geheugengebruik (modellen blijven geladen in RAM)
- Houd CPU-gebruik tijdens inferentie in de gaten
- Controleer schijf-I/O tijdens het laden van modellen

**Command Line Monitoring:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## Deel 4: Praktische Richtlijnen voor Modelselectie

### Modellen Kiezen per Toepassing

**Voor Algemene Chat en Q&A:**
- Begin met: `phi-4-mini` (snel, efficiënt)
- Upgrade naar: `phi-4` (betere redeneervaardigheden)
- Geavanceerd: `qwen2.5-7b` (langere context)

**Voor Codegeneratie:**
- Aanbevolen: `deepseek-r1-7b`
- Alternatief: `qwen2.5-7b` (ook goed voor code)

**Voor Complexe Redenering:**
- Beste: `qwen2.5-7b` of `qwen2.5-14b`
- Budgetoptie: `phi-4`

### Hardwarevereisten Gids

**Minimale Systeemvereisten:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**Aanbevolen voor Beste Prestaties:**
- 32GB+ RAM voor comfortabel wisselen tussen modellen
- SSD-opslag voor sneller laden van modellen
- Moderne CPU met goede single-thread prestaties
- NPU-ondersteuning (Windows 11 Copilot+ PC's) voor versnelling

### Workflow voor Modelwisseling

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## Deel 5: Eenvoudige Modelbenchmarking

### Basisprestatietests

Hier is een eenvoudige aanpak om modelprestaties te vergelijken:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### Handmatige Kwaliteitsbeoordeling

Test elk model met consistente prompts en beoordeel handmatig:

**Testprompts:**
1. "Leg quantum computing uit in eenvoudige termen."
2. "Schrijf een Python-functie om een lijst te sorteren."
3. "Wat zijn de voor- en nadelen van werken op afstand?"
4. "Vat de voordelen van edge AI samen."

**Beoordelingscriteria:**
- **Nauwkeurigheid**: Is de informatie correct?
- **Duidelijkheid**: Is de uitleg gemakkelijk te begrijpen?
- **Volledigheid**: Behandelt het de volledige vraag?
- **Snelheid**: Hoe snel reageert het?

### Resourcegebruik Monitoren

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## Deel 6: Volgende Stappen

- Abonneer je op Model Mondays voor nieuwe modellen en tips: https://aka.ms/model-mondays
- Draag bevindingen bij aan het `models.json` bestand van je team
- Bereid je voor op Sessie 4: vergelijken van LLMs vs SLMs, lokale vs cloud-inferentie, en praktische demo's

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.