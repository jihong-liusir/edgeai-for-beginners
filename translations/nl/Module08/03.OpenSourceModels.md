<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T23:53:06+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "nl"
}
-->
# Sessie 3: Ontdekking en Beheer van Open-Source Modellen

## Overzicht

Deze sessie richt zich op praktische modelontdekking en -beheer met Foundry Local. Je leert hoe je beschikbare modellen kunt opsommen, verschillende opties kunt testen en basisprestaties kunt begrijpen. De aanpak benadrukt hands-on verkenning met de foundry CLI om je te helpen de juiste modellen voor jouw toepassingen te selecteren.

## Leerdoelen

- Beheers de foundry CLI-commando's voor modelontdekking en -beheer
- Begrijp modelcache en lokale opslagpatronen
- Leer snel verschillende modellen te testen en te vergelijken
- Stel praktische workflows op voor modelselectie en benchmarking
- Verken het groeiende ecosysteem van modellen beschikbaar via Foundry Local

## Vereisten

- Sessie 1: Aan de slag met Foundry Local voltooid
- Foundry Local CLI geïnstalleerd en toegankelijk
- Voldoende opslagruimte voor modeldownloads (modellen kunnen variëren van 1GB tot 20GB+)
- Basiskennis van modeltypes en toepassingen

## Deel 6: Praktische Oefening

### Oefening: Modelontdekking en Vergelijking

Maak je eigen model-evaluatiescript op basis van Voorbeeld 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Jouw Taak

1. **Voer het script Voorbeeld 03 uit**: `samples\03\list_and_bench.cmd`
2. **Probeer verschillende modellen**: Test minstens 3 verschillende modellen
3. **Vergelijk prestaties**: Noteer verschillen in snelheid en responskwaliteit
4. **Documenteer bevindingen**: Maak een eenvoudige vergelijkingsgrafiek

### Voorbeeld Vergelijkingsformaat

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Deel 7: Problemen Oplossen en Best Practices

### Veelvoorkomende Problemen en Oplossingen

**Model Start Niet:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Onvoldoende Geheugen:**
- Begin met kleinere modellen (`phi-4-mini`)
- Sluit andere applicaties
- Upgrade RAM als je vaak tegen limieten aanloopt

**Trage Prestaties:**
- Zorg ervoor dat het model volledig geladen is (controleer gedetailleerde uitvoer)
- Sluit onnodige achtergrondapplicaties
- Overweeg snellere opslag (SSD)

### Best Practices

1. **Begin Klein**: Start met `phi-4-mini` om de setup te valideren
2. **Eén Model Tegelijk**: Stop eerdere modellen voordat je nieuwe start
3. **Monitor Resources**: Houd het geheugengebruik in de gaten
4. **Test Consistent**: Gebruik dezelfde prompts voor eerlijke vergelijkingen
5. **Documenteer Resultaten**: Houd aantekeningen bij over modelprestaties voor jouw toepassingen

## Deel 8: Volgende Stappen en Referenties

### Voorbereiding op Sessie 4

- **Focus van Sessie 4**: Optimalisatietools en -technieken
- **Vereisten**: Vertrouwd met modelwisselingen en basisprestatietests
- **Aanbevolen**: Identificeer 2-3 favoriete modellen uit deze sessie

### Aanvullende Bronnen

- **[Foundry Local Documentatie](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Officiële documentatie
- **[CLI Referentie](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Complete commandoreferentie
- **[Model Mondays](https://aka.ms/model-mondays)**: Wekelijkse modelspotlights
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Community en issues
- **[Voorbeeld 03: Modelontdekking](samples/03/README.md)**: Hands-on voorbeeldscript

### Belangrijkste Inzichten

✅ **Modelontdekking**: Gebruik `foundry model list` om beschikbare modellen te verkennen  
✅ **Snel Testen**: Het `list_and_bench.cmd`-patroon voor snelle evaluatie  
✅ **Prestatiemonitoring**: Basisgebruik van resources en responstijdmeting  
✅ **Modelselectie**: Praktische richtlijnen voor het kiezen van modellen per toepassing  
✅ **Cachebeheer**: Begrip van opslag- en opruimprocedures  

Je hebt nu de praktische vaardigheden om geschikte modellen te ontdekken, testen en selecteren voor jouw AI-toepassingen met behulp van Foundry Local's eenvoudige CLI-aanpak.

## Leerdoelen
- Ontdek en evalueer open-source modellen voor lokale inferentie
- Compileer en voer geselecteerde Hugging Face-modellen uit binnen Foundry Local
- Pas strategieën toe voor modelselectie op basis van nauwkeurigheid, latentie en resourcebehoeften
- Beheer modellen lokaal met cache en versiebeheer

## Deel 1: Modelontdekking met Foundry CLI

### Basiscommando's voor Modelbeheer

De foundry CLI biedt eenvoudige commando's voor modelontdekking en -beheer:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Je Eerste Modellen Uitvoeren

Begin met populaire, goed geteste modellen om prestatiekenmerken te begrijpen:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Opmerking:** De `--verbose` vlag geeft gedetailleerde opstartinformatie, waaronder:
- Voortgang van modeldownload (bij eerste uitvoering)
- Details over geheugentoewijzing
- Informatie over servicebinding
- Prestatie-initialisatiemetrics

### Begrip van Modelcategorieën

**Kleine Taalmodellen (SLMs):**
- `phi-4-mini`: Snel, efficiënt, geweldig voor algemene chat
- `phi-4`: Meer capabele versie met betere redeneervaardigheden

**Middelgrote Modellen:**
- `qwen2.5-7b-instruct`: Uitstekende redeneervaardigheden en langere context
- `deepseek-r1-distill-qwen-7b`: Geoptimaliseerd voor codegeneratie

**Grotere Modellen:**
- `llama-3.2`: Meta's nieuwste open-source model
- `qwen2.5-14b-instruct`: Redeneren op ondernemingsniveau

## Deel 2: Snel Modellen Testen en Vergelijken

### Voorbeeld 03 Aanpak: Eenvoudig Lijst en Benchmark

Gebaseerd op ons Voorbeeld 03-patroon, hier is de minimale workflow:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Modelprestaties Testen

Zodra een model draait, test het met consistente prompts:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell Testalternatief

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Deel 3: Modelcache en Opslagbeheer

### Begrip van de Modelcache

Foundry Local beheert automatisch modeldownloads en caching:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Overwegingen voor Modelopslag

**Typische Modelgroottes:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Opslag Best Practices:**
- Houd 2-3 modellen in cache voor snelle wisseling
- Verwijder ongebruikte modellen om ruimte vrij te maken: `foundry cache clean`
- Monitor schijfgebruik, vooral op kleinere SSD's
- Overweeg de afweging tussen modelgrootte en capaciteit

### Modelprestatiemonitoring

Terwijl modellen draaien, monitor je systeemresources:

**Windows Taakbeheer:**
- Houd geheugengebruik in de gaten (modellen blijven geladen in RAM)
- Monitor CPU-gebruik tijdens inferentie
- Controleer schijf I/O tijdens het laden van modellen

**Command Line Monitoring:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Deel 4: Praktische Richtlijnen voor Modelselectie

### Modellen Kiezen per Toepassing

**Voor Algemene Chat en V&A:**
- Begin met: `phi-4-mini` (snel, efficiënt)
- Upgrade naar: `phi-4` (betere redeneervaardigheden)
- Geavanceerd: `qwen2.5-7b-instruct` (langere context)

**Voor Codegeneratie:**
- Aanbevolen: `deepseek-r1-distill-qwen-7b`
- Alternatief: `qwen2.5-7b-instruct` (ook goed voor code)

**Voor Complexe Redenering:**
- Beste: `qwen2.5-7b-instruct` of `qwen2.5-14b-instruct`
- Budgetoptie: `phi-4`

### Hardwarevereisten Gids

**Minimale Systeemeisen:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Aanbevolen voor Beste Prestaties:**
- 32GB+ RAM voor comfortabel schakelen tussen modellen
- SSD-opslag voor sneller laden van modellen
- Moderne CPU met goede single-thread prestaties
- NPU-ondersteuning (Windows 11 Copilot+ PC's) voor versnelling

### Workflow voor Modelwisselingen

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Deel 5: Eenvoudige Modelbenchmarking

### Basisprestatietests

Hier is een eenvoudige aanpak om modelprestaties te vergelijken:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Handmatige Kwaliteitsbeoordeling

Test elk model met consistente prompts en beoordeel handmatig:

**Testprompts:**
1. "Leg kwantumcomputing uit in eenvoudige termen."
2. "Schrijf een Python-functie om een lijst te sorteren."
3. "Wat zijn de voor- en nadelen van thuiswerken?"
4. "Vat de voordelen van edge AI samen."

**Beoordelingscriteria:**
- **Nauwkeurigheid**: Is de informatie correct?
- **Duidelijkheid**: Is de uitleg makkelijk te begrijpen?
- **Volledigheid**: Behandelt het de volledige vraag?
- **Snelheid**: Hoe snel reageert het?

### Resourcegebruik Monitoren

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Deel 6: Volgende Stappen
- Abonneer je op Model Mondays voor nieuwe modellen en tips: https://aka.ms/model-mondays
- Draag bevindingen bij aan het `models.json` bestand van je team
- Bereid je voor op Sessie 4: vergelijken van LLMs vs SLMs, lokaal vs cloud inferentie, en hands-on demo's

---

