<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T21:50:00+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "nl"
}
-->
# Sessie 3: Open-Source Modellen met Foundry Local

## Overzicht

In deze sessie leer je hoe je open-source modellen kunt integreren in Foundry Local: het selecteren van communitymodellen, het integreren van Hugging Face-content en het toepassen van "bring your own model" (BYOM)-strategieën. Je ontdekt ook de Model Mondays-serie voor doorlopende kennis en modelontdekking.

Referenties:
- Foundry Local documentatie: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face modellen compileren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Leerdoelen
- Open-source modellen ontdekken en evalueren voor lokale inferentie
- Hugging Face modellen compileren en uitvoeren binnen Foundry Local
- Modelselectiestrategieën toepassen op basis van nauwkeurigheid, latency en resourcebehoeften
- Modellen lokaal beheren met cache en versiebeheer

## Deel 1: Modelontdekking en selectie (Stap-voor-stap)

Stap 1) Lijst beschikbare modellen in de lokale catalogus
```cmd
foundry model list
```

Stap 2) Probeer snel twee kandidaten uit (automatisch downloaden bij eerste uitvoering)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Stap 3) Noteer basisstatistieken
- Observeer latency (subjectief) en kwaliteit voor een vaste prompt
- Bekijk geheugengebruik via Taakbeheer terwijl elk model draait

## Deel 2: Catalogusmodellen uitvoeren via CLI (Stap-voor-stap)

Stap 1) Start een model
```cmd
foundry model run llama-3.2
```

Stap 2) Verstuur een testprompt via de OpenAI-compatibele endpoint
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## Deel 3: BYOM – Hugging Face Modellen Compileren (Stap-voor-stap)

Volg de officiële handleiding voor het compileren van modellen. Hieronder een overzicht van de stappen—raadpleeg het Microsoft Learn-artikel voor exacte commando's en ondersteunde configuraties.

Stap 1) Bereid een werkmap voor
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Stap 2) Compileer een ondersteund HF-model
- Gebruik de stappen uit de Learn-documentatie om het model te converteren en plaats het gecompileerde ONNX-model in je `models`-map
- Bevestig met:
```cmd
foundry cache ls
```
Je zou de naam van je gecompileerde model moeten zien (bijvoorbeeld `llama-3.2`).

Stap 3) Voer het gecompileerde model uit
```cmd
foundry model run llama-3.2 --verbose
```

Notities:
- Zorg voor voldoende schijfruimte en RAM om te compileren en uit te voeren
- Begin met kleinere modellen om de workflow te valideren, schaal daarna op

## Deel 4: Praktische Modelcuratie (Stap-voor-stap)

Stap 1) Maak een `models.json`-register
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Stap 2) Klein selectiescript
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## Deel 5: Hands-On Benchmarks (Stap-voor-stap)

Stap 1) Eenvoudige latency benchmark
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Stap 2) Kwaliteitscontrole
- Gebruik een vaste promptset, sla outputs op in een CSV/JSON
- Beoordeel handmatig vloeiendheid, relevantie en correctheid (1–5)

## Deel 6: Volgende stappen
- Abonneer je op Model Mondays voor nieuwe modellen en tips: https://aka.ms/model-mondays
- Deel je bevindingen met je team via `models.json`
- Bereid je voor op Sessie 4: vergelijking tussen LLMs en SLMs, lokale versus cloud-inferentie, en praktische demo's

---

