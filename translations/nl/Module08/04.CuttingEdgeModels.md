<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T21:52:01+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "nl"
}
-->
# Sessie 4: Geavanceerde Modellen – LLMs, SLMs en On-Device Inference

## Overzicht

Vergelijk LLMs en SLMs, evalueer de afwegingen tussen lokale en cloud inference, en implementeer demo's die EdgeAI-scenario's demonstreren met Phi en ONNX Runtime. We belichten ook Chainlit RAG, WebGPU inference-opties en Open WebUI-integratie.

Referenties:
- Foundry Local documentatie: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI handleiding (chat-app met Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Leerdoelen
- Begrijp de afwegingen tussen LLM en SLM op het gebied van kosten, latentie en nauwkeurigheid
- Kies tussen lokale en cloud inference voor specifieke zakelijke behoeften
- Implementeer een kleine RAG-demo met Chainlit
- Verken WebGPU voor versnelling in de browser
- Verbind Open WebUI met Foundry Local

## Deel 1: LLM vs SLM – Beslissingsmatrix

Overweeg:
- Latentie: SLMs op apparaten leveren vaak reacties binnen een seconde
- Kosten: lokale inference verlaagt cloudkosten
- Privacy: gevoelige gegevens blijven op het apparaat
- Capaciteit: LLMs kunnen beter presteren bij complexe taken
- Betrouwbaarheid: hybride strategieën verminderen het risico op downtime

## Deel 2: Lokaal vs Cloud – Hybride Patronen

- Eerst lokaal met cloud als fallback voor grote/complexe prompts
- Eerst cloud met lokaal voor privacygevoelige of offline scenario's
- Routeren op basis van taaktype (code-generatie naar DeepSeek, algemene chat naar Phi/Qwen)

## Deel 3: RAG Chat App met Chainlit (Minimaal)

Installeer afhankelijkheden:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Uitvoeren:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Uitbreiden: voeg een eenvoudige retriever toe (lokale bestanden) en plaats de opgehaalde context vóór de gebruikersprompt.

## Deel 4: WebGPU Inference (Heads-up)

Voer kleine modellen direct uit in de browser met WebGPU. Dit is ideaal voor privacygerichte demo's en ervaringen zonder installatie. Hieronder staat een minimale, stapsgewijze voorbeeld met ONNX Runtime Web en de WebGPU execution provider.

1) Controleer WebGPU-ondersteuning
- Chromium-browsers: chrome://gpu → bevestig dat “WebGPU” is ingeschakeld
- Programmatic check (we controleren dit ook in de code): `if (!('gpu' in navigator)) { /* geen WebGPU */ }`

2) Maak een minimaal project
Maak een map en twee bestanden: `index.html` en `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Lokaal serveren (Windows cmd.exe)
Gebruik een eenvoudige statische server zodat de browser het model kan ophalen.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Open http://localhost:5173 in je browser. Je zou initialisatielogs, sessiecreatie met WebGPU en een argmax-voorspelling moeten zien.

4) Problemen oplossen
- Als WebGPU niet beschikbaar is: update Chrome/Edge en zorg ervoor dat GPU-drivers actueel zijn, controleer vervolgens chrome://flags voor “Enable WebGPU”.
- Bij CORS- of fetch-fouten: zorg ervoor dat je bestanden via http:// serveert (niet file://) en dat de model-URL cross-origin verzoeken toestaat.
- Terugvallen op CPU: wijzig `executionProviders: ['wasm']` om basisgedrag te verifiëren.

5) Volgende stappen
- Vervang door een domeinspecifiek ONNX-model (bijv. beeldclassificatie of een klein tekstmodel).
- Voeg preprocessing/postprocessing-logica toe voor echte inputs.
- Voor grotere modellen of productielatentie, geef de voorkeur aan Foundry Local of ONNX Runtime Server.

## Deel 5: Open WebUI + Foundry Local (Stapsgewijs)

Dit verbindt Open WebUI met Foundry Local’s OpenAI-compatibele endpoint voor een lokale chat-UI.

1) Vereisten
- Foundry Local geïnstalleerd en werkend (`foundry --version`)
- Eén model klaar om lokaal te draaien (bijv. `phi-4-mini`)
- Docker Desktop geïnstalleerd (aanbevolen voor Open WebUI)

2) Start een model met Foundry Local
```powershell
foundry model run phi-4-mini
```
Dit biedt een OpenAI-compatibele API op `http://localhost:8000`.

3) Start Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Opmerkingen:
- Op Windows kan `host.docker.internal` de container toegang geven tot je host op `localhost`.
- We stellen `OPENAI_API_BASE_URL` in op Foundry Local’s endpoint en een dummy `OPENAI_API_KEY`.

4) Configureren vanuit de Open WebUI UI (alternatief)
- Ga naar http://localhost:3000
- Voltooi de initiële setup (admin-gebruiker)
- Ga naar Instellingen → Modellen/Providers
- Stel Basis-URL in: `http://host.docker.internal:8000/v1`
- Stel API-sleutel in: `local-key` (placeholder)
- Opslaan

5) Voer een testprompt uit
- In Open WebUI chat, selecteer of voer modelnaam `phi-4-mini` in
- Prompt: “Noem vijf voordelen van AI-inference op apparaten.”
- Je zou een reactie moeten zien die wordt gestreamd vanuit je lokale model

6) Problemen oplossen
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Optioneel: Open WebUI-data behouden
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Hands-On Checklist
- [ ] Vergelijk reacties/latentie tussen SLM en LLM lokaal
- [ ] Voer de Chainlit-demo uit met minstens twee modellen
- [ ] Verbind Open WebUI met je lokale endpoint en test

## Volgende stappen
- Bereid je voor op agent-workflows in Sessie 5
- Identificeer scenario's waar hybride lokaal/cloud de ROI verbetert

---

