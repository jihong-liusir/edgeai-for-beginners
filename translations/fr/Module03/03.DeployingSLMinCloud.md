<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-07-22T04:59:15+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "fr"
}
-->
# Déploiement Cloud Conteneurisé - Solutions à Échelle de Production

Ce tutoriel complet couvre trois approches principales pour déployer le modèle Phi-4-mini-instruct de Microsoft dans des environnements conteneurisés : vLLM, Ollama et SLM Engine avec ONNX Runtime. Ce modèle de 3,8 milliards de paramètres représente un choix optimal pour les tâches de raisonnement tout en maintenant une efficacité adaptée aux déploiements en périphérie.

## Table des Matières

1. [Introduction au Déploiement Conteneurisé de Phi-4-mini](../../../Module03)
2. [Objectifs d'Apprentissage](../../../Module03)
3. [Comprendre la Classification de Phi-4-mini](../../../Module03)
4. [Déploiement Conteneurisé avec vLLM](../../../Module03)
5. [Déploiement Conteneurisé avec Ollama](../../../Module03)
6. [SLM Engine avec ONNX Runtime](../../../Module03)
7. [Cadre de Comparaison](../../../Module03)
8. [Meilleures Pratiques](../../../Module03)

## Introduction au Déploiement Conteneurisé de Phi-4-mini

Les Small Language Models (SLMs) représentent une avancée cruciale dans l'EdgeAI, permettant des capacités sophistiquées de traitement du langage naturel sur des appareils aux ressources limitées. Ce tutoriel se concentre sur les stratégies de déploiement conteneurisé pour le modèle Phi-4-mini-instruct de Microsoft, un modèle de raisonnement à la pointe de la technologie qui équilibre capacité et efficacité.

### Modèle en Vedette : Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 milliards de paramètres)** : Le dernier modèle léger de Microsoft, ajusté pour les instructions, conçu pour des environnements contraints en mémoire et en calcul, avec des capacités exceptionnelles dans :
- **Raisonnement mathématique et calculs complexes**
- **Génération, débogage et analyse de code**
- **Résolution de problèmes logiques et raisonnement pas à pas**
- **Applications éducatives nécessitant des explications détaillées**
- **Appels de fonctions et intégration d'outils**

Faisant partie de la catégorie des "Small SLMs" (1,5 à 13,9 milliards de paramètres), Phi-4-mini trouve un équilibre optimal entre capacité de raisonnement et efficacité des ressources.

### Avantages du Déploiement Conteneurisé de Phi-4-mini

- **Efficacité Opérationnelle** : Inférence rapide pour les tâches de raisonnement avec des exigences computationnelles réduites
- **Flexibilité de Déploiement** : Capacités d'IA sur appareil avec une confidentialité renforcée grâce au traitement local
- **Rentabilité** : Réduction des coûts opérationnels par rapport aux modèles plus grands tout en maintenant la qualité
- **Isolation** : Séparation propre entre les instances de modèle et environnements d'exécution sécurisés
- **Scalabilité** : Mise à l'échelle horizontale facile pour augmenter le débit de raisonnement

## Objectifs d'Apprentissage

À la fin de ce tutoriel, vous serez capable de :

- Déployer et optimiser Phi-4-mini-instruct dans divers environnements conteneurisés
- Mettre en œuvre des stratégies avancées de quantification et de compression pour différents scénarios de déploiement
- Configurer une orchestration conteneurisée prête pour la production pour les charges de travail de raisonnement
- Évaluer et sélectionner les cadres de déploiement appropriés en fonction des exigences spécifiques des cas d'utilisation
- Appliquer les meilleures pratiques en matière de sécurité, de surveillance et de mise à l'échelle pour les déploiements conteneurisés de SLM

## Comprendre la Classification de Phi-4-mini

### Spécifications du Modèle

**Détails Techniques :**
- **Paramètres** : 3,8 milliards (catégorie Small SLM)
- **Architecture** : Transformer dense à décodeur unique avec attention par groupes de requêtes
- **Longueur de Contexte** : 128K tokens (32K recommandés pour des performances optimales)
- **Vocabulaire** : 200K tokens avec support multilingue
- **Données d'Entraînement** : 5T tokens de contenu dense en raisonnement de haute qualité

### Exigences en Ressources

| Type de Déploiement | RAM Min | RAM Recommandée | VRAM (GPU) | Stockage | Cas d'Utilisation Typiques |
|---------------------|---------|-----------------|------------|----------|---------------------------|
| **Développement**   | 6GB     | 8GB             | -          | 8GB      | Tests locaux, prototypage |
| **Production CPU**  | 8GB     | 12GB            | -          | 10GB     | Serveurs Edge, déploiement optimisé en coût |
| **Production GPU**  | 6GB     | 8GB             | 4-6GB      | 8GB      | Services de raisonnement à haut débit |
| **Optimisé Edge**   | 4GB     | 6GB             | -          | 6GB      | Déploiement quantifié, passerelles IoT |

### Capacités de Phi-4-mini

- **Excellence Mathématique** : Résolution avancée de problèmes d'arithmétique, d'algèbre et de calcul
- **Intelligence de Code** : Génération de code en Python, JavaScript et autres langages, avec débogage
- **Raisonnement Logique** : Décomposition des problèmes et construction de solutions étape par étape
- **Support Éducatif** : Explications détaillées adaptées aux scénarios d'apprentissage et d'enseignement
- **Appels de Fonction** : Support natif pour l'intégration d'outils et les interactions API

## Déploiement Conteneurisé avec vLLM

vLLM offre un excellent support pour Phi-4-mini-instruct avec des performances d'inférence optimisées et des API compatibles OpenAI, ce qui en fait un choix idéal pour les services de raisonnement en production.

### Exemples de Démarrage Rapide

#### Déploiement CPU Basique (Développement)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Déploiement en Production Accéléré par GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Configuration de Production

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Test des Capacités de Raisonnement de Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Déploiement Conteneurisé avec Ollama

Ollama offre un excellent support pour Phi-4-mini-instruct avec un déploiement et une gestion simplifiés, ce qui le rend idéal pour le développement et les déploiements équilibrés en production.

### Configuration Rapide

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Configuration de Production

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimisation du Modèle et Variantes

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Exemples d'Utilisation de l'API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine avec ONNX Runtime

ONNX Runtime offre des performances optimales pour le déploiement en périphérie de Phi-4-mini-instruct avec des optimisations avancées et une compatibilité multiplateforme.

### Configuration de Base

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Implémentation Simplifiée du Serveur

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script de Conversion du Modèle

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Configuration de Production

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Test du Déploiement ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Cadre de Comparaison

### Comparaison des Cadres pour Phi-4-mini

| Fonctionnalité         | vLLM   | Ollama | ONNX Runtime |
|------------------------|--------|--------|--------------|
| **Complexité de Configuration** | Modérée | Facile | Complexe |
| **Performance (GPU)**  | Excellente (~25 tok/s) | Très Bonne (~20 tok/s) | Bonne (~15 tok/s) |
| **Performance (CPU)**  | Bonne (~8 tok/s) | Très Bonne (~12 tok/s) | Excellente (~15 tok/s) |
| **Utilisation Mémoire**| 8-12GB | 6-10GB | 4-8GB |
| **Compatibilité API**   | Compatible OpenAI | REST Personnalisé | FastAPI Personnalisé |
| **Appels de Fonction**  | ✅ Natif | ✅ Supporté | ⚠️ Implémentation Personnalisée |
| **Support de Quantification** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Quantification ONNX |
| **Prêt pour la Production** | ✅ Excellent | ✅ Très Bon | ✅ Bon |
| **Déploiement Edge**    | Bon | Excellent | Exceptionnel |

## Ressources Supplémentaires

### Documentation Officielle
- **Microsoft Phi-4 Model Card** : Spécifications détaillées et directives d'utilisation
- **Documentation vLLM** : Options avancées de configuration et d'optimisation
- **Bibliothèque de Modèles Ollama** : Modèles communautaires et exemples de personnalisation
- **Guides ONNX Runtime** : Optimisation des performances et stratégies de déploiement

### Outils de Développement
- **Hugging Face Transformers** : Pour l'interaction et la personnalisation des modèles
- **Spécification API OpenAI** : Pour les tests de compatibilité avec vLLM
- **Meilleures Pratiques Docker** : Sécurité et optimisation des conteneurs
- **Déploiement Kubernetes** : Modèles d'orchestration pour la mise à l'échelle en production

### Ressources d'Apprentissage
- **Benchmarking des Performances SLM** : Méthodologies d'analyse comparative
- **Déploiement Edge AI** : Meilleures pratiques pour les environnements contraints en ressources
- **Optimisation des Tâches de Raisonnement** : Stratégies de prompting pour les problèmes mathématiques et logiques
- **Sécurité des Conteneurs** : Pratiques de durcissement pour les déploiements de modèles IA

## Résultats d'Apprentissage

Après avoir complété ce module, vous serez capable de :

1. Déployer le modèle Phi-4-mini-instruct dans des environnements conteneurisés en utilisant plusieurs cadres
2. Configurer et optimiser les déploiements SLM pour différents environnements matériels
3. Mettre en œuvre les meilleures pratiques de sécurité pour les déploiements IA conteneurisés
4. Comparer et sélectionner les cadres de déploiement appropriés en fonction des exigences spécifiques des cas d'utilisation
5. Appliquer des stratégies de surveillance et de mise à l'échelle pour des services SLM de niveau production

## Et après

- Retourner au [Module 1](../Module01/README.md)
- Retourner au [Module 2](../Module02/README.md)

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.