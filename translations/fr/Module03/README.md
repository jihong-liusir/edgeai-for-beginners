<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-07-22T04:51:06+00:00",
  "source_file": "Module03/README.md",
  "language_code": "fr"
}
-->
# Chapitre 03 : Déploiement des Small Language Models (SLMs)

Ce chapitre complet explore le cycle de vie entier du déploiement des Small Language Models (SLMs), en couvrant les bases théoriques, les stratégies de mise en œuvre pratiques et les solutions conteneurisées prêtes pour la production. Le chapitre est structuré en trois sections progressives qui emmènent les lecteurs des concepts fondamentaux aux scénarios de déploiement avancés.

## Structure du chapitre et parcours d'apprentissage

### **[Section 1 : Apprentissage avancé des SLM - Fondations et optimisation](./01.SLMAdvancedLearning.md)**
La section d'ouverture établit les bases théoriques pour comprendre les Small Language Models et leur importance stratégique dans les déploiements d'IA en périphérie. Cette section couvre :

- **Cadre de classification des paramètres** : Exploration détaillée des catégories de SLM, des Micro SLMs (100M-1,4B paramètres) aux Medium SLMs (14B-30B paramètres), avec un focus particulier sur des modèles comme Phi-4-mini-3.8B, la série Qwen3 et Google Gemma3, incluant une analyse des exigences matérielles et de l'empreinte mémoire pour chaque niveau de modèle
- **Techniques d'optimisation avancées** : Couverture complète des méthodes de quantification utilisant les frameworks Llama.cpp, Microsoft Olive et Apple MLX, incluant la quantification BitNET 1-bit de pointe avec des exemples de code pratiques montrant les pipelines de quantification et les résultats de benchmarking
- **Stratégies d'acquisition de modèles** : Analyse approfondie de l'écosystème Hugging Face et du catalogue de modèles Azure AI Foundry pour le déploiement de SLMs de niveau entreprise, avec des exemples de code pour le téléchargement programmatique, la validation et la conversion de formats
- **APIs pour développeurs** : Exemples de code en Python, C++ et C# montrant comment charger des modèles, effectuer des inférences et s'intégrer avec des frameworks populaires comme PyTorch, TensorFlow et ONNX Runtime

Cette section fondamentale met l'accent sur l'équilibre entre l'efficacité opérationnelle, la flexibilité de déploiement et la rentabilité qui rend les SLMs idéaux pour les scénarios de calcul en périphérie, avec des exemples de code pratiques que les développeurs peuvent directement implémenter dans leurs projets.

### **[Section 2 : Déploiement en environnement local - Solutions axées sur la confidentialité](./02.DeployingSLMinLocalEnv.md)**
La deuxième section passe de la théorie à la mise en œuvre pratique, en se concentrant sur les stratégies de déploiement local qui privilégient la souveraineté des données et l'indépendance opérationnelle. Les points clés incluent :

- **Plateforme universelle Ollama** : Exploration complète du déploiement multiplateforme avec un accent sur les workflows conviviaux pour les développeurs, la gestion du cycle de vie des modèles et la personnalisation via les Modelfiles, incluant des exemples complets d'intégration REST API et des scripts d'automatisation CLI
- **Microsoft Foundry Local** : Solutions de déploiement de niveau entreprise avec optimisation basée sur ONNX, intégration Windows ML et fonctionnalités de sécurité complètes, avec des exemples de code en C# et Python pour l'intégration native des applications
- **Analyse comparative** : Comparaison détaillée des frameworks couvrant l'architecture technique, les caractéristiques de performance et les lignes directrices pour l'optimisation des cas d'utilisation, avec du code de benchmark pour évaluer la vitesse d'inférence et l'utilisation de la mémoire sur différents matériels
- **Intégration API** : Applications exemples montrant comment construire des services web, des applications de chat et des pipelines de traitement de données en utilisant des déploiements locaux de SLM, avec des exemples de code en Node.js, Python Flask/FastAPI et ASP.NET Core
- **Frameworks de test** : Approches de test automatisé pour l'assurance qualité des modèles, incluant des exemples de tests unitaires et d'intégration pour les implémentations de SLM

Cette section fournit des conseils pratiques pour les organisations cherchant à mettre en œuvre des solutions d'IA préservant la confidentialité tout en maintenant un contrôle total sur leur environnement de déploiement, avec des exemples de code prêts à l'emploi que les développeurs peuvent adapter à leurs besoins spécifiques.

### **[Section 3 : Déploiement conteneurisé dans le cloud - Solutions à l'échelle de la production](./03.DeployingSLMinCloud.md)**
La section finale culmine avec des stratégies avancées de déploiement conteneurisé, mettant en avant le modèle Phi-4-mini-instruct de Microsoft comme étude de cas principale. Cette section couvre :

- **Déploiement vLLM** : Optimisation des inférences haute performance avec des APIs compatibles OpenAI, une accélération GPU avancée et une configuration de niveau production, incluant des Dockerfiles complets, des manifestes Kubernetes et des paramètres de tuning de performance
- **Orchestration de conteneurs Ollama** : Workflows de déploiement simplifiés avec Docker Compose, variantes d'optimisation des modèles et intégration d'interface web, avec des exemples de pipelines CI/CD pour le déploiement et les tests automatisés
- **Implémentation ONNX Runtime** : Déploiement optimisé pour la périphérie avec conversion complète des modèles, stratégies de quantification et compatibilité multiplateforme, incluant des exemples de code détaillés pour l'optimisation et le déploiement des modèles
- **Surveillance et observabilité** : Mise en œuvre de tableaux de bord Prometheus/Grafana avec des métriques personnalisées pour la surveillance des performances des SLM, incluant des configurations d'alertes et d'agrégation de logs
- **Équilibrage de charge et mise à l'échelle** : Exemples pratiques de stratégies de mise à l'échelle horizontale et verticale avec des configurations d'autoscaling basées sur l'utilisation CPU/GPU et les modèles de requêtes
- **Renforcement de la sécurité** : Meilleures pratiques de sécurité pour les conteneurs incluant la réduction des privilèges, les politiques réseau et la gestion des secrets pour les clés API et les identifiants d'accès aux modèles

Chaque approche de déploiement est présentée avec des exemples complets de configuration, des procédures de test, des checklists de préparation à la production et des templates d'infrastructure en tant que code que les développeurs peuvent directement appliquer à leurs workflows de déploiement.

## Résultats d'apprentissage clés

En complétant ce chapitre, les lecteurs maîtriseront :

1. **Sélection stratégique de modèles** : Comprendre les limites des paramètres et sélectionner les SLMs appropriés en fonction des contraintes de ressources et des exigences de performance
2. **Maîtrise de l'optimisation** : Implémenter des techniques avancées de quantification sur différents frameworks pour atteindre un équilibre optimal entre performance et efficacité
3. **Flexibilité de déploiement** : Choisir entre des solutions locales axées sur la confidentialité et des déploiements conteneurisés évolutifs en fonction des besoins organisationnels
4. **Préparation à la production** : Configurer des systèmes de surveillance, de sécurité et de mise à l'échelle pour des déploiements SLM de niveau entreprise

## Orientation pratique et applications réelles

Le chapitre maintient une forte orientation pratique tout au long, en proposant :

- **Exemples pratiques** : Fichiers de configuration complets, procédures de test API et scripts de déploiement
- **Benchmarking de performance** : Comparaisons détaillées de la vitesse d'inférence, de l'utilisation de la mémoire et des exigences en ressources
- **Considérations de sécurité** : Pratiques de sécurité de niveau entreprise, cadres de conformité et stratégies de protection des données
- **Meilleures pratiques** : Lignes directrices éprouvées pour la surveillance, la mise à l'échelle et la maintenance

## Perspective tournée vers l'avenir

Le chapitre se termine par des perspectives sur les tendances émergentes, notamment :

- Architectures de modèles avancées avec des ratios d'efficacité améliorés
- Intégration matérielle plus profonde avec des accélérateurs IA spécialisés
- Évolution de l'écosystème vers la standardisation et l'interopérabilité
- Modèles d'adoption en entreprise motivés par la confidentialité et les exigences de conformité

Cette approche complète garantit que les lecteurs sont bien équipés pour naviguer à la fois dans les défis actuels du déploiement des SLM et dans les développements technologiques futurs, en prenant des décisions éclairées qui s'alignent sur leurs exigences et contraintes organisationnelles spécifiques.

Le chapitre sert à la fois de guide pratique pour une mise en œuvre immédiate et de ressource stratégique pour la planification à long terme du déploiement de l'IA, en mettant l'accent sur l'équilibre critique entre capacité, efficacité et excellence opérationnelle qui définit les déploiements SLM réussis.

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.