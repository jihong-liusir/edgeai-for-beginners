<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-15T16:51:53+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fr"
}
-->
# Section 3 : Suite d'Optimisation Microsoft Olive

## Table des matières
1. [Introduction](../../../Module04)
2. [Qu'est-ce que Microsoft Olive ?](../../../Module04)
3. [Installation](../../../Module04)
4. [Guide de démarrage rapide](../../../Module04)
5. [Exemple : Conversion de Qwen3 en ONNX INT4](../../../Module04)
6. [Utilisation avancée](../../../Module04)
7. [Bonnes pratiques](../../../Module04)
8. [Dépannage](../../../Module04)
9. [Ressources supplémentaires](../../../Module04)

## Introduction

Microsoft Olive est un outil puissant et facile à utiliser pour l'optimisation de modèles, conçu pour tirer parti des spécificités matérielles. Il simplifie le processus d'optimisation des modèles d'apprentissage automatique pour leur déploiement sur différentes plateformes matérielles. Que vous cibliez des CPU, des GPU ou des accélérateurs IA spécialisés, Olive vous aide à atteindre des performances optimales tout en préservant la précision des modèles.

## Qu'est-ce que Microsoft Olive ?

Olive est un outil d'optimisation de modèles, conscient des spécificités matérielles, qui intègre des techniques de pointe dans les domaines de la compression, de l'optimisation et de la compilation des modèles. Il fonctionne avec ONNX Runtime comme solution d'optimisation de bout en bout pour l'inférence.

### Fonctionnalités principales

- **Optimisation adaptée au matériel** : Sélectionne automatiquement les meilleures techniques d'optimisation pour votre matériel cible
- **Plus de 40 composants d'optimisation intégrés** : Inclut la compression de modèles, la quantification, l'optimisation de graphes, et bien plus
- **Interface CLI simple** : Commandes faciles pour les tâches d'optimisation courantes
- **Support multi-framework** : Compatible avec PyTorch, les modèles Hugging Face et ONNX
- **Support des modèles populaires** : Olive peut optimiser automatiquement des architectures de modèles populaires comme Llama, Phi, Qwen, Gemma, etc.

### Avantages

- **Réduction du temps de développement** : Plus besoin d'expérimenter manuellement différentes techniques d'optimisation
- **Amélioration des performances** : Gains significatifs en vitesse (jusqu'à 6x dans certains cas)
- **Déploiement multiplateforme** : Les modèles optimisés fonctionnent sur différents matériels et systèmes d'exploitation
- **Précision maintenue** : Les optimisations préservent la qualité des modèles tout en améliorant leurs performances

## Installation

### Prérequis

- Python 3.8 ou version ultérieure
- Gestionnaire de paquets pip
- Environnement virtuel (recommandé)

### Installation de base

Créez et activez un environnement virtuel :

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installez Olive avec les fonctionnalités d'auto-optimisation :

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dépendances optionnelles

Olive propose diverses dépendances optionnelles pour des fonctionnalités supplémentaires :

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Vérification de l'installation

```bash
olive --help
```

Si l'installation est réussie, vous devriez voir le message d'aide de la CLI Olive.

## Guide de démarrage rapide

### Votre première optimisation

Optimisons un petit modèle de langage en utilisant la fonctionnalité d'auto-optimisation d'Olive :

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ce que fait cette commande

Le processus d'optimisation comprend : l'acquisition du modèle depuis le cache local, la capture du graphe ONNX et le stockage des poids dans un fichier de données ONNX, l'optimisation du graphe ONNX, et la quantification du modèle en int4 via la méthode RTN.

### Explication des paramètres de commande

- `--model_name_or_path` : Identifiant du modèle Hugging Face ou chemin local
- `--output_path` : Répertoire où le modèle optimisé sera sauvegardé
- `--device` : Matériel cible (cpu, gpu)
- `--provider` : Fournisseur d'exécution (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai` : Utiliser ONNX Runtime Generate AI pour l'inférence
- `--precision` : Précision de quantification (int4, int8, fp16)
- `--log_level` : Niveau de verbosité des journaux (0=minimal, 1=verbeux)

## Exemple : Conversion de Qwen3 en ONNX INT4

À partir de l'exemple fourni par Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), voici comment optimiser un modèle Qwen3 :

### Étape 1 : Télécharger le modèle (optionnel)

Pour réduire le temps de téléchargement, mettez en cache uniquement les fichiers essentiels :

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Étape 2 : Optimiser le modèle Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Étape 3 : Tester le modèle optimisé

Créez un script Python simple pour tester votre modèle optimisé :

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Structure de sortie

Après optimisation, votre répertoire de sortie contiendra :

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Utilisation avancée

### Fichiers de configuration

Pour des workflows d'optimisation plus complexes, vous pouvez utiliser des fichiers de configuration JSON :

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Exécutez avec une configuration :

```bash
olive run --config config.json
```

### Optimisation GPU

Pour l'optimisation GPU avec CUDA :

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pour DirectML (Windows) :

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Affinage avec Olive

Olive prend également en charge l'affinage des modèles :

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bonnes pratiques

### 1. Sélection du modèle
- Commencez par des modèles plus petits pour les tests (par ex., 0.5B-7B paramètres)
- Assurez-vous que l'architecture de votre modèle cible est prise en charge par Olive

### 2. Considérations matérielles
- Adaptez votre cible d'optimisation au matériel de déploiement
- Utilisez l'optimisation GPU si vous disposez de matériel compatible CUDA
- Envisagez DirectML pour les machines Windows avec graphiques intégrés

### 3. Sélection de la précision
- **INT4** : Compression maximale, légère perte de précision
- **INT8** : Bon équilibre entre taille et précision
- **FP16** : Perte de précision minimale, réduction modérée de la taille

### 4. Tests et validation
- Testez toujours les modèles optimisés avec vos cas d'utilisation spécifiques
- Comparez les métriques de performance (latence, débit, précision)
- Utilisez des données d'entrée représentatives pour l'évaluation

### 5. Optimisation itérative
- Commencez par l'auto-optimisation pour des résultats rapides
- Utilisez des fichiers de configuration pour un contrôle précis
- Expérimentez avec différents passes d'optimisation

## Dépannage

### Problèmes courants

#### 1. Problèmes d'installation
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problèmes CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problèmes de mémoire
- Utilisez des tailles de lot plus petites pendant l'optimisation
- Essayez la quantification avec une précision plus élevée en premier (int8 au lieu de int4)
- Assurez-vous d'avoir suffisamment d'espace disque pour la mise en cache des modèles

#### 4. Erreurs de chargement de modèle
- Vérifiez le chemin du modèle et les permissions d'accès
- Vérifiez si le modèle nécessite `trust_remote_code=True`
- Assurez-vous que tous les fichiers nécessaires du modèle sont téléchargés

### Obtenir de l'aide

- **Documentation** : [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problèmes GitHub** : [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemples** : [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Ressources supplémentaires

### Liens officiels
- **Dépôt GitHub** : [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Documentation ONNX Runtime** : [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemple Hugging Face** : [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemples communautaires
- **Notebooks Jupyter** : Disponibles dans le dépôt GitHub Olive
- **Extension VS Code** : L'extension AI Toolkit utilise Olive pour l'optimisation des modèles
- **Articles de blog** : Le blog Open Source de Microsoft propose des tutoriels détaillés sur Olive

### Outils associés
- **ONNX Runtime** : Moteur d'inférence haute performance
- **Hugging Face Transformers** : Source de nombreux modèles compatibles
- **Azure Machine Learning** : Workflows d'optimisation basés sur le cloud

## ➡️ Et après ?

- [04 : Suite d'Optimisation OpenVINO Toolkit](./04.openvino.md)

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.