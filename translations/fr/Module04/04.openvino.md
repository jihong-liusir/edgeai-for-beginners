<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-15T17:06:12+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "fr"
}
-->
# Section 4 : Suite d'optimisation OpenVINO Toolkit

## Table des matières
1. [Introduction](../../../Module04)
2. [Qu'est-ce qu'OpenVINO ?](../../../Module04)
3. [Installation](../../../Module04)
4. [Guide de démarrage rapide](../../../Module04)
5. [Exemple : Conversion et optimisation de modèles avec OpenVINO](../../../Module04)
6. [Utilisation avancée](../../../Module04)
7. [Bonnes pratiques](../../../Module04)
8. [Dépannage](../../../Module04)
9. [Ressources supplémentaires](../../../Module04)

## Introduction

OpenVINO (Open Visual Inference and Neural Network Optimization) est l'outil open-source d'Intel pour déployer des solutions d'IA performantes dans le cloud, sur site et en périphérie. Que vous cibliez des CPU, GPU, VPU ou des accélérateurs d'IA spécialisés, OpenVINO offre des capacités d'optimisation complètes tout en maintenant la précision des modèles et en permettant un déploiement multiplateforme.

## Qu'est-ce qu'OpenVINO ?

OpenVINO est un outil open-source qui permet aux développeurs d'optimiser, de convertir et de déployer efficacement des modèles d'IA sur diverses plateformes matérielles. Il se compose de trois principaux composants : OpenVINO Runtime pour l'inférence, Neural Network Compression Framework (NNCF) pour l'optimisation des modèles, et OpenVINO Model Server pour un déploiement évolutif.

### Fonctionnalités clés

- **Déploiement multiplateforme** : Prend en charge Linux, Windows et macOS avec des API Python, C++ et C
- **Accélération matérielle** : Découverte automatique des dispositifs et optimisation pour CPU, GPU, VPU et accélérateurs d'IA
- **Framework de compression de modèles** : Techniques avancées de quantification, élagage et optimisation via NNCF
- **Compatibilité avec les frameworks** : Support direct pour les modèles TensorFlow, ONNX, PaddlePaddle et PyTorch
- **Support de l'IA générative** : OpenVINO GenAI spécialisé pour le déploiement de modèles de langage étendus et d'applications d'IA générative

### Avantages

- **Optimisation des performances** : Améliorations significatives de la vitesse avec une perte minimale de précision
- **Empreinte de déploiement réduite** : Dépendances externes minimales pour simplifier l'installation et le déploiement
- **Temps de démarrage amélioré** : Chargement et mise en cache des modèles optimisés pour une initialisation plus rapide des applications
- **Déploiement évolutif** : Des dispositifs en périphérie à l'infrastructure cloud avec des API cohérentes
- **Prêt pour la production** : Fiabilité de niveau entreprise avec une documentation complète et un support communautaire

## Installation

### Prérequis

- Python 3.8 ou supérieur
- Gestionnaire de paquets pip
- Environnement virtuel (recommandé)
- Matériel compatible (CPU Intel recommandé, mais prend en charge diverses architectures)

### Installation de base

Créez et activez un environnement virtuel :

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installez OpenVINO Runtime :

```bash
pip install openvino
```

Installez NNCF pour l'optimisation des modèles :

```bash
pip install nncf
```

### Installation d'OpenVINO GenAI

Pour les applications d'IA générative :

```bash
pip install openvino-genai
```

### Dépendances optionnelles

Paquets supplémentaires pour des cas d'utilisation spécifiques :

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Vérification de l'installation

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Si l'installation est réussie, vous devriez voir les informations de version d'OpenVINO.

## Guide de démarrage rapide

### Votre première optimisation de modèle

Convertissons et optimisons un modèle Hugging Face avec OpenVINO :

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Ce que fait ce processus

Le workflow d'optimisation comprend : le chargement du modèle original depuis Hugging Face, la conversion au format OpenVINO Intermediate Representation (IR), l'application des optimisations par défaut et la compilation pour le matériel cible.

### Explication des paramètres clés

- `export=True` : Convertit le modèle au format IR d'OpenVINO
- `compile=False` : Retarde la compilation jusqu'à l'exécution pour plus de flexibilité
- `device` : Matériel cible ("CPU", "GPU", "AUTO" pour une sélection automatique)
- `save_pretrained()` : Enregistre le modèle optimisé pour une réutilisation

## Exemple : Conversion et optimisation de modèles avec OpenVINO

### Étape 1 : Conversion de modèle avec quantification NNCF

Voici comment appliquer une quantification après entraînement avec NNCF :

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Étape 2 : Optimisation avancée avec compression des poids

Pour les modèles basés sur des transformateurs, appliquez une compression des poids :

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Étape 3 : Inférence avec un modèle optimisé

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Structure de sortie

Après optimisation, le répertoire de votre modèle contiendra :

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Utilisation avancée

### Configuration avec NNCF YAML

Pour des workflows d'optimisation complexes, utilisez des fichiers de configuration NNCF :

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Appliquez la configuration :

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimisation GPU

Pour l'accélération GPU :

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimisation du traitement par lots

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Déploiement avec Model Server

Déployez des modèles optimisés avec OpenVINO Model Server :

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Code client pour le serveur de modèles :

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Bonnes pratiques

### 1. Sélection et préparation des modèles
- Utilisez des modèles provenant de frameworks pris en charge (PyTorch, TensorFlow, ONNX)
- Assurez-vous que les entrées des modèles ont des formes fixes ou dynamiques connues
- Testez avec des ensembles de données représentatifs pour la calibration

### 2. Sélection de la stratégie d'optimisation
- **Quantification après entraînement** : Commencez ici pour une optimisation rapide
- **Compression des poids** : Idéal pour les modèles de langage étendus et les transformateurs
- **Entraînement conscient de la quantification** : À utiliser lorsque la précision est critique

### 3. Optimisation spécifique au matériel
- **CPU** : Utilisez la quantification INT8 pour un équilibre entre performance et précision
- **GPU** : Exploitez la précision FP16 et le traitement par lots
- **VPU** : Concentrez-vous sur la simplification des modèles et la fusion des couches

### 4. Ajustement des performances
- **Mode débit** : Pour le traitement par lots à haut volume
- **Mode latence** : Pour les applications interactives en temps réel
- **Dispositif AUTO** : Laissez OpenVINO sélectionner le matériel optimal

### 5. Gestion de la mémoire
- Utilisez les formes dynamiques avec précaution pour éviter les surcharges de mémoire
- Implémentez la mise en cache des modèles pour des chargements plus rapides
- Surveillez l'utilisation de la mémoire pendant l'optimisation

### 6. Validation de la précision
- Validez toujours les modèles optimisés par rapport aux performances originales
- Utilisez des ensembles de test représentatifs pour l'évaluation
- Envisagez une optimisation progressive (commencez avec des paramètres conservateurs)

## Dépannage

### Problèmes courants

#### 1. Problèmes d'installation
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Erreurs de conversion de modèle
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Problèmes de performance
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Problèmes de mémoire
- Réduisez la taille des lots du modèle pendant l'optimisation
- Utilisez le streaming pour les ensembles de données volumineux
- Activez la mise en cache des modèles : `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Dégradation de la précision
- Utilisez une précision plus élevée (INT8 au lieu de INT4)
- Augmentez la taille de l'ensemble de données de calibration
- Appliquez une optimisation en précision mixte

### Surveillance des performances

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Obtenir de l'aide

- **Documentation** : [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Problèmes GitHub** : [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum communautaire** : [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Ressources supplémentaires

### Liens officiels
- **Page d'accueil OpenVINO** : [openvino.ai](https://openvino.ai/)
- **Répertoire GitHub** : [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Répertoire NNCF** : [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo** : [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Ressources d'apprentissage
- **Notebooks OpenVINO** : [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Guide de démarrage rapide** : [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Guide d'optimisation** : [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Outils d'intégration
- **Hugging Face Optimum Intel** : [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server** : [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI** : [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmarks de performance
- **Benchmarks officiels** : [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo** : [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Exemples communautaires
- **Notebooks Jupyter** : [Répertoire des notebooks OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Tutoriels complets disponibles dans le répertoire des notebooks OpenVINO
- **Applications d'exemple** : [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Exemples concrets pour divers domaines (vision par ordinateur, NLP, audio)
- **Articles de blog** : [Blog Intel AI](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Articles de blog Intel AI et communautaires avec des cas d'utilisation détaillés

### Outils associés
- **Intel Neural Compressor** : [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Techniques d'optimisation supplémentaires pour le matériel Intel
- **TensorFlow Lite** : [tensorflow.org/lite](https://www.tensorflow.org/lite) - Pour des comparaisons de déploiement mobile et en périphérie
- **ONNX Runtime** : [onnxruntime.ai](https://onnxruntime.ai/) - Alternatives pour moteur d'inférence multiplateforme

## ➡️ Et après

- [05 : Exploration approfondie du framework Apple MLX](./05.AppleMLX.md)

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.