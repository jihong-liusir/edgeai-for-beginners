<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:13:24+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "fr"
}
-->
# Section 4 : D√©ploiement - Mise en ≈ìuvre d'un mod√®le pr√™t pour la production

## Vue d'ensemble

Ce tutoriel complet vous guidera √† travers le processus de d√©ploiement de mod√®les quantifi√©s et ajust√©s √† l'aide de Foundry Local. Nous couvrirons la conversion du mod√®le, l'optimisation par quantification et la configuration du d√©ploiement de A √† Z.

## Pr√©requis

Avant de commencer, assurez-vous d'avoir les √©l√©ments suivants :

- ‚úÖ Un mod√®le ONNX ajust√© pr√™t pour le d√©ploiement
- ‚úÖ Un ordinateur Windows ou Mac
- ‚úÖ Python 3.10 ou une version ult√©rieure
- ‚úÖ Au moins 8 Go de RAM disponible
- ‚úÖ Foundry Local install√© sur votre syst√®me

## Partie 1 : Configuration de l'environnement

### Installation des outils n√©cessaires

Ouvrez votre terminal (Command Prompt sur Windows, Terminal sur Mac) et ex√©cutez les commandes suivantes dans l'ordre :

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Note importante** : Vous aurez √©galement besoin de CMake version 3.31 ou plus r√©cente, t√©l√©chargeable sur [cmake.org](https://cmake.org/download/).

## Partie 2 : Conversion et quantification du mod√®le

### Choisir le bon format

Pour les petits mod√®les de langage ajust√©s, nous recommandons d'utiliser le format **ONNX** car il offre :

- üöÄ Une meilleure optimisation des performances
- üîß Un d√©ploiement ind√©pendant du mat√©riel
- üè≠ Des capacit√©s pr√™tes pour la production
- üì± Une compatibilit√© multiplateforme

### M√©thode 1 : Conversion en une commande (recommand√©e)

Utilisez la commande suivante pour convertir directement votre mod√®le ajust√© :

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Explication des param√®tres :**
- `--model_name_or_path` : Chemin vers votre mod√®le ajust√©
- `--device cpu` : Utiliser le CPU pour l'optimisation
- `--precision int4` : Utiliser la quantification INT4 (r√©duction de taille d'environ 75 %)
- `--output_path` : Chemin de sortie pour le mod√®le converti

### M√©thode 2 : Approche avec fichier de configuration (utilisateurs avanc√©s)

Cr√©ez un fichier de configuration nomm√© `finetuned_conversion_config.json` :

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Puis ex√©cutez :

```bash
olive run --config ./finetuned_conversion_config.json
```

### Comparaison des options de quantification

| Pr√©cision | Taille du fichier | Vitesse d'inf√©rence | Qualit√© du mod√®le | Utilisation recommand√©e |
|-----------|--------------------|---------------------|-------------------|-------------------------|
| FP16      | Baseline √ó 0.5    | Rapide              | Meilleure         | Mat√©riel haut de gamme  |
| INT8      | Baseline √ó 0.25   | Tr√®s rapide         | Bonne             | Choix √©quilibr√©         |
| INT4      | Baseline √ó 0.125  | La plus rapide      | Acceptable        | Ressources limit√©es     |

üí° **Recommandation** : Commencez avec la quantification INT4 pour votre premier d√©ploiement. Si la qualit√© n'est pas satisfaisante, essayez INT8 ou FP16.

## Partie 3 : Configuration du d√©ploiement avec Foundry Local

### Cr√©ation de la configuration du mod√®le

Acc√©dez au r√©pertoire des mod√®les de Foundry Local :

```bash
foundry cache cd ./models/
```

Cr√©ez la structure de r√©pertoires pour votre mod√®le :

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Cr√©ez le fichier de configuration `inference_model.json` dans le r√©pertoire de votre mod√®le :

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configurations sp√©cifiques aux mod√®les

#### Pour les mod√®les de la s√©rie Qwen :

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Partie 4 : Test et optimisation du mod√®le

### V√©rification de l'installation du mod√®le

V√©rifiez si Foundry Local peut reconna√Ætre votre mod√®le :

```bash
foundry cache ls
```

Vous devriez voir `your-finetuned-model-int4` dans la liste.

### Lancement des tests du mod√®le

```bash
foundry model run your-finetuned-model-int4
```

### √âvaluation des performances

Surveillez les indicateurs cl√©s pendant les tests :

1. **Temps de r√©ponse** : Mesurez le temps moyen par r√©ponse
2. **Utilisation de la m√©moire** : Surveillez la consommation de RAM
3. **Utilisation du CPU** : V√©rifiez la charge du processeur
4. **Qualit√© de sortie** : √âvaluez la pertinence et la coh√©rence des r√©ponses

### Liste de validation de la qualit√©

- ‚úÖ Le mod√®le r√©pond correctement aux requ√™tes du domaine ajust√©
- ‚úÖ Le format des r√©ponses correspond √† la structure de sortie attendue
- ‚úÖ Pas de fuites de m√©moire lors d'une utilisation prolong√©e
- ‚úÖ Performances coh√©rentes pour diff√©rentes longueurs d'entr√©e
- ‚úÖ Gestion correcte des cas limites et des entr√©es invalides

## R√©sum√©

F√©licitations ! Vous avez r√©ussi √† accomplir :

- ‚úÖ La conversion du format du mod√®le ajust√©
- ‚úÖ L'optimisation par quantification du mod√®le
- ‚úÖ La configuration du d√©ploiement avec Foundry Local
- ‚úÖ L'ajustement des performances et le d√©pannage

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.