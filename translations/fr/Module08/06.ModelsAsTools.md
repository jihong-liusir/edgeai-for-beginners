<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T12:25:03+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "fr"
}
-->
# Session 6 : Foundry Local – Les modèles comme outils

## Aperçu

Traitez les modèles d'IA comme des outils modulaires et personnalisables qui fonctionnent directement sur l'appareil avec Foundry Local. Cette session met l'accent sur des workflows pratiques pour une inférence respectueuse de la vie privée et à faible latence, ainsi que sur l'intégration de ces outils via des SDK, des API ou une interface CLI. Vous apprendrez également à évoluer vers Azure AI Foundry lorsque cela est nécessaire.

Références :
- Documentation Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Intégration avec les SDK d'inférence : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Compiler des modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Objectifs d'apprentissage
- Concevoir des modèles comme des outils fonctionnant sur l'appareil
- Intégrer via une API REST compatible OpenAI ou des SDK
- Personnaliser les modèles pour des cas d'utilisation spécifiques au domaine
- Planifier une mise à l'échelle hybride vers Azure AI Foundry

## Partie 1 : Abstractions d'outils (Étape par étape)

Objectif : Représenter les modèles comme des outils avec des contrats clairs et un routeur simple.

Étape 1) Définir l'interface et le registre des outils  
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```
  
Étape 2) Implémenter deux outils basés sur Foundry Local  
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```
  
Étape 3) Routeur par tâche  
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```
  

## Partie 2 : Intégration SDK et API (Étape par étape)

Objectif : Utiliser le SDK Python OpenAI avec le point de terminaison Foundry Local.

Étape 1) Installer  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```
  
Étape 2) Configurer les variables d'environnement  
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```
  
Étape 3) Appeler l'API de chat  
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```
  

## Partie 3 : Personnalisation de domaine (Étape par étape)

Objectif : Adapter les résultats à un domaine en utilisant des modèles de prompts et un schéma JSON.

Étape 1) Créer un modèle de prompt pour le domaine  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Étape 2) Imposer une sortie JSON  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Partie 4 : Mode hors ligne et posture de sécurité (Étape par étape)

Objectif : Garantir la confidentialité et la résilience lors de l'exécution des modèles comme outils localement.

Étape 1) Pré-chauffer et valider le point de terminaison local  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Étape 2) Assainir les entrées  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Étape 3) Activer le mode local uniquement et la journalisation  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Partie 5 : Mise à l'échelle vers Azure AI Foundry (Étape par étape)

Objectif : Répliquer les modèles locaux avec des points de terminaison Azure pour une capacité supplémentaire.

Étape 1) Décider de la stratégie de routage  
- Priorité au local pour la confidentialité/la latence, recours à Azure en cas d'erreurs ou de prompts volumineux  

Étape 2) Implémenter un simple routeur de base  
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```
  

## Liste de vérification pratique
- [ ] Enregistrer au moins deux outils et router les requêtes
- [ ] Appeler Foundry Local via le SDK OpenAI et REST brut
- [ ] Imposer des sorties JSON pour un modèle de domaine
- [ ] Assainir et journaliser les appels localement
- [ ] Implémenter un routeur hybride simple avec recours à Azure

## Conclusion

Foundry Local permet une IA robuste sur l'appareil où les modèles deviennent des outils composables. Avec des interfaces claires, une gouvernance et une mise à l'échelle hybride, les équipes peuvent déployer des applications d'IA en temps réel, sécurisées, respectueuses de la vie privée et prêtes pour l'entreprise.

---

