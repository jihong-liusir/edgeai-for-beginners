<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T12:24:21+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "fr"
}
-->
# Session 1 : Premiers pas avec Foundry Local

## Vue d'ensemble

Microsoft Foundry Local apporte les capacités d'Azure AI Foundry directement dans votre environnement de développement Windows 11, permettant un développement IA à faible latence et respectueux de la confidentialité avec des outils de niveau entreprise. Cette session couvre l'installation complète, la configuration et le déploiement pratique de modèles populaires tels que phi, qwen, deepseek et GPT-OSS-20B.

## Objectifs d'apprentissage

À la fin de cette session, vous serez capable de :
- Installer et configurer Foundry Local sur Windows 11
- Maîtriser les commandes CLI et les options de configuration
- Comprendre les stratégies de mise en cache des modèles pour des performances optimales
- Exécuter avec succès les modèles phi, qwen, deepseek et GPT-OSS-20B
- Créer votre première application IA avec Foundry Local

## Prérequis

### Configuration système requise
- **Windows 11** : Version 22H2 ou ultérieure
- **RAM** : 16 Go minimum, 32 Go recommandés
- **Stockage** : 50 Go d'espace libre pour les modèles et le cache
- **Matériel** : Appareil avec NPU ou GPU recommandé (PC Copilot+ ou GPU NVIDIA)
- **Réseau** : Connexion internet haut débit pour télécharger les modèles

### Environnement de développement
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## Partie 1 : Installation et configuration

### Étape 1 : Installer Foundry Local

Installez Foundry Local en utilisant Winget ou téléchargez l'installateur depuis GitHub :

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Étape 2 : Vérifier l'installation

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Partie 2 : Comprendre le CLI

### Structure des commandes principales

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Partie 3 : Gestion et mise en cache des modèles

Foundry Local utilise une mise en cache intelligente des modèles pour optimiser les performances et l'utilisation du stockage :

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Partie 4 : Déploiement pratique des modèles

### Exécution des modèles Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Travailler avec les modèles Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### Exécution des modèles DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### Exécution de GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Partie 5 : Créer votre première application

### Interface de chat simple (API compatible OpenAI)

Créez une application de chat basique en utilisant l'API REST compatible OpenAI de Foundry Local. Assurez-vous qu'un modèle est en cours d'exécution dans un autre terminal.

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### Exécuter l'application de chat

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## Partie 6 : Dépannage et bonnes pratiques

### Problèmes courants et solutions

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### Surveillance des ressources système (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Bonnes pratiques

- Privilégiez les commandes `foundry model ...`, `foundry cache ...` et `foundry service ...` (voir la référence CLI)
- Effectuez des mises à jour régulières pour accéder aux nouveaux modèles et correctifs
- Commencez avec des modèles plus petits (Phi mini, Qwen 7B) avant de passer à des modèles plus grands
- Surveillez l'utilisation du CPU/GPU/mémoire lors de l'ajustement des invites et des paramètres

## Partie 7 : Exercices pratiques

### Exercice 1 : Exécutions rapides de plusieurs modèles

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Exercice 2 : Benchmark de latence de base

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## Références

- Premiers pas avec Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- Référence CLI et aperçu des commandes : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- Compiler des modèles Hugging Face pour Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- GitHub de Microsoft Foundry Local : https://github.com/microsoft/Foundry-Local

---

