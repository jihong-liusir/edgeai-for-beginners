<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "382a763fcea7087e68a94c26216e5e70",
  "translation_date": "2025-09-22T12:22:28+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "fr"
}
-->
# Session 5 : Créez rapidement des agents alimentés par l'IA avec Foundry Local

Note : Les capacités des agents dans Foundry Local évoluent—vérifiez les notes de version les plus récentes avant de mettre en œuvre des modèles avancés.

## Aperçu

Utilisez Foundry Local pour prototyper rapidement des applications agentiques : invites système, ancrage et modèles d'orchestration. Lorsque le support des agents est disponible, vous pouvez standardiser l'appel de fonctions compatibles avec OpenAI ou utiliser Azure AI Agents côté cloud dans des conceptions hybrides.

Références :
- Documentation Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Agents Azure AI Foundry : https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Exemple d'appel de fonction (exemples Foundry Local) : https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## Objectifs d'apprentissage
- Concevoir des invites système et des stratégies d'ancrage pour un comportement fiable
- Implémenter des modèles d'appel de fonctions (utilisation d'outils)
- Orchestrer des workflows multi-agents (local et hybride)
- Planifier l'observabilité et la sécurité

## Partie 1 : Invites système et ancrage

- Définir des rôles stricts, des contraintes et des schémas de sortie
- Ancrer les réponses avec des données locales ou d'entreprise
- Imposer des sorties JSON pour l'automatisation en aval

## Partie 2 : Appel de fonctions (compatible OpenAI)

```python
# tools.py
import json

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

FUNCTIONS = [
    {
        "name": "get_weather",
        "description": "Get current weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "City name"}
            },
            "required": ["city"]
        }
    }
]
```

```python
# agent.py
import requests
import json
from tools import FUNCTIONS, get_weather

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def call_model(messages, functions=None):
    payload = {
        "model": MODEL,
        "messages": messages,
        "functions": functions,
        "function_call": "auto"
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    return r.json()

messages = [{"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": "What's the weather in Paris?"}]

resp = call_model(messages, functions=FUNCTIONS)
choice = resp["choices"][0]["message"]

if "function_call" in choice:
    fc = choice["function_call"]
    if fc["name"] == "get_weather":
        args = json.loads(fc["arguments"])
        result = get_weather(args["city"])
        messages.append(choice)
        messages.append({"role": "function", "name": "get_weather", "content": result})
        final = call_model(messages)
        print(final["choices"][0]["message"]["content"]) 
else:
    print(choice.get("content"))
```

Exécuter :
```powershell
# Ensure a model is running
foundry model run phi-4-mini
python agent.py
```


## Partie 3 : Orchestration multi-agents (modèle)

Concevez un coordinateur qui attribue des tâches à des agents spécialisés (récupération, raisonnement, exécution) en utilisant le point de terminaison compatible OpenAI de Foundry Local.

Étape 1) Définir les agents spécialisés  
```python
# agents/specialists.py
import requests
BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

headers = {"Content-Type": "application/json", "Authorization": "Bearer local-key"}

def chat(messages, max_tokens=300, temperature=0.4):
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json={
        "model": MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }, headers=headers, timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

class RetrievalAgent:
    SYSTEM = "You retrieve relevant snippets from knowledge sources based on a query."
    def run(self, query: str) -> str:
        # Placeholder: in real use, fetch from local files or vector DB
        messages = [{"role": "system", "content": self.SYSTEM},
                    {"role": "user", "content": f"Retrieve key facts for: {query}"}]
        return chat(messages)

class ReasoningAgent:
    SYSTEM = "You analyze inputs step by step and produce structured conclusions."
    def run(self, context: str, question: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\nThink step-by-step and produce a concise answer."}
        ]
        return chat(messages)

class ExecutionAgent:
    SYSTEM = "You transform decisions into actionable steps (JSON with actions)."
    def run(self, decision: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Turn this decision into 3 executable steps as JSON:\n{decision}"}
        ]
        return chat(messages)
```
  
Étape 2) Construire le coordinateur  
```python
# agents/coordinator.py
from agents.specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent

class Coordinator:
    def __init__(self):
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()

    def handle(self, user_goal: str) -> dict:
        # 1. Retrieve context
        context = self.retrieval.run(user_goal)
        # 2. Reason on context
        decision = self.reasoning.run(context, user_goal)
        # 3. Produce actionable steps
        actions = self.execution.run(decision)
        return {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions
        }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    coord = Coordinator()
    result = coord.handle("Create a plan to onboard 5 new customers this month")
    print(result)
```
  
Étape 3) Valider avec Foundry Local  
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator
python -m samples.05.agents.coordinator
```
  

Directives :
- Implémenter des mécanismes de reprise et des délais entre les agents
- Ajouter un petit stockage en mémoire (dict) pour l'état des conversations/threads
- Introduire une limitation de débit lors de l'enchaînement de plusieurs appels

## Partie 4 : Observabilité et sécurité

Suivez les invites, les réponses et les erreurs localement, tout en appliquant une hygiène des données dans votre pile d'agents.

Étape 1) Journalisation légère des requêtes (optionnel)

Note : L'assistant suivant n'est pas inclus par défaut. Créez `infra/obs.py` si vous souhaitez une journalisation JSON locale pour vos expériences.  
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```
  

Intégrer la journalisation dans les agents (optionnel) :  
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```
  

Étape 2) Valider la disponibilité et la santé de base via CLI  
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```
  

Étape 3) Redaction et hygiène des données sensibles  
- Avant d'envoyer des messages au modèle, supprimez ou hachez les champs sensibles (emails, numéros de téléphone, identifiants)  
- Conservez les données sources brutes sur l'appareil, ne transmettez que les chaînes de contexte nécessaires  

Exemple d'assistant de redaction :  
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  

Utilisation dans les agents :  
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```
  

Étape 4) Disjoncteurs et gestion des erreurs  
- Enveloppez chaque appel d'agent avec try/except et un backoff exponentiel  
- Interrompez le pipeline en cas d'échecs répétés  

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```
  

Étape 5) Traçabilité locale et exportation  
- Stockez les journaux JSON sous `./agent_logs`  
- Compressez et faites tourner les journaux périodiquement  
- Exportez des résumés pour les revues (comptes, latence moyenne, taux d'erreur)  

Étape 6) Vérification croisée avec la documentation Microsoft Learn  
- Foundry Local fournit une API compatible OpenAI (validée avec `curl /v1/models`)  
- Utilisez `foundry model run <name>` pour confirmer la disponibilité du modèle  
- Suivez les directives officielles pour l'intégration client et les applications d'exemple (Open WebUI/how-tos)  

Références  
- Foundry Local (Learn) : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
- Open WebUI how-to : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
- Exemple d'appel de fonction : https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling  

## Prochaines étapes
- Explorez Azure AI Agents pour l'orchestration hébergée sur le cloud  
- Ajoutez des connecteurs d'entreprise (Microsoft Graph, Recherche, bases de données)  

---

