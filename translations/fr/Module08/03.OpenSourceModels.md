<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T10:18:58+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fr"
}
-->
# Session 3 : Découverte et gestion des modèles open-source

## Aperçu

Cette session se concentre sur la découverte et la gestion pratiques des modèles avec Foundry Local. Vous apprendrez à lister les modèles disponibles, tester différentes options et comprendre les caractéristiques de performance de base. L'approche met l'accent sur l'exploration pratique avec le CLI de Foundry pour vous aider à sélectionner les modèles adaptés à vos cas d'utilisation.

## Objectifs d'apprentissage

- Maîtriser les commandes du CLI de Foundry pour la découverte et la gestion des modèles
- Comprendre les mécanismes de cache des modèles et les schémas de stockage local
- Apprendre à tester et comparer rapidement différents modèles
- Établir des workflows pratiques pour la sélection et le benchmarking des modèles
- Explorer l'écosystème croissant de modèles disponibles via Foundry Local

## Prérequis

- Avoir terminé la Session 1 : Premiers pas avec Foundry Local
- CLI de Foundry Local installé et accessible
- Espace de stockage suffisant pour télécharger les modèles (les modèles peuvent varier de 1 Go à plus de 20 Go)
- Compréhension de base des types de modèles et de leurs cas d'utilisation

## Partie 6 : Exercice pratique

### Exercice : Découverte et comparaison des modèles

Créez votre propre script d'évaluation de modèles basé sur l'exemple 03 :

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Votre tâche

1. **Exécutez le script Sample 03** : `samples\03\list_and_bench.cmd`
2. **Essayez différents modèles** : Testez au moins 3 modèles différents
3. **Comparez les performances** : Notez les différences de vitesse et de qualité des réponses
4. **Documentez vos observations** : Créez un tableau de comparaison simple

### Format de comparaison exemple

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Partie 7 : Dépannage et bonnes pratiques

### Problèmes courants et solutions

**Le modèle ne démarre pas :**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Mémoire insuffisante :**
- Commencez avec des modèles plus petits (`phi-4-mini`)
- Fermez les autres applications
- Mettez à niveau la RAM si vous atteignez fréquemment des limites

**Performances lentes :**
- Assurez-vous que le modèle est entièrement chargé (vérifiez la sortie détaillée)
- Fermez les applications inutiles en arrière-plan
- Envisagez un stockage plus rapide (SSD)

### Bonnes pratiques

1. **Commencez petit** : Utilisez `phi-4-mini` pour valider la configuration
2. **Un modèle à la fois** : Arrêtez les modèles précédents avant d'en démarrer de nouveaux
3. **Surveillez les ressources** : Gardez un œil sur l'utilisation de la mémoire
4. **Testez de manière cohérente** : Utilisez les mêmes invites pour des comparaisons équitables
5. **Documentez les résultats** : Prenez des notes sur les performances des modèles pour vos cas d'utilisation

## Partie 8 : Prochaines étapes et références

### Préparation pour la Session 4

- **Focus de la Session 4** : Outils et techniques d'optimisation
- **Prérequis** : Être à l'aise avec le changement de modèles et les tests de performance de base
- **Recommandé** : Identifier 2-3 modèles favoris à partir de cette session

### Ressources supplémentaires

- **[Documentation Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)** : Documentation officielle
- **[Référence CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)** : Référence complète des commandes
- **[Model Mondays](https://aka.ms/model-mondays)** : Présentation hebdomadaire des modèles
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)** : Communauté et problèmes
- **[Exemple 03 : Découverte de modèles](samples/03/README.md)** : Script d'exemple pratique

### Points clés à retenir

✅ **Découverte de modèles** : Utilisez `foundry model list` pour explorer les modèles disponibles  
✅ **Tests rapides** : Le modèle `list_and_bench.cmd` pour une évaluation rapide  
✅ **Surveillance des performances** : Utilisation de base des ressources et mesure des temps de réponse  
✅ **Sélection de modèles** : Lignes directrices pratiques pour choisir des modèles selon les cas d'utilisation  
✅ **Gestion du cache** : Comprendre les procédures de stockage et de nettoyage  

Vous avez maintenant les compétences pratiques pour découvrir, tester et sélectionner des modèles adaptés à vos applications d'IA en utilisant l'approche simple du CLI de Foundry Local.

## Objectifs d'apprentissage

- Découvrir et évaluer des modèles open-source pour l'inférence locale
- Compiler et exécuter des modèles Hugging Face sélectionnés dans Foundry Local
- Appliquer des stratégies de sélection de modèles en fonction de la précision, de la latence et des besoins en ressources
- Gérer les modèles localement avec le cache et le versioning

## Partie 1 : Découverte de modèles avec le CLI de Foundry

### Commandes de gestion de modèles de base

Le CLI de Foundry fournit des commandes simples pour la découverte et la gestion des modèles :

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Exécution de vos premiers modèles

Commencez avec des modèles populaires et bien testés pour comprendre leurs caractéristiques de performance :

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Note :** Le flag `--verbose` fournit des informations détaillées sur le démarrage, notamment :
- Progression du téléchargement du modèle (lors de la première exécution)
- Détails sur l'allocation de mémoire
- Informations sur la liaison des services
- Métriques d'initialisation des performances

### Comprendre les catégories de modèles

**Petits modèles de langage (SLMs) :**
- `phi-4-mini` : Rapide, efficace, idéal pour les discussions générales
- `phi-4` : Version plus performante avec un meilleur raisonnement

**Modèles moyens :**
- `qwen2.5-7b-instruct` : Excellent raisonnement et contexte plus long
- `deepseek-r1-distill-qwen-7b` : Optimisé pour la génération de code

**Modèles plus grands :**
- `llama-3.2` : Dernier modèle open-source de Meta
- `qwen2.5-14b-instruct` : Raisonnement de niveau entreprise

## Partie 2 : Tests rapides et comparaison de modèles

### Approche Sample 03 : Liste et benchmark simples

Basé sur le modèle Sample 03, voici le workflow minimal :

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Tester les performances des modèles

Une fois qu'un modèle est en cours d'exécution, testez-le avec des invites cohérentes :

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternative de test avec PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Partie 3 : Gestion du cache et du stockage des modèles

### Comprendre le cache des modèles

Foundry Local gère automatiquement les téléchargements et le cache des modèles :

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Considérations sur le stockage des modèles

**Tailles typiques des modèles :**
- `phi-4-mini` : ~2,5 Go
- `qwen2.5-7b-instruct` : ~4,1 Go  
- `deepseek-r1-distill-qwen-7b` : ~4,3 Go
- `llama-3.2` : ~4,9 Go
- `qwen2.5-14b-instruct` : ~8,2 Go

**Bonnes pratiques de stockage :**
- Gardez 2-3 modèles en cache pour un changement rapide
- Supprimez les modèles inutilisés pour libérer de l'espace : `foundry cache clean`
- Surveillez l'utilisation du disque, surtout sur les SSD de petite taille
- Envisagez les compromis entre taille et capacité des modèles

### Surveillance des performances des modèles

Pendant l'exécution des modèles, surveillez les ressources système :

**Gestionnaire de tâches Windows :**
- Surveillez l'utilisation de la mémoire (les modèles restent chargés en RAM)
- Suivez l'utilisation du CPU pendant l'inférence
- Vérifiez les entrées/sorties du disque lors du chargement initial des modèles

**Surveillance en ligne de commande :**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Partie 4 : Lignes directrices pratiques pour la sélection de modèles

### Choisir des modèles selon les cas d'utilisation

**Pour les discussions générales et les questions/réponses :**
- Commencez avec : `phi-4-mini` (rapide, efficace)
- Passez à : `phi-4` (meilleur raisonnement)
- Avancé : `qwen2.5-7b-instruct` (contexte plus long)

**Pour la génération de code :**
- Recommandé : `deepseek-r1-distill-qwen-7b`
- Alternative : `qwen2.5-7b-instruct` (également bon pour le code)

**Pour un raisonnement complexe :**
- Meilleur choix : `qwen2.5-7b-instruct` ou `qwen2.5-14b-instruct`
- Option économique : `phi-4`

### Guide des exigences matérielles

**Configuration minimale requise :**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Recommandé pour des performances optimales :**
- RAM de 32 Go ou plus pour un changement confortable entre plusieurs modèles
- Stockage SSD pour un chargement plus rapide des modèles
- CPU moderne avec de bonnes performances en mono-thread
- Support NPU (PCs Windows 11 Copilot+) pour l'accélération

### Workflow de changement de modèles

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Partie 5 : Benchmarking simple des modèles

### Test de performance de base

Voici une approche simple pour comparer les performances des modèles :

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Évaluation manuelle de la qualité

Pour chaque modèle, testez avec des invites cohérentes et évaluez manuellement :

**Prompts de test :**
1. "Expliquez l'informatique quantique en termes simples."
2. "Écrivez une fonction Python pour trier une liste."
3. "Quels sont les avantages et les inconvénients du travail à distance ?"
4. "Résumez les avantages de l'IA en périphérie."

**Critères d'évaluation :**
- **Précision** : Les informations sont-elles correctes ?
- **Clarté** : L'explication est-elle facile à comprendre ?
- **Exhaustivité** : Répond-elle entièrement à la question ?
- **Vitesse** : Quelle est la rapidité de la réponse ?

### Surveillance de l'utilisation des ressources

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Partie 6 : Prochaines étapes

- Abonnez-vous à Model Mondays pour découvrir de nouveaux modèles et astuces : https://aka.ms/model-mondays
- Contribuez à `models.json` de votre équipe avec vos observations
- Préparez-vous pour la Session 4 : comparaison entre LLMs et SLMs, inférence locale vs cloud, et démonstrations pratiques

---

