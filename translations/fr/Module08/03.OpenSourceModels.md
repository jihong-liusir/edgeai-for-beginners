<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T22:54:00+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fr"
}
-->
# Session 3 : Découverte et gestion des modèles open-source

## Aperçu

Cette session se concentre sur la découverte et la gestion pratiques des modèles avec Foundry Local. Vous apprendrez à lister les modèles disponibles, tester différentes options et comprendre les caractéristiques de performance de base. L'approche met l'accent sur l'exploration pratique avec le CLI de Foundry pour vous aider à sélectionner les modèles adaptés à vos cas d'utilisation.

## Objectifs d'apprentissage

- Maîtriser les commandes du CLI de Foundry pour la découverte et la gestion des modèles
- Comprendre les modèles de cache et de stockage local des modèles
- Apprendre à tester et comparer rapidement différents modèles
- Établir des workflows pratiques pour la sélection et le benchmarking des modèles
- Explorer l'écosystème croissant de modèles disponibles via Foundry Local

## Prérequis

- Avoir terminé la Session 1 : Premiers pas avec Foundry Local
- CLI de Foundry Local installé et accessible
- Espace de stockage suffisant pour télécharger les modèles (les modèles peuvent varier de 1 Go à plus de 20 Go)
- Compréhension de base des types de modèles et des cas d'utilisation

## Aperçu

Cette session explore comment intégrer des modèles open-source à Foundry Local.

## Partie 6 : Exercice pratique

### Exercice : Découverte et comparaison des modèles

Créez votre propre script d'évaluation de modèles basé sur l'exemple 03 :

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### Votre tâche

1. **Exécutez le script Sample 03** : `samples\03\list_and_bench.cmd`
2. **Essayez différents modèles** : Testez au moins 3 modèles différents
3. **Comparez les performances** : Notez les différences de vitesse et de qualité des réponses
4. **Documentez vos observations** : Créez un tableau de comparaison simple

### Format de comparaison exemple

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## Partie 7 : Dépannage et bonnes pratiques

### Problèmes courants et solutions

**Le modèle ne démarre pas :**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**Mémoire insuffisante :**
- Commencez avec des modèles plus petits (`phi-4-mini`)
- Fermez les autres applications
- Augmentez la RAM si vous atteignez fréquemment des limites

**Performances lentes :**
- Assurez-vous que le modèle est entièrement chargé (vérifiez la sortie détaillée)
- Fermez les applications en arrière-plan inutiles
- Envisagez un stockage plus rapide (SSD)

### Bonnes pratiques

1. **Commencez petit** : Utilisez `phi-4-mini` pour valider la configuration
2. **Un modèle à la fois** : Arrêtez les modèles précédents avant d'en démarrer de nouveaux
3. **Surveillez les ressources** : Gardez un œil sur l'utilisation de la mémoire
4. **Testez de manière cohérente** : Utilisez les mêmes invites pour des comparaisons équitables
5. **Documentez les résultats** : Prenez des notes sur les performances des modèles pour vos cas d'utilisation

## Partie 8 : Prochaines étapes et références

### Préparation pour la Session 4

- **Focus de la Session 4** : Outils et techniques d'optimisation
- **Prérequis** : Être à l'aise avec le changement de modèles et les tests de performance de base
- **Recommandé** : Identifier 2-3 modèles favoris à partir de cette session

### Ressources supplémentaires

- **[Documentation Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)** : Documentation officielle
- **[Référence CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)** : Référence complète des commandes
- **[Model Mondays](https://aka.ms/model-mondays)** : Présentation hebdomadaire des modèles
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)** : Communauté et problèmes
- **[Exemple 03 : Découverte de modèles](samples/03/README.md)** : Script d'exemple pratique

### Points clés à retenir

✅ **Découverte de modèles** : Utilisez `foundry model list` pour explorer les modèles disponibles  
✅ **Tests rapides** : Le modèle `list_and_bench.cmd` pour une évaluation rapide  
✅ **Surveillance des performances** : Mesure de l'utilisation des ressources et du temps de réponse  
✅ **Sélection de modèles** : Lignes directrices pratiques pour choisir des modèles selon les cas d'utilisation  
✅ **Gestion du cache** : Comprendre les procédures de stockage et de nettoyage  

Vous avez maintenant les compétences pratiques pour découvrir, tester et sélectionner des modèles adaptés à vos applications d'IA en utilisant l'approche simple du CLI de Foundry Local : sélection de modèles communautaires, intégration de contenu Hugging Face et adoption de stratégies "apportez votre propre modèle" (BYOM). Vous découvrirez également la série Model Mondays pour un apprentissage continu et la découverte de modèles.

Références :
- Documentation Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compiler des modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays : https://aka.ms/model-mondays
- Foundry Local GitHub : https://github.com/microsoft/Foundry-Local

## Objectifs d'apprentissage

- Découvrir et évaluer des modèles open-source pour l'inférence locale
- Compiler et exécuter des modèles Hugging Face sélectionnés dans Foundry Local
- Appliquer des stratégies de sélection de modèles pour l'exactitude, la latence et les besoins en ressources
- Gérer les modèles localement avec le cache et le versioning

## Partie 1 : Découverte de modèles avec le CLI de Foundry

### Commandes de gestion de modèles de base

Le CLI de Foundry fournit des commandes simples pour la découverte et la gestion des modèles :

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### Exécution de vos premiers modèles

Commencez avec des modèles populaires et bien testés pour comprendre les caractéristiques de performance :

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**Note :** L'option `--verbose` fournit des informations détaillées sur le démarrage, notamment :
- Progression du téléchargement du modèle (lors de la première exécution)
- Détails de l'allocation de mémoire
- Informations sur la liaison de service
- Métriques d'initialisation des performances

### Comprendre les catégories de modèles

**Petits modèles de langage (SLMs) :**
- `phi-4-mini` : Rapide, efficace, idéal pour les discussions générales
- `phi-4` : Version plus performante avec un meilleur raisonnement

**Modèles moyens :**
- `qwen2.5-7b` : Excellent raisonnement et contexte plus long
- `deepseek-r1-7b` : Optimisé pour la génération de code

**Modèles plus grands :**
- `llama-3.2` : Dernier modèle open-source de Meta
- `qwen2.5-14b` : Raisonnement de niveau entreprise

## Partie 2 : Tests rapides et comparaison de modèles

### Approche Sample 03 : Liste et benchmark simples

Basé sur notre modèle Sample 03, voici le workflow minimal :

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### Tester les performances des modèles

Une fois un modèle en cours d'exécution, testez-le avec des invites cohérentes :

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### Alternative de test avec PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## Partie 3 : Gestion du cache et du stockage des modèles

### Comprendre le cache des modèles

Foundry Local gère automatiquement les téléchargements et le cache des modèles :

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### Considérations sur le stockage des modèles

**Tailles typiques des modèles :**
- `phi-4-mini` : ~2,5 Go
- `qwen2.5-7b` : ~4,1 Go  
- `deepseek-r1-7b` : ~4,3 Go
- `llama-3.2` : ~4,9 Go
- `qwen2.5-14b` : ~8,2 Go

**Bonnes pratiques de stockage :**
- Gardez 2-3 modèles en cache pour un changement rapide
- Supprimez les modèles inutilisés pour libérer de l'espace : `foundry cache clean`
- Surveillez l'utilisation du disque, surtout sur les SSD plus petits
- Envisagez le compromis entre taille et capacité des modèles

### Surveillance des performances des modèles

Pendant l'exécution des modèles, surveillez les ressources système :

**Gestionnaire de tâches Windows :**
- Surveillez l'utilisation de la mémoire (les modèles restent chargés en RAM)
- Surveillez l'utilisation du CPU pendant l'inférence
- Vérifiez les E/S disque lors du chargement initial des modèles

**Surveillance en ligne de commande :**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## Partie 4 : Lignes directrices pratiques pour la sélection de modèles

### Choisir des modèles selon les cas d'utilisation

**Pour les discussions générales et les questions-réponses :**
- Commencez avec : `phi-4-mini` (rapide, efficace)
- Passez à : `phi-4` (meilleur raisonnement)
- Avancé : `qwen2.5-7b` (contexte plus long)

**Pour la génération de code :**
- Recommandé : `deepseek-r1-7b`
- Alternative : `qwen2.5-7b` (également bon pour le code)

**Pour un raisonnement complexe :**
- Meilleur choix : `qwen2.5-7b` ou `qwen2.5-14b`
- Option économique : `phi-4`

### Guide des exigences matérielles

**Exigences minimales du système :**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**Recommandé pour des performances optimales :**
- RAM de 32 Go ou plus pour un changement confortable entre plusieurs modèles
- Stockage SSD pour un chargement plus rapide des modèles
- CPU moderne avec de bonnes performances en mono-thread
- Support NPU (PCs Windows 11 Copilot+) pour l'accélération

### Workflow de changement de modèles

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## Partie 5 : Benchmarking simple des modèles

### Test de performance de base

Voici une approche simple pour comparer les performances des modèles :

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### Évaluation manuelle de la qualité

Pour chaque modèle, testez avec des invites cohérentes et évaluez manuellement :

**Prompts de test :**
1. "Expliquez l'informatique quantique en termes simples."
2. "Écrivez une fonction Python pour trier une liste."
3. "Quels sont les avantages et les inconvénients du travail à distance ?"
4. "Résumez les avantages de l'IA en périphérie."

**Critères d'évaluation :**
- **Exactitude** : Les informations sont-elles correctes ?
- **Clarté** : L'explication est-elle facile à comprendre ?
- **Complétude** : Répond-elle entièrement à la question ?
- **Vitesse** : Quelle est la rapidité de la réponse ?

### Surveillance de l'utilisation des ressources

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## Partie 6 : Prochaines étapes

- Abonnez-vous à Model Mondays pour découvrir de nouveaux modèles et astuces : https://aka.ms/model-mondays
- Contribuez à votre fichier `models.json` d'équipe avec vos observations
- Préparez-vous pour la Session 4 : comparaison des LLMs vs SLMs, inférence locale vs cloud, et démonstrations pratiques

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.