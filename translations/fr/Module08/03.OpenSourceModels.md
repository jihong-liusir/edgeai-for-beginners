<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T12:23:47+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fr"
}
-->
# Session 3 : Modèles Open Source avec Foundry Local

## Aperçu

Cette session explore comment intégrer des modèles open source à Foundry Local : sélectionner des modèles communautaires, intégrer du contenu Hugging Face et adopter des stratégies « apportez votre propre modèle » (BYOM). Vous découvrirez également la série Model Mondays pour un apprentissage continu et la découverte de nouveaux modèles.

Références :
- Documentation Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compiler des modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays : https://aka.ms/model-mondays
- GitHub Foundry Local : https://github.com/microsoft/Foundry-Local

## Objectifs d'apprentissage
- Découvrir et évaluer des modèles open source pour une inférence locale
- Compiler et exécuter des modèles Hugging Face sélectionnés dans Foundry Local
- Appliquer des stratégies de sélection de modèles en fonction de la précision, de la latence et des besoins en ressources
- Gérer les modèles localement avec le cache et le versioning

## Partie 1 : Découverte et sélection de modèles (Étape par étape)

Étape 1) Lister les modèles disponibles dans le catalogue local  
```cmd
foundry model list
```
  
Étape 2) Essayer rapidement deux candidats (téléchargement automatique lors de la première exécution)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Étape 3) Noter les métriques de base  
- Observer la latence (subjective) et la qualité pour un prompt fixe  
- Surveiller l'utilisation de la mémoire via le Gestionnaire des tâches pendant l'exécution de chaque modèle  

## Partie 2 : Exécution de modèles du catalogue via CLI (Étape par étape)

Étape 1) Démarrer un modèle  
```cmd
foundry model run llama-3.2
```
  
Étape 2) Envoyer un prompt de test via l'endpoint compatible OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Partie 3 : BYOM – Compiler des modèles Hugging Face (Étape par étape)

Suivez le guide officiel pour compiler des modèles. Aperçu général ci-dessous — consultez l'article Microsoft Learn pour les commandes exactes et les configurations prises en charge.

Étape 1) Préparer un répertoire de travail  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Étape 2) Compiler un modèle Hugging Face pris en charge  
- Suivez les étapes du guide pour convertir et placer le modèle ONNX compilé dans votre répertoire `models`  
- Confirmez avec :  
```cmd
foundry cache ls
```
  
Vous devriez voir le nom de votre modèle compilé (par exemple, `llama-3.2`).  

Étape 3) Exécuter le modèle compilé  
```cmd
foundry model run llama-3.2 --verbose
```
  
Notes :  
- Assurez-vous d'avoir suffisamment d'espace disque et de RAM pour la compilation et l'exécution  
- Commencez par des modèles plus petits pour valider le processus, puis passez à des modèles plus grands  

## Partie 4 : Organisation pratique des modèles (Étape par étape)

Étape 1) Créer un registre `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Étape 2) Script de sélection simple  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Partie 5 : Benchmarks pratiques (Étape par étape)

Étape 1) Benchmark simple de latence  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Étape 2) Vérification rapide de la qualité  
- Utilisez un ensemble de prompts fixes, capturez les résultats dans un fichier CSV/JSON  
- Évaluez manuellement la fluidité, la pertinence et la justesse (1–5)  

## Partie 6 : Prochaines étapes
- Abonnez-vous à Model Mondays pour découvrir de nouveaux modèles et astuces : https://aka.ms/model-mondays  
- Contribuez à `models.json` de votre équipe avec vos découvertes  
- Préparez-vous pour la Session 4 : comparaison entre LLMs et SLMs, inférence locale vs cloud, et démonstrations pratiques  

---

