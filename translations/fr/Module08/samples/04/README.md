<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f1754a482b6a84e07287a5b775e65b6",
  "translation_date": "2025-09-30T22:55:54+00:00",
  "source_file": "Module08/samples/04/README.md",
  "language_code": "fr"
}
-->
# Exemple 04 : Applications de chat en production avec Chainlit

Un exemple complet d√©montrant plusieurs approches pour cr√©er des applications de chat pr√™tes pour la production en utilisant Microsoft Foundry Local, avec des interfaces web modernes, des r√©ponses en streaming et des technologies de navigateur de pointe.

## Contenu inclus

- **üöÄ Application de chat Chainlit** (`app.py`) : Application de chat pr√™te pour la production avec streaming
- **üåê D√©mo WebGPU** (`webgpu-demo/`) : Inf√©rence IA bas√©e sur le navigateur avec acc√©l√©ration mat√©rielle
- **üé® Int√©gration Open WebUI** (`open-webui-guide.md`) : Interface professionnelle similaire √† ChatGPT
- **üìö Notebook √©ducatif** (`chainlit_app.ipynb`) : Mat√©riel d'apprentissage interactif

## D√©marrage rapide

### 1. Application de chat Chainlit

```cmd
# Navigate to Module08 directory
cd Module08

# Start your model
foundry model run phi-4-mini

# Run Chainlit app (using port 8080 to avoid conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Accessible √† : `http://localhost:8080`

### 2. D√©mo WebGPU dans le navigateur

```cmd
# Navigate to WebGPU demo
cd Module08\samples\04\webgpu-demo

# Serve the demo
python -m http.server 5173
```

Accessible √† : `http://localhost:5173`

### 3. Configuration Open WebUI

```cmd
# Run Open WebUI with Docker
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

Accessible √† : `http://localhost:3000`

## Mod√®les d'architecture

### Matrice de d√©cision Local vs Cloud

| Sc√©nario | Recommandation | Raison |
|----------|----------------|--------|
| **Donn√©es sensibles** | üè† Local (Foundry) | Les donn√©es ne quittent jamais l'appareil |
| **Raisonnement complexe** | ‚òÅÔ∏è Cloud (Azure OpenAI) | Acc√®s √† des mod√®les plus grands |
| **Chat en temps r√©el** | üè† Local (Foundry) | Latence r√©duite, r√©ponses plus rapides |
| **Analyse de documents** | üîÑ Hybride | Extraction locale, analyse dans le cloud |
| **G√©n√©ration de code** | üè† Local (Foundry) | Confidentialit√© + mod√®les sp√©cialis√©s |
| **T√¢ches de recherche** | ‚òÅÔ∏è Cloud (Azure OpenAI) | N√©cessite une base de connaissances √©tendue |

### Comparaison des technologies

| Technologie | Cas d'utilisation | Avantages | Inconv√©nients |
|-------------|--------------------|-----------|---------------|
| **Chainlit** | D√©veloppeurs Python, prototypage rapide | Configuration facile, support du streaming | Limit√© √† Python |
| **WebGPU** | Confidentialit√© maximale, sc√©narios hors ligne | Natif au navigateur, aucun serveur requis | Taille de mod√®le limit√©e |
| **Open WebUI** | D√©ploiement en production, √©quipes | Interface professionnelle, gestion des utilisateurs | N√©cessite Docker |

## Pr√©requis

- **Foundry Local** : Install√© et en cours d'ex√©cution ([T√©l√©charger](https://aka.ms/foundry-local-installer))
- **Python** : Version 3.10+ avec environnement virtuel
- **Mod√®le** : Au moins un mod√®le charg√© (`foundry model run phi-4-mini`)
- **Navigateur** : Chrome/Edge avec support WebGPU pour les d√©mos
- **Docker** : Pour Open WebUI (optionnel)

## Installation et configuration

### 1. Configuration de l'environnement Python

```cmd
# Navigate to Module08 directory
cd Module08

# Create and activate virtual environment
py -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration de Foundry Local

```cmd
# Verify Foundry Local installation
foundry --version

# Start the service
foundry service start

# Load a model
foundry model run phi-4-mini

# Verify model is running
foundry service ps
```

## Applications d'exemple

### Application de chat Chainlit

**Caract√©ristiques :**
- üöÄ **Streaming en temps r√©el** : Les tokens apparaissent au fur et √† mesure de leur g√©n√©ration
- üõ°Ô∏è **Gestion robuste des erreurs** : D√©gradation et r√©cup√©ration en douceur
- üé® **Interface moderne** : Interface de chat professionnelle pr√™te √† l'emploi
- üîß **Configuration flexible** : Variables d'environnement et d√©tection automatique
- üì± **Design r√©actif** : Fonctionne sur ordinateurs et appareils mobiles

**D√©marrage rapide :**
```cmd
# Run with default settings (recommended)
chainlit run samples\04\app.py -w --port 8080

# Use specific model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Manual endpoint configuration
set BASE_URL=http://localhost:51211
set API_KEY=your-api-key
chainlit run samples\04\app.py -w --port 8080
```

### D√©mo WebGPU dans le navigateur

**Caract√©ristiques :**
- üåê **IA native au navigateur** : Aucun serveur requis, fonctionne enti√®rement dans le navigateur
- ‚ö° **Acc√©l√©ration WebGPU** : Acc√©l√©ration mat√©rielle lorsque disponible
- üîí **Confidentialit√© maximale** : Les donn√©es ne quittent jamais votre appareil
- üéØ **Installation z√©ro** : Fonctionne dans tout navigateur compatible
- üîÑ **Fallback en douceur** : Bascule sur le CPU si WebGPU n'est pas disponible

**Ex√©cution :**
```cmd
cd samples\04\webgpu-demo
python -m http.server 5173
# Open http://localhost:5173
```

### Int√©gration Open WebUI

**Caract√©ristiques :**
- üé® **Interface similaire √† ChatGPT** : UI professionnelle et famili√®re
- üë• **Support multi-utilisateurs** : Comptes utilisateurs et historique des conversations
- üìÅ **Traitement de fichiers** : T√©l√©chargement et analyse de documents
- üîÑ **Changement de mod√®le** : Commutation facile entre diff√©rents mod√®les
- üê≥ **D√©ploiement Docker** : Configuration pr√™te pour la production en conteneur

**Configuration rapide :**
```cmd
docker run -d --name open-webui -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 \
  -e OPENAI_API_KEY=foundry-local-key \
  ghcr.io/open-webui/open-webui:main
```

## R√©f√©rences de configuration

### Variables d'environnement

| Variable | Description | Valeur par d√©faut | Exemple |
|----------|-------------|-------------------|---------|
| `MODEL` | Alias du mod√®le √† utiliser | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Point de terminaison Foundry Local | D√©tect√© automatiquement | `http://localhost:51211` |
| `API_KEY` | Cl√© API (optionnelle pour local) | `""` | `your-api-key` |

## R√©solution des probl√®mes

### Probl√®mes courants

**Application Chainlit :**

1. **Service non disponible :**
   ```cmd
   # Check Foundry Local status
   foundry service status
   foundry service ps
   
   # Validate API endpoint (note: port 51211)
   curl http://localhost:51211/v1/models
   ```

2. **Conflits de port :**
   ```cmd
   # Check what's using port 8080
   netstat -ano | findstr :8080
   
   # Use different port if needed
   chainlit run samples\04\app.py -w --port 3000
   ```

3. **Probl√®mes d'environnement Python :**
   ```cmd
   # Verify correct interpreter in VS Code
   # Ctrl+Shift+P ‚Üí Python: Select Interpreter
   # Choose: Module08/.venv/Scripts/python.exe
   
   # Reinstall dependencies
   pip install -r requirements.txt
   ```

**D√©mo WebGPU :**

1. **WebGPU non pris en charge :**
   - Mettez √† jour vers Chrome/Edge 113+
   - Activez WebGPU : `chrome://flags/#enable-unsafe-webgpu`
   - V√©rifiez le statut GPU : `chrome://gpu`
   - La d√©mo basculera automatiquement sur le CPU

2. **Erreurs de chargement de mod√®le :**
   - Assurez-vous d'avoir une connexion internet pour t√©l√©charger le mod√®le
   - V√©rifiez la console du navigateur pour les erreurs CORS
   - V√©rifiez que vous servez via HTTP (pas file://)

**Open WebUI :**

1. **Connexion refus√©e :**
   ```cmd
   # Check Docker is running
   docker --version
   
   # Check container status
   docker ps | findstr open-webui
   
   # View container logs
   docker logs open-webui
   ```

2. **Mod√®les non affich√©s :**
   ```cmd
   # Verify Foundry Local endpoint
   curl http://localhost:51211/v1/models
   
   # Restart Open WebUI
   docker restart open-webui
   ```

### Liste de validation

```cmd
# ‚úÖ 1. Foundry Local Setup
foundry --version                    # Should show version
foundry service status               # Should show "running"
foundry model list                   # Should show loaded models
curl http://localhost:51211/v1/models  # Should return JSON

# ‚úÖ 2. Python Environment  
python --version                     # Should be 3.10+
pip list | findstr chainlit         # Should show chainlit package
pip list | findstr openai           # Should show openai package

# ‚úÖ 3. Application Testing
chainlit run samples\04\app.py -w --port 8080  # Should open browser
# Test WebGPU demo at localhost:5173
# Test Open WebUI at localhost:3000
```

## Utilisation avanc√©e

### Optimisation des performances

**Chainlit :**
- Utilisez le streaming pour une meilleure perception des performances
- Impl√©mentez le pooling de connexions pour une haute concurrence
- Mettez en cache les r√©ponses des mod√®les pour les requ√™tes r√©p√©t√©es
- Surveillez l'utilisation de la m√©moire avec des historiques de conversation volumineux

**WebGPU :**
- Utilisez WebGPU pour une confidentialit√© et une vitesse maximales
- Impl√©mentez la quantification des mod√®les pour des mod√®les plus petits
- Utilisez les Web Workers pour le traitement en arri√®re-plan
- Mettez en cache les mod√®les compil√©s dans le stockage du navigateur

**Open WebUI :**
- Utilisez des volumes persistants pour l'historique des conversations
- Configurez des limites de ressources pour le conteneur Docker
- Impl√©mentez des strat√©gies de sauvegarde pour les donn√©es utilisateur
- Configurez un proxy inverse pour la terminaison SSL

### Mod√®les d'int√©gration

**Hybride Local/Cloud :**
```python
# Route based on complexity and privacy requirements
async def intelligent_routing(prompt: str, metadata: dict):
    if metadata.get("contains_pii"):
        return await foundry_local_completion(prompt)  # Privacy-sensitive
    elif len(prompt.split()) > 200:
        return await azure_openai_completion(prompt)   # Complex reasoning
    else:
        return await foundry_local_completion(prompt)  # Default local
```

**Pipeline multi-modal :**
```python
# Combine different AI capabilities
async def analyze_document(file_path: str):
    # 1. OCR with WebGPU (browser-based)
    text = await webgpu_ocr(file_path)
    
    # 2. Analysis with Foundry Local (private)
    summary = await foundry_local_analyze(text)
    
    # 3. Enhancement with cloud (if needed)
    if summary.confidence < 0.8:
        summary = await azure_openai_enhance(summary)
    
    return summary
```

## D√©ploiement en production

### Consid√©rations de s√©curit√©

- **Cl√©s API** : Utilisez des variables d'environnement, ne les codez jamais en dur
- **R√©seau** : Utilisez HTTPS en production, envisagez un VPN pour l'acc√®s en √©quipe
- **Contr√¥le d'acc√®s** : Impl√©mentez l'authentification pour Open WebUI
- **Confidentialit√© des donn√©es** : Auditez les donn√©es locales vs celles envoy√©es au cloud
- **Mises √† jour** : Gardez Foundry Local et les conteneurs √† jour

### Surveillance et maintenance

- **V√©rifications de sant√©** : Impl√©mentez la surveillance des points de terminaison
- **Journalisation** : Centralisez les journaux de tous les composants
- **M√©triques** : Suivez les temps de r√©ponse, les taux d'erreur, l'utilisation des ressources
- **Sauvegarde** : Sauvegarde r√©guli√®re des donn√©es de conversation et des configurations

## R√©f√©rences et ressources

### Documentation
- [Documentation Chainlit](https://docs.chainlit.io/) - Guide complet du framework
- [Documentation Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/) - Documentation officielle de Microsoft
- [ONNX Runtime Web](https://onnxruntime.ai/docs/get-started/with-javascript/web.html) - Int√©gration WebGPU
- [Documentation Open WebUI](https://docs.openwebui.com/) - Configuration avanc√©e

### Fichiers d'exemple
- [`app.py`](../../../../../Module08/samples/04/app.py) - Application Chainlit en production
- [`chainlit_app.ipynb`](./chainlit_app.ipynb) - Notebook √©ducatif
- [`webgpu-demo/`](../../../../../Module08/samples/04/webgpu-demo) - Inf√©rence IA bas√©e sur le navigateur
- [`open-webui-guide.md`](./open-webui-guide.md) - Configuration compl√®te Open WebUI

### Exemples associ√©s
- [Documentation Session 4](../../04.CuttingEdgeModels.md) - Guide complet de la session
- [Exemples Foundry Local](https://github.com/microsoft/foundry-local/tree/main/samples) - Exemples officiels

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.