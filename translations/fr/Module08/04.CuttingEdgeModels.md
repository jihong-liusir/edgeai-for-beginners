<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T10:20:07+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "fr"
}
-->
# Session 4 : Cr√©er des applications de chat en production avec Chainlit

## Aper√ßu

Cette session est consacr√©e √† la cr√©ation d'applications de chat pr√™tes pour la production en utilisant Chainlit et Microsoft Foundry Local. Vous apprendrez √† concevoir des interfaces web modernes pour les conversations avec l'IA, √† impl√©menter des r√©ponses en streaming et √† d√©ployer des applications de chat robustes avec une gestion des erreurs et une conception de l'exp√©rience utilisateur appropri√©es.

**Ce que vous allez construire :**
- **Application de chat Chainlit** : Interface web moderne avec r√©ponses en streaming
- **D√©mo WebGPU** : Inf√©rence dans le navigateur pour des applications respectueuses de la vie priv√©e  
- **Int√©gration Open WebUI** : Interface de chat professionnelle avec Foundry Local
- **Mod√®les de production** : Gestion des erreurs, surveillance et strat√©gies de d√©ploiement

## Objectifs d'apprentissage

- Cr√©er des applications de chat pr√™tes pour la production avec Chainlit
- Impl√©menter des r√©ponses en streaming pour am√©liorer l'exp√©rience utilisateur
- Ma√Ætriser les mod√®les d'int√©gration du SDK Foundry Local
- Appliquer une gestion des erreurs et une d√©gradation progressive
- D√©ployer et configurer des applications de chat pour diff√©rents environnements
- Comprendre les mod√®les modernes d'interface web pour l'IA conversationnelle

## Pr√©requis

- **Foundry Local** : Install√© et en cours d'ex√©cution ([Guide d'installation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python** : Version 3.10 ou ult√©rieure avec capacit√© de cr√©er des environnements virtuels
- **Mod√®le** : Au moins un mod√®le charg√© (`foundry model run phi-4-mini`)
- **Navigateur** : Navigateur web moderne avec support WebGPU (Chrome/Edge)
- **Docker** : Pour l'int√©gration Open WebUI (optionnel)

## Partie 1 : Comprendre les applications de chat modernes

### Aper√ßu de l'architecture

```
User Browser ‚Üê‚Üí Chainlit UI ‚Üê‚Üí Python Backend ‚Üê‚Üí Foundry Local ‚Üê‚Üí AI Model
      ‚Üì              ‚Üì              ‚Üì              ‚Üì            ‚Üì
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Technologies cl√©s

**Mod√®les SDK Foundry Local :**
- `FoundryLocalManager(alias)` : Gestion automatique des services
- `manager.endpoint` et `manager.api_key` : D√©tails de connexion
- `manager.get_model_info(alias).id` : Identification des mod√®les

**Framework Chainlit :**
- `@cl.on_chat_start` : Initialiser les sessions de chat
- `@cl.on_message` : G√©rer les messages entrants des utilisateurs  
- `cl.Message().stream_token()` : Streaming en temps r√©el
- G√©n√©ration automatique d'interface utilisateur et gestion des WebSockets

## Partie 2 : Matrice de d√©cision Local vs Cloud

### Caract√©ristiques de performance

| Aspect | Local (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latence** | üöÄ 50-200ms (sans r√©seau) | ‚è±Ô∏è 200-2000ms (d√©pend du r√©seau) |
| **Confidentialit√©** | üîí Les donn√©es ne quittent jamais l'appareil | ‚ö†Ô∏è Les donn√©es sont envoy√©es au cloud |
| **Co√ªt** | üí∞ Gratuit apr√®s achat du mat√©riel | üí∏ Paiement par token |
| **Hors ligne** | ‚úÖ Fonctionne sans internet | ‚ùå N√©cessite internet |
| **Taille du mod√®le** | ‚ö†Ô∏è Limit√© par le mat√©riel | ‚úÖ Acc√®s aux mod√®les les plus grands |
| **√âvolutivit√©** | ‚ö†Ô∏è D√©pend du mat√©riel | ‚úÖ √âvolutivit√© illimit√©e |

### Mod√®les de strat√©gie hybride

**Priorit√© au local avec secours :**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Routage bas√© sur les t√¢ches :**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Partie 3 : Exemple 04 - Application de chat Chainlit

### D√©marrage rapide

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

L'application s'ouvre automatiquement √† l'adresse `http://localhost:8080` avec une interface de chat moderne.

### Impl√©mentation principale

L'application Exemple 04 d√©montre des mod√®les pr√™ts pour la production :

**D√©couverte automatique des services :**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Gestionnaire de chat en streaming :**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Options de configuration

**Variables d'environnement :**

| Variable | Description | Valeur par d√©faut | Exemple |
|----------|-------------|-------------------|---------|
| `MODEL` | Alias du mod√®le √† utiliser | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Point de terminaison Foundry Local | D√©tect√© automatiquement | `http://localhost:51211` |
| `API_KEY` | Cl√© API (optionnelle pour le local) | `""` | `your-api-key` |

**Utilisation avanc√©e :**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Partie 4 : Cr√©er et utiliser des notebooks Jupyter

### Aper√ßu du support des notebooks

L'Exemple 04 inclut un notebook Jupyter complet (`chainlit_app.ipynb`) qui propose :

- **üìö Contenu √©ducatif** : Mat√©riel d'apprentissage √©tape par √©tape
- **üî¨ Exploration interactive** : Ex√©cuter et exp√©rimenter avec des cellules de code
- **üìä D√©monstrations visuelles** : Graphiques, diagrammes et visualisation des r√©sultats
- **üõ†Ô∏è Outils de d√©veloppement** : Test et d√©bogage

### Cr√©er vos propres notebooks

#### √âtape 1 : Configurer l'environnement Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### √âtape 2 : Cr√©er un nouveau notebook

**Avec VS Code :**
1. Ouvrez VS Code dans le r√©pertoire Module08
2. Cr√©ez un nouveau fichier avec l'extension `.ipynb`
3. S√©lectionnez le kernel "Foundry Local" lorsque cela est demand√©
4. Commencez √† ajouter des cellules avec votre contenu

**Avec Jupyter Lab :**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Bonnes pratiques pour la structure des notebooks

#### Organisation des cellules

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("‚úÖ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Exemples interactifs et exercices

#### Exercice 1 : Test de configuration client

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nüß™ Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'‚úÖ Success' if result['status'] == 'ok' else '‚ùå Failed'}")
```

#### Exercice 2 : Simulation de r√©ponse en streaming

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("üåä Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n‚úÖ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Partie 5 : D√©mo d'inf√©rence dans le navigateur avec WebGPU

### Aper√ßu

WebGPU permet d'ex√©cuter des mod√®les d'IA directement dans le navigateur pour une confidentialit√© maximale et une exp√©rience sans installation. Cet exemple d√©montre l'ex√©cution de ONNX Runtime Web avec WebGPU.

### √âtape 1 : V√©rifier le support WebGPU

**Exigences du navigateur :**
- Chrome/Edge 113+ avec WebGPU activ√©
- V√©rification : `chrome://gpu` ‚Üí confirmer le statut "WebGPU"
- V√©rification programmatique : `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### √âtape 2 : Cr√©er une d√©mo WebGPU

Cr√©er un r√©pertoire : `samples/04/webgpu-demo/`

**index.html :**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>üöÄ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js :**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '‚ùå WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'üîç WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('‚úÖ ONNX Runtime session created with WebGPU');
        log(`üìä Input names: ${session.inputNames.join(', ')}`);
        log(`üìä Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '‚úÖ WebGPU inference complete!';
        log(`üéØ Predicted class: ${maxIdx}`);
        log(`üìà Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `‚ùå Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### √âtape 3 : Ex√©cuter la d√©mo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Partie 6 : Int√©gration Open WebUI

### Aper√ßu

Open WebUI offre une interface professionnelle de type ChatGPT qui se connecte √† l'API compatible OpenAI de Foundry Local.

### √âtape 1 : Pr√©requis

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### √âtape 2 : Configuration Docker (recommand√©e)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Remarque :** `host.docker.internal` permet aux conteneurs Docker d'acc√©der √† la machine h√¥te sous Windows.

### √âtape 3 : Configuration

1. **Ouvrir le navigateur :** Acc√©dez √† `http://localhost:3000`
2. **Configuration initiale :** Cr√©ez un compte administrateur
3. **Configuration du mod√®le :**
   - Param√®tres ‚Üí Mod√®les ‚Üí API OpenAI  
   - URL de base : `http://host.docker.internal:51211/v1`
   - Cl√© API : `foundry-local-key` (n'importe quelle valeur fonctionne)
4. **Tester la connexion :** Les mod√®les devraient appara√Ætre dans le menu d√©roulant

### R√©solution des probl√®mes

**Probl√®mes courants :**

1. **Connexion refus√©e :**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Les mod√®les n'apparaissent pas :**
   - V√©rifiez que le mod√®le est charg√© : `foundry model list`
   - V√©rifiez la r√©ponse de l'API : `curl http://localhost:51211/v1/models`
   - Red√©marrez le conteneur Open WebUI

## Partie 7 : Consid√©rations pour le d√©ploiement en production

### Configuration de l'environnement

**Configuration de d√©veloppement :**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**D√©ploiement en production :**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Probl√®mes courants de port et solutions

**Pr√©vention des conflits sur le port 51211 :**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Surveillance des performances

**Impl√©mentation de v√©rification de sant√© :**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## R√©sum√©

La session 4 a couvert la cr√©ation d'applications Chainlit pr√™tes pour la production pour l'IA conversationnelle. Vous avez appris :

- ‚úÖ **Framework Chainlit** : Interface moderne et support du streaming pour les applications de chat
- ‚úÖ **Int√©gration Foundry Local** : Utilisation du SDK et mod√®les de configuration  
- ‚úÖ **Inf√©rence WebGPU** : IA dans le navigateur pour une confidentialit√© maximale
- ‚úÖ **Configuration Open WebUI** : D√©ploiement d'une interface de chat professionnelle
- ‚úÖ **Mod√®les de production** : Gestion des erreurs, surveillance et √©volutivit√©

L'application Exemple 04 d√©montre les meilleures pratiques pour cr√©er des interfaces de chat robustes qui exploitent des mod√®les d'IA locaux via Microsoft Foundry Local tout en offrant une excellente exp√©rience utilisateur.

## R√©f√©rences

- **[Exemple 04 : Application Chainlit](samples/04/README.md)** : Application compl√®te avec documentation
- **[Notebook √©ducatif Chainlit](samples/04/chainlit_app.ipynb)** : Mat√©riel d'apprentissage interactif
- **[Documentation Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)** : Documentation compl√®te de la plateforme
- **[Documentation Chainlit](https://docs.chainlit.io/)** : Documentation officielle du framework
- **[Guide d'int√©gration Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)** : Tutoriel officiel

---

