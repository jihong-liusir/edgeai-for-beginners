<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T12:26:06+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "fr"
}
-->
# Session 4 : Modèles de pointe – LLMs, SLMs et inférence sur appareil

## Aperçu

Comparer les LLMs et les SLMs, évaluer les compromis entre inférence locale et cloud, et implémenter des démonstrations mettant en avant des scénarios EdgeAI avec Phi et ONNX Runtime. Nous mettrons également en lumière Chainlit RAG, les options d'inférence WebGPU et l'intégration d'Open WebUI.

Références :
- Documentation Foundry Local : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI : https://onnxruntime.ai/
- Guide Open WebUI (application de chat avec Open WebUI) : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Objectifs d'apprentissage
- Comprendre les compromis entre LLM et SLM en termes de coût, latence et précision
- Choisir entre inférence locale et cloud selon les besoins spécifiques de l'entreprise
- Implémenter une petite démonstration RAG avec Chainlit
- Explorer WebGPU pour l'accélération côté navigateur
- Connecter Open WebUI à Foundry Local

## Partie 1 : LLM vs SLM – Matrice de décision

À considérer :
- Latence : les SLMs sur appareil offrent souvent des réponses en moins d'une seconde
- Coût : l'inférence locale réduit les coûts liés au cloud
- Confidentialité : les données sensibles restent sur l'appareil
- Capacité : les LLMs peuvent surpasser les SLMs dans les tâches complexes
- Fiabilité : les stratégies hybrides réduisent les risques de panne

## Partie 2 : Local vs Cloud – Modèles hybrides

- Priorité locale avec recours au cloud pour les prompts volumineux ou complexes
- Priorité cloud avec recours au local pour les scénarios sensibles à la confidentialité ou hors ligne
- Routage par type de tâche (génération de code vers DeepSeek, chat général vers Phi/Qwen)

## Partie 3 : Application de chat RAG avec Chainlit (Minimal)

Installer les dépendances :
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py` :
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Exécuter :
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Étendre : ajouter un simple récupérateur (fichiers locaux) et préfixer le contexte récupéré au prompt utilisateur.

## Partie 4 : Inférence WebGPU (Aperçu)

Exécuter de petits modèles directement dans le navigateur avec WebGPU. Idéal pour des démonstrations axées sur la confidentialité et des expériences sans installation. Voici un exemple minimal, étape par étape, utilisant ONNX Runtime Web avec le fournisseur d'exécution WebGPU.

1) Vérifier la compatibilité WebGPU
- Navigateurs Chromium : chrome://gpu → confirmer que “WebGPU” est activé
- Vérification programmatique (nous vérifierons également dans le code) : `if (!('gpu' in navigator)) { /* pas de WebGPU */ }`

2) Créer un projet minimal
Créer un dossier et deux fichiers : `index.html` et `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Servir localement (Windows cmd.exe)
Utiliser un serveur statique simple pour que le navigateur puisse récupérer le modèle.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Ouvrir http://localhost:5173 dans votre navigateur. Vous devriez voir les journaux d'initialisation, la création de session avec WebGPU et une prédiction argmax.

4) Résolution des problèmes
- Si WebGPU est indisponible : mettre à jour Chrome/Edge et s'assurer que les pilotes GPU sont à jour, puis vérifier chrome://flags pour “Activer WebGPU”.
- En cas d'erreurs CORS ou de récupération : s'assurer que les fichiers sont servis via http:// (et non file://) et que l'URL du modèle autorise les requêtes cross-origin.
- Repli sur CPU : changer `executionProviders: ['wasm']` pour vérifier le comportement de base.

5) Prochaines étapes
- Remplacer par un modèle ONNX spécifique au domaine (ex. : classification d'images ou petit modèle de texte).
- Ajouter une logique de prétraitement/post-traitement pour des entrées réelles.
- Pour des modèles plus volumineux ou une latence en production, privilégier Foundry Local ou ONNX Runtime Server.

## Partie 5 : Open WebUI + Foundry Local (Étape par étape)

Cela connecte Open WebUI au point de terminaison compatible OpenAI de Foundry Local pour une interface de chat locale.

1) Prérequis
- Foundry Local installé et fonctionnel (`foundry --version`)
- Un modèle prêt à être exécuté localement (ex. : `phi-4-mini`)
- Docker Desktop installé (recommandé pour Open WebUI)

2) Démarrer un modèle avec Foundry Local
```powershell
foundry model run phi-4-mini
```
Cela expose une API compatible OpenAI à `http://localhost:8000`.

3) Démarrer Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Remarques :
- Sous Windows, `host.docker.internal` permet au conteneur d'accéder à votre hôte via `localhost`.
- Nous définissons `OPENAI_API_BASE_URL` sur le point de terminaison de Foundry Local et une clé API fictive `OPENAI_API_KEY`.

4) Configurer depuis l'interface Open WebUI (alternative)
- Accéder à http://localhost:3000
- Compléter la configuration initiale (utilisateur admin)
- Aller dans Paramètres → Modèles/Fournisseurs
- Définir l'URL de base : `http://host.docker.internal:8000/v1`
- Définir la clé API : `local-key` (placeholder)
- Enregistrer

5) Tester un prompt
- Dans le chat Open WebUI, sélectionner ou entrer le nom du modèle `phi-4-mini`
- Prompt : “Listez cinq avantages de l'inférence IA sur appareil.”
- Vous devriez voir une réponse diffusée depuis votre modèle local

6) Résolution des problèmes
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Optionnel : Persister les données Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Liste de contrôle pratique
- [ ] Comparer les réponses/la latence entre SLM et LLM localement
- [ ] Exécuter la démonstration Chainlit avec au moins deux modèles
- [ ] Connecter Open WebUI à votre point de terminaison local et tester

## Prochaines étapes
- Préparer les workflows d'agents pour la session 5
- Identifier les scénarios où le modèle hybride local/cloud améliore le retour sur investissement

---

