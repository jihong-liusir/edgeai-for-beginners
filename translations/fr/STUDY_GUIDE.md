<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T12:00:28+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "fr"
}
-->
# EdgeAI pour Débutants : Parcours d'Apprentissage et Planning d'Étude

### Parcours d'Apprentissage Intensif (1 semaine)

| Jour | Thème | Heures Estimées |
|------|-------|------------------|
| Jour 1 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 2 | Module 2 : Fondations des SLM | 3 heures |
| Jour 3 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 4-5 | Module 4 : Optimisation des Modèles (6 frameworks) | 4 heures |
| Jour 6 | Module 5 : SLMOps | 3 heures |
| Jour 7 | Module 6-7 : Agents IA & Outils de Développement | 5 heures |

### Parcours d'Apprentissage Intensif (2 semaines)

| Jour | Thème | Heures Estimées |
|------|-------|------------------|
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des Modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Agents IA | 2 heures |
| Jour 13-14 | Module 7 : Outils de Développement | 3 heures |

### Étude à Temps Partiel (4 semaines)

| Semaine | Thème | Heures Estimées |
|---------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de Développement & Intégration | 3 heures |

| Jour | Thème | Heures Estimées |
|------|-------|------------------|
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des Modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Systèmes Agentiques SLM | 2 heures |
| Jour 13-14 | Module 7 : Exemples d'Implémentation EdgeAI | 2 heures |

| Module | Date de Fin | Heures Passées | Points Clés |
|--------|-------------|---------------|-------------|
| Module 1 : Fondamentaux d'EdgeAI | | | |
| Module 2 : Fondations des SLM | | | |
| Module 3 : Déploiement des SLM | | | |
| Module 4 : Optimisation des Modèles (6 frameworks) | | | |
| Module 5 : SLMOps | | | |
| Module 6 : Systèmes Agentiques SLM | | | |
| Module 7 : Exemples d'Implémentation EdgeAI | | | |
| Exercices Pratiques | | | |
| Mini-Projet | | | |

### Étude à Temps Partiel (4 semaines)

| Semaine | Thème | Heures Estimées |
|---------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de Développement & Intégration | 3 heures |

## Introduction

Bienvenue dans le guide d'étude "EdgeAI pour Débutants" ! Ce document est conçu pour vous aider à naviguer efficacement dans les supports de cours et à maximiser votre expérience d'apprentissage. Il propose des parcours d'apprentissage structurés, des plannings d'étude suggérés, des résumés des concepts clés et des ressources complémentaires pour approfondir votre compréhension des technologies EdgeAI.

Ce cours concis de 20 heures offre des connaissances essentielles sur EdgeAI dans un format efficace, idéal pour les professionnels et les étudiants occupés souhaitant acquérir rapidement des compétences pratiques dans ce domaine émergent.

## Aperçu du Cours

Ce cours est organisé en sept modules complets :

1. **Fondamentaux et Transformation d'EdgeAI** - Comprendre les concepts de base et le changement technologique
2. **Fondations des Petits Modèles de Langage (SLM)** - Explorer les différentes familles de SLM et leurs architectures
3. **Déploiement des Petits Modèles de Langage** - Mettre en œuvre des stratégies de déploiement pratiques
4. **Conversion de Format et Quantification des Modèles** - Optimisation avancée avec 6 frameworks, dont OpenVINO
5. **SLMOps - Opérations des Petits Modèles de Langage** - Gestion du cycle de vie en production et déploiement
6. **Systèmes Agentiques SLM** - Agents IA, appels de fonctions et protocole de contexte de modèle
7. **Exemples d'Implémentation EdgeAI** - Outils IA, développement sous Windows et implémentations spécifiques à la plateforme
8. **Microsoft Foundry Local – Kit Complet pour Développeurs** - Développement local-first avec intégration hybride Azure (Module 08)

## Comment Utiliser ce Guide d'Étude

- **Apprentissage Progressif** : Suivez les modules dans l'ordre pour une expérience d'apprentissage cohérente
- **Points de Contrôle des Connaissances** : Utilisez les questions d'auto-évaluation après chaque section
- **Pratique Pratique** : Réalisez les exercices suggérés pour renforcer les concepts théoriques
- **Ressources Complémentaires** : Explorez des supports supplémentaires pour approfondir les sujets qui vous intéressent le plus

## Recommandations de Planning d'Étude

### Parcours d'Apprentissage Intensif (1 semaine)

| Jour | Thème | Heures Estimées |
|------|-------|-----------------|
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 6 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 8 heures |
| Jour 5 | Module 3 : Déploiement des SLM | 3 heures |
| Jour 6 | Module 8 : Kit Foundry Local | 3 heures |

### Étude à Temps Partiel (3 semaines)

| Semaine | Thème | Heures Estimées |
|---------|-------|-----------------|
| Semaine 1 | Module 1 : Fondamentaux d'EdgeAI | 6-7 heures |
| Semaine 2 | Module 2 : Fondations des SLM | 7-8 heures |
| Semaine 3 | Module 3 : Déploiement des SLM (3h) + Module 8 : Kit Foundry Local (2-3h) | 5-6 heures |

## Module 1 : Fondamentaux et Transformation d'EdgeAI

### Objectifs d'Apprentissage Clés

- Comprendre les différences entre l'IA basée sur le cloud et l'IA basée sur l'edge
- Maîtriser les techniques d'optimisation pour les environnements à ressources limitées
- Analyser des applications concrètes des technologies EdgeAI
- Configurer un environnement de développement pour les projets EdgeAI

### Domaines d'Étude Prioritaires

#### Section 1 : Fondamentaux d'EdgeAI
- **Concepts Prioritaires** : 
  - Paradigmes de calcul Edge vs Cloud
  - Techniques de quantification des modèles
  - Options d'accélération matérielle (NPU, GPU, CPU)
  - Avantages en termes de confidentialité et de sécurité

- **Matériel Supplémentaire** :
  - [Documentation TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentation Edge Impulse](https://docs.edgeimpulse.com)

#### Section 2 : Études de Cas Réels
- **Concepts Prioritaires** : 
  - Écosystème de modèles Microsoft Phi & Mu
  - Implémentations pratiques dans divers secteurs
  - Considérations de déploiement

#### Section 3 : Guide de Mise en Œuvre Pratique
- **Concepts Prioritaires** : 
  - Configuration de l'environnement de développement
  - Outils de quantification et d'optimisation
  - Méthodes d'évaluation des implémentations EdgeAI

#### Section 4 : Matériel de Déploiement Edge
- **Concepts Prioritaires** : 
  - Comparaisons des plateformes matérielles
  - Stratégies d'optimisation pour un matériel spécifique
  - Considérations de déploiement

### Questions d'Auto-Évaluation

1. Comparez et contrastez les implémentations d'IA basées sur le cloud et sur l'edge.
2. Expliquez trois techniques clés pour optimiser les modèles pour le déploiement edge.
3. Quels sont les principaux avantages de l'exécution des modèles d'IA à l'edge ?
4. Décrivez le processus de quantification d'un modèle et son impact sur les performances.
5. Expliquez comment différents accélérateurs matériels (NPU, GPU, CPU) influencent le déploiement EdgeAI.

### Exercices Pratiques

1. **Configuration Rapide de l'Environnement** : Configurez un environnement de développement minimal avec les packages essentiels (30 minutes)
2. **Exploration de Modèles** : Téléchargez et examinez un petit modèle de langage pré-entraîné (1 heure)
3. **Quantification de Base** : Essayez une quantification simple sur un petit modèle (1 heure)

## Module 2 : Fondations des Petits Modèles de Langage (SLM)

### Objectifs d'Apprentissage Clés

- Comprendre les principes architecturaux des différentes familles de SLM
- Comparer les capacités des modèles selon différentes échelles de paramètres
- Évaluer les modèles en fonction de leur efficacité, de leurs capacités et des exigences de déploiement
- Identifier les cas d'utilisation appropriés pour chaque famille de modèles

### Domaines d'Étude Prioritaires

#### Section 1 : Famille de Modèles Microsoft Phi
- **Concepts Prioritaires** : 
  - Évolution de la philosophie de conception
  - Architecture axée sur l'efficacité
  - Capacités spécialisées

#### Section 2 : Famille Qwen
- **Concepts Prioritaires** : 
  - Contributions open source
  - Options de déploiement évolutives
  - Architecture avancée pour le raisonnement

#### Section 3 : Famille Gemma
- **Concepts Prioritaires** : 
  - Innovation axée sur la recherche
  - Capacités multimodales
  - Optimisation pour les appareils mobiles

#### Section 4 : Famille BitNET
- **Concepts Prioritaires** : 
  - Technologie de quantification à 1 bit
  - Cadre d'optimisation pour l'inférence
  - Considérations de durabilité

#### Section 5 : Modèle Microsoft Mu
- **Concepts Prioritaires** : 
  - Architecture axée sur les appareils
  - Intégration système avec Windows
  - Fonctionnement respectueux de la vie privée

#### Section 6 : Phi-Silica
- **Concepts Prioritaires** : 
  - Architecture optimisée pour NPU
  - Indicateurs de performance
  - Intégration pour les développeurs

### Questions d'Auto-Évaluation

1. Comparez les approches architecturales des familles de modèles Phi et Qwen.
2. Expliquez en quoi la technologie de quantification de BitNET diffère de la quantification traditionnelle.
3. Quels sont les avantages uniques du modèle Mu pour l'intégration avec Windows ?
4. Décrivez comment Phi-Silica exploite le matériel NPU pour optimiser les performances.
5. Pour une application mobile avec une connectivité limitée, quelle famille de modèles serait la plus appropriée et pourquoi ?

### Exercices Pratiques

1. **Comparaison de Modèles** : Benchmark rapide de deux modèles SLM différents (1 heure)
2. **Génération de Texte Simple** : Implémentation basique de génération de texte avec un petit modèle (1 heure)
3. **Optimisation Rapide** : Appliquez une technique d'optimisation pour améliorer la vitesse d'inférence (1 heure)

## Module 3 : Déploiement des Petits Modèles de Langage

### Objectifs d'Apprentissage Clés

- Sélectionner les modèles appropriés en fonction des contraintes de déploiement
- Maîtriser les techniques d'optimisation pour divers scénarios de déploiement
- Implémenter des SLM dans des environnements locaux et cloud
- Concevoir des configurations prêtes pour la production pour les applications EdgeAI

### Domaines d'Étude Prioritaires

#### Section 1 : Apprentissage Avancé des SLM
- **Concepts Prioritaires** : 
  - Cadre de classification des paramètres
  - Techniques d'optimisation avancées
  - Stratégies d'acquisition de modèles

#### Section 2 : Déploiement en Environnement Local
- **Concepts Prioritaires** : 
  - Déploiement sur la plateforme Ollama
  - Solutions locales Microsoft Foundry
  - Analyse comparative des frameworks

#### Section 3 : Déploiement Cloud Conteneurisé
- **Concepts Prioritaires** : 
  - Inférence haute performance avec vLLM
  - Orchestration de conteneurs
  - Implémentation ONNX Runtime

### Questions d'Auto-Évaluation

1. Quels facteurs doivent être pris en compte lors du choix entre un déploiement local et un déploiement cloud ?
2. Comparez Ollama et Microsoft Foundry Local en tant qu'options de déploiement.
3. Expliquez les avantages de la conteneurisation pour le déploiement des SLM.
4. Quels sont les principaux indicateurs de performance à surveiller pour un SLM déployé en edge ?
5. Décrivez un workflow complet de déploiement, de la sélection du modèle à l'implémentation en production.

### Exercices Pratiques

1. **Déploiement Local de Base** : Déployez un SLM simple en utilisant Ollama (1 heure)
2. **Vérification des Performances** : Effectuez un benchmark rapide sur votre modèle déployé (30 minutes)
3. **Intégration Simple** : Créez une application minimale utilisant votre modèle déployé (1 heure)

## Module 4 : Conversion de Format et Quantification des Modèles

### Objectifs d'Apprentissage Clés

- Maîtriser les techniques avancées de quantification, de la précision 1 bit à 8 bits
- Comprendre les stratégies de conversion de format (GGUF, ONNX)
- Implémenter l'optimisation sur six frameworks (Llama.cpp, Olive, OpenVINO, MLX, synthèse de workflow)
- Déployer des modèles optimisés pour les environnements edge en production sur du matériel Intel, Apple et multiplateforme

### Domaines d'Étude Prioritaires

#### Section 1 : Fondations de la Quantification
- **Concepts Prioritaires** : 
  - Cadre de classification des précisions
  - Compromis entre performance et précision
  - Optimisation de l'empreinte mémoire

#### Section 2 : Implémentation Llama.cpp
- **Concepts Prioritaires** : 
  - Déploiement multiplateforme
  - Optimisation du format GGUF
  - Techniques d'accélération matérielle

#### Section 3 : Suite Microsoft Olive
- **Concepts Prioritaires** : 
  - Optimisation adaptée au matériel
  - Déploiement de niveau entreprise
  - Workflows d'optimisation automatisés

#### Section 4 : Toolkit OpenVINO
- **Concepts Prioritaires** : 
  - Optimisation pour matériel Intel
  - Framework de compression des réseaux neuronaux (NNCF)
  - Déploiement d'inférence multiplateforme
  - OpenVINO GenAI pour le déploiement des LLM

#### Section 5 : Framework Apple MLX
- **Concepts prioritaires** :  
  - Optimisation pour Apple Silicon  
  - Architecture de mémoire unifiée  
  - Capacités de fine-tuning LoRA  

#### Section 6 : Synthèse du workflow de développement Edge AI  
- **Concepts prioritaires** :  
  - Architecture de workflow unifiée  
  - Arbres de décision pour la sélection des frameworks  
  - Validation de la préparation à la production  
  - Stratégies pour pérenniser les solutions  

### Questions d'auto-évaluation  

1. Comparez les stratégies de quantification selon différents niveaux de précision (de 1 bit à 8 bits).  
2. Expliquez les avantages du format GGUF pour le déploiement en périphérie.  
3. Comment l'optimisation adaptée au matériel dans Microsoft Olive améliore-t-elle l'efficacité du déploiement ?  
4. Quels sont les principaux avantages de NNCF d'OpenVINO pour la compression des modèles ?  
5. Décrivez comment Apple MLX exploite l'architecture de mémoire unifiée pour l'optimisation.  
6. En quoi la synthèse de workflow aide-t-elle à sélectionner les frameworks d'optimisation optimaux ?  

### Exercices pratiques  

1. **Quantification de modèle** : Appliquez différents niveaux de quantification à un modèle et comparez les résultats (1 heure).  
2. **Optimisation avec OpenVINO** : Utilisez NNCF pour compresser un modèle destiné au matériel Intel (1 heure).  
3. **Comparaison de frameworks** : Testez le même modèle sur trois frameworks d'optimisation différents (1 heure).  
4. **Benchmarking de performance** : Mesurez l'impact de l'optimisation sur la vitesse d'inférence et l'utilisation de la mémoire (1 heure).  

## Module 5 : SLMOps - Opérations sur les petits modèles de langage  

### Objectifs d'apprentissage clés  

- Comprendre les principes de gestion du cycle de vie des SLMOps  
- Maîtriser les techniques de distillation et de fine-tuning pour le déploiement en périphérie  
- Mettre en œuvre des stratégies de déploiement en production avec suivi  
- Construire des workflows d'opérations et de maintenance de SLM adaptés aux entreprises  

### Domaines d'étude prioritaires  

#### Section 1 : Introduction aux SLMOps  
- **Concepts prioritaires** :  
  - Changement de paradigme des SLMOps dans les opérations d'IA  
  - Architecture axée sur l'efficacité des coûts et la confidentialité  
  - Impact stratégique sur les entreprises et avantages compétitifs  

#### Section 2 : Distillation de modèles  
- **Concepts prioritaires** :  
  - Techniques de transfert de connaissances  
  - Mise en œuvre du processus de distillation en deux étapes  
  - Workflows de distillation avec Azure ML  

#### Section 3 : Stratégies de fine-tuning  
- **Concepts prioritaires** :  
  - Fine-tuning efficace en termes de paramètres (PEFT)  
  - Méthodes avancées LoRA et QLoRA  
  - Entraînement multi-adaptateurs et optimisation des hyperparamètres  

#### Section 4 : Déploiement en production  
- **Concepts prioritaires** :  
  - Conversion et quantification des modèles pour la production  
  - Configuration de déploiement avec Foundry Local  
  - Benchmarking de performance et validation de qualité  

### Questions d'auto-évaluation  

1. En quoi les SLMOps diffèrent-ils des MLOps traditionnels ?  
2. Expliquez les avantages de la distillation de modèles pour le déploiement en périphérie.  
3. Quels sont les principaux éléments à prendre en compte pour le fine-tuning des SLM dans des environnements à ressources limitées ?  
4. Décrivez un pipeline complet de déploiement en production pour des applications d'IA en périphérie.  

### Exercices pratiques  

1. **Distillation de base** : Créez un modèle plus petit à partir d'un modèle enseignant plus grand (1 heure).  
2. **Expérience de fine-tuning** : Effectuez le fine-tuning d'un modèle pour un domaine spécifique (1 heure).  
3. **Pipeline de déploiement** : Configurez un pipeline CI/CD de base pour le déploiement de modèles (1 heure).  

## Module 6 : Systèmes agentiques SLM - Agents IA et appels de fonctions  

### Objectifs d'apprentissage clés  

- Construire des agents IA intelligents pour des environnements en périphérie en utilisant des petits modèles de langage  
- Mettre en œuvre des capacités d'appel de fonctions avec des workflows systématiques  
- Maîtriser l'intégration du protocole de contexte de modèle (MCP) pour une interaction standardisée avec les outils  
- Créer des systèmes agentiques sophistiqués avec une intervention humaine minimale  

### Domaines d'étude prioritaires  

#### Section 1 : Agents IA et fondations des SLM  
- **Concepts prioritaires** :  
  - Cadre de classification des agents (réflexes, basés sur des modèles, basés sur des objectifs, agents apprenants)  
  - Analyse des compromis entre SLM et LLM  
  - Modèles de conception spécifiques aux agents en périphérie  
  - Optimisation des ressources pour les agents  

#### Section 2 : Appels de fonctions dans les petits modèles de langage  
- **Concepts prioritaires** :  
  - Mise en œuvre de workflows systématiques (détection d'intention, sortie JSON, exécution externe)  
  - Implémentations spécifiques aux plateformes (Phi-4-mini, modèles Qwen sélectionnés, Microsoft Foundry Local)  
  - Exemples avancés (collaboration multi-agents, sélection dynamique d'outils)  
  - Considérations pour la production (limitation de débit, journalisation des audits, mesures de sécurité)  

#### Section 3 : Intégration du protocole de contexte de modèle (MCP)  
- **Concepts prioritaires** :  
  - Architecture du protocole et conception de systèmes en couches  
  - Support multi-backend (Ollama pour le développement, vLLM pour la production)  
  - Protocoles de connexion (modes STDIO et SSE)  
  - Applications réelles (automatisation web, traitement de données, intégration API)  

### Questions d'auto-évaluation  

1. Quels sont les principaux éléments architecturaux à prendre en compte pour les agents IA en périphérie ?  
2. Comment les appels de fonctions améliorent-ils les capacités des agents ?  
3. Expliquez le rôle du protocole de contexte de modèle dans la communication des agents.  

### Exercices pratiques  

1. **Agent simple** : Construisez un agent IA de base avec des appels de fonctions (1 heure).  
2. **Intégration MCP** : Implémentez MCP dans une application d'agent (30 minutes).  

## Module 7 : Exemples d'implémentation EdgeAI  

### Objectifs d'apprentissage clés  

- Maîtriser l'AI Toolkit pour Visual Studio Code pour des workflows de développement EdgeAI complets  
- Acquérir une expertise sur la plateforme Windows AI Foundry et les stratégies d'optimisation NPU  
- Mettre en œuvre EdgeAI sur plusieurs plateformes matérielles et scénarios de déploiement  
- Construire des applications EdgeAI prêtes pour la production avec des optimisations spécifiques aux plateformes  

### Domaines d'étude prioritaires  

#### Section 1 : AI Toolkit pour Visual Studio Code  
- **Concepts prioritaires** :  
  - Environnement de développement Edge AI complet dans VS Code  
  - Catalogue de modèles et découverte pour le déploiement en périphérie  
  - Workflows de test local, optimisation et développement d'agents  
  - Suivi des performances et évaluation pour les scénarios en périphérie  

#### Section 2 : Guide de développement Windows EdgeAI  
- **Concepts prioritaires** :  
  - Aperçu complet de la plateforme Windows AI Foundry  
  - API Phi Silica pour une inférence NPU efficace  
  - APIs de vision par ordinateur pour le traitement d'images et OCR  
  - CLI Foundry Local pour le développement et les tests locaux  

#### Section 3 : Implémentations spécifiques aux plateformes  
- **Concepts prioritaires** :  
  - Déploiement sur NVIDIA Jetson Orin Nano (performance IA de 67 TOPS)  
  - Applications mobiles avec .NET MAUI et ONNX Runtime GenAI  
  - Solutions Azure EdgeAI avec architecture hybride cloud-périphérie  
  - Optimisation Windows ML avec support matériel universel  
  - Applications Foundry Local avec implémentation RAG axée sur la confidentialité  

### Questions d'auto-évaluation  

1. Comment l'AI Toolkit simplifie-t-il le workflow de développement EdgeAI ?  
2. Comparez les stratégies de déploiement sur différentes plateformes matérielles.  
3. Quels sont les avantages de Windows AI Foundry pour le développement en périphérie ?  
4. Expliquez le rôle de l'optimisation NPU dans les applications modernes d'Edge AI.  
5. Comment l'API Phi Silica exploite-t-elle le matériel NPU pour optimiser les performances ?  
6. Comparez les avantages du déploiement local et cloud pour les applications sensibles à la confidentialité.  

### Exercices pratiques  

1. **Configuration AI Toolkit** : Configurez AI Toolkit et optimisez un modèle (1 heure).  
2. **Windows AI Foundry** : Construisez une application Windows AI simple en utilisant l'API Phi Silica (1 heure).  
3. **Déploiement multi-plateformes** : Déployez le même modèle sur deux plateformes différentes (1 heure).  
4. **Optimisation NPU** : Testez les performances NPU avec les outils Windows AI Foundry (30 minutes).  

## Module 8 : Microsoft Foundry Local – Kit complet pour développeurs  

### Objectifs d'apprentissage clés  

- Installer et configurer Foundry Local sur Windows  
- Exécuter, découvrir et gérer des modèles localement via le CLI Foundry  
- Intégrer avec des clients REST et SDK compatibles OpenAI  
- Construire des exemples pratiques : chat Chainlit, agents et routeur de modèles  
- Comprendre les modèles hybrides avec Azure AI Foundry  

### Domaines d'étude prioritaires  

- Installation et bases du CLI (modèle, service, cache)  
- Intégration SDK (clients compatibles OpenAI et Azure OpenAI)  
- Validation rapide avec Open WebUI  
- Modèles d'agents et appels de fonctions  
- Modèles comme outils (conception de routeur et registre)  

### Questions d'auto-évaluation  

1. Comment découvrir le point de terminaison local et lister les modèles disponibles ?  
2. Quelles sont les différences entre l'utilisation de Foundry Local REST et Azure OpenAI ?  
3. Comment concevriez-vous un routeur simple pour sélectionner des modèles comme outils ?  
4. Quelles catégories du CLI sont les plus pertinentes pour le développement quotidien ?  
5. Comment valider la préparation de Foundry Local avant d'exécuter des applications ?  

### Exercices pratiques  

1. Installez/mettez à jour Foundry Local et exécutez `phi-4-mini` localement (30 minutes).  
2. Appelez `/v1/models` et exécutez un chat simple via REST (30 minutes).  
3. Lancez l'exemple d'application Chainlit et discutez localement (30 minutes).  
4. Exécutez le coordinateur multi-agents et inspectez les résultats (30 minutes).  
5. Essayez le routeur de modèles comme outils avec des substitutions basées sur l'environnement (30 minutes).  

## Guide de répartition du temps  

Pour vous aider à tirer le meilleur parti des 20 heures de cours, voici une répartition suggérée :  

| Activité | Temps alloué | Description |  
|----------|--------------|-------------|  
| Lecture des matériaux principaux | 9 heures | Concentration sur les concepts essentiels de chaque module |  
| Exercices pratiques | 6 heures | Mise en œuvre pratique des techniques clés |  
| Auto-évaluation | 2 heures | Test de compréhension via des questions et réflexions |  
| Mini-projet | 3 heures | Application des connaissances à une petite implémentation pratique |  

### Domaines prioritaires selon les contraintes de temps  

**Si vous n'avez que 10 heures :**  
- Complétez les modules 1, 2 et 3 (concepts fondamentaux EdgeAI).  
- Réalisez au moins un exercice pratique par module.  
- Concentrez-vous sur la compréhension des concepts clés plutôt que sur les détails d'implémentation.  

**Si vous pouvez consacrer les 20 heures :**  
- Complétez les sept modules.  
- Effectuez les exercices pratiques clés de chaque module.  
- Réalisez un mini-projet du module 7.  
- Explorez au moins 2-3 ressources complémentaires.  

**Si vous avez plus de 20 heures :**  
- Complétez tous les modules avec des exercices détaillés.  
- Réalisez plusieurs mini-projets.  
- Explorez des techniques d'optimisation avancées dans le module 4.  
- Implémentez un déploiement en production à partir du module 5.  

## Ressources essentielles  

Ces ressources soigneusement sélectionnées offrent le meilleur rapport qualité-temps pour vos études :  

### Documentation incontournable  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - L'outil d'optimisation de modèles le plus efficace  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La méthode la plus rapide pour déployer des SLM localement  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Référence pour un modèle optimisé pour la périphérie  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Toolkit d'optimisation complet d'Intel  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Environnement de développement EdgeAI intégré  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plateforme de développement EdgeAI spécifique à Windows  

### Outils gain de temps  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accès rapide aux modèles et déploiement  
- [Gradio](https://www.gradio.app/docs/interface) - Développement rapide d'interface utilisateur pour les démos IA  
- [Microsoft Olive](https://github.com/microsoft/Olive) - Optimisation simplifiée des modèles  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inférence CPU efficace  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework de compression de réseaux neuronaux  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit de déploiement de grands modèles de langage  

## Modèle de suivi des progrès  

Utilisez ce modèle simplifié pour suivre vos progrès dans le cours de 20 heures :  

| Module | Date de complétion | Temps passé | Points clés retenus |  
|--------|--------------------|-------------|----------------------|  
| Module 1 : Fondamentaux EdgeAI | | | |  
| Module 2 : Fondations SLM | | | |  
| Module 3 : Déploiement SLM | | | |  
| Module 4 : Optimisation de modèles | | | |  
| Module 5 : SLMOps | | | |  
| Module 6 : Agents IA | | | |  
| Module 7 : Outils de développement | | | |  
| Module 8 : Toolkit Foundry Local | | | |  
| Exercices pratiques | | | |  
| Mini-projet | | | |  

## Idées de mini-projets  

Considérez la réalisation de l'un de ces projets pour mettre en pratique les concepts EdgeAI (chacun conçu pour durer 2 à 4 heures) :  

### Projets débutants (2-3 heures chacun)  
1. **Assistant texte en périphérie** : Créez un outil simple de complétion de texte hors ligne en utilisant un petit modèle de langage.  
2. **Tableau de comparaison de modèles** : Construisez une visualisation de base des métriques de performance pour différents SLM.  
3. **Expérience d'optimisation** : Mesurez l'impact de différents niveaux de quantification sur le même modèle de base.  

### Projets intermédiaires (3-4 heures chacun)  
4. **Workflow AI Toolkit** : Utilisez l'AI Toolkit de VS Code pour optimiser et déployer un modèle de bout en bout.  
5. **Application Windows AI Foundry** : Créez une application Windows en utilisant l'API Phi Silica et l'optimisation NPU.  
6. **Déploiement multi-plateformes** : Déployez le même modèle optimisé sur Windows (OpenVINO) et mobile (.NET MAUI).  
7. **Agent avec appels de fonctions** : Construisez un agent IA avec des capacités d'appel de fonctions pour des scénarios en périphérie.  

### Projets d'intégration avancés (4-5 heures chacun)  
8. **Pipeline d'optimisation OpenVINO** : Implémentez une optimisation complète du modèle en utilisant NNCF et l'outil GenAI  
9. **Pipeline SLMOps** : Mettez en œuvre un cycle de vie complet du modèle, de l'entraînement au déploiement en périphérie  
10. **Système Edge Multi-Modèles** : Déployez plusieurs modèles spécialisés travaillant ensemble sur du matériel en périphérie  
11. **Système d'intégration MCP** : Construisez un système agentique utilisant le Model Context Protocol pour l'interaction avec des outils  

## Références

- Microsoft Learn (Foundry Local)  
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Démarrage : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Référence CLI : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Intégration avec les SDKs d'inférence : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Guide pour ouvrir WebUI : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Compilation de modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agents (vue d'ensemble) : https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Outils d'optimisation et d'inférence  
  - Microsoft Olive (docs) : https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub) : https://github.com/microsoft/Olive  
  - ONNX Runtime (démarrage) : https://onnxruntime.ai/docs/get-started/with-python.html  
  - Intégration ONNX Runtime Olive : https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (docs) : https://docs.openvino.ai/2025/index.html  
  - Apple MLX (docs) : https://ml-explore.github.io/mlx/build/html/index.html  
- Frameworks de déploiement et modèles  
  - Llama.cpp : https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers : https://huggingface.co/docs/transformers/index  
  - vLLM (docs) : https://docs.vllm.ai/  
  - Ollama (démarrage rapide) : https://github.com/ollama/ollama#get-started  
- Outils pour développeurs (Windows et VS Code)  
  - AI Toolkit pour VS Code : https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (vue d'ensemble) : https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Communauté d'apprentissage

Rejoignez la discussion et connectez-vous avec d'autres apprenants :  
- Discussions GitHub sur le [dépôt EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Conclusion

EdgeAI représente l'avant-garde de la mise en œuvre de l'intelligence artificielle, apportant des capacités puissantes directement aux appareils tout en répondant à des préoccupations essentielles telles que la confidentialité, la latence et la connectivité. Ce cours de 20 heures vous fournit les connaissances essentielles et les compétences pratiques pour commencer à travailler immédiatement avec les technologies EdgeAI.

Le cours est délibérément concis et axé sur les concepts les plus importants, vous permettant d'acquérir rapidement une expertise précieuse sans un engagement de temps écrasant. N'oubliez pas que la pratique, même avec des exemples simples, est la clé pour renforcer ce que vous avez appris.

Bon apprentissage !

---

