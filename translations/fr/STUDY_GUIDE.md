<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-10-08T18:52:41+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "fr"
}
-->
# EdgeAI pour Débutants : Parcours d'apprentissage et calendrier d'étude

### Parcours d'apprentissage intensif (1 semaine)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 2 | Module 2 : Fondations des SLM | 3 heures |
| Jour 3 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 4-5 | Module 4 : Optimisation des modèles (6 frameworks) | 4 heures |
| Jour 6 | Module 5 : SLMOps | 3 heures |
| Jour 7 | Module 6-7 : Agents IA & Outils de développement | 4 heures |
| Jour 8 | Module 8 : Toolkit local Foundry (Implémentation moderne) | 1 heure |

### Parcours d'apprentissage intensif (2 semaines)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Agents IA | 2 heures |
| Jour 13-14 | Module 7 : Outils de développement | 3 heures |

### Étude à temps partiel (4 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de développement & Intégration | 3 heures |

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Systèmes agentiques SLM | 2 heures |
| Jour 13-14 | Module 7 : Échantillons d'implémentation EdgeAI | 2 heures |

| Module | Date de fin | Heures passées | Points clés |
|--------|----------------|-------------|--------------|
| Module 0 : Introduction à EdgeAI | | | |
| Module 1 : Fondamentaux d'EdgeAI | | | |
| Module 2 : Fondations des SLM | | | |
| Module 3 : Déploiement des SLM | | | |
| Module 4 : Optimisation des modèles (6 frameworks) | | | |
| Module 5 : SLMOps | | | |
| Module 6 : Systèmes agentiques SLM | | | |
| Module 7 : Échantillons d'implémentation EdgeAI | | | |
| Exercices pratiques | | | |
| Mini-projet | | | |

### Étude à temps partiel (4 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de développement & Intégration | 3 heures |

## Introduction

Bienvenue dans le guide d'étude EdgeAI pour débutants ! Ce document est conçu pour vous aider à naviguer efficacement dans les matériaux du cours et à maximiser votre expérience d'apprentissage. Il propose des parcours d'apprentissage structurés, des calendriers d'étude suggérés, des résumés des concepts clés et des ressources supplémentaires pour approfondir votre compréhension des technologies Edge AI.

Ce cours concis de 20 heures offre des connaissances essentielles sur EdgeAI dans un format efficace, idéal pour les professionnels et les étudiants occupés qui souhaitent acquérir rapidement des compétences pratiques dans ce domaine émergent.

## Aperçu du cours

Ce cours est organisé en huit modules complets :

0. **Introduction à EdgeAI** - Fondations et mise en contexte avec des applications industrielles et des objectifs d'apprentissage
1. **Fondamentaux et transformation d'EdgeAI** - Comprendre les concepts de base et le changement technologique
2. **Fondations des Small Language Models (SLM)** - Exploration des différentes familles de SLM et de leurs architectures
3. **Déploiement des Small Language Models** - Mise en œuvre de stratégies de déploiement pratiques
4. **Conversion de format de modèle et quantification** - Optimisation avancée avec 6 frameworks, y compris OpenVINO
5. **SLMOps - Opérations des Small Language Models** - Gestion du cycle de vie de production et déploiement
6. **Systèmes agentiques SLM** - Agents IA, appels de fonctions et protocole de contexte de modèle
7. **Échantillons d'implémentation EdgeAI** - Toolkit IA, développement sous Windows et implémentations spécifiques à la plateforme
8. **Microsoft Foundry Local – Toolkit complet pour développeurs** - Développement local-first avec intégration hybride Azure (Module 08)

## Comment utiliser ce guide d'étude

- **Apprentissage progressif** : Suivez les modules dans l'ordre pour une expérience d'apprentissage cohérente
- **Points de contrôle des connaissances** : Utilisez les questions d'auto-évaluation après chaque section
- **Pratique pratique** : Complétez les exercices suggérés pour renforcer les concepts théoriques
- **Ressources supplémentaires** : Explorez des matériaux supplémentaires pour les sujets qui vous intéressent le plus

## Recommandations de calendrier d'étude

### Parcours d'apprentissage intensif (1 semaine)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 6 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 8 heures |
| Jour 5 | Module 3 : Déploiement des SLM | 3 heures |
| Jour 6 | Module 8 : Toolkit local Foundry | 3 heures |

### Étude à temps partiel (3 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 0 : Introduction + Module 1 : Fondamentaux d'EdgeAI | 7-9 heures |
| Semaine 2 | Module 2 : Fondations des SLM | 7-8 heures |
| Semaine 3 | Module 3 : Déploiement des SLM (3h) + Module 8 : Toolkit local Foundry (2-3h) | 5-6 heures |

## Module 0 : Introduction à EdgeAI

### Objectifs d'apprentissage clés

- Comprendre ce qu'est Edge AI et pourquoi il est important dans le paysage technologique actuel
- Identifier les principales industries transformées par Edge AI et leurs cas d'utilisation spécifiques
- Comprendre les avantages des Small Language Models (SLM) pour le déploiement en périphérie
- Établir des attentes claires en matière d'apprentissage et de résultats pour le cours complet
- Reconnaître les opportunités de carrière et les compétences requises dans le domaine d'Edge AI

### Domaines d'étude

#### Section 1 : Paradigme et définition d'Edge AI
- **Concepts prioritaires** : 
  - Edge AI vs traitement traditionnel dans le cloud
  - Convergence entre matériel, optimisation de modèle et exigences commerciales
  - Déploiement d'IA en temps réel, respect de la vie privée et efficacité économique

#### Section 2 : Applications industrielles
- **Concepts prioritaires** : 
  - Fabrication & Industrie 4.0 : Maintenance prédictive et contrôle qualité
  - Santé : Imagerie diagnostique et surveillance des patients
  - Systèmes autonomes : Véhicules autonomes et transport
  - Villes intelligentes : Gestion du trafic et sécurité publique
  - Technologie grand public : Smartphones, wearables et maisons intelligentes

#### Section 3 : Fondations des Small Language Models
- **Concepts prioritaires** : 
  - Caractéristiques des SLM et comparaisons de performances
  - Efficacité des paramètres vs compromis de capacité
  - Contraintes de déploiement en périphérie et stratégies d'optimisation

#### Section 4 : Cadre d'apprentissage et parcours de carrière
- **Concepts prioritaires** : 
  - Architecture du cours et approche de maîtrise progressive
  - Compétences techniques et objectifs de mise en œuvre pratique
  - Opportunités de carrière et applications industrielles

### Questions d'auto-évaluation

1. Quels sont les trois principales tendances technologiques qui ont permis Edge AI ?
2. Comparez les avantages et les défis de Edge AI par rapport à l'IA basée sur le cloud.
3. Nommez trois industries où Edge AI apporte une valeur commerciale critique et expliquez pourquoi.
4. Comment les Small Language Models rendent-ils Edge AI pratique pour le déploiement dans le monde réel ?
5. Quelles sont les compétences techniques clés que vous développerez tout au long de ce cours ?
6. Décrivez l'approche d'apprentissage en quatre phases utilisée dans ce cours.

### Exercices pratiques

1. **Recherche industrielle** : Choisissez une application industrielle et recherchez une implémentation réelle d'Edge AI (30 minutes)
2. **Exploration de modèles** : Parcourez les Small Language Models disponibles sur Hugging Face et comparez leurs nombres de paramètres et capacités (30 minutes)
3. **Planification d'apprentissage** : Passez en revue la structure complète du cours et créez votre propre calendrier d'étude (15 minutes)

### Matériaux supplémentaires

- [Aperçu du marché Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Aperçu des Small Language Models - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Fondation Edge Computing](https://www.edgecomputing.org/)

## Module 1 : Fondamentaux et transformation d'EdgeAI

### Objectifs d'apprentissage clés

- Comprendre les différences entre l'IA basée sur le cloud et l'IA basée sur la périphérie
- Maîtriser les techniques d'optimisation de base pour les environnements à ressources limitées
- Analyser les applications réelles des technologies EdgeAI
- Configurer un environnement de développement pour les projets EdgeAI

### Domaines d'étude

#### Section 1 : Fondamentaux d'EdgeAI
- **Concepts prioritaires** : 
  - Paradigmes Edge vs Cloud computing
  - Techniques de quantification des modèles
  - Options d'accélération matérielle (NPUs, GPUs, CPUs)
  - Avantages en matière de confidentialité et de sécurité

- **Matériaux supplémentaires** :
  - [Documentation TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentation Edge Impulse](https://docs.edgeimpulse.com)

#### Section 2 : Études de cas réels
- **Concepts prioritaires** : 
  - Écosystème de modèles Microsoft Phi & Mu
  - Implémentations pratiques dans les industries
  - Considérations de déploiement

#### Section 3 : Guide d'implémentation pratique
- **Concepts prioritaires** : 
  - Configuration de l'environnement de développement
  - Outils de quantification et d'optimisation
  - Méthodes d'évaluation pour les implémentations EdgeAI

#### Section 4 : Matériel de déploiement en périphérie
- **Concepts prioritaires** : 
  - Comparaisons des plateformes matérielles
  - Stratégies d'optimisation pour un matériel spécifique
  - Considérations de déploiement

### Questions d'auto-évaluation

1. Comparez et contrastez l'IA basée sur le cloud avec les implémentations d'IA en périphérie.
2. Expliquez trois techniques clés pour optimiser les modèles pour le déploiement en périphérie.
3. Quels sont les principaux avantages de l'exécution de modèles d'IA en périphérie ?
4. Décrivez le processus de quantification d'un modèle et son impact sur les performances.
5. Expliquez comment différents accélérateurs matériels (NPUs, GPUs, CPUs) influencent le déploiement EdgeAI.

### Exercices pratiques

1. **Configuration rapide de l'environnement** : Configurez un environnement de développement minimal avec les packages essentiels (30 minutes)
2. **Exploration de modèles** : Téléchargez et examinez un modèle de langage pré-entraîné (1 heure)
3. **Quantification de base** : Essayez une quantification simple sur un petit modèle (1 heure)

## Module 2 : Fondations des Small Language Models

### Objectifs d'apprentissage clés

- Comprendre les principes architecturaux des différentes familles de SLM
- Comparer les capacités des modèles selon différentes échelles de paramètres
- Évaluer les modèles en fonction de leur efficacité, capacité et exigences de déploiement
- Reconnaître les cas d'utilisation appropriés pour différentes familles de modèles

### Domaines d'étude

#### Section 1 : Famille de modèles Microsoft Phi
- **Concepts prioritaires** : 
  - Évolution de la philosophie de conception
  - Architecture axée sur l'efficacité
  - Capacités spécialisées

#### Section 2 : Famille Qwen
- **Concepts prioritaires** : 
  - Contributions open source
  - Options de déploiement évolutives
  - Architecture de raisonnement avancée

#### Section 3 : Famille Gemma
- **Concepts prioritaires** : 
  - Innovation axée sur la recherche
  - Capacités multimodales
  - Optimisation pour mobile

#### Section 4 : Famille BitNET
- **Concepts prioritaires** : 
  - Technologie de quantification à 1 bit
  - Cadre d'optimisation d'inférence
  - Considérations de durabilité

#### Section 5 : Modèle Microsoft Mu
- **Concepts prioritaires** : 
  - Architecture axée sur les appareils
  - Intégration système avec Windows
  - Fonctionnement respectueux de la vie privée

#### Section 6 : Phi-Silica
- **Concepts prioritaires** : 
  - Architecture optimisée pour NPU
  - Indicateurs de performance
  - Intégration pour développeurs

### Questions d'auto-évaluation

1. Comparez les approches architecturales des familles de modèles Phi et Qwen.
2. Expliquez comment la technologie de quantification de BitNET diffère de la quantification traditionnelle.
3. Quels sont les avantages uniques du modèle Mu pour l'intégration avec Windows ?
4. Décrivez comment Phi-Silica exploite le matériel NPU pour optimiser les performances.
5. Pour une application mobile avec une connectivité limitée, quelle famille de modèles serait la plus appropriée et pourquoi ?

### Exercices pratiques

1. **Comparaison de modèles** : Benchmark rapide de deux modèles SLM différents (1 heure)
2. **Génération de texte simple** : Implémentation basique de génération de texte avec un petit modèle (1 heure)
3. **Optimisation rapide** : Appliquer une technique d'optimisation pour améliorer la vitesse d'inférence (1 heure)

## Module 3 : Déploiement de petits modèles de langage (SLM)

### Objectifs d'apprentissage clés

- Sélectionner des modèles adaptés en fonction des contraintes de déploiement
- Maîtriser les techniques d'optimisation pour divers scénarios de déploiement
- Implémenter des SLM dans des environnements locaux et cloud
- Concevoir des configurations prêtes pour la production dans des applications EdgeAI

### Domaines d'étude prioritaires

#### Section 1 : Apprentissage avancé des SLM
- **Concepts prioritaires** : 
  - Cadre de classification des paramètres
  - Techniques d'optimisation avancées
  - Stratégies d'acquisition de modèles

#### Section 2 : Déploiement en environnement local
- **Concepts prioritaires** : 
  - Déploiement sur la plateforme Ollama
  - Solutions locales Microsoft Foundry
  - Analyse comparative des frameworks

#### Section 3 : Déploiement cloud conteneurisé
- **Concepts prioritaires** : 
  - Inférence haute performance avec vLLM
  - Orchestration de conteneurs
  - Implémentation ONNX Runtime

### Questions d'auto-évaluation

1. Quels facteurs doivent être pris en compte lors du choix entre un déploiement local et un déploiement cloud ?
2. Comparez Ollama et Microsoft Foundry Local en tant qu'options de déploiement.
3. Expliquez les avantages de la conteneurisation pour le déploiement des SLM.
4. Quels sont les principaux indicateurs de performance à surveiller pour un SLM déployé en périphérie ?
5. Décrivez un workflow complet de déploiement, de la sélection du modèle à la mise en production.

### Exercices pratiques

1. **Déploiement local basique** : Déployer un SLM simple en utilisant Ollama (1 heure)
2. **Vérification des performances** : Effectuer un benchmark rapide sur le modèle déployé (30 minutes)
3. **Intégration simple** : Créer une application minimale utilisant le modèle déployé (1 heure)

## Module 4 : Conversion de format et quantification des modèles

### Objectifs d'apprentissage clés

- Maîtriser les techniques avancées de quantification, de 1 bit à 8 bits de précision
- Comprendre les stratégies de conversion de format (GGUF, ONNX)
- Implémenter des optimisations sur six frameworks (Llama.cpp, Olive, OpenVINO, MLX, synthèse de workflow)
- Déployer des modèles optimisés pour des environnements de production en périphérie sur des matériels Intel, Apple et multiplateformes

### Domaines d'étude prioritaires

#### Section 1 : Fondements de la quantification
- **Concepts prioritaires** : 
  - Cadre de classification de précision
  - Compromis entre performance et précision
  - Optimisation de l'empreinte mémoire

#### Section 2 : Implémentation avec Llama.cpp
- **Concepts prioritaires** : 
  - Déploiement multiplateforme
  - Optimisation du format GGUF
  - Techniques d'accélération matérielle

#### Section 3 : Suite Microsoft Olive
- **Concepts prioritaires** : 
  - Optimisation adaptée au matériel
  - Déploiement de niveau entreprise
  - Workflows d'optimisation automatisés

#### Section 4 : Toolkit OpenVINO
- **Concepts prioritaires** : 
  - Optimisation pour matériel Intel
  - Framework de compression de réseaux neuronaux (NNCF)
  - Déploiement d'inférence multiplateforme
  - OpenVINO GenAI pour le déploiement de LLM

#### Section 5 : Framework Apple MLX
- **Concepts prioritaires** : 
  - Optimisation pour Apple Silicon
  - Architecture mémoire unifiée
  - Capacités de fine-tuning LoRA

#### Section 6 : Synthèse de workflow pour le développement Edge AI
- **Concepts prioritaires** : 
  - Architecture de workflow unifiée
  - Arbres de décision pour la sélection de frameworks
  - Validation de la préparation à la production
  - Stratégies de pérennisation

### Questions d'auto-évaluation

1. Comparez les stratégies de quantification selon différents niveaux de précision (1 bit à 8 bits).
2. Expliquez les avantages du format GGUF pour le déploiement en périphérie.
3. Comment l'optimisation adaptée au matériel dans Microsoft Olive améliore-t-elle l'efficacité du déploiement ?
4. Quels sont les principaux avantages du NNCF d'OpenVINO pour la compression des modèles ?
5. Décrivez comment Apple MLX exploite l'architecture mémoire unifiée pour l'optimisation.
6. Comment la synthèse de workflow aide-t-elle à sélectionner les frameworks d'optimisation optimaux ?

### Exercices pratiques

1. **Quantification de modèle** : Appliquer différents niveaux de quantification à un modèle et comparer les résultats (1 heure)
2. **Optimisation OpenVINO** : Utiliser NNCF pour compresser un modèle destiné au matériel Intel (1 heure)
3. **Comparaison de frameworks** : Tester le même modèle sur trois frameworks d'optimisation différents (1 heure)
4. **Benchmarking de performances** : Mesurer l'impact de l'optimisation sur la vitesse d'inférence et l'utilisation de la mémoire (1 heure)

## Module 5 : SLMOps - Opérations pour petits modèles de langage

### Objectifs d'apprentissage clés

- Comprendre les principes de gestion du cycle de vie des SLMOps
- Maîtriser les techniques de distillation et de fine-tuning pour le déploiement en périphérie
- Implémenter des stratégies de déploiement en production avec surveillance
- Construire des workflows d'opérations et de maintenance de SLM de niveau entreprise

### Domaines d'étude prioritaires

#### Section 1 : Introduction aux SLMOps
- **Concepts prioritaires** : 
  - Changement de paradigme des SLMOps dans les opérations d'IA
  - Architecture axée sur l'efficacité des coûts et la confidentialité
  - Impact stratégique sur les entreprises et avantages compétitifs

#### Section 2 : Distillation de modèles
- **Concepts prioritaires** : 
  - Techniques de transfert de connaissances
  - Implémentation du processus de distillation en deux étapes
  - Workflows de distillation Azure ML

#### Section 3 : Stratégies de fine-tuning
- **Concepts prioritaires** : 
  - Fine-tuning efficace des paramètres (PEFT)
  - Méthodes avancées LoRA et QLoRA
  - Entraînement multi-adaptateurs et optimisation des hyperparamètres

#### Section 4 : Déploiement en production
- **Concepts prioritaires** : 
  - Conversion et quantification de modèles pour la production
  - Configuration de déploiement Foundry Local
  - Benchmarking des performances et validation de la qualité

### Questions d'auto-évaluation

1. En quoi les SLMOps diffèrent-ils des MLOps traditionnels ?
2. Expliquez les avantages de la distillation de modèles pour le déploiement en périphérie.
3. Quels sont les principaux éléments à prendre en compte pour le fine-tuning des SLM dans des environnements à ressources limitées ?
4. Décrivez un pipeline complet de déploiement en production pour des applications Edge AI.

### Exercices pratiques

1. **Distillation basique** : Créer un modèle plus petit à partir d'un modèle enseignant plus grand (1 heure)
2. **Expérience de fine-tuning** : Fine-tuner un modèle pour un domaine spécifique (1 heure)
3. **Pipeline de déploiement** : Configurer un pipeline CI/CD basique pour le déploiement de modèles (1 heure)

## Module 6 : Systèmes agentiques SLM - Agents IA et appels de fonctions

### Objectifs d'apprentissage clés

- Construire des agents IA intelligents pour des environnements en périphérie en utilisant des petits modèles de langage
- Implémenter des capacités d'appel de fonctions avec des workflows systématiques
- Maîtriser l'intégration du protocole de contexte de modèle (MCP) pour une interaction standardisée avec les outils
- Créer des systèmes agentiques sophistiqués avec une intervention humaine minimale

### Domaines d'étude prioritaires

#### Section 1 : Agents IA et fondements des SLM
- **Concepts prioritaires** : 
  - Cadre de classification des agents (réflexes, basés sur des modèles, basés sur des objectifs, agents apprenants)
  - Analyse des compromis entre SLM et LLM
  - Modèles de conception spécifiques aux agents en périphérie
  - Optimisation des ressources pour les agents

#### Section 2 : Appels de fonctions dans les petits modèles de langage
- **Concepts prioritaires** : 
  - Implémentation de workflows systématiques (détection d'intention, sortie JSON, exécution externe)
  - Implémentations spécifiques aux plateformes (Phi-4-mini, modèles Qwen sélectionnés, Microsoft Foundry Local)
  - Exemples avancés (collaboration multi-agents, sélection dynamique d'outils)
  - Considérations pour la production (limitation de taux, journalisation des audits, mesures de sécurité)

#### Section 3 : Intégration du protocole de contexte de modèle (MCP)
- **Concepts prioritaires** : 
  - Architecture du protocole et conception de systèmes en couches
  - Support multi-backend (Ollama pour le développement, vLLM pour la production)
  - Protocoles de connexion (modes STDIO et SSE)
  - Applications réelles (automatisation web, traitement de données, intégration API)

### Questions d'auto-évaluation

1. Quels sont les principaux éléments architecturaux à prendre en compte pour les agents IA en périphérie ?
2. Comment les appels de fonctions améliorent-ils les capacités des agents ?
3. Expliquez le rôle du protocole de contexte de modèle dans la communication des agents.

### Exercices pratiques

1. **Agent simple** : Construire un agent IA basique avec des appels de fonctions (1 heure)
2. **Intégration MCP** : Implémenter MCP dans une application d'agent (30 minutes)

## Atelier : Parcours d'apprentissage pratique

### Objectifs d'apprentissage clés

- Construire des applications IA prêtes pour la production en utilisant le SDK Foundry Local et les meilleures pratiques
- Implémenter une gestion complète des erreurs et des modèles de retour utilisateur
- Créer des pipelines RAG avec évaluation de la qualité et surveillance des performances
- Développer des systèmes multi-agents avec des modèles de coordination
- Maîtriser le routage intelligent des modèles pour la sélection basée sur les tâches
- Déployer des solutions IA locales avec des architectures respectueuses de la confidentialité

### Domaines d'étude prioritaires

#### Session 01 : Premiers pas avec Foundry Local
- **Concepts prioritaires** :
  - Intégration du SDK FoundryLocalManager et découverte automatique des services
  - Implémentations de chat basique et en streaming
  - Modèles de gestion des erreurs et retour utilisateur
  - Configuration basée sur l'environnement

#### Session 02 : Construire des solutions IA avec RAG
- **Concepts prioritaires** :
  - Embeddings vectoriels en mémoire avec sentence-transformers
  - Implémentation de pipeline RAG (recherche → génération)
  - Évaluation de la qualité avec les métriques RAGAS
  - Sécurité des imports pour les dépendances optionnelles

#### Session 03 : Modèles open source
- **Concepts prioritaires** :
  - Stratégies de benchmarking multi-modèles
  - Mesures de latence et de débit
  - Dégradation progressive et récupération des erreurs
  - Comparaison des performances entre familles de modèles

#### Session 04 : Modèles de pointe
- **Concepts prioritaires** :
  - Méthodologie de comparaison SLM vs LLM
  - Indications de type et formatage complet des sorties
  - Gestion des erreurs par modèle
  - Résultats structurés pour l'analyse

#### Session 05 : Agents alimentés par l'IA
- **Concepts prioritaires** :
  - Orchestration multi-agents avec modèle de coordination
  - Gestion de la mémoire des agents et suivi des états
  - Gestion des erreurs dans les pipelines et journalisation des étapes
  - Surveillance des performances et statistiques

#### Session 06 : Modèles en tant qu'outils
- **Concepts prioritaires** :
  - Détection d'intention et correspondance de modèles
  - Algorithmes de routage de modèles basés sur des mots-clés
  - Pipelines multi-étapes (planification → exécution → affinage)
  - Documentation complète des fonctions

### Questions d'auto-évaluation

1. Comment FoundryLocalManager simplifie-t-il la gestion des services par rapport aux appels REST manuels ?
2. Expliquez l'importance des gardes d'importation pour les dépendances optionnelles comme sentence-transformers.
3. Quelles stratégies garantissent une dégradation progressive dans le benchmarking multi-modèles ?
4. Comment le modèle de coordination orchestre-t-il plusieurs agents spécialisés ?
5. Décrivez les composants d'un routeur de modèles intelligent.
6. Quels sont les éléments clés d'une gestion des erreurs prête pour la production ?

### Exercices pratiques

1. **Application de chat** : Implémenter un chat en streaming avec gestion des erreurs (45 minutes)
2. **Pipeline RAG** : Construire un pipeline RAG minimal avec évaluation de la qualité (1 heure)
3. **Benchmarking de modèles** : Comparer 3+ modèles sur leurs performances (1 heure)
4. **Système multi-agents** : Créer un coordinateur avec 2 agents spécialisés (1,5 heure)
5. **Routeur intelligent** : Construire une sélection de modèles basée sur les tâches (1 heure)
6. **Déploiement en production** : Ajouter une surveillance et une gestion complète des erreurs (45 minutes)

### Répartition du temps

**Apprentissage concentré (1 semaine)** :
- Jour 1 : Session 01-02 (Chat + RAG) - 3 heures
- Jour 2 : Session 03-04 (Benchmarking + Comparaison) - 3 heures
- Jour 3 : Session 05-06 (Agents + Routage) - 3 heures
- Jour 4 : Exercices pratiques et validation - 2 heures

**Étude à temps partiel (2 semaines)** :
- Semaine 1 : Sessions 01-03 (6 heures au total)
- Semaine 2 : Sessions 04-06 + exercices (5 heures au total)

## Module 7 : Exemples d'implémentation EdgeAI

### Objectifs d'apprentissage clés

- Maîtriser l'AI Toolkit pour Visual Studio Code pour des workflows de développement EdgeAI complets
- Acquérir une expertise sur la plateforme Windows AI Foundry et les stratégies d'optimisation NPU
- Implémenter EdgeAI sur plusieurs plateformes matérielles et scénarios de déploiement
- Construire des applications EdgeAI prêtes pour la production avec des optimisations spécifiques à la plateforme

### Domaines d'étude prioritaires

#### Section 1 : AI Toolkit pour Visual Studio Code
- **Concepts prioritaires** : 
  - Environnement de développement Edge AI complet dans VS Code
  - Catalogue de modèles et découverte pour le déploiement en périphérie
  - Workflows de test local, optimisation et développement d'agents
  - Surveillance des performances et évaluation pour les scénarios en périphérie

#### Section 2 : Guide de développement Windows EdgeAI
- **Concepts prioritaires** : 
  - Aperçu complet de la plateforme Windows AI Foundry
  - API Phi Silica pour une inférence NPU efficace
  - API de vision par ordinateur pour le traitement d'images et l'OCR
  - CLI Foundry Local pour le développement et les tests locaux

#### Section 3 : Implémentations spécifiques à la plateforme
- **Concepts prioritaires** : 
  - Déploiement sur NVIDIA Jetson Orin Nano (performance IA de 67 TOPS)
  - Applications mobiles avec .NET MAUI et ONNX Runtime GenAI
  - Solutions Azure EdgeAI avec architecture hybride cloud-périphérie
  - Optimisation Windows ML avec support matériel universel
  - Applications Foundry Local avec implémentation RAG axée sur la confidentialité

### Questions d'auto-évaluation

1. Comment l'AI Toolkit simplifie-t-il le workflow de développement EdgeAI ?
2. Comparez les stratégies de déploiement sur différentes plateformes matérielles.
3. Quels sont les avantages de Windows AI Foundry pour le développement en périphérie ?
4. Expliquez le rôle de l'optimisation NPU dans les applications modernes d'IA en périphérie.  
5. Comment l'API Phi Silica exploite-t-elle le matériel NPU pour optimiser les performances ?  
6. Comparez les avantages du déploiement local par rapport au déploiement dans le cloud pour les applications sensibles à la confidentialité.  

### Exercices pratiques

1. **Configuration de l'AI Toolkit** : Configurez l'AI Toolkit et optimisez un modèle (1 heure)  
2. **Windows AI Foundry** : Créez une application simple Windows AI en utilisant l'API Phi Silica (1 heure)  
3. **Déploiement multiplateforme** : Déployez le même modèle sur deux plateformes différentes (1 heure)  
4. **Optimisation NPU** : Testez les performances du NPU avec les outils Windows AI Foundry (30 minutes)  

## Module 8 : Microsoft Foundry Local – Kit complet pour développeurs (modernisé)

### Objectifs d'apprentissage clés

- Installer et configurer Foundry Local avec une intégration moderne du SDK  
- Implémenter des systèmes multi-agents avancés avec des modèles de coordination  
- Construire des routeurs de modèles intelligents avec une sélection automatique basée sur les tâches  
- Déployer des solutions d'IA prêtes pour la production avec un suivi complet  
- Intégrer avec Azure AI Foundry pour des scénarios de déploiement hybride  
- Maîtriser les modèles modernes de SDK avec FoundryLocalManager et le client OpenAI  

### Domaines d'étude prioritaires

#### Section 1 : Installation et configuration modernes  
- **Concepts prioritaires** :  
  - Intégration du SDK FoundryLocalManager  
  - Découverte automatique des services et surveillance de l'état  
  - Modèles de configuration basés sur l'environnement  
  - Considérations pour le déploiement en production  

#### Section 2 : Systèmes multi-agents avancés  
- **Concepts prioritaires** :  
  - Modèle de coordination avec des agents spécialisés  
  - Spécialisation des agents pour la récupération, le raisonnement et l'exécution  
  - Mécanismes de boucle de rétroaction pour le raffinement  
  - Suivi des performances et des statistiques  

#### Section 3 : Routage intelligent des modèles  
- **Concepts prioritaires** :  
  - Algorithmes de sélection de modèles basés sur des mots-clés  
  - Prise en charge de plusieurs modèles (général, raisonnement, code, créatif)  
  - Configuration des variables d'environnement pour plus de flexibilité  
  - Vérification de l'état des services et gestion des erreurs  

#### Section 4 : Mise en œuvre prête pour la production  
- **Concepts prioritaires** :  
  - Gestion complète des erreurs et mécanismes de secours  
  - Suivi des requêtes et des performances  
  - Exemples interactifs avec des notebooks Jupyter et des benchmarks  
  - Modèles d'intégration avec des applications existantes  

### Questions d'auto-évaluation

1. En quoi l'approche moderne de FoundryLocalManager diffère-t-elle des appels REST manuels ?  
2. Expliquez le modèle de coordination et comment il orchestre les agents spécialisés.  
3. Comment le routeur intelligent sélectionne-t-il les modèles appropriés en fonction du contenu des requêtes ?  
4. Quels sont les composants clés d'un système d'agent d'IA prêt pour la production ?  
5. Comment mettre en œuvre une surveillance complète de l'état des services Foundry Local ?  
6. Comparez les avantages de l'approche modernisée par rapport aux modèles de mise en œuvre traditionnels.  

### Exercices pratiques

1. **Configuration du SDK moderne** : Configurez FoundryLocalManager avec la découverte automatique des services (30 minutes)  
2. **Système multi-agents** : Exécutez le coordinateur avancé avec des agents spécialisés (30 minutes)  
3. **Routage intelligent** : Testez le routeur de modèles avec différents types de requêtes (30 minutes)  
4. **Exploration interactive** : Utilisez les notebooks Jupyter pour explorer des fonctionnalités avancées (45 minutes)  
5. **Déploiement en production** : Implémentez des modèles de surveillance et de gestion des erreurs (30 minutes)  
6. **Intégration hybride** : Configurez des scénarios de secours avec Azure AI Foundry (30 minutes)  

## Guide de répartition du temps

Pour vous aider à tirer le meilleur parti des 30 heures de cours (y compris l'atelier), voici une répartition suggérée de votre temps :  

| Activité | Allocation de temps | Description |  
|----------|----------------------|-------------|  
| Lecture des documents de base | 12 heures | Se concentrer sur les concepts essentiels de chaque module |  
| Exercices pratiques | 10 heures | Mise en œuvre pratique des techniques clés (y compris l'atelier) |  
| Auto-évaluation | 3 heures | Tester votre compréhension à travers des questions et des réflexions |  
| Mini-projet | 5 heures | Appliquer vos connaissances à une petite mise en œuvre pratique |  

### Domaines prioritaires en fonction des contraintes de temps

**Si vous n'avez que 10 heures :**  
- Complétez le Module 0 (Introduction) et les Modules 1, 2 et 3 (concepts de base d'EdgeAI)  
- Réalisez au moins un exercice pratique par module  
- Concentrez-vous sur la compréhension des concepts de base plutôt que sur les détails de mise en œuvre  

**Si vous pouvez consacrer 20 heures :**  
- Complétez les huit modules (y compris l'Introduction)  
- Réalisez les exercices pratiques clés de chaque module  
- Terminez un mini-projet du Module 7  
- Explorez au moins 2-3 ressources supplémentaires  

**Si vous avez plus de 20 heures :**  
- Complétez tous les modules (y compris l'Introduction) avec des exercices détaillés  
- Réalisez plusieurs mini-projets  
- Explorez des techniques d'optimisation avancées dans le Module 4  
- Implémentez un déploiement en production à partir du Module 5  

## Ressources essentielles

Ces ressources soigneusement sélectionnées offrent le meilleur rapport qualité-temps pour vos études :  

### Documentation incontournable  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - L'outil d'optimisation de modèles le plus efficace  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Le moyen le plus rapide de déployer des SLM localement  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Référence pour un modèle optimisé pour la périphérie  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Kit d'outils d'optimisation complet d'Intel  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Environnement de développement intégré pour EdgeAI  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plateforme de développement EdgeAI spécifique à Windows  

### Outils pour gagner du temps  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accès rapide aux modèles et déploiement  
- [Gradio](https://www.gradio.app/docs/interface) - Développement rapide d'interface utilisateur pour les démos d'IA  
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplification de l'optimisation des modèles  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inférence efficace sur CPU  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Cadre de compression des réseaux neuronaux  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Kit d'outils de déploiement de modèles de langage de grande taille  

## Modèle de suivi des progrès

Utilisez ce modèle simplifié pour suivre vos progrès d'apprentissage tout au long du cours de 20 heures :  

| Module | Date d'achèvement | Heures passées | Points clés retenus |  
|--------|--------------------|----------------|----------------------|  
| Module 0 : Introduction à EdgeAI | | | |  
| Module 1 : Fondamentaux d'EdgeAI | | | |  
| Module 2 : Fondations des SLM | | | |  
| Module 3 : Déploiement des SLM | | | |  
| Module 4 : Optimisation des modèles | | | |  
| Module 5 : SLMOps | | | |  
| Module 6 : Agents d'IA | | | |  
| Module 7 : Outils de développement | | | |  
| Atelier : Apprentissage pratique | | | |  
| Module 8 : Foundry Local Toolkit | | | |  
| Exercices pratiques | | | |  
| Mini-projet | | | |  

## Idées de mini-projets

Envisagez de réaliser l'un de ces projets pour mettre en pratique les concepts d'EdgeAI (chacun conçu pour durer 2 à 4 heures) :  

### Projets débutants (2-3 heures chacun)  
1. **Assistant texte en périphérie** : Créez un outil simple de complétion de texte hors ligne en utilisant un petit modèle de langage  
2. **Tableau de bord de comparaison de modèles** : Construisez une visualisation de base des métriques de performance pour différents SLM  
3. **Expérience d'optimisation** : Mesurez l'impact de différents niveaux de quantification sur le même modèle de base  

### Projets intermédiaires (3-4 heures chacun)  
4. **Workflow AI Toolkit** : Utilisez l'AI Toolkit de VS Code pour optimiser et déployer un modèle de bout en bout  
5. **Application Windows AI Foundry** : Créez une application Windows en utilisant l'API Phi Silica et l'optimisation NPU  
6. **Déploiement multiplateforme** : Déployez le même modèle optimisé sur Windows (OpenVINO) et mobile (.NET MAUI)  
7. **Agent d'appel de fonction** : Construisez un agent d'IA avec des capacités d'appel de fonction pour des scénarios en périphérie  

### Projets d'intégration avancés (4-5 heures chacun)  
8. **Pipeline d'optimisation OpenVINO** : Implémentez une optimisation complète de modèle en utilisant NNCF et le kit d'outils GenAI  
9. **Pipeline SLMOps** : Implémentez un cycle de vie complet de modèle, de l'entraînement au déploiement en périphérie  
10. **Système multi-modèles en périphérie** : Déployez plusieurs modèles spécialisés travaillant ensemble sur du matériel en périphérie  
11. **Système d'intégration MCP** : Construisez un système agentique en utilisant le protocole Model Context pour l'interaction avec les outils  

## Références

- Microsoft Learn (Foundry Local)  
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Démarrage : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Référence CLI : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Intégration avec les SDK d'inférence : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Guide pour l'interface Web ouverte : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Compilation des modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agents (vue d'ensemble) : https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Outils d'optimisation et d'inférence  
  - Microsoft Olive (docs) : https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub) : https://github.com/microsoft/Olive  
  - ONNX Runtime (démarrage) : https://onnxruntime.ai/docs/get-started/with-python.html  
  - Intégration ONNX Runtime Olive : https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (docs) : https://docs.openvino.ai/2025/index.html  
  - Apple MLX (docs) : https://ml-explore.github.io/mlx/build/html/index.html  
- Cadres de déploiement et modèles  
  - Llama.cpp : https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers : https://huggingface.co/docs/transformers/index  
  - vLLM (docs) : https://docs.vllm.ai/  
  - Ollama (démarrage rapide) : https://github.com/ollama/ollama#get-started  
- Outils pour développeurs (Windows et VS Code)  
  - AI Toolkit pour VS Code : https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (vue d'ensemble) : https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Communauté d'apprentissage

Rejoignez la discussion et connectez-vous avec d'autres apprenants :  
- Discussions GitHub sur le [dépôt EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Conclusion

L'EdgeAI représente l'avant-garde de la mise en œuvre de l'intelligence artificielle, apportant des capacités puissantes directement aux appareils tout en répondant aux préoccupations cruciales concernant la confidentialité, la latence et la connectivité. Ce cours de 20 heures vous fournit les connaissances essentielles et les compétences pratiques pour commencer à travailler immédiatement avec les technologies EdgeAI.  

Le cours est conçu pour être concis et axé sur les concepts les plus importants, vous permettant d'acquérir rapidement une expertise précieuse sans un engagement de temps excessif. Rappelez-vous que la pratique, même avec des exemples simples, est la clé pour renforcer ce que vous avez appris.  

Bon apprentissage !  

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.