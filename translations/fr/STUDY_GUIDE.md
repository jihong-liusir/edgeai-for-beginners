<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T07:21:47+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "fr"
}
-->
# EdgeAI pour Débutants : Parcours d'apprentissage et calendrier d'étude

### Parcours d'apprentissage intensif (1 semaine)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 2 | Module 2 : Fondations des SLM | 3 heures |
| Jour 3 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 4-5 | Module 4 : Optimisation des modèles (6 frameworks) | 4 heures |
| Jour 6 | Module 5 : SLMOps | 3 heures |
| Jour 7 | Module 6-7 : Agents IA & Outils de développement | 4 heures |
| Jour 8 | Module 8 : Toolkit local Foundry (Implémentation moderne) | 1 heure |

### Parcours d'apprentissage intensif (2 semaines)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Agents IA | 2 heures |
| Jour 13-14 | Module 7 : Outils de développement | 3 heures |

### Étude à temps partiel (4 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de développement & Intégration | 3 heures |

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 3 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 3 heures |
| Jour 5-6 | Module 3 : Déploiement des SLM | 2 heures |
| Jour 7-8 | Module 4 : Optimisation des modèles | 4 heures |
| Jour 9-10 | Module 5 : SLMOps | 3 heures |
| Jour 11-12 | Module 6 : Systèmes agentiques SLM | 2 heures |
| Jour 13-14 | Module 7 : Échantillons d'implémentation EdgeAI | 2 heures |

| Module | Date de fin | Temps passé | Points clés |
|--------|----------------|-------------|--------------|
| Module 0 : Introduction à EdgeAI | | | |
| Module 1 : Fondamentaux d'EdgeAI | | | |
| Module 2 : Fondations des SLM | | | |
| Module 3 : Déploiement des SLM | | | |
| Module 4 : Optimisation des modèles (6 frameworks) | | | |
| Module 5 : SLMOps | | | |
| Module 6 : Systèmes agentiques SLM | | | |
| Module 7 : Échantillons d'implémentation EdgeAI | | | |
| Exercices pratiques | | | |
| Mini-projet | | | |

### Étude à temps partiel (4 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 1-2 : Fondamentaux & Fondations des SLM | 6 heures |
| Semaine 2 | Module 3-4 : Déploiement & Optimisation | 6 heures |
| Semaine 3 | Module 5-6 : SLMOps & Agents IA | 5 heures |
| Semaine 4 | Module 7 : Outils de développement & Intégration | 3 heures |

## Introduction

Bienvenue dans le guide d'étude EdgeAI pour débutants ! Ce document est conçu pour vous aider à naviguer efficacement dans les matériaux du cours et à maximiser votre expérience d'apprentissage. Il propose des parcours d'apprentissage structurés, des calendriers d'étude suggérés, des résumés des concepts clés et des ressources supplémentaires pour approfondir votre compréhension des technologies Edge AI.

Ce cours concis de 20 heures offre des connaissances essentielles sur EdgeAI dans un format efficace, idéal pour les professionnels et les étudiants occupés qui souhaitent acquérir rapidement des compétences pratiques dans ce domaine émergent.

## Aperçu du cours

Ce cours est organisé en huit modules complets :

0. **Introduction à EdgeAI** - Fondations et mise en contexte avec des applications industrielles et des objectifs d'apprentissage
1. **Fondamentaux et transformation d'EdgeAI** - Comprendre les concepts de base et le changement technologique
2. **Fondations des Small Language Models (SLM)** - Exploration des différentes familles de SLM et de leurs architectures
3. **Déploiement des Small Language Models** - Mise en œuvre de stratégies de déploiement pratiques
4. **Conversion de format de modèle et quantification** - Optimisation avancée avec 6 frameworks, y compris OpenVINO
5. **SLMOps - Opérations des Small Language Models** - Gestion du cycle de vie en production et déploiement
6. **Systèmes agentiques SLM** - Agents IA, appels de fonctions et protocole de contexte de modèle
7. **Échantillons d'implémentation EdgeAI** - Toolkit IA, développement sous Windows et implémentations spécifiques à la plateforme
8. **Microsoft Foundry Local – Toolkit complet pour développeurs** - Développement local avec intégration hybride Azure (Module 08)

## Comment utiliser ce guide d'étude

- **Apprentissage progressif** : Suivez les modules dans l'ordre pour une expérience d'apprentissage cohérente
- **Points de contrôle des connaissances** : Utilisez les questions d'auto-évaluation après chaque section
- **Pratique concrète** : Réalisez les exercices suggérés pour renforcer les concepts théoriques
- **Ressources supplémentaires** : Explorez des matériaux complémentaires pour les sujets qui vous intéressent le plus

## Recommandations de calendrier d'étude

### Parcours d'apprentissage intensif (1 semaine)

| Jour | Focus | Heures estimées |
|------|-------|------------------|
| Jour 0 | Module 0 : Introduction à EdgeAI | 1-2 heures |
| Jour 1-2 | Module 1 : Fondamentaux d'EdgeAI | 6 heures |
| Jour 3-4 | Module 2 : Fondations des SLM | 8 heures |
| Jour 5 | Module 3 : Déploiement des SLM | 3 heures |
| Jour 6 | Module 8 : Toolkit local Foundry | 3 heures |

### Étude à temps partiel (3 semaines)

| Semaine | Focus | Heures estimées |
|------|-------|------------------|
| Semaine 1 | Module 0 : Introduction + Module 1 : Fondamentaux d'EdgeAI | 7-9 heures |
| Semaine 2 | Module 2 : Fondations des SLM | 7-8 heures |
| Semaine 3 | Module 3 : Déploiement des SLM (3h) + Module 8 : Toolkit local Foundry (2-3h) | 5-6 heures |

## Module 0 : Introduction à EdgeAI

### Objectifs d'apprentissage clés

- Comprendre ce qu'est Edge AI et pourquoi il est important dans le paysage technologique actuel
- Identifier les principales industries transformées par Edge AI et leurs cas d'utilisation spécifiques
- Comprendre les avantages des Small Language Models (SLM) pour le déploiement en périphérie
- Établir des attentes claires en matière d'apprentissage et de résultats pour le cours complet
- Reconnaître les opportunités de carrière et les compétences requises dans le domaine d'Edge AI

### Domaines d'étude prioritaires

#### Section 1 : Paradigme et définition d'Edge AI
- **Concepts prioritaires** : 
  - Edge AI vs traitement traditionnel dans le cloud
  - Convergence entre matériel, optimisation de modèle et exigences commerciales
  - Déploiement d'IA en temps réel, respectueux de la vie privée et rentable

#### Section 2 : Applications industrielles
- **Concepts prioritaires** : 
  - Fabrication & Industrie 4.0 : Maintenance prédictive et contrôle qualité
  - Santé : Imagerie diagnostique et surveillance des patients
  - Systèmes autonomes : Véhicules autonomes et transport
  - Villes intelligentes : Gestion du trafic et sécurité publique
  - Technologie grand public : Smartphones, wearables et maisons connectées

#### Section 3 : Fondations des Small Language Models
- **Concepts prioritaires** : 
  - Caractéristiques des SLM et comparaisons de performances
  - Efficacité des paramètres vs compromis de capacités
  - Contraintes de déploiement en périphérie et stratégies d'optimisation

#### Section 4 : Cadre d'apprentissage et parcours de carrière
- **Concepts prioritaires** : 
  - Architecture du cours et approche de maîtrise progressive
  - Compétences techniques et objectifs de mise en œuvre pratique
  - Opportunités de carrière et applications industrielles

### Questions d'auto-évaluation

1. Quels sont les trois principales tendances technologiques qui ont permis Edge AI ?
2. Comparez les avantages et les défis de Edge AI par rapport à l'IA basée sur le cloud.
3. Nommez trois industries où Edge AI apporte une valeur commerciale critique et expliquez pourquoi.
4. Comment les Small Language Models rendent-ils Edge AI pratique pour le déploiement réel ?
5. Quelles sont les compétences techniques clés que vous développerez tout au long de ce cours ?
6. Décrivez l'approche d'apprentissage en quatre phases utilisée dans ce cours.

### Exercices pratiques

1. **Recherche industrielle** : Choisissez une application industrielle et recherchez une implémentation réelle d'Edge AI (30 minutes)
2. **Exploration de modèles** : Parcourez les Small Language Models disponibles sur Hugging Face et comparez leurs nombres de paramètres et capacités (30 minutes)
3. **Planification d'apprentissage** : Passez en revue la structure complète du cours et créez votre propre calendrier d'étude (15 minutes)

### Matériaux complémentaires

- [Aperçu du marché Edge AI - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Aperçu des Small Language Models - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Fondation Edge Computing](https://www.edgecomputing.org/)

## Module 1 : Fondamentaux et transformation d'EdgeAI

### Objectifs d'apprentissage clés

- Comprendre les différences entre l'IA basée sur le cloud et l'IA basée sur la périphérie
- Maîtriser les techniques d'optimisation essentielles pour des environnements à ressources limitées
- Analyser des applications réelles des technologies EdgeAI
- Configurer un environnement de développement pour des projets EdgeAI

### Domaines d'étude prioritaires

#### Section 1 : Fondamentaux d'EdgeAI
- **Concepts prioritaires** : 
  - Paradigmes Edge vs Cloud computing
  - Techniques de quantification des modèles
  - Options d'accélération matérielle (NPUs, GPUs, CPUs)
  - Avantages en matière de confidentialité et de sécurité

- **Matériaux complémentaires** :
  - [Documentation TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentation Edge Impulse](https://docs.edgeimpulse.com)

#### Section 2 : Études de cas réelles
- **Concepts prioritaires** : 
  - Écosystème de modèles Microsoft Phi & Mu
  - Implémentations pratiques dans diverses industries
  - Considérations de déploiement

#### Section 3 : Guide d'implémentation pratique
- **Concepts prioritaires** : 
  - Configuration de l'environnement de développement
  - Outils de quantification et d'optimisation
  - Méthodes d'évaluation pour les implémentations EdgeAI

#### Section 4 : Matériel de déploiement en périphérie
- **Concepts prioritaires** : 
  - Comparaisons des plateformes matérielles
  - Stratégies d'optimisation pour des matériels spécifiques
  - Considérations de déploiement

### Questions d'auto-évaluation

1. Comparez et contrastez l'IA basée sur le cloud avec les implémentations d'IA en périphérie.
2. Expliquez trois techniques clés pour optimiser les modèles pour le déploiement en périphérie.
3. Quels sont les principaux avantages de l'exécution de modèles d'IA en périphérie ?
4. Décrivez le processus de quantification d'un modèle et son impact sur les performances.
5. Expliquez comment différents accélérateurs matériels (NPUs, GPUs, CPUs) influencent le déploiement EdgeAI.

### Exercices pratiques

1. **Configuration rapide de l'environnement** : Configurez un environnement de développement minimal avec les packages essentiels (30 minutes)
2. **Exploration de modèles** : Téléchargez et examinez un modèle de langage pré-entraîné (1 heure)
3. **Quantification de base** : Essayez une quantification simple sur un petit modèle (1 heure)

## Module 2 : Fondations des Small Language Models

### Objectifs d'apprentissage clés

- Comprendre les principes architecturaux des différentes familles de SLM
- Comparer les capacités des modèles selon différentes échelles de paramètres
- Évaluer les modèles en fonction de leur efficacité, de leurs capacités et des exigences de déploiement
- Identifier les cas d'utilisation appropriés pour différentes familles de modèles

### Domaines d'étude prioritaires

#### Section 1 : Famille de modèles Microsoft Phi
- **Concepts prioritaires** : 
  - Évolution de la philosophie de conception
  - Architecture axée sur l'efficacité
  - Capacités spécialisées

#### Section 2 : Famille Qwen
- **Concepts prioritaires** : 
  - Contributions open source
  - Options de déploiement évolutives
  - Architecture avancée de raisonnement

#### Section 3 : Famille Gemma
- **Concepts prioritaires** : 
  - Innovation axée sur la recherche
  - Capacités multimodales
  - Optimisation pour les mobiles

#### Section 4 : Famille BitNET
- **Concepts prioritaires** : 
  - Technologie de quantification à 1 bit
  - Cadre d'optimisation d'inférence
  - Considérations de durabilité

#### Section 5 : Modèle Microsoft Mu
- **Concepts prioritaires** : 
  - Architecture axée sur les appareils
  - Intégration système avec Windows
  - Fonctionnement respectueux de la vie privée

#### Section 6 : Phi-Silica
- **Concepts prioritaires** : 
  - Architecture optimisée pour NPU
  - Indicateurs de performance
  - Intégration pour les développeurs

### Questions d'auto-évaluation

1. Comparez les approches architecturales des familles de modèles Phi et Qwen.
2. Expliquez comment la technologie de quantification de BitNET diffère de la quantification traditionnelle.
3. Quels sont les avantages uniques du modèle Mu pour l'intégration avec Windows ?  
4. Décrivez comment Phi-Silica exploite le matériel NPU pour optimiser les performances.  
5. Pour une application mobile avec une connectivité limitée, quelle famille de modèles serait la plus appropriée et pourquoi ?  

### Exercices pratiques  

1. **Comparaison de modèles** : Benchmark rapide de deux modèles SLM différents (1 heure)  
2. **Génération de texte simple** : Implémentation basique de génération de texte avec un petit modèle (1 heure)  
3. **Optimisation rapide** : Appliquer une technique d'optimisation pour améliorer la vitesse d'inférence (1 heure)  

## Module 3 : Déploiement de petits modèles de langage (SLM)  

### Objectifs d'apprentissage clés  

- Sélectionner les modèles appropriés en fonction des contraintes de déploiement  
- Maîtriser les techniques d'optimisation pour divers scénarios de déploiement  
- Implémenter des SLM dans des environnements locaux et cloud  
- Concevoir des configurations prêtes pour la production dans des applications EdgeAI  

### Domaines d'étude prioritaires  

#### Section 1 : Apprentissage avancé des SLM  
- **Concepts prioritaires** :  
  - Cadre de classification des paramètres  
  - Techniques d'optimisation avancées  
  - Stratégies d'acquisition de modèles  

#### Section 2 : Déploiement en environnement local  
- **Concepts prioritaires** :  
  - Déploiement sur la plateforme Ollama  
  - Solutions locales Microsoft Foundry  
  - Analyse comparative des frameworks  

#### Section 3 : Déploiement cloud conteneurisé  
- **Concepts prioritaires** :  
  - Inférence haute performance avec vLLM  
  - Orchestration de conteneurs  
  - Implémentation d'ONNX Runtime  

### Questions d'auto-évaluation  

1. Quels facteurs doivent être pris en compte lors du choix entre un déploiement local et un déploiement cloud ?  
2. Comparez Ollama et Microsoft Foundry Local en tant qu'options de déploiement.  
3. Expliquez les avantages de la conteneurisation pour le déploiement des SLM.  
4. Quels sont les principaux indicateurs de performance à surveiller pour un SLM déployé en périphérie ?  
5. Décrivez un workflow complet de déploiement, de la sélection du modèle à la mise en production.  

### Exercices pratiques  

1. **Déploiement local basique** : Déployer un SLM simple en utilisant Ollama (1 heure)  
2. **Vérification des performances** : Effectuer un benchmark rapide sur votre modèle déployé (30 minutes)  
3. **Intégration simple** : Créer une application minimale utilisant votre modèle déployé (1 heure)  

## Module 4 : Conversion de format et quantification des modèles  

### Objectifs d'apprentissage clés  

- Maîtriser les techniques avancées de quantification, de 1 bit à 8 bits de précision  
- Comprendre les stratégies de conversion de format (GGUF, ONNX)  
- Implémenter des optimisations sur six frameworks (Llama.cpp, Olive, OpenVINO, MLX, synthèse de workflow)  
- Déployer des modèles optimisés pour des environnements de production en périphérie sur du matériel Intel, Apple et multiplateforme  

### Domaines d'étude prioritaires  

#### Section 1 : Fondements de la quantification  
- **Concepts prioritaires** :  
  - Cadre de classification des précisions  
  - Compromis entre performance et précision  
  - Optimisation de l'empreinte mémoire  

#### Section 2 : Implémentation avec Llama.cpp  
- **Concepts prioritaires** :  
  - Déploiement multiplateforme  
  - Optimisation du format GGUF  
  - Techniques d'accélération matérielle  

#### Section 3 : Suite Microsoft Olive  
- **Concepts prioritaires** :  
  - Optimisation adaptée au matériel  
  - Déploiement de niveau entreprise  
  - Workflows d'optimisation automatisés  

#### Section 4 : Toolkit OpenVINO  
- **Concepts prioritaires** :  
  - Optimisation pour matériel Intel  
  - Framework de compression de réseaux neuronaux (NNCF)  
  - Déploiement d'inférence multiplateforme  
  - OpenVINO GenAI pour le déploiement de LLM  

#### Section 5 : Framework Apple MLX  
- **Concepts prioritaires** :  
  - Optimisation pour Apple Silicon  
  - Architecture mémoire unifiée  
  - Capacités de fine-tuning LoRA  

#### Section 6 : Synthèse de workflow pour le développement Edge AI  
- **Concepts prioritaires** :  
  - Architecture de workflow unifiée  
  - Arbres de décision pour la sélection des frameworks  
  - Validation de la préparation à la production  
  - Stratégies pour pérenniser les solutions  

### Questions d'auto-évaluation  

1. Comparez les stratégies de quantification pour différents niveaux de précision (1 bit à 8 bits).  
2. Expliquez les avantages du format GGUF pour le déploiement en périphérie.  
3. Comment l'optimisation adaptée au matériel dans Microsoft Olive améliore-t-elle l'efficacité du déploiement ?  
4. Quels sont les principaux avantages du NNCF d'OpenVINO pour la compression des modèles ?  
5. Décrivez comment Apple MLX exploite l'architecture mémoire unifiée pour l'optimisation.  
6. Comment la synthèse de workflow aide-t-elle à sélectionner les frameworks d'optimisation optimaux ?  

### Exercices pratiques  

1. **Quantification de modèle** : Appliquer différents niveaux de quantification à un modèle et comparer les résultats (1 heure)  
2. **Optimisation avec OpenVINO** : Utiliser NNCF pour compresser un modèle pour du matériel Intel (1 heure)  
3. **Comparaison de frameworks** : Tester le même modèle sur trois frameworks d'optimisation différents (1 heure)  
4. **Benchmarking des performances** : Mesurer l'impact de l'optimisation sur la vitesse d'inférence et l'utilisation mémoire (1 heure)  

## Module 5 : SLMOps - Opérations pour petits modèles de langage  

### Objectifs d'apprentissage clés  

- Comprendre les principes de gestion du cycle de vie des SLMOps  
- Maîtriser les techniques de distillation et de fine-tuning pour le déploiement en périphérie  
- Implémenter des stratégies de déploiement en production avec surveillance  
- Construire des workflows d'opérations et de maintenance de SLM de niveau entreprise  

### Domaines d'étude prioritaires  

#### Section 1 : Introduction aux SLMOps  
- **Concepts prioritaires** :  
  - Changement de paradigme des SLMOps dans les opérations d'IA  
  - Architecture axée sur la confidentialité et l'efficacité des coûts  
  - Impact stratégique sur les entreprises et avantages concurrentiels  

#### Section 2 : Distillation de modèles  
- **Concepts prioritaires** :  
  - Techniques de transfert de connaissances  
  - Implémentation du processus de distillation en deux étapes  
  - Workflows de distillation avec Azure ML  

#### Section 3 : Stratégies de fine-tuning  
- **Concepts prioritaires** :  
  - Fine-tuning efficace en termes de paramètres (PEFT)  
  - Méthodes avancées LoRA et QLoRA  
  - Entraînement multi-adaptateurs et optimisation des hyperparamètres  

#### Section 4 : Déploiement en production  
- **Concepts prioritaires** :  
  - Conversion et quantification des modèles pour la production  
  - Configuration de déploiement Foundry Local  
  - Benchmarking des performances et validation de la qualité  

### Questions d'auto-évaluation  

1. En quoi les SLMOps diffèrent-ils des MLOps traditionnels ?  
2. Expliquez les avantages de la distillation de modèles pour le déploiement en périphérie.  
3. Quels sont les principaux éléments à prendre en compte pour le fine-tuning des SLM dans des environnements à ressources limitées ?  
4. Décrivez un pipeline complet de déploiement en production pour des applications Edge AI.  

### Exercices pratiques  

1. **Distillation basique** : Créer un modèle plus petit à partir d'un modèle enseignant plus grand (1 heure)  
2. **Expérience de fine-tuning** : Affiner un modèle pour un domaine spécifique (1 heure)  
3. **Pipeline de déploiement** : Configurer un pipeline CI/CD basique pour le déploiement de modèles (1 heure)  

## Module 6 : Systèmes agentiques SLM - Agents IA et appels de fonctions  

### Objectifs d'apprentissage clés  

- Construire des agents IA intelligents pour des environnements Edge en utilisant des SLM  
- Implémenter des capacités d'appel de fonctions avec des workflows systématiques  
- Maîtriser l'intégration du protocole de contexte de modèle (MCP) pour une interaction standardisée avec les outils  
- Créer des systèmes agentiques sophistiqués nécessitant une intervention humaine minimale  

### Domaines d'étude prioritaires  

#### Section 1 : Agents IA et fondements des SLM  
- **Concepts prioritaires** :  
  - Cadre de classification des agents (réflexes, basés sur des modèles, basés sur des objectifs, agents apprenants)  
  - Analyse des compromis entre SLM et LLM  
  - Modèles de conception d'agents spécifiques à la périphérie  
  - Optimisation des ressources pour les agents  

#### Section 2 : Appels de fonctions dans les SLM  
- **Concepts prioritaires** :  
  - Implémentation de workflows systématiques (détection d'intention, sortie JSON, exécution externe)  
  - Implémentations spécifiques aux plateformes (Phi-4-mini, modèles Qwen sélectionnés, Microsoft Foundry Local)  
  - Exemples avancés (collaboration multi-agents, sélection dynamique d'outils)  
  - Considérations pour la production (limitation de débit, journalisation des audits, mesures de sécurité)  

#### Section 3 : Intégration du protocole de contexte de modèle (MCP)  
- **Concepts prioritaires** :  
  - Architecture du protocole et conception de système en couches  
  - Support multi-backend (Ollama pour le développement, vLLM pour la production)  
  - Protocoles de connexion (modes STDIO et SSE)  
  - Applications réelles (automatisation web, traitement de données, intégration API)  

### Questions d'auto-évaluation  

1. Quels sont les principaux éléments architecturaux à considérer pour les agents IA en périphérie ?  
2. Comment les appels de fonctions améliorent-ils les capacités des agents ?  
3. Expliquez le rôle du protocole de contexte de modèle dans la communication des agents.  

### Exercices pratiques  

1. **Agent simple** : Construire un agent IA basique avec des appels de fonctions (1 heure)  
2. **Intégration MCP** : Implémenter MCP dans une application d'agent (30 minutes)  

## Module 7 : Exemples d'implémentation EdgeAI  

### Objectifs d'apprentissage clés  

- Maîtriser l'AI Toolkit pour Visual Studio Code pour des workflows complets de développement EdgeAI  
- Acquérir une expertise sur la plateforme Windows AI Foundry et les stratégies d'optimisation NPU  
- Implémenter EdgeAI sur plusieurs plateformes matérielles et scénarios de déploiement  
- Construire des applications EdgeAI prêtes pour la production avec des optimisations spécifiques à la plateforme  

### Domaines d'étude prioritaires  

#### Section 1 : AI Toolkit pour Visual Studio Code  
- **Concepts prioritaires** :  
  - Environnement de développement Edge AI complet dans VS Code  
  - Catalogue de modèles et découverte pour le déploiement en périphérie  
  - Workflows de test local, d'optimisation et de développement d'agents  
  - Surveillance des performances et évaluation pour les scénarios en périphérie  

#### Section 2 : Guide de développement Windows EdgeAI  
- **Concepts prioritaires** :  
  - Aperçu complet de la plateforme Windows AI Foundry  
  - API Phi Silica pour une inférence NPU efficace  
  - API de vision par ordinateur pour le traitement d'images et l'OCR  
  - CLI Foundry Local pour le développement et les tests locaux  

#### Section 3 : Implémentations spécifiques à la plateforme  
- **Concepts prioritaires** :  
  - Déploiement NVIDIA Jetson Orin Nano (67 TOPS de performance IA)  
  - Applications mobiles avec .NET MAUI et ONNX Runtime GenAI  
  - Solutions Azure EdgeAI avec architecture hybride cloud-périphérie  
  - Optimisation Windows ML avec support matériel universel  
  - Applications Foundry Local avec implémentation RAG axée sur la confidentialité  

### Questions d'auto-évaluation  

1. Comment l'AI Toolkit simplifie-t-il le workflow de développement EdgeAI ?  
2. Comparez les stratégies de déploiement sur différentes plateformes matérielles.  
3. Quels sont les avantages de Windows AI Foundry pour le développement en périphérie ?  
4. Expliquez le rôle de l'optimisation NPU dans les applications modernes d'Edge AI.  
5. Comment l'API Phi Silica exploite-t-elle le matériel NPU pour optimiser les performances ?  
6. Comparez les avantages du déploiement local par rapport au déploiement cloud pour des applications sensibles à la confidentialité.  

### Exercices pratiques  

1. **Configuration de l'AI Toolkit** : Configurer l'AI Toolkit et optimiser un modèle (1 heure)  
2. **Windows AI Foundry** : Construire une application IA Windows simple en utilisant l'API Phi Silica (1 heure)  
3. **Déploiement multiplateforme** : Déployer le même modèle sur deux plateformes différentes (1 heure)  
4. **Optimisation NPU** : Tester les performances NPU avec les outils Windows AI Foundry (30 minutes)  

## Module 8 : Microsoft Foundry Local – Kit complet pour développeurs (modernisé)  

### Objectifs d'apprentissage clés  

- Installer et configurer Foundry Local avec une intégration SDK moderne  
- Implémenter des systèmes multi-agents avancés avec des modèles de coordination  
- Construire des routeurs de modèles intelligents avec sélection automatique basée sur les tâches  
- Déployer des solutions IA prêtes pour la production avec une surveillance complète  
- Intégrer avec Azure AI Foundry pour des scénarios de déploiement hybrides  
- Maîtriser les modèles SDK modernes avec FoundryLocalManager et le client OpenAI  

### Domaines d'étude prioritaires  

#### Section 1 : Installation et configuration modernes  
- **Concepts prioritaires** :  
  - Intégration SDK FoundryLocalManager  
  - Découverte automatique des services et surveillance de la santé  
  - Modèles de configuration basés sur l'environnement  
  - Considérations pour le déploiement en production  

#### Section 2 : Systèmes multi-agents avancés  
- **Concepts prioritaires** :  
  - Modèle de coordination avec agents spécialisés  
  - Spécialisation des agents pour la récupération, le raisonnement et l'exécution  
  - Mécanismes de boucle de rétroaction pour le raffinement  
  - Suivi des performances et statistiques  

#### Section 3 : Routage intelligent des modèles  
- **Concepts prioritaires** :  
  - Algorithmes de sélection de modèles basés sur des mots-clés  
  - Support de plusieurs modèles (général, raisonnement, code, créatif)  
  - Configuration flexible via des variables d'environnement  
  - Vérification de la santé des services et gestion des erreurs  

#### Section 4 : Implémentation prête pour la production  
- **Concepts prioritaires** :  
  - Gestion complète des erreurs et mécanismes de secours  
  - Surveillance des requêtes et suivi des performances  
  - Exemples interactifs dans des notebooks Jupyter avec benchmarks  
  - Modèles d'intégration avec des applications existantes  

### Questions d'auto-évaluation  

1. En quoi l'approche moderne FoundryLocalManager diffère-t-elle des appels REST manuels ?  
2. Expliquez le modèle de coordination et comment il orchestre les agents spécialisés.  
3. Comment le routeur intelligent sélectionne-t-il les modèles appropriés en fonction du contenu des requêtes ?  
4. Quels sont les composants clés d'un système d'agent IA prêt pour la production ?  
5. Comment implémentez-vous une surveillance complète de la santé des services Foundry Local ?  
6. Comparez les avantages de l'approche modernisée par rapport aux modèles d'implémentation traditionnels.  

### Exercices pratiques  

1. **Configuration SDK moderne** : Configurer FoundryLocalManager avec découverte automatique des services (30 minutes)  
2. **Système multi-agents** : Exécuter le coordinateur avancé avec des agents spécialisés (30 minutes)  
3. **Routage intelligent** : Tester le routeur de modèles avec différents types de requêtes (30 minutes)  
4. **Exploration interactive** : Utiliser les notebooks Jupyter pour explorer les fonctionnalités avancées (45 minutes)  
5. **Déploiement en production** : Implémenter des modèles de surveillance et de gestion des erreurs (30 minutes)  
6. **Intégration hybride** : Configurer des scénarios de secours avec Azure AI Foundry (30 minutes)  

## Guide de répartition du temps  

Pour vous aider à tirer le meilleur parti des 20 heures de cours, voici une suggestion de répartition du temps :  

| Activité | Répartition du temps | Description |  
|----------|----------------------|-------------|  
| Lecture des matériaux principaux | 9 heures | Se concentrer sur les concepts essentiels de chaque module |  
| Exercices pratiques | 6 heures | Mise en œuvre pratique des techniques clés |
| Auto-évaluation | 2 heures | Tester votre compréhension à travers des questions et réflexions |
| Mini-projet | 3 heures | Appliquer vos connaissances à une petite mise en œuvre pratique |

### Points clés selon la contrainte de temps

**Si vous n'avez que 10 heures :**
- Complétez le Module 0 (Introduction) et les Modules 1, 2 et 3 (concepts fondamentaux d'EdgeAI)
- Réalisez au moins un exercice pratique par module
- Concentrez-vous sur la compréhension des concepts clés plutôt que sur les détails de mise en œuvre

**Si vous pouvez consacrer les 20 heures complètes :**
- Complétez les huit modules (y compris l'Introduction)
- Effectuez les exercices pratiques essentiels de chaque module
- Réalisez un mini-projet du Module 7
- Explorez au moins 2-3 ressources complémentaires

**Si vous avez plus de 20 heures :**
- Complétez tous les modules (y compris l'Introduction) avec des exercices détaillés
- Réalisez plusieurs mini-projets
- Explorez des techniques avancées d'optimisation dans le Module 4
- Implémentez le déploiement en production à partir du Module 5

## Ressources essentielles

Ces ressources soigneusement sélectionnées offrent le meilleur rapport qualité-temps pour vos études :

### Documentation incontournable
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - L'outil d'optimisation de modèles le plus efficace
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - La méthode la plus rapide pour déployer des SLMs localement
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Référence pour un modèle optimisé pour les périphériques
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - La boîte à outils d'optimisation complète d'Intel
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Environnement de développement intégré pour EdgeAI
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Plateforme de développement EdgeAI spécifique à Windows

### Outils pour gagner du temps
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Accès rapide aux modèles et déploiement
- [Gradio](https://www.gradio.app/docs/interface) - Développement rapide d'interface utilisateur pour les démonstrations IA
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplification de l'optimisation des modèles
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inférence efficace sur CPU
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Cadre de compression des réseaux neuronaux
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Boîte à outils de déploiement de modèles de langage étendus

## Modèle de suivi des progrès

Utilisez ce modèle simplifié pour suivre vos progrès dans le cours de 20 heures :

| Module | Date de réalisation | Temps passé | Points clés retenus |
|--------|---------------------|-------------|---------------------|
| Module 0 : Introduction à EdgeAI | | | |
| Module 1 : Fondamentaux d'EdgeAI | | | |
| Module 2 : Fondations des SLM | | | |
| Module 3 : Déploiement des SLM | | | |
| Module 4 : Optimisation des modèles | | | |
| Module 5 : SLMOps | | | |
| Module 6 : Agents IA | | | |
| Module 7 : Outils de développement | | | |
| Module 8 : Boîte à outils locale Foundry | | | |
| Exercices pratiques | | | |
| Mini-projet | | | |

## Idées de mini-projets

Envisagez de réaliser l'un de ces projets pour pratiquer les concepts d'EdgeAI (chacun conçu pour durer 2 à 4 heures) :

### Projets débutants (2-3 heures chacun)
1. **Assistant texte Edge** : Créez un outil simple de complétion de texte hors ligne en utilisant un petit modèle de langage
2. **Tableau de comparaison de modèles** : Construisez une visualisation basique des métriques de performance entre différents SLMs
3. **Expérience d'optimisation** : Mesurez l'impact de différents niveaux de quantification sur le même modèle de base

### Projets intermédiaires (3-4 heures chacun)
4. **Workflow avec AI Toolkit** : Utilisez l'AI Toolkit de VS Code pour optimiser et déployer un modèle de bout en bout
5. **Application Windows AI Foundry** : Créez une application Windows en utilisant l'API Phi Silica et l'optimisation NPU
6. **Déploiement multiplateforme** : Déployez le même modèle optimisé sur Windows (OpenVINO) et mobile (.NET MAUI)
7. **Agent d'appel de fonctions** : Construisez un agent IA avec des capacités d'appel de fonctions pour des scénarios Edge

### Projets d'intégration avancés (4-5 heures chacun)
8. **Pipeline d'optimisation OpenVINO** : Implémentez une optimisation complète de modèle en utilisant NNCF et la boîte à outils GenAI
9. **Pipeline SLMOps** : Implémentez un cycle de vie complet de modèle, de l'entraînement au déploiement Edge
10. **Système Edge multi-modèles** : Déployez plusieurs modèles spécialisés travaillant ensemble sur du matériel Edge
11. **Système d'intégration MCP** : Construisez un système agentique utilisant le protocole de contexte de modèle pour l'interaction avec des outils

## Références

- Microsoft Learn (Foundry Local)
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Démarrage : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - Référence CLI : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Intégration avec les SDK d'inférence : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Application WebUI ouverte : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compilation de modèles Hugging Face : https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Vue d'ensemble : https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agents (vue d'ensemble) : https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Outils d'optimisation et d'inférence
  - Microsoft Olive (docs) : https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub) : https://github.com/microsoft/Olive
  - ONNX Runtime (démarrage) : https://onnxruntime.ai/docs/get-started/with-python.html
  - Intégration Olive avec ONNX Runtime : https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (docs) : https://docs.openvino.ai/2025/index.html
  - Apple MLX (docs) : https://ml-explore.github.io/mlx/build/html/index.html
- Cadres de déploiement et modèles
  - Llama.cpp : https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers : https://huggingface.co/docs/transformers/index
  - vLLM (docs) : https://docs.vllm.ai/
  - Ollama (démarrage rapide) : https://github.com/ollama/ollama#get-started
- Outils de développement (Windows et VS Code)
  - AI Toolkit pour VS Code : https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (vue d'ensemble) : https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Communauté d'apprentissage

Rejoignez la discussion et connectez-vous avec d'autres apprenants :
- Discussions GitHub sur le [repository EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI représente l'avant-garde de la mise en œuvre de l'intelligence artificielle, apportant des capacités puissantes directement aux appareils tout en répondant aux préoccupations critiques liées à la confidentialité, la latence et la connectivité. Ce cours de 20 heures vous fournit les connaissances essentielles et les compétences pratiques pour commencer à travailler avec les technologies EdgeAI immédiatement.

Le cours est délibérément concis et axé sur les concepts les plus importants, vous permettant d'acquérir rapidement une expertise précieuse sans un engagement de temps écrasant. N'oubliez pas que la pratique, même avec des exemples simples, est la clé pour renforcer ce que vous avez appris.

Bon apprentissage !

---

