<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T19:35:19+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "fr"
}
-->
# Guide de d√©marrage rapide - Carnets de l'atelier

## Table des mati√®res

- [Pr√©requis](../../../../Workshop/notebooks)
- [Configuration initiale](../../../../Workshop/notebooks)
- [Session 04 : Comparaison de mod√®les](../../../../Workshop/notebooks)
- [Session 05 : Orchestrateur multi-agents](../../../../Workshop/notebooks)
- [Session 06 : Routage bas√© sur l'intention](../../../../Workshop/notebooks)
- [Variables d'environnement](../../../../Workshop/notebooks)
- [Commandes courantes](../../../../Workshop/notebooks)

---

## Pr√©requis

### 1. Installer Foundry Local

**Windows :**
```bash
winget install Microsoft.FoundryLocal
```

**macOS :**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**V√©rifier l'installation :**
```bash
foundry --version
```

### 2. Installer les d√©pendances Python

```bash
cd Workshop
pip install -r requirements.txt
```

Ou installer individuellement :
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Configuration initiale

### D√©marrer le service Foundry Local

**Requis avant d'ex√©cuter tout carnet :**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

R√©sultat attendu :
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### T√©l√©charger et charger les mod√®les

Les carnets utilisent ces mod√®les par d√©faut :

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### V√©rifier la configuration

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Session 04 : Comparaison de mod√®les

### Objectif
Comparer les performances entre les mod√®les de langage de petite taille (SLM) et les mod√®les de langage de grande taille (LLM).

### Configuration rapide

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Ex√©cution du carnet

1. **Ouvrir** `session04_model_compare.ipynb` dans VS Code ou Jupyter
2. **Red√©marrer le noyau** (Kernel ‚Üí Restart Kernel)
3. **Ex√©cuter toutes les cellules** dans l'ordre

### Configuration cl√©

**Mod√®les par d√©faut :**
- **SLM :** `phi-4-mini` (~4 Go de RAM, plus rapide)
- **LLM :** `qwen2.5-3b` (~3 Go de RAM, optimis√© pour la m√©moire)

**Variables d'environnement (optionnel) :**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### R√©sultat attendu

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### Personnalisation

**Utiliser des mod√®les diff√©rents :**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Prompt personnalis√© :**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Liste de validation

- [ ] La cellule 12 affiche les mod√®les corrects (phi-4-mini, qwen2.5-3b)
- [ ] La cellule 12 affiche le bon point de terminaison (port 59959)
- [ ] Le diagnostic de la cellule 16 est r√©ussi (‚úÖ Le service est en cours d'ex√©cution)
- [ ] La v√©rification pr√©alable de la cellule 20 est r√©ussie (les deux mod√®les sont OK)
- [ ] La comparaison de la cellule 22 est termin√©e avec des valeurs de latence
- [ ] La validation de la cellule 24 affiche üéâ TOUS LES TESTS R√âUSSIS !

### Estimation du temps
- **Premi√®re ex√©cution :** 5-10 minutes (y compris le t√©l√©chargement des mod√®les)
- **Ex√©cutions suivantes :** 1-2 minutes

---

## Session 05 : Orchestrateur multi-agents

### Objectif
D√©montrer la collaboration multi-agents en utilisant le SDK Foundry Local - les agents travaillent ensemble pour produire des r√©sultats affin√©s.

### Configuration rapide

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Ex√©cution du carnet

1. **Ouvrir** `session05_agents_orchestrator.ipynb`
2. **Red√©marrer le noyau**
3. **Ex√©cuter toutes les cellules** dans l'ordre

### Configuration cl√©

**Configuration par d√©faut (m√™me mod√®le pour les deux agents) :**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Configuration avanc√©e (mod√®les diff√©rents) :**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Architecture

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### R√©sultat attendu

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Extensions

**Ajouter plus d'agents :**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Tests en lot :**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Estimation du temps
- **Premi√®re ex√©cution :** 3-5 minutes
- **Ex√©cutions suivantes :** 1-2 minutes par question

---

## Session 06 : Routage bas√© sur l'intention

### Objectif
Acheminer intelligemment les prompts vers des mod√®les sp√©cialis√©s en fonction de l'intention d√©tect√©e.

### Configuration rapide

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Remarque :** La session 06 utilise par d√©faut des mod√®les CPU pour une compatibilit√© maximale.

### Ex√©cution du carnet

1. **Ouvrir** `session06_models_router.ipynb`
2. **Red√©marrer le noyau**
3. **Ex√©cuter toutes les cellules** dans l'ordre

### Configuration cl√©

**Catalogue par d√©faut (mod√®les CPU) :**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternative (mod√®les GPU) :**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### D√©tection d'intention

Le routeur utilise des motifs regex pour d√©tecter l'intention :

| Intention         | Exemples de motifs         | Achemin√© vers          |
|-------------------|---------------------------|------------------------|
| `code`           | "refactor", "implement function" | phi-3.5-mini-cpu      |
| `classification` | "categorize", "classify this"    | qwen2.5-0.5b-cpu      |
| `summarize`      | "summarize", "tl;dr"            | phi-4-mini-cpu        |
| `general`        | Tout le reste                  | phi-4-mini-cpu        |

### R√©sultat attendu

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### Personnalisation

**Ajouter une intention personnalis√©e :**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Activer le suivi des tokens :**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### Passer aux mod√®les GPU

Si vous avez 8 Go+ de VRAM :

1. Dans **Cellule #6**, commenter le catalogue CPU
2. D√©commenter le catalogue GPU
3. Charger les mod√®les GPU :
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Red√©marrer le noyau et r√©ex√©cuter le carnet

### Estimation du temps
- **Premi√®re ex√©cution :** 5-10 minutes (chargement des mod√®les)
- **Ex√©cutions suivantes :** 30-60 secondes par test

---

## Variables d'environnement

### Configuration globale

√Ä d√©finir avant de d√©marrer Jupyter/VS Code :

**Windows (Invite de commandes) :**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell) :**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux :**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Configuration dans le carnet

√Ä d√©finir au d√©but de tout carnet :

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Commandes courantes

### Gestion du service

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Gestion des mod√®les

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Test des points de terminaison

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Commandes de diagnostic

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Bonnes pratiques

### Avant de d√©marrer tout carnet

1. **V√©rifier que le service est en cours d'ex√©cution :**
   ```bash
   foundry service status
   ```

2. **V√©rifier que les mod√®les sont charg√©s :**
   ```bash
   foundry model ls
   ```

3. **Red√©marrer le noyau du carnet** si vous r√©ex√©cutez

4. **Effacer tous les r√©sultats** pour une ex√©cution propre

### Gestion des ressources

1. **Utiliser les mod√®les CPU par d√©faut** pour la compatibilit√©
2. **Passer aux mod√®les GPU** uniquement si vous avez 8 Go+ de VRAM
3. **Fermer les autres applications GPU** avant de lancer
4. **Garder le service actif** entre les sessions de carnets
5. **Surveiller l'utilisation des ressources** avec le Gestionnaire des t√¢ches / nvidia-smi

### D√©pannage

1. **Toujours v√©rifier le service en premier** avant de d√©boguer le code
2. **Red√©marrer le noyau** si vous voyez une configuration obsol√®te
3. **R√©ex√©cuter les cellules de diagnostic** apr√®s tout changement
4. **V√©rifier que les noms des mod√®les** correspondent √† ceux charg√©s
5. **V√©rifier que le port du point de terminaison** correspond au statut du service

---

## R√©f√©rence rapide : Alias des mod√®les

### Mod√®les courants

| Alias            | Taille | Meilleur usage              | RAM/VRAM | Variantes              |
|------------------|--------|----------------------------|----------|------------------------|
| `phi-4-mini`    | ~4B    | Chat g√©n√©ral, r√©sum√©       | 4-6 Go   | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini`  | ~3.5B  | G√©n√©ration de code         | 3-5 Go   | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b`    | ~3B    | T√¢ches g√©n√©rales, efficace | 3-4 Go   | `-cpu`, `-cuda-gpu`         |
| `qwen2.5-1.5b`  | ~1.5B  | Rapide, faible ressource   | 2-3 Go   | `-cpu`, `-cuda-gpu`         |
| `qwen2.5-0.5b`  | ~0.5B  | Classification, minimal    | 1-2 Go   | `-cpu`, `-cuda-gpu`         |

### Nommage des variantes

- **Nom de base** (ex. `phi-4-mini`) : S√©lectionne automatiquement la meilleure variante pour votre mat√©riel
- **`-cpu`** : Optimis√© pour CPU, fonctionne partout
- **`-cuda-gpu`** : Optimis√© pour GPU NVIDIA, n√©cessite 8 Go+ de VRAM
- **`-npu`** : Optimis√© pour NPU Qualcomm, n√©cessite des pilotes NPU

**Recommandation :** Utilisez les noms de base (sans suffixe) et laissez Foundry Local s√©lectionner automatiquement la meilleure variante.

---

## Indicateurs de succ√®s

Vous √™tes pr√™t lorsque vous voyez :

‚úÖ `foundry service status` indique "running"  
‚úÖ `foundry model ls` affiche les mod√®les requis  
‚úÖ Service accessible au bon point de terminaison  
‚úÖ V√©rification de sant√© retourne 200 OK  
‚úÖ Les cellules de diagnostic du carnet sont r√©ussies  
‚úÖ Aucun message d'erreur de connexion dans les r√©sultats  

---

## Obtenir de l'aide

### Documentation
- **R√©pertoire principal** : https://github.com/microsoft/Foundry-Local
- **SDK Python** : https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **R√©f√©rence CLI** : https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **D√©pannage** : Voir `troubleshooting.md` dans ce r√©pertoire

### Probl√®mes GitHub
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Derni√®re mise √† jour :** 8 octobre 2025  
**Version :** Carnets de l'atelier 2.0

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.