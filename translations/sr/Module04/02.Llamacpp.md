<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-19T00:50:11+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "sr"
}
-->
# Секција 2: Водич за имплементацију Llama.cpp

## Садржај
1. [Увод](../../../Module04)
2. [Шта је Llama.cpp?](../../../Module04)
3. [Инсталација](../../../Module04)
4. [Компилација из извора](../../../Module04)
5. [Квантизација модела](../../../Module04)
6. [Основна употреба](../../../Module04)
7. [Напредне функције](../../../Module04)
8. [Интеграција са Python-ом](../../../Module04)
9. [Решавање проблема](../../../Module04)
10. [Најбоље праксе](../../../Module04)

## Увод

Овај свеобухватни туторијал ће вас водити кроз све што треба да знате о Llama.cpp, од основне инсталације до напредних сценарија употребе. Llama.cpp је моћна имплементација на C++ која омогућава ефикасно извршавање великих језичких модела (LLM) уз минималну конфигурацију и одличне перформансе на различитим хардверским конфигурацијама.

## Шта је Llama.cpp?

Llama.cpp је оквир за извршавање LLM-а написан у C/C++ који омогућава локално покретање великих језичких модела уз минималну конфигурацију и врхунске перформансе на широком спектру хардвера. Кључне карактеристике укључују:

### Основне карактеристике
- **Чиста имплементација у C/C++** без зависности
- **Мултиплатформска компатибилност** (Windows, macOS, Linux)
- **Хардверска оптимизација** за различите архитектуре
- **Подршка за квантизацију** (од 1.5-битне до 8-битне целобројне квантизације)
- **Убрзање на CPU и GPU** 
- **Ефикасност меморије** за ограничена окружења

### Предности
- Ефикасно ради на CPU-у без потребе за специјализованим хардвером
- Подржава више GPU бекендова (CUDA, Metal, OpenCL, Vulkan)
- Лаган и преносив
- Apple Silicon је приоритет - оптимизован преко ARM NEON, Accelerate и Metal оквира
- Подржава различите нивое квантизације за смањену употребу меморије

## Инсталација

### Метод 1: Унапред компилирани бинарни фајлови (препоручено за почетнике)

#### Преузимање са GitHub Releases
1. Посетите [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Преузмите одговарајући бинарни фајл за ваш систем:
   - `llama-<верзија>-bin-win-<функција>-<арх>.zip` за Windows
   - `llama-<верзија>-bin-macos-<функција>-<арх>.zip` за macOS
   - `llama-<верзија>-bin-linux-<функција>-<арх>.zip` за Linux

3. Извуците архиву и додајте директоријум у PATH вашег система.

#### Коришћење менаџера пакета

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (различите дистрибуције):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Метод 2: Python пакет (llama-cpp-python)

#### Основна инсталација
```bash
pip install llama-cpp-python
```

#### Са хардверским убрзањем
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Компилација из извора

### Предуслови

**Системски захтеви:**
- C++ компилатор (GCC, Clang или MSVC)
- CMake (верзија 3.14 или новија)
- Git
- Алатке за компилацију за вашу платформу

**Инсталација предуслова:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Инсталирајте Visual Studio 2022 са алаткама за развој C++
- Инсталирајте CMake са званичног сајта
- Инсталирајте Git

### Основни процес компилације

1. **Клонирајте репозиторијум:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Конфигуришите компилацију:**
```bash
cmake -B build
```

3. **Компилирајте пројекат:**
```bash
cmake --build build --config Release
```

За бржу компилацију, користите паралелне задатке:
```bash
cmake --build build --config Release -j 8
```

### Компилације специфичне за хардвер

#### Подршка за CUDA (NVIDIA GPU)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Подршка за Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Подршка за OpenBLAS (оптимизација за CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Подршка за Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Напредне опције компилације

#### Debug компилација
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Са додатним функцијама
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Квантизација модела

### Разумевање GGUF формата

GGUF (Generalized GGML Unified Format) је оптимизован формат фајлова дизајниран за ефикасно покретање великих језичких модела користећи Llama.cpp и друге оквире. Омогућава:

- Стандардизовано складиштење тежина модела
- Побољшану компатибилност на различитим платформама
- Унапређене перформансе
- Ефикасно руковање метаподацима

### Типови квантизације

Llama.cpp подржава различите нивое квантизације:

| Тип | Битови | Опис | Примена |
|-----|-------|------|---------|
| F16 | 16 | Полупрецизност | Висок квалитет, велика меморија |
| Q8_0 | 8 | 8-битна квантизација | Добар баланс |
| Q4_0 | 4 | 4-битна квантизација | Умерен квалитет, мања величина |
| Q2_K | 2 | 2-битна квантизација | Најмања величина, нижи квалитет |

### Конвертовање модела

#### Из PyTorch-а у GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Директно преузимање са Hugging Face-а
Многи модели су доступни у GGUF формату на Hugging Face-у:
- Претражите моделе са "GGUF" у називу
- Преузмите одговарајући ниво квантизације
- Користите директно са llama.cpp

## Основна употреба

### Командна линија

#### Једноставно генерисање текста
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Коришћење модела са Hugging Face-а
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Режим сервера
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Уобичајени параметри

| Параметар | Опис | Пример |
|-----------|------|--------|
| `-m` | Путања до фајла модела | `-m model.gguf` |
| `-p` | Текст упита | `-p "Здраво свет"` |
| `-n` | Број токена за генерисање | `-n 100` |
| `-c` | Величина контекста | `-c 4096` |
| `-t` | Број нити | `-t 8` |
| `-ngl` | GPU слојеви | `-ngl 32` |
| `-temp` | Температура | `-temp 0.7` |

### Интерактивни режим

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Напредне функције

### API сервера

#### Покретање сервера
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Употреба API-ја
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Оптимизација перформанси

#### Управљање меморијом
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Мултитрединг
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Убрзање на GPU-у
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Интеграција са Python-ом

### Основна употреба са llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Интерфејс за ћаскање

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Стримовање одговора

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Интеграција са LangChain-ом

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Решавање проблема

### Уобичајени проблеми и решења

#### Грешке при компилацији

**Проблем: CMake није пронађен**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Проблем: Компилатор није пронађен**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Проблеми при извршавању

**Проблем: Учитавање модела не успева**
- Проверите путању до фајла модела
- Проверите дозволе фајла
- Уверите се да имате довољно RAM-а
- Пробајте различите нивое квантизације

**Проблем: Лоше перформансе**
- Омогућите хардверско убрзање
- Повећајте број нити
- Користите одговарајућу квантизацију
- Проверите употребу GPU меморије

#### Проблеми са меморијом

**Проблем: Недостатак меморије**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Проблеми специфични за платформу

#### Windows
- Користите MinGW или Visual Studio компилатор
- Уверите се да је PATH правилно конфигурисан
- Проверите да ли антивирус омета рад

#### macOS
- Омогућите Metal за Apple Silicon
- Користите Rosetta 2 за компатибилност ако је потребно
- Проверите Xcode алатке командне линије

#### Linux
- Инсталирајте развојне пакете
- Проверите верзије GPU драјвера
- Проверите инсталацију CUDA алатки

## Најбоље праксе

### Избор модела
1. **Изаберите одговарајућу квантизацију** на основу вашег хардвера
2. **Размотрите величину модела** у односу на компромис у квалитету
3. **Тестирајте различите моделе** за ваш специфичан случај употребе

### Оптимизација перформанси
1. **Користите GPU убрзање** када је доступно
2. **Оптимизујте број нити** за ваш CPU
3. **Поставите одговарајућу величину контекста** за ваш случај употребе
4. **Омогућите мапирање меморије** за велике моделе

### Продукциона имплементација
1. **Користите режим сервера** за приступ API-ју
2. **Имплементирајте правилно руковање грешкама**
3. **Пратите употребу ресурса**
4. **Поставите логовање и мониторинг**

### Радни ток развоја
1. **Почните са мањим моделима** за тестирање
2. **Користите контролу верзија** за конфигурације модела
3. **Документујте своје конфигурације**
4. **Тестирајте на различитим платформама**

### Безбедносни аспекти
1. **Проверите улазне упите**
2. **Имплементирајте ограничење брзине**
3. **Осигурајте API крајње тачке**
4. **Пратите обрасце злоупотребе**

## Закључак

Llama.cpp пружа моћан и ефикасан начин за локално покретање великих језичких модела на различитим хардверским конфигурацијама. Било да развијате AI апликације, спроводите истраживања или једноставно експериментишете са LLM-овима, овај оквир нуди флексибилност и перформансе потребне за широк спектар случајева употребе.

Кључни закључци:
- Изаберите метод инсталације који најбоље одговара вашим потребама
- Оптимизујте за вашу специфичну хардверску конфигурацију
- Почните са основном употребом и постепено истражујте напредне функције
- Размотрите коришћење Python веза ради лакше интеграције
- Пратите најбоље праксе за продукционе имплементације

За више информација и ажурирања, посетите [званични Llama.cpp репозиторијум](https://github.com/ggml-org/llama.cpp) и консултујте се са свеобухватном документацијом и ресурсима заједнице.

## ➡️ Шта је следеће

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.