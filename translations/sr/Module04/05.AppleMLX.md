<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-19T00:45:28+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "sr"
}
-->
# Секција 4: Дубинско истраживање Apple MLX оквира

## Садржај
1. [Увод у Apple MLX](../../../Module04)
2. [Кључне карактеристике за развој LLM-а](../../../Module04)
3. [Упутство за инсталацију](../../../Module04)
4. [Почетак рада са MLX-ом](../../../Module04)
5. [MLX-LM: Језички модели](../../../Module04)
6. [Рад са великим језичким моделима](../../../Module04)
7. [Интеграција са Hugging Face](../../../Module04)
8. [Конверзија и квантовање модела](../../../Module04)
9. [Фино подешавање језичких модела](../../../Module04)
10. [Напредне функције LLM-а](../../../Module04)
11. [Најбоље праксе за LLM-ове](../../../Module04)
12. [Решавање проблема](../../../Module04)
13. [Додатни ресурси](../../../Module04)

## Увод у Apple MLX

Apple MLX је оквир за машинско учење, посебно дизајниран за ефикасно и флексибилно машинско учење на Apple Silicon-у, развијен од стране Apple Machine Learning Research-а. Објављен у децембру 2023. године, MLX представља Apple-ов одговор на оквире као што су PyTorch и TensorFlow, са посебним фокусом на омогућавање моћних могућности великих језичких модела на Mac рачунарима.

### Шта MLX чини посебним за LLM-ове?

MLX је дизајниран да у потпуности искористи јединствену меморијску архитектуру Apple Silicon-а, што га чини посебно погодним за покретање и фино подешавање великих језичких модела локално на Mac рачунарима. Оквир елиминише многе проблеме са компатибилношћу које су корисници Mac-а традиционално имали приликом рада са LLM-овима.

### Ко треба да користи MLX за LLM-ове?

- **Корисници Mac-а** који желе да покрећу LLM-ове локално без зависности од облака
- **Истраживачи** који експериментишу са финим подешавањем и прилагођавањем језичких модела
- **Програмери** који граде AI апликације са могућностима језичких модела
- **Свако** ко жели да искористи Apple Silicon за генерисање текста, ћаскање и језичке задатке

## Кључне карактеристике за развој LLM-а

### 1. Јединствена меморијска архитектура
Јединствена меморија Apple Silicon-а омогућава MLX-у да ефикасно обрађује велике језичке моделе без прекомерног копирања меморије, што значи да можете радити са већим моделима на истом хардверу.

### 2. Нативна оптимизација за Apple Silicon
MLX је изграђен од темеља за Apple-ове M серије чипова, пружајући оптималне перформансе за трансформаторске архитектуре које се често користе у језичким моделима.

### 3. Подршка за квантовање
Уграђена подршка за квантовање од 4 бита и 8 бита смањује захтеве за меморијом уз задржавање квалитета модела, омогућавајући покретање већих модела на потрошачком хардверу.

### 4. Интеграција са Hugging Face-ом
Беспрекорна интеграција са Hugging Face екосистемом пружа приступ хиљадама унапред обучених језичких модела уз једноставне алате за конверзију.

### 5. LoRA фино подешавање
Подршка за Low-Rank Adaptation (LoRA) омогућава ефикасно фино подешавање великих модела уз минималне рачунарске ресурсе.

## Упутство за инсталацију

### Системски захтеви
- **macOS 13.0+** (за оптимизацију Apple Silicon-а)
- **Python 3.8+**
- **Apple Silicon** (M1, M2, M3, M4 серија)
- **Нативно ARM окружење** (не покреће се под Rosetta-ом)
- **8GB+ RAM-а** (препоручује се 16GB+ за веће моделе)

### Брза инсталација за LLM-ове

Најлакши начин за почетак рада са језичким моделима је инсталација MLX-LM:

```bash
pip install mlx-lm
```

Ова једна команда инсталира и основни MLX оквир и алатке за језичке моделе.

### Постављање виртуелног окружења (препоручено)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Додатне зависности за аудио моделе

Ако планирате да радите са говорним моделима као што је Whisper:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## Почетак рада са MLX-ом

### Ваш први језички модел

Хајде да започнемо са једноставним примером генерисања текста:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Пример Python API-ја

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Разумевање учитавања модела

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Језички модели

### Подржане архитектуре модела

MLX-LM подржава широк спектар популарних архитектура језичких модела:

- **LLaMA и LLaMA 2** - основни модели компаније Meta
- **Mistral и Mixtral** - ефикасни и моћни модели
- **Phi-3** - компактни језички модели компаније Microsoft
- **Qwen** - мултијезички модели компаније Alibaba
- **Code Llama** - специјализовани за генерисање кода
- **Gemma** - отворени језички модели компаније Google

### Командна линија

Командна линија MLX-LM-а пружа моћне алатке за рад са језичким моделима:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Python API за напредне случајеве употребе

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Рад са великим језичким моделима

### Шаблони генерисања текста

#### Генерисање у једном кораку
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Праћење инструкција
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Креативно писање
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Разговори у више корака

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Интеграција са Hugging Face-ом

### Проналажење модела компатибилних са MLX-ом

MLX ради беспрекорно са Hugging Face екосистемом:

- **Прегледајте MLX моделе**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX заједница**: https://huggingface.co/mlx-community (унапред конвертовани модели)
- **Оригинални модели**: Већина LLaMA, Mistral, Phi и Qwen модела ради уз конверзију

### Учитавање модела са Hugging Face-а

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Преузимање модела за офлајн употребу

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Конверзија и квантовање модела

### Конвертовање Hugging Face модела у MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Разумевање квантовања

Квантовање смањује величину модела и употребу меморије уз минималан губитак квалитета:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Прилагођено квантовање

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Фино подешавање језичких модела

### LoRA (Low-Rank Adaptation) фино подешавање

MLX подржава ефикасно фино подешавање користећи LoRA, што вам омогућава да прилагодите велике моделе уз минималне рачунарске ресурсе:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Припрема података за обуку

Направите JSON датотеку са вашим примерима за обуку:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Команда за фино подешавање

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Коришћење фино подешених модела

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Напредне функције LLM-а

### Кеширање упита ради ефикасности

За поновну употребу истог контекста, MLX подржава кеширање упита ради побољшања перформанси:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Стриминг генерисања текста

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Рад са моделима за генерисање кода

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Рад са моделима за ћаскање

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Најбоље праксе за LLM-ове

### Управљање меморијом

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Смернице за избор модела

**За експериментисање и учење:**
- Користите моделе квантоване на 4 бита (нпр. `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Почните са мањим моделима као што је Phi-3-mini

**За производне апликације:**
- Размотрите компромис између величине модела и квалитета
- Тестирајте и квантоване и моделе пуне прецизности
- Извршите бенчмарк на вашим специфичним случајевима употребе

**За специфичне задатке:**
- **Генерисање кода**: CodeLlama, Code Llama Instruct
- **Опште ћаскање**: Mistral-7B-Instruct, Phi-3
- **Мултијезички**: Qwen модели
- **Креативно писање**: Више температуре са Mistral или LLaMA

### Најбоље праксе за инжењеринг упита

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Оптимизација перформанси

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Решавање проблема

### Уобичајени проблеми и решења

#### Проблеми са инсталацијом

**Проблем**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Решење**: Користите нативни ARM Python или Miniconda:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Проблеми са меморијом

**Проблем**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Проблеми са учитавањем модела

**Проблем**: Модел не успева да се учита или генерише лош излаз
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Проблеми са перформансама

**Проблем**: Спора брзина генерисања
- Затворите друге апликације које троше меморију
- Користите квантоване моделе кад год је могуће
- Уверите се да не радите под Rosetta-ом
- Проверите доступну меморију пре учитавања модела

### Савети за дебаговање

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Додатни ресурси

### Званична документација и репозиторијуми

- **MLX GitHub репозиторијум**: https://github.com/ml-explore/mlx
- **MLX-LM примери**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX документација**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX интеграција**: https://huggingface.co/docs/hub/en/mlx

### Колекције модела

- **MLX модели заједнице**: https://huggingface.co/mlx-community
- **Трендинг MLX модели**: https://huggingface.co/models?library=mlx&sort=trending

### Пример апликација

1. **Лични AI асистент**: Направите локални chatbot са меморијом разговора
2. **Помоћник за кодирање**: Креирајте асистента за кодирање за ваш развојни ток
3. **Генератор садржаја**: Развијте алатке за писање, сумирање и креирање садржаја
4. **Прилагођени фино подешени модели**: Прилагодите моделе за задатке специфичне за домен
5. **Мултимодалне апликације**: Комбинујте генерисање текста са другим MLX могућностима

### Заједница и учење

- **MLX дискусије заједнице**: GitHub Issues и Discussions
- **Hugging Face форуми**: Подршка заједнице и дељење модела
- **Apple Developer документација**: Званични Apple ML ресурси

### Цитирање

Ако користите MLX у вашем истраживању, молимо вас да цитирате:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Закључак

Apple MLX је револуционисао начин покретања великих језичких модела на Mac рачунарима. Пружајући нативну оптимизацију за Apple Silicon, беспрекорну интеграцију са Hugging Face-ом и моћне функције као што су квантовање и LoRA фино подешавање, MLX омогућава покретање софистицираних језичких модела локално уз одличне перформансе.

Било да градите chatbot-ове, асистенте за кодирање, генераторе садржаја или прилагођене фино подешене моделе, MLX пружа алатке и перформансе потребне за искоришћавање пуног потенцијала вашег Apple Silicon Mac-а за апликације језичких модела. Фокус оквира на ефикасност и једноставност употребе чини га одличним избором и за истраживање и за производне апликације.

Започните са основним примерима у овом упутству, истражите богати екосистем унапред конвертованих модела на Hugging Face-у и постепено напредујте ка сложенијим функцијама као што су фино подешавање и развој прилагођених модела. Како MLX екосистем наставља да расте, постаје све моћнија платформа за развој језичких модела на Apple хардверу.

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.