<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-19T00:41:37+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sr"
}
-->
# Секција 3: Microsoft Olive Optimization Suite

## Садржај
1. [Увод](../../../Module04)
2. [Шта је Microsoft Olive?](../../../Module04)
3. [Инсталација](../../../Module04)
4. [Водич за брзи почетак](../../../Module04)
5. [Пример: Конвертовање Qwen3 у ONNX INT4](../../../Module04)
6. [Напредна употреба](../../../Module04)
7. [Најбоље праксе](../../../Module04)
8. [Решавање проблема](../../../Module04)
9. [Додатни ресурси](../../../Module04)

## Увод

Microsoft Olive је моћан и једноставан алат за оптимизацију модела који је свестан хардвера. Олакшава процес оптимизације модела машинског учења за примену на различитим хардверским платформама. Без обзира да ли циљате CPU, GPU или специјализоване AI акцелераторе, Olive вам помаже да постигнете оптималне перформансе уз очување тачности модела.

## Шта је Microsoft Olive?

Olive је једноставан алат за оптимизацију модела који је свестан хардвера и комбинује водеће технике у компресији модела, оптимизацији и компилацији. Ради са ONNX Runtime као E2E решењем за оптимизацију инференције.

### Кључне карактеристике

- **Оптимизација свесна хардвера**: Аутоматски бира најбоље технике оптимизације за ваш циљани хардвер
- **40+ уграђених компоненти за оптимизацију**: Обухвата компресију модела, квантизацију, оптимизацију графа и још много тога
- **Једноставан CLI интерфејс**: Лаки команди за уобичајене задатке оптимизације
- **Подршка за више оквира**: Ради са PyTorch, Hugging Face моделима и ONNX
- **Подршка за популарне моделе**: Olive може аутоматски оптимизовати популарне архитектуре модела као што су Llama, Phi, Qwen, Gemma итд.

### Предности

- **Скраћено време развоја**: Нема потребе за ручним експериментисањем са различитим техникама оптимизације
- **Побољшање перформанси**: Значајна побољшања брзине (до 6x у неким случајевима)
- **Примена на више платформи**: Оптимизовани модели раде на различитим хардверима и оперативним системима
- **Очувана тачност**: Оптимизације чувају квалитет модела уз побољшање перформанси

## Инсталација

### Предуслови

- Python 3.8 или новији
- pip менаџер пакета
- Виртуелно окружење (препоручено)

### Основна инсталација

Креирајте и активирајте виртуелно окружење:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Инсталирајте Olive са функцијама за аутоматску оптимизацију:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Опциони зависности

Olive нуди различите опционе зависности за додатне функције:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Провера инсталације

```bash
olive --help
```

Ако је успешно, требало би да видите поруку помоћи за Olive CLI.

## Водич за брзи почетак

### Ваша прва оптимизација

Оптимизујмо мали језички модел користећи функцију аутоматске оптимизације Olive-а:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Шта ова команда ради

Процес оптимизације укључује: преузимање модела из локалне кеш меморије, хватање ONNX графа и чување тежина у ONNX датотеци, оптимизацију ONNX графа и квантизацију модела на int4 користећи RTN метод.

### Објашњење параметара команде

- `--model_name_or_path`: Hugging Face идентификатор модела или локална путања
- `--output_path`: Директоријум где ће оптимизовани модел бити сачуван
- `--device`: Циљани уређај (cpu, gpu)
- `--provider`: Провајдер извршења (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Користи ONNX Runtime Generate AI за инференцију
- `--precision`: Прецизност квантизације (int4, int8, fp16)
- `--log_level`: Вербалност логовања (0=минимално, 1=детаљно)

## Пример: Конвертовање Qwen3 у ONNX INT4

На основу датог Hugging Face примера на [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ево како да оптимизујете Qwen3 модел:

### Корак 1: Преузимање модела (опционо)

Да бисте минимизирали време преузимања, кеширајте само неопходне датотеке:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Корак 2: Оптимизација Qwen3 модела

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Корак 3: Тестирање оптимизованог модела

Креирајте једноставан Python скрипт за тестирање вашег оптимизованог модела:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Структура излазних података

Након оптимизације, ваш излазни директоријум ће садржати:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Напредна употреба

### Конфигурационе датотеке

За сложеније радне токове оптимизације, можете користити JSON конфигурационе датотеке:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Покрените са конфигурацијом:

```bash
olive run --config config.json
```

### Оптимизација за GPU

За оптимизацију CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

За DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Фино подешавање са Olive

Olive такође подржава фино подешавање модела:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Најбоље праксе

### 1. Избор модела
- Почните са мањим моделима за тестирање (нпр. 0.5B-7B параметара)
- Уверите се да је архитектура вашег циљаног модела подржана од стране Olive-а

### 2. Хардверске карактеристике
- Ускладите циљ оптимизације са хардвером за примену
- Користите GPU оптимизацију ако имате хардвер компатибилан са CUDA
- Размотрите DirectML за Windows машине са интегрисаном графиком

### 3. Избор прецизности
- **INT4**: Максимална компресија, благо губљење тачности
- **INT8**: Добар баланс величине и тачности
- **FP16**: Минималан губитак тачности, умерено смањење величине

### 4. Тестирање и валидација
- Увек тестирајте оптимизоване моделе са вашим специфичним случајевима употребе
- Упоредите метрике перформанси (латенција, пропусност, тачност)
- Користите репрезентативне улазне податке за евалуацију

### 5. Итеративна оптимизација
- Почните са аутоматском оптимизацијом за брзе резултате
- Користите конфигурационе датотеке за прецизну контролу
- Експериментишите са различитим фазама оптимизације

## Решавање проблема

### Уобичајени проблеми

#### 1. Проблеми са инсталацијом
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Проблеми са CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Проблеми са меморијом
- Користите мање величине серија током оптимизације
- Прво пробајте квантизацију са већом прецизношћу (int8 уместо int4)
- Уверите се да имате довољно простора на диску за кеширање модела

#### 4. Грешке при учитавању модела
- Проверите путању модела и дозволе за приступ
- Проверите да ли модел захтева `trust_remote_code=True`
- Уверите се да су све потребне датотеке модела преузете

### Добијање помоћи

- **Документација**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Примери**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Додатни ресурси

### Званични линкови
- **GitHub репозиторијум**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime документација**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face пример**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Примери из заједнице
- **Jupyter бележнице**: Доступне у Olive GitHub репозиторијуму
- **VS Code екстензија**: AI Toolkit екстензија користи Olive за оптимизацију модела
- **Блог постови**: Microsoft Open Source Blog има детаљне туторијале за Olive

### Повезани алати
- **ONNX Runtime**: Енджин за инференцију високих перформанси
- **Hugging Face Transformers**: Извор многих компатибилних модела
- **Azure Machine Learning**: Радни токови оптимизације засновани на облаку

## ➡️ Шта следи

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.