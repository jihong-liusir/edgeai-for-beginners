<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:27:27+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "sr"
}
-->
# Секција 4: OpenVINO Toolkit Optimization Suite

## Садржај
1. [Увод](../../../Module04)
2. [Шта је OpenVINO?](../../../Module04)
3. [Инсталација](../../../Module04)
4. [Водич за брзи почетак](../../../Module04)
5. [Пример: Конверзија и оптимизација модела са OpenVINO](../../../Module04)
6. [Напредна употреба](../../../Module04)
7. [Најбоље праксе](../../../Module04)
8. [Решавање проблема](../../../Module04)
9. [Додатни ресурси](../../../Module04)

## Увод

OpenVINO (Open Visual Inference and Neural Network Optimization) је Intel-ов алат отвореног кода за имплементацију ефикасних AI решења у облаку, локалним окружењима и на edge уређајима. Без обзира да ли циљате CPU, GPU, VPU или специјализоване AI акцелераторе, OpenVINO пружа свеобухватне могућности оптимизације уз очување тачности модела и омогућавање крос-платформске имплементације.

## Шта је OpenVINO?

OpenVINO је алат отвореног кода који омогућава програмерима да ефикасно оптимизују, конвертују и имплементирају AI моделе на различитим хардверским платформама. Састоји се од три главне компоненте: OpenVINO Runtime за инференцију, Neural Network Compression Framework (NNCF) за оптимизацију модела и OpenVINO Model Server за скалабилну имплементацију.

### Кључне карактеристике

- **Крос-платформска имплементација**: Подржава Linux, Windows и macOS са Python, C++ и C API-јима
- **Хардверска акцелерација**: Аутоматско откривање уређаја и оптимизација за CPU, GPU, VPU и AI акцелераторе
- **Оквир за компресију модела**: Напредне технике квантовања, орезивања и оптимизације кроз NNCF
- **Компатибилност са оквирима**: Директна подршка за TensorFlow, ONNX, PaddlePaddle и PyTorch моделе
- **Подршка за генеративни AI**: Специјализовани OpenVINO GenAI за имплементацију великих језичких модела и генеративних AI апликација

### Предности

- **Оптимизација перформанси**: Значајна побољшања брзине уз минималан губитак тачности
- **Смањен простор за имплементацију**: Минималне спољашње зависности олакшавају инсталацију и имплементацију
- **Побољшано време покретања**: Оптимизовано учитавање модела и кеширање за бржу иницијализацију апликација
- **Скалабилна имплементација**: Од edge уређаја до cloud инфраструктуре са конзистентним API-јима
- **Спремно за производњу**: Поузданост на нивоу предузећа уз свеобухватну документацију и подршку заједнице

## Инсталација

### Предуслови

- Python 3.8 или новији
- pip менаџер пакета
- Виртуелно окружење (препоручено)
- Компатибилан хардвер (Intel CPU препоручен, али подржава различите архитектуре)

### Основна инсталација

Креирајте и активирајте виртуелно окружење:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Инсталирајте OpenVINO Runtime:

```bash
pip install openvino
```

Инсталирајте NNCF за оптимизацију модела:

```bash
pip install nncf
```

### Инсталација OpenVINO GenAI

За генеративне AI апликације:

```bash
pip install openvino-genai
```

### Опционалне зависности

Додатни пакети за специфичне случајеве употребе:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Провера инсталације

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Ако је успешно, требало би да видите информације о верзији OpenVINO-а.

## Водич за брзи почетак

### Ваш први модел оптимизације

Хајде да конвертујемо и оптимизујемо Hugging Face модел користећи OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Шта овај процес ради

Радни ток оптимизације укључује: учитавање оригиналног модела са Hugging Face-а, конверзију у OpenVINO Intermediate Representation (IR) формат, примену подразумеваних оптимизација и компилацију за циљни хардвер.

### Објашњење кључних параметара

- `export=True`: Конвертује модел у OpenVINO IR формат
- `compile=False`: Одлаже компилацију до времена извршавања ради флексибилности
- `device`: Циљни хардвер ("CPU", "GPU", "AUTO" за аутоматски избор)
- `save_pretrained()`: Чува оптимизовани модел за поновну употребу

## Пример: Конверзија и оптимизација модела са OpenVINO

### Корак 1: Конверзија модела са NNCF квантовањем

Ево како да примените квантовање након тренинга користећи NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Корак 2: Напредна оптимизација са компресијом тежина

За моделе засноване на трансформерима, примените компресију тежина:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Корак 3: Инференција са оптимизованим моделом

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Структура излазних података

Након оптимизације, ваш директоријум модела ће садржати:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Напредна употреба

### Конфигурација са NNCF YAML

За сложене радне токове оптимизације, користите NNCF конфигурационе датотеке:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Примените конфигурацију:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU оптимизација

За GPU акцелерацију:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Оптимизација обраде у серијама

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Имплементација Model Server-а

Имплементирајте оптимизоване моделе са OpenVINO Model Server-ом:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Клијентски код за Model Server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Најбоље праксе

### 1. Избор и припрема модела
- Користите моделе из подржаних оквира (PyTorch, TensorFlow, ONNX)
- Осигурајте да улази модела имају фиксне или познате динамичке облике
- Тестирајте са репрезентативним скуповима података за калибрацију

### 2. Избор стратегије оптимизације
- **Квантовање након тренинга**: Почните овде за брзу оптимизацију
- **Компресија тежина**: Идеално за велике језичке моделе и трансформере
- **Тренинг свестан квантовања**: Користите када је тачност критична

### 3. Хардверска специфична оптимизација
- **CPU**: Користите INT8 квантовање за уравнотежене перформансе
- **GPU**: Искористите FP16 прецизност и обраду у серијама
- **VPU**: Фокусирајте се на поједностављење модела и фузију слојева

### 4. Подешавање перформанси
- **Режим пропусности**: За обраду великих серија
- **Режим кашњења**: За интерактивне апликације у реалном времену
- **AUTO уређај**: Дозволите OpenVINO-у да изабере оптималан хардвер

### 5. Управљање меморијом
- Користите динамичке облике пажљиво да избегнете прекомерно оптерећење меморије
- Примените кеширање модела за брже учитавање
- Пратите употребу меморије током оптимизације

### 6. Валидација тачности
- Увек валидирајте оптимизоване моделе у односу на оригиналне перформансе
- Користите репрезентативне тест скупове података за евалуацију
- Размотрите постепену оптимизацију (почните са конзервативним подешавањима)

## Решавање проблема

### Уобичајени проблеми

#### 1. Проблеми са инсталацијом
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Грешке у конверзији модела
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Проблеми са перформансама
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Проблеми са меморијом
- Смањите величину серије модела током оптимизације
- Користите стриминг за велике скупове података
- Омогућите кеширање модела: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Деградација тачности
- Користите већу прецизност (INT8 уместо INT4)
- Повећајте величину скупа података за калибрацију
- Примените оптимизацију мешовите прецизности

### Праћење перформанси

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Добијање помоћи

- **Документација**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Форум заједнице**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Додатни ресурси

### Званични линкови
- **OpenVINO Почетна страна**: [openvino.ai](https://openvino.ai/)
- **GitHub Репозиторијум**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Репозиторијум**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Ресурси за учење
- **OpenVINO Бележнице**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Водич за брзи почетак**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Водич за оптимизацију**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Алатке за интеграцију
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Бенчмарци перформанси
- **Званични бенчмарци**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Примери из заједнице
- **Jupyter Бележнице**: [OpenVINO Notebooks Репозиторијум](https://github.com/openvinotoolkit/openvino_notebooks) - Свеобухватни туторијали доступни у OpenVINO бележницама
- **Пример апликација**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Примери из стварног света за различите домене (рачунарски вид, NLP, аудио)
- **Блог постови**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI и блог постови заједнице са детаљним случајевима употребе

### Повезани алати
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Додатне технике оптимизације за Intel хардвер
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - За поређење мобилних и edge имплементација
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Алтернативни inference engine за крос-платформску употребу

## ➡️ Шта је следеће

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.