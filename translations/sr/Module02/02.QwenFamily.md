<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c8c05964be6fb235b026feed0bf066e",
  "translation_date": "2025-10-01T21:50:37+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "sr"
}
-->
# Одељак 2: Основе породице Qwen

Породица модела Qwen представља свеобухватан приступ Alibaba Cloud-а великим језичким моделима и мултимодалној вештачкој интелигенцији, показујући да отворени модели могу постићи изузетне резултате уз приступачност у различитим сценаријима примене. Важно је разумети како породица Qwen омогућава моћне AI способности са флексибилним опцијама примене, истовремено одржавајући конкурентне перформансе у разноврсним задацима.

## Ресурси за програмере

### Репозиторијум модела Hugging Face
Изабрани модели породице Qwen доступни су преко [Hugging Face](https://huggingface.co/models?search=qwen), пружајући приступ неким варијантама ових модела. Можете истражити доступне варијанте, прилагодити их за своје специфичне потребе и применити их кроз различите оквире.

### Алатке за локални развој
За локални развој и тестирање, можете користити [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) за покретање доступних Qwen модела на вашем развојном рачунару уз оптимизоване перформансе.

### Ресурси за документацију
- [Документација модела Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Оптимизација Qwen модела за примену на ивици](https://github.com/microsoft/olive)

## Увод

У овом туторијалу истражићемо породицу модела Qwen компаније Alibaba и њене основне концепте. Покрићемо еволуцију породице Qwen, иновативне методологије тренинга које чине Qwen моделе ефикасним, кључне варијанте у породици и практичне примене у различитим сценаријима.

## Циљеви учења

До краја овог туторијала, моћи ћете:

- Разумети филозофију дизајна и еволуцију породице модела Qwen компаније Alibaba
- Идентификовати кључне иновације које омогућавају Qwen моделима да постигну високе перформансе у различитим величинама параметара
- Препознати предности и ограничења различитих варијанти Qwen модела
- Применити знање о Qwen моделима за избор одговарајућих варијанти за реалне сценарије

## Разумевање савременог пејзажа AI модела

Пејзаж AI модела значајно се развио, са различитим организацијама које следе различите приступе развоју језичких модела. Док се неке фокусирају на власничке моделе затвореног кода, друге наглашавају приступачност и транспарентност отвореног кода. Традиционални приступ укључује или масивне власничке моделе доступне само преко API-ја или моделе отвореног кода који могу заостајати у способностима.

Овај парадигма ствара изазове за организације које траже моћне AI способности уз задржавање контроле над својим подацима, трошковима и флексибилношћу примене. Традиционални приступ често захтева избор између врхунских перформанси и практичних разматрања примене.

## Изазов приступачне AI изврсности

Потреба за висококвалитетном, приступачном AI постала је све важнија у различитим сценаријима. Размотрите примене које захтевају флексибилне опције примене за различите организационе потребе, исплативе имплементације где трошкови API-ја могу постати значајни, мултијезичке способности за глобалне примене или специјализовано доменско знање у областима као што су програмирање и математика.

### Кључни захтеви примене

Савремене AI примене суочавају се са неколико основних захтева који ограничавају практичну применљивост:

- **Приступачност**: Доступност отвореног кода за транспарентност и прилагођавање
- **Исплативост**: Разумни захтеви за рачунарске ресурсе за различите буџете
- **Флексибилност**: Више величина модела за различите сценарије примене
- **Глобални домет**: Јаке мултијезичке и међукултурне способности
- **Специјализација**: Варијанте специфичне за домен за одређене случајеве употребе

## Филозофија модела Qwen

Породица модела Qwen представља свеобухватан приступ развоју AI модела, приоритетно стављајући приступачност отвореног кода, мултијезичке способности и практичну примену уз одржавање конкурентних карактеристика перформанси. Qwen модели то постижу кроз разноврсне величине модела, висококвалитетне методологије тренинга и специјализоване варијанте за различите домене.

Породица Qwen обухвата различите приступе дизајниране да пруже опције широм спектра перформанси и ефикасности, омогућавајући примену од мобилних уређаја до серверских система предузећа, истовремено пружајући значајне AI способности. Циљ је демократизовати приступ висококвалитетној AI уз пружање флексибилности у избору примене.

### Основни принципи дизајна Qwen модела

Qwen модели су изграђени на неколико основних принципа који их разликују од других породица језичких модела:

- **Прво отворени код**: Потпуна транспарентност и приступачност за истраживање и комерцијалну употребу
- **Свеобухватан тренинг**: Тренинг на масивним, разноврсним скуповима података који покривају више језика и домена
- **Скалабилна архитектура**: Више величина модела за усклађивање са различитим рачунарским захтевима
- **Специјализована изврсност**: Варијанте специфичне за домен оптимизоване за одређене задатке

## Кључне технологије које омогућавају породицу Qwen

### Масивни тренинг у великом обиму

Један од дефинишућих аспеката породице Qwen је масивни обим података за тренинг и рачунарских ресурса уложених у развој модела. Qwen модели користе пажљиво одабране, мултијезичке скупове података који обухватају трилионе токена, дизајниране да пруже свеобухватно знање о свету и способности резоновања.

Овај приступ функционише комбиновањем висококвалитетног веб садржаја, академске литературе, репозиторијума кода и мултијезичких ресурса. Методологија тренинга наглашава и ширину знања и дубину разумевања у различитим доменима и језицима.

### Напредно резоновање и размишљање

Недавни Qwen модели укључују софистициране способности резоновања које омогућавају сложено решавање проблема у више корака:

**Мод размишљања (Qwen3)**: Модели могу да се укључе у детаљно резоновање корак по корак пре него што пруже коначне одговоре, слично људским приступима решавању проблема.

**Двоструки мод рада**: Способност пребацивања између брзог одговора за једноставне упите и дубљег размишљања за сложене проблеме.

**Интеграција ланца мисли**: Природна интеграција корака резоновања који побољшавају транспарентност и тачност у сложеним задацима.

### Архитектонске иновације

Породица Qwen укључује неколико архитектонских оптимизација дизајнираних за перформансе и ефикасност:

**Скалабилни дизајн**: Конзистентна архитектура широм величина модела која омогућава лако скалирање и поређење.

**Мултимодална интеграција**: Беспрекорна интеграција обраде текста, визије и аудио садржаја у уједињене архитектуре.

**Оптимизација примене**: Више опција квантовања и формата примене за различите хардверске конфигурације.

## Величина модела и опције примене

Савремена окружења примене имају користи од флексибилности Qwen модела у различитим рачунарским захтевима:

### Мали модели (0.5B-3B)

Qwen пружа ефикасне мале моделе погодне за примену на ивици, мобилне апликације и окружења са ограниченим ресурсима, уз одржавање импресивних способности.

### Средњи модели (7B-32B)

Модели средњег опсега нуде побољшане способности за професионалне примене, пружајући одличан баланс између перформанси и рачунарских захтева.

### Велики модели (72B+)

Модели пуне величине пружају врхунске перформансе за захтевне примене, истраживање и примене у предузећима које захтевају максималне способности.

## Предности породице Qwen модела

### Приступачност отвореног кода

Qwen модели пружају потпуну транспарентност и могућности прилагођавања, омогућавајући организацијама да разумеју, модификују и прилагоде моделе својим специфичним потребама без зависности од добављача.

### Флексибилност примене

Опсег величина модела омогућава примену у разноврсним хардверским конфигурацијама, од мобилних уређаја до врхунских сервера, пружајући организацијама флексибилност у избору AI инфраструктуре.

### Мултијезичка изврсност

Qwen модели се истичу у мултијезичком разумевању и генерисању, подржавајући десетине језика са посебном снагом у енглеском и кинеском, чинећи их погодним за глобалне примене.

### Конкурентне перформансе

Qwen модели доследно постижу конкурентне резултате на бенчмарковима уз пружање приступачности отвореног кода, показујући да отворени модели могу парирати власничким алтернативама.

### Специјализоване способности

Варијанте специфичне за домен, као што су Qwen-Coder и Qwen-Math, пружају специјализовано знање уз одржавање општих способности разумевања језика.

## Практични примери и случајеви употребе

Пре него што се упустимо у техничке детаље, истражимо неке конкретне примере шта Qwen модели могу да постигну:

### Пример математичког резоновања

Qwen-Math се истиче у решавању математичких проблема корак по корак. На пример, када се затражи решење сложеног проблема из калкулуса:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Пример мултијезичке подршке

Qwen модели показују јаке мултијезичке способности у различитим језицима:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Пример мултимодалних способности

Qwen-VL може истовремено обрађивати текст и слике:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Пример генерисања кода

Qwen-Coder се истиче у генерисању и објашњавању кода у различитим програмским језицима:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

Ова имплементација прати најбоље праксе са јасним именима променљивих, свеобухватном документацијом и ефикасном логиком.
```

### Edge Deployment Example

Qwen models can be deployed on various edge devices with optimized configurations:

```
# Пример примене на мобилном уређају уз квантовање
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Учитавање квантованог модела за мобилну примену

```
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Еволуција породице Qwen

### Qwen 1.0 и 1.5: Основни модели

Рани Qwen модели успоставили су основне принципе свеобухватног тренинга и приступачности отвореног кода:

- **Qwen-7B (7B параметара)**: Почетно издање са фокусом на разумевање кинеског и енглеског језика
- **Qwen-14B (14B параметара)**: Побољшане способности са унапређеним резоновањем и знањем
- **Qwen-72B (72B параметара)**: Модел великог обима који пружа врхунске перформансе
- **Серија Qwen1.5**: Проширена на више величина (0.5B до 110B) са побољшаним руковањем дугим контекстом

### Породица Qwen2: Мултимодална експанзија

Серија Qwen2 означила је значајан напредак у језичким и мултимодалним способностима:

- **Qwen2-0.5B до 72B**: Свеобухватан опсег језичких модела за различите потребе примене
- **Qwen2-57B-A14B (MoE)**: Архитектура мешавине експерата за ефикасно коришћење параметара
- **Qwen2-VL**: Напредне способности визије и језика за разумевање слика
- **Qwen2-Audio**: Способности обраде и разумевања аудио садржаја
- **Qwen2-Math**: Специјализовано математичко резоновање и решавање проблема

### Породица Qwen2.5: Побољшане перформансе

Серија Qwen2.5 донела је значајна побољшања у свим димензијама:

- **Проширен тренинг**: 18 трилиона токена тренинг података за побољшане способности
- **Проширен контекст**: До 128K токена дужине контекста, са Turbo варијантом која подржава 1M токена
- **Побољшана специјализација**: Унапређене Qwen2.5-Coder и Qwen2.5-Math варијанте
- **Боља мултијезичка подршка**: Побољшане перформансе у 27+ језика

### Породица Qwen3: Напредно резоновање

Најновија генерација помера границе способности резоновања и размишљања:

- **Qwen3-235B-A22B**: Врхунски модел мешавине експерата са укупно 235B параметара
- **Qwen3-30B-A3B**: Ефикасан MoE модел са снажним перформансама по активном параметру
- **Густи модели**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B за различите сценарије примене
- **Мод размишљања**: Хибридни приступ резоновању који подржава и брзе одговоре и дубоко размишљање
- **Мултијезичка изврсност**: Подршка за 119 језика и дијалеката
- **Унапређен тренинг**: 36 трилиона токена разноврсних, висококвалитетних тренинг података

## Примене Qwen модела

### Примене у предузећима

Организације користе Qwen моделе за анализу докумената, аутоматизацију корисничке подршке, помоћ у генерисању кода и апликације пословне интелигенције. Природа отвореног кода омогућава прилаго
- Qwen3-235B-A22B постиже конкурентне резултате у евалуацијама кодирања, математике и општих способности у поређењу са другим врхунским моделима као што су DeepSeek-R1, o1, o3-mini, Grok-3 и Gemini-2.5-Pro  
- Qwen3-30B-A3B надмашује QwQ-32B са 10 пута више активираних параметара  
- Qwen3-4B може да се такмичи са перформансама Qwen2.5-72B-Instruct  

**Достигнућа у ефикасности:**  
- Основни модели Qwen3-MoE постижу сличне перформансе као густо базни модели Qwen2.5, користећи само 10% активних параметара  
- Значајна уштеда трошкова у тренингу и инференцији у поређењу са густим моделима  

**Мултијезичне способности:**  
- Модели Qwen3 подржавају 119 језика и дијалеката  
- Јаке перформансе у различитим језичким и културним контекстима  

**Обим тренинга:**  
- Qwen3 користи скоро дупло више, са приближно 36 трилиона токена који покривају 119 језика и дијалеката у поређењу са Qwen2.5 који има 18 трилиона токена  

### Матрица поређења модела  

| Серија модела | Опсег параметара | Дужина контекста | Кључне предности | Најбоље примене |  
|---------------|------------------|------------------|------------------|-----------------|  
| **Qwen2.5** | 0.5B-72B | 32K-128K | Уравнотежене перформансе, мултијезичност | Опште апликације, продукциона употреба |  
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Генерација кода, програмирање | Развој софтвера, помоћ у кодирању |  
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Математичко резоновање | Едукативне платформе, СТЕМ апликације |  
| **Qwen2.5-VL** | Различито | Променљиво | Разумевање визије и језика | Мултимодалне апликације, анализа слика |  
| **Qwen3** | 0.6B-235B | Променљиво | Напредно резоновање, мод размишљања | Комплексно резоновање, истраживачке апликације |  
| **Qwen3 MoE** | 30B-235B укупно | Променљиво | Ефикасне перформансе великог обима | Корпоративне апликације, потребе за високим перформансама |  

## Водич за избор модела  

### За основне апликације  
- **Qwen2.5-0.5B/1.5B**: Мобилне апликације, уређаји на ивици, апликације у реалном времену  
- **Qwen2.5-3B/7B**: Општи чет-ботови, генерисање садржаја, системи питања и одговора  

### За математичке и резоновање задатке  
- **Qwen2.5-Math**: Решавање математичких проблема и СТЕМ едукација  
- **Qwen3 са модом размишљања**: Комплексно резоновање које захтева анализу корак по корак  

### За програмирање и развој  
- **Qwen2.5-Coder**: Генерација кода, дебаговање, помоћ у програмирању  
- **Qwen3**: Напредни задаци програмирања са способностима резоновања  

### За мултимодалне апликације  
- **Qwen2.5-VL**: Разумевање слика, визуелно питање и одговор  
- **Qwen-Audio**: Обрада звука и разумевање говора  

### За корпоративну употребу  
- **Qwen2.5-32B/72B**: Разумевање језика високих перформанси  
- **Qwen3-235B-A22B**: Максималне способности за захтевне апликације  

## Платформе за примену и доступност  

### Облачне платформе  
- **Hugging Face Hub**: Свеобухватан репозиторијум модела са подршком заједнице  
- **ModelScope**: Алибабина платформа модела са алатима за оптимизацију  
- **Разни облачни провајдери**: Подршка кроз стандардне ML платформе  

### Оквири за локални развој  
- **Transformers**: Стандардна интеграција Hugging Face-а за лаку примену  
- **vLLM**: Сервер високих перформанси за продукциона окружења  
- **Ollama**: Поједностављена локална примена и управљање  
- **ONNX Runtime**: Крос-платформска оптимизација за различит хардвер  
- **llama.cpp**: Ефикасна C++ имплементација за различите платформе  

### Ресурси за учење  
- **Qwen документација**: Званична документација и картице модела  
- **Hugging Face Model Hub**: Интерактивни демо и примери заједнице  
- **Истраживачки радови**: Технички радови на arxiv-у за дубље разумевање  
- **Форуми заједнице**: Активна подршка заједнице и дискусије  

### Како започети са Qwen моделима  

#### Платформе за развој  
1. **Hugging Face Transformers**: Започните са стандардном Python интеграцијом  
2. **ModelScope**: Истражите Алибабине алате за оптимизовану примену  
3. **Локална примена**: Користите Ollama или директне трансформере за локално тестирање  

#### Пут учења  
1. **Разумевање основних концепата**: Проучите архитектуру и способности Qwen породице  
2. **Експериментисање са варијантама**: Испробајте различите величине модела да разумете компромисе у перформансама  
3. **Практична имплементација**: Примените моделе у развојним окружењима  
4. **Оптимизација примене**: Фино подесите за продукционе случајеве  

#### Најбоље праксе  
- **Започните са мањим моделима**: Почните са мањим моделима (1.5B-7B) за почетни развој  
- **Користите шаблоне за чет**: Примените правилно форматирање за оптималне резултате  
- **Пратите ресурсе**: Пратите употребу меморије и брзину инференције  
- **Размотрите специјализацију**: Изаберите варијанте специфичне за домен када је то прикладно  

## Напредни обрасци употребе  

### Примери фино подешавања  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```
  
### Специјализовано инжењерство упита  

**За задатке комплексног резоновања:**  
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```
  
**За генерисање кода са контекстом:**  
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```
  
### Мултијезичне апликације  

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```
  
### 🔧 Обрасци продукционе примене  

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```
  

## Стратегије оптимизације перформанси  

### Оптимизација меморије  

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```
  
### Оптимизација инференције  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```
  

## Најбоље праксе и смернице  

### Безбедност и приватност  

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```
  
### Праћење и евалуација  

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```
  

## Закључак  

Породица Qwen модела представља свеобухватан приступ демократизацији AI технологије уз одржавање конкурентних перформанси у различитим апликацијама. Кроз посвећеност отвореном приступу, мултијезичним способностима и флексибилним опцијама примене, Qwen омогућава организацијама и програмерима да искористе моћне AI способности без обзира на ресурсе или специфичне захтеве.  

### Кључни закључци  

**Изврсност отвореног кода**: Qwen показује да модели отвореног кода могу постићи перформансе конкурентне са власничким алтернативама уз пружање транспарентности, прилагођавања и контроле.  

**Скалабилна архитектура**: Опсег од 0.5B до 235B параметара омогућава примену у целом спектру рачунарских окружења, од мобилних уређаја до корпоративних кластера.  

**Специјализоване способности**: Варијанте специфичне за домен као што су Qwen-Coder, Qwen-Math и Qwen-VL пружају специјализовану експертизу уз одржавање општег разумевања језика.  

**Глобална доступност**: Јака мултијезична подршка за 119+ језика чини Qwen погодним за међународне апликације и разноврсне корисничке базе.  

**Континуирана иновација**: Еволуција од Qwen 1.0 до Qwen3 показује конзистентно побољшање способности, ефикасности и опција примене.  

### Поглед у будућност  

Како се породица Qwen модела наставља развијати, можемо очекивати:  
- **Побољшану ефикасност**: Континуирана оптимизација за боље односе перформанси-параметара  
- **Проширене мултимодалне способности**: Интеграцију софистицираније обраде визије, звука и текста  
- **Унапређено резоновање**: Напредни механизми размишљања и способности решавања проблема у више корака  
- **Боље алате за примену**: Побољшане оквире и алате за оптимизацију за различите сценарије примене  
- **Раст заједнице**: Проширен екосистем алата, апликација и доприноса заједнице  

### Следећи кораци  

Без обзира да ли градите чет-бот, развијате едукативне алате, креирате асистенте за кодирање или радите на мултијезичним апликацијама, породица Qwen пружа скалабилна решења са јаком подршком заједнице и свеобухватном документацијом.  

За најновије информације, издања модела и детаљну техничку документацију, посетите званичне Qwen репозиторијуме на Hugging Face-у и истражите активне дискусије и примере заједнице.  

Будућност развоја AI лежи у доступним, транспарентним и моћним алатима који омогућавају иновације у свим секторима и размерама. Породица Qwen модела оличава ову визију, пружајући организацијама и програмерима основу за изградњу следеће генерације апликација заснованих на AI.  

## Додатни ресурси  

- **Званична документација**: [Qwen документација](https://qwen.readthedocs.io/)  
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)  
- **Технички радови**: [Qwen истраживачке публикације](https://arxiv.org/search/?query=Qwen&searchtype=all)  
- **Заједница**: [GitHub дискусије и проблеми](https://github.com/QwenLM/)  
- **ModelScope платформа**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)  

## Исходи учења  

Након завршетка овог модула, моћи ћете да:  
1. Објасните архитектонске предности породице Qwen модела и њен приступ отвореном коду  
2. Изаберете одговарајућу варијанту Qwen модела на основу специфичних захтева апликације и ограничења ресурса  
3. Примените Qwen моделе у различитим сценаријима примене са оптимизованим конфигурацијама  
4. Примените технике квантовања и оптимизације за побољшање перформанси Qwen модела  
5. Процените компромисе између величине модела, перформанси и способности у оквиру породице Qwen  

## Шта следи  

- [03: Основе породице Gemma](03.GemmaFamily.md)  

---

**Одрицање од одговорности**:  
Овај документ је преведен помоћу услуге за превођење уз помоћ вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.