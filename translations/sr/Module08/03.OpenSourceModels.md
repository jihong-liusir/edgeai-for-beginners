<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:01:55+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "sr"
}
-->
# Сесија 3: Модели отвореног кода са Foundry Local

## Преглед

Ова сесија истражује како интегрисати моделе отвореног кода у Foundry Local: избор модела из заједнице, интеграцију садржаја Hugging Face и стратегије „донеси свој модел“ (BYOM). Такође ћете открити серију Model Mondays за континуирано учење и откривање модела.

Референце:
- Foundry Local документација: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Компилација Hugging Face модела: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Циљеви учења
- Откривање и процена модела отвореног кода за локалну инференцију
- Компилација и покретање одабраних Hugging Face модела у Foundry Local
- Примена стратегија избора модела за тачност, кашњење и потребе ресурса
- Управљање моделима локално уз кеширање и верзионисање

## Део 1: Откривање и избор модела (Корак по корак)

Корак 1) Списак доступних модела у локалном каталогу  
```cmd
foundry model list
```
  
Корак 2) Брзо тестирање два кандидата (аутоматско преузимање при првом покретању)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Корак 3) Забележите основне метрике  
- Посматрајте кашњење (субјективно) и квалитет за фиксни упит  
- Пратите употребу меморије преко Task Manager-а док се сваки модел покреће  

## Део 2: Покретање модела из каталога преко CLI (Корак по корак)

Корак 1) Покрените модел  
```cmd
foundry model run llama-3.2
```
  
Корак 2) Пошаљите тестни упит преко OpenAI-компатибилног ендпоинта  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Део 3: BYOM – Компилација Hugging Face модела (Корак по корак)

Пратите званични водич за компилацију модела. Испод је висок ниво процеса—погледајте Microsoft Learn чланак за тачне команде и подржане конфигурације.

Корак 1) Припремите радни директоријум  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Корак 2) Компилирајте подржани HF модел  
- Користите кораке из Learn документације за конвертовање и постављање компилираног ONNX модела у ваш `models` директоријум  
- Потврдите са:  
```cmd
foundry cache ls
```
  
Требало би да видите име вашег компилираног модела (на пример, `llama-3.2`).  

Корак 3) Покрените компилирани модел  
```cmd
foundry model run llama-3.2 --verbose
```
  
Напомене:  
- Обезбедите довољно диска и RAM-а за компилацију и покретање  
- Почните са мањим моделима да потврдите процес, а затим прелазите на веће  

## Део 4: Практична селекција модела (Корак по корак)

Корак 1) Креирајте `models.json` регистар  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Корак 2) Мали селектор скрипт  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Део 5: Практични бенчмаркови (Корак по корак)

Корак 1) Једноставан бенчмарк кашњења  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Корак 2) Провера квалитета  
- Користите фиксни сет упита, сачувајте излаз у CSV/JSON  
- Ручно оцените флуентност, релевантност и исправност (1–5)  

## Део 6: Следећи кораци
- Претплатите се на Model Mondays за нове моделе и савете: https://aka.ms/model-mondays  
- Допринесите налазима у тимски `models.json`  
- Припремите се за Сесију 4: поређење LLM-ова и SLM-ова, локална и облачна инференција, и практичне демонстрације  

---

