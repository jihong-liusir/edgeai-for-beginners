<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-19T01:44:00+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "sr"
}
-->
# Контејнеризовано облачно распоређивање - решења за производну скалу

Овај свеобухватни туторијал покрива три главна приступа за распоређивање Microsoft-овог Phi-4-mini-instruct модела у контејнеризованим окружењима: vLLM, Ollama и SLM Engine са ONNX Runtime. Овај модел са 3.8 милијарди параметара представља оптималан избор за задатке резоновања уз одржавање ефикасности за распоређивање на ивици.

## Садржај

1. [Увод у контејнерско распоређивање Phi-4-mini](../../../Module03)
2. [Циљеви учења](../../../Module03)
3. [Разумевање класификације Phi-4-mini](../../../Module03)
4. [vLLM контејнерско распоређивање](../../../Module03)
5. [Ollama контејнерско распоређивање](../../../Module03)
6. [SLM Engine са ONNX Runtime](../../../Module03)
7. [Оквир за поређење](../../../Module03)
8. [Најбоље праксе](../../../Module03)

## Увод у контејнерско распоређивање Phi-4-mini

Мали језички модели (SLM) представљају кључни напредак у EdgeAI, омогућавајући софистициране способности обраде природног језика на уређајима са ограниченим ресурсима. Овај туторијал се фокусира на стратегије контејнерског распоређивања за Microsoft-ов Phi-4-mini-instruct, модел најновије генерације за резоновање који балансира између способности и ефикасности.

### Истакнути модел: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 милијарди параметара)**: Најновији лагани модел Microsoft-а, подешен за инструкције, дизајниран за окружења са ограниченом меморијом/рачунарским ресурсима, са изузетним способностима у:
- **Математичком резоновању и сложеним прорачунима**
- **Генерисању, дебаговању и анализи кода**
- **Логичком решавању проблема и резоновању корак по корак**
- **Едукативним апликацијама које захтевају детаљна објашњења**
- **Позивању функција и интеграцији алата**

Део категорије "Мали SLM-ови" (1.5B - 13.9B параметара), Phi-4-mini постиже оптималан баланс између способности резоновања и ефикасности ресурса.

### Предности контејнерског распоређивања Phi-4-mini

- **Оперативна ефикасност**: Брза инференција за задатке резоновања уз ниже захтеве за рачунарске ресурсе
- **Флексибилност распоређивања**: AI способности на уређају уз побољшану приватност кроз локалну обраду
- **Исплативост**: Смањени оперативни трошкови у поређењу са већим моделима уз одржавање квалитета
- **Изолација**: Чиста одвојеност између инстанци модела и сигурних окружења за извршавање
- **Скалабилност**: Лако хоризонтално скалирање за повећан капацитет резоновања

## Циљеви учења

До краја овог туторијала, бићете у могућности да:

- Распоредите и оптимизујете Phi-4-mini-instruct у различитим контејнеризованим окружењима
- Примените напредне стратегије квантовања и компресије за различите сценарије распоређивања
- Конфигуришете контејнерску оркестрацију спремну за производњу за задатке резоновања
- Процените и изаберете одговарајуће оквире за распоређивање на основу специфичних захтева случаја употребе
- Примените најбоље праксе за сигурност, праћење и скалирање контејнеризованих SLM распоређивања

## Разумевање класификације Phi-4-mini

### Спецификације модела

**Технички детаљи:**
- **Параметри**: 3.8 милијарди (категорија малих SLM-ова)
- **Архитектура**: Густ декодер само Transformer са груписаним упитима
- **Дужина контекста**: 128K токена (32K препоручено за оптималне перформансе)
- **Вокабулар**: 200K токена са подршком за више језика
- **Тренинг подаци**: 5T токена садржаја богатог резоновањем

### Захтеви за ресурсе

| Тип распоређивања | Мин RAM | Препоручен RAM | VRAM (GPU) | Складиште | Типични случајеви употребе |
|-------------------|---------|----------------|------------|-----------|---------------------------|
| **Развој** | 6GB | 8GB | - | 8GB | Локално тестирање, прототипирање |
| **Производња CPU** | 8GB | 12GB | - | 10GB | Edge сервери, исплативо распоређивање |
| **Производња GPU** | 6GB | 8GB | 4-6GB | 8GB | Услуге резоновања високог капацитета |
| **Оптимизовано за ивицу** | 4GB | 6GB | - | 6GB | Квантовано распоређивање, IoT капије |

### Способности Phi-4-mini

- **Математичка изврсност**: Напредно решавање аритметичких, алгебарских и калкулус проблема
- **Интелигенција кода**: Генерисање кода на Python-у, JavaScript-у и више језика уз дебаговање
- **Логичко резоновање**: Декомпозиција проблема корак по корак и конструкција решења
- **Едукативна подршка**: Детаљна објашњења погодна за учење и наставу
- **Позивање функција**: Нативна подршка за интеграцију алата и API интеракције

## vLLM контејнерско распоређивање

vLLM пружа одличну подршку за Phi-4-mini-instruct уз оптимизоване перформансе инференције и OpenAI-компатибилне API-је, што га чини идеалним за производне услуге резоновања.

### Примери брзог почетка

#### Основно CPU распоређивање (развој)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Производно распоређивање убрзано GPU-ом
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Конфигурација за производњу

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестирање способности резоновања Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama контејнерско распоређивање

Ollama пружа одличну подршку за Phi-4-mini-instruct уз поједностављено распоређивање и управљање, што га чини идеалним за развој и уравнотежена производна распоређивања.

### Брза поставка

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Конфигурација за производњу

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Оптимизација модела и варијанте

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Примери употребе API-ја

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine са ONNX Runtime

ONNX Runtime пружа оптималне перформансе за распоређивање на ивици Phi-4-mini-instruct модела уз напредну оптимизацију и компатибилност на више платформи.

### Основна поставка

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Поједностављена имплементација сервера

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Скрипта за конверзију модела

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Конфигурација за производњу

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестирање ONNX распоређивања

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Оквир за поређење

### Поређење оквира за Phi-4-mini

| Карактеристика | vLLM | Ollama | ONNX Runtime |
|----------------|------|--------|--------------|
| **Комплексност поставке** | Умерена | Лака | Комплексна |
| **Перформансе (GPU)** | Одличне (~25 ток/с) | Врло добре (~20 ток/с) | Добре (~15 ток/с) |
| **Перформансе (CPU)** | Добре (~8 ток/с) | Врло добре (~12 ток/с) | Одличне (~15 ток/с) |
| **Употреба меморије** | 8-12GB | 6-10GB | 4-8GB |
| **API компатибилност** | OpenAI компатибилан | Прилагођен REST | Прилагођен FastAPI |
| **Позивање функција** | ✅ Нативно | ✅ Подржано | ⚠️ Прилагођена имплементација |
| **Подршка за квантовање** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX квантовање |
| **Спремност за производњу** | ✅ Одлична | ✅ Врло добра | ✅ Добра |
| **Распоређивање на ивици** | Добро | Одлично | Изванредно |

## Додатни ресурси

### Званична документација
- **Microsoft Phi-4 Model Card**: Детаљне спецификације и упутства за употребу
- **vLLM документација**: Напредне опције конфигурације и оптимизације
- **Ollama библиотека модела**: Модели из заједнице и примери прилагођавања
- **ONNX Runtime водичи**: Стратегије оптимизације перформанси и распоређивања

### Алатке за развој
- **Hugging Face Transformers**: За интеракцију и прилагођавање модела
- **OpenAI API спецификација**: За тестирање компатибилности са vLLM
- **Најбоље праксе за Docker**: Сигурност контејнера и смернице за оптимизацију
- **Kubernetes распоређивање**: Шаблони оркестрације за скалирање у производњи

### Ресурси за учење
- **Бенчмаркинг перформанси SLM-ова**: Методологије за компаративну анализу
- **Распоређивање Edge AI**: Најбоље праксе за окружења са ограниченим ресурсима
- **Оптимизација задатака резоновања**: Стратегије за подешавање упита за математичке и логичке проблеме
- **Сигурност контејнера**: Практике за учвршћивање AI распоређивања

## Резултати учења

Након завршетка овог модула, бићете у могућности да:

1. Распоредите Phi-4-mini-instruct модел у контејнеризованим окружењима користећи више оквира
2. Конфигуришете и оптимизујете SLM распоређивања за различита хардверска окружења
3. Примените најбоље праксе за сигурност контејнеризованих AI распоређивања
4. Упоредите и изаберете одговарајуће оквире за распоређивање на основу специфичних захтева случаја употребе
5. Примените стратегије за праћење и скалирање за SLM услуге спремне за производњу

## Шта следи

- Вратите се на [Модул 1](../Module01/README.md)
- Вратите се на [Модул 2](../Module02/README.md)

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.