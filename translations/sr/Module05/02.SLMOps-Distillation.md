<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-19T01:09:16+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "sr"
}
-->
# Одељак 2: Дестилација модела - од теорије до праксе

## Садржај
1. [Увод у дестилацију модела](../../../Module05)
2. [Зашто је дестилација важна](../../../Module05)
3. [Процес дестилације](../../../Module05)
4. [Практична имплементација](../../../Module05)
5. [Пример дестилације у Azure ML](../../../Module05)
6. [Најбоље праксе и оптимизација](../../../Module05)
7. [Примене у стварном свету](../../../Module05)
8. [Закључак](../../../Module05)

## Увод у дестилацију модела {#introduction}

Дестилација модела је моћна техника која нам омогућава да креирамо мање, ефикасније моделе, уз очување великог дела перформанси већих, сложенијих модела. Овај процес подразумева тренирање компактног "ученик" модела да опонаша понашање већег "учитељ" модела.

**Кључне предности:**
- **Смањени захтеви за рачунарским ресурсима** током инференције
- **Мања употреба меморије** и потребе за складиштењем
- **Брже време инференције** уз задржавање разумне тачности
- **Исплативо распоређивање** у окружењима са ограниченим ресурсима

## Зашто је дестилација важна {#why-distillation-matters}

Велики језички модели (LLMs) постају све моћнији, али и све захтевнији у погледу ресурса. Иако модел са милијардама параметара може пружити одличне резултате, често није практичан за многе стварне примене због:

### Ограничења ресурса
- **Рачунарски трошкови**: Велики модели захтевају значајну GPU меморију и процесорску снагу
- **Кашњење инференције**: Сложени модели дуже генеришу одговоре
- **Потрошња енергије**: Већи модели троше више енергије, што повећава оперативне трошкове
- **Трошкови инфраструктуре**: Хостовање великих модела захтева скупи хардвер

### Практична ограничења
- **Мобилна примена**: Велики модели не могу ефикасно да раде на мобилним уређајима
- **Апликације у реалном времену**: Апликације које захтевају ниско кашњење не могу да прихвате спору инференцију
- **Рачунарство на ивици**: IoT и уређаји на ивици имају ограничене рачунарске ресурсе
- **Трошкови**: Многе организације не могу да приуште инфраструктуру за распоређивање великих модела

## Процес дестилације {#the-distillation-process}

Дестилација модела прати двостепени процес који преноси знање са учитељ модела на ученик модел:

### Фаза 1: Генерисање синтетичких података

Учитељ модел генерише одговоре за ваш скуп података за тренирање, стварајући висококвалитетне синтетичке податке који одражавају знање и обрасце размишљања учитеља.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Кључни аспекти ове фазе:**
- Учитељ модел обрађује сваки пример за тренирање
- Генерисани одговори постају "истина" за тренирање ученика
- Овај процес хвата обрасце одлучивања учитеља
- Квалитет синтетичких података директно утиче на перформансе ученик модела

### Фаза 2: Фино подешавање ученик модела

Ученик модел се тренира на синтетичком скупу података, учећи да реплицира понашање и одговоре учитеља.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Циљеви тренирања:**
- Минимизирање разлике између излаза ученика и учитеља
- Очување знања учитеља у мањем простору параметара
- Одржавање перформанси уз смањење сложености модела

## Практична имплементација {#practical-implementation}

### Избор учитељ и ученик модела

**Избор учитељ модела:**
- Изаберите LLM моделе великог обима (100B+ параметара) са доказаним перформансама за ваш специфичан задатак
- Популарни учитељ модели укључују:
  - **DeepSeek V3** (671B параметара) - одличан за резоновање и генерисање кода
  - **Meta Llama 3.1 405B Instruct** - свеобухватне способности за општу употребу
  - **GPT-4** - снажне перформансе за различите задатке
  - **Claude 3.5 Sonnet** - одличан за сложене задатке резоновања
- Уверите се да учитељ модел добро ради на вашим доменским подацима

**Избор ученик модела:**
- Балансирајте величину модела и захтеве за перформансама
- Фокусирајте се на ефикасне, мање моделе као што су:
  - **Microsoft Phi-4-mini** - најновији ефикасни модел са снажним способностима резоновања
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K и 128K варијанте)
  - Microsoft Phi-3.5 Mini Instruct

### Кораци имплементације

1. **Припрема података**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Подешавање учитељ модела**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Генерисање синтетичких података**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Тренирање ученик модела**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Пример дестилације у Azure ML {#azure-ml-example}

Azure Machine Learning пружа свеобухватну платформу за имплементацију дестилације модела. Ево како да искористите Azure ML за ваш радни ток дестилације:

### Предуслови

1. **Azure ML Workspace**: Поставите свој радни простор у одговарајућем региону
   - Осигурајте приступ учитељ моделима великог обима (DeepSeek V3, Llama 405B)
   - Конфигуришите регионе на основу доступности модела

2. **Рачунарски ресурси**: Конфигуришите одговарајуће рачунарске инстанце за тренирање
   - Инстанце са великом меморијом за инференцију учитељ модела
   - GPU-омогућене инстанце за фино подешавање ученик модела

### Подржани типови задатака

Azure ML подржава дестилацију за различите задатке:

- **Интерпретација природног језика (NLI)**
- **Конверзацијски AI**
- **Питања и одговори (QA)**
- **Математичко резоновање**
- **Сажимање текста**

### Пример имплементације

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Праћење и евалуација

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Најбоље праксе и оптимизација {#best-practices}

### Квалитет података

**Квалитетни подаци за тренирање су кључни:**
- Осигурајте разноврсне и репрезентативне примере за тренирање
- Користите доменске податке кад год је могуће
- Верификујте излазе учитељ модела пре него што их употребите за тренирање ученика
- Балансирајте скуп података како бисте избегли пристрасност у учењу ученик модела

### Подешавање хиперпараметара

**Кључни параметри за оптимизацију:**
- **Стопа учења**: Почните са мањим стопама (1e-5 до 5e-5) за фино подешавање
- **Величина серије**: Балансирајте између ограничења меморије и стабилности тренирања
- **Број епоха**: Пратите прекомерно прилагођавање; обично су довољне 2-5 епоха
- **Скалирање температуре**: Подесите мекоћу излаза учитеља за бољи пренос знања

### Разматрања архитектуре модела

**Компатибилност учитељ-ученик:**
- Осигурајте архитектонску компатибилност између учитељ и ученик модела
- Размотрите подударање међуслојева за бољи пренос знања
- Користите технике преноса пажње када је применљиво

### Стратегије евалуације

**Свеобухватан приступ евалуацији:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Примене у стварном свету {#real-world-applications}

### Мобилна и ивична примена

Дестиловани модели омогућавају AI могућности на уређајима са ограниченим ресурсима:
- **Апликације за паметне телефоне** са обрадом текста у реалном времену
- **IoT уређаји** који обављају локалну инференцију
- **Уграђени системи** са ограниченим рачунарским ресурсима

### Исплативи производни системи

Организације користе дестилацију за смањење оперативних трошкова:
- **Четботови за корисничку подршку** са бржим временом одговора
- **Системи за модерацију садржаја** који ефикасно обрађују велике количине података
- **Системи за превођење у реалном времену** са нижим кашњењем

### Доменске примене

Дестилација помаже у креирању специјализованих модела:
- **Помоћ у медицинској дијагностици** са локалном инференцијом која чува приватност
- **Анализа правних докумената** оптимизована за специфичне правне домене
- **Процена финансијског ризика** са брзим доношењем одлука

### Студија случаја: Корисничка подршка са DeepSeek V3 → Phi-4-mini

Технолошка компанија је имплементирала дестилацију за свој систем корисничке подршке:

**Детаљи имплементације:**
- **Учитељ модел**: DeepSeek V3 (671B параметара) - одличан за резоновање сложених корисничких упита
- **Ученик модел**: Phi-4-mini - оптимизован за брзу инференцију и распоређивање
- **Подаци за тренирање**: 50,000 разговора корисничке подршке
- **Задатак**: Подршка у више корака са решавањем техничких проблема

**Постигнути резултати:**
- **Смањење времена инференције за 85%** (са 3.2с на 0.48с по одговору)
- **Смањење захтева за меморијом за 95%** (са 1.2TB на 60GB)
- **Очување 92% тачности** оригиналног модела на задацима подршке
- **Смањење оперативних трошкова за 60%**
- **Побољшана скалабилност** - сада може да обради 10x више корисника истовремено

**Разбијање перформанси:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Закључак {#conclusion}

Дестилација модела представља кључну технику за демократизацију приступа напредним AI могућностима. Омогућавајући креирање мањих, ефикаснијих модела који задржавају велики део перформанси својих већих колега, дестилација одговара на растућу потребу за практичним распоређивањем AI-а.

### Кључни закључци

1. **Дестилација премошћује јаз** између перформанси модела и практичних ограничења
2. **Двостепени процес** осигурава ефикасан пренос знања са учитеља на ученика
3. **Azure ML пружа робусну инфраструктуру** за имплементацију радних токова дестилације
4. **Правилна евалуација и оптимизација** су суштински за успешну дестилацију
5. **Примене у стварном свету** показују значајне предности у трошковима, брзини и доступности

### Будући правци

Како се област наставља развијати, можемо очекивати:
- **Напредне технике дестилације** са бољим методама преноса знања
- **Дестилацију са више учитеља** за побољшане способности ученик модела
- **Аутоматизовану оптимизацију** процеса дестилације
- **Ширу подршку моделима** кроз различите архитектуре и домене

Дестилација модела омогућава организацијама да искористе најсавременије AI могућности уз задржавање практичних ограничења распоређивања, чинећи напредне језичке моделе доступним у широком спектру примена и окружења.

## ➡️ Шта следи

- [03: Фино подешавање - прилагођавање модела за специфичне задатке](./03.SLMOps-Finetuing.md)

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.