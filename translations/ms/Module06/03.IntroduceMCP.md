<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T14:22:06+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ms"
}
-->
# Seksyen 03 - Integrasi Protokol Konteks Model (MCP)

## Pengenalan kepada MCP (Protokol Konteks Model)

Protokol Konteks Model (MCP) adalah kerangka kerja revolusioner yang membolehkan model bahasa berinteraksi dengan alat dan sistem luaran secara standard. Berbeza dengan pendekatan tradisional di mana model terasing, MCP mencipta jambatan antara model AI dan dunia nyata melalui protokol yang terdefinisi dengan baik.

### Apa itu MCP?

MCP berfungsi sebagai protokol komunikasi yang membolehkan model bahasa untuk:
- Menyambung kepada sumber data luaran
- Melaksanakan alat dan fungsi
- Berinteraksi dengan API dan perkhidmatan
- Mengakses maklumat masa nyata
- Melakukan operasi kompleks berbilang langkah

Protokol ini mengubah model bahasa statik menjadi agen dinamik yang mampu melaksanakan tugas praktikal di luar penjanaan teks.

## Model Bahasa Kecil (SLM) dalam MCP

Model Bahasa Kecil mewakili pendekatan yang efisien untuk penerapan AI, menawarkan beberapa kelebihan:

### Kelebihan SLM
- **Kecekapan Sumber**: Keperluan pengiraan yang lebih rendah
- **Masa Respons Lebih Cepat**: Latensi yang dikurangkan untuk aplikasi masa nyata  
- **Kos Efektif**: Keperluan infrastruktur yang minimum
- **Privasi**: Boleh dijalankan secara tempatan tanpa penghantaran data
- **Penyesuaian**: Lebih mudah untuk disesuaikan dengan domain tertentu

### Mengapa SLM Berfungsi Baik dengan MCP

SLM yang digabungkan dengan MCP mencipta kombinasi yang kuat di mana keupayaan penaakulan model ditingkatkan oleh alat luaran, mengimbangi bilangan parameter yang lebih kecil melalui fungsi yang dipertingkatkan.

## Gambaran Keseluruhan Python MCP SDK

Python MCP SDK menyediakan asas untuk membina aplikasi yang diaktifkan MCP. SDK ini merangkumi:

- **Perpustakaan Klien**: Untuk menyambung ke pelayan MCP
- **Kerangka Pelayan**: Untuk mencipta pelayan MCP tersuai
- **Pengendali Protokol**: Untuk mengurus komunikasi
- **Integrasi Alat**: Untuk melaksanakan fungsi luaran

## Pelaksanaan Praktikal: Klien MCP Phi-4

Mari kita terokai pelaksanaan dunia nyata menggunakan model mini Phi-4 Microsoft yang diintegrasikan dengan keupayaan MCP.

### Seni Bina Sistem

Pelaksanaan ini mengikuti seni bina berlapis:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Komponen Utama

#### 1. Kelas Klien MCP

**BaseMCPClient**: Asas abstrak yang menyediakan fungsi umum
- Protokol pengurus konteks async
- Definisi antara muka standard
- Pengurusan sumber

**Phi4MiniMCPClient**: Pelaksanaan berasaskan STDIO
- Komunikasi proses tempatan
- Pengendalian input/output standard
- Pengurusan subprocess

**Phi4MiniSSEMCPClient**: Pelaksanaan Server-Sent Events
- Komunikasi penstriman HTTP
- Pengendalian acara masa nyata
- Kesambungan pelayan berasaskan web

#### 2. Integrasi LLM

**OllamaClient**: Hosting model tempatan
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Penyajian berprestasi tinggi
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Saluran Pemprosesan Alat

Saluran pemprosesan alat mengubah alat MCP menjadi format yang serasi dengan model bahasa:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Memulakan: Panduan Langkah Demi Langkah

### Langkah 1: Persediaan Persekitaran

Pasang kebergantungan yang diperlukan:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Langkah 2: Konfigurasi Asas

Tetapkan pembolehubah persekitaran anda:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Langkah 3: Menjalankan Klien MCP Pertama Anda

**Persediaan Asas Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Menggunakan Backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Sambungan Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Pelayan MCP Tersuai:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Langkah 4: Penggunaan Programatik

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Ciri Lanjutan

### Sokongan Multi-Backend

Pelaksanaan ini menyokong kedua-dua backend Ollama dan vLLM, membolehkan anda memilih berdasarkan keperluan anda:

- **Ollama**: Lebih baik untuk pembangunan dan ujian tempatan
- **vLLM**: Dioptimumkan untuk pengeluaran dan senario throughput tinggi

### Protokol Sambungan Fleksibel

Dua mod sambungan disokong:

**Mod STDIO**: Komunikasi proses langsung
- Latensi lebih rendah
- Sesuai untuk alat tempatan
- Persediaan mudah

**Mod SSE**: Penstriman berasaskan HTTP
- Mampu rangkaian
- Lebih baik untuk sistem teragih
- Kemas kini masa nyata

### Keupayaan Integrasi Alat

Sistem ini boleh diintegrasikan dengan pelbagai alat:
- Automasi web (Playwright)
- Operasi fail
- Interaksi API
- Perintah sistem
- Fungsi tersuai

## Pengendalian Ralat dan Amalan Terbaik

### Pengurusan Ralat Komprehensif

Pelaksanaan ini merangkumi pengendalian ralat yang kukuh untuk:

**Ralat Sambungan:**
- Kegagalan pelayan MCP
- Tamat masa rangkaian
- Isu kesambungan

**Ralat Pelaksanaan Alat:**
- Alat yang hilang
- Pengesahan parameter
- Kegagalan pelaksanaan

**Ralat Pemprosesan Respons:**
- Isu penguraian JSON
- Ketidakkonsistenan format
- Anomali respons LLM

### Amalan Terbaik

1. **Pengurusan Sumber**: Gunakan pengurus konteks async
2. **Pengendalian Ralat**: Laksanakan blok try-catch yang komprehensif
3. **Log**: Aktifkan tahap log yang sesuai
4. **Keselamatan**: Sahkan input dan sanitasi output
5. **Prestasi**: Gunakan pengumpulan sambungan dan caching

## Aplikasi Dunia Nyata

### Automasi Web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Pemprosesan Data
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrasi API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Pengoptimuman Prestasi

### Pengurusan Memori
- Pengendalian sejarah mesej yang efisien
- Pembersihan sumber yang betul
- Pengumpulan sambungan

### Pengoptimuman Rangkaian
- Operasi HTTP async
- Tamat masa yang boleh dikonfigurasi
- Pemulihan ralat yang lancar

### Pemprosesan Serentak
- I/O tidak menyekat
- Pelaksanaan alat selari
- Corak async yang efisien

## Pertimbangan Keselamatan

### Perlindungan Data
- Pengurusan kunci API yang selamat
- Pengesahan input
- Sanitasi output

### Keselamatan Rangkaian
- Sokongan HTTPS
- Lalai titik akhir tempatan
- Pengendalian token yang selamat

### Keselamatan Pelaksanaan
- Penapisan alat
- Persekitaran sandboxed
- Log audit

## Kesimpulan

SLM yang diintegrasikan dengan MCP mewakili perubahan paradigma dalam pembangunan aplikasi AI. Dengan menggabungkan kecekapan model kecil dengan kuasa alat luaran, pembangun boleh mencipta sistem pintar yang cekap sumber dan sangat mampu.

Pelaksanaan klien MCP Phi-4 menunjukkan bagaimana integrasi ini boleh dicapai dalam praktik, menyediakan asas kukuh untuk membina aplikasi berkuasa AI yang canggih.

Poin utama:
- MCP menjembatani jurang antara model bahasa dan sistem luaran
- SLM menawarkan kecekapan tanpa mengorbankan keupayaan apabila ditingkatkan dengan alat
- Seni bina modular membolehkan peluasan dan penyesuaian yang mudah
- Pengendalian ralat dan langkah keselamatan yang betul adalah penting untuk penggunaan pengeluaran

Tutorial ini menyediakan asas untuk membina aplikasi MCP yang dikuasakan SLM anda sendiri, membuka kemungkinan untuk automasi, pemprosesan data, dan integrasi sistem pintar.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.