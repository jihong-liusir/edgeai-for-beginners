<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-18T14:40:57+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "ms"
}
-->
# Seksyen 4: Penerokaan Mendalam Apple MLX Framework

## Kandungan
1. [Pengenalan kepada Apple MLX](../../../Module04)
2. [Ciri Utama untuk Pembangunan LLM](../../../Module04)
3. [Panduan Pemasangan](../../../Module04)
4. [Memulakan dengan MLX](../../../Module04)
5. [MLX-LM: Model Bahasa](../../../Module04)
6. [Bekerja dengan Model Bahasa Besar](../../../Module04)
7. [Integrasi Hugging Face](../../../Module04)
8. [Penukaran dan Kuantisasi Model](../../../Module04)
9. [Penalaan Model Bahasa](../../../Module04)
10. [Ciri LLM Lanjutan](../../../Module04)
11. [Amalan Terbaik untuk LLM](../../../Module04)
12. [Penyelesaian Masalah](../../../Module04)
13. [Sumber Tambahan](../../../Module04)

## Pengenalan kepada Apple MLX

Apple MLX ialah rangka kerja array yang direka khusus untuk pembelajaran mesin yang cekap dan fleksibel pada Apple Silicon, dibangunkan oleh Apple Machine Learning Research. Dilancarkan pada Disember 2023, MLX merupakan jawapan Apple kepada rangka kerja seperti PyTorch dan TensorFlow, dengan fokus khas pada keupayaan model bahasa besar yang berkuasa pada komputer Mac.

### Apa yang Membuat MLX Istimewa untuk LLM?

MLX direka untuk memanfaatkan sepenuhnya seni bina memori bersatu Apple Silicon, menjadikannya sangat sesuai untuk menjalankan dan menala model bahasa besar secara tempatan pada komputer Mac. Rangka kerja ini menghapuskan banyak isu keserasian yang biasanya dihadapi oleh pengguna Mac semasa bekerja dengan LLM.

### Siapa yang Patut Menggunakan MLX untuk LLM?

- **Pengguna Mac** yang ingin menjalankan LLM secara tempatan tanpa pergantungan awan
- **Penyelidik** yang bereksperimen dengan penalaan dan penyesuaian model bahasa
- **Pembangun** yang membina aplikasi AI dengan keupayaan model bahasa
- **Sesiapa sahaja** yang ingin memanfaatkan Apple Silicon untuk penjanaan teks, sembang, dan tugas bahasa

## Ciri Utama untuk Pembangunan LLM

### 1. Seni Bina Memori Bersatu
Memori bersatu Apple Silicon membolehkan MLX mengendalikan model bahasa besar dengan cekap tanpa beban salinan memori yang biasa dalam rangka kerja lain. Ini bermakna anda boleh bekerja dengan model yang lebih besar pada perkakasan yang sama.

### 2. Pengoptimuman Apple Silicon Asli
MLX dibina dari awal untuk cip siri M Apple, memberikan prestasi optimum untuk seni bina transformer yang biasa digunakan dalam model bahasa.

### 3. Sokongan Kuantisasi
Sokongan terbina dalam untuk kuantisasi 4-bit dan 8-bit mengurangkan keperluan memori sambil mengekalkan kualiti model, membolehkan model yang lebih besar dijalankan pada perkakasan pengguna.

### 4. Integrasi Hugging Face
Integrasi lancar dengan ekosistem Hugging Face menyediakan akses kepada ribuan model bahasa terlatih dengan alat penukaran yang mudah.

### 5. Penalaan LoRA
Sokongan untuk Penyesuaian Pangkat Rendah (LoRA) membolehkan penalaan model besar dengan sumber pengiraan yang minimum.

## Panduan Pemasangan

### Keperluan Sistem
- **macOS 13.0+** (untuk pengoptimuman Apple Silicon)
- **Python 3.8+**
- **Apple Silicon** (siri M1, M2, M3, M4)
- **Persekitaran ARM asli** (tidak berjalan di bawah Rosetta)
- **RAM 8GB+** (16GB+ disyorkan untuk model yang lebih besar)

### Pemasangan Pantas untuk LLM

Cara paling mudah untuk bermula dengan model bahasa ialah memasang MLX-LM:

```bash
pip install mlx-lm
```

Perintah tunggal ini memasang kedua-dua rangka kerja teras MLX dan utiliti model bahasa.

### Menyediakan Persekitaran Maya (Disyorkan)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Kebergantungan Tambahan untuk Model Audio

Jika anda merancang untuk bekerja dengan model ucapan seperti Whisper:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## Memulakan dengan MLX

### Model Bahasa Pertama Anda

Mari mulakan dengan menjalankan contoh penjanaan teks yang mudah:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Contoh API Python

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Memahami Pemuatan Model

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Model Bahasa

### Seni Bina Model yang Disokong

MLX-LM menyokong pelbagai seni bina model bahasa popular:

- **LLaMA dan LLaMA 2** - Model asas Meta
- **Mistral dan Mixtral** - Model yang cekap dan berkuasa
- **Phi-3** - Model bahasa kompak Microsoft
- **Qwen** - Model pelbagai bahasa Alibaba
- **Code Llama** - Khusus untuk penjanaan kod
- **Gemma** - Model bahasa terbuka Google

### Antara Muka Baris Perintah

Antara muka baris perintah MLX-LM menyediakan alat yang berkuasa untuk bekerja dengan model bahasa:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### API Python untuk Kes Penggunaan Lanjutan

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Bekerja dengan Model Bahasa Besar

### Corak Penjanaan Teks

#### Penjanaan Satu Pusingan
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Mengikuti Arahan
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Penulisan Kreatif
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Perbualan Berbilang Pusingan

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Integrasi Hugging Face

### Mencari Model Serasi MLX

MLX berfungsi dengan lancar dengan ekosistem Hugging Face:

- **Layari model MLX**: https://huggingface.co/models?library=mlx&sort=trending
- **Komuniti MLX**: https://huggingface.co/mlx-community (model yang telah ditukar)
- **Model asal**: Kebanyakan model LLaMA, Mistral, Phi, dan Qwen berfungsi dengan penukaran

### Memuatkan Model dari Hugging Face

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Memuat Turun Model untuk Penggunaan Luar Talian

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Penukaran dan Kuantisasi Model

### Menukar Model Hugging Face kepada MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Memahami Kuantisasi

Kuantisasi mengurangkan saiz model dan penggunaan memori dengan kehilangan kualiti yang minimum:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Kuantisasi Tersuai

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Penalaan Model Bahasa

### Penalaan LoRA (Penyesuaian Pangkat Rendah)

MLX menyokong penalaan cekap menggunakan LoRA, yang membolehkan anda menyesuaikan model besar dengan sumber pengiraan yang minimum:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Menyediakan Data Latihan

Buat fail JSON dengan contoh latihan anda:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Perintah Penalaan

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Menggunakan Model yang Telah Ditala

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Ciri LLM Lanjutan

### Caching Prompt untuk Kecekapan

Untuk penggunaan berulang konteks yang sama, MLX menyokong caching prompt untuk meningkatkan prestasi:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Penjanaan Teks Secara Streaming

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Bekerja dengan Model Penjanaan Kod

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Bekerja dengan Model Sembang

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Amalan Terbaik untuk LLM

### Pengurusan Memori

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Garis Panduan Pemilihan Model

**Untuk Eksperimen dan Pembelajaran:**
- Gunakan model kuantisasi 4-bit (contoh: `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Mulakan dengan model yang lebih kecil seperti Phi-3-mini

**Untuk Aplikasi Pengeluaran:**
- Pertimbangkan keseimbangan antara saiz model dan kualiti
- Uji kedua-dua model kuantisasi dan ketepatan penuh
- Penanda aras pada kes penggunaan spesifik anda

**Untuk Tugas Tertentu:**
- **Penjanaan Kod**: CodeLlama, Code Llama Instruct
- **Sembang Umum**: Mistral-7B-Instruct, Phi-3
- **Pelbagai Bahasa**: Model Qwen
- **Penulisan Kreatif**: Tetapan suhu yang lebih tinggi dengan Mistral atau LLaMA

### Amalan Terbaik Kejuruteraan Prompt

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Pengoptimuman Prestasi

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Penyelesaian Masalah

### Isu dan Penyelesaian Biasa

#### Masalah Pemasangan

**Isu**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Penyelesaian**: Gunakan Python ARM asli atau Miniconda:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Masalah Memori

**Isu**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Masalah Pemuatan Model

**Isu**: Model gagal dimuat atau menghasilkan output yang kurang baik
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Masalah Prestasi

**Isu**: Kelajuan penjanaan perlahan
- Tutup aplikasi lain yang menggunakan memori tinggi
- Gunakan model kuantisasi jika boleh
- Pastikan anda tidak berjalan di bawah Rosetta
- Periksa memori yang tersedia sebelum memuatkan model

### Petua Debugging

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Sumber Tambahan

### Dokumentasi dan Repositori Rasmi

- **Repositori GitHub MLX**: https://github.com/ml-explore/mlx
- **Contoh MLX-LM**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **Dokumentasi MLX**: https://ml-explore.github.io/mlx/
- **Integrasi Hugging Face MLX**: https://huggingface.co/docs/hub/en/mlx

### Koleksi Model

- **Model Komuniti MLX**: https://huggingface.co/mlx-community
- **Model MLX Trending**: https://huggingface.co/models?library=mlx&sort=trending

### Aplikasi Contoh

1. **Pembantu AI Peribadi**: Bina chatbot tempatan dengan memori perbualan
2. **Pembantu Kod**: Cipta pembantu pengekodan untuk aliran kerja pembangunan anda
3. **Penjana Kandungan**: Bangunkan alat untuk penulisan, ringkasan, dan penciptaan kandungan
4. **Model yang Ditala Tersuai**: Sesuaikan model untuk tugas khusus domain
5. **Aplikasi Multi-modal**: Gabungkan penjanaan teks dengan keupayaan MLX lain

### Komuniti dan Pembelajaran

- **Perbincangan Komuniti MLX**: Isu dan Perbincangan GitHub
- **Forum Hugging Face**: Sokongan komuniti dan perkongsian model
- **Dokumentasi Pembangun Apple**: Sumber ML rasmi Apple

### Petikan

Jika anda menggunakan MLX dalam penyelidikan anda, sila petik:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Kesimpulan

Apple MLX telah merevolusikan landskap menjalankan model bahasa besar pada komputer Mac. Dengan menyediakan pengoptimuman Apple Silicon asli, integrasi Hugging Face yang lancar, dan ciri berkuasa seperti kuantisasi dan penalaan LoRA, MLX menjadikan ia mungkin untuk menjalankan model bahasa yang canggih secara tempatan dengan prestasi yang cemerlang.

Sama ada anda membina chatbot, pembantu kod, penjana kandungan, atau model yang ditala tersuai, MLX menyediakan alat dan prestasi yang diperlukan untuk memanfaatkan sepenuhnya Mac Apple Silicon anda untuk aplikasi model bahasa. Fokus rangka kerja ini pada kecekapan dan kemudahan penggunaan menjadikannya pilihan yang sangat baik untuk kedua-dua penyelidikan dan aplikasi pengeluaran.

Mulakan dengan contoh asas dalam tutorial ini, terokai ekosistem kaya model yang telah ditukar pada Hugging Face, dan secara beransur-ansur tingkatkan kepada ciri yang lebih maju seperti penalaan dan pembangunan model tersuai. Apabila ekosistem MLX terus berkembang, ia menjadi platform yang semakin berkuasa untuk pembangunan model bahasa pada perkakasan Apple.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.