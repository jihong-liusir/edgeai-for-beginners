<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T14:34:52+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ms"
}
-->
# Seksyen 4: OpenVINO Toolkit Optimization Suite

## Kandungan
1. [Pengenalan](../../../Module04)
2. [Apa itu OpenVINO?](../../../Module04)
3. [Pemasangan](../../../Module04)
4. [Panduan Permulaan Cepat](../../../Module04)
5. [Contoh: Menukar dan Mengoptimumkan Model dengan OpenVINO](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Amalan Terbaik](../../../Module04)
8. [Penyelesaian Masalah](../../../Module04)
9. [Sumber Tambahan](../../../Module04)

## Pengenalan

OpenVINO (Open Visual Inference and Neural Network Optimization) ialah toolkit sumber terbuka Intel untuk melaksanakan penyelesaian AI yang berprestasi tinggi di awan, premis, dan persekitaran edge. Sama ada anda menyasarkan CPU, GPU, VPU, atau pemecut AI khusus, OpenVINO menyediakan keupayaan pengoptimuman yang komprehensif sambil mengekalkan ketepatan model dan membolehkan pelaksanaan merentas platform.

## Apa itu OpenVINO?

OpenVINO ialah toolkit sumber terbuka yang membolehkan pembangun mengoptimumkan, menukar, dan melaksanakan model AI dengan cekap di pelbagai platform perkakasan. Ia terdiri daripada tiga komponen utama: OpenVINO Runtime untuk inferens, Neural Network Compression Framework (NNCF) untuk pengoptimuman model, dan OpenVINO Model Server untuk pelaksanaan yang boleh diskalakan.

### Ciri Utama

- **Pelaksanaan Merentas Platform**: Menyokong Linux, Windows, dan macOS dengan API Python, C++, dan C
- **Pemecutan Perkakasan**: Penemuan peranti automatik dan pengoptimuman untuk CPU, GPU, VPU, dan pemecut AI
- **Rangka Kerja Pemampatan Model**: Teknik kuantisasi, pemangkasan, dan pengoptimuman lanjutan melalui NNCF
- **Keserasian Rangka Kerja**: Sokongan langsung untuk model TensorFlow, ONNX, PaddlePaddle, dan PyTorch
- **Sokongan AI Generatif**: OpenVINO GenAI khusus untuk melaksanakan model bahasa besar dan aplikasi AI generatif

### Kelebihan

- **Pengoptimuman Prestasi**: Peningkatan kelajuan yang ketara dengan kehilangan ketepatan yang minimum
- **Jejak Pelaksanaan yang Dikurangkan**: Kebergantungan luaran yang minimum memudahkan pemasangan dan pelaksanaan
- **Masa Permulaan yang Dipertingkatkan**: Pemuatan dan caching model yang dioptimumkan untuk permulaan aplikasi yang lebih pantas
- **Pelaksanaan yang Boleh Diskalakan**: Dari peranti edge ke infrastruktur awan dengan API yang konsisten
- **Sedia untuk Pengeluaran**: Kebolehpercayaan peringkat perusahaan dengan dokumentasi yang komprehensif dan sokongan komuniti

## Pemasangan

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengurus pakej pip
- Persekitaran maya (disyorkan)
- Perkakasan yang serasi (CPU Intel disyorkan, tetapi menyokong pelbagai seni bina)

### Pemasangan Asas

Buat dan aktifkan persekitaran maya:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Pasang OpenVINO Runtime:

```bash
pip install openvino
```

Pasang NNCF untuk pengoptimuman model:

```bash
pip install nncf
```

### Pemasangan OpenVINO GenAI

Untuk aplikasi AI generatif:

```bash
pip install openvino-genai
```

### Kebergantungan Pilihan

Pakej tambahan untuk kes penggunaan tertentu:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Sahkan Pemasangan

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Jika berjaya, anda akan melihat maklumat versi OpenVINO.

## Panduan Permulaan Cepat

### Pengoptimuman Model Pertama Anda

Mari kita tukar dan optimumkan model Hugging Face menggunakan OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Apa yang Dilakukan oleh Proses Ini

Aliran kerja pengoptimuman melibatkan: memuatkan model asal dari Hugging Face, menukar kepada format Intermediate Representation (IR) OpenVINO, menerapkan pengoptimuman lalai, dan menyusun untuk perkakasan sasaran.

### Penjelasan Parameter Utama

- `export=True`: Menukar model kepada format IR OpenVINO
- `compile=False`: Menangguhkan penyusunan sehingga waktu larian untuk fleksibiliti
- `device`: Perkakasan sasaran ("CPU", "GPU", "AUTO" untuk pemilihan automatik)
- `save_pretrained()`: Menyimpan model yang dioptimumkan untuk kegunaan semula

## Contoh: Menukar dan Mengoptimumkan Model dengan OpenVINO

### Langkah 1: Penukaran Model dengan Kuantisasi NNCF

Berikut cara menerapkan kuantisasi selepas latihan menggunakan NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Langkah 2: Pengoptimuman Lanjutan dengan Pemampatan Berat

Untuk model berasaskan transformer, terapkan pemampatan berat:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Langkah 3: Inferens dengan Model yang Dioptimumkan

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struktur Output

Selepas pengoptimuman, direktori model anda akan mengandungi:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Penggunaan Lanjutan

### Konfigurasi dengan NNCF YAML

Untuk aliran kerja pengoptimuman yang kompleks, gunakan fail konfigurasi NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Terapkan konfigurasi:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Pengoptimuman GPU

Untuk pemecutan GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Pengoptimuman Pemprosesan Batch

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Pelaksanaan Model Server

Laksanakan model yang dioptimumkan dengan OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Kod klien untuk model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Amalan Terbaik

### 1. Pemilihan dan Penyediaan Model
- Gunakan model dari rangka kerja yang disokong (PyTorch, TensorFlow, ONNX)
- Pastikan input model mempunyai bentuk tetap atau dinamik yang diketahui
- Uji dengan set data representatif untuk penentukuran

### 2. Pemilihan Strategi Pengoptimuman
- **Kuantisasi Selepas Latihan**: Mulakan di sini untuk pengoptimuman cepat
- **Pemampatan Berat**: Sesuai untuk model bahasa besar dan transformer
- **Latihan Sedar Kuantisasi**: Gunakan apabila ketepatan adalah kritikal

### 3. Pengoptimuman Khusus Perkakasan
- **CPU**: Gunakan kuantisasi INT8 untuk prestasi seimbang
- **GPU**: Manfaatkan ketepatan FP16 dan pemprosesan batch
- **VPU**: Fokus pada penyederhanaan model dan penggabungan lapisan

### 4. Penalaan Prestasi
- **Mod Throughput**: Untuk pemprosesan batch volum tinggi
- **Mod Latensi**: Untuk aplikasi interaktif masa nyata
- **Peranti AUTO**: Biarkan OpenVINO memilih perkakasan yang optimum

### 5. Pengurusan Memori
- Gunakan bentuk dinamik dengan bijak untuk mengelakkan overhead memori
- Laksanakan caching model untuk pemuatan semula yang lebih pantas
- Pantau penggunaan memori semasa pengoptimuman

### 6. Pengesahan Ketepatan
- Sentiasa sahkan model yang dioptimumkan terhadap prestasi asal
- Gunakan set data ujian representatif untuk penilaian
- Pertimbangkan pengoptimuman secara beransur-ansur (mulakan dengan tetapan konservatif)

## Penyelesaian Masalah

### Masalah Biasa

#### 1. Masalah Pemasangan
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Ralat Penukaran Model
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Masalah Prestasi
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Masalah Memori
- Kurangkan saiz batch model semasa pengoptimuman
- Gunakan penstriman untuk set data besar
- Aktifkan caching model: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Penurunan Ketepatan
- Gunakan ketepatan lebih tinggi (INT8 berbanding INT4)
- Tingkatkan saiz set data penentukuran
- Terapkan pengoptimuman ketepatan campuran

### Pemantauan Prestasi

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Mendapatkan Bantuan

- **Dokumentasi**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Isu GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum Komuniti**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Sumber Tambahan

### Pautan Rasmi
- **Laman Utama OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Repositori GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Repositori NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Sumber Pembelajaran
- **Notebook OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Panduan Permulaan Cepat**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Panduan Pengoptimuman**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Alat Integrasi
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Penanda Aras Prestasi
- **Penanda Aras Rasmi**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Contoh Komuniti
- **Notebook Jupyter**: [Repositori Notebook OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Tutorial komprehensif tersedia dalam repositori notebook OpenVINO
- **Aplikasi Contoh**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Contoh dunia sebenar untuk pelbagai domain (penglihatan komputer, NLP, audio)
- **Catatan Blog**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Catatan blog Intel AI dan komuniti dengan kes penggunaan terperinci

### Alat Berkaitan
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Teknik pengoptimuman tambahan untuk perkakasan Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Untuk perbandingan pelaksanaan mudah alih dan edge
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternatif enjin inferens merentas platform

## ➡️ Apa yang seterusnya

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat penting, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.