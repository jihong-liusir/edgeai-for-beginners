<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T14:42:35+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "ms"
}
-->
# Seksyen 2: Panduan Pelaksanaan Llama.cpp

## Kandungan
1. [Pengenalan](../../../Module04)
2. [Apa itu Llama.cpp?](../../../Module04)
3. [Pemasangan](../../../Module04)
4. [Membina dari Sumber](../../../Module04)
5. [Kuantisasi Model](../../../Module04)
6. [Penggunaan Asas](../../../Module04)
7. [Ciri Lanjutan](../../../Module04)
8. [Integrasi Python](../../../Module04)
9. [Penyelesaian Masalah](../../../Module04)
10. [Amalan Terbaik](../../../Module04)

## Pengenalan

Tutorial komprehensif ini akan membimbing anda melalui semua yang perlu anda ketahui tentang Llama.cpp, daripada pemasangan asas hingga senario penggunaan lanjutan. Llama.cpp adalah pelaksanaan C++ yang berkuasa yang membolehkan inferens Model Bahasa Besar (LLM) dengan persediaan minimum dan prestasi cemerlang merentasi pelbagai konfigurasi perkakasan.

## Apa itu Llama.cpp?

Llama.cpp adalah rangka kerja inferens LLM yang ditulis dalam C/C++ yang membolehkan model bahasa besar dijalankan secara tempatan dengan persediaan minimum dan prestasi terkini pada pelbagai perkakasan. Ciri utama termasuk:

### Ciri Teras
- **Pelaksanaan C/C++ tulen** tanpa pergantungan
- **Keserasian merentas platform** (Windows, macOS, Linux)
- **Pengoptimuman perkakasan** untuk pelbagai seni bina
- **Sokongan kuantisasi** (1.5-bit hingga 8-bit integer quantization)
- **Pecutan CPU dan GPU**
- **Kecekapan memori** untuk persekitaran terhad

### Kelebihan
- Berfungsi dengan cekap pada CPU tanpa memerlukan perkakasan khusus
- Menyokong pelbagai backend GPU (CUDA, Metal, OpenCL, Vulkan)
- Ringan dan mudah alih
- Apple silicon adalah keutamaan - dioptimumkan melalui ARM NEON, Accelerate dan Metal frameworks
- Menyokong pelbagai tahap kuantisasi untuk penggunaan memori yang lebih rendah

## Pemasangan

### Kaedah 1: Binari Pra-bina (Disyorkan untuk Pemula)

#### Muat Turun dari GitHub Releases
1. Lawati [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Muat turun binari yang sesuai untuk sistem anda:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` untuk Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` untuk macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` untuk Linux

3. Ekstrak arkib dan tambahkan direktori ke PATH sistem anda

#### Menggunakan Pengurus Pakej

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Pelbagai distribusi):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Kaedah 2: Pakej Python (llama-cpp-python)

#### Pemasangan Asas
```bash
pip install llama-cpp-python
```

#### Dengan Pecutan Perkakasan
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Membina dari Sumber

### Prasyarat

**Keperluan Sistem:**
- Pengkompil C++ (GCC, Clang, atau MSVC)
- CMake (versi 3.14 atau lebih tinggi)
- Git
- Alat binaan untuk platform anda

**Memasang Prasyarat:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Pasang Visual Studio 2022 dengan alat pembangunan C++
- Pasang CMake dari laman web rasmi
- Pasang Git

### Proses Binaan Asas

1. **Klon repositori:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurasi binaan:**
```bash
cmake -B build
```

3. **Bina projek:**
```bash
cmake --build build --config Release
```

Untuk kompilasi lebih pantas, gunakan pekerjaan selari:
```bash
cmake --build build --config Release -j 8
```

### Binaan Khusus Perkakasan

#### Sokongan CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Sokongan Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Sokongan OpenBLAS (Pengoptimuman CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Sokongan Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Pilihan Binaan Lanjutan

#### Binaan Debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Dengan Ciri Tambahan
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kuantisasi Model

### Memahami Format GGUF

GGUF (Generalized GGML Unified Format) adalah format fail yang dioptimumkan untuk menjalankan model bahasa besar dengan cekap menggunakan Llama.cpp dan rangka kerja lain. Ia menyediakan:

- Penyimpanan berat model yang standard
- Keserasian yang lebih baik merentasi platform
- Prestasi yang dipertingkatkan
- Pengendalian metadata yang cekap

### Jenis Kuantisasi

Llama.cpp menyokong pelbagai tahap kuantisasi:

| Jenis | Bit | Penerangan | Kes Penggunaan |
|-------|-----|------------|----------------|
| F16 | 16 | Ketepatan separuh | Kualiti tinggi, memori besar |
| Q8_0 | 8 | Kuantisasi 8-bit | Keseimbangan baik |
| Q4_0 | 4 | Kuantisasi 4-bit | Kualiti sederhana, saiz lebih kecil |
| Q2_K | 2 | Kuantisasi 2-bit | Saiz paling kecil, kualiti lebih rendah |

### Menukar Model

#### Dari PyTorch ke GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Muat Turun Langsung dari Hugging Face
Banyak model tersedia dalam format GGUF di Hugging Face:
- Cari model dengan "GGUF" dalam nama
- Muat turun tahap kuantisasi yang sesuai
- Gunakan secara langsung dengan llama.cpp

## Penggunaan Asas

### Antara Muka Baris Perintah

#### Penjanaan Teks Mudah
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Menggunakan Model dari Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Mod Pelayan
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Parameter Biasa

| Parameter | Penerangan | Contoh |
|-----------|------------|--------|
| `-m` | Laluan fail model | `-m model.gguf` |
| `-p` | Teks prompt | `-p "Hello world"` |
| `-n` | Bilangan token untuk dijana | `-n 100` |
| `-c` | Saiz konteks | `-c 4096` |
| `-t` | Bilangan benang | `-t 8` |
| `-ngl` | Lapisan GPU | `-ngl 32` |
| `-temp` | Suhu | `-temp 0.7` |

### Mod Interaktif

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Ciri Lanjutan

### API Pelayan

#### Memulakan Pelayan
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Penggunaan API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Pengoptimuman Prestasi

#### Pengurusan Memori
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multi-threading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Pecutan GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integrasi Python

### Penggunaan Asas dengan llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Antara Muka Chat

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Respons Penstriman

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integrasi dengan LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Penyelesaian Masalah

### Isu dan Penyelesaian Biasa

#### Kesalahan Binaan

**Isu: CMake tidak dijumpai**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Isu: Pengkompil tidak dijumpai**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Isu Semasa Runtime

**Isu: Pemuatan model gagal**
- Sahkan laluan fail model
- Periksa kebenaran fail
- Pastikan RAM mencukupi
- Cuba tahap kuantisasi yang berbeza

**Isu: Prestasi buruk**
- Aktifkan pecutan perkakasan
- Tingkatkan bilangan benang
- Gunakan kuantisasi yang sesuai
- Periksa penggunaan memori GPU

#### Isu Memori

**Isu: Kekurangan memori**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Isu Khusus Platform

#### Windows
- Gunakan pengkompil MinGW atau Visual Studio
- Pastikan konfigurasi PATH yang betul
- Periksa gangguan antivirus

#### macOS
- Aktifkan Metal untuk Apple Silicon
- Gunakan Rosetta 2 untuk keserasian jika diperlukan
- Periksa alat baris perintah Xcode

#### Linux
- Pasang pakej pembangunan
- Periksa versi pemacu GPU
- Sahkan pemasangan toolkit CUDA

## Amalan Terbaik

### Pemilihan Model
1. **Pilih kuantisasi yang sesuai** berdasarkan perkakasan anda
2. **Pertimbangkan saiz model** vs. kompromi kualiti
3. **Uji model yang berbeza** untuk kes penggunaan spesifik anda

### Pengoptimuman Prestasi
1. **Gunakan pecutan GPU** jika tersedia
2. **Optimumkan bilangan benang** untuk CPU anda
3. **Tetapkan saiz konteks yang sesuai** untuk kes penggunaan anda
4. **Aktifkan pemetaan memori** untuk model besar

### Penggunaan dalam Pengeluaran
1. **Gunakan mod pelayan** untuk akses API
2. **Laksanakan pengendalian kesalahan yang betul**
3. **Pantau penggunaan sumber**
4. **Tetapkan logging dan pemantauan**

### Aliran Kerja Pembangunan
1. **Mulakan dengan model yang lebih kecil** untuk ujian
2. **Gunakan kawalan versi** untuk konfigurasi model
3. **Dokumentasikan konfigurasi anda**
4. **Uji merentasi platform yang berbeza**

### Pertimbangan Keselamatan
1. **Sahkan prompt input**
2. **Laksanakan had kadar**
3. **Amankan titik akhir API**
4. **Pantau corak penyalahgunaan**

## Kesimpulan

Llama.cpp menyediakan cara yang berkuasa dan cekap untuk menjalankan model bahasa besar secara tempatan merentasi pelbagai konfigurasi perkakasan. Sama ada anda sedang membangunkan aplikasi AI, menjalankan penyelidikan, atau sekadar bereksperimen dengan LLM, rangka kerja ini menawarkan fleksibiliti dan prestasi yang diperlukan untuk pelbagai kes penggunaan.

Perkara utama:
- Pilih kaedah pemasangan yang paling sesuai dengan keperluan anda
- Optimumkan untuk konfigurasi perkakasan spesifik anda
- Mulakan dengan penggunaan asas dan secara beransur-ansur terokai ciri lanjutan
- Pertimbangkan menggunakan pengikatan Python untuk integrasi yang lebih mudah
- Ikuti amalan terbaik untuk penggunaan dalam pengeluaran

Untuk maklumat lanjut dan kemas kini, lawati [repositori rasmi Llama.cpp](https://github.com/ggml-org/llama.cpp) dan rujuk dokumentasi komprehensif serta sumber komuniti yang tersedia.

## ➡️ Apa yang seterusnya

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.