<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T22:27:36+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ms"
}
-->
# Seksyen 3: Microsoft Olive Optimization Suite

## Kandungan
1. [Pengenalan](../../../Module04)
2. [Apa itu Microsoft Olive?](../../../Module04)
3. [Pemasangan](../../../Module04)
4. [Panduan Permulaan Cepat](../../../Module04)
5. [Contoh: Menukar Qwen3 kepada ONNX INT4](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Amalan Terbaik](../../../Module04)
8. [Penyelesaian Masalah](../../../Module04)
9. [Sumber Tambahan](../../../Module04)

## Pengenalan

Microsoft Olive ialah alat pengoptimuman model yang berkuasa dan mudah digunakan, yang peka terhadap perkakasan. Ia memudahkan proses pengoptimuman model pembelajaran mesin untuk digunakan pada pelbagai platform perkakasan. Sama ada anda menyasarkan CPU, GPU, atau pemecut AI khusus, Olive membantu anda mencapai prestasi optimum sambil mengekalkan ketepatan model.

## Apa itu Microsoft Olive?

Olive ialah alat pengoptimuman model yang peka terhadap perkakasan dan mudah digunakan, yang menggabungkan teknik-teknik terkemuka dalam industri seperti pemampatan model, pengoptimuman, dan penyusunan. Ia berfungsi dengan ONNX Runtime sebagai penyelesaian pengoptimuman inferens hujung-ke-hujung.

### Ciri Utama

- **Pengoptimuman Peka Perkakasan**: Secara automatik memilih teknik pengoptimuman terbaik untuk perkakasan sasaran anda
- **40+ Komponen Pengoptimuman Terbina**: Meliputi pemampatan model, kuantisasi, pengoptimuman graf, dan banyak lagi
- **Antara Muka CLI Mudah**: Perintah mudah untuk tugas pengoptimuman biasa
- **Sokongan Pelbagai Rangka Kerja**: Berfungsi dengan PyTorch, model Hugging Face, dan ONNX
- **Sokongan Model Popular**: Olive boleh mengoptimumkan seni bina model popular seperti Llama, Phi, Qwen, Gemma, dan lain-lain secara automatik

### Kelebihan

- **Pengurangan Masa Pembangunan**: Tidak perlu bereksperimen secara manual dengan teknik pengoptimuman yang berbeza
- **Peningkatan Prestasi**: Peningkatan kelajuan yang ketara (sehingga 6x dalam beberapa kes)
- **Penggunaan Merentas Platform**: Model yang dioptimumkan berfungsi pada pelbagai perkakasan dan sistem operasi
- **Ketepatan Terpelihara**: Pengoptimuman mengekalkan kualiti model sambil meningkatkan prestasi

## Pemasangan

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengurus pakej pip
- Persekitaran maya (disyorkan)

### Pemasangan Asas

Buat dan aktifkan persekitaran maya:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Pasang Olive dengan ciri pengoptimuman automatik:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Kebergantungan Pilihan

Olive menawarkan pelbagai kebergantungan pilihan untuk ciri tambahan:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Sahkan Pemasangan

```bash
olive --help
```

Jika berjaya, anda akan melihat mesej bantuan CLI Olive.

## Panduan Permulaan Cepat

### Pengoptimuman Pertama Anda

Mari kita optimakan model bahasa kecil menggunakan ciri pengoptimuman automatik Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Apa yang Dilakukan oleh Perintah Ini

Proses pengoptimuman melibatkan: mendapatkan model dari cache tempatan, menangkap Graf ONNX dan menyimpan berat dalam fail data ONNX, mengoptimumkan Graf ONNX, dan mengkuantisasi model kepada int4 menggunakan kaedah RTN.

### Penjelasan Parameter Perintah

- `--model_name_or_path`: Pengenal model Hugging Face atau laluan tempatan
- `--output_path`: Direktori di mana model yang dioptimumkan akan disimpan
- `--device`: Peranti sasaran (cpu, gpu)
- `--provider`: Penyedia pelaksanaan (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gunakan ONNX Runtime Generate AI untuk inferens
- `--precision`: Ketepatan kuantisasi (int4, int8, fp16)
- `--log_level`: Verbositi log (0=minimal, 1=terperinci)

## Contoh: Menukar Qwen3 kepada ONNX INT4

Berdasarkan contoh Hugging Face yang disediakan di [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), berikut adalah cara untuk mengoptimumkan model Qwen3:

### Langkah 1: Muat Turun Model (Pilihan)

Untuk meminimumkan masa muat turun, cache hanya fail penting:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Langkah 2: Optimumkan Model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Langkah 3: Uji Model yang Dioptimumkan

Buat skrip Python ringkas untuk menguji model yang dioptimumkan:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur Output

Selepas pengoptimuman, direktori output anda akan mengandungi:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Penggunaan Lanjutan

### Fail Konfigurasi

Untuk aliran kerja pengoptimuman yang lebih kompleks, anda boleh menggunakan fail konfigurasi JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Jalankan dengan konfigurasi:

```bash
olive run --config config.json
```

### Pengoptimuman GPU

Untuk pengoptimuman GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Untuk DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Penalaan Model dengan Olive

Olive juga menyokong penalaan model:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Amalan Terbaik

### 1. Pemilihan Model
- Mulakan dengan model yang lebih kecil untuk ujian (contoh: parameter 0.5B-7B)
- Pastikan seni bina model sasaran anda disokong oleh Olive

### 2. Pertimbangan Perkakasan
- Padankan sasaran pengoptimuman anda dengan perkakasan penggunaan
- Gunakan pengoptimuman GPU jika anda mempunyai perkakasan yang serasi dengan CUDA
- Pertimbangkan DirectML untuk mesin Windows dengan grafik bersepadu

### 3. Pemilihan Ketepatan
- **INT4**: Pemampatan maksimum, sedikit kehilangan ketepatan
- **INT8**: Keseimbangan baik antara saiz dan ketepatan
- **FP16**: Kehilangan ketepatan minimum, pengurangan saiz sederhana

### 4. Ujian dan Pengesahan
- Sentiasa uji model yang dioptimumkan dengan kes penggunaan spesifik anda
- Bandingkan metrik prestasi (latensi, throughput, ketepatan)
- Gunakan data input yang mewakili untuk penilaian

### 5. Pengoptimuman Berulang
- Mulakan dengan pengoptimuman automatik untuk hasil cepat
- Gunakan fail konfigurasi untuk kawalan yang lebih terperinci
- Bereksperimen dengan laluan pengoptimuman yang berbeza

## Penyelesaian Masalah

### Masalah Biasa

#### 1. Masalah Pemasangan
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masalah CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masalah Memori
- Gunakan saiz batch yang lebih kecil semasa pengoptimuman
- Cuba kuantisasi dengan ketepatan yang lebih tinggi dahulu (int8 berbanding int4)
- Pastikan ruang cakera mencukupi untuk cache model

#### 4. Ralat Pemuatan Model
- Sahkan laluan model dan kebenaran akses
- Periksa jika model memerlukan `trust_remote_code=True`
- Pastikan semua fail model yang diperlukan telah dimuat turun

### Mendapatkan Bantuan

- **Dokumentasi**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Isu GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Contoh**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Sumber Tambahan

### Pautan Rasmi
- **Repositori GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Dokumentasi ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Contoh Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Contoh Komuniti
- **Jupyter Notebooks**: Tersedia dalam repositori GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Sambungan VS Code**: Gambaran keseluruhan AI Toolkit untuk VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Catatan Blog**: Blog Sumber Terbuka Microsoft — https://opensource.microsoft.com/blog/

### Alat Berkaitan
- **ONNX Runtime**: Enjin inferens berprestasi tinggi — https://onnxruntime.ai/
- **Hugging Face Transformers**: Sumber banyak model yang serasi — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Aliran kerja pengoptimuman berasaskan awan — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Apa yang seterusnya

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

