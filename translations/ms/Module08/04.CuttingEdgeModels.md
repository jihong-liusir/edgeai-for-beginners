<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T22:39:44+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ms"
}
-->
# Sesi 4: Model Terkini – LLM, SLM, dan Inferens Peranti

## Gambaran Keseluruhan

Bandingkan LLM dan SLM, nilai pertukaran antara inferens tempatan dan awan, serta laksanakan demo yang mempamerkan senario EdgeAI menggunakan Phi dan ONNX Runtime. Kami juga akan menekankan Chainlit RAG, pilihan inferens WebGPU, dan integrasi Open WebUI.

Rujukan:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Panduan Open WebUI (aplikasi sembang dengan Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Objektif Pembelajaran
- Fahami pertukaran antara LLM dan SLM dari segi kos, kependaman, dan ketepatan
- Pilih antara inferens tempatan dan awan berdasarkan keperluan perniagaan tertentu
- Laksanakan demo RAG kecil menggunakan Chainlit
- Terokai WebGPU untuk pecutan di sisi pelayar
- Sambungkan Open WebUI ke Foundry Local

## Bahagian 1: LLM vs SLM – Matriks Keputusan

Pertimbangkan:
- Kependaman: SLM pada peranti sering memberikan respons kurang dari satu saat
- Kos: inferens tempatan mengurangkan kos awan
- Privasi: data sensitif kekal pada peranti
- Keupayaan: LLM mungkin mengatasi SLM dalam tugas yang kompleks
- Kebolehpercayaan: strategi hibrid mengurangkan risiko masa henti

## Bahagian 2: Tempatan vs Awan – Corak Hibrid

- Tempatan dahulu dengan sandaran awan untuk arahan besar/kompleks
- Awan dahulu dengan tempatan untuk senario sensitif privasi atau luar talian
- Laluan berdasarkan jenis tugas (penjanaan kod ke DeepSeek, sembang umum ke Phi/Qwen)

## Bahagian 3: Aplikasi Sembang RAG dengan Chainlit (Minimal)

Pasang kebergantungan:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Jalankan:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Kembangkan: tambahkan pengambil mudah (fail tempatan) dan tambahkan konteks yang diambil ke arahan pengguna.

## Bahagian 4: Inferens WebGPU (Peringatan)

Jalankan model kecil terus dalam pelayar menggunakan WebGPU. Ini sesuai untuk demo yang mengutamakan privasi dan pengalaman tanpa pemasangan. Di bawah adalah contoh langkah demi langkah menggunakan ONNX Runtime Web dengan penyedia eksekusi WebGPU.

1) Periksa sokongan WebGPU
- Pelayar Chromium: chrome://gpu → sahkan “WebGPU” diaktifkan
- Pemeriksaan programatik (kami juga akan periksa dalam kod): `if (!('gpu' in navigator)) { /* tiada WebGPU */ }`

2) Buat projek minimal
Buat folder dan dua fail: `index.html` dan `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Hidangkan secara tempatan (Windows cmd.exe)
Gunakan pelayan statik mudah supaya pelayar boleh mengambil model.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Buka http://localhost:5173 dalam pelayar anda. Anda sepatutnya melihat log inisialisasi, penciptaan sesi dengan WebGPU, dan ramalan argmax.

4) Penyelesaian masalah
- Jika WebGPU tidak tersedia: kemas kini Chrome/Edge dan pastikan pemacu GPU terkini, kemudian periksa chrome://flags untuk “Enable WebGPU”.
- Jika berlaku ralat CORS atau fetch: pastikan anda menghidangkan fail melalui http:// (bukan file://) dan URL model membenarkan permintaan rentas asal.
- Sandaran ke CPU: tukar `executionProviders: ['wasm']` untuk mengesahkan tingkah laku asas.

5) Langkah seterusnya
- Tukar kepada model ONNX khusus domain (contohnya, klasifikasi imej atau model teks kecil).
- Tambahkan logik prapemprosesan/pascapemprosesan untuk input sebenar.
- Untuk model yang lebih besar atau kependaman pengeluaran, pilih Foundry Local atau ONNX Runtime Server.

## Bahagian 5: Open WebUI + Foundry Local (Langkah demi langkah)

Ini menyambungkan Open WebUI ke titik akhir Foundry Local yang serasi dengan OpenAI untuk UI sembang tempatan.

1) Prasyarat
- Foundry Local dipasang dan berfungsi (`foundry --version`)
- Satu model sedia untuk dijalankan secara tempatan (contohnya, `phi-4-mini`)
- Docker Desktop dipasang (disyorkan untuk Open WebUI)

2) Mulakan model dengan Foundry Local
```powershell
foundry model run phi-4-mini
```
Ini mendedahkan API serasi OpenAI di `http://localhost:8000`.

3) Mulakan Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Nota:
- Pada Windows, `host.docker.internal` membolehkan kontena mencapai hos anda di `localhost`.
- Kami menetapkan `OPENAI_API_BASE_URL` ke titik akhir Foundry Local dan `OPENAI_API_KEY` palsu.

4) Konfigurasi dari UI Open WebUI (alternatif)
- Layari ke http://localhost:3000
- Lengkapkan persediaan awal (pengguna admin)
- Pergi ke Tetapan → Model/Penyedia
- Tetapkan Base URL: `http://host.docker.internal:8000/v1`
- Tetapkan API Key: `local-key` (placeholder)
- Simpan

5) Jalankan arahan ujian
- Dalam sembang Open WebUI, pilih atau masukkan nama model `phi-4-mini`
- Arahan: “Senaraikan lima manfaat inferens AI pada peranti.”
- Anda sepatutnya melihat respons yang disiarkan dari model tempatan anda

6) Penyelesaian masalah
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Pilihan: Kekalkan data Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Senarai Semak Praktikal
- [ ] Bandingkan respons/kependaman antara SLM dan LLM secara tempatan
- [ ] Jalankan demo Chainlit terhadap sekurang-kurangnya dua model
- [ ] Sambungkan Open WebUI ke titik akhir tempatan anda dan uji

## Langkah Seterusnya
- Bersedia untuk aliran kerja ejen dalam Sesi 5
- Kenal pasti senario di mana hibrid tempatan/awan meningkatkan ROI

---

