<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T00:47:53+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ms"
}
-->
# Sesi 4: Membina Aplikasi Chat Produksi dengan Chainlit

## Gambaran Keseluruhan

Sesi ini memberi fokus kepada pembangunan aplikasi chat yang sedia untuk produksi menggunakan Chainlit dan Microsoft Foundry Local. Anda akan belajar cara mencipta antara muka web moden untuk perbualan AI, melaksanakan respons penstriman, dan menyebarkan aplikasi chat yang kukuh dengan pengendalian ralat yang betul serta reka bentuk pengalaman pengguna.

**Apa yang Akan Anda Bina:**
- **Aplikasi Chat Chainlit**: Antara muka web moden dengan respons penstriman
- **Demo WebGPU**: Inferens berasaskan pelayar untuk aplikasi yang mengutamakan privasi  
- **Integrasi Open WebUI**: Antara muka chat profesional dengan Foundry Local
- **Corak Produksi**: Pengendalian ralat, pemantauan, dan strategi penyebaran

## Objektif Pembelajaran

- Membina aplikasi chat yang sedia untuk produksi dengan Chainlit
- Melaksanakan respons penstriman untuk meningkatkan pengalaman pengguna
- Menguasai corak integrasi SDK Foundry Local
- Mengaplikasikan pengendalian ralat yang betul dan degradasi yang lancar
- Menyebarkan dan mengkonfigurasi aplikasi chat untuk pelbagai persekitaran
- Memahami corak UI web moden untuk AI perbualan

## Prasyarat

- **Foundry Local**: Dipasang dan berjalan ([Panduan Pemasangan](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Versi 3.10 atau lebih baru dengan keupayaan persekitaran maya
- **Model**: Sekurang-kurangnya satu model dimuatkan (`foundry model run phi-4-mini`)
- **Pelayar**: Pelayar web moden dengan sokongan WebGPU (Chrome/Edge)
- **Docker**: Untuk integrasi Open WebUI (pilihan)

## Bahagian 1: Memahami Aplikasi Chat Moden

### Gambaran Keseluruhan Seni Bina

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Teknologi Utama

**Corak SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Pengurusan perkhidmatan automatik
- `manager.endpoint` dan `manager.api_key`: Butiran sambungan
- `manager.get_model_info(alias).id`: Pengenalan model

**Rangka Kerja Chainlit:**
- `@cl.on_chat_start`: Memulakan sesi chat
- `@cl.on_message`: Mengendalikan mesej pengguna yang masuk  
- `cl.Message().stream_token()`: Penstriman masa nyata
- Penjanaan UI automatik dan pengurusan WebSocket

## Bahagian 2: Matriks Keputusan Tempatan vs Awan

### Ciri Prestasi

| Aspek | Tempatan (Foundry) | Awan (Azure OpenAI) |
|-------|--------------------|---------------------|
| **Kelewatan** | 🚀 50-200ms (tanpa rangkaian) | ⏱️ 200-2000ms (bergantung pada rangkaian) |
| **Privasi** | 🔒 Data tidak pernah meninggalkan peranti | ⚠️ Data dihantar ke awan |
| **Kos** | 💰 Percuma selepas perkakasan | 💸 Bayar per token |
| **Luar Talian** | ✅ Berfungsi tanpa internet | ❌ Memerlukan internet |
| **Saiz Model** | ⚠️ Terhad oleh perkakasan | ✅ Akses kepada model terbesar |
| **Penskalaan** | ⚠️ Bergantung pada perkakasan | ✅ Penskalaan tanpa had |

### Corak Strategi Hibrid

**Tempatan-Dulu dengan Sandaran:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Penghalaan Berdasarkan Tugas:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Bahagian 3: Sampel 04 - Aplikasi Chat Chainlit

### Permulaan Pantas

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Aplikasi ini akan dibuka secara automatik di `http://localhost:8080` dengan antara muka chat moden.

### Pelaksanaan Teras

Aplikasi Sampel 04 menunjukkan corak sedia untuk produksi:

**Penemuan Perkhidmatan Automatik:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Pengendali Chat Penstriman:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Pilihan Konfigurasi

**Pembolehubah Persekitaran:**

| Pembolehubah | Penerangan | Lalai | Contoh |
|--------------|------------|-------|--------|
| `MODEL` | Alias model untuk digunakan | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Titik akhir Foundry Local | Dikesan secara automatik | `http://localhost:51211` |
| `API_KEY` | Kunci API (pilihan untuk tempatan) | `""` | `your-api-key` |

**Penggunaan Lanjutan:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Bahagian 4: Mencipta dan Menggunakan Jupyter Notebooks

### Gambaran Keseluruhan Sokongan Notebook

Sampel 04 termasuk notebook Jupyter yang komprehensif (`chainlit_app.ipynb`) yang menyediakan:

- **📚 Kandungan Pendidikan**: Bahan pembelajaran langkah demi langkah
- **🔬 Eksplorasi Interaktif**: Jalankan dan eksperimen dengan sel kod
- **📊 Demonstrasi Visual**: Carta, diagram, dan visualisasi output
- **🛠️ Alat Pembangunan**: Keupayaan ujian dan debugging

### Mencipta Notebook Anda Sendiri

#### Langkah 1: Sediakan Persekitaran Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Langkah 2: Cipta Notebook Baru

**Menggunakan VS Code:**
1. Buka VS Code dalam direktori Module08
2. Cipta fail baru dengan sambungan `.ipynb`
3. Pilih kernel "Foundry Local" apabila diminta
4. Mulakan dengan menambah sel dengan kandungan anda

**Menggunakan Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Amalan Terbaik Struktur Notebook

#### Pengorganisasian Sel

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Contoh Interaktif dan Latihan

#### Latihan 1: Ujian Konfigurasi Klien

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Latihan 2: Simulasi Respons Penstriman

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Bahagian 5: Demo Inferens Pelayar WebGPU

### Gambaran Keseluruhan

WebGPU membolehkan model AI dijalankan terus dalam pelayar untuk privasi maksimum dan pengalaman tanpa pemasangan. Sampel ini menunjukkan pelaksanaan ONNX Runtime Web dengan eksekusi WebGPU.

### Langkah 1: Periksa Sokongan WebGPU

**Keperluan Pelayar:**
- Chrome/Edge 113+ dengan WebGPU diaktifkan
- Periksa: `chrome://gpu` → sahkan status "WebGPU"
- Periksa secara programatik: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Langkah 2: Cipta Demo WebGPU

Cipta direktori: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Langkah 3: Jalankan Demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Bahagian 6: Integrasi Open WebUI

### Gambaran Keseluruhan

Open WebUI menyediakan antara muka profesional seperti ChatGPT yang berhubung dengan API Foundry Local yang serasi dengan OpenAI.

### Langkah 1: Prasyarat

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Langkah 2: Persediaan Docker (Disyorkan)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Nota:** `host.docker.internal` membolehkan kontena Docker mengakses mesin hos pada Windows.

### Langkah 3: Konfigurasi

1. **Buka Pelayar:** Navigasi ke `http://localhost:3000`
2. **Persediaan Awal:** Cipta akaun admin
3. **Konfigurasi Model:**
   - Tetapan → Model → API OpenAI  
   - URL Asas: `http://host.docker.internal:51211/v1`
   - Kunci API: `foundry-local-key` (sebarang nilai boleh digunakan)
4. **Ujian Sambungan:** Model sepatutnya muncul dalam senarai dropdown

### Penyelesaian Masalah

**Isu Biasa:**

1. **Sambungan Ditolak:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Model Tidak Muncul:**
   - Sahkan model dimuatkan: `foundry model list`
   - Periksa respons API: `curl http://localhost:51211/v1/models`
   - Mulakan semula kontena Open WebUI

## Bahagian 7: Pertimbangan Penyebaran Produksi

### Konfigurasi Persekitaran

**Persediaan Pembangunan:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Penyebaran Produksi:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Isu Port Biasa dan Penyelesaian

**Pencegahan Konflik Port 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Pemantauan Prestasi

**Pelaksanaan Pemeriksaan Kesihatan:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Ringkasan

Sesi 4 merangkumi pembangunan aplikasi Chainlit yang sedia untuk produksi bagi AI perbualan. Anda telah mempelajari tentang:

- ✅ **Rangka Kerja Chainlit**: UI moden dan sokongan penstriman untuk aplikasi chat
- ✅ **Integrasi Foundry Local**: Penggunaan SDK dan corak konfigurasi  
- ✅ **Inferens WebGPU**: AI berasaskan pelayar untuk privasi maksimum
- ✅ **Persediaan Open WebUI**: Penyebaran antara muka chat profesional
- ✅ **Corak Produksi**: Pengendalian ralat, pemantauan, dan penskalaan

Aplikasi Sampel 04 menunjukkan amalan terbaik untuk membina antara muka chat yang kukuh yang memanfaatkan model AI tempatan melalui Microsoft Foundry Local sambil menyediakan pengalaman pengguna yang cemerlang.

## Rujukan

- **[Sampel 04: Aplikasi Chainlit](samples/04/README.md)**: Aplikasi lengkap dengan dokumentasi
- **[Notebook Pendidikan Chainlit](samples/04/chainlit_app.ipynb)**: Bahan pembelajaran interaktif
- **[Dokumentasi Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Dokumentasi platform lengkap
- **[Dokumentasi Chainlit](https://docs.chainlit.io/)**: Dokumentasi rasmi rangka kerja
- **[Panduan Integrasi Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Tutorial rasmi

---

