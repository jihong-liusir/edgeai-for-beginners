<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T22:39:02+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "ms"
}
-->
# Sesi 6: Foundry Local â€“ Model Sebagai Alat

## Gambaran Keseluruhan

Anggap model AI sebagai alat modular yang boleh disesuaikan dan dijalankan terus pada peranti dengan Foundry Local. Sesi ini menekankan aliran kerja praktikal untuk inferens yang melindungi privasi dan berlatensi rendah serta cara mengintegrasikan alat ini melalui SDK, API, atau CLI. Anda juga akan belajar cara meningkatkan skala ke Azure AI Foundry apabila diperlukan.

Rujukan:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integrasi dengan SDK inferens: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Kompilasi model Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Objektif Pembelajaran
- Reka corak model-sebagai-alat pada peranti
- Integrasi melalui REST API atau SDK yang serasi dengan OpenAI
- Sesuaikan model untuk kes penggunaan khusus domain
- Rancang untuk peningkatan skala hibrid ke Azure AI Foundry

## Bahagian 1: Abstraksi Alat (Langkah demi langkah)

Matlamat: Wakili model sebagai alat dengan kontrak yang jelas dan router yang ringkas.

Langkah 1) Tentukan antara muka alat dan daftar
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```

Langkah 2) Laksanakan dua alat yang disokong oleh Foundry Local
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```

Langkah 3) Router berdasarkan tugas
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```

## Bahagian 2: Integrasi SDK dan API (Langkah demi langkah)

Matlamat: Gunakan OpenAI Python SDK terhadap endpoint Foundry Local.

Langkah 1) Pasang
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```

Langkah 2) Konfigurasi pembolehubah persekitaran
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```

Langkah 3) Panggil API chat
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```

## Bahagian 3: Penyesuaian Domain (Langkah demi langkah)

Matlamat: Sesuaikan output untuk domain menggunakan templat prompt dan skema JSON.

Langkah 1) Cipta templat prompt domain
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```

Langkah 2) Paksa output JSON
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```

## Bahagian 4: Mod Luar Talian dan Postur Keselamatan (Langkah demi langkah)

Matlamat: Pastikan privasi dan ketahanan apabila menjalankan model sebagai alat secara tempatan.

Langkah 1) Panaskan dan sahkan endpoint tempatan
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```

Langkah 2) Sanitasi input
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

Langkah 3) Bendera tempatan sahaja dan log
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```

## Bahagian 5: Peningkatan Skala ke Azure AI Foundry (Langkah demi langkah)

Matlamat: Cerminkan model tempatan dengan endpoint Azure untuk kapasiti tambahan.

Langkah 1) Tentukan strategi routing
- Tempatan dahulu untuk privasi/latensi, fallback Azure pada ralat atau prompt besar

Langkah 2) Laksanakan stub router yang ringkas
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```

## Senarai Semak Hands-On
- [ ] Daftar sekurang-kurangnya dua alat dan route permintaan
- [ ] Panggil Foundry Local melalui OpenAI SDK dan REST mentah
- [ ] Paksa output JSON untuk templat domain
- [ ] Sanitasi dan log panggilan secara tempatan
- [ ] Laksanakan router hibrid ringkas dengan fallback Azure

## Penutup

Foundry Local membolehkan AI yang kukuh pada peranti di mana model menjadi alat yang boleh disusun. Dengan antara muka yang jelas, tadbir urus, dan peningkatan skala hibrid, pasukan boleh melancarkan aplikasi AI masa nyata yang selamat, menghormati privasi pengguna, dan bersedia untuk perusahaan.

---

