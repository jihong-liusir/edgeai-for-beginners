<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-25T00:47:08+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ms"
}
-->
# Sesi 3: Penemuan dan Pengurusan Model Sumber Terbuka

## Gambaran Keseluruhan

Sesi ini memberi tumpuan kepada penemuan dan pengurusan model secara praktikal dengan Foundry Local. Anda akan belajar cara menyenaraikan model yang tersedia, menguji pelbagai pilihan, dan memahami ciri-ciri prestasi asas. Pendekatan ini menekankan penerokaan secara langsung menggunakan CLI Foundry untuk membantu anda memilih model yang sesuai untuk kes penggunaan anda.

## Objektif Pembelajaran

- Kuasai arahan CLI Foundry untuk penemuan dan pengurusan model
- Fahami corak cache model dan storan tempatan
- Belajar menguji dan membandingkan model dengan cepat
- Tetapkan aliran kerja praktikal untuk pemilihan dan penanda aras model
- Terokai ekosistem model yang semakin berkembang melalui Foundry Local

## Prasyarat

- Selesai Sesi 1: Memulakan Foundry Local
- CLI Foundry Local dipasang dan boleh diakses
- Ruang storan yang mencukupi untuk muat turun model (model boleh berukuran antara 1GB hingga 20GB+)
- Pemahaman asas tentang jenis model dan kes penggunaan

## Gambaran Keseluruhan

Sesi ini meneroka cara membawa model sumber terbuka ke Foundry Local.

## Bahagian 6: Latihan Praktikal

### Latihan: Penemuan dan Perbandingan Model

Cipta skrip penilaian model anda sendiri berdasarkan Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Tugasan Anda

1. **Jalankan skrip Sample 03**: `samples\03\list_and_bench.cmd`
2. **Cuba model yang berbeza**: Uji sekurang-kurangnya 3 model berbeza
3. **Bandingkan prestasi**: Catat perbezaan dalam kelajuan dan kualiti respons
4. **Dokumentasikan penemuan**: Cipta carta perbandingan ringkas

### Format Perbandingan Contoh

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Bahagian 7: Penyelesaian Masalah dan Amalan Terbaik

### Isu Biasa dan Penyelesaian

**Model Tidak Bermula:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Memori Tidak Mencukupi:**
- Mulakan dengan model yang lebih kecil (`phi-4-mini`)
- Tutup aplikasi lain
- Tingkatkan RAM jika sering menghadapi had

**Prestasi Perlahan:**
- Pastikan model dimuat sepenuhnya (periksa output verbose)
- Tutup aplikasi latar belakang yang tidak diperlukan
- Pertimbangkan storan yang lebih pantas (SSD)

### Amalan Terbaik

1. **Mulakan Kecil**: Mulakan dengan `phi-4-mini` untuk mengesahkan persediaan
2. **Satu Model pada Satu Masa**: Hentikan model sebelumnya sebelum memulakan yang baru
3. **Pantau Sumber**: Perhatikan penggunaan memori
4. **Uji Secara Konsisten**: Gunakan arahan yang sama untuk perbandingan yang adil
5. **Dokumentasikan Hasil**: Simpan nota tentang prestasi model untuk kes penggunaan anda

## Bahagian 8: Langkah Seterusnya dan Rujukan

### Persediaan untuk Sesi 4

- **Fokus Sesi 4**: Alat dan teknik pengoptimuman
- **Prasyarat**: Selesa dengan pertukaran model dan ujian prestasi asas
- **Disyorkan**: Kenal pasti 2-3 model kegemaran daripada sesi ini

### Sumber Tambahan

- **[Dokumentasi Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Dokumentasi rasmi
- **[Rujukan CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Rujukan arahan lengkap
- **[Model Mondays](https://aka.ms/model-mondays)**: Sorotan model mingguan
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Komuniti dan isu
- **[Sample 03: Penemuan Model](samples/03/README.md)**: Skrip contoh praktikal

### Poin Penting

✅ **Penemuan Model**: Gunakan `foundry model list` untuk meneroka model yang tersedia  
✅ **Ujian Cepat**: Corak `list_and_bench.cmd` untuk penilaian pantas  
✅ **Pemantauan Prestasi**: Pengukuran penggunaan sumber dan masa respons asas  
✅ **Pemilihan Model**: Garis panduan praktikal untuk memilih model mengikut kes penggunaan  
✅ **Pengurusan Cache**: Memahami prosedur storan dan pembersihan  

Anda kini mempunyai kemahiran praktikal untuk menemui, menguji, dan memilih model yang sesuai untuk aplikasi AI anda menggunakan pendekatan CLI Foundry Local yang mudah.

## Objektif Pembelajaran

- Menemui dan menilai model sumber terbuka untuk inferens tempatan
- Kompilasi dan jalankan model Hugging Face terpilih dalam Foundry Local
- Terapkan strategi pemilihan model untuk ketepatan, kependaman, dan keperluan sumber
- Urus model secara tempatan dengan cache dan versi

## Bahagian 1: Penemuan Model dengan Foundry CLI

### Arahan Pengurusan Model Asas

CLI Foundry menyediakan arahan mudah untuk penemuan dan pengurusan model:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Menjalankan Model Pertama Anda

Mulakan dengan model yang popular dan telah diuji untuk memahami ciri-ciri prestasi:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Nota:** Flag `--verbose` memberikan maklumat permulaan terperinci, termasuk:
- Kemajuan muat turun model (pada kali pertama dijalankan)
- Butiran peruntukan memori
- Maklumat pengikatan perkhidmatan
- Metrik inisialisasi prestasi

### Memahami Kategori Model

**Model Bahasa Kecil (SLMs):**
- `phi-4-mini`: Pantas, cekap, sesuai untuk sembang umum
- `phi-4`: Versi yang lebih mampu dengan penaakulan yang lebih baik

**Model Sederhana:**
- `qwen2.5-7b-instruct`: Penaakulan yang cemerlang dan konteks yang lebih panjang
- `deepseek-r1-distill-qwen-7b`: Dioptimumkan untuk penjanaan kod

**Model Lebih Besar:**
- `llama-3.2`: Model sumber terbuka terkini dari Meta
- `qwen2.5-14b-instruct`: Penaakulan gred perusahaan

## Bahagian 2: Ujian dan Perbandingan Model Cepat

### Pendekatan Sample 03: Senarai dan Penanda Aras Mudah

Berdasarkan corak Sample 03 kami, berikut adalah aliran kerja minimum:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Ujian Prestasi Model

Setelah model berjalan, uji dengan arahan yang konsisten:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternatif Ujian PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Bahagian 3: Pengurusan Cache dan Storan Model

### Memahami Cache Model

Foundry Local secara automatik mengurus muat turun dan cache model:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Pertimbangan Storan Model

**Saiz Model Biasa:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Amalan Storan Terbaik:**
- Simpan 2-3 model dalam cache untuk pertukaran cepat
- Buang model yang tidak digunakan untuk menjimatkan ruang: `foundry cache clean`
- Pantau penggunaan cakera, terutamanya pada SSD yang lebih kecil
- Pertimbangkan saiz model berbanding kompromi keupayaan

### Pemantauan Prestasi Model

Semasa model berjalan, pantau sumber sistem:

**Windows Task Manager:**
- Perhatikan penggunaan memori (model kekal dimuat dalam RAM)
- Pantau penggunaan CPU semasa inferens
- Periksa I/O cakera semasa pemuatan model awal

**Pemantauan Baris Perintah:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Bahagian 4: Garis Panduan Pemilihan Model Praktikal

### Memilih Model Mengikut Kes Penggunaan

**Untuk Sembang Umum dan Soal Jawab:**
- Mulakan dengan: `phi-4-mini` (pantas, cekap)
- Tingkatkan kepada: `phi-4` (penaakulan lebih baik)
- Lanjutan: `qwen2.5-7b-instruct` (konteks lebih panjang)

**Untuk Penjanaan Kod:**
- Disyorkan: `deepseek-r1-distill-qwen-7b`
- Alternatif: `qwen2.5-7b-instruct` (juga bagus untuk kod)

**Untuk Penaakulan Kompleks:**
- Terbaik: `qwen2.5-7b-instruct` atau `qwen2.5-14b-instruct`
- Pilihan bajet: `phi-4`

### Panduan Keperluan Perkakasan

**Keperluan Sistem Minimum:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Disyorkan untuk Prestasi Terbaik:**
- RAM 32GB+ untuk pertukaran model yang selesa
- Storan SSD untuk pemuatan model yang lebih pantas
- CPU moden dengan prestasi satu benang yang baik
- Sokongan NPU (PC Windows 11 Copilot+) untuk pecutan

### Aliran Kerja Pertukaran Model

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Bahagian 5: Penanda Aras Model Mudah

### Ujian Prestasi Asas

Berikut adalah pendekatan mudah untuk membandingkan prestasi model:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Penilaian Kualiti Manual

Untuk setiap model, uji dengan arahan yang konsisten dan nilai secara manual:

**Arahan Ujian:**
1. "Terangkan pengkomputeran kuantum dalam istilah mudah."
2. "Tulis fungsi Python untuk menyusun senarai."
3. "Apakah kebaikan dan keburukan kerja jarak jauh?"
4. "Ringkaskan manfaat AI tepi."

**Kriteria Penilaian:**
- **Ketepatan**: Adakah maklumat betul?
- **Kejelasan**: Adakah penjelasan mudah difahami?
- **Kelengkapan**: Adakah ia menjawab soalan sepenuhnya?
- **Kelajuan**: Seberapa cepat ia memberi respons?

### Pemantauan Penggunaan Sumber

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Bahagian 6: Langkah Seterusnya

- Langgan Model Mondays untuk model dan petua baru: https://aka.ms/model-mondays
- Sumbangkan penemuan kepada `models.json` pasukan anda
- Bersedia untuk Sesi 4: membandingkan LLM vs SLM, inferens tempatan vs awan, dan demo praktikal

---

