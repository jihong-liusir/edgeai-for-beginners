<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T22:37:41+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ms"
}
-->
# Sesi 3: Model Sumber Terbuka dengan Foundry Local

## Gambaran Keseluruhan

Sesi ini meneroka cara membawa model sumber terbuka ke Foundry Local: memilih model komuniti, mengintegrasikan kandungan Hugging Face, dan mengamalkan strategi "bawa model anda sendiri" (BYOM). Anda juga akan menemui siri Model Mondays untuk pembelajaran berterusan dan penemuan model.

Rujukan:
- Dokumentasi Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilasi model Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- GitHub Foundry Local: https://github.com/microsoft/Foundry-Local

## Objektif Pembelajaran
- Menemui dan menilai model sumber terbuka untuk inferens tempatan
- Kompilasi dan jalankan model Hugging Face terpilih dalam Foundry Local
- Mengaplikasikan strategi pemilihan model berdasarkan ketepatan, kependaman, dan keperluan sumber
- Menguruskan model secara tempatan dengan cache dan versi

## Bahagian 1: Penemuan dan Pemilihan Model (Langkah demi langkah)

Langkah 1) Senaraikan model yang tersedia dalam katalog tempatan  
```cmd
foundry model list
```
  
Langkah 2) Cuba dua calon dengan cepat (muat turun automatik pada kali pertama dijalankan)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Langkah 3) Catatkan metrik asas  
- Perhatikan kependaman (subjektif) dan kualiti untuk satu arahan tetap  
- Pantau penggunaan memori melalui Task Manager semasa setiap model dijalankan  

## Bahagian 2: Menjalankan Model Katalog melalui CLI (Langkah demi langkah)

Langkah 1) Mulakan model  
```cmd
foundry model run llama-3.2
```
  
Langkah 2) Hantar arahan ujian melalui endpoint yang serasi dengan OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Bahagian 3: BYOM – Kompilasi Model Hugging Face (Langkah demi langkah)

Ikuti panduan rasmi untuk kompilasi model. Aliran tahap tinggi di bawah—lihat artikel Microsoft Learn untuk arahan tepat dan konfigurasi yang disokong.

Langkah 1) Sediakan direktori kerja  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Langkah 2) Kompilasi model HF yang disokong  
- Gunakan langkah-langkah dari dokumen Learn untuk menukar dan meletakkan model ONNX yang telah dikompilasi dalam direktori `models` anda  
- Sahkan dengan:  
```cmd
foundry cache ls
```
  
Anda sepatutnya melihat nama model yang telah dikompilasi (contohnya, `llama-3.2`).  

Langkah 3) Jalankan model yang telah dikompilasi  
```cmd
foundry model run llama-3.2 --verbose
```
  
Nota:  
- Pastikan cakera dan RAM mencukupi untuk kompilasi dan menjalankan  
- Mulakan dengan model yang lebih kecil untuk mengesahkan aliran, kemudian tingkatkan skala  

## Bahagian 4: Kurasi Model Praktikal (Langkah demi langkah)

Langkah 1) Cipta daftar `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Langkah 2) Skrip pemilih kecil  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Bahagian 5: Penanda Aras Praktikal (Langkah demi langkah)

Langkah 1) Penanda aras kependaman mudah  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Langkah 2) Semakan kualiti  
- Gunakan set arahan tetap, tangkap output ke CSV/JSON  
- Nilai secara manual kefasihan, relevansi, dan ketepatan (1–5)  

## Bahagian 6: Langkah Seterusnya
- Langgan Model Mondays untuk model dan tip baharu: https://aka.ms/model-mondays  
- Sumbangkan penemuan kepada `models.json` pasukan anda  
- Bersedia untuk Sesi 4: membandingkan LLM vs SLM, inferens tempatan vs awan, dan demo praktikal  

---

