<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T19:06:20+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "ms"
}
-->
# Seksyen 1: Pembelajaran Lanjutan SLM - Asas dan Pengoptimuman

Small Language Models (SLMs) merupakan satu kemajuan penting dalam EdgeAI, membolehkan keupayaan pemprosesan bahasa semula jadi yang canggih pada peranti dengan sumber terhad. Memahami cara untuk melaksanakan, mengoptimumkan, dan menggunakan SLM dengan berkesan adalah penting untuk membina penyelesaian AI berasaskan edge yang praktikal.

## Pengenalan

Dalam pelajaran ini, kita akan meneroka Small Language Models (SLMs) dan strategi pelaksanaan lanjutan mereka. Kita akan membincangkan konsep asas SLM, sempadan parameter dan klasifikasinya, teknik pengoptimuman, serta strategi pelaksanaan praktikal untuk persekitaran pengkomputeran edge.

## Objektif Pembelajaran

Pada akhir pelajaran ini, anda akan dapat:

- üî¢ Memahami sempadan parameter dan klasifikasi Small Language Models.
- üõ†Ô∏è Mengenal pasti teknik pengoptimuman utama untuk pelaksanaan SLM pada peranti edge.
- üöÄ Mempelajari pelaksanaan strategi kuantisasi dan pemampatan lanjutan untuk SLM.

## Memahami Sempadan Parameter dan Klasifikasi SLM

Small Language Models (SLMs) ialah model AI yang direka untuk memproses, memahami, dan menjana kandungan bahasa semula jadi dengan parameter yang jauh lebih sedikit berbanding model besar mereka. Walaupun Large Language Models (LLMs) mengandungi ratusan bilion hingga trilion parameter, SLM direka khusus untuk kecekapan dan pelaksanaan edge.

Kerangka klasifikasi parameter membantu kita memahami kategori berbeza SLM dan kes penggunaan yang sesuai. Klasifikasi ini penting untuk memilih model yang tepat untuk senario pengkomputeran edge tertentu.

### Kerangka Klasifikasi Parameter

Memahami sempadan parameter membantu dalam memilih model yang sesuai untuk senario pengkomputeran edge yang berbeza:

- **üî¨ Micro SLMs**: 100M - 1.4B parameter (sangat ringan untuk peranti mudah alih)
- **üì± Small SLMs**: 1.5B - 13.9B parameter (prestasi dan kecekapan yang seimbang)
- **‚öñÔ∏è Medium SLMs**: 14B - 30B parameter (hampir kepada keupayaan LLM sambil mengekalkan kecekapan)

Sempadan yang tepat masih fleksibel dalam komuniti penyelidikan, tetapi kebanyakan pengamal menganggap model dengan kurang daripada 30 bilion parameter sebagai "kecil," dengan beberapa sumber menetapkan ambang lebih rendah pada 10 bilion parameter.

### Kelebihan Utama SLM

SLM menawarkan beberapa kelebihan asas yang menjadikannya ideal untuk aplikasi pengkomputeran edge:

**Kecekapan Operasi**: SLM memberikan masa inferens yang lebih pantas kerana parameter yang lebih sedikit untuk diproses, menjadikannya ideal untuk aplikasi masa nyata. Ia memerlukan sumber pengkomputeran yang lebih rendah, membolehkan pelaksanaan pada peranti dengan sumber terhad sambil menggunakan tenaga yang lebih sedikit dan mengekalkan jejak karbon yang rendah.

**Fleksibiliti Pelaksanaan**: Model ini membolehkan keupayaan AI pada peranti tanpa keperluan sambungan internet, meningkatkan privasi dan keselamatan melalui pemprosesan tempatan, boleh disesuaikan untuk aplikasi khusus domain, dan sesuai untuk pelbagai persekitaran pengkomputeran edge.

**Keberkesanan Kos**: SLM menawarkan latihan dan pelaksanaan yang lebih kos efektif berbanding LLM, dengan kos operasi yang lebih rendah dan keperluan jalur lebar yang lebih kecil untuk aplikasi edge.

## Strategi Pemerolehan Model Lanjutan

### Ekosistem Hugging Face

Hugging Face berfungsi sebagai hab utama untuk menemui dan mengakses SLM terkini. Platform ini menyediakan sumber yang komprehensif untuk penemuan dan pelaksanaan model:

**Ciri Penemuan Model**: Platform ini menawarkan penapisan lanjutan mengikut jumlah parameter, jenis lesen, dan metrik prestasi. Pengguna boleh mengakses alat perbandingan model secara bersebelahan, penanda aras prestasi masa nyata dan hasil penilaian, serta demo WebGPU untuk ujian segera.

**Koleksi SLM Terkurasi**: Model popular termasuk Phi-4-mini-3.8B untuk tugas penaakulan lanjutan, siri Qwen3 (0.6B/1.7B/4B) untuk aplikasi pelbagai bahasa, Google Gemma3 untuk tugas umum yang cekap, dan model eksperimen seperti BitNET untuk pelaksanaan ultra-rendah ketepatan. Platform ini juga menampilkan koleksi yang didorong oleh komuniti dengan model khusus untuk domain tertentu serta varian yang telah dilatih dan disesuaikan untuk pelbagai kes penggunaan.

### Katalog Model Azure AI Foundry

Katalog Model Azure AI Foundry menyediakan akses kepada SLM dengan keupayaan integrasi yang dipertingkatkan untuk perusahaan:

**Integrasi Perusahaan**: Katalog ini termasuk model yang dijual secara langsung oleh Azure dengan sokongan gred perusahaan dan SLA, menampilkan Phi-4-mini-3.8B untuk keupayaan penaakulan lanjutan dan Llama 3-8B untuk pelaksanaan pengeluaran. Ia juga menampilkan model seperti Qwen3 8B daripada model sumber terbuka pihak ketiga yang dipercayai.

**Manfaat Perusahaan**: Alat terbina dalam untuk penyesuaian, pemerhatian, dan AI yang bertanggungjawab diintegrasikan dengan Provisioned Throughput yang boleh digunakan merentasi keluarga model. Sokongan langsung Microsoft dengan SLA perusahaan, ciri keselamatan dan pematuhan yang diintegrasikan, serta aliran kerja pelaksanaan yang komprehensif meningkatkan pengalaman perusahaan.

## Teknik Kuantisasi dan Pengoptimuman Lanjutan

### Kerangka Pengoptimuman Llama.cpp

Llama.cpp menyediakan teknik kuantisasi terkini untuk kecekapan maksimum dalam pelaksanaan edge:

**Kaedah Kuantisasi**: Kerangka ini menyokong pelbagai tahap kuantisasi termasuk Q4_0 (kuantisasi 4-bit dengan pengurangan saiz yang sangat baik - ideal untuk pelaksanaan mudah alih Qwen3-0.6B), Q5_1 (kuantisasi 5-bit yang mengimbangi kualiti dan pemampatan - sesuai untuk inferens edge Phi-4-mini-3.8B), dan Q8_0 (kuantisasi 8-bit untuk kualiti hampir asal - disyorkan untuk penggunaan pengeluaran Google Gemma3). BitNET mewakili teknologi terkini dengan kuantisasi 1-bit untuk senario pemampatan ekstrem.

**Manfaat Pelaksanaan**: Inferens yang dioptimumkan CPU dengan pecutan SIMD menyediakan pemuatan dan pelaksanaan model yang cekap memori. Keserasian merentas platform merentasi seni bina x86, ARM, dan Apple Silicon membolehkan keupayaan pelaksanaan yang bebas perkakasan.

**Contoh Pelaksanaan Praktikal**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Perbandingan Jejak Memori**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Suite Pengoptimuman Microsoft Olive

Microsoft Olive menawarkan aliran kerja pengoptimuman model yang komprehensif yang direka untuk persekitaran pengeluaran:

**Teknik Pengoptimuman**: Suite ini termasuk kuantisasi dinamik untuk pemilihan ketepatan automatik (terutamanya berkesan dengan model siri Qwen3), pengoptimuman graf dan penggabungan operator (dioptimumkan untuk seni bina Google Gemma3), pengoptimuman khusus perkakasan untuk CPU, GPU, dan NPU (dengan sokongan khas untuk Phi-4-mini-3.8B pada peranti ARM), dan saluran pengoptimuman berbilang peringkat. Model BitNET memerlukan aliran kerja kuantisasi 1-bit khusus dalam kerangka Olive.

**Automasi Aliran Kerja**: Penanda aras automatik merentasi varian pengoptimuman memastikan pemeliharaan metrik kualiti semasa pengoptimuman. Integrasi dengan kerangka ML popular seperti PyTorch dan ONNX menyediakan keupayaan pengoptimuman pelaksanaan awan dan edge.

**Contoh Pelaksanaan Praktikal**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Kerangka Apple MLX

Apple MLX menyediakan pengoptimuman asli yang direka khusus untuk peranti Apple Silicon:

**Pengoptimuman Apple Silicon**: Kerangka ini menggunakan seni bina memori bersatu dengan integrasi Metal Performance Shaders, inferens ketepatan campuran automatik (terutamanya berkesan dengan Google Gemma3), dan penggunaan lebar jalur memori yang dioptimumkan. Phi-4-mini-3.8B menunjukkan prestasi yang luar biasa pada cip siri M, manakala Qwen3-1.7B memberikan keseimbangan optimum untuk pelaksanaan MacBook Air.

**Ciri Pembangunan**: Sokongan API Python dan Swift dengan operasi array yang serasi NumPy, keupayaan pembezaan automatik, dan integrasi lancar dengan alat pembangunan Apple menyediakan persekitaran pembangunan yang komprehensif.

**Contoh Pelaksanaan Praktikal**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategi Pelaksanaan dan Inferens Pengeluaran

### Ollama: Pelaksanaan Tempatan yang Dipermudahkan

Ollama mempermudah pelaksanaan SLM dengan ciri sedia perusahaan untuk persekitaran tempatan dan edge:

**Keupayaan Pelaksanaan**: Pemasangan dan pelaksanaan model dengan satu arahan dengan penarikan dan caching model automatik. Sokongan untuk Phi-4-mini-3.8B, keseluruhan siri Qwen3 (0.6B/1.7B/4B), dan Google Gemma3 dengan REST API untuk integrasi aplikasi serta pengurusan dan penukaran model pelbagai. Model BitNET memerlukan konfigurasi binaan eksperimen untuk sokongan kuantisasi 1-bit.

**Ciri Lanjutan**: Sokongan penyesuaian model, penjanaan Dockerfile untuk pelaksanaan berasaskan kontena, pecutan GPU dengan pengesanan automatik, serta pilihan kuantisasi dan pengoptimuman model menyediakan fleksibiliti pelaksanaan yang komprehensif.

### VLLM: Inferens Berprestasi Tinggi

VLLM menyediakan pengoptimuman inferens gred pengeluaran untuk senario throughput tinggi:

**Pengoptimuman Prestasi**: PagedAttention untuk pengiraan perhatian yang cekap memori (terutamanya bermanfaat untuk seni bina transformer Phi-4-mini-3.8B), pembungkusan dinamik untuk pengoptimuman throughput (dioptimumkan untuk pemprosesan selari siri Qwen3), paralelisme tensor untuk penskalaan multi-GPU (sokongan Google Gemma3), dan pengekodan spekulatif untuk pengurangan latensi. Model BitNET memerlukan kernel inferens khusus untuk operasi 1-bit.

**Integrasi Perusahaan**: Titik akhir API yang serasi dengan OpenAI, sokongan pelaksanaan Kubernetes, integrasi pemantauan dan pemerhatian, serta keupayaan penskalaan automatik menyediakan penyelesaian pelaksanaan gred perusahaan.

### Foundry Local: Penyelesaian Edge Microsoft

Foundry Local menyediakan keupayaan pelaksanaan edge yang komprehensif untuk persekitaran perusahaan:

**Ciri Pengkomputeran Edge**: Reka bentuk seni bina offline-first dengan pengoptimuman kekangan sumber, pengurusan daftar model tempatan, dan keupayaan penyegerakan edge-ke-cloud memastikan pelaksanaan edge yang boleh dipercayai.

**Keselamatan dan Pematuhan**: Pemprosesan data tempatan untuk pemeliharaan privasi, kawalan keselamatan perusahaan, log audit dan pelaporan pematuhan, serta pengurusan akses berdasarkan peranan menyediakan keselamatan yang komprehensif untuk pelaksanaan edge.

## Amalan Terbaik untuk Pelaksanaan SLM

### Garis Panduan Pemilihan Model

Apabila memilih SLM untuk pelaksanaan edge, pertimbangkan faktor berikut:

**Pertimbangan Jumlah Parameter**: Pilih micro SLM seperti Qwen3-0.6B untuk aplikasi mudah alih ultra-ringan, small SLM seperti Qwen3-1.7B atau Google Gemma3 untuk senario prestasi seimbang, dan medium SLM seperti Phi-4-mini-3.8B atau Qwen3-4B apabila menghampiri keupayaan LLM sambil mengekalkan kecekapan. Model BitNET menawarkan pemampatan ultra-eksperimen untuk aplikasi penyelidikan tertentu.

**Penyelarasan Kes Penggunaan**: Padankan keupayaan model dengan keperluan aplikasi tertentu, dengan mempertimbangkan faktor seperti kualiti respons, kelajuan inferens, kekangan memori, dan keperluan operasi offline.

### Pemilihan Strategi Pengoptimuman

**Pendekatan Kuantisasi**: Pilih tahap kuantisasi yang sesuai berdasarkan keperluan kualiti dan kekangan perkakasan. Pertimbangkan Q4_0 untuk pemampatan maksimum (ideal untuk pelaksanaan mudah alih Qwen3-0.6B), Q5_1 untuk keseimbangan kualiti-pemampatan (sesuai untuk Phi-4-mini-3.8B dan Google Gemma3), dan Q8_0 untuk pemeliharaan kualiti hampir asal (disyorkan untuk persekitaran pengeluaran Qwen3-4B). Kuantisasi 1-bit BitNET mewakili sempadan pemampatan ekstrem untuk aplikasi khusus.

**Pemilihan Kerangka**: Pilih kerangka pengoptimuman berdasarkan perkakasan sasaran dan keperluan pelaksanaan. Gunakan Llama.cpp untuk pelaksanaan yang dioptimumkan CPU, Microsoft Olive untuk aliran kerja pengoptimuman yang komprehensif, dan Apple MLX untuk peranti Apple Silicon.

## Contoh Model Praktikal dan Kes Penggunaan

### Senario Pelaksanaan Dunia Nyata

**Aplikasi Mudah Alih**: Qwen3-0.6B cemerlang dalam aplikasi chatbot telefon pintar dengan jejak memori yang minimum, manakala Google Gemma3 menyediakan prestasi seimbang untuk alat pendidikan berasaskan tablet. Phi-4-mini-3.8B menawarkan keupayaan penaakulan yang unggul untuk aplikasi produktiviti mudah alih.

**Pengkomputeran Desktop dan Edge**: Qwen3-1.7B memberikan prestasi optimum untuk aplikasi pembantu desktop, Phi-4-mini-3.8B menyediakan keupayaan penjanaan kod lanjutan untuk alat pembangun, dan Qwen3-4B membolehkan analisis dokumen yang canggih pada persekitaran stesen kerja.

**Penyelidikan dan Eksperimen**: Model BitNET membolehkan penerokaan inferens ketepatan ultra-rendah untuk penyelidikan akademik dan aplikasi bukti konsep yang memerlukan kekangan sumber yang ekstrem.

### Penanda Aras Prestasi dan Perbandingan

**Kelajuan Inferens**: Qwen3-0.6B mencapai masa inferens terpantas pada CPU mudah alih, Google Gemma3 menyediakan nisbah kelajuan-kualiti yang seimbang untuk aplikasi umum, Phi-4-mini-3.8B menawarkan kelajuan penaakulan yang unggul untuk tugas kompleks, dan BitNET memberikan throughput maksimum teori dengan perkakasan khusus.

**Keperluan Memori**: Jejak memori model berkisar dari Qwen3-0.6B (di bawah 1GB kuantisasi) hingga Phi-4-mini-3.8B (kira-kira 3-4GB kuantisasi), dengan BitNET mencapai jejak sub-500MB dalam konfigurasi eksperimen.

## Cabaran dan Pertimbangan

### Perdagangan Prestasi

Pelaksanaan SLM melibatkan pertimbangan yang teliti terhadap perdagangan antara saiz model, kelajuan inferens, dan kualiti output. Sebagai contoh, walaupun Qwen3-0.6B menawarkan kelajuan dan kecekapan yang luar biasa, Phi-4-mini-3.8B menyediakan keupayaan penaakulan yang unggul dengan kos keperluan sumber yang meningkat. Google Gemma3 mencapai keseimbangan yang sesuai untuk kebanyakan aplikasi umum.

### Keserasian Perkakasan

Peranti edge yang berbeza mempunyai keupayaan dan kekangan yang berbeza. Qwen3-0.6B berjalan dengan cekap pada pemproses ARM asas, Google Gemma3 memerlukan sumber pengkomputeran sederhana, dan Phi-4-mini-3.8B mendapat manfaat daripada perkakasan edge yang lebih tinggi. Model BitNET memerlukan pelaksanaan perkakasan atau perisian khusus untuk operasi 1-bit yang optimum.

### Keselamatan dan Privasi

Walaupun SLM membolehkan pemprosesan tempatan untuk privasi yang dipertingkatkan, langkah keselamatan yang sesuai mesti dilaksanakan untuk melindungi model dan data dalam persekitaran edge. Ini amat penting apabila melaksanakan model seperti Phi-4-mini-3.8B dalam persekitaran perusahaan atau siri Qwen3 dalam aplikasi pelbagai bahasa yang mengendalikan data sensitif.

## Trend Masa Depan dalam Pembangunan SLM

Landskap SLM terus berkembang dengan kemajuan dalam seni bina model, teknik pengoptimuman, dan strategi pelaksanaan. Perkembangan masa depan termasuk seni bina yang lebih cekap, kaedah kuantisasi yang lebih baik, dan integrasi yang lebih baik dengan pemecut perkakasan edge.

Memahami trend ini dan mengekalkan kesedaran tentang teknologi yang muncul akan menjadi penting untuk kekal terkini dengan amalan terbaik pembangunan dan pelaksanaan SLM.

## ‚û°Ô∏è Apa yang seterusnya

- [02: Melaksanakan SLM dalam Persekitaran Tempatan](02.DeployingSLMinLocalEnv.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.