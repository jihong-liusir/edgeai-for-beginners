<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-18T14:24:15+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ms"
}
-->
# Seksyen 3: Panduan Pelaksanaan Praktikal

## Gambaran Keseluruhan

Panduan komprehensif ini akan membantu anda bersedia untuk kursus EdgeAI, yang memberi tumpuan kepada membina penyelesaian AI praktikal yang beroperasi dengan cekap pada peranti edge. Kursus ini menekankan pembangunan secara langsung menggunakan rangka kerja moden dan model terkini yang dioptimumkan untuk pelaksanaan edge.

## 1. Persediaan Persekitaran Pembangunan

### Bahasa Pengaturcaraan & Rangka Kerja

**Persekitaran Python**
- **Versi**: Python 3.10 atau lebih tinggi (disyorkan: Python 3.11)
- **Pengurus Pakej**: pip atau conda
- **Persekitaran Maya**: Gunakan venv atau persekitaran conda untuk pengasingan
- **Perpustakaan Utama**: Kami akan memasang perpustakaan EdgeAI tertentu semasa kursus

**Persekitaran Microsoft .NET**
- **Versi**: .NET 8 atau lebih tinggi
- **IDE**: Visual Studio 2022, Visual Studio Code, atau JetBrains Rider
- **SDK**: Pastikan .NET SDK dipasang untuk pembangunan merentas platform

### Alat Pembangunan

**Editor Kod & IDE**
- Visual Studio Code (disyorkan untuk pembangunan merentas platform)
- PyCharm atau Visual Studio (untuk pembangunan khusus bahasa)
- Jupyter Notebooks untuk pembangunan interaktif dan prototaip

**Kawalan Versi**
- Git (versi terkini)
- Akaun GitHub untuk mengakses repositori dan kolaborasi

## 2. Keperluan & Cadangan Perkakasan

### Keperluan Sistem Minimum
- **CPU**: Pemproses multi-teras (Intel i5/AMD Ryzen 5 atau setara)
- **RAM**: Minimum 8GB, disyorkan 16GB
- **Storan**: Ruang tersedia 50GB untuk model dan alat pembangunan
- **OS**: Windows 10/11, macOS 10.15+, atau Linux (Ubuntu 20.04+)

### Strategi Sumber Pengkomputeran
Kursus ini direka untuk boleh diakses merentasi konfigurasi perkakasan yang berbeza:

**Pembangunan Tempatan (Fokus CPU/NPU)**
- Pembangunan utama akan menggunakan CPU dan percepatan NPU
- Sesuai untuk kebanyakan komputer riba dan desktop moden
- Fokus pada kecekapan dan senario pelaksanaan praktikal

**Sumber GPU Awan (Pilihan)**
- **Azure Machine Learning**: Untuk latihan intensif dan eksperimen
- **Google Colab**: Tier percuma tersedia untuk tujuan pendidikan
- **Kaggle Notebooks**: Platform pengkomputeran awan alternatif

### Pertimbangan Peranti Edge
- Pemahaman tentang pemproses berasaskan ARM
- Pengetahuan tentang kekangan perkakasan mudah alih dan IoT
- Kefahaman tentang pengoptimuman penggunaan kuasa

## 3. Keluarga Model Teras & Sumber

### Keluarga Model Utama

**Keluarga Microsoft Phi-4**
- **Penerangan**: Model padat dan cekap yang direka untuk pelaksanaan edge
- **Kekuatan**: Nisbah prestasi-ke-saiz yang cemerlang, dioptimumkan untuk tugas penaakulan
- **Sumber**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Kes Penggunaan**: Penjanaan kod, penaakulan matematik, perbualan umum

**Keluarga Qwen-3**
- **Penerangan**: Generasi terbaru model pelbagai bahasa oleh Alibaba
- **Kekuatan**: Keupayaan pelbagai bahasa yang kuat, seni bina yang cekap
- **Sumber**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Kes Penggunaan**: Aplikasi pelbagai bahasa, penyelesaian AI merentas budaya

**Keluarga Google Gemma-3n**
- **Penerangan**: Model ringan Google yang dioptimumkan untuk pelaksanaan edge
- **Kekuatan**: Inferens pantas, seni bina mesra mudah alih
- **Sumber**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Kes Penggunaan**: Aplikasi mudah alih, pemprosesan masa nyata

### Kriteria Pemilihan Model
- **Perdagangan Prestasi vs. Saiz**: Memahami bila untuk memilih model kecil vs. besar
- **Pengoptimuman Khusus Tugas**: Memadankan model dengan kes penggunaan tertentu
- **Kekangan Pelaksanaan**: Memori, latensi, dan pertimbangan penggunaan kuasa

## 4. Alat Kuantisasi & Pengoptimuman

### Rangka Kerja Llama.cpp
- **Repositori**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **Tujuan**: Enjin inferens berprestasi tinggi untuk LLM
- **Ciri Utama**:
  - Inferens dioptimumkan untuk CPU
  - Pelbagai format kuantisasi (Q4, Q5, Q8)
  - Keserasian merentas platform
  - Pelaksanaan cekap memori
- **Pemasangan dan Penggunaan Asas**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repositori**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **Tujuan**: Alat pengoptimuman model untuk pelaksanaan edge
- **Ciri Utama**:
  - Alur kerja pengoptimuman model automatik
  - Pengoptimuman yang sedar perkakasan
  - Integrasi dengan ONNX Runtime
  - Alat penanda aras prestasi
- **Pemasangan dan Penggunaan Asas**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Definisi model dan konfigurasi pengoptimuman
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Jalankan alur kerja pengoptimuman
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Simpan model yang dioptimumkan
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # Pasang MLX
  pip install mlx
  
  # Skrip Python contoh untuk memuat dan mengoptimumkan model
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repositori**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **Tujuan**: Percepatan inferens merentas platform untuk model ONNX
- **Ciri Utama**:
  - Pengoptimuman khusus perkakasan (CPU, GPU, NPU)
  - Pengoptimuman graf untuk inferens
  - Sokongan kuantisasi
  - Sokongan merentas bahasa (Python, C++, C#, JavaScript)
- **Pemasangan dan Penggunaan Asas**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```
## 5. Bacaan & Sumber Disyorkan

### Dokumentasi Penting
- **Dokumentasi ONNX Runtime**: Memahami inferens merentas platform 
- **Panduan Transformers Hugging Face**: Pemuatan model dan inferens
- **Corak Reka Bentuk Edge AI**: Amalan terbaik untuk pelaksanaan edge

### Kertas Teknikal
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Sumber Komuniti
- **Komuniti Slack/Discord EdgeAI**: Sokongan rakan sebaya dan perbincangan
- **Repositori GitHub**: Pelaksanaan contoh dan tutorial
- **Saluran YouTube**: Penyelaman teknikal dan tutorial

## 6. Penilaian & Pengesahan

### Senarai Semak Pra-Kursus
- [ ] Python 3.10+ dipasang dan disahkan
- [ ] .NET 8+ dipasang dan disahkan
- [ ] Persekitaran pembangunan dikonfigurasi
- [ ] Akaun Hugging Face dibuat
- [ ] Kefahaman asas tentang keluarga model sasaran
- [ ] Alat kuantisasi dipasang dan diuji
- [ ] Keperluan perkakasan dipenuhi
- [ ] Akaun pengkomputeran awan disediakan (jika diperlukan)

## Objektif Pembelajaran Utama

Menjelang akhir panduan ini, anda akan dapat:

1. Menyediakan persekitaran pembangunan lengkap untuk pembangunan aplikasi EdgeAI
2. Memasang dan mengkonfigurasi alat dan rangka kerja yang diperlukan untuk pengoptimuman model
3. Memilih konfigurasi perkakasan dan perisian yang sesuai untuk projek EdgeAI anda
4. Memahami pertimbangan utama untuk melaksanakan model AI pada peranti edge
5. Menyediakan sistem anda untuk latihan praktikal dalam kursus

## Sumber Tambahan

### Dokumentasi Rasmi
- **Dokumentasi Python**: Dokumentasi rasmi bahasa Python
- **Dokumentasi Microsoft .NET**: Sumber pembangunan rasmi .NET
- **Dokumentasi ONNX Runtime**: Panduan komprehensif untuk ONNX Runtime
- **Dokumentasi TensorFlow Lite**: Dokumentasi rasmi TensorFlow Lite

### Alat Pembangunan
- **Visual Studio Code**: Editor kod ringan dengan sambungan pembangunan AI
- **Jupyter Notebooks**: Persekitaran pengkomputeran interaktif untuk eksperimen ML
- **Docker**: Platform kontainerisasi untuk persekitaran pembangunan yang konsisten
- **Git**: Sistem kawalan versi untuk pengurusan kod

### Sumber Pembelajaran
- **Kertas Penyelidikan EdgeAI**: Penyelidikan akademik terkini tentang model cekap
- **Kursus Dalam Talian**: Bahan pembelajaran tambahan tentang pengoptimuman AI
- **Forum Komuniti**: Platform soal jawab untuk cabaran pembangunan EdgeAI
- **Set Data Penanda Aras**: Set data standard untuk menilai prestasi model

## Hasil Pembelajaran

Selepas melengkapkan panduan persediaan ini, anda akan:

1. Mempunyai persekitaran pembangunan yang dikonfigurasi sepenuhnya untuk pembangunan EdgeAI
2. Memahami keperluan perkakasan dan perisian untuk senario pelaksanaan yang berbeza
3. Mengenali rangka kerja dan alat utama yang digunakan sepanjang kursus
4. Mampu memilih model yang sesuai berdasarkan kekangan dan keperluan peranti
5. Mempunyai pengetahuan asas tentang teknik pengoptimuman untuk pelaksanaan edge

## ➡️ Apa yang seterusnya

- [04: Perkakasan dan Pelaksanaan EdgeAI](04.EdgeDeployment.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.