<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T14:52:25+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ms"
}
-->
# Bahagian 4: Penggunaan - Pelaksanaan Model Sedia-Produk

## Gambaran Keseluruhan

Tutorial komprehensif ini akan membimbing anda melalui keseluruhan proses penggunaan model yang telah ditala halus dan dikuantisasi menggunakan Foundry Local. Kita akan merangkumi penukaran model, pengoptimuman kuantisasi, dan konfigurasi penggunaan dari awal hingga akhir.

## Prasyarat

Sebelum memulakan, pastikan anda mempunyai perkara berikut:

- ✅ Model onnx yang telah ditala halus dan sedia untuk digunakan
- ✅ Komputer Windows atau Mac
- ✅ Python 3.10 atau lebih tinggi
- ✅ RAM sekurang-kurangnya 8GB tersedia
- ✅ Foundry Local dipasang pada sistem anda

## Bahagian 1: Persediaan Persekitaran

### Memasang Alat yang Diperlukan

Buka terminal anda (Command Prompt di Windows, Terminal di Mac) dan jalankan perintah berikut secara berurutan:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **Nota Penting**: Anda juga memerlukan CMake versi 3.31 atau lebih baru, yang boleh dimuat turun dari [cmake.org](https://cmake.org/download/).

## Bahagian 2: Penukaran dan Kuantisasi Model

### Memilih Format yang Sesuai

Untuk model bahasa kecil yang telah ditala halus, kami mengesyorkan menggunakan **format ONNX** kerana ia menawarkan:

- 🚀 Pengoptimuman prestasi yang lebih baik
- 🔧 Penggunaan yang tidak bergantung pada perkakasan
- 🏭 Keupayaan sedia-produksi
- 📱 Keserasian merentas platform

### Kaedah 1: Penukaran Satu Perintah (Disyorkan)

Gunakan perintah berikut untuk menukar model yang telah ditala halus secara langsung:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Penjelasan Parameter:**
- `--model_name_or_path`: Laluan ke model yang telah ditala halus
- `--device cpu`: Gunakan CPU untuk pengoptimuman
- `--precision int4`: Gunakan kuantisasi INT4 (pengurangan saiz sekitar 75%)
- `--output_path`: Laluan output untuk model yang telah ditukar

### Kaedah 2: Pendekatan Fail Konfigurasi (Pengguna Lanjutan)

Cipta fail konfigurasi bernama `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Kemudian jalankan:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Perbandingan Pilihan Kuantisasi

| Ketepatan | Saiz Fail | Kelajuan Inferens | Kualiti Model | Penggunaan Disyorkan |
|-----------|-----------|-------------------|---------------|-----------------------|
| FP16      | Asas × 0.5 | Pantas           | Terbaik       | Perkakasan kelas tinggi |
| INT8      | Asas × 0.25 | Sangat Pantas    | Baik          | Pilihan seimbang       |
| INT4      | Asas × 0.125 | Paling Pantas   | Boleh Diterima| Sumber terhad         |

💡 **Cadangan**: Mulakan dengan kuantisasi INT4 untuk penggunaan pertama anda. Jika kualiti tidak memuaskan, cuba INT8 atau FP16.

## Bahagian 3: Konfigurasi Penggunaan Foundry Local

### Mencipta Konfigurasi Model

Navigasi ke direktori model Foundry Local:

```bash
foundry cache cd ./models/
```

Cipta struktur direktori model anda:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Cipta fail konfigurasi `inference_model.json` dalam direktori model anda:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Konfigurasi Templat Khusus Model

#### Untuk Model Siri Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Bahagian 4: Ujian dan Pengoptimuman Model

### Mengesahkan Pemasangan Model

Periksa sama ada Foundry Local dapat mengenali model anda:

```bash
foundry cache ls
```

Anda sepatutnya melihat `your-finetuned-model-int4` dalam senarai.

### Memulakan Ujian Model

```bash
foundry model run your-finetuned-model-int4
```

### Penanda Aras Prestasi

Pantau metrik utama semasa ujian:

1. **Masa Respons**: Ukur masa purata bagi setiap respons
2. **Penggunaan Memori**: Pantau penggunaan RAM
3. **Penggunaan CPU**: Periksa beban pemproses
4. **Kualiti Output**: Nilai kesesuaian dan koheren respons

### Senarai Semak Pengesahan Kualiti

- ✅ Model memberikan respons yang sesuai untuk pertanyaan domain yang telah ditala halus
- ✅ Format respons sepadan dengan struktur output yang dijangkakan
- ✅ Tiada kebocoran memori semasa penggunaan berpanjangan
- ✅ Prestasi konsisten merentas panjang input yang berbeza
- ✅ Pengendalian kes tepi dan input tidak sah yang betul

## Ringkasan

Tahniah! Anda telah berjaya menyelesaikan:

- ✅ Penukaran format model yang telah ditala halus
- ✅ Pengoptimuman kuantisasi model
- ✅ Konfigurasi penggunaan Foundry Local
- ✅ Penalaan prestasi dan penyelesaian masalah

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.