<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-18T14:48:50+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "ms"
}
-->
# Seksyen 2: Penyulingan Model - Dari Teori ke Praktik

## Kandungan
1. [Pengenalan kepada Penyulingan Model](../../../Module05)
2. [Mengapa Penyulingan Penting](../../../Module05)
3. [Proses Penyulingan](../../../Module05)
4. [Pelaksanaan Praktikal](../../../Module05)
5. [Contoh Penyulingan Azure ML](../../../Module05)
6. [Amalan Terbaik dan Pengoptimuman](../../../Module05)
7. [Aplikasi Dunia Sebenar](../../../Module05)
8. [Kesimpulan](../../../Module05)

## Pengenalan kepada Penyulingan Model {#introduction}

Penyulingan model adalah teknik yang berkuasa untuk mencipta model yang lebih kecil dan cekap sambil mengekalkan sebahagian besar prestasi model yang lebih besar dan kompleks. Proses ini melibatkan latihan model "pelajar" yang padat untuk meniru tingkah laku model "guru" yang lebih besar.

**Manfaat Utama:**
- **Keperluan pengiraan yang berkurang** untuk inferens
- **Penggunaan memori dan keperluan storan yang lebih rendah**
- **Masa inferens yang lebih pantas** sambil mengekalkan ketepatan yang munasabah
- **Penggunaan kos efektif** dalam persekitaran yang terhad sumber

## Mengapa Penyulingan Penting {#why-distillation-matters}

Model Bahasa Besar (LLM) semakin berkuasa tetapi juga semakin memerlukan sumber yang tinggi. Walaupun model dengan berbilion parameter mungkin memberikan hasil yang cemerlang, ia mungkin tidak praktikal untuk banyak aplikasi dunia sebenar disebabkan oleh:

### Kekangan Sumber
- **Beban pengiraan**: Model besar memerlukan memori GPU dan kuasa pemprosesan yang signifikan
- **Kelewatan inferens**: Model kompleks mengambil masa lebih lama untuk menghasilkan respons
- **Penggunaan tenaga**: Model besar menggunakan lebih banyak kuasa, meningkatkan kos operasi
- **Kos infrastruktur**: Menempatkan model besar memerlukan perkakasan yang mahal

### Had Praktikal
- **Penggunaan mudah alih**: Model besar tidak dapat berjalan dengan cekap pada peranti mudah alih
- **Aplikasi masa nyata**: Aplikasi yang memerlukan kelewatan rendah tidak dapat menampung inferens yang perlahan
- **Pengkomputeran tepi**: Peranti IoT dan tepi mempunyai sumber pengiraan yang terhad
- **Pertimbangan kos**: Banyak organisasi tidak mampu membiayai infrastruktur untuk penempatan model besar

## Proses Penyulingan {#the-distillation-process}

Penyulingan model mengikuti proses dua peringkat yang memindahkan pengetahuan dari model guru ke model pelajar:

### Peringkat 1: Penjanaan Data Sintetik

Model guru menghasilkan respons untuk dataset latihan anda, mencipta data sintetik berkualiti tinggi yang menangkap pengetahuan dan corak penaakulan model guru.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Aspek utama peringkat ini:**
- Model guru memproses setiap contoh latihan
- Respons yang dihasilkan menjadi "kebenaran asas" untuk latihan pelajar
- Proses ini menangkap corak pengambilan keputusan model guru
- Kualiti data sintetik secara langsung mempengaruhi prestasi model pelajar

### Peringkat 2: Penalaan Model Pelajar

Model pelajar dilatih pada dataset sintetik, belajar untuk meniru tingkah laku dan respons model guru.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Objektif latihan:**
- Meminimumkan perbezaan antara output pelajar dan guru
- Mengekalkan pengetahuan guru dalam ruang parameter yang lebih kecil
- Mengekalkan prestasi sambil mengurangkan kerumitan model

## Pelaksanaan Praktikal {#practical-implementation}

### Memilih Model Guru dan Pelajar

**Pemilihan Model Guru:**
- Pilih LLM berskala besar (100B+ parameter) dengan prestasi terbukti untuk tugas tertentu anda
- Model guru popular termasuk:
  - **DeepSeek V3** (671B parameter) - cemerlang untuk penaakulan dan penjanaan kod
  - **Meta Llama 3.1 405B Instruct** - keupayaan serba guna yang komprehensif
  - **GPT-4** - prestasi kukuh merentasi pelbagai tugas
  - **Claude 3.5 Sonnet** - cemerlang untuk tugas penaakulan kompleks
- Pastikan model guru berprestasi baik pada data khusus domain anda

**Pemilihan Model Pelajar:**
- Imbangi antara saiz model dan keperluan prestasi
- Fokus pada model yang cekap dan lebih kecil seperti:
  - **Microsoft Phi-4-mini** - model cekap terkini dengan keupayaan penaakulan yang kukuh
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (varian 4K dan 128K)
  - Microsoft Phi-3.5 Mini Instruct

### Langkah Pelaksanaan

1. **Penyediaan Data**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Persediaan Model Guru**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Penjanaan Data Sintetik**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Latihan Model Pelajar**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Contoh Penyulingan Azure ML {#azure-ml-example}

Azure Machine Learning menyediakan platform yang komprehensif untuk melaksanakan penyulingan model. Berikut adalah cara menggunakan Azure ML untuk aliran kerja penyulingan anda:

### Prasyarat

1. **Ruang Kerja Azure ML**: Sediakan ruang kerja anda di rantau yang sesuai
   - Pastikan akses kepada model guru berskala besar (DeepSeek V3, Llama 405B)
   - Konfigurasikan rantau berdasarkan ketersediaan model

2. **Sumber Pengiraan**: Konfigurasikan contoh pengiraan yang sesuai untuk latihan
   - Contoh memori tinggi untuk inferens model guru
   - Pengiraan yang diaktifkan GPU untuk penalaan model pelajar

### Jenis Tugas yang Disokong

Azure ML menyokong penyulingan untuk pelbagai tugas:

- **Interpretasi Bahasa Semula Jadi (NLI)**
- **AI Perbualan**
- **Soal Jawab (QA)**
- **Penaakulan Matematik**
- **Peringkasan Teks**

### Pelaksanaan Contoh

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Pemantauan dan Penilaian

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Amalan Terbaik dan Pengoptimuman {#best-practices}

### Kualiti Data

**Data latihan berkualiti tinggi adalah penting:**
- Pastikan contoh latihan yang pelbagai dan representatif
- Gunakan data khusus domain apabila boleh
- Sahkan output model guru sebelum menggunakannya untuk latihan pelajar
- Imbangi dataset untuk mengelakkan bias dalam pembelajaran model pelajar

### Penalaan Hiperparameter

**Parameter utama untuk dioptimumkan:**
- **Kadar pembelajaran**: Mulakan dengan kadar yang lebih kecil (1e-5 hingga 5e-5) untuk penalaan
- **Saiz batch**: Imbangi antara kekangan memori dan kestabilan latihan
- **Bilangan epoch**: Pantau untuk overfitting; biasanya 2-5 epoch mencukupi
- **Penskalaan suhu**: Laraskan kelembutan output guru untuk pemindahan pengetahuan yang lebih baik

### Pertimbangan Seni Bina Model

**Keserasian Guru-Pelajar:**
- Pastikan keserasian seni bina antara model guru dan pelajar
- Pertimbangkan padanan lapisan pertengahan untuk pemindahan pengetahuan yang lebih baik
- Gunakan teknik pemindahan perhatian apabila sesuai

### Strategi Penilaian

**Pendekatan penilaian yang komprehensif:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Aplikasi Dunia Sebenar {#real-world-applications}

### Penggunaan Mudah Alih dan Tepi

Model yang disuling membolehkan keupayaan AI pada peranti yang terhad sumber:
- **Aplikasi telefon pintar** dengan pemprosesan teks masa nyata
- **Peranti IoT** yang melakukan inferens tempatan
- **Sistem terbenam** dengan sumber pengiraan yang terhad

### Sistem Pengeluaran Kos Efektif

Organisasi menggunakan penyulingan untuk mengurangkan kos operasi:
- **Chatbot perkhidmatan pelanggan** dengan masa respons yang lebih pantas
- **Sistem moderasi kandungan** yang memproses jumlah tinggi dengan cekap
- **Perkhidmatan terjemahan masa nyata** dengan keperluan kelewatan yang lebih rendah

### Aplikasi Khusus Domain

Penyulingan membantu mencipta model khusus:
- **Bantuan diagnosis perubatan** dengan inferens tempatan yang melindungi privasi
- **Analisis dokumen undang-undang** yang dioptimumkan untuk domain undang-undang tertentu
- **Penilaian risiko kewangan** dengan keupayaan membuat keputusan yang pantas

### Kajian Kes: Sokongan Pelanggan dengan DeepSeek V3 → Phi-4-mini

Sebuah syarikat teknologi melaksanakan penyulingan untuk sistem sokongan pelanggan mereka:

**Butiran Pelaksanaan:**
- **Model Guru**: DeepSeek V3 (671B parameter) - cemerlang untuk penaakulan bagi pertanyaan pelanggan yang kompleks
- **Model Pelajar**: Phi-4-mini - dioptimumkan untuk inferens pantas dan penempatan
- **Data Latihan**: 50,000 perbualan sokongan pelanggan
- **Tugas**: Sokongan perbualan berbilang giliran dengan penyelesaian masalah teknikal

**Hasil Dicapai:**
- **Pengurangan 85%** dalam masa inferens (dari 3.2s kepada 0.48s setiap respons)
- **Pengurangan 95%** dalam keperluan memori (dari 1.2TB kepada 60GB)
- **Pengekalan 92%** ketepatan model asal pada tugas sokongan
- **Pengurangan 60%** dalam kos operasi
- **Peningkatan skalabiliti** - kini boleh menangani 10x lebih banyak pengguna serentak

**Pecahan Prestasi:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Kesimpulan {#conclusion}

Penyulingan model mewakili teknik penting untuk mendemokrasikan akses kepada keupayaan AI yang maju. Dengan membolehkan penciptaan model yang lebih kecil dan cekap yang mengekalkan sebahagian besar prestasi model yang lebih besar, penyulingan menangani keperluan yang semakin meningkat untuk penempatan AI yang praktikal.

### Pengajaran Utama

1. **Penyulingan merapatkan jurang** antara prestasi model dan kekangan praktikal
2. **Proses dua peringkat** memastikan pemindahan pengetahuan yang berkesan dari guru ke pelajar
3. **Azure ML menyediakan infrastruktur yang kukuh** untuk melaksanakan aliran kerja penyulingan
4. **Penilaian dan pengoptimuman yang betul** adalah penting untuk penyulingan yang berjaya
5. **Aplikasi dunia sebenar** menunjukkan manfaat yang signifikan dalam kos, kelajuan, dan kebolehcapaian

### Arah Masa Depan

Apabila bidang ini terus berkembang, kita boleh menjangkakan:
- **Teknik penyulingan yang lebih maju** dengan kaedah pemindahan pengetahuan yang lebih baik
- **Penyulingan berbilang guru** untuk keupayaan model pelajar yang dipertingkatkan
- **Pengoptimuman automatik** proses penyulingan
- **Sokongan model yang lebih luas** merentasi seni bina dan domain yang berbeza

Penyulingan model memperkasakan organisasi untuk memanfaatkan keupayaan AI terkini sambil mengekalkan kekangan penempatan yang praktikal, menjadikan model bahasa maju boleh diakses merentasi pelbagai aplikasi dan persekitaran.

## ➡️ Apa yang seterusnya

- [03: Penalaan - Menyesuaikan Model untuk Tugas Khusus](./03.SLMOps-Finetuing.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.