<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-18T14:04:29+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "ms"
}
-->
# Seksyen 3: Asas Keluarga Gemma

Keluarga model Gemma mewakili pendekatan komprehensif Google terhadap model bahasa besar sumber terbuka dan AI multimodal, menunjukkan bahawa model yang mudah diakses boleh mencapai prestasi luar biasa sambil boleh digunakan dalam pelbagai senario, daripada peranti mudah alih hingga stesen kerja perusahaan. Penting untuk memahami bagaimana keluarga Gemma membolehkan keupayaan AI yang kuat dengan pilihan penggunaan yang fleksibel sambil mengekalkan prestasi kompetitif dan amalan AI yang bertanggungjawab.

## Pengenalan

Dalam tutorial ini, kita akan meneroka keluarga model Gemma Google dan konsep asasnya. Kita akan membincangkan evolusi keluarga Gemma, metodologi latihan inovatif yang menjadikan model Gemma berkesan, varian utama dalam keluarga ini, dan aplikasi praktikal dalam pelbagai senario penggunaan.

## Objektif Pembelajaran

Pada akhir tutorial ini, anda akan dapat:

- Memahami falsafah reka bentuk dan evolusi keluarga model Gemma Google
- Mengenal pasti inovasi utama yang membolehkan model Gemma mencapai prestasi tinggi dalam pelbagai saiz parameter
- Mengenali manfaat dan batasan varian model Gemma yang berbeza
- Mengaplikasikan pengetahuan tentang model Gemma untuk memilih varian yang sesuai bagi senario dunia sebenar

## Memahami Lanskap Model AI Moden

Lanskap AI telah berkembang dengan ketara, dengan pelbagai organisasi mengejar pendekatan yang berbeza dalam pembangunan model bahasa. Ada yang menumpukan pada model tertutup yang hanya boleh diakses melalui API, sementara yang lain menekankan aksesibiliti dan ketelusan sumber terbuka. Pendekatan tradisional melibatkan sama ada model proprietari besar dengan kos berterusan atau model sumber terbuka yang mungkin memerlukan kepakaran teknikal yang signifikan untuk digunakan.

Paradigma ini mencipta cabaran bagi organisasi yang mencari keupayaan AI yang kuat sambil mengekalkan kawalan terhadap data, kos, dan fleksibiliti penggunaan. Pendekatan konvensional sering memerlukan pilihan antara prestasi canggih dan pertimbangan penggunaan praktikal.

## Cabaran Kecemerlangan AI yang Mudah Diakses

Keperluan untuk AI berkualiti tinggi dan mudah diakses menjadi semakin penting dalam pelbagai senario. Pertimbangkan aplikasi yang memerlukan pilihan penggunaan fleksibel untuk keperluan organisasi yang berbeza, pelaksanaan kos efektif di mana kos API boleh menjadi signifikan, keupayaan multimodal untuk pemahaman yang komprehensif, atau penggunaan khusus pada peranti mudah alih dan tepi.

### Keperluan Penggunaan Utama

Penggunaan AI moden menghadapi beberapa keperluan asas yang mengehadkan kebolehgunaan praktikal:

- **Aksesibiliti**: Ketersediaan sumber terbuka untuk ketelusan dan penyesuaian
- **Keberkesanan Kos**: Keperluan pengiraan yang munasabah untuk pelbagai bajet
- **Fleksibiliti**: Pelbagai saiz model untuk senario penggunaan yang berbeza
- **Pemahaman Multimodal**: Keupayaan pemprosesan visi, teks, dan audio
- **Penggunaan Tepi**: Prestasi yang dioptimumkan pada peranti mudah alih dan terhad sumber

## Falsafah Model Gemma

Keluarga model Gemma mewakili pendekatan komprehensif Google terhadap pembangunan model AI, memprioritaskan aksesibiliti sumber terbuka, keupayaan multimodal, dan penggunaan praktikal sambil mengekalkan ciri prestasi kompetitif. Model Gemma mencapai ini melalui pelbagai saiz model, metodologi latihan berkualiti tinggi yang berasal dari penyelidikan Gemini, dan varian khusus untuk domain dan senario penggunaan yang berbeza.

Keluarga Gemma merangkumi pelbagai pendekatan yang direka untuk menyediakan pilihan di seluruh spektrum prestasi-kecekapan, membolehkan penggunaan daripada peranti mudah alih hingga pelayan perusahaan sambil menyediakan keupayaan AI yang bermakna. Matlamatnya adalah untuk mendemokrasikan akses kepada teknologi AI berkualiti tinggi sambil menyediakan fleksibiliti dalam pilihan penggunaan.

### Prinsip Reka Bentuk Teras Gemma

Model Gemma dibina berdasarkan beberapa prinsip asas yang membezakannya daripada keluarga model bahasa lain:

- **Sumber Terbuka Pertama**: Ketelusan dan aksesibiliti lengkap untuk penyelidikan dan penggunaan komersial
- **Pembangunan Berasaskan Penyelidikan**: Dibina menggunakan penyelidikan dan teknologi yang sama yang menggerakkan model Gemini
- **Seni Bina Boleh Skala**: Pelbagai saiz model untuk memenuhi keperluan pengiraan yang berbeza
- **AI Bertanggungjawab**: Langkah keselamatan bersepadu dan amalan pembangunan yang bertanggungjawab

## Teknologi Utama yang Memungkinkan Keluarga Gemma

### Metodologi Latihan Lanjutan

Salah satu aspek yang menentukan keluarga Gemma adalah pendekatan latihan canggih yang berasal dari penyelidikan Gemini Google. Model Gemma memanfaatkan distilasi daripada model yang lebih besar, pembelajaran pengukuhan daripada maklum balas manusia (RLHF), dan teknik penggabungan model untuk mencapai prestasi yang dipertingkatkan dalam matematik, pengekodan, dan pengikut arahan.

Proses latihan melibatkan distilasi daripada model arahan yang lebih besar, pembelajaran pengukuhan daripada maklum balas manusia (RLHF) untuk sejajar dengan keutamaan manusia, pembelajaran pengukuhan daripada maklum balas mesin (RLMF) untuk penaakulan matematik, dan pembelajaran pengukuhan daripada maklum balas pelaksanaan (RLEF) untuk keupayaan pengekodan.

### Integrasi dan Pemahaman Multimodal

Model Gemma terkini menggabungkan keupayaan multimodal canggih yang membolehkan pemahaman komprehensif merentasi jenis input yang berbeza:

**Integrasi Visi-Bahasa (Gemma 3)**: Gemma 3 boleh memproses teks dan imej secara serentak, membolehkannya menganalisis imej, menjawab soalan tentang kandungan visual, mengekstrak teks daripada imej, dan memahami data visual yang kompleks.

**Pemprosesan Audio (Gemma 3n)**: Gemma 3n mempunyai keupayaan audio lanjutan termasuk pengiktirafan pertuturan automatik (ASR) dan terjemahan pertuturan automatik (AST), dengan prestasi yang sangat baik untuk terjemahan antara bahasa Inggeris dan Sepanyol, Perancis, Itali, dan Portugis.

**Pemprosesan Input Berselang**: Model Gemma menyokong input berselang antara mod, membolehkan pemahaman interaksi multimodal yang kompleks di mana teks, imej, dan audio boleh diproses bersama.

### Inovasi Seni Bina

Keluarga Gemma menggabungkan beberapa pengoptimuman seni bina yang direka untuk prestasi dan kecekapan:

**Pengembangan Tingkap Konteks**: Model Gemma 3 mempunyai tingkap konteks 128K-token, 16x lebih besar daripada model Gemma sebelumnya, membolehkan pemprosesan sejumlah besar maklumat termasuk pelbagai dokumen atau ratusan imej.

**Seni Bina Mobile-First (Gemma 3n)**: Gemma 3n memanfaatkan teknologi Per-Layer Embeddings (PLE) dan seni bina MatFormer, membolehkan model yang lebih besar berjalan dengan jejak memori yang setanding dengan model tradisional yang lebih kecil.

**Keupayaan Panggilan Fungsi**: Gemma 3 menyokong panggilan fungsi, membolehkan pembangun membina antara muka bahasa semula jadi untuk antara muka pengaturcaraan dan mencipta sistem automasi pintar.

## Saiz Model dan Pilihan Penggunaan

Persekitaran penggunaan moden mendapat manfaat daripada fleksibiliti model Gemma merentasi pelbagai keperluan pengiraan:

### Model Kecil (0.6B-4B)

Gemma menyediakan model kecil yang cekap sesuai untuk penggunaan tepi, aplikasi mudah alih, dan persekitaran terhad sumber sambil mengekalkan keupayaan yang mengagumkan. Model 1B adalah ideal untuk aplikasi kecil, sementara model 4B menawarkan prestasi dan fleksibiliti yang seimbang dengan sokongan multimodal.

### Model Sederhana (8B-14B)

Model pertengahan menawarkan keupayaan yang dipertingkatkan untuk aplikasi profesional, memberikan keseimbangan yang sangat baik antara prestasi dan keperluan pengiraan untuk penggunaan stesen kerja dan pelayan.

### Model Besar (27B+)

Model berskala penuh memberikan prestasi canggih untuk aplikasi yang menuntut, penyelidikan, dan penggunaan perusahaan yang memerlukan keupayaan maksimum. Model 27B mewakili pilihan paling mampu yang masih boleh berjalan pada satu GPU.

### Model Dioptimumkan untuk Mudah Alih (Gemma 3n)

Model Gemma 3n E2B dan E4B direka khusus untuk penggunaan mudah alih dan tepi, dengan jumlah parameter efektif sebanyak 2B dan 4B masing-masing, sambil menggunakan seni bina inovatif untuk meminimumkan jejak memori kepada serendah 2GB untuk E2B dan 3GB untuk E4B.

## Manfaat Keluarga Model Gemma

### Aksesibiliti Sumber Terbuka

Model Gemma menyediakan ketelusan dan keupayaan penyesuaian lengkap dengan berat terbuka yang membenarkan penggunaan komersial yang bertanggungjawab, membolehkan organisasi menyesuaikan dan menggunakannya dalam projek dan aplikasi mereka sendiri.

### Fleksibiliti Penggunaan

Pelbagai saiz model membolehkan penggunaan merentasi konfigurasi perkakasan yang pelbagai, daripada peranti mudah alih hingga pelayan berprestasi tinggi, dengan pengoptimuman untuk pelbagai platform termasuk Google Cloud TPUs, NVIDIA GPUs, AMD GPUs melalui ROCm, dan pelaksanaan CPU melalui Gemma.cpp.

### Kecemerlangan Multilingual

Model Gemma cemerlang dalam pemahaman dan penjanaan pelbagai bahasa, menyokong lebih daripada 140 bahasa dengan keupayaan pelbagai bahasa yang tiada tandingan, menjadikannya sesuai untuk aplikasi global.

### Prestasi Kompetitif

Model Gemma secara konsisten mencapai keputusan kompetitif dalam penanda aras, dengan Gemma 3 menduduki tempat tinggi di kalangan model proprietari dan terbuka yang popular dalam penilaian keutamaan pengguna.

### Keupayaan Khusus

Aplikasi khusus domain mendapat manfaat daripada pemahaman multimodal Gemma, keupayaan panggilan fungsi, dan prestasi yang dioptimumkan merentasi pelbagai platform perkakasan.

## Contoh Praktikal dan Kes Penggunaan

Sebelum menyelami butiran teknikal, mari kita terokai beberapa contoh konkrit tentang apa yang boleh dicapai oleh model Gemma:

### Contoh Pemahaman Multimodal

Gemma 3 boleh memproses teks dan imej secara serentak untuk analisis komprehensif:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### Contoh Sokongan Multilingual

Model Gemma menunjukkan keupayaan pelbagai bahasa yang kuat merentasi lebih daripada 140 bahasa:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### Contoh Panggilan Fungsi

Gemma 3 menyokong panggilan fungsi untuk membina antara muka bahasa semula jadi:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### Contoh Penggunaan Mudah Alih (Gemma 3n)

Gemma 3n dioptimumkan untuk penggunaan mudah alih dan tepi dengan penggunaan memori yang cekap:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### Contoh Pemprosesan Audio (Gemma 3n)

Gemma 3n termasuk keupayaan audio lanjutan untuk pengiktirafan pertuturan dan terjemahan:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Evolusi Keluarga Gemma

### Gemma 1.0 dan 2.0: Model Asas

Model Gemma awal menetapkan prinsip asas aksesibiliti sumber terbuka dan penggunaan praktikal:

- **Gemma-2B dan 7B**: Pelepasan awal yang menumpukan pada pemahaman bahasa yang cekap
- **Siri Gemma 1.5**: Pengendalian konteks yang diperluas dan prestasi yang dipertingkatkan
- **Keluarga Gemma 2**: Pengenalan keupayaan multimodal dan saiz model yang diperluas

### Gemma 3: Kecemerlangan Multimodal

Siri Gemma 3 menandakan kemajuan yang signifikan dalam keupayaan multimodal dan prestasi. Dibina daripada penyelidikan dan teknologi yang sama yang menggerakkan model Gemini 2.0, Gemma 3 memperkenalkan pemahaman visi-bahasa, tingkap konteks 128K-token, panggilan fungsi, dan sokongan untuk lebih daripada 140 bahasa.

Ciri utama Gemma 3 termasuk:
- **Gemma 3-1B hingga 27B**: Julat komprehensif untuk keperluan penggunaan yang berbeza
- **Pemahaman Multimodal**: Keupayaan penaakulan teks dan visual yang canggih
- **Konteks Diperluas**: Keupayaan pemprosesan 128K-token
- **Panggilan Fungsi**: Pembinaan antara muka bahasa semula jadi
- **Latihan Dipertingkatkan**: Dioptimumkan menggunakan distilasi dan pembelajaran pengukuhan

### Gemma 3n: Inovasi Mobile-First

Gemma 3n mewakili kejayaan dalam seni bina AI mobile-first, menampilkan teknologi Per-Layer Embeddings (PLE) yang inovatif, seni bina MatFormer untuk fleksibiliti pengiraan, dan keupayaan multimodal komprehensif termasuk pemprosesan audio.

Inovasi Gemma 3n termasuk:
- **Model E2B dan E4B**: Prestasi parameter efektif 2B dan 4B dengan jejak memori yang dikurangkan
- **Keupayaan Audio**: ASR berkualiti tinggi dan terjemahan pertuturan
- **Pemahaman Video**: Keupayaan pemprosesan video yang dipertingkatkan dengan ketara
- **Pengoptimuman Mudah Alih**: Direka untuk AI masa nyata pada telefon dan tablet

## Aplikasi Model Gemma

### Aplikasi Perusahaan

Organisasi menggunakan model Gemma untuk analisis dokumen dengan kandungan visual, automasi perkhidmatan pelanggan dengan sokongan multimodal, bantuan pengekodan pintar, dan aplikasi kecerdasan perniagaan. Sifat sumber terbuka membolehkan penyesuaian untuk keperluan perniagaan tertentu sambil mengekalkan privasi dan kawalan data.

### Pengkomputeran Mudah Alih dan Tepi

Aplikasi mudah alih memanfaatkan Gemma 3n untuk AI masa nyata yang beroperasi secara langsung pada peranti, membolehkan pengalaman peribadi dan peribadi dengan keupayaan AI multimodal yang pantas. Aplikasi termasuk terjemahan masa nyata, pembantu pintar, penjanaan kandungan, dan cadangan yang diperibadikan.

### Teknologi Pendidikan

Platform pendidikan menggunakan model Gemma untuk pengalaman pembelajaran multimodal, penjanaan kandungan automatik dengan elemen visual, bantuan pembelajaran bahasa dengan pemprosesan audio, dan pengalaman pendidikan interaktif yang menggabungkan teks, imej, dan pertuturan.

### Aplikasi Global

Aplikasi antarabangsa mendapat manfaat daripada keupayaan pelbagai bahasa dan silang budaya model Gemma, membolehkan pengalaman AI yang konsisten merentasi bahasa dan konteks budaya yang berbeza dengan pemahaman visual dan audio.

## Cabaran dan Batasan

### Keperluan Pengiraan

Walaupun Gemma menyediakan model merentasi pelbagai saiz, varian yang lebih besar masih memerlukan sumber pengiraan yang signifikan untuk prestasi optimum. Keperluan memori berkisar dari kira-kira 2GB untuk model kecil yang dikuantifikasi hingga 54GB untuk model terbesar 27B.

### Prestasi Domain Khusus

Walaupun model Gemma berprestasi baik merentasi domain umum dan tugas multimodal, aplikasi yang sangat khusus mungkin mendapat manfaat daripada penyesuaian domain atau pengoptimuman tugas tertentu.

### Kerumitan Pemilihan Model

Julat model, varian, dan pilihan penggunaan yang tersedia boleh menjadikan pemilihan mencabar bagi pengguna yang baru dalam ekosistem ini, memerlukan pertimbangan yang teliti terhadap kompromi prestasi-kecekapan.

### Pengoptimuman Perkakasan

Walaupun model Gemma dioptimumkan untuk pelbagai platform termasuk NVIDIA GPUs, Google Cloud TPUs, dan AMD GPUs, prestasi mungkin berbeza merentasi konfigurasi perkakasan yang berbeza.

## Masa Depan Keluarga Model Gemma

Keluarga model Gemma mewakili evolusi berterusan ke arah AI berkualiti tinggi yang didemokrasikan dengan pembangunan berterusan pengoptimuman kecekapan yang dipertingkatkan, keupayaan multimodal yang diperluas, dan integrasi yang lebih baik merentasi senario penggunaan yang berbeza.

Perkembangan masa depan termasuk integrasi seni bina Gemma 3n ke dalam platform utama seperti Android dan Chrome, membolehkan pengalaman AI yang mudah diakses merentasi pelbagai peranti dan aplikasi.

Apabila teknologi terus berkembang, kita boleh menjangkakan model Gemma menjadi semakin mampu sambil mengekalkan aksesibiliti sumber terbuka mereka, membolehkan penggunaan AI merentasi pelbagai senario dan kes penggunaan daripada aplikasi mudah alih hingga sistem perusahaan.

## Contoh Pembangunan dan Integrasi

### Permulaan Pantas dengan Transformers

Berikut adalah cara untuk memulakan dengan model Gemma menggunakan perpustakaan Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3-8B model
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation
messages = [
    {"role": "user", "content": "Explain quantum computing and its potential applications."}
]

# Generate response
input_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Penggunaan Multimodal dengan Gemma 3

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

# Load Gemma 3 vision model
model_name = "google/gemma-3-4b-it"
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# Process image and text input
image = Image.open("chart.jpg")
messages = [
    {
        "role": "user", 
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Analyze this chart and explain the key trends you observe."}
        ]
    }
]

# Prepare inputs
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

# Generate multimodal response
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

response = processor.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Pelaksanaan Panggilan Fungsi

```python
import json

# Define available functions
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    },
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    }
]

def gemma_function_calling(user_query, available_functions):
    """Implement function calling with Gemma 3"""
    
    # Create system prompt with function definitions
    system_prompt = f"""You are a helpful assistant with access to the following functions:

{json.dumps(available_functions, indent=2)}

When the user asks for something that requires a function call, respond with a JSON object containing:
- "function_name": the name of the function to call
- "parameters": the parameters to pass to the function

If no function call is needed, respond normally."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    # Process with Gemma
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        temperature=0.3  # Lower temperature for more structured output
    )
    
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    # Parse function call if present
    try:
        function_call = json.loads(response.strip())
        if "function_name" in function_call:
            return function_call
    except json.JSONDecodeError:
        pass
    
    return {"response": response}

# Example usage
user_request = "What's the weather like in Tokyo and calculate 15% of 850"
result = gemma_function_calling(user_request, functions)
print(result)
```

### 📱 Penggunaan Mudah Alih dengan Gemma 3n

```python
# Optimized mobile deployment
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class MobileGemmaService:
    """Optimized Gemma 3n service for mobile deployment"""
    
    def __init__(self, model_name="google/gemma-3n-E2B-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._load_optimized_model()
    
    def _load_optimized_model(self):
        """Load model with mobile optimizations"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load with optimizations for mobile
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # Quantization for efficiency
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if torch.cuda.is_available():
            self.model = torch.jit.trace(self.model, example_inputs=...)  # JIT optimization
    
    def mobile_chat(self, user_input, max_tokens=150):
        """Optimized chat for mobile devices"""
        messages = [{"role": "user", "content": user_input}]
        
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            max_length=512,
            truncation=True
        )
        
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(input_text, "").strip()
    
    def process_audio(self, audio_path, task="transcribe"):
        """Process audio input with Gemma 3n"""
        if task == "transcribe":
            prompt = f"<audio>{audio_path}</audio>\nTranscribe:"
        elif task == "translate":
            prompt = f"<audio>{audio_path}</audio>\nTranslate to English:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return result.replace(prompt, "").strip()

# Initialize mobile service
mobile_gemma = MobileGemmaService()

# Example mobile usage
quick_response = mobile_gemma.mobile_chat("Summarize renewable energy benefits in 2 sentences")
print(f"Mobile Response: {quick_response}")

# Audio processing example
# audio_transcript = mobile_gemma.process_audio("voice_note.wav", task="transcribe")
# print(f"Audio Transcript: {audio_transcript}")
```

### Penggunaan API dengan vLLM

```python
from vllm import LLM, SamplingParams
import asyncio
from typing import List, Dict

class GemmaAPIService:
    """High-performance Gemma API service using vLLM"""
    
    def __init__(self, model_name="google/gemma-3-8b-it"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            trust_remote_code=True
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop_token_ids=[self.llm.get_tokenizer().eos_token_id]
        )
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for API processing"""
        tokenizer = self.llm.get_tokenizer()
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_batch(self, batch_messages: List[List[Dict[str, str]]]) -> List[str]:
        """Generate responses for batch of conversations"""
        formatted_prompts = [self.format_messages(messages) for messages in batch_messages]
        
        # Generate responses
        outputs = self.llm.generate(formatted_prompts, self.sampling_params)
        
        # Extract responses
        responses = []
        for output in outputs:
            response = output.outputs[0].text.strip()
            responses.append(response)
        
        return responses
    
    async def stream_generate(self, messages: List[Dict[str, str]]):
        """Stream generation for real-time applications"""
        formatted_prompt = self.format_messages(messages)
        
        # Note: vLLM streaming implementation would go here
        # This is a simplified example
        outputs = self.llm.generate([formatted_prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        # Simulate streaming by yielding chunks
        words = response.split()
        for i in range(0, len(words), 3):  # Yield 3 words at a time
            chunk = " ".join(words[i:i+3])
            yield chunk
            await asyncio.sleep(0.1)  # Simulate streaming delay

# Example API usage
async def api_example():
    api_service = GemmaAPIService()
    
    # Batch processing
    batch_conversations = [
        [{"role": "user", "content": "Explain machine learning briefly"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}],
        [{"role": "user", "content": "How does photosynthesis work?"}]
    ]
    
    responses = await api_service.generate_batch(batch_conversations)
    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response}")
    
    # Streaming example
    print("\nStreaming response:")
    stream_messages = [{"role": "user", "content": "Write a short story about AI and humans working together"}]
    async for chunk in api_service.stream_generate(stream_messages):
        print(chunk, end=" ", flush=True)

# Run API example
# asyncio.run(api_example())
```

## Penanda Aras Prestasi dan Pencapaian

Keluarga model Gemma telah mencapai prestasi luar biasa merentasi pelbagai penanda aras sambil mengekalkan aksesibiliti sumber terbuka dan ciri penggunaan yang cekap:

### Sorotan Prestasi Utama

**Kecemerlangan Multimodal:**
- Gemma 3 memberikan keupayaan yang hebat untuk pembangun dengan kemampuan penalaran teks dan visual yang maju, menyokong input imej dan teks untuk pemahaman multimodal.
- Gemma 3n mendapat kedudukan tinggi di kalangan model proprietari dan terbuka yang popular dalam skor Elo Chatbot Arena, menunjukkan keutamaan pengguna yang kuat.

**Pencapaian Kecekapan:**
- Model Gemma 3 boleh mengendalikan input arahan sehingga 128K token, iaitu 16x lebih besar daripada tetingkap konteks model Gemma sebelumnya.
- Gemma 3n menggunakan Per-Layer Embeddings (PLE) yang memberikan pengurangan penggunaan RAM yang ketara sambil mengekalkan keupayaan model yang lebih besar.

**Pengoptimuman Mudah Alih:**
- Gemma 3n E2B beroperasi dengan hanya 2GB memori manakala E4B hanya memerlukan 3GB, walaupun mempunyai jumlah parameter mentah sebanyak 5B dan 8B masing-masing.
- Keupayaan AI masa nyata terus pada peranti mudah alih dengan operasi yang mengutamakan privasi dan bersedia secara luar talian.

**Skala Latihan:**
- Gemma 3 dilatih pada 2T token untuk model 1B, 4T untuk 4B, 12T untuk 12B, dan 14T token untuk model 27B menggunakan Google TPUs dan JAX Framework.

### Matriks Perbandingan Model

| Siri Model   | Julat Parameter | Panjang Konteks | Kekuatan Utama | Kes Penggunaan Terbaik |
|--------------|------------------|----------------|---------------|----------------|
| **Gemma 3**  | 1B-27B          | 128K          | Pemahaman multimodal, pemanggilan fungsi | Aplikasi umum, tugas bahasa-visual |
| **Gemma 3n** | E2B (5B), E4B (8B) | Berubah-ubah | Pengoptimuman mudah alih, pemprosesan audio | Aplikasi mudah alih, pengkomputeran tepi, AI masa nyata |
| **Gemma 2.5**| 0.5B-72B        | 32K-128K      | Prestasi seimbang, pelbagai bahasa | Penggunaan produksi, aliran kerja sedia ada |
| **Gemma-VL** | Pelbagai        | Berubah-ubah  | Pengkhususan bahasa-visual | Analisis imej, menjawab soalan visual |

## Panduan Pemilihan Model

### Untuk Aplikasi Asas
- **Gemma 3-1B**: Tugas teks ringan, aplikasi mudah alih yang mudah.
- **Gemma 3-4B**: Prestasi seimbang dengan sokongan multimodal untuk kegunaan umum.

### Untuk Aplikasi Multimodal
- **Gemma 3-4B/12B**: Pemahaman imej, menjawab soalan visual.
- **Gemma 3n**: Aplikasi multimodal mudah alih dengan keupayaan pemprosesan audio.

### Untuk Penggunaan Mudah Alih dan Tepi
- **Gemma 3n E2B**: Peranti dengan sumber terhad, AI mudah alih masa nyata.
- **Gemma 3n E4B**: Prestasi mudah alih yang dipertingkatkan dengan keupayaan audio.

### Untuk Penggunaan Perusahaan
- **Gemma 3-12B/27B**: Pemahaman bahasa dan visual berprestasi tinggi.
- **Keupayaan pemanggilan fungsi**: Membina sistem automasi pintar.

### Untuk Aplikasi Global
- **Mana-mana varian Gemma 3**: Sokongan lebih daripada 140 bahasa dengan pemahaman budaya.
- **Gemma 3n**: Aplikasi global yang mengutamakan mudah alih dengan terjemahan audio.

## Platform Penggunaan dan Kebolehcapaian

### Platform Awan
- **Vertex AI**: Keupayaan MLOps hujung ke hujung dengan pengalaman tanpa pelayan.
- **Google Kubernetes Engine (GKE)**: Penggunaan kontena yang boleh diskalakan untuk beban kerja kompleks.
- **Google GenAI API**: Akses API langsung untuk prototaip pantas.
- **NVIDIA API Catalog**: Prestasi yang dioptimumkan pada GPU NVIDIA.

### Rangka Kerja Pembangunan Tempatan
- **Hugging Face Transformers**: Integrasi standard untuk pembangunan.
- **Ollama**: Penggunaan tempatan yang dipermudahkan dan pengurusan.
- **vLLM**: Penyajian berprestasi tinggi untuk produksi.
- **Gemma.cpp**: Pelaksanaan yang dioptimumkan CPU.
- **Google AI Edge**: Pengoptimuman penggunaan mudah alih dan tepi.

### Sumber Pembelajaran
- **Google AI Studio**: Cuba model Gemma dengan hanya beberapa klik.
- **Kaggle dan Hugging Face**: Muat turun berat model dan contoh komuniti.
- **Laporan Teknikal**: Dokumentasi dan kertas penyelidikan yang komprehensif.
- **Forum Komuniti**: Sokongan komuniti aktif dan perbincangan.

### Memulakan Model Gemma

#### Platform Pembangunan
1. **Google AI Studio**: Mulakan dengan eksperimen berasaskan web.
2. **Hugging Face Hub**: Terokai model dan pelaksanaan komuniti.
3. **Penggunaan Tempatan**: Gunakan Ollama atau Transformers untuk pembangunan.

#### Laluan Pembelajaran
1. **Fahami Konsep Asas**: Kajian keupayaan multimodal dan pilihan penggunaan.
2. **Eksperimen dengan Varian**: Cuba saiz model yang berbeza dan versi khusus.
3. **Latih Kemahiran Pelaksanaan**: Gunakan model dalam persekitaran pembangunan.
4. **Optimumkan untuk Produksi**: Laraskan untuk kes penggunaan dan platform tertentu.

#### Amalan Terbaik
- **Mulakan Kecil**: Mulakan dengan Gemma 3-4B untuk pembangunan dan ujian awal.
- **Gunakan Templat Rasmi**: Terapkan templat sembang yang sesuai untuk hasil optimum.
- **Pantau Sumber**: Jejak penggunaan memori dan prestasi inferens.
- **Pertimbangkan Pengkhususan**: Pilih varian yang sesuai untuk keperluan multimodal atau mudah alih.

## Corak Penggunaan Lanjutan

### Contoh Penalaan

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```

### Kejuruteraan Arahan Khusus

**Untuk Tugas Multimodal:**
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```

**Untuk Pemanggilan Fungsi dengan Konteks:**
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```

### Aplikasi Pelbagai Bahasa dengan Konteks Budaya

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```

### Corak Penggunaan Produksi

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```

## Strategi Pengoptimuman Prestasi

### Pengoptimuman Memori

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```

### Pengoptimuman Inferens

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```

## Amalan Terbaik dan Garis Panduan

### Keselamatan dan Privasi

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```

### Pemantauan dan Penilaian

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```

## Kesimpulan

Keluarga model Gemma mewakili pendekatan komprehensif Google untuk mendemokrasikan teknologi AI sambil mengekalkan prestasi kompetitif merentasi pelbagai aplikasi dan senario penggunaan. Melalui komitmennya terhadap kebolehcapaian sumber terbuka, keupayaan multimodal, dan reka bentuk seni bina inovatif, Gemma membolehkan organisasi dan pembangun memanfaatkan keupayaan AI yang hebat tanpa mengira sumber atau keperluan khusus mereka.

### Poin Penting

**Kecemerlangan Sumber Terbuka**: Gemma menunjukkan bahawa model sumber terbuka boleh mencapai prestasi yang kompetitif dengan alternatif proprietari sambil memberikan ketelusan, penyesuaian, dan kawalan terhadap penggunaan AI.

**Inovasi Multimodal**: Integrasi keupayaan teks, visual, dan audio dalam Gemma 3 dan Gemma 3n mewakili kemajuan yang signifikan dalam AI multimodal yang boleh diakses, membolehkan pemahaman yang komprehensif merentasi jenis input yang berbeza.

**Seni Bina Mudah Alih**: Teknologi Per-Layer Embeddings (PLE) dan pengoptimuman mudah alih Gemma 3n menunjukkan bahawa AI yang hebat boleh beroperasi dengan cekap pada peranti dengan sumber terhad tanpa mengorbankan keupayaan.

**Penggunaan yang Boleh Diskalakan**: Julat dari 1B hingga 27B parameter, dengan varian mudah alih khusus, membolehkan penggunaan merentasi spektrum penuh persekitaran pengkomputeran sambil mengekalkan kualiti dan prestasi yang konsisten.

**Integrasi AI yang Bertanggungjawab**: Langkah keselamatan terbina melalui ShieldGemma 2 dan amalan pembangunan yang bertanggungjawab memastikan keupayaan AI yang hebat boleh digunakan dengan selamat dan beretika.

### Pandangan Masa Depan

Apabila keluarga Gemma terus berkembang, kita boleh menjangkakan:

**Keupayaan Mudah Alih yang Dipertingkatkan**: Pengoptimuman lanjut untuk penggunaan mudah alih dan tepi dengan integrasi seni bina Gemma 3n ke dalam platform utama seperti Android dan Chrome.

**Pemahaman Multimodal yang Diperluas**: Kemajuan berterusan dalam integrasi bahasa-visual-audio untuk pengalaman AI yang lebih komprehensif.

**Kecekapan yang Diperbaiki**: Inovasi seni bina yang berterusan untuk memberikan nisbah prestasi-per-parameter yang lebih baik dan keperluan pengkomputeran yang dikurangkan.

**Integrasi Ekosistem yang Lebih Luas**: Sokongan yang dipertingkatkan merentasi rangka kerja pembangunan, platform awan, dan alat penggunaan untuk integrasi yang lancar ke dalam aliran kerja sedia ada.

**Pertumbuhan Komuniti**: Pengembangan berterusan Gemmaverse dengan model, alat, dan aplikasi yang dicipta oleh komuniti yang memperluaskan keupayaan teras.

### Langkah Seterusnya

Sama ada anda membina aplikasi mudah alih dengan keupayaan AI masa nyata, membangunkan alat pendidikan multimodal, mencipta sistem automasi pintar, atau bekerja pada aplikasi global yang memerlukan sokongan pelbagai bahasa, keluarga Gemma menyediakan penyelesaian yang boleh diskalakan dengan sokongan komuniti yang kuat dan dokumentasi yang komprehensif.

**Cadangan Memulakan:**
1. **Eksperimen dengan Google AI Studio** untuk pengalaman langsung segera.
2. **Muat turun model dari Hugging Face** untuk pembangunan tempatan dan penyesuaian.
3. **Terokai varian khusus** seperti Gemma 3n untuk aplikasi mudah alih.
4. **Laksanakan keupayaan multimodal** untuk pengalaman AI yang komprehensif.
5. **Ikuti amalan keselamatan terbaik** untuk penggunaan produksi.

**Untuk Pembangunan Mudah Alih**: Mulakan dengan Gemma 3n E2B untuk penggunaan yang cekap sumber dengan keupayaan audio dan visual.

**Untuk Aplikasi Perusahaan**: Pertimbangkan model Gemma 3-12B atau 27B untuk keupayaan maksimum dengan pemanggilan fungsi dan penalaran maju.

**Untuk Aplikasi Global**: Manfaatkan sokongan lebih daripada 140 bahasa Gemma dengan kejuruteraan arahan yang sedar budaya.

**Untuk Kes Penggunaan Khusus**: Terokai pendekatan penalaan dan teknik pengoptimuman khusus domain.

### 🔮 Pendemokrasian AI

Keluarga Gemma mencerminkan masa depan pembangunan AI di mana model yang hebat dan berkemampuan boleh diakses oleh semua orang daripada pembangun individu kepada perusahaan besar. Dengan menggabungkan penyelidikan terkini dengan kebolehcapaian sumber terbuka, Google telah mencipta asas yang membolehkan inovasi merentasi semua sektor dan skala.

Kejayaan Gemma dengan lebih daripada 100 juta muat turun dan 60,000+ varian komuniti menunjukkan kuasa kolaborasi terbuka dalam memajukan teknologi AI. Apabila kita bergerak ke hadapan, keluarga Gemma akan terus berfungsi sebagai pemangkin untuk inovasi AI, membolehkan pembangunan aplikasi yang sebelum ini hanya mungkin dengan model proprietari yang mahal.

Masa depan AI adalah terbuka, boleh diakses, dan berkuasa – dan keluarga Gemma sedang memimpin jalan untuk menjadikan visi ini satu realiti.

## Sumber Tambahan

**Dokumentasi dan Model Rasmi:**
- **Google AI Studio**: [Cuba model Gemma secara langsung](https://aistudio.google.com)
- **Koleksi Hugging Face**: 
  - [Gemma 3 Release](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
  - [Gemma 3n Preview](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Dokumentasi Pembangun Google AI**: [Panduan Gemma yang komprehensif](https://ai.google.dev/gemma)
- **Dokumentasi Vertex AI**: [Panduan penggunaan perusahaan](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)

**Sumber Teknikal:**
- **Kertas Penyelidikan dan Laporan Teknikal**: [Penerbitan Google DeepMind](https://deepmind.google/models/gemma/)
- **Blog Pembangun**: [Pengumuman dan tutorial terkini](https://developers.googleblog.com)
- **Kad Model**: Spesifikasi teknikal terperinci dan penanda aras prestasi

**Komuniti dan Sokongan:**
- **Komuniti Hugging Face**: Perbincangan aktif dan contoh komuniti
- **Repositori GitHub**: Pelaksanaan dan alat sumber terbuka
- **Forum Pembangun**: Sokongan komuniti pembangun Google AI
- **Stack Overflow**: Soalan yang ditandai dan penyelesaian komuniti

**Alat Pembangunan:**
- **Ollama**: [Penggunaan tempatan yang mudah](https://ollama.ai)
- **vLLM**: [Penyajian berprestasi tinggi](https://github.com/vllm-project/vllm)
- **Perpustakaan Transformers**: [Integrasi Hugging Face](https://huggingface.co/docs/transformers)
- **Google AI Edge**: Pengoptimuman penggunaan mudah alih dan tepi

**Laluan Pembelajaran:**
- **Pemula**: Mulakan dengan Google AI Studio → Contoh Hugging Face → Penggunaan tempatan
- **Pembangun**: Integrasi Transformers → Aplikasi tersuai → Penggunaan produksi
- **Penyelidik**: Kertas teknikal → Penalaan → Aplikasi baru
- **Perusahaan**: Penggunaan Vertex AI → Pelaksanaan keselamatan → Pengoptimuman skala

Keluarga model Gemma bukan sekadar koleksi model AI, tetapi ekosistem lengkap untuk membina masa depan aplikasi AI yang boleh diakses, berkuasa, dan bertanggungjawab. Mula meneroka hari ini dan sertai komuniti pembangun dan penyelidik yang semakin berkembang yang mendorong batasan apa yang mungkin dengan AI sumber terbuka.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.