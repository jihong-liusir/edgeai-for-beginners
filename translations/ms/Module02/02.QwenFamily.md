<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T13:51:10+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ms"
}
-->
# Seksyen 2: Asas Keluarga Qwen

Keluarga model Qwen mewakili pendekatan komprehensif Alibaba Cloud terhadap model bahasa besar dan AI multimodal, menunjukkan bahawa model sumber terbuka boleh mencapai prestasi luar biasa sambil boleh diakses dalam pelbagai senario pelaksanaan. Penting untuk memahami bagaimana keluarga Qwen membolehkan keupayaan AI yang kuat dengan pilihan pelaksanaan yang fleksibel sambil mengekalkan prestasi kompetitif dalam pelbagai tugas.

## Sumber untuk Pembangun

### Repositori Model Hugging Face
Model terpilih dari keluarga Qwen tersedia melalui [Hugging Face](https://huggingface.co/models?search=qwen), memberikan akses kepada beberapa varian model ini. Anda boleh meneroka varian yang tersedia, menyesuaikannya untuk kegunaan khusus anda, dan melaksanakannya melalui pelbagai rangka kerja.

### Alat Pembangunan Tempatan
Untuk pembangunan dan ujian tempatan, anda boleh menggunakan [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) untuk menjalankan model Qwen yang tersedia pada mesin pembangunan anda dengan prestasi yang dioptimumkan.

### Sumber Dokumentasi
- [Dokumentasi Model Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Mengoptimumkan Model Qwen untuk Pelaksanaan Edge](https://github.com/microsoft/olive)

## Pengenalan

Dalam tutorial ini, kita akan meneroka keluarga model Qwen dari Alibaba dan konsep asasnya. Kita akan membincangkan evolusi keluarga Qwen, metodologi latihan inovatif yang menjadikan model Qwen berkesan, varian utama dalam keluarga ini, dan aplikasi praktikal dalam pelbagai senario.

## Objektif Pembelajaran

Pada akhir tutorial ini, anda akan dapat:

- Memahami falsafah reka bentuk dan evolusi keluarga model Qwen dari Alibaba
- Mengenal pasti inovasi utama yang membolehkan model Qwen mencapai prestasi tinggi dalam pelbagai saiz parameter
- Mengenali manfaat dan batasan varian model Qwen yang berbeza
- Mengaplikasikan pengetahuan tentang model Qwen untuk memilih varian yang sesuai bagi senario dunia sebenar

## Memahami Lanskap Model AI Moden

Lanskap AI telah berkembang dengan ketara, dengan pelbagai organisasi mengejar pendekatan yang berbeza dalam pembangunan model bahasa. Ada yang fokus pada model tertutup yang bersifat proprietari, sementara yang lain menekankan aksesibiliti dan ketelusan sumber terbuka. Pendekatan tradisional melibatkan sama ada model proprietari besar yang hanya boleh diakses melalui API atau model sumber terbuka yang mungkin ketinggalan dalam keupayaan.

Paradigma ini mencipta cabaran bagi organisasi yang mencari keupayaan AI yang kuat sambil mengekalkan kawalan terhadap data mereka, kos, dan fleksibiliti pelaksanaan. Pendekatan konvensional sering memerlukan pilihan antara prestasi canggih dan pertimbangan pelaksanaan praktikal.

## Cabaran AI yang Mudah Diakses

Keperluan untuk AI berkualiti tinggi dan mudah diakses menjadi semakin penting dalam pelbagai senario. Pertimbangkan aplikasi yang memerlukan pilihan pelaksanaan fleksibel untuk keperluan organisasi yang berbeza, pelaksanaan kos efektif di mana kos API boleh menjadi signifikan, keupayaan multibahasa untuk aplikasi global, atau kepakaran domain khusus dalam bidang seperti pengkodan dan matematik.

### Keperluan Pelaksanaan Utama

Pelaksanaan AI moden menghadapi beberapa keperluan asas yang mengehadkan kebolehgunaan praktikal:

- **Aksesibiliti**: Ketersediaan sumber terbuka untuk ketelusan dan penyesuaian
- **Keberkesanan Kos**: Keperluan pengiraan yang munasabah untuk pelbagai bajet
- **Fleksibiliti**: Pelbagai saiz model untuk senario pelaksanaan yang berbeza
- **Jangkauan Global**: Keupayaan multibahasa dan lintas budaya yang kuat
- **Pengkhususan**: Varian khusus domain untuk kegunaan tertentu

## Falsafah Model Qwen

Keluarga model Qwen mewakili pendekatan komprehensif terhadap pembangunan model AI, mengutamakan aksesibiliti sumber terbuka, keupayaan multibahasa, dan pelaksanaan praktikal sambil mengekalkan ciri prestasi kompetitif. Model Qwen mencapai ini melalui pelbagai saiz model, metodologi latihan berkualiti tinggi, dan varian khusus untuk domain yang berbeza.

Keluarga Qwen merangkumi pelbagai pendekatan yang direka untuk menyediakan pilihan di seluruh spektrum prestasi-kecekapan, membolehkan pelaksanaan dari peranti mudah alih ke pelayan perusahaan sambil menyediakan keupayaan AI yang bermakna. Matlamatnya adalah untuk mendemokrasikan akses kepada AI berkualiti tinggi sambil menyediakan fleksibiliti dalam pilihan pelaksanaan.

### Prinsip Reka Bentuk Teras Qwen

Model Qwen dibina berdasarkan beberapa prinsip asas yang membezakannya daripada keluarga model bahasa lain:

- **Sumber Terbuka Utama**: Ketelusan dan aksesibiliti lengkap untuk penyelidikan dan penggunaan komersial
- **Latihan Komprehensif**: Latihan pada dataset besar dan pelbagai yang merangkumi pelbagai bahasa dan domain
- **Seni Bina Boleh Skala**: Pelbagai saiz model untuk memenuhi keperluan pengiraan yang berbeza
- **Kecemerlangan Khusus**: Varian khusus domain yang dioptimumkan untuk tugas tertentu

## Teknologi Utama yang Memungkinkan Keluarga Qwen

### Latihan Skala Besar

Salah satu aspek yang menentukan keluarga Qwen adalah skala besar data latihan dan sumber pengiraan yang dilaburkan dalam pembangunan model. Model Qwen memanfaatkan dataset multibahasa yang dikurasi dengan teliti yang merangkumi trilion token, direka untuk menyediakan pengetahuan dunia yang komprehensif dan keupayaan penaakulan.

Pendekatan ini berfungsi dengan menggabungkan kandungan web berkualiti tinggi, literatur akademik, repositori kod, dan sumber multibahasa. Metodologi latihan menekankan kedua-dua keluasan pengetahuan dan kedalaman pemahaman dalam pelbagai domain dan bahasa.

### Penaakulan dan Pemikiran Lanjutan

Model Qwen terkini menggabungkan keupayaan penaakulan yang canggih yang membolehkan penyelesaian masalah pelbagai langkah yang kompleks:

**Mod Pemikiran (Qwen3)**: Model boleh terlibat dalam penaakulan langkah demi langkah yang terperinci sebelum memberikan jawapan akhir, serupa dengan pendekatan penyelesaian masalah manusia.

**Operasi Mod Dual**: Keupayaan untuk beralih antara mod respons cepat untuk pertanyaan mudah dan mod pemikiran mendalam untuk masalah kompleks.

**Integrasi Rantaian Pemikiran**: Penggabungan semula jadi langkah penaakulan yang meningkatkan ketelusan dan ketepatan dalam tugas yang kompleks.

### Inovasi Seni Bina

Keluarga Qwen menggabungkan beberapa pengoptimuman seni bina yang direka untuk prestasi dan kecekapan:

**Reka Bentuk Boleh Skala**: Seni bina konsisten di seluruh saiz model yang membolehkan penskalaan dan perbandingan yang mudah.

**Integrasi Multimodal**: Integrasi lancar keupayaan pemprosesan teks, penglihatan, dan audio dalam seni bina yang bersatu.

**Pengoptimuman Pelaksanaan**: Pelbagai pilihan kuantisasi dan format pelaksanaan untuk konfigurasi perkakasan yang berbeza.

## Saiz Model dan Pilihan Pelaksanaan

Persekitaran pelaksanaan moden mendapat manfaat daripada fleksibiliti model Qwen dalam pelbagai keperluan pengiraan:

### Model Kecil (0.5B-3B)

Qwen menyediakan model kecil yang cekap sesuai untuk pelaksanaan edge, aplikasi mudah alih, dan persekitaran dengan sumber terhad sambil mengekalkan keupayaan yang mengagumkan.

### Model Sederhana (7B-32B)

Model pertengahan menawarkan keupayaan yang dipertingkatkan untuk aplikasi profesional, memberikan keseimbangan yang sangat baik antara prestasi dan keperluan pengiraan.

### Model Besar (72B+)

Model berskala penuh memberikan prestasi canggih untuk aplikasi yang menuntut, penyelidikan, dan pelaksanaan perusahaan yang memerlukan keupayaan maksimum.

## Manfaat Keluarga Model Qwen

### Aksesibiliti Sumber Terbuka

Model Qwen menyediakan ketelusan dan keupayaan penyesuaian lengkap, membolehkan organisasi memahami, mengubah, dan menyesuaikan model kepada keperluan khusus mereka tanpa terikat kepada vendor.

### Fleksibiliti Pelaksanaan

Pelbagai saiz model membolehkan pelaksanaan dalam konfigurasi perkakasan yang pelbagai, dari peranti mudah alih ke pelayan berprestasi tinggi, memberikan organisasi fleksibiliti dalam pilihan infrastruktur AI mereka.

### Kecemerlangan Multibahasa

Model Qwen cemerlang dalam pemahaman dan penjanaan multibahasa, menyokong puluhan bahasa dengan kekuatan khusus dalam bahasa Inggeris dan Cina, menjadikannya sesuai untuk aplikasi global.

### Prestasi Kompetitif

Model Qwen secara konsisten mencapai keputusan kompetitif dalam penanda aras sambil menyediakan aksesibiliti sumber terbuka, menunjukkan bahawa model terbuka boleh menandingi alternatif proprietari.

### Keupayaan Khusus

Varian khusus domain seperti Qwen-Coder dan Qwen-Math menyediakan kepakaran khusus sambil mengekalkan keupayaan pemahaman bahasa umum.

## Contoh Praktikal dan Kegunaan

Sebelum mendalami butiran teknikal, mari kita terokai beberapa contoh konkrit tentang apa yang boleh dicapai oleh model Qwen:

### Contoh Penaakulan Matematik

Qwen-Math cemerlang dalam penyelesaian masalah matematik langkah demi langkah. Sebagai contoh, apabila diminta menyelesaikan masalah kalkulus yang kompleks:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Contoh Sokongan Multibahasa

Model Qwen menunjukkan keupayaan multibahasa yang kuat dalam pelbagai bahasa:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Contoh Keupayaan Multimodal

Qwen-VL boleh memproses teks dan imej secara serentak:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Contoh Penjanaan Kod

Qwen-Coder cemerlang dalam menjana dan menerangkan kod dalam pelbagai bahasa pengaturcaraan:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Lakukan carian binari pada senarai yang disusun untuk mencari nilai sasaran.
    
    Args:
        arr (list): Senarai yang disusun elemen yang boleh dibandingkan
        target: Nilai yang dicari
        
    Returns:
        int: Indeks sasaran jika dijumpai, -1 jika tidak dijumpai
        
    Kompleksiti Masa: O(log n)
    Kompleksiti Ruang: O(1)
    """
    # Inisialisasi penunjuk kiri dan kanan
    left, right = 0, len(arr) - 1
    
    # Teruskan mencari selagi ruang carian sah
    while left <= right:
        # Kira indeks tengah untuk mengelakkan limpahan integer
        mid = left + (right - left) // 2
        
        # Periksa jika kita menemui sasaran
        if arr[mid] == target:
            return mid
        
        # Jika sasaran lebih kecil, cari separuh kiri
        elif arr[mid] > target:
            right = mid - 1
        
        # Jika sasaran lebih besar, cari separuh kanan
        else:
            left = mid + 1
    
    # Sasaran tidak dijumpai
    return -1

# Contoh penggunaan:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Indeks 7: {result}")  # Output: Indeks 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Contoh Pelaksanaan Edge

Model Qwen boleh dilaksanakan pada pelbagai peranti edge dengan konfigurasi yang dioptimumkan:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Evolusi Keluarga Qwen

### Qwen 1.0 dan 1.5: Model Asas

Model awal Qwen menetapkan prinsip asas latihan komprehensif dan aksesibiliti sumber terbuka:

- **Qwen-7B (7B parameter)**: Pelepasan awal yang fokus pada pemahaman bahasa Cina dan Inggeris
- **Qwen-14B (14B parameter)**: Keupayaan yang dipertingkatkan dengan penaakulan dan pengetahuan yang lebih baik
- **Qwen-72B (72B parameter)**: Model berskala besar yang memberikan prestasi canggih
- **Siri Qwen1.5**: Diperluas kepada pelbagai saiz (0.5B hingga 110B) dengan pengendalian konteks panjang yang lebih baik

### Keluarga Qwen2: Pengembangan Multimodal

Siri Qwen2 menandakan kemajuan yang signifikan dalam keupayaan bahasa dan multimodal:

- **Qwen2-0.5B hingga 72B**: Pelbagai model bahasa yang komprehensif untuk keperluan pelaksanaan yang berbeza
- **Qwen2-57B-A14B (MoE)**: Seni bina campuran pakar untuk penggunaan parameter yang cekap
- **Qwen2-VL**: Keupayaan penglihatan-bahasa yang maju untuk pemahaman imej
- **Qwen2-Audio**: Pemprosesan dan pemahaman audio
- **Qwen2-Math**: Penaakulan matematik dan penyelesaian masalah khusus

### Keluarga Qwen2.5: Prestasi Dipertingkatkan

Siri Qwen2.5 membawa peningkatan yang signifikan dalam semua dimensi:

- **Latihan Diperluas**: 18 trilion token data latihan untuk keupayaan yang lebih baik
- **Konteks Diperluas**: Panjang konteks sehingga 128K token, dengan varian Turbo menyokong 1M token
- **Pengkhususan Dipertingkatkan**: Varian Qwen2.5-Coder dan Qwen2.5-Math yang lebih baik
- **Sokongan Multibahasa Lebih Baik**: Prestasi yang dipertingkatkan dalam lebih daripada 27 bahasa

### Keluarga Qwen3: Penaakulan Lanjutan

Generasi terkini mendorong batas keupayaan penaakulan dan pemikiran:

- **Qwen3-235B-A22B**: Model campuran pakar utama dengan 235B parameter keseluruhan
- **Qwen3-30B-A3B**: Model MoE yang cekap dengan prestasi yang kuat bagi setiap parameter aktif
- **Model Padat**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B untuk pelbagai senario pelaksanaan
- **Mod Pemikiran**: Pendekatan penaakulan hibrid yang menyokong respons cepat dan pemikiran mendalam
- **Kecemerlangan Multibahasa**: Sokongan untuk 119 bahasa dan dialek
- **Latihan Dipertingkatkan**: 36 trilion token data latihan yang pelbagai dan berkualiti tinggi

## Aplikasi Model Qwen

### Aplikasi Perusahaan

Organisasi menggunakan model Qwen untuk analisis dokumen, automasi perkhidmatan pelanggan, bantuan penjanaan kod, dan aplikasi kecerdasan perniagaan. Sifat sumber terbuka membolehkan penyesuaian untuk keperluan perniagaan tertentu sambil mengekalkan privasi dan kawalan data.

### Pengkomputeran Mudah Alih dan Edge

Aplikasi mudah alih memanfaatkan model Qwen untuk terjemahan masa nyata, pembantu pintar, penjanaan kandungan, dan cadangan yang diperibadikan. Pelbagai saiz model membolehkan pelaksanaan dari peranti mudah alih ke pelayan edge.

### Teknologi Pendidikan

Platform pendidikan menggunakan model Qwen untuk bimbingan peribadi, penjanaan kandungan automatik, bantuan pembelajaran bahasa, dan pengalaman pendidikan interaktif. Model khusus seperti Qwen-Math menyediakan kepakaran domain khusus.

### Aplikasi Global

Aplikasi antarabangsa mendapat manfaat daripada keupayaan multibahasa model Qwen yang kuat, membolehkan pengalaman AI yang konsisten dalam pelbagai bahasa dan konteks budaya.

## Cabaran dan Batasan

### Keperluan Pengiraan

Walaupun Qwen menyediakan model dalam pelbagai saiz, varian yang lebih besar masih memerlukan sumber pengiraan yang signifikan untuk prestasi optimum, yang mungkin mengehadkan pilihan pelaksanaan bagi sesetengah organisasi.

### Prestasi Domain Khusus

Walaupun model Qwen berprestasi baik dalam domain umum, aplikasi yang sangat khusus mungkin mendapat manfaat daripada penyesuaian domain atau model khusus.

### Kerumitan Pemilihan Model

Pelbagai model dan varian yang tersedia boleh menjadikan pemilihan mencabar bagi pengguna yang baru dalam ekosistem ini.

### Ketidakseimbangan Bahasa

Walaupun menyokong banyak bahasa, prestasi mungkin berbeza dalam bahasa yang berbeza, dengan keupayaan terkuat dalam bahasa Inggeris dan Cina.

## Masa Depan Keluarga Model Qwen

Keluarga model Qwen mewakili evolusi berterusan ke arah AI berkualiti tinggi yang didemokrasikan. Perkembangan masa depan termasuk pengoptimuman kecekapan yang dipertingkatkan, keupayaan multimodal yang diperluas, mekanisme penaakulan yang lebih baik, dan integrasi yang lebih baik dalam pelbagai senario pelaksanaan.

Apabila teknologi terus berkembang, kita boleh menjangkakan model Qwen menjadi semakin mampu sambil mengekalkan aksesibiliti sumber terbuka mereka, membolehkan pelaksanaan AI dalam pelbagai senario dan kegunaan.

Keluarga Qwen menunjukkan bahawa masa depan pembangunan AI boleh merangkumi prestasi canggih dan aksesibiliti terbuka, menyediakan organisasi dengan alat yang kuat sambil mengekalkan ketelusan dan kawalan.

## Contoh Pembangunan dan Integrasi

### Permulaan Cepat dengan Transformers
Berikut adalah cara untuk memulakan dengan model Qwen menggunakan perpustakaan Hugging Face Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Menggunakan Model Qwen2.5

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Penggunaan Model Khusus

**Penjanaan Kod dengan Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Penyelesaian Masalah Matematik:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Tugas Vision-Language:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Mod Pemikiran (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 Penggunaan Mudah Alih dan Edge

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Contoh Penggunaan API

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Penanda Aras Prestasi dan Pencapaian

Keluarga model Qwen telah mencapai prestasi yang luar biasa merentasi pelbagai penanda aras sambil mengekalkan aksesibiliti sumber terbuka:

### Sorotan Prestasi Utama

**Kecemerlangan Pemikiran:**
- Qwen3-235B-A22B mencapai keputusan kompetitif dalam penilaian penanda aras untuk pengekodan, matematik, dan keupayaan umum apabila dibandingkan dengan model terkemuka lain seperti DeepSeek-R1, o1, o3-mini, Grok-3, dan Gemini-2.5-Pro
- Qwen3-30B-A3B mengatasi QwQ-32B dengan 10 kali parameter aktif
- Qwen3-4B mampu menyaingi prestasi Qwen2.5-72B-Instruct

**Pencapaian Kecekapan:**
- Model asas Qwen3-MoE mencapai prestasi serupa dengan model asas padat Qwen2.5 sambil hanya menggunakan 10% parameter aktif
- Penjimatan kos yang ketara dalam latihan dan inferens berbanding model padat

**Keupayaan Multibahasa:**
- Model Qwen3 menyokong 119 bahasa dan dialek
- Prestasi yang kukuh merentasi konteks linguistik dan budaya yang pelbagai

**Skala Latihan:**
- Qwen3 menggunakan hampir dua kali ganda jumlah token, dengan kira-kira 36 trilion token yang merangkumi 119 bahasa dan dialek berbanding 18 trilion token Qwen2.5

### Matriks Perbandingan Model

| Siri Model     | Julat Parameter | Panjang Konteks | Kekuatan Utama         | Kes Penggunaan Terbaik       |
|----------------|-----------------|-----------------|------------------------|------------------------------|
| **Qwen2.5**    | 0.5B-72B        | 32K-128K        | Prestasi seimbang, multibahasa | Aplikasi umum, penggunaan produksi |
| **Qwen2.5-Coder** | 1.5B-32B     | 128K            | Penjanaan kod, pengaturcaraan | Pembangunan perisian, bantuan pengekodan |
| **Qwen2.5-Math** | 1.5B-72B      | 4K-128K         | Pemikiran matematik    | Platform pendidikan, aplikasi STEM |
| **Qwen2.5-VL** | Pelbagai        | Berubah-ubah    | Pemahaman vision-language | Aplikasi multimodal, analisis imej |
| **Qwen3**      | 0.6B-235B       | Berubah-ubah    | Pemikiran maju, mod pemikiran | Pemikiran kompleks, aplikasi penyelidikan |
| **Qwen3 MoE**  | 30B-235B total | Berubah-ubah    | Prestasi skala besar yang cekap | Aplikasi perusahaan, keperluan prestasi tinggi |

## Panduan Pemilihan Model

### Untuk Aplikasi Asas
- **Qwen2.5-0.5B/1.5B**: Aplikasi mudah alih, peranti edge, aplikasi masa nyata
- **Qwen2.5-3B/7B**: Chatbot umum, penjanaan kandungan, sistem soal jawab

### Untuk Tugas Matematik dan Pemikiran
- **Qwen2.5-Math**: Penyelesaian masalah matematik dan pendidikan STEM
- **Qwen3 dengan Mod Pemikiran**: Pemikiran kompleks yang memerlukan analisis langkah demi langkah

### Untuk Pengaturcaraan dan Pembangunan
- **Qwen2.5-Coder**: Penjanaan kod, debugging, bantuan pengaturcaraan
- **Qwen3**: Tugas pengaturcaraan maju dengan keupayaan pemikiran

### Untuk Aplikasi Multimodal
- **Qwen2.5-VL**: Pemahaman imej, soal jawab visual
- **Qwen-Audio**: Pemprosesan audio dan pemahaman ucapan

### Untuk Penggunaan Perusahaan
- **Qwen2.5-32B/72B**: Pemahaman bahasa berprestasi tinggi
- **Qwen3-235B-A22B**: Keupayaan maksimum untuk aplikasi yang menuntut

## Platform Penggunaan dan Aksesibiliti
### Platform Awan
- **Hugging Face Hub**: Repositori model komprehensif dengan sokongan komuniti
- **ModelScope**: Platform model Alibaba dengan alat pengoptimuman
- **Pelbagai Penyedia Awan**: Sokongan melalui platform ML standard

### Rangka Kerja Pembangunan Tempatan
- **Transformers**: Integrasi standard Hugging Face untuk penggunaan mudah
- **vLLM**: Penyajian berprestasi tinggi untuk persekitaran produksi
- **Ollama**: Penggunaan dan pengurusan tempatan yang dipermudahkan
- **ONNX Runtime**: Pengoptimuman merentas platform untuk pelbagai perkakasan
- **llama.cpp**: Pelaksanaan C++ yang cekap untuk platform yang pelbagai

### Sumber Pembelajaran
- **Dokumentasi Qwen**: Dokumentasi rasmi dan kad model
- **Hugging Face Model Hub**: Demo interaktif dan contoh komuniti
- **Kertas Penyelidikan**: Kertas teknikal di arxiv untuk pemahaman mendalam
- **Forum Komuniti**: Sokongan komuniti aktif dan perbincangan

### Memulakan dengan Model Qwen

#### Platform Pembangunan
1. **Hugging Face Transformers**: Mulakan dengan integrasi Python standard
2. **ModelScope**: Terokai alat penggunaan yang dioptimumkan oleh Alibaba
3. **Penggunaan Tempatan**: Gunakan Ollama atau transformers secara langsung untuk ujian tempatan

#### Laluan Pembelajaran
1. **Fahami Konsep Asas**: Kajian seni bina keluarga Qwen dan keupayaannya
2. **Eksperimen dengan Varian**: Cuba saiz model yang berbeza untuk memahami kompromi prestasi
3. **Latih Kemahiran Pelaksanaan**: Gunakan model dalam persekitaran pembangunan
4. **Optimumkan Penggunaan**: Laraskan untuk kes penggunaan produksi

#### Amalan Terbaik
- **Mulakan Kecil**: Mulakan dengan model kecil (1.5B-7B) untuk pembangunan awal
- **Gunakan Templat Chat**: Terapkan format yang sesuai untuk hasil yang optimum
- **Pantau Sumber**: Jejak penggunaan memori dan kelajuan inferens
- **Pertimbangkan Pengkhususan**: Pilih varian khusus domain apabila sesuai

## Corak Penggunaan Lanjutan

### Contoh Penalaan

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Kejuruteraan Prompt Khusus

**Untuk Tugas Pemikiran Kompleks:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Untuk Penjanaan Kod dengan Konteks:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Aplikasi Multibahasa

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Corak Penggunaan Produksi

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategi Pengoptimuman Prestasi

### Pengoptimuman Memori

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Pengoptimuman Inferens

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Amalan Terbaik dan Garis Panduan

### Keselamatan dan Privasi

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Pemantauan dan Penilaian

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Kesimpulan

Keluarga model Qwen mewakili pendekatan komprehensif untuk mendemokrasikan teknologi AI sambil mengekalkan prestasi kompetitif merentasi pelbagai aplikasi. Melalui komitmennya terhadap aksesibiliti sumber terbuka, keupayaan multibahasa, dan pilihan penggunaan yang fleksibel, Qwen membolehkan organisasi dan pembangun memanfaatkan keupayaan AI yang kuat tanpa mengira sumber atau keperluan khusus mereka.

### Poin Penting

**Kecemerlangan Sumber Terbuka**: Qwen menunjukkan bahawa model sumber terbuka boleh mencapai prestasi yang kompetitif dengan alternatif proprietari sambil menyediakan ketelusan, penyesuaian, dan kawalan.

**Seni Bina Skala**: Julat dari 0.5B hingga 235B parameter membolehkan penggunaan merentasi spektrum penuh persekitaran pengkomputeran, dari peranti mudah alih hingga kluster perusahaan.

**Keupayaan Khusus**: Varian khusus domain seperti Qwen-Coder, Qwen-Math, dan Qwen-VL menyediakan kepakaran khusus sambil mengekalkan pemahaman bahasa umum.

**Aksesibiliti Global**: Sokongan multibahasa yang kukuh merentasi 119+ bahasa menjadikan Qwen sesuai untuk aplikasi antarabangsa dan pangkalan pengguna yang pelbagai.

**Inovasi Berterusan**: Evolusi dari Qwen 1.0 ke Qwen3 menunjukkan peningkatan konsisten dalam keupayaan, kecekapan, dan pilihan penggunaan.

### Pandangan Masa Depan

Apabila keluarga Qwen terus berkembang, kita boleh menjangkakan:

- **Kecekapan Dipertingkatkan**: Pengoptimuman berterusan untuk nisbah prestasi-per-parameter yang lebih baik
- **Keupayaan Multimodal Diperluas**: Integrasi pemprosesan vision, audio, dan teks yang lebih canggih
- **Pemikiran Dipertingkatkan**: Mekanisme pemikiran maju dan keupayaan penyelesaian masalah berbilang langkah
- **Alat Penggunaan Lebih Baik**: Rangka kerja dan alat pengoptimuman yang dipertingkatkan untuk pelbagai senario penggunaan
- **Pertumbuhan Komuniti**: Ekosistem alat, aplikasi, dan sumbangan komuniti yang diperluas

### Langkah Seterusnya

Sama ada anda sedang membina chatbot, membangunkan alat pendidikan, mencipta pembantu pengekodan, atau bekerja pada aplikasi multibahasa, keluarga Qwen menyediakan penyelesaian yang boleh diskalakan dengan sokongan komuniti yang kukuh dan dokumentasi yang komprehensif.

Untuk kemas kini terkini, pelepasan model, dan dokumentasi teknikal terperinci, lawati repositori rasmi Qwen di Hugging Face dan terokai perbincangan komuniti aktif serta contoh-contoh.

Masa depan pembangunan AI terletak pada alat yang boleh diakses, telus, dan berkuasa yang membolehkan inovasi merentasi semua sektor dan skala. Keluarga Qwen mencerminkan visi ini, menyediakan organisasi dan pembangun dengan asas untuk membina generasi seterusnya aplikasi berkuasa AI.

## Sumber Tambahan

- **Dokumentasi Rasmi**: [Dokumentasi Qwen](https://qwen.readthedocs.io/)
- **Model Hub**: [Koleksi Qwen di Hugging Face](https://huggingface.co/collections/Qwen/)
- **Kertas Teknikal**: [Penerbitan Penyelidikan Qwen](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Komuniti**: [Perbincangan dan Isu GitHub](https://github.com/QwenLM/)
- **Platform ModelScope**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Hasil Pembelajaran

Selepas melengkapkan modul ini, anda akan dapat:

1. Menerangkan kelebihan seni bina keluarga model Qwen dan pendekatan sumber terbukanya
2. Memilih varian Qwen yang sesuai berdasarkan keperluan aplikasi dan kekangan sumber
3. Melaksanakan model Qwen dalam pelbagai senario penggunaan dengan konfigurasi yang dioptimumkan
4. Menerapkan teknik kuantisasi dan pengoptimuman untuk meningkatkan prestasi model Qwen
5. Menilai kompromi antara saiz model, prestasi, dan keupayaan merentasi keluarga Qwen

## Apa yang seterusnya

- [03: Asas Keluarga Gemma](03.GemmaFamily.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.