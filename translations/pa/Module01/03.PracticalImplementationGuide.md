<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:23:54+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 3: ਪ੍ਰੈਕਟਿਕਲ ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ ਗਾਈਡ

## ਝਲਕ

ਇਹ ਵਿਸਤ੍ਰਿਤ ਗਾਈਡ ਤੁਹਾਨੂੰ EdgeAI ਕੋਰਸ ਲਈ ਤਿਆਰ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰੇਗੀ, ਜੋ ਕਿ ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਚੱਲਣ ਵਾਲੇ ਪ੍ਰੈਕਟਿਕਲ AI ਹੱਲ ਬਣਾਉਣ 'ਤੇ ਧਿਆਨ ਕੇਂਦਰਿਤ ਕਰਦਾ ਹੈ। ਕੋਰਸ ਹੱਥ-ਵਰਤੋਂ ਵਿਕਾਸ 'ਤੇ ਜ਼ੋਰ ਦਿੰਦਾ ਹੈ, ਜੋ ਆਧੁਨਿਕ ਫਰੇਮਵਰਕ ਅਤੇ ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਅਨੁਕੂਲ ਮਾਡਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰਦਾ ਹੈ।

## 1. ਵਿਕਾਸ ਵਾਤਾਵਰਣ ਸੈਟਅੱਪ

### ਪ੍ਰੋਗ੍ਰਾਮਿੰਗ ਭਾਸ਼ਾਵਾਂ ਅਤੇ ਫਰੇਮਵਰਕ

**ਪਾਇਥਨ ਵਾਤਾਵਰਣ**
- **ਵਰਜਨ**: ਪਾਇਥਨ 3.10 ਜਾਂ ਉੱਚਾ (ਸਿਫਾਰਸ਼ੀ: ਪਾਇਥਨ 3.11)
- **ਪੈਕੇਜ ਮੈਨੇਜਰ**: pip ਜਾਂ conda
- **ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ**: ਵਾਤਾਵਰਣ ਨੂੰ ਅਲੱਗ ਕਰਨ ਲਈ venv ਜਾਂ conda ਵਰਤੋ
- **ਮੁੱਖ ਲਾਇਬ੍ਰੇਰੀਆਂ**: ਕੋਰਸ ਦੌਰਾਨ ਖਾਸ EdgeAI ਲਾਇਬ੍ਰੇਰੀਆਂ ਇੰਸਟਾਲ ਕੀਤੀਆਂ ਜਾਣਗੀਆਂ

**ਮਾਈਕਰੋਸਾਫਟ .NET ਵਾਤਾਵਰਣ**
- **ਵਰਜਨ**: .NET 8 ਜਾਂ ਉੱਚਾ
- **IDE**: Visual Studio 2022, Visual Studio Code, ਜਾਂ JetBrains Rider
- **SDK**: ਪਲੇਟਫਾਰਮ-ਅਗਨੋਸਟਿਕ ਵਿਕਾਸ ਲਈ .NET SDK ਇੰਸਟਾਲ ਹੋਣਾ ਯਕੀਨੀ ਬਣਾਓ

### ਵਿਕਾਸ ਟੂਲ

**ਕੋਡ ਐਡੀਟਰ ਅਤੇ IDEs**
- Visual Studio Code (ਪਲੇਟਫਾਰਮ-ਅਗਨੋਸਟਿਕ ਵਿਕਾਸ ਲਈ ਸਿਫਾਰਸ਼ੀ)
- PyCharm ਜਾਂ Visual Studio (ਭਾਸ਼ਾ-ਵਿਸ਼ੇਸ਼ ਵਿਕਾਸ ਲਈ)
- Jupyter Notebooks ਇੰਟਰੈਕਟਿਵ ਵਿਕਾਸ ਅਤੇ ਪ੍ਰੋਟੋਟਾਈਪਿੰਗ ਲਈ

**ਵਰਜਨ ਕੰਟਰੋਲ**
- Git (ਤਾਜ਼ਾ ਵਰਜਨ)
- GitHub ਖਾਤਾ ਰਿਪੋਜ਼ਿਟਰੀਆਂ ਅਤੇ ਸਹਿਯੋਗ ਲਈ

## 2. ਹਾਰਡਵੇਅਰ ਦੀਆਂ ਲੋੜਾਂ ਅਤੇ ਸਿਫਾਰਸ਼ਾਂ

### ਘੱਟੋ-ਘੱਟ ਸਿਸਟਮ ਲੋੜਾਂ
- **CPU**: ਮਲਟੀ-ਕੋਰ ਪ੍ਰੋਸੈਸਰ (Intel i5/AMD Ryzen 5 ਜਾਂ ਸਮਾਨ)
- **RAM**: ਘੱਟੋ-ਘੱਟ 8GB, 16GB ਸਿਫਾਰਸ਼ੀ
- **ਸਟੋਰੇਜ**: ਮਾਡਲਾਂ ਅਤੇ ਵਿਕਾਸ ਟੂਲਾਂ ਲਈ 50GB ਉਪਲਬਧ ਜਗ੍ਹਾ
- **OS**: Windows 10/11, macOS 10.15+, ਜਾਂ Linux (Ubuntu 20.04+)

### ਕੰਪਿਊਟ ਰਿਸੋਰਸ ਸਟ੍ਰੈਟਜੀ
ਕੋਰਸ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ 'ਤੇ ਪਹੁੰਚਯੋਗ ਬਣਾਇਆ ਗਿਆ ਹੈ:

**ਲੋਕਲ ਵਿਕਾਸ (CPU/NPU ਫੋਕਸ)**
- ਮੁੱਖ ਵਿਕਾਸ CPU ਅਤੇ NPU ਐਕਸਲੇਰੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰੇਗਾ
- ਜ਼ਿਆਦਾਤਰ ਆਧੁਨਿਕ ਲੈਪਟਾਪ ਅਤੇ ਡੈਸਕਟਾਪ ਲਈ ਉਚਿਤ
- ਕੁਸ਼ਲਤਾ ਅਤੇ ਪ੍ਰੈਕਟਿਕਲ ਡਿਪਲੌਇਮੈਂਟ ਸਥਿਤੀਆਂ 'ਤੇ ਧਿਆਨ

**ਕਲਾਉਡ GPU ਰਿਸੋਰਸ (ਵਿਕਲਪਿਕ)**
- **Azure Machine Learning**: ਗੰਭੀਰ ਤਰਬੀਅਤ ਅਤੇ ਪ੍ਰਯੋਗਾਂ ਲਈ
- **Google Colab**: ਸਿੱਖਿਆ ਦੇ ਉਦੇਸ਼ਾਂ ਲਈ ਮੁਫ਼ਤ ਟੀਅਰ ਉਪਲਬਧ
- **Kaggle Notebooks**: ਵਿਕਲਪਿਕ ਕਲਾਉਡ ਕੰਪਿਊਟਿੰਗ ਪਲੇਟਫਾਰਮ

### ਐਜ ਡਿਵਾਈਸ ਵਿਚਾਰ
- ARM-ਅਧਾਰਤ ਪ੍ਰੋਸੈਸਰਾਂ ਦੀ ਸਮਝ
- ਮੋਬਾਈਲ ਅਤੇ IoT ਹਾਰਡਵੇਅਰ ਦੀਆਂ ਪਾਬੰਦੀਆਂ ਦਾ ਗਿਆਨ
- ਪਾਵਰ ਖਪਤ ਅਨੁਕੂਲਤਾ ਨਾਲ ਜਾਣੂ ਹੋਣਾ

## 3. ਕੋਰ ਮਾਡਲ ਪਰਿਵਾਰ ਅਤੇ ਸਰੋਤ

### ਮੁੱਖ ਮਾਡਲ ਪਰਿਵਾਰ

**Microsoft Phi-4 ਪਰਿਵਾਰ**
- **ਵੇਰਵਾ**: ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਡਿਜ਼ਾਈਨ ਕੀਤੇ ਗਏ ਸੰਕੁਚਿਤ, ਕੁਸ਼ਲ ਮਾਡਲ
- **ਤਾਕਤਾਂ**: ਸ਼ਾਨਦਾਰ ਪ੍ਰਦਰਸ਼ਨ-ਤੋਂ-ਆਕਾਰ ਅਨੁਪਾਤ, ਤਰਕਸੰਗਤ ਕਾਰਜਾਂ ਲਈ ਅਨੁਕੂਲ
- **ਸਰੋਤ**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **ਵਰਤੋਂ ਦੇ ਕੇਸ**: ਕੋਡ ਜਨਰੇਸ਼ਨ, ਗਣਿਤੀ ਤਰਕ, ਆਮ ਗੱਲਬਾਤ

**Qwen-3 ਪਰਿਵਾਰ**
- **ਵੇਰਵਾ**: Alibaba ਦੇ ਬਹੁਭਾਸ਼ੀ ਮਾਡਲਾਂ ਦੀ ਨਵੀਂ ਪੀੜ੍ਹੀ
- **ਤਾਕਤਾਂ**: ਮਜ਼ਬੂਤ ਬਹੁਭਾਸ਼ੀ ਸਮਰੱਥਾ, ਕੁਸ਼ਲ ਆਰਕੀਟੈਕਚਰ
- **ਸਰੋਤ**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **ਵਰਤੋਂ ਦੇ ਕੇਸ**: ਬਹੁਭਾਸ਼ੀ ਐਪਲੀਕੇਸ਼ਨ, ਸੰਸਕ੍ਰਿਤਿਕ AI ਹੱਲ

**Google Gemma-3n ਪਰਿਵਾਰ**
- **ਵੇਰਵਾ**: ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਅਨੁਕੂਲ ਗੂਗਲ ਦੇ ਹਲਕੇ ਮਾਡਲ
- **ਤਾਕਤਾਂ**: ਤੇਜ਼ ਇੰਫਰੈਂਸ, ਮੋਬਾਈਲ-ਅਨੁਕੂਲ ਆਰਕੀਟੈਕਚਰ
- **ਸਰੋਤ**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **ਵਰਤੋਂ ਦੇ ਕੇਸ**: ਮੋਬਾਈਲ ਐਪਲੀਕੇਸ਼ਨ, ਰੀਅਲ-ਟਾਈਮ ਪ੍ਰੋਸੈਸਿੰਗ

### ਮਾਡਲ ਚੋਣ ਮਾਪਦੰਡ
- **ਪ੍ਰਦਰਸ਼ਨ ਵਿਰੁੱਧ ਆਕਾਰ ਦਾ ਸਮਝੌਤਾ**: ਛੋਟੇ ਅਤੇ ਵੱਡੇ ਮਾਡਲਾਂ ਦੀ ਚੋਣ ਕਰਨ ਦਾ ਗਿਆਨ
- **ਟਾਸਕ-ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ**: ਮਾਡਲਾਂ ਨੂੰ ਖਾਸ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਨਾਲ ਮਿਲਾਉਣਾ
- **ਡਿਪਲੌਇਮੈਂਟ ਪਾਬੰਦੀਆਂ**: ਮੈਮਰੀ, ਲੈਟੈਂਸੀ, ਅਤੇ ਪਾਵਰ ਖਪਤ ਦੇ ਵਿਚਾਰ

## 4. ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਅਤੇ ਅਨੁਕੂਲਤਾ ਟੂਲ

### Llama.cpp ਫਰੇਮਵਰਕ
- **ਰਿਪੋਜ਼ਿਟਰੀ**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **ਉਦੇਸ਼**: LLMs ਲਈ ਉੱਚ-ਪ੍ਰਦਰਸ਼ਨ ਇੰਫਰੈਂਸ ਇੰਜਨ
- **ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ**:
  - CPU-ਅਨੁਕੂਲ ਇੰਫਰੈਂਸ
  - ਕਈ ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਫਾਰਮੈਟ (Q4, Q5, Q8)
  - ਪਲੇਟਫਾਰਮ-ਅਗਨੋਸਟਿਕ ਅਨੁਕੂਲਤਾ
  - ਮੈਮਰੀ-ਕੁਸ਼ਲ ਕਾਰਜ
- **ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਬੁਨਿਆਦੀ ਵਰਤੋਂ**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **ਰਿਪੋਜ਼ਿਟਰੀ**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **ਉਦੇਸ਼**: ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਟੂਲਕਿਟ
- **ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ**:
  - ਆਟੋਮੈਟਿਕ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਵਰਕਫਲੋਜ਼
  - ਹਾਰਡਵੇਅਰ-ਅਨੁਕੂਲਤਾ
  - ONNX Runtime ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ
  - ਪ੍ਰਦਰਸ਼ਨ ਬੈਂਚਮਾਰਕਿੰਗ ਟੂਲ
- **ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਬੁਨਿਆਦੀ ਵਰਤੋਂ**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਲਈ ਉਦਾਹਰਣ ਪਾਇਥਨ ਸਕ੍ਰਿਪਟ
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS ਵਰਤੋਂਕਾਰਾਂ ਲਈ)
- **ਰਿਪੋਜ਼ਿਟਰੀ**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **ਉਦੇਸ਼**: Apple Silicon ਲਈ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਫਰੇਮਵਰਕ
- **ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ**:
  - ਮੂਲ Apple Silicon ਅਨੁਕੂਲਤਾ
  - ਮੈਮਰੀ-ਕੁਸ਼ਲ ਕਾਰਜ
  - PyTorch-ਜਿਵੇਂ API
  - ਯੂਨਿਫਾਇਡ ਮੈਮਰੀ ਆਰਕੀਟੈਕਚਰ ਸਹਾਇਤਾ
- **ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਬੁਨਿਆਦੀ ਵਰਤੋਂ**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **ਰਿਪੋਜ਼ਿਟਰੀ**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **ਉਦੇਸ਼**: ONNX ਮਾਡਲਾਂ ਲਈ ਪਲੇਟਫਾਰਮ-ਅਗਨੋਸਟਿਕ ਇੰਫਰੈਂਸ ਐਕਸਲੇਰੇਸ਼ਨ
- **ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ**:
  - ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ (CPU, GPU, NPU)
  - ਇੰਫਰੈਂਸ ਲਈ ਗ੍ਰਾਫ ਅਨੁਕੂਲਤਾ
  - ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਸਹਾਇਤਾ
  - ਕ੍ਰਾਸ-ਭਾਸ਼ਾ ਸਹਾਇਤਾ (Python, C++, C#, JavaScript)
- **ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਬੁਨਿਆਦੀ ਵਰਤੋਂ**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```
## 5. ਸਿਫਾਰਸ਼ੀ ਪੜ੍ਹਾਈ ਅਤੇ ਸਰੋਤ

### ਜ਼ਰੂਰੀ ਦਸਤਾਵੇਜ਼
- **ONNX Runtime ਦਸਤਾਵੇਜ਼**: ਪਲੇਟਫਾਰਮ-ਅਗਨੋਸਟਿਕ ਇੰਫਰੈਂਸ ਨੂੰ ਸਮਝਣਾ
- **Hugging Face Transformers ਗਾਈਡ**: ਮਾਡਲ ਲੋਡਿੰਗ ਅਤੇ ਇੰਫਰੈਂਸ
- **Edge AI ਡਿਜ਼ਾਈਨ ਪੈਟਰਨ**: ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਵਧੀਆ ਅਭਿਆਸ

### ਤਕਨੀਕੀ ਪੇਪਰ
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### ਕਮਿਊਨਿਟੀ ਸਰੋਤ
- **EdgeAI Slack/Discord Communities**: ਸਹਿਯੋਗ ਅਤੇ ਚਰਚਾ
- **GitHub ਰਿਪੋਜ਼ਿਟਰੀਆਂ**: ਉਦਾਹਰਣ ਅਮਲ ਅਤੇ ਟਿਊਟੋਰਿਅਲ
- **YouTube ਚੈਨਲ**: ਤਕਨੀਕੀ ਡੀਪ-ਡਾਈਵ ਅਤੇ ਟਿਊਟੋਰਿਅਲ

## 6. ਮੁਲਾਂਕਣ ਅਤੇ ਪ੍ਰਮਾਣਿਕਤਾ

### ਕੋਰਸ ਤੋਂ ਪਹਿਲਾਂ ਚੈੱਕਲਿਸਟ
- [ ] Python 3.10+ ਇੰਸਟਾਲ ਅਤੇ ਪ੍ਰਮਾਣਿਤ
- [ ] .NET 8+ ਇੰਸਟਾਲ ਅਤੇ ਪ੍ਰਮਾਣਿਤ
- [ ] ਵਿਕਾਸ ਵਾਤਾਵਰਣ ਸੰਰਚਿਤ
- [ ] Hugging Face ਖਾਤਾ ਬਣਾਇਆ
- [ ] ਟਾਰਗੇਟ ਮਾਡਲ ਪਰਿਵਾਰਾਂ ਨਾਲ ਬੁਨਿਆਦੀ ਜਾਣੂ
- [ ] ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਟੂਲ ਇੰਸਟਾਲ ਅਤੇ ਟੈਸਟ ਕੀਤੇ
- [ ] ਹਾਰਡਵੇਅਰ ਦੀਆਂ ਲੋੜਾਂ ਪੂਰੀਆਂ ਕੀਤੀਆਂ
- [ ] ਕਲਾਉਡ ਕੰਪਿਊਟਿੰਗ ਖਾਤੇ ਸੈਟਅੱਪ ਕੀਤੇ (ਜੇ ਲੋੜ ਹੋਵੇ)

## ਮੁੱਖ ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼

ਇਸ ਗਾਈਡ ਦੇ ਅੰਤ ਤੱਕ, ਤੁਸੀਂ ਇਹ ਕਰਨ ਦੇ ਯੋਗ ਹੋਵੋਗੇ:

1. EdgeAI ਐਪਲੀਕੇਸ਼ਨ ਵਿਕਾਸ ਲਈ ਪੂਰਾ ਵਿਕਾਸ ਵਾਤਾਵਰਣ ਸੈਟਅੱਪ ਕਰੋ
2. ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਲਈ ਜ਼ਰੂਰੀ ਟੂਲ ਅਤੇ ਫਰੇਮਵਰਕ ਇੰਸਟਾਲ ਅਤੇ ਸੰਰਚਿਤ ਕਰੋ
3. ਆਪਣੇ EdgeAI ਪ੍ਰੋਜੈਕਟਾਂ ਲਈ ਉਚਿਤ ਹਾਰਡਵੇਅਰ ਅਤੇ ਸਾਫਟਵੇਅਰ ਸੰਰਚਨਾਵਾਂ ਦੀ ਚੋਣ ਕਰੋ
4. ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ AI ਮਾਡਲਾਂ ਨੂੰ ਡਿਪਲੌਇ ਕਰਨ ਲਈ ਮੁੱਖ ਵਿਚਾਰਾਂ ਨੂੰ ਸਮਝੋ
5. ਕੋਰਸ ਵਿੱਚ ਹੱਥ-ਵਰਤੋਂ ਅਭਿਆਸਾਂ ਲਈ ਆਪਣੀ ਸਿਸਟਮ ਤਿਆਰ ਕਰੋ

## ਵਾਧੂ ਸਰੋਤ

### ਅਧਿਕਾਰਤ ਦਸਤਾਵੇਜ਼
- **Python ਦਸਤਾਵੇਜ਼**: ਅਧਿਕਾਰਤ ਪਾਇਥਨ ਭਾਸ਼ਾ ਦਸਤਾਵੇਜ਼
- **Microsoft .NET ਦਸਤਾਵੇਜ਼**: ਅਧਿਕਾਰਤ .NET ਵਿਕਾਸ ਸਰੋਤ
- **ONNX Runtime ਦਸਤਾਵੇਜ਼**: ONNX Runtime ਲਈ ਵਿਸਤ੍ਰਿਤ ਗਾਈਡ
- **TensorFlow Lite ਦਸਤਾਵੇਜ਼**: ਅਧਿਕਾਰਤ TensorFlow Lite ਦਸਤਾਵੇਜ਼

### ਵਿਕਾਸ ਟੂਲ
- **Visual Studio Code**: AI ਵਿਕਾਸ ਐਕਸਟੈਂਸ਼ਨ ਨਾਲ ਹਲਕਾ ਕੋਡ ਐਡੀਟਰ
- **Jupyter Notebooks**: ML ਪ੍ਰਯੋਗਾਂ ਲਈ ਇੰਟਰੈਕਟਿਵ ਕੰਪਿਊਟਿੰਗ ਵਾਤਾਵਰਣ
- **Docker**: ਸਥਿਰ ਵਿਕਾਸ ਵਾਤਾਵਰਣਾਂ ਲਈ ਕੰਟੇਨਰਾਈਜ਼ੇਸ਼ਨ ਪਲੇਟਫਾਰਮ
- **Git**: ਕੋਡ ਪ੍ਰਬੰਧਨ ਲਈ ਵਰਜਨ ਕੰਟਰੋਲ ਸਿਸਟਮ

### ਸਿੱਖਣ ਦੇ ਸਰੋਤ
- **EdgeAI ਰਿਸਰਚ ਪੇਪਰ**: ਕੁਸ਼ਲ ਮਾਡਲਾਂ 'ਤੇ ਤਾਜ਼ਾ ਅਕਾਦਮਿਕ ਰਿਸਰਚ
- **ਆਨਲਾਈਨ ਕੋਰਸ**: AI ਅਨੁਕੂਲਤਾ 'ਤੇ ਸਹਾਇਕ ਸਿੱਖਣ ਸਮੱਗਰੀ
- **ਕਮਿਊਨਿਟੀ ਫੋਰਮ**: EdgeAI ਵਿਕਾਸ ਚੁਣੌਤੀਆਂ ਲਈ Q&A ਪਲੇਟਫਾਰਮ
- **ਬੈਂਚਮਾਰਕ ਡੇਟਾਸੈਟ**: ਮਾਡਲ ਪ੍ਰਦਰਸ਼ਨ ਦਾ ਮੁਲਾਂਕਣ ਕਰਨ ਲਈ ਮਿਆਰੀ ਡੇਟਾਸੈਟ

## ਸਿੱਖਣ ਦੇ ਨਤੀਜੇ

ਇਸ ਤਿਆਰੀ ਗਾਈਡ ਨੂੰ ਪੂਰਾ ਕਰਨ ਤੋਂ ਬਾਅਦ, ਤੁਸੀਂ:

1. EdgeAI ਵਿਕਾਸ ਲਈ ਪੂਰੀ ਤਰ੍ਹਾਂ ਸੰਰਚਿਤ ਵਿਕਾਸ ਵਾਤਾਵਰਣ ਰੱਖਦੇ ਹੋਵੋਗੇ
2. ਵੱਖ-ਵੱਖ ਡਿਪਲੌਇਮੈਂਟ ਸਥਿਤੀਆਂ ਲਈ ਹਾਰਡਵੇਅਰ ਅਤੇ ਸਾਫਟਵੇਅਰ ਦੀਆਂ ਲੋੜਾਂ ਨੂੰ ਸਮਝਦੇ ਹੋਵੋਗੇ
3. ਕੋਰਸ ਦੌਰਾਨ ਵਰਤੇ ਜਾਣ ਵਾਲੇ ਮੁੱਖ ਫਰੇਮਵਰਕ ਅਤੇ ਟੂਲਾਂ ਨਾਲ ਜਾਣੂ ਹੋਵੋਗੇ
4. ਡਿਵਾਈਸ ਪਾਬੰਦੀਆਂ ਅਤੇ ਲੋੜਾਂ ਦੇ ਅਧਾਰ 'ਤੇ ਉਚਿਤ ਮਾਡਲ ਚੁਣ ਸਕੋਗੇ
5. ਐਜ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਅਨੁਕੂਲਤਾ ਤਕਨੀਕਾਂ ਦਾ ਮੂਲ ਗਿਆਨ ਰੱਖਦੇ ਹੋਵੋਗੇ

## ➡️ ਅਗਲਾ ਕੀ ਹੈ

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਹਾਲਾਂਕਿ ਅਸੀਂ ਸਹੀਅਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦਸਤਾਵੇਜ਼ ਦਾ ਮੂਲ ਰੂਪ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।