<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-17T23:38:43+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 4 : ਐਪਲ MLX ਫਰੇਮਵਰਕ ਡੀਪ ਡਾਈਵ

## ਸੂਚੀ
1. [ਐਪਲ MLX ਦਾ ਪਰਿਚਯ](../../../Module04)
2. [LLM ਵਿਕਾਸ ਲਈ ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ](../../../Module04)
3. [ਇੰਸਟਾਲੇਸ਼ਨ ਗਾਈਡ](../../../Module04)
4. [MLX ਨਾਲ ਸ਼ੁਰੂਆਤ](../../../Module04)
5. [MLX-LM: ਭਾਸ਼ਾ ਮਾਡਲ](../../../Module04)
6. [ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨਾ](../../../Module04)
7. [ਹੱਗਿੰਗ ਫੇਸ ਇੰਟੀਗ੍ਰੇਸ਼ਨ](../../../Module04)
8. [ਮਾਡਲ ਕਨਵਰਜ਼ਨ ਅਤੇ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ](../../../Module04)
9. [ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨਾ](../../../Module04)
10. [ਤਕਨੀਕੀ LLM ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ](../../../Module04)
11. [LLMs ਲਈ ਸਰਵੋਤਮ ਅਭਿਆਸ](../../../Module04)
12. [ਟ੍ਰਬਲਸ਼ੂਟਿੰਗ](../../../Module04)
13. [ਵਾਧੂ ਸਰੋਤ](../../../Module04)

## ਐਪਲ MLX ਦਾ ਪਰਿਚਯ

ਐਪਲ MLX ਇੱਕ ਐਰੇ ਫਰੇਮਵਰਕ ਹੈ ਜੋ ਐਪਲ ਸਿਲਿਕਾਨ 'ਤੇ ਕੁਸ਼ਲ ਅਤੇ ਲਚੀਲੇ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਹ ਐਪਲ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਰਿਸਰਚ ਦੁਆਰਾ ਵਿਕਸਿਤ ਕੀਤਾ ਗਿਆ ਹੈ। ਦਸੰਬਰ 2023 ਵਿੱਚ ਜਾਰੀ ਕੀਤਾ ਗਿਆ, MLX PyTorch ਅਤੇ TensorFlow ਵਰਗੇ ਫਰੇਮਵਰਕਾਂ ਲਈ ਐਪਲ ਦਾ ਜਵਾਬ ਹੈ, ਜਿਸ ਵਿੱਚ ਮੈਕ ਕੰਪਿਊਟਰਾਂ 'ਤੇ ਸ਼ਕਤੀਸ਼ਾਲੀ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲ ਸਮਰੱਥਾਵਾਂ ਨੂੰ ਯੋਗ ਬਣਾਉਣ 'ਤੇ ਖਾਸ ਧਿਆਨ ਦਿੱਤਾ ਗਿਆ ਹੈ।

### LLMs ਲਈ MLX ਨੂੰ ਵਿਸ਼ੇਸ਼ ਕੀ ਬਣਾਉਂਦਾ ਹੈ?

MLX ਨੂੰ ਐਪਲ ਸਿਲਿਕਾਨ ਦੇ ਯੂਨਿਫਾਇਡ ਮੈਮਰੀ ਆਰਕੀਟੈਕਚਰ ਦਾ ਪੂਰੀ ਤਰ੍ਹਾਂ ਲਾਭ ਉਠਾਉਣ ਲਈ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਹ ਮੈਕ ਕੰਪਿਊਟਰਾਂ 'ਤੇ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਸਥਾਨਕ ਤੌਰ 'ਤੇ ਚਲਾਉਣ ਅਤੇ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਉਚਿਤ ਹੈ। ਫਰੇਮਵਰਕ ਉਹਨਾਂ ਅਨੁਕੂਲਤਾ ਸਮੱਸਿਆਵਾਂ ਨੂੰ ਦੂਰ ਕਰਦਾ ਹੈ ਜੋ ਮੈਕ ਉਪਭੋਗਤਾਵਾਂ ਨੂੰ ਆਮ ਤੌਰ 'ਤੇ LLMs ਨਾਲ ਕੰਮ ਕਰਦੇ ਸਮੇਂ ਆਉਂਦੀਆਂ ਸਨ।

### MLX ਨੂੰ LLMs ਲਈ ਕੌਣ ਵਰਤਣਾ ਚਾਹੀਦਾ ਹੈ?

- **ਮੈਕ ਉਪਭੋਗਤਾ** ਜੋ ਕਲਾਉਡ ਨਿਰਭਰਤਾਵਾਂ ਤੋਂ ਬਿਨਾਂ ਸਥਾਨਕ ਤੌਰ 'ਤੇ LLMs ਚਲਾਉਣਾ ਚਾਹੁੰਦੇ ਹਨ  
- **ਗਵੇਸ਼ਕ** ਜੋ ਭਾਸ਼ਾ ਮਾਡਲ ਫਾਈਨ-ਟਿਊਨਿੰਗ ਅਤੇ ਕਸਟਮਾਈਜ਼ੇਸ਼ਨ 'ਤੇ ਪ੍ਰਯੋਗ ਕਰ ਰਹੇ ਹਨ  
- **ਡਿਵੈਲਪਰ** ਜੋ ਭਾਸ਼ਾ ਮਾਡਲ ਸਮਰੱਥਾਵਾਂ ਨਾਲ AI ਐਪਲੀਕੇਸ਼ਨ ਬਣਾਉਣ ਚਾਹੁੰਦੇ ਹਨ  
- **ਕੋਈ ਵੀ** ਜੋ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ, ਚੈਟ, ਅਤੇ ਭਾਸ਼ਾ ਕਾਰਜਾਂ ਲਈ ਐਪਲ ਸਿਲਿਕਾਨ ਦੀ ਸਮਰੱਥਾ ਦਾ ਲਾਭ ਉਠਾਉਣਾ ਚਾਹੁੰਦਾ ਹੈ  

## LLM ਵਿਕਾਸ ਲਈ ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

### 1. ਯੂਨਿਫਾਇਡ ਮੈਮਰੀ ਆਰਕੀਟੈਕਚਰ  
ਐਪਲ ਸਿਲਿਕਾਨ ਦੀ ਯੂਨਿਫਾਇਡ ਮੈਮਰੀ MLX ਨੂੰ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਸੰਭਾਲਣ ਦੀ ਯੋਗਤਾ ਦਿੰਦੀ ਹੈ। ਇਸ ਨਾਲ ਉਹ ਮਾਡਲਾਂ ਨੂੰ ਉਸੇ ਹਾਰਡਵੇਅਰ 'ਤੇ ਚਲਾਉਣ ਦੀ ਸਮਰੱਥਾ ਮਿਲਦੀ ਹੈ।  

### 2. ਮੂਲ ਐਪਲ ਸਿਲਿਕਾਨ ਅਨੁਕੂਲਤਾ  
MLX ਨੂੰ ਐਪਲ ਦੇ M-ਸਿਰੀਜ਼ ਚਿਪਸ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਬਣਾਇਆ ਗਿਆ ਹੈ, ਜੋ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਵਿੱਚ ਵਰਤੇ ਜਾਣ ਵਾਲੇ ਟ੍ਰਾਂਸਫਾਰਮਰ ਆਰਕੀਟੈਕਚਰਾਂ ਲਈ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।  

### 3. ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਸਮਰਥਨ  
4-ਬਿਟ ਅਤੇ 8-ਬਿਟ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਲਈ ਬਣਾਇਆ ਗਿਆ ਸਮਰਥਨ ਮਾਡਲ ਗੁਣਵੱਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਮੈਮਰੀ ਦੀ ਲੋੜ ਨੂੰ ਘਟਾਉਂਦਾ ਹੈ। ਇਸ ਨਾਲ ਵੱਡੇ ਮਾਡਲਾਂ ਨੂੰ ਉਪਭੋਗਤਾ ਹਾਰਡਵੇਅਰ 'ਤੇ ਚਲਾਉਣਾ ਸੰਭਵ ਹੁੰਦਾ ਹੈ।  

### 4. ਹੱਗਿੰਗ ਫੇਸ ਇੰਟੀਗ੍ਰੇਸ਼ਨ  
ਹੱਗਿੰਗ ਫੇਸ ਇਕੋਸਿਸਟਮ ਨਾਲ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਸਧਾਰਨ ਕਨਵਰਜ਼ਨ ਟੂਲਾਂ ਨਾਲ ਹਜ਼ਾਰਾਂ ਪ੍ਰੀ-ਟ੍ਰੇਨਡ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਤੱਕ ਪਹੁੰਚ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।  

### 5. LoRA ਫਾਈਨ-ਟਿਊਨਿੰਗ  
Low-Rank Adaptation (LoRA) ਲਈ ਸਮਰਥਨ ਵੱਡੇ ਮਾਡਲਾਂ ਨੂੰ ਘੱਟ ਗਣਨਾਤਮਕ ਸਰੋਤਾਂ ਨਾਲ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਦੀ ਯੋਗਤਾ ਦਿੰਦਾ ਹੈ।  

## ਇੰਸਟਾਲੇਸ਼ਨ ਗਾਈਡ

### ਸਿਸਟਮ ਦੀਆਂ ਲੋੜਾਂ  
- **macOS 13.0+** (ਐਪਲ ਸਿਲਿਕਾਨ ਅਨੁਕੂਲਤਾ ਲਈ)  
- **Python 3.8+**  
- **ਐਪਲ ਸਿਲਿਕਾਨ** (M1, M2, M3, M4 ਸਿਰੀਜ਼)  
- **ਮੂਲ ARM ਵਾਤਾਵਰਣ** (Rosetta ਦੇ ਤਹਿਤ ਨਹੀਂ ਚੱਲ ਰਿਹਾ)  
- **8GB+ RAM** (ਵੱਡੇ ਮਾਡਲਾਂ ਲਈ 16GB+ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਗਈ ਹੈ)  

### LLMs ਲਈ ਤੇਜ਼ ਇੰਸਟਾਲੇਸ਼ਨ  

ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂਆਤ ਕਰਨ ਦਾ ਸਭ ਤੋਂ ਆਸਾਨ ਤਰੀਕਾ MLX-LM ਨੂੰ ਇੰਸਟਾਲ ਕਰਨਾ ਹੈ:  

```bash
pip install mlx-lm
```  

ਇਹ ਇੱਕ ਕਮਾਂਡ MLX ਫਰੇਮਵਰਕ ਅਤੇ ਭਾਸ਼ਾ ਮਾਡਲ ਯੂਟਿਲਿਟੀਜ਼ ਦੋਵਾਂ ਨੂੰ ਇੰਸਟਾਲ ਕਰਦੀ ਹੈ।  

### ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ ਸੈਟਅਪ (ਸਿਫਾਰਸ਼ ਕੀਤੀ ਗਈ)  

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```  

### ਆਡੀਓ ਮਾਡਲਾਂ ਲਈ ਵਾਧੂ ਡਿਪੈਂਡੈਂਸੀਜ਼  

ਜੇਕਰ ਤੁਸੀਂ Whisper ਵਰਗੇ ਸਪੀਚ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨ ਦੀ ਯੋਜਨਾ ਬਣਾਉਂਦੇ ਹੋ:  

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```  

## MLX ਨਾਲ ਸ਼ੁਰੂਆਤ

### ਤੁਹਾਡਾ ਪਹਿਲਾ ਭਾਸ਼ਾ ਮਾਡਲ  

ਆਓ ਇੱਕ ਸਧਾਰਨ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਉਦਾਹਰਨ ਚਲਾਉਣ ਨਾਲ ਸ਼ੁਰੂ ਕਰੀਏ:  

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```  

### Python API ਉਦਾਹਰਨ  

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```  

### ਮਾਡਲ ਲੋਡਿੰਗ ਨੂੰ ਸਮਝਣਾ  

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```  

## MLX-LM: ਭਾਸ਼ਾ ਮਾਡਲ

### ਸਮਰਥਿਤ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰ  

MLX-LM ਹੇਠਾਂ ਦਿੱਤੇ ਪ੍ਰਸਿੱਧ ਭਾਸ਼ਾ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:  

- **LLaMA ਅਤੇ LLaMA 2** - Meta ਦੇ ਮੂਲ ਮਾਡਲ  
- **Mistral ਅਤੇ Mixtral** - ਕੁਸ਼ਲ ਅਤੇ ਸ਼ਕਤੀਸ਼ਾਲੀ ਮਾਡਲ  
- **Phi-3** - Microsoft ਦੇ ਸੰਕੁਚਿਤ ਭਾਸ਼ਾ ਮਾਡਲ  
- **Qwen** - Alibaba ਦੇ ਬਹੁਭਾਸ਼ੀ ਮਾਡਲ  
- **Code Llama** - ਕੋਡ ਜਨਰੇਸ਼ਨ ਲਈ ਖਾਸ  
- **Gemma** - Google ਦੇ ਖੁੱਲੇ ਭਾਸ਼ਾ ਮਾਡਲ  

### ਕਮਾਂਡ ਲਾਈਨ ਇੰਟਰਫੇਸ  

MLX-LM ਕਮਾਂਡ ਲਾਈਨ ਇੰਟਰਫੇਸ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨ ਲਈ ਸ਼ਕਤੀਸ਼ਾਲੀ ਟੂਲ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ:  

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```  

### ਤਕਨੀਕੀ ਵਰਤੋਂ ਲਈ Python API  

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```  

## ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨਾ

### ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ ਪੈਟਰਨ

#### ਸਿੰਗਲ-ਟਰਨ ਜਨਰੇਸ਼ਨ  
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```  

#### ਨਿਰਦੇਸ਼ਾਂ ਦੀ ਪਾਲਣਾ  
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```  

#### ਰਚਨਾਤਮਕ ਲਿਖਤ  
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```  

### ਬਹੁ-ਟਰਨ ਗੱਲਬਾਤਾਂ  

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```  

## ਹੱਗਿੰਗ ਫੇਸ ਇੰਟੀਗ੍ਰੇਸ਼ਨ

### MLX-ਅਨੁਕੂਲ ਮਾਡਲ ਲੱਭਣਾ  

MLX ਹੱਗਿੰਗ ਫੇਸ ਇਕੋਸਿਸਟਮ ਨਾਲ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਕੰਮ ਕਰਦਾ ਹੈ:  

- **MLX ਮਾਡਲਾਂ ਨੂੰ ਬ੍ਰਾਊਜ਼ ਕਰੋ**: https://huggingface.co/models?library=mlx&sort=trending  
- **MLX ਕਮਿਊਨਿਟੀ**: https://huggingface.co/mlx-community (ਪ੍ਰੀ-ਕਨਵਰਟਡ ਮਾਡਲ)  
- **ਅਸਲ ਮਾਡਲ**: ਜ਼ਿਆਦਾਤਰ LLaMA, Mistral, Phi, ਅਤੇ Qwen ਮਾਡਲ ਕਨਵਰਜ਼ਨ ਨਾਲ ਕੰਮ ਕਰਦੇ ਹਨ  

### ਹੱਗਿੰਗ ਫੇਸ ਤੋਂ ਮਾਡਲ ਲੋਡ ਕਰਨਾ  

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```  

### ਆਫਲਾਈਨ ਵਰਤੋਂ ਲਈ ਮਾਡਲ ਡਾਊਨਲੋਡ ਕਰਨਾ  

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```  

## ਮਾਡਲ ਕਨਵਰਜ਼ਨ ਅਤੇ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ

### ਹੱਗਿੰਗ ਫੇਸ ਮਾਡਲਾਂ ਨੂੰ MLX ਵਿੱਚ ਕਨਵਰਟ ਕਰਨਾ  

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```  

### ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਨੂੰ ਸਮਝਣਾ  

ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦਾ ਆਕਾਰ ਅਤੇ ਮੈਮਰੀ ਦੀ ਵਰਤੋਂ ਘਟਾਉਂਦਾ ਹੈ:  

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```  

### ਕਸਟਮ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ  

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```  

## ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨਾ

### LoRA (Low-Rank Adaptation) ਫਾਈਨ-ਟਿਊਨਿੰਗ  

MLX ਘੱਟ ਗਣਨਾਤਮਕ ਸਰੋਤਾਂ ਨਾਲ ਵੱਡੇ ਮਾਡਲਾਂ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਲਈ LoRA ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:  

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```  

### ਟ੍ਰੇਨਿੰਗ ਡਾਟਾ ਤਿਆਰ ਕਰਨਾ  

ਆਪਣੇ ਟ੍ਰੇਨਿੰਗ ਉਦਾਹਰਨਾਂ ਨਾਲ ਇੱਕ JSON ਫਾਈਲ ਬਣਾਓ:  

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```  

### ਫਾਈਨ-ਟਿਊਨਿੰਗ ਕਮਾਂਡ  

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```  

### ਫਾਈਨ-ਟਿਊਨਡ ਮਾਡਲਾਂ ਦੀ ਵਰਤੋਂ  

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```  

## ਤਕਨੀਕੀ LLM ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

### ਕੁਸ਼ਲਤਾ ਲਈ ਪ੍ਰੋੰਪਟ ਕੈਸ਼ਿੰਗ  

ਇੱਕ ਹੀ ਸੰਦਰਭ ਦੀ ਦੁਹਰਾਈ ਵਰਤੋਂ ਲਈ, MLX ਪ੍ਰੋੰਪਟ ਕੈਸ਼ਿੰਗ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:  

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```  

### ਸਟ੍ਰੀਮਿੰਗ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ  

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```  

### ਕੋਡ ਜਨਰੇਸ਼ਨ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨਾ  

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```  

### ਚੈਟ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨਾ  

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```  

## LLMs ਲਈ ਸਰਵੋਤਮ ਅਭਿਆਸ

### ਮੈਮਰੀ ਪ੍ਰਬੰਧਨ  

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```  

### ਮਾਡਲ ਚੋਣ ਲਈ ਦਿਸ਼ਾ-ਨਿਰਦੇਸ਼  

**ਪ੍ਰਯੋਗ ਅਤੇ ਸਿੱਖਣ ਲਈ:**  
- 4-ਬਿਟ ਕੁਆੰਟਾਈਜ਼ਡ ਮਾਡਲ ਵਰਤੋ (ਜਿਵੇਂ `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)  
- ਛੋਟੇ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ ਜਿਵੇਂ Phi-3-mini  

**ਉਤਪਾਦਨ ਐਪਲੀਕੇਸ਼ਨ ਲਈ:**  
- ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗੁਣਵੱਤਾ ਦੇ ਵਿਚਕਾਰ ਵਪਾਰ ਨੂੰ ਵਿਚਾਰ ਕਰੋ  
- ਕੁਆੰਟਾਈਜ਼ਡ ਅਤੇ ਫੁੱਲ-ਪ੍ਰਿਸੀਜ਼ਨ ਮਾਡਲਾਂ ਦੋਵਾਂ ਦੀ ਜਾਂਚ ਕਰੋ  
- ਆਪਣੇ ਖਾਸ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ 'ਤੇ ਬੈਂਚਮਾਰਕ ਕਰੋ  

**ਖਾਸ ਕਾਰਜਾਂ ਲਈ:**  
- **ਕੋਡ ਜਨਰੇਸ਼ਨ**: CodeLlama, Code Llama Instruct  
- **ਜਨਰਲ ਚੈਟ**: Mistral-7B-Instruct, Phi-3  
- **ਬਹੁਭਾਸ਼ੀ**: Qwen ਮਾਡਲ  
- **ਰਚਨਾਤਮਕ ਲਿਖਤ**: Mistral ਜਾਂ LLaMA ਨਾਲ ਉੱਚ ਤਾਪਮਾਨ ਸੈਟਿੰਗਾਂ  

### ਪ੍ਰੋੰਪਟ ਇੰਜੀਨੀਅਰਿੰਗ ਸਰਵੋਤਮ ਅਭਿਆਸ  

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```  

### ਪ੍ਰਦਰਸ਼ਨ ਅਨੁਕੂਲਤਾ  

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```  

## ਟ੍ਰਬਲਸ਼ੂਟਿੰਗ

### ਆਮ ਸਮੱਸਿਆਵਾਂ ਅਤੇ ਹੱਲ

#### ਇੰਸਟਾਲੇਸ਼ਨ ਸਮੱਸਿਆਵਾਂ  

**ਸਮੱਸਿਆ**: "No matching distribution found for mlx-lm"  
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```  

**ਹੱਲ**: ਮੂਲ ARM Python ਜਾਂ Miniconda ਵਰਤੋ:  
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```  

#### ਮੈਮਰੀ ਸਮੱਸਿਆਵਾਂ  

**ਸਮੱਸਿਆ**: "RuntimeError: Out of memory"  
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```  

#### ਮਾਡਲ ਲੋਡਿੰਗ ਸਮੱਸਿਆਵਾਂ  

**ਸਮੱਸਿਆ**: ਮਾਡਲ ਲੋਡ ਕਰਨ ਵਿੱਚ ਅਸਫਲ ਜਾਂ ਖਰਾਬ ਆਉਟਪੁੱਟ ਜਨਰੇਟ ਕਰਦਾ ਹੈ  
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```  

#### ਪ੍ਰਦਰਸ਼ਨ ਸਮੱਸਿਆਵਾਂ  

**ਸਮੱਸਿਆ**: ਜਨਰੇਸ਼ਨ ਦੀ ਗਤੀ ਹੌਲੀ ਹੈ  
- ਹੋਰ ਮੈਮਰੀ-ਗਹਿਰਾਈ ਐਪਲੀਕੇਸ਼ਨ ਬੰਦ ਕਰੋ  
- ਸੰਭਵ ਹੋਣ 'ਤੇ ਕੁਆੰਟਾਈਜ਼ਡ ਮਾਡਲ ਵਰਤੋ  
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਸੀਂ Rosetta ਦੇ ਤਹਿਤ ਨਹੀਂ ਚੱਲ ਰਹੇ  
- ਮਾਡਲ ਲੋਡ ਕਰਨ ਤੋਂ ਪਹਿਲਾਂ ਉਪਲਬਧ ਮੈਮਰੀ ਦੀ ਜਾਂਚ ਕਰੋ  

### ਡੀਬੱਗਿੰਗ ਟਿਪਸ  

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```  

## ਵਾਧੂ ਸਰੋਤ

### ਅਧਿਕਾਰਤ ਦਸਤਾਵੇਜ਼ ਅਤੇ ਰਿਪੋਜ਼ਿਟਰੀਜ਼  

- **MLX GitHub ਰਿਪੋਜ਼ਿਟਰੀ**: https://github.com/ml-explore/mlx  
- **MLX-LM ਉਦਾਹਰਨਾਂ**: https://github.com/ml-explore/mlx-examples/tree/main/llms  
- **MLX ਦਸਤਾਵੇਜ਼**: https://ml-explore.github.io/mlx/  
- **ਹੱਗਿੰਗ ਫੇਸ MLX ਇੰਟੀਗ੍ਰੇਸ਼ਨ**: https://huggingface.co/docs/hub/en/mlx  

### ਮਾਡਲ ਕਲੈਕਸ਼ਨ  

- **MLX ਕਮਿਊਨਿਟੀ ਮਾਡਲ**: https://huggingface.co/mlx-community  
- **ਟ੍ਰੈਂਡਿੰਗ MLX ਮਾਡਲ**: https://huggingface.co/models?library=mlx&sort=trending  

### ਉਦਾਹਰਨ ਐਪਲੀਕੇਸ਼ਨ  

1. **ਪ੍ਰਸਨਲ AI ਅਸਿਸਟੈਂਟ**: ਗੱਲਬਾਤ ਮੈਮਰੀ ਨਾਲ ਸਥਾਨਕ ਚੈਟਬੋਟ ਬਣਾਓ  
2. **ਕੋਡ ਹੈਲਪਰ**: ਆਪਣੇ ਵਿਕਾਸ ਵਰਕਫਲੋ ਲਈ ਕੋਡਿੰਗ ਅਸਿਸਟੈਂਟ ਬਣਾਓ  
3. **ਸਮੱਗਰੀ ਜਨਰੇਟਰ**: ਲਿਖਤ, ਸੰਖੇਪਣ, ਅਤੇ ਸਮੱਗਰੀ ਬਣਾਉਣ ਲਈ ਟੂਲ ਵਿਕਸਿਤ ਕਰੋ  
4. **ਕਸਟਮ ਫਾਈਨ-ਟਿਊਨਡ ਮਾਡਲ**: ਖੇਤਰ-ਵਿਸ਼ੇਸ਼ ਕਾਰਜਾਂ ਲਈ ਮਾਡਲ ਅਨੁਕੂਲਿਤ ਕਰੋ  
5. **ਮਲਟੀ-ਮੋਡਲ ਐਪਲੀਕੇਸ਼ਨ**: MLX ਦੀਆਂ ਹੋਰ ਸਮਰੱਥਾਵਾਂ ਨਾਲ ਟੈਕਸਟ ਜ

---

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚੱਜੇਪਣ ਹੋ ਸਕਦੇ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।