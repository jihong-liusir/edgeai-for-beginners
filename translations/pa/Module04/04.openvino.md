<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T23:30:20+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 4 : OpenVINO ਟੂਲਕਿਟ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਸੂਟ

## ਸੂਚੀ
1. [ਪ੍ਰਸਤਾਵਨਾ](../../../Module04)
2. [OpenVINO ਕੀ ਹੈ?](../../../Module04)
3. [ਇੰਸਟਾਲੇਸ਼ਨ](../../../Module04)
4. [ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਮਾਰਗਦਰਸ਼ਕ](../../../Module04)
5. [ਉਦਾਹਰਨ: ਮਾਡਲਾਂ ਨੂੰ OpenVINO ਨਾਲ ਕਨਵਰਟ ਅਤੇ ਅਪਟੀਮਾਈਜ਼ ਕਰਨਾ](../../../Module04)
6. [ਤਕਨੀਕੀ ਵਰਤੋਂ](../../../Module04)
7. [ਸਰਵੋਤਮ ਅਭਿਆਸ](../../../Module04)
8. [ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ](../../../Module04)
9. [ਵਾਧੂ ਸਰੋਤ](../../../Module04)

## ਪ੍ਰਸਤਾਵਨਾ

OpenVINO (Open Visual Inference and Neural Network Optimization) ਇੰਟਲ ਦਾ ਖੁੱਲ੍ਹਾ-ਸਰੋਤ ਟੂਲਕਿਟ ਹੈ ਜੋ ਕਲਾਉਡ, ਓਨ-ਪ੍ਰੇਮਿਸ, ਅਤੇ ਐਜ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਪ੍ਰਦਰਸ਼ਨਸ਼ੀਲ AI ਹੱਲਾਂ ਨੂੰ ਤੈਨਾਤ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ। ਚਾਹੇ ਤੁਸੀਂ CPUs, GPUs, VPUs ਜਾਂ ਵਿਸ਼ੇਸ਼ AI ਐਕਸਲੇਰੇਟਰਾਂ ਨੂੰ ਟਾਰਗੇਟ ਕਰ ਰਹੇ ਹੋਵੋ, OpenVINO ਮਾਡਲ ਦੀ ਸ਼ੁੱਧਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਅਤੇ ਪਲੇਟਫਾਰਮਾਂ ਵਿੱਚ ਤੈਨਾਤੀ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦੇ ਹੋਏ ਵਿਸ਼ਾਲ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਸਮਰੱਥਾਵਾਂ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

## OpenVINO ਕੀ ਹੈ?

OpenVINO ਇੱਕ ਖੁੱਲ੍ਹਾ-ਸਰੋਤ ਟੂਲਕਿਟ ਹੈ ਜੋ ਵਿਕਾਸਕਾਰਾਂ ਨੂੰ AI ਮਾਡਲਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਪਲੇਟਫਾਰਮਾਂ 'ਤੇ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਅਪਟੀਮਾਈਜ਼, ਕਨਵਰਟ ਅਤੇ ਤੈਨਾਤ ਕਰਨ ਦੀ ਯੋਗਤਾ ਦਿੰਦਾ ਹੈ। ਇਹ ਤਿੰਨ ਮੁੱਖ ਹਿੱਸਿਆਂ 'ਤੇ ਆਧਾਰਿਤ ਹੈ: OpenVINO Runtime ਇੰਫਰੈਂਸ ਲਈ, Neural Network Compression Framework (NNCF) ਮਾਡਲ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਈ, ਅਤੇ OpenVINO Model Server ਸਕੇਲਬਲ ਤੈਨਾਤੀ ਲਈ।

### ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

- **ਪਲੇਟਫਾਰਮਾਂ ਵਿੱਚ ਤੈਨਾਤੀ**: Linux, Windows, ਅਤੇ macOS ਲਈ ਸਹਾਇਕ, Python, C++, ਅਤੇ C APIs ਦੇ ਨਾਲ
- **ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ**: CPU, GPU, VPU, ਅਤੇ AI ਐਕਸਲੇਰੇਟਰਾਂ ਲਈ ਆਟੋਮੈਟਿਕ ਡਿਵਾਈਸ ਖੋਜ ਅਤੇ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ
- **ਮਾਡਲ ਕੰਪ੍ਰੈਸ਼ਨ ਫਰੇਮਵਰਕ**: NNCF ਰਾਹੀਂ ਅਗਰਸਰ ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ, ਪ੍ਰੂਨਿੰਗ, ਅਤੇ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਤਕਨੀਕਾਂ
- **ਫਰੇਮਵਰਕ ਅਨੁਕੂਲਤਾ**: TensorFlow, ONNX, PaddlePaddle, ਅਤੇ PyTorch ਮਾਡਲਾਂ ਲਈ ਸਿੱਧੀ ਸਹਾਇਤਾ
- **ਜਨਰੇਟਿਵ AI ਸਹਾਇਤਾ**: ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਅਤੇ ਜਨਰੇਟਿਵ AI ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਤੈਨਾਤ ਕਰਨ ਲਈ ਵਿਸ਼ੇਸ਼ OpenVINO GenAI

### ਫਾਇਦੇ

- **ਪ੍ਰਦਰਸ਼ਨ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ**: ਘੱਟ ਸ਼ੁੱਧਤਾ ਨੁਕਸਾਨ ਨਾਲ ਮਹੱਤਵਪੂਰਨ ਗਤੀਵਿਧੀਆਂ ਵਿੱਚ ਸੁਧਾਰ
- **ਤੈਨਾਤੀ ਫੁਟਪ੍ਰਿੰਟ ਘਟਾਉਣਾ**: ਘੱਟ ਬਾਹਰੀ ਨਿਰਭਰਤਾਵਾਂ ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਤੈਨਾਤੀ ਨੂੰ ਸਧਾਰਨ ਬਣਾਉਂਦੀਆਂ ਹਨ
- **ਸ਼ੁਰੂਆਤੀ ਸਮਾਂ ਸੁਧਾਰ**: ਤੇਜ਼ ਐਪਲੀਕੇਸ਼ਨ ਸ਼ੁਰੂਆਤ ਲਈ ਅਪਟੀਮਾਈਜ਼ ਮਾਡਲ ਲੋਡਿੰਗ ਅਤੇ ਕੈਸ਼ਿੰਗ
- **ਸਕੇਲਬਲ ਤੈਨਾਤੀ**: ਐਜ ਡਿਵਾਈਸਾਂ ਤੋਂ ਕਲਾਉਡ ਇੰਫਰਾਸਟਰਕਚਰ ਤੱਕ ਸਥਿਰ APIs ਦੇ ਨਾਲ
- **ਪ੍ਰੋਡਕਸ਼ਨ ਤਿਆਰ**: ਵਿਸ਼ਵਾਸਯੋਗਤਾ ਦੇ ਨਾਲ ਵਿਸ਼ਾਲ ਦਸਤਾਵੇਜ਼ ਅਤੇ ਕਮਿਊਨਿਟੀ ਸਹਾਇਤਾ

## ਇੰਸਟਾਲੇਸ਼ਨ

### ਪੂਰਵ ਸ਼ਰਤਾਂ

- Python 3.8 ਜਾਂ ਇਸ ਤੋਂ ਉੱਚਾ
- pip ਪੈਕੇਜ ਮੈਨੇਜਰ
- ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ (ਸਿਫਾਰਸ਼ ਕੀਤੀ ਗਈ)
- ਅਨੁਕੂਲ ਹਾਰਡਵੇਅਰ (ਇੰਟਲ CPUs ਸਿਫਾਰਸ਼ ਕੀਤੇ ਗਏ, ਪਰ ਵੱਖ-ਵੱਖ ਆਰਕੀਟੈਕਚਰਾਂ ਦਾ ਸਮਰਥਨ ਹੈ)

### ਬੁਨਿਆਦੀ ਇੰਸਟਾਲੇਸ਼ਨ

ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ ਬਣਾਓ ਅਤੇ ਸਕ੍ਰਿਆ ਕਰੋ:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

OpenVINO Runtime ਇੰਸਟਾਲ ਕਰੋ:

```bash
pip install openvino
```

ਮਾਡਲ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਈ NNCF ਇੰਸਟਾਲ ਕਰੋ:

```bash
pip install nncf
```

### OpenVINO GenAI ਇੰਸਟਾਲੇਸ਼ਨ

ਜਨਰੇਟਿਵ AI ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ:

```bash
pip install openvino-genai
```

### ਵਿਕਲਪਕ ਨਿਰਭਰਤਾਵਾਂ

ਖਾਸ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਲਈ ਵਾਧੂ ਪੈਕੇਜ:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### ਇੰਸਟਾਲੇਸ਼ਨ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

ਜੇ ਸਫਲ ਹੋਵੇ, ਤਾਂ ਤੁਹਾਨੂੰ OpenVINO ਵਰਜਨ ਜਾਣਕਾਰੀ ਦਿਖਾਈ ਦੇਣੀ ਚਾਹੀਦੀ ਹੈ।

## ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਮਾਰਗਦਰਸ਼ਕ

### ਤੁਹਾਡਾ ਪਹਿਲਾ ਮਾਡਲ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

ਆਓ OpenVINO ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ Hugging Face ਮਾਡਲ ਨੂੰ ਕਨਵਰਟ ਅਤੇ ਅਪਟੀਮਾਈਜ਼ ਕਰੀਏ:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### ਇਹ ਪ੍ਰਕਿਰਿਆ ਕੀ ਕਰਦੀ ਹੈ

ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ: ਮੂਲ ਮਾਡਲ ਨੂੰ Hugging Face ਤੋਂ ਲੋਡ ਕਰਨਾ, OpenVINO Intermediate Representation (IR) ਫਾਰਮੈਟ ਵਿੱਚ ਕਨਵਰਟ ਕਰਨਾ, ਡਿਫਾਲਟ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਾਗੂ ਕਰਨਾ, ਅਤੇ ਟਾਰਗੇਟ ਹਾਰਡਵੇਅਰ ਲਈ ਕੰਪਾਇਲ ਕਰਨਾ।

### ਮੁੱਖ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਵਿਆਖਿਆ

- `export=True`: ਮਾਡਲ ਨੂੰ OpenVINO IR ਫਾਰਮੈਟ ਵਿੱਚ ਕਨਵਰਟ ਕਰਦਾ ਹੈ
- `compile=False`: ਲਚਕਤਾ ਲਈ ਰਨਟਾਈਮ ਤੱਕ ਕੰਪਾਇਲੇਸ਼ਨ ਨੂੰ ਦੇਰੀ ਕਰਦਾ ਹੈ
- `device`: ਟਾਰਗੇਟ ਹਾਰਡਵੇਅਰ ("CPU", "GPU", "AUTO" ਆਟੋਮੈਟਿਕ ਚੋਣ ਲਈ)
- `save_pretrained()`: ਅਪਟੀਮਾਈਜ਼ ਮਾਡਲ ਨੂੰ ਦੁਬਾਰਾ ਵਰਤਣ ਲਈ ਸੇਵ ਕਰਦਾ ਹੈ

## ਉਦਾਹਰਨ: ਮਾਡਲਾਂ ਨੂੰ OpenVINO ਨਾਲ ਕਨਵਰਟ ਅਤੇ ਅਪਟੀਮਾਈਜ਼ ਕਰਨਾ

### ਪਦਾਅਵਾਰ 1: NNCF ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ ਨਾਲ ਮਾਡਲ ਕਨਵਰਸ਼ਨ

ਇੱਥੇ ਪੋਸਟ-ਟ੍ਰੇਨਿੰਗ ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ ਲਾਗੂ ਕਰਨ ਦਾ ਤਰੀਕਾ ਹੈ:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### ਪਦਾਅਵਾਰ 2: ਵਜ਼ਨ ਕੰਪ੍ਰੈਸ਼ਨ ਨਾਲ ਅਗਰਸਰ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

ਟ੍ਰਾਂਸਫਾਰਮਰ-ਅਧਾਰਿਤ ਮਾਡਲਾਂ ਲਈ, ਵਜ਼ਨ ਕੰਪ੍ਰੈਸ਼ਨ ਲਾਗੂ ਕਰੋ:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### ਪਦਾਅਵਾਰ 3: ਅਪਟੀਮਾਈਜ਼ ਮਾਡਲ ਨਾਲ ਇੰਫਰੈਂਸ

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### ਆਉਟਪੁੱਟ ਸਟ੍ਰਕਚਰ

ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਤੋਂ ਬਾਅਦ, ਤੁਹਾਡਾ ਮਾਡਲ ਡਾਇਰੈਕਟਰੀ ਸ਼ਾਮਲ ਕਰੇਗੀ:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## ਤਕਨੀਕੀ ਵਰਤੋਂ

### NNCF YAML ਨਾਲ ਸੰਰਚਨਾ

ਜਟਿਲ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲ ਲਈ, NNCF ਸੰਰਚਨਾ ਫਾਈਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰੋ:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

ਸੰਰਚਨਾ ਲਾਗੂ ਕਰੋ:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

GPU ਐਕਸਲੇਰੇਸ਼ਨ ਲਈ:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### ਬੈਚ ਪ੍ਰੋਸੈਸਿੰਗ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### ਮਾਡਲ ਸਰਵਰ ਤੈਨਾਤੀ

ਅਪਟੀਮਾਈਜ਼ ਮਾਡਲਾਂ ਨੂੰ OpenVINO Model Server ਨਾਲ ਤੈਨਾਤ ਕਰੋ:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

ਮਾਡਲ ਸਰਵਰ ਲਈ ਕਲਾਇੰਟ ਕੋਡ:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## ਸਰਵੋਤਮ ਅਭਿਆਸ

### 1. ਮਾਡਲ ਚੋਣ ਅਤੇ ਤਿਆਰੀ
- ਸਹਾਇਕ ਫਰੇਮਵਰਕਾਂ (PyTorch, TensorFlow, ONNX) ਤੋਂ ਮਾਡਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਮਾਡਲ ਇਨਪੁਟਸ ਫਿਕਸ ਜਾਂ ਜਾਣੇ-ਪਛਾਣੇ ਡਾਇਨਾਮਿਕ ਸ਼ੇਪਸ ਹਨ
- ਕੈਲੀਬਰੇਸ਼ਨ ਲਈ ਪ੍ਰਤੀਨਿਧੀ ਡੇਟਾਸੈਟ ਨਾਲ ਟੈਸਟ ਕਰੋ

### 2. ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਰਣਨੀਤੀ ਦੀ ਚੋਣ
- **ਪੋਸਟ-ਟ੍ਰੇਨਿੰਗ ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ**: ਤੇਜ਼ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਈ ਇੱਥੇ ਸ਼ੁਰੂ ਕਰੋ
- **ਵਜ਼ਨ ਕੰਪ੍ਰੈਸ਼ਨ**: ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਅਤੇ ਟ੍ਰਾਂਸਫਾਰਮਰਾਂ ਲਈ ਆਦਰਸ਼
- **ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ-ਅਵੇਅਰ ਟ੍ਰੇਨਿੰਗ**: ਜਦੋਂ ਸ਼ੁੱਧਤਾ ਮਹੱਤਵਪੂਰਨ ਹੋਵੇ

### 3. ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ
- **CPU**: ਸੰਤੁਲਿਤ ਪ੍ਰਦਰਸ਼ਨ ਲਈ INT8 ਕਵਾਂਟੀਜ਼ੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰੋ
- **GPU**: FP16 ਸ਼ੁੱਧਤਾ ਅਤੇ ਬੈਚ ਪ੍ਰੋਸੈਸਿੰਗ ਦਾ ਲਾਭ ਲਵੋ
- **VPU**: ਮਾਡਲ ਸਧਾਰਨ ਅਤੇ ਲੇਅਰ ਫਿਊਜ਼ਨ 'ਤੇ ਧਿਆਨ ਦਿਓ

### 4. ਪ੍ਰਦਰਸ਼ਨ ਟਿਊਨਿੰਗ
- **ਥਰੂਪੁੱਟ ਮੋਡ**: ਵੱਡੇ-ਪੈਮਾਨੇ ਬੈਚ ਪ੍ਰੋਸੈਸਿੰਗ ਲਈ
- **ਲੇਟੈਂਸੀ ਮੋਡ**: ਰੀਅਲ-ਟਾਈਮ ਇੰਟਰੈਕਟਿਵ ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ
- **AUTO ਡਿਵਾਈਸ**: OpenVINO ਨੂੰ ਸਹੀ ਹਾਰਡਵੇਅਰ ਚੁਣਨ ਦਿਓ

### 5. ਮੈਮੋਰੀ ਪ੍ਰਬੰਧਨ
- ਮੈਮੋਰੀ ਓਵਰਹੈੱਡ ਤੋਂ ਬਚਣ ਲਈ ਡਾਇਨਾਮਿਕ ਸ਼ੇਪਸ ਦੀ ਸਾਵਧਾਨੀ ਨਾਲ ਵਰਤੋਂ ਕਰੋ
- ਤੇਜ਼ ਲੋਡਿੰਗ ਲਈ ਮਾਡਲ ਕੈਸ਼ਿੰਗ ਲਾਗੂ ਕਰੋ
- ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਦੌਰਾਨ ਮੈਮੋਰੀ ਦੀ ਵਰਤੋਂ ਦੀ ਨਿਗਰਾਨੀ ਕਰੋ

### 6. ਸ਼ੁੱਧਤਾ ਦੀ ਪੁਸ਼ਟੀ
- ਹਮੇਸ਼ਾ ਅਪਟੀਮਾਈਜ਼ ਮਾਡਲਾਂ ਨੂੰ ਮੂਲ ਪ੍ਰਦਰਸ਼ਨ ਦੇ ਖਿਲਾਫ ਵੈਧ ਕਰਨਾ
- ਮੁਲਾਂਕਣ ਲਈ ਪ੍ਰਤੀਨਿਧੀ ਟੈਸਟ ਡੇਟਾਸੈਟ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਧੀਰੇ-ਧੀਰੇ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ 'ਤੇ ਵਿਚਾਰ ਕਰੋ (ਸੁਰੱਖਿਅਤ ਸੈਟਿੰਗਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ)

## ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ

### ਆਮ ਸਮੱਸਿਆਵਾਂ

#### 1. ਇੰਸਟਾਲੇਸ਼ਨ ਸਮੱਸਿਆਵਾਂ
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. ਮਾਡਲ ਕਨਵਰਸ਼ਨ ਗਲਤੀਆਂ
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. ਪ੍ਰਦਰਸ਼ਨ ਸਮੱਸਿਆਵਾਂ
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. ਮੈਮੋਰੀ ਸਮੱਸਿਆਵਾਂ
- ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਦੌਰਾਨ ਮਾਡਲ ਬੈਚ ਆਕਾਰ ਘਟਾਓ
- ਵੱਡੇ ਡੇਟਾਸੈਟ ਲਈ ਸਟ੍ਰੀਮਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਮਾਡਲ ਕੈਸ਼ਿੰਗ ਨੂੰ ਯਕੀਨੀ ਬਣਾਓ: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. ਸ਼ੁੱਧਤਾ ਘਟਾਉਣਾ
- ਉੱਚ ਸ਼ੁੱਧਤਾ ਦੀ ਵਰਤੋਂ ਕਰੋ (INT8 ਦੀ ਬਜਾਏ INT4)
- ਕੈਲੀਬਰੇਸ਼ਨ ਡੇਟਾਸੈਟ ਆਕਾਰ ਵਧਾਓ
- ਮਿਕਸਡ ਸ਼ੁੱਧਤਾ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਾਗੂ ਕਰੋ

### ਪ੍ਰਦਰਸ਼ਨ ਨਿਗਰਾਨੀ

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### ਮਦਦ ਪ੍ਰਾਪਤ ਕਰਨਾ

- **ਦਸਤਾਵੇਜ਼**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub ਸਮੱਸਿਆਵਾਂ**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **ਕਮਿਊਨਿਟੀ ਫੋਰਮ**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## ਵਾਧੂ ਸਰੋਤ

### ਅਧਿਕਾਰਤ ਲਿੰਕ
- **OpenVINO ਮੁੱਖ ਪੰਨਾ**: [openvino.ai](https://openvino.ai/)
- **GitHub ਰਿਪੋਜ਼ਟਰੀ**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF ਰਿਪੋਜ਼ਟਰੀ**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **ਮਾਡਲ ਜੂ**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### ਸਿੱਖਣ ਦੇ ਸਰੋਤ
- **OpenVINO ਨੋਟਬੁੱਕਸ**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਮਾਰਗਦਰਸ਼ਕ**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਮਾਰਗਦਰਸ਼ਕ**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਟੂਲ
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### ਪ੍ਰਦਰਸ਼ਨ ਬੈਂਚਮਾਰਕ
- **ਅਧਿਕਾਰਤ ਬੈਂਚਮਾਰਕ**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF ਮਾਡਲ ਜੂ**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### ਕਮਿਊਨਿਟੀ ਉਦਾਹਰਨ
- **Jupyter ਨੋਟਬੁੱਕਸ**: [OpenVINO ਨੋਟਬੁੱਕਸ ਰਿਪੋਜ਼ਟਰੀ](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO ਨੋਟਬੁੱਕਸ ਰਿਪੋਜ਼ਟਰੀ ਵਿੱਚ ਉਪਲਬਧ ਵਿਸਤ੍ਰਿਤ ਟਿਊਟੋਰਿਅਲ
- **ਨਮੂਨਾ ਐਪਲੀਕੇਸ਼ਨ**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - ਵੱਖ-ਵੱਖ ਖੇਤਰਾਂ ਲਈ ਅਸਲ-ਦੁਨੀਆ ਦੇ ਉਦਾਹਰਨ (ਕੰਪਿਊਟਰ ਵਿਜ਼ਨ, NLP,

---

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਤਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।