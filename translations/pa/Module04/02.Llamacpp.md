<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T23:41:01+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 2 : Llama.cpp ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ ਗਾਈਡ

## ਸੂਚੀ
1. [ਜਾਣ ਪਛਾਣ](../../../Module04)
2. [Llama.cpp ਕੀ ਹੈ?](../../../Module04)
3. [ਇੰਸਟਾਲੇਸ਼ਨ](../../../Module04)
4. [ਸੋਰਸ ਤੋਂ ਬਣਾਉਣਾ](../../../Module04)
5. [ਮਾਡਲ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ](../../../Module04)
6. [ਮੂਲ ਵਰਤੋਂ](../../../Module04)
7. [ਤਕਨੀਕੀ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ](../../../Module04)
8. [Python ਇੰਟੀਗ੍ਰੇਸ਼ਨ](../../../Module04)
9. [ਮਸਲੇ ਹੱਲ](../../../Module04)
10. [ਸਰਵੋਤਮ ਪਧਤੀਆਂ](../../../Module04)

## ਜਾਣ ਪਛਾਣ

ਇਹ ਵਿਸਤ੍ਰਿਤ ਟਿਊਟੋਰਿਅਲ ਤੁਹਾਨੂੰ Llama.cpp ਬਾਰੇ ਹਰ ਚੀਜ਼ ਵਿੱਚ ਮਾਹਰ ਬਣਾਉਣ ਵਿੱਚ ਮਦਦ ਕਰੇਗਾ, ਬੁਨਿਆਦੀ ਇੰਸਟਾਲੇਸ਼ਨ ਤੋਂ ਲੈ ਕੇ ਤਕਨੀਕੀ ਵਰਤੋਂ ਤੱਕ। Llama.cpp ਇੱਕ ਸ਼ਕਤੀਸ਼ਾਲੀ C++ ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ ਹੈ ਜੋ ਘੱਟ ਸੈਟਅਪ ਨਾਲ ਅਤੇ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਕਨਫਿਗਰੇਸ਼ਨ 'ਤੇ ਸ਼ਾਨਦਾਰ ਪ੍ਰਦਰਸ਼ਨ ਦੇ ਨਾਲ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ (LLMs) ਦੀ ਸਮਝਦਾਰੀ ਨੂੰ ਸਮਰੱਥ ਬਣਾਉਂਦਾ ਹੈ।

## Llama.cpp ਕੀ ਹੈ?

Llama.cpp ਇੱਕ LLM ਇੰਫਰੈਂਸ ਫਰੇਮਵਰਕ ਹੈ ਜੋ C/C++ ਵਿੱਚ ਲਿਖਿਆ ਗਿਆ ਹੈ ਅਤੇ ਘੱਟ ਸੈਟਅਪ ਨਾਲ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਸਥਾਨਕ ਤੌਰ 'ਤੇ ਚਲਾਉਣ ਦੀ ਸਮਰੱਥਾ ਦਿੰਦਾ ਹੈ। ਇਸ ਦੀਆਂ ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਹਨ:

### ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ
- **ਸਧਾਰਨ C/C++ ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ** ਬਿਨਾਂ ਕਿਸੇ ਡਿਪੈਂਡੈਂਸੀ ਦੇ
- **ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਅਨੁਕੂਲਤਾ** (Windows, macOS, Linux)
- **ਹਾਰਡਵੇਅਰ ਅਨੁਕੂਲਤਾ** ਵੱਖ-ਵੱਖ ਆਰਕੀਟੈਕਚਰ ਲਈ
- **ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਸਹਾਇਤਾ** (1.5-ਬਿਟ ਤੋਂ 8-ਬਿਟ ਇੰਟੀਜਰ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ)
- **CPU ਅਤੇ GPU ਐਕਸਲੇਰੇਸ਼ਨ** ਸਹਾਇਤਾ
- **ਮੈਮੋਰੀ ਕੁਸ਼ਲਤਾ** ਸੰਕੁਚਿਤ ਵਾਤਾਵਰਣਾਂ ਲਈ

### ਫਾਇਦੇ
- CPU 'ਤੇ ਬਿਨਾਂ ਕਿਸੇ ਵਿਸ਼ੇਸ਼ ਹਾਰਡਵੇਅਰ ਦੀ ਲੋੜ ਦੇ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਚਲਦਾ ਹੈ
- ਕਈ GPU ਬੈਕਐਂਡਸ (CUDA, Metal, OpenCL, Vulkan) ਨੂੰ ਸਹਾਇਕ
- ਹਲਕਾ ਅਤੇ ਪੋਰਟੇਬਲ
- Apple ਸਿਲਿਕਾਨ ਲਈ ਪਹਿਲੀ ਪਸੰਦ - ARM NEON, Accelerate ਅਤੇ Metal ਫਰੇਮਵਰਕ ਦੁਆਰਾ ਅਨੁਕੂਲਿਤ
- ਘੱਟ ਮੈਮੋਰੀ ਵਰਤੋਂ ਲਈ ਵੱਖ-ਵੱਖ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਪੱਧਰਾਂ ਦਾ ਸਮਰਥਨ

## ਇੰਸਟਾਲੇਸ਼ਨ

### ਵਿਧੀ 1: ਪ੍ਰੀ-ਬਿਲਟ ਬਾਈਨਰੀਜ਼ (ਨਵਾਂ ਸ਼ੁਰੂ ਕਰਨ ਵਾਲਿਆਂ ਲਈ ਸਿਫਾਰਸ਼ੀ)

#### GitHub ਰਿਲੀਜ਼ ਤੋਂ ਡਾਊਨਲੋਡ ਕਰੋ
1. [Llama.cpp GitHub ਰਿਲੀਜ਼](https://github.com/ggml-org/llama.cpp/releases) 'ਤੇ ਜਾਓ
2. ਆਪਣੇ ਸਿਸਟਮ ਲਈ ਉਚਿਤ ਬਾਈਨਰੀ ਡਾਊਨਲੋਡ ਕਰੋ:
   - Windows ਲਈ `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS ਲਈ `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux ਲਈ `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. ਆਰਕਾਈਵ ਨੂੰ ਐਕਸਟ੍ਰੈਕਟ ਕਰੋ ਅਤੇ ਡਾਇਰੈਕਟਰੀ ਨੂੰ ਆਪਣੇ ਸਿਸਟਮ ਦੇ PATH ਵਿੱਚ ਸ਼ਾਮਲ ਕਰੋ

#### ਪੈਕੇਜ ਮੈਨੇਜਰ ਵਰਤ ਕੇ

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (ਵੱਖ-ਵੱਖ ਡਿਸਟ੍ਰੀਬਿਊਸ਼ਨ):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### ਵਿਧੀ 2: Python ਪੈਕੇਜ (llama-cpp-python)

#### ਬੁਨਿਆਦੀ ਇੰਸਟਾਲੇਸ਼ਨ
```bash
pip install llama-cpp-python
```

#### ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਨਾਲ
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## ਸੋਰਸ ਤੋਂ ਬਣਾਉਣਾ

### ਪੂਰਵ ਸ਼ਰਤਾਂ

**ਸਿਸਟਮ ਦੀਆਂ ਲੋੜਾਂ:**
- C++ ਕੰਪਾਇਲਰ (GCC, Clang, ਜਾਂ MSVC)
- CMake (ਵਰਜਨ 3.14 ਜਾਂ ਉੱਚਾ)
- Git
- ਆਪਣੇ ਪਲੇਟਫਾਰਮ ਲਈ ਬਿਲਡ ਟੂਲਜ਼

**ਪੂਰਵ ਸ਼ਰਤਾਂ ਇੰਸਟਾਲ ਕਰਨਾ:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022 ਨੂੰ C++ ਵਿਕਾਸ ਟੂਲਜ਼ ਨਾਲ ਇੰਸਟਾਲ ਕਰੋ
- CMake ਨੂੰ ਅਧਿਕਾਰਤ ਵੈਬਸਾਈਟ ਤੋਂ ਇੰਸਟਾਲ ਕਰੋ
- Git ਇੰਸਟਾਲ ਕਰੋ

### ਬੁਨਿਆਦੀ ਬਿਲਡ ਪ੍ਰਕਿਰਿਆ

1. **ਰੀਪੋਜ਼ਟਰੀ ਕਲੋਨ ਕਰੋ:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **ਬਿਲਡ ਕਨਫਿਗਰ ਕਰੋ:**
```bash
cmake -B build
```

3. **ਪ੍ਰੋਜੈਕਟ ਬਿਲਡ ਕਰੋ:**
```bash
cmake --build build --config Release
```

ਤੇਜ਼ ਕੰਪਾਇਲੇਸ਼ਨ ਲਈ, ਪੈਰਲਲ ਜੌਬਸ ਵਰਤੋ:
```bash
cmake --build build --config Release -j 8
```

### ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਬਿਲਡ

#### CUDA ਸਹਾਇਤਾ (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal ਸਹਾਇਤਾ (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS ਸਹਾਇਤਾ (CPU ਅਨੁਕੂਲਤਾ)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan ਸਹਾਇਤਾ
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### ਤਕਨੀਕੀ ਬਿਲਡ ਵਿਕਲਪ

#### ਡਿਬੱਗ ਬਿਲਡ
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### ਵਾਧੂ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਨਾਲ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## ਮਾਡਲ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ

### GGUF ਫਾਰਮੈਟ ਦੀ ਸਮਝ

GGUF (Generalized GGML Unified Format) ਇੱਕ ਅਨੁਕੂਲਿਤ ਫਾਈਲ ਫਾਰਮੈਟ ਹੈ ਜੋ Llama.cpp ਅਤੇ ਹੋਰ ਫਰੇਮਵਰਕਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਨੂੰ ਕੁਸ਼ਲਤਾਪੂਰਵਕ ਚਲਾਉਣ ਲਈ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਹ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ:

- ਮਾਡਲ ਵਜ਼ਨ ਸਟੋਰੇਜ ਦਾ ਮਿਆਰੀਕਰਨ
- ਪਲੇਟਫਾਰਮਾਂ ਵਿੱਚ ਵਧੀਆ ਅਨੁਕੂਲਤਾ
- ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ
- ਕੁਸ਼ਲ ਮੈਟਾਡੇਟਾ ਸੰਭਾਲ

### ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਕਿਸਮਾਂ

Llama.cpp ਵੱਖ-ਵੱਖ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਪੱਧਰਾਂ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:

| ਕਿਸਮ | ਬਿਟ | ਵੇਰਵਾ | ਵਰਤੋਂ ਦਾ ਕੇਸ |
|------|------|-------------|----------|
| F16 | 16 | ਹਾਫ ਪ੍ਰਿਸੀਸ਼ਨ | ਉੱਚ ਗੁਣਵੱਤਾ, ਵੱਡੀ ਮੈਮੋਰੀ |
| Q8_0 | 8 | 8-ਬਿਟ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ | ਵਧੀਆ ਸੰਤੁਲਨ |
| Q4_0 | 4 | 4-ਬਿਟ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ | ਦਰਮਿਆਨੀ ਗੁਣਵੱਤਾ, ਛੋਟਾ ਆਕਾਰ |
| Q2_K | 2 | 2-ਬਿਟ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ | ਸਭ ਤੋਂ ਛੋਟਾ ਆਕਾਰ, ਘੱਟ ਗੁਣਵੱਤਾ |

### ਮਾਡਲਾਂ ਨੂੰ ਰੂਪਾਂਤਰਿਤ ਕਰਨਾ

#### PyTorch ਤੋਂ GGUF ਵਿੱਚ
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face ਤੋਂ ਸਿੱਧਾ ਡਾਊਨਲੋਡ
ਕਈ ਮਾਡਲ GGUF ਫਾਰਮੈਟ ਵਿੱਚ Hugging Face 'ਤੇ ਉਪਲਬਧ ਹਨ:
- "GGUF" ਨਾਮ ਵਾਲੇ ਮਾਡਲਾਂ ਦੀ ਖੋਜ ਕਰੋ
- ਉਚਿਤ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਪੱਧਰ ਡਾਊਨਲੋਡ ਕਰੋ
- Llama.cpp ਨਾਲ ਸਿੱਧੇ ਵਰਤੋਂ ਕਰੋ

## ਮੂਲ ਵਰਤੋਂ

### ਕਮਾਂਡ ਲਾਈਨ ਇੰਟਰਫੇਸ

#### ਸਧਾਰਨ ਟੈਕਸਟ ਜਨਰੇਸ਼ਨ
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face ਤੋਂ ਮਾਡਲਾਂ ਦੀ ਵਰਤੋਂ
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### ਸਰਵਰ ਮੋਡ
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### ਆਮ ਪੈਰਾਮੀਟਰ

| ਪੈਰਾਮੀਟਰ | ਵੇਰਵਾ | ਉਦਾਹਰਨ |
|-----------|-------------|---------|
| `-m` | ਮਾਡਲ ਫਾਈਲ ਪਾਥ | `-m model.gguf` |
| `-p` | ਪ੍ਰੋਮਪਟ ਟੈਕਸਟ | `-p "Hello world"` |
| `-n` | ਜਨਰੇਟ ਕਰਨ ਲਈ ਟੋਕਨ ਦੀ ਗਿਣਤੀ | `-n 100` |
| `-c` | ਸੰਦਰਭ ਆਕਾਰ | `-c 4096` |
| `-t` | ਥ੍ਰੇਡ ਦੀ ਗਿਣਤੀ | `-t 8` |
| `-ngl` | GPU ਲੇਅਰ | `-ngl 32` |
| `-temp` | ਤਾਪਮਾਨ | `-temp 0.7` |

### ਇੰਟਰਐਕਟਿਵ ਮੋਡ

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## ਤਕਨੀਕੀ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

### ਸਰਵਰ API

#### ਸਰਵਰ ਸ਼ੁਰੂ ਕਰਨਾ
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API ਵਰਤੋਂ
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### ਪ੍ਰਦਰਸ਼ਨ ਅਨੁਕੂਲਤਾ

#### ਮੈਮੋਰੀ ਪ੍ਰਬੰਧਨ
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### ਮਲਟੀ-ਥ੍ਰੇਡਿੰਗ
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU ਐਕਸਲੇਰੇਸ਼ਨ
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python ਇੰਟੀਗ੍ਰੇਸ਼ਨ

### llama-cpp-python ਨਾਲ ਬੁਨਿਆਦੀ ਵਰਤੋਂ

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### ਚੈਟ ਇੰਟਰਫੇਸ

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### ਸਟ੍ਰੀਮਿੰਗ ਜਵਾਬ

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## ਮਸਲੇ ਹੱਲ

### ਆਮ ਸਮੱਸਿਆਵਾਂ ਅਤੇ ਹੱਲ

#### ਬਿਲਡ ਐਰਰ

**ਸਮੱਸਿਆ: CMake ਨਹੀਂ ਮਿਲਿਆ**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**ਸਮੱਸਿਆ: ਕੰਪਾਇਲਰ ਨਹੀਂ ਮਿਲਿਆ**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### ਰਨਟਾਈਮ ਸਮੱਸਿਆਵਾਂ

**ਸਮੱਸਿਆ: ਮਾਡਲ ਲੋਡ ਕਰਨ ਵਿੱਚ ਅਸਫਲ**
- ਮਾਡਲ ਫਾਈਲ ਪਾਥ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ
- ਫਾਈਲ ਅਧਿਕਾਰਾਂ ਦੀ ਜਾਂਚ ਕਰੋ
- ਯਥਾਰਥ RAM ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ
- ਵੱਖ-ਵੱਖ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਪੱਧਰਾਂ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ

**ਸਮੱਸਿਆ: ਖਰਾਬ ਪ੍ਰਦਰਸ਼ਨ**
- ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਚਾਲੂ ਕਰੋ
- ਥ੍ਰੇਡ ਦੀ ਗਿਣਤੀ ਵਧਾਓ
- ਉਚਿਤ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਵਰਤੋ
- GPU ਮੈਮੋਰੀ ਦੀ ਵਰਤੋਂ ਦੀ ਜਾਂਚ ਕਰੋ

#### ਮੈਮੋਰੀ ਸਮੱਸਿਆਵਾਂ

**ਸਮੱਸਿਆ: ਮੈਮੋਰੀ ਖਤਮ**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### ਪਲੇਟਫਾਰਮ-ਵਿਸ਼ੇਸ਼ ਸਮੱਸਿਆਵਾਂ

#### Windows
- MinGW ਜਾਂ Visual Studio ਕੰਪਾਇਲਰ ਦੀ ਵਰਤੋਂ ਕਰੋ
- PATH ਕਨਫਿਗਰੇਸ਼ਨ ਨੂੰ ਸਹੀ ਬਣਾਓ
- ਐਂਟੀਵਾਇਰਸ ਹਸਤਖੇਪ ਦੀ ਜਾਂਚ ਕਰੋ

#### macOS
- Apple Silicon ਲਈ Metal ਚਾਲੂ ਕਰੋ
- ਜ਼ਰੂਰਤ ਪੈਣ 'ਤੇ Rosetta 2 ਦੀ ਵਰਤੋਂ ਕਰੋ
- Xcode ਕਮਾਂਡ ਲਾਈਨ ਟੂਲਜ਼ ਦੀ ਜਾਂਚ ਕਰੋ

#### Linux
- ਵਿਕਾਸ ਪੈਕੇਜ ਇੰਸਟਾਲ ਕਰੋ
- GPU ਡ੍ਰਾਈਵਰ ਵਰਜਨ ਦੀ ਜਾਂਚ ਕਰੋ
- CUDA ਟੂਲਕਿਟ ਇੰਸਟਾਲੇਸ਼ਨ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ

## ਸਰਵੋਤਮ ਪਧਤੀਆਂ

### ਮਾਡਲ ਚੋਣ
1. **ਆਪਣੇ ਹਾਰਡਵੇਅਰ ਦੇ ਅਨੁਸਾਰ ਉਚਿਤ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਚੁਣੋ**
2. **ਮਾਡਲ ਆਕਾਰ ਅਤੇ ਗੁਣਵੱਤਾ ਦੇ ਸਮਝੌਤੇ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ**
3. **ਆਪਣੇ ਵਿਸ਼ੇਸ਼ ਵਰਤੋਂ ਦੇ ਕੇਸ ਲਈ ਵੱਖ-ਵੱਖ ਮਾਡਲਾਂ ਦੀ ਜਾਂਚ ਕਰੋ**

### ਪ੍ਰਦਰਸ਼ਨ ਅਨੁਕੂਲਤਾ
1. **GPU ਐਕਸਲੇਰੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰੋ** ਜਦੋਂ ਉਪਲਬਧ ਹੋਵੇ
2. **ਆਪਣੇ CPU ਲਈ ਥ੍ਰੇਡ ਦੀ ਗਿਣਤੀ ਅਨੁਕੂਲਿਤ ਕਰੋ**
3. **ਆਪਣੇ ਵਰਤੋਂ ਦੇ ਕੇਸ ਲਈ ਉਚਿਤ ਸੰਦਰਭ ਆਕਾਰ ਸੈਟ ਕਰੋ**
4. **ਵੱਡੇ ਮਾਡਲਾਂ ਲਈ ਮੈਮੋਰੀ ਮੈਪਿੰਗ ਚਾਲੂ ਕਰੋ**

### ਉਤਪਾਦਨ ਤੈਨਾਤੀ
1. **API ਪਹੁੰਚ ਲਈ ਸਰਵਰ ਮੋਡ ਦੀ ਵਰਤੋਂ ਕਰੋ**
2. **ਸਹੀ ਤਰੁਟੀਆਂ ਸੰਭਾਲਣ ਨੂੰ ਲਾਗੂ ਕਰੋ**
3. **ਸੰਸਾਧਨ ਦੀ ਵਰਤੋਂ ਦੀ ਨਿਗਰਾਨੀ ਕਰੋ**
4. **ਲੌਗਿੰਗ ਅਤੇ ਮਾਨੀਟਰਿੰਗ ਸੈਟ ਕਰੋ**

### ਵਿਕਾਸ ਵਰਕਫਲੋ
1. **ਟੈਸਟਿੰਗ ਲਈ ਛੋਟੇ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ**
2. **ਮਾਡਲ ਕਨਫਿਗਰੇਸ਼ਨ ਲਈ ਵਰਜਨ ਕੰਟਰੋਲ ਦੀ ਵਰਤੋਂ ਕਰੋ**
3. **ਆਪਣੀਆਂ ਕਨਫਿਗਰੇਸ਼ਨ ਨੂੰ ਦਸਤਾਵੇਜ਼ ਬਣਾਓ**
4. **ਵੱਖ-ਵੱਖ ਪਲੇਟਫਾਰਮਾਂ 'ਤੇ ਟੈਸਟ ਕਰੋ**

### ਸੁਰੱਖਿਆ ਵਿਚਾਰ
1. **ਇਨਪੁਟ ਪ੍ਰੋਮਪਟ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ**
2. **ਰੇਟ ਲਿਮਿਟਿੰਗ ਲਾਗੂ ਕਰੋ**
3. **API ਐਂਡਪੌਇੰਟਸ ਨੂੰ ਸੁਰੱਖਿਅਤ ਕਰੋ**
4. **ਦੁਰਵਿਵਹਾਰ ਦੇ ਪੈਟਰਨ ਦੀ ਨਿਗਰਾਨੀ ਕਰੋ**

## ਨਤੀਜਾ

Llama.cpp ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਕਨਫਿਗਰੇਸ਼ਨ 'ਤੇ ਸਥਾਨਕ ਤੌਰ 'ਤੇ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਚਲਾਉਣ ਲਈ ਇੱਕ ਸ਼ਕਤੀਸ਼ਾਲੀ ਅਤੇ ਕੁਸ਼ਲ ਤਰੀਕਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਚਾਹੇ ਤੁਸੀਂ AI ਐਪਲੀਕੇਸ਼ਨ ਵਿਕਸਿਤ ਕਰ ਰਹੇ ਹੋ, ਖੋਜ ਕਰ ਰਹੇ ਹੋ, ਜਾਂ ਸਿਰਫ LLMs ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਰਹੇ ਹੋ, ਇਹ ਫਰੇਮਵਰਕ ਵੱਖ-ਵੱਖ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਲਈ ਲਚਕਤਾ ਅਤੇ ਪ੍ਰਦਰਸ਼ਨ ਦੀ ਲੋੜ ਪੂਰੀ ਕਰਦਾ ਹੈ।

ਮੁੱਖ ਸਿੱਖਣ ਵਾਲੀਆਂ ਗੱਲਾਂ:
- ਉਹ ਇੰਸਟਾਲੇਸ਼ਨ ਵਿਧੀ ਚੁਣੋ ਜੋ ਤੁਹਾਡੇ ਲਈ ਸਭ ਤੋਂ ਵਧੀਆ ਹੋਵੇ
- ਆਪਣੇ ਵਿਸ਼ੇਸ਼ ਹਾਰਡਵੇਅਰ ਕਨਫਿਗਰੇਸ਼ਨ ਲਈ ਅਨੁਕੂਲਿਤ ਕਰੋ
- ਬੁਨਿਆਦੀ ਵਰਤੋਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ ਅਤੇ ਹੌਲੀ-ਹੌਲੀ ਤਕਨੀਕੀ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਦੀ ਖੋਜ ਕਰੋ
- ਆਸਾਨ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਲਈ Python ਬਾਈਡਿੰਗ ਦੀ ਵਰਤੋਂ ਕਰਨ ਬਾਰੇ ਸੋਚੋ
- ਉਤਪਾਦਨ ਤੈਨਾਤੀ ਲਈ ਸਰਵੋਤਮ ਪਧਤੀਆਂ ਦੀ ਪਾਲਣਾ ਕਰੋ

ਵਧੇਰੇ ਜਾਣਕਾਰੀ ਅਤੇ ਅੱਪਡੇਟ ਲਈ, [ਅਧਿਕਾਰਤ Llama.cpp ਰੀਪੋਜ਼ਟਰੀ](https://github.com/ggml-org/llama.cpp) 'ਤੇ ਜਾਓ ਅਤੇ ਉਪਲਬਧ ਵਿਸਤ੍ਰਿਤ ਦਸਤਾਵੇਜ਼ ਅਤੇ ਕਮਿਊਨਿਟੀ ਸਰੋਤਾਂ ਨੂੰ ਵੇਖੋ।

## ➡️ ਅਗਲਾ ਕੀ ਹੈ

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**ਅਸਵੀਕਾਰਨਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਹਾਲਾਂਕਿ ਅਸੀਂ ਸਹੀਅਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।