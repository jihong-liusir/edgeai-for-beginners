<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T23:36:06+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 3 : ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਸੂਟ

## ਸਮੱਗਰੀ ਦੀ ਸੂਚੀ
1. [ਪ੍ਰਸਤਾਵਨਾ](../../../Module04)
2. [ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਕੀ ਹੈ?](../../../Module04)
3. [ਇੰਸਟਾਲੇਸ਼ਨ](../../../Module04)
4. [ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਗਾਈਡ](../../../Module04)
5. [ਉਦਾਹਰਨ: Qwen3 ਨੂੰ ONNX INT4 ਵਿੱਚ ਬਦਲਣਾ](../../../Module04)
6. [ਤਕਨੀਕੀ ਵਰਤੋਂ](../../../Module04)
7. [ਸਰਵੋਤਮ ਅਭਿਆਸ](../../../Module04)
8. [ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ](../../../Module04)
9. [ਵਾਧੂ ਸਰੋਤ](../../../Module04)

## ਪ੍ਰਸਤਾਵਨਾ

ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਇੱਕ ਸ਼ਕਤੀਸ਼ਾਲੀ ਅਤੇ ਆਸਾਨ ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਮਾਡਲ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਟੂਲਕਿਟ ਹੈ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਪਲੇਟਫਾਰਮਾਂ 'ਤੇ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਅਪਟਿਮਾਈਜ਼ ਕਰਨ ਦੀ ਪ੍ਰਕਿਰਿਆ ਨੂੰ ਸਧਾਰਨ ਬਣਾਉਂਦਾ ਹੈ। ਚਾਹੇ ਤੁਸੀਂ CPUs, GPUs ਜਾਂ ਵਿਸ਼ੇਸ਼ AI ਐਕਸਲੇਰੇਟਰਾਂ ਨੂੰ ਟਾਰਗੇਟ ਕਰ ਰਹੇ ਹੋਵੋ, ਓਲਿਵ ਤੁਹਾਨੂੰ ਮਾਡਲ ਦੀ ਸ਼ੁੱਧਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ।

## ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਕੀ ਹੈ?

ਓਲਿਵ ਇੱਕ ਆਸਾਨ ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਮਾਡਲ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਟੂਲ ਹੈ ਜੋ ਮਾਡਲ ਕੰਪ੍ਰੈਸ਼ਨ, ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਅਤੇ ਕੰਪਾਇਲੇਸ਼ਨ ਵਿੱਚ ਉਦਯੋਗ-ਅਗੇਤਕ ਤਕਨੀਕਾਂ ਨੂੰ ਜੋੜਦਾ ਹੈ। ਇਹ ONNX Runtime ਨਾਲ ਇੱਕ E2E ਇੰਫਰੈਂਸ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਹੱਲ ਵਜੋਂ ਕੰਮ ਕਰਦਾ ਹੈ।

### ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

- **ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ**: ਤੁਹਾਡੇ ਟਾਰਗੇਟ ਹਾਰਡਵੇਅਰ ਲਈ ਸਵੈਚਾਲਿਤ ਤੌਰ 'ਤੇ ਵਧੀਆ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਤਕਨੀਕਾਂ ਦੀ ਚੋਣ ਕਰਦਾ ਹੈ
- **40+ ਬਿਲਟ-ਇਨ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਕੰਪੋਨੈਂਟਸ**: ਮਾਡਲ ਕੰਪ੍ਰੈਸ਼ਨ, ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ, ਗ੍ਰਾਫ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਅਤੇ ਹੋਰ ਕਵਰ ਕਰਦਾ ਹੈ
- **ਆਸਾਨ CLI ਇੰਟਰਫੇਸ**: ਆਮ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਕੰਮਾਂ ਲਈ ਸਧਾਰਨ ਕਮਾਂਡ
- **ਮਲਟੀ-ਫਰੇਮਵਰਕ ਸਪੋਰਟ**: PyTorch, Hugging Face ਮਾਡਲਾਂ ਅਤੇ ONNX ਨਾਲ ਕੰਮ ਕਰਦਾ ਹੈ
- **ਪ੍ਰਸਿੱਧ ਮਾਡਲ ਸਪੋਰਟ**: ਓਲਿਵ ਪ੍ਰਸਿੱਧ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਜਿਵੇਂ ਕਿ Llama, Phi, Qwen, Gemma ਆਦਿ ਨੂੰ ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਅਪਟਿਮਾਈਜ਼ ਕਰ ਸਕਦਾ ਹੈ

### ਫਾਇਦੇ

- **ਡਿਵੈਲਪਮੈਂਟ ਸਮਾਂ ਘਟਾਓ**: ਵੱਖ-ਵੱਖ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਤਕਨੀਕਾਂ ਨਾਲ ਹੱਥੋਂ-ਹੱਥ ਤਜਰਬਾ ਕਰਨ ਦੀ ਲੋੜ ਨਹੀਂ
- **ਪ੍ਰਦਰਸ਼ਨ ਵਿੱਚ ਸੁਧਾਰ**: ਮਹੱਤਵਪੂਰਨ ਗਤੀਵਿਧੀਆਂ (ਕਈ ਕੇਸਾਂ ਵਿੱਚ 6x ਤੱਕ)
- **ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਡਿਪਲੌਇਮੈਂਟ**: ਅਪਟਿਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਅਤੇ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮਾਂ 'ਤੇ ਕੰਮ ਕਰਦੇ ਹਨ
- **ਸ਼ੁੱਧਤਾ ਬਰਕਰਾਰ**: ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਪ੍ਰਦਰਸ਼ਨ ਵਿੱਚ ਸੁਧਾਰ ਕਰਦੇ ਹਨ

## ਇੰਸਟਾਲੇਸ਼ਨ

### ਪੂਰਵ ਸ਼ਰਤਾਂ

- Python 3.8 ਜਾਂ ਇਸ ਤੋਂ ਉੱਚਾ
- pip ਪੈਕੇਜ ਮੈਨੇਜਰ
- ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ (ਸਿਫਾਰਸ਼ੀ)

### ਬੁਨਿਆਦੀ ਇੰਸਟਾਲੇਸ਼ਨ

ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ ਬਣਾਓ ਅਤੇ ਐਕਟੀਵੇਟ ਕਰੋ:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ਓਲਿਵ ਨੂੰ ਆਟੋ-ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਨਾਲ ਇੰਸਟਾਲ ਕਰੋ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ਵਿਕਲਪਿਕ ਡਿਪੈਂਡੈਂਸੀਜ਼

ਓਲਿਵ ਵਾਧੂ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਲਈ ਵੱਖ-ਵੱਖ ਵਿਕਲਪਿਕ ਡਿਪੈਂਡੈਂਸੀਜ਼ ਪੇਸ਼ ਕਰਦਾ ਹੈ:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ਇੰਸਟਾਲੇਸ਼ਨ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ

```bash
olive --help
```

ਜੇ ਸਫਲ ਹੋਵੇ, ਤਾਂ ਤੁਹਾਨੂੰ ਓਲਿਵ CLI ਮਦਦ ਸੁਨੇਹਾ ਦੇਖਣਾ ਚਾਹੀਦਾ ਹੈ।

## ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਗਾਈਡ

### ਤੁਹਾਡਾ ਪਹਿਲਾ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ

ਓਲਿਵ ਦੇ ਆਟੋ-ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਵਿਸ਼ੇਸ਼ਤਾ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲ ਨੂੰ ਅਪਟਿਮਾਈਜ਼ ਕਰੀਏ:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਇਹ ਕਮਾਂਡ ਕੀ ਕਰਦੀ ਹੈ

ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ: ਮਾਡਲ ਨੂੰ ਸਥਾਨਕ ਕੈਸ਼ ਤੋਂ ਪ੍ਰਾਪਤ ਕਰਨਾ, ONNX ਗ੍ਰਾਫ ਨੂੰ ਕੈਪਚਰ ਕਰਨਾ ਅਤੇ ONNX ਡਾਟਾ ਫਾਈਲ ਵਿੱਚ ਵਜ਼ਨ ਸਟੋਰ ਕਰਨਾ, ONNX ਗ੍ਰਾਫ ਨੂੰ ਅਪਟਿਮਾਈਜ਼ ਕਰਨਾ, ਅਤੇ RTN ਵਿਧੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਨੂੰ int4 ਵਿੱਚ ਕੁਆੰਟਾਈਜ਼ ਕਰਨਾ।

### ਕਮਾਂਡ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਵਿਆਖਿਆ

- `--model_name_or_path`: Hugging Face ਮਾਡਲ ਪਛਾਣਕਰਤਾ ਜਾਂ ਸਥਾਨਕ ਪਾਥ
- `--output_path`: ਡਾਇਰੈਕਟਰੀ ਜਿੱਥੇ ਅਪਟਿਮਾਈਜ਼ ਕੀਤਾ ਮਾਡਲ ਸਟੋਰ ਕੀਤਾ ਜਾਵੇਗਾ
- `--device`: ਟਾਰਗੇਟ ਡਿਵਾਈਸ (cpu, gpu)
- `--provider`: ਐਗਜ਼ਿਕਿਊਸ਼ਨ ਪ੍ਰੋਵਾਈਡਰ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ਨੂੰ ਇੰਫਰੈਂਸ ਲਈ ਵਰਤੋ
- `--precision`: ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਸ਼ੁੱਧਤਾ (int4, int8, fp16)
- `--log_level`: ਲੌਗਿੰਗ ਵਰਬੋਸਿਟੀ (0=minimal, 1=verbose)

## ਉਦਾਹਰਨ: Qwen3 ਨੂੰ ONNX INT4 ਵਿੱਚ ਬਦਲਣਾ

Hugging Face ਦੇ ਦਿੱਤੇ ਉਦਾਹਰਨ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) ਦੇ ਅਧਾਰ 'ਤੇ, ਇੱਥੇ Qwen3 ਮਾਡਲ ਨੂੰ ਅਪਟਿਮਾਈਜ਼ ਕਰਨ ਦਾ ਤਰੀਕਾ ਹੈ:

### ਪਦਾਅਵਾਰ: ਮਾਡਲ ਡਾਊਨਲੋਡ ਕਰੋ (ਵਿਕਲਪਿਕ)

ਡਾਊਨਲੋਡ ਸਮਾਂ ਘਟਾਉਣ ਲਈ, ਸਿਰਫ਼ ਜ਼ਰੂਰੀ ਫਾਈਲਾਂ ਕੈਸ਼ ਕਰੋ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ਪਦਾਅਵਾਰ: Qwen3 ਮਾਡਲ ਨੂੰ ਅਪਟਿਮਾਈਜ਼ ਕਰੋ

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਪਦਾਅਵਾਰ: ਅਪਟਿਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਦੀ ਜਾਂਚ ਕਰੋ

ਅਪਟਿਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਦੀ ਜਾਂਚ ਕਰਨ ਲਈ ਇੱਕ ਸਧਾਰਨ Python ਸਕ੍ਰਿਪਟ ਬਣਾਓ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ਆਉਟਪੁੱਟ ਸਟ੍ਰਕਚਰ

ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਦੇ ਬਾਅਦ, ਤੁਹਾਡੀ ਆਉਟਪੁੱਟ ਡਾਇਰੈਕਟਰੀ ਵਿੱਚ ਇਹ ਸ਼ਾਮਲ ਹੋਵੇਗਾ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## ਤਕਨੀਕੀ ਵਰਤੋਂ

### ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ

ਵਧੇਰੇ ਜਟਿਲ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲੋਜ਼ ਲਈ, ਤੁਸੀਂ JSON ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹੋ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

ਕਨਫਿਗਰੇਸ਼ਨ ਨਾਲ ਚਲਾਓ:

```bash
olive run --config config.json
```

### GPU ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ

CUDA GPU ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਲਈ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) ਲਈ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਓਲਿਵ ਨਾਲ ਫਾਈਨ-ਟਿਊਨਿੰਗ

ਓਲਿਵ ਮਾਡਲਾਂ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## ਸਰਵੋਤਮ ਅਭਿਆਸ

### 1. ਮਾਡਲ ਚੋਣ
- ਟੈਸਟਿੰਗ ਲਈ ਛੋਟੇ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ (ਜਿਵੇਂ ਕਿ 0.5B-7B ਪੈਰਾਮੀਟਰ)
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਹਾਡਾ ਟਾਰਗੇਟ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰ ਓਲਿਵ ਦੁਆਰਾ ਸਪੋਰਟ ਕੀਤਾ ਗਿਆ ਹੈ

### 2. ਹਾਰਡਵੇਅਰ ਵਿਚਾਰ
- ਆਪਣੇ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਟਾਰਗੇਟ ਨੂੰ ਆਪਣੇ ਡਿਪਲੌਇਮੈਂਟ ਹਾਰਡਵੇਅਰ ਨਾਲ ਮੇਲ ਕਰੋ
- ਜੇ ਤੁਹਾਡੇ ਕੋਲ CUDA-ਅਨੁਕੂਲ ਹਾਰਡਵੇਅਰ ਹੈ ਤਾਂ GPU ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰੋ
- Windows ਮਸ਼ੀਨਾਂ ਲਈ DirectML ਨੂੰ ਵਿਚਾਰੋ ਜਿਨ੍ਹਾਂ ਵਿੱਚ ਇੰਟੀਗ੍ਰੇਟਡ ਗ੍ਰਾਫਿਕਸ ਹਨ

### 3. ਸ਼ੁੱਧਤਾ ਦੀ ਚੋਣ
- **INT4**: ਵੱਧ ਤੋਂ ਵੱਧ ਕੰਪ੍ਰੈਸ਼ਨ, ਥੋੜ੍ਹਾ ਸ਼ੁੱਧਤਾ ਘਟਾਓ
- **INT8**: ਆਕਾਰ ਅਤੇ ਸ਼ੁੱਧਤਾ ਦਾ ਵਧੀਆ ਸੰਤੁਲਨ
- **FP16**: ਘੱਟ ਤੋਂ ਘੱਟ ਸ਼ੁੱਧਤਾ ਘਟਾਓ, ਮੋਡਰੇਟ ਆਕਾਰ ਘਟਾਓ

### 4. ਟੈਸਟਿੰਗ ਅਤੇ ਵੈਲੀਡੇਸ਼ਨ
- ਹਮੇਸ਼ਾ ਆਪਣੇ ਵਿਸ਼ੇਸ਼ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਨਾਲ ਅਪਟਿਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲਾਂ ਦੀ ਜਾਂਚ ਕਰੋ
- ਪ੍ਰਦਰਸ਼ਨ ਮਾਪਦੰਡਾਂ ਦੀ ਤੁਲਨਾ ਕਰੋ (ਲੇਟੈਂਸੀ, ਥਰੂਪੁੱਟ, ਸ਼ੁੱਧਤਾ)
- ਮੁਲਾਂਕਣ ਲਈ ਪ੍ਰਤੀਨਿਧੀ ਇਨਪੁੱਟ ਡਾਟਾ ਦੀ ਵਰਤੋਂ ਕਰੋ

### 5. ਦੁਹਰਾਈ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ
- ਤੇਜ਼ ਨਤੀਜਿਆਂ ਲਈ ਆਟੋ-ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ
- ਸੁਖਮ-ਦਰਜਾ ਨਿਯੰਤਰਣ ਲਈ ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਵੱਖ-ਵੱਖ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਪਾਸਾਂ ਨਾਲ ਤਜਰਬਾ ਕਰੋ

## ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ

### ਆਮ ਸਮੱਸਿਆਵਾਂ

#### 1. ਇੰਸਟਾਲੇਸ਼ਨ ਸਮੱਸਿਆਵਾਂ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU ਸਮੱਸਿਆਵਾਂ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. ਮੈਮਰੀ ਸਮੱਸਿਆਵਾਂ
- ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਦੌਰਾਨ ਛੋਟੇ ਬੈਚ ਆਕਾਰ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਪਹਿਲਾਂ ਉੱਚ ਸ਼ੁੱਧਤਾ ਨਾਲ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ (int8 ਬਦਲੇ int4)
- ਮਾਡਲ ਕੈਸ਼ਿੰਗ ਲਈ ਯਥਾਰਥ ਡਿਸਕ ਸਪੇਸ ਯਕੀਨੀ ਬਣਾਓ

#### 4. ਮਾਡਲ ਲੋਡ ਕਰਨ ਦੀਆਂ ਗਲਤੀਆਂ
- ਮਾਡਲ ਪਾਥ ਅਤੇ ਪਹੁੰਚ ਅਧਿਕਾਰਾਂ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ
- ਜਾਂਚ ਕਰੋ ਕਿ ਮਾਡਲ ਨੂੰ `trust_remote_code=True` ਦੀ ਲੋੜ ਹੈ
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਸਾਰੇ ਜ਼ਰੂਰੀ ਮਾਡਲ ਫਾਈਲਾਂ ਡਾਊਨਲੋਡ ਕੀਤੀਆਂ ਗਈਆਂ ਹਨ

### ਮਦਦ ਪ੍ਰਾਪਤ ਕਰਨਾ

- **ਦਸਤਾਵੇਜ਼**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ਉਦਾਹਰਨਾਂ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## ਵਾਧੂ ਸਰੋਤ

### ਅਧਿਕਾਰਤ ਲਿੰਕ
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### ਕਮਿਊਨਿਟੀ ਉਦਾਹਰਨ
- **Jupyter Notebooks**: ਓਲਿਵ GitHub Repository ਵਿੱਚ ਉਪਲਬਧ
- **VS Code Extension**: AI Toolkit ਐਕਸਟੈਂਸ਼ਨ ਓਲਿਵ ਨੂੰ ਮਾਡਲ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਲਈ ਵਰਤਦਾ ਹੈ
- **Blog Posts**: Microsoft Open Source Blog ਵਿੱਚ ਓਲਿਵ ਟਿਊਟੋਰਿਅਲ ਦੀ ਵਿਸਤ੍ਰਿਤ ਜਾਣਕਾਰੀ ਹੈ

### ਸੰਬੰਧਿਤ ਟੂਲ
- **ONNX Runtime**: ਉੱਚ-ਪ੍ਰਦਰਸ਼ਨ ਇੰਫਰੈਂਸ ਇੰਜਨ
- **Hugging Face Transformers**: ਕਈ ਅਨੁਕੂਲ ਮਾਡਲਾਂ ਦਾ ਸਰੋਤ
- **Azure Machine Learning**: ਕਲਾਉਡ-ਅਧਾਰਿਤ ਅਪਟਿਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲੋਜ਼

## ➡️ ਅਗਲਾ ਕੀ ਹੈ

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**ਅਸਵੀਕਰਤੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਤਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।