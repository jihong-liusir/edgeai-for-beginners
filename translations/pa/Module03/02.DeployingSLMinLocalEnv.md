<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-17T23:59:17+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 2: ਸਥਾਨਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਡਿਪਲੌਇਮੈਂਟ - ਗੋਪਨੀਯਤਾ-ਪਹਿਲਾਂ ਹੱਲ

ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲ (SLMs) ਦੀ ਸਥਾਨਕ ਡਿਪਲੌਇਮੈਂਟ ਗੋਪਨੀਯਤਾ ਸੁਰੱਖਿਅਤ ਅਤੇ ਲਾਗਤ-ਪ੍ਰਭਾਵੀ AI ਹੱਲਾਂ ਵੱਲ ਇੱਕ ਨਵਾਂ ਰੁਝਾਨ ਹੈ। ਇਹ ਵਿਸਤ੍ਰਿਤ ਗਾਈਡ ਦੋ ਸ਼ਕਤੀਸ਼ਾਲੀ ਫਰੇਮਵਰਕ—Ollama ਅਤੇ Microsoft Foundry Local—ਦੀ ਜਾਂਚ ਕਰਦੀ ਹੈ, ਜੋ ਵਿਕਾਸਕਾਰਾਂ ਨੂੰ SLMs ਦੀ ਪੂਰੀ ਸਮਰਥਾ ਨੂੰ ਸਥਾਨਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਪੂਰੀ ਨਿਯੰਤਰਣ ਨਾਲ ਵਰਤਣ ਦੀ ਯੋਗਤਾ ਦਿੰਦੇ ਹਨ।

## ਪਰਿਚਯ

ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ ਸਥਾਨਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਲਈ ਉੱਚ-ਸਤਹ ਡਿਪਲੌਇਮੈਂਟ ਰਣਨੀਤੀਆਂ ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ। ਅਸੀਂ ਸਥਾਨਕ AI ਡਿਪਲੌਇਮੈਂਟ ਦੇ ਮੂਲ ਸੰਕਲਪਾਂ ਨੂੰ ਕਵਰ ਕਰਾਂਗੇ, ਦੋ ਪ੍ਰਮੁੱਖ ਪਲੇਟਫਾਰਮਾਂ (Ollama ਅਤੇ Microsoft Foundry Local) ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ, ਅਤੇ ਉਤਪਾਦਨ-ਤਿਆਰ ਹੱਲਾਂ ਲਈ ਵਿਹੰਗਮ ਕਾਰਜਨੁਮਾਂ ਦੀ ਸਲਾਹ ਦਿਆਂਗੇ।

## ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼

ਇਸ ਪਾਠ ਦੇ ਅੰਤ ਤੱਕ, ਤੁਸੀਂ ਸਮਰਥ ਹੋਵੋਗੇ:

- ਸਥਾਨਕ SLM ਡਿਪਲੌਇਮੈਂਟ ਫਰੇਮਵਰਕਾਂ ਦੀ ਆਰਕੀਟੈਕਚਰ ਅਤੇ ਫਾਇਦਿਆਂ ਨੂੰ ਸਮਝਣਾ।
- Ollama ਅਤੇ Microsoft Foundry Local ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਉਤਪਾਦਨ-ਤਿਆਰ ਡਿਪਲੌਇਮੈਂਟ ਨੂੰ ਲਾਗੂ ਕਰਨਾ।
- ਵਿਸ਼ੇਸ਼ ਜ਼ਰੂਰਤਾਂ ਅਤੇ ਪਾਬੰਦੀਆਂ ਦੇ ਆਧਾਰ 'ਤੇ ਸਹੀ ਪਲੇਟਫਾਰਮ ਦੀ ਚੋਣ ਅਤੇ ਤੁਲਨਾ ਕਰਨਾ।
- ਪ੍ਰਦਰਸ਼ਨ, ਸੁਰੱਖਿਆ, ਅਤੇ ਸਕੇਲਬਿਲਿਟੀ ਲਈ ਸਥਾਨਕ ਡਿਪਲੌਇਮੈਂਟ ਨੂੰ ਅਨੁਕੂਲ ਬਣਾਉਣਾ।

## ਸਥਾਨਕ SLM ਡਿਪਲੌਇਮੈਂਟ ਆਰਕੀਟੈਕਚਰ ਨੂੰ ਸਮਝਣਾ

ਸਥਾਨਕ SLM ਡਿਪਲੌਇਮੈਂਟ ਕਲਾਉਡ-ਨਿਰਭਰਤ AI ਸੇਵਾਵਾਂ ਤੋਂ ਸਥਾਨਕ, ਗੋਪਨੀਯਤਾ-ਸੁਰੱਖਿਅਤ ਹੱਲਾਂ ਵੱਲ ਇੱਕ ਮੂਲ ਬਦਲਾਅ ਦਾ ਪ੍ਰਤੀਕ ਹੈ। ਇਹ ਪਹੁੰਚ ਸੰਗਠਨਾਂ ਨੂੰ ਆਪਣੇ AI ਢਾਂਚੇ 'ਤੇ ਪੂਰੀ ਨਿਯੰਤਰਣ ਬਣਾਈ ਰੱਖਣ ਦੀ ਯੋਗਤਾ ਦਿੰਦੀ ਹੈ, ਜਦੋਂ ਕਿ ਡਾਟਾ ਸਾਰਵਭੌਮਤਾ ਅਤੇ ਸੰਚਾਲਨਕ ਸਵਤੰਤਰਤਾ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ।

### ਡਿਪਲੌਇਮੈਂਟ ਫਰੇਮਵਰਕ ਵਰਗੀਕਰਨ

ਵੱਖ-ਵੱਖ ਡਿਪਲੌਇਮੈਂਟ ਪਹੁੰਚਾਂ ਨੂੰ ਸਮਝਣਾ ਵਿਸ਼ੇਸ਼ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਲਈ ਸਹੀ ਰਣਨੀਤੀ ਦੀ ਚੋਣ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ:

- **ਵਿਕਾਸ-ਕੇਂਦਰਿਤ**: ਪ੍ਰਯੋਗ ਅਤੇ ਪ੍ਰੋਟੋਟਾਈਪਿੰਗ ਲਈ ਸਧਾਰਨ ਸੈਟਅੱਪ
- **ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗਰੇਡ**: ਇੰਟਰਪ੍ਰਾਈਜ਼ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਯੋਗਤਾਵਾਂ ਨਾਲ ਉਤਪਾਦਨ-ਤਿਆਰ ਹੱਲ  
- **ਕਰਾਸ-ਪਲੇਟਫਾਰਮ**: ਵੱਖ-ਵੱਖ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮ ਅਤੇ ਹਾਰਡਵੇਅਰ 'ਤੇ ਵਿਸ਼ਵ-ਵਿਆਪੀ ਅਨੁਕੂਲਤਾ

### ਸਥਾਨਕ SLM ਡਿਪਲੌਇਮੈਂਟ ਦੇ ਮੁੱਖ ਫਾਇਦੇ

ਸਥਾਨਕ SLM ਡਿਪਲੌਇਮੈਂਟ ਕਈ ਮੂਲ ਫਾਇਦੇ ਪੇਸ਼ ਕਰਦਾ ਹੈ ਜੋ ਇਸਨੂੰ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਅਤੇ ਗੋਪਨੀਯਤਾ-ਸੰਵੇਦਨਸ਼ੀਲ ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ ਆਦਰਸ਼ ਬਣਾਉਂਦੇ ਹਨ:

**ਗੋਪਨੀਯਤਾ ਅਤੇ ਸੁਰੱਖਿਆ**: ਸਥਾਨਕ ਪ੍ਰੋਸੈਸਿੰਗ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ ਕਿ ਸੰਵੇਦਨਸ਼ੀਲ ਡਾਟਾ ਕਦੇ ਵੀ ਸੰਗਠਨ ਦੇ ਢਾਂਚੇ ਤੋਂ ਬਾਹਰ ਨਹੀਂ ਜਾਂਦਾ, GDPR, HIPAA, ਅਤੇ ਹੋਰ ਨਿਯਮਕ ਪਾਬੰਦੀਆਂ ਨਾਲ ਅਨੁਕੂਲਤਾ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਕਲਾਸੀਫਾਈਡ ਵਾਤਾਵਰਣਾਂ ਲਈ ਏਅਰ-ਗੈਪਡ ਡਿਪਲੌਇਮੈਂਟ ਸੰਭਵ ਹਨ, ਜਦੋਂ ਕਿ ਪੂਰੇ ਆਡਿਟ ਟ੍ਰੇਲ ਸੁਰੱਖਿਆ ਦੀ ਨਿਗਰਾਨੀ ਬਣਾਈ ਰੱਖਦੇ ਹਨ।

**ਲਾਗਤ ਦੀ ਪ੍ਰਭਾਵਸ਼ੀਲਤਾ**: ਪ੍ਰਤੀ-ਟੋਕਨ ਕੀਮਤ ਮਾਡਲਾਂ ਦੇ ਖਤਮ ਹੋਣ ਨਾਲ ਸੰਚਾਲਨ ਲਾਗਤ ਵਿੱਚ ਕਾਫੀ ਕਮੀ ਆਉਂਦੀ ਹੈ। ਘੱਟ ਬੈਂਡਵਿਡਥ ਦੀ ਲੋੜ ਅਤੇ ਕਲਾਉਡ ਨਿਰਭਰਤਾ ਵਿੱਚ ਕਮੀ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਬਜਟਿੰਗ ਲਈ ਪੂਰੀ ਤਰ੍ਹਾਂ ਪੇਸ਼ਗੀ ਲਾਗਤ ਢਾਂਚੇ ਪ੍ਰਦਾਨ ਕਰਦੀ ਹੈ।

**ਪ੍ਰਦਰਸ਼ਨ ਅਤੇ ਭਰੋਸੇਯੋਗਤਾ**: ਨੈੱਟਵਰਕ ਲੈਟੈਂਸੀ ਤੋਂ ਬਿਨਾਂ ਤੇਜ਼ ਇੰਫਰੈਂਸ ਸਮਾਂ ਰੀਅਲ-ਟਾਈਮ ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਆਫਲਾਈਨ ਕਾਰਜਕੁਸ਼ਲਤਾ ਇੰਟਰਨੈਟ ਕਨੈਕਟਿਵਿਟੀ ਤੋਂ ਬਿਨਾਂ ਲਗਾਤਾਰ ਸੰਚਾਲਨ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ, ਜਦੋਂ ਕਿ ਸਥਾਨਕ ਸਰੋਤ ਅਨੁਕੂਲਤਾ ਸਥਿਰ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦੀ ਹੈ।

## Ollama: ਯੂਨੀਵਰਸਲ ਸਥਾਨਕ ਡਿਪਲੌਇਮੈਂਟ ਪਲੇਟਫਾਰਮ

### ਮੁੱਖ ਆਰਕੀਟੈਕਚਰ ਅਤੇ ਫਲਸਫਾ

Ollama ਨੂੰ ਇੱਕ ਯੂਨੀਵਰਸਲ, ਵਿਕਾਸਕਾਰ-ਅਨੁਕੂਲ ਪਲੇਟਫਾਰਮ ਵਜੋਂ ਇੰਜੀਨੀਅਰ ਕੀਤਾ ਗਿਆ ਹੈ ਜੋ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ ਅਤੇ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮਾਂ ਵਿੱਚ ਸਥਾਨਕ LLM ਡਿਪਲੌਇਮੈਂਟ ਨੂੰ ਲੋਕਤੰਤਰਿਕ ਬਣਾਉਂਦਾ ਹੈ।

**ਤਕਨੀਕੀ ਅਧਾਰ**: ਮਜ਼ਬੂਤ llama.cpp ਫਰੇਮਵਰਕ 'ਤੇ ਬਣਿਆ, Ollama ਕੁਸ਼ਲ GGUF ਮਾਡਲ ਫਾਰਮੈਟ ਦੀ ਵਰਤੋਂ ਕਰਦਾ ਹੈ ਜੋ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਅਨੁਕੂਲਤਾ Windows, macOS, ਅਤੇ Linux ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਸਥਿਰ ਵਿਹਾਰ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ, ਜਦੋਂ ਕਿ ਬੁੱਧੀਮਾਨ ਸਰੋਤ ਪ੍ਰਬੰਧਨ CPU, GPU, ਅਤੇ ਮੈਮਰੀ ਦੀ ਵਰਤੋਂ ਨੂੰ ਅਨੁਕੂਲ ਬਣਾਉਂਦਾ ਹੈ।

**ਡਿਜ਼ਾਈਨ ਫਲਸਫਾ**: Ollama ਸਧਾਰਨਤਾ ਨੂੰ ਤਰਜੀਹ ਦਿੰਦਾ ਹੈ ਬਿਨਾਂ ਕਾਰਜਕੁਸ਼ਲਤਾ ਦੀ ਕੁਰਬਾਨੀ ਦਿੱਤੇ, ਤੁਰੰਤ ਉਤਪਾਦਕਤਾ ਲਈ ਜ਼ੀਰੋ-ਕੰਫਿਗਰੇਸ਼ਨ ਡਿਪਲੌਇਮੈਂਟ ਪੇਸ਼ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ ਵਿਆਪਕ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਬਣਾਈ ਰੱਖਦਾ ਹੈ ਜਦੋਂ ਕਿ ਵੱਖ-ਵੱਖ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਵਿੱਚ ਸਥਿਰ APIs ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

### ਉੱਚ-ਸਤਹ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਅਤੇ ਯੋਗਤਾਵਾਂ

**ਮਾਡਲ ਪ੍ਰਬੰਧਨ ਵਿੱਚ ਸ਼੍ਰੇਸ਼ਠਤਾ**: Ollama ਆਟੋਮੈਟਿਕ ਪੁਲਿੰਗ, ਕੈਸ਼ਿੰਗ, ਅਤੇ ਵਰਜਨਿੰਗ ਨਾਲ ਵਿਸਤ੍ਰਿਤ ਮਾਡਲ ਲਾਈਫਸਾਈਕਲ ਪ੍ਰਬੰਧਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral, ਅਤੇ ਵਿਸ਼ੇਸ਼ ਐਮਬੈਡਿੰਗ ਮਾਡਲਾਂ ਸਮੇਤ ਵਿਆਪਕ ਮਾਡਲ ਪੇਸ਼ਕਸ਼ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ।

**Modelfiles ਰਾਹੀਂ ਕਸਟਮਾਈਜ਼ੇਸ਼ਨ**: ਉੱਚ-ਸਤਹ ਦੇ ਉਪਭੋਗਤਾ ਵਿਸ਼ੇਸ਼ ਪੈਰਾਮੀਟਰਾਂ, ਸਿਸਟਮ ਪ੍ਰੋੰਪਟਾਂ, ਅਤੇ ਵਿਹਾਰ ਸੰਸ਼ੋਧਨਾਂ ਨਾਲ ਕਸਟਮ ਮਾਡਲ ਸੰਰਚਨਾਵਾਂ ਬਣਾਉਣ ਦੇ ਯੋਗ ਹਨ। ਇਹ ਖੇਤਰ-ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ ਅਤੇ ਵਿਸ਼ੇਸ਼ ਐਪਲੀਕੇਸ਼ਨ ਜ਼ਰੂਰਤਾਂ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

**ਪ੍ਰਦਰਸ਼ਨ ਅਨੁਕੂਲਤਾ**: Ollama ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਉਪਲਬਧ ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਦੀ ਪਛਾਣ ਅਤੇ ਵਰਤੋਂ ਕਰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ NVIDIA CUDA, Apple Metal, ਅਤੇ OpenCL ਸ਼ਾਮਲ ਹਨ। ਬੁੱਧੀਮਾਨ ਮੈਮਰੀ ਪ੍ਰਬੰਧਨ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ ਵਿੱਚ ਵਧੀਆ ਸਰੋਤ ਵਰਤੋਂ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

### ਉਤਪਾਦਨ ਕਾਰਜਨੁਮਾਂ ਰਣਨੀਤੀਆਂ

**ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਸੈਟਅੱਪ**: Ollama ਵੱਖ-ਵੱਖ ਪਲੇਟਫਾਰਮਾਂ ਵਿੱਚ ਸਥਾਨਕ ਇੰਸਟਾਲਰਾਂ, ਪੈਕੇਜ ਮੈਨੇਜਰਾਂ (WinGet, Homebrew, APT), ਅਤੇ Docker ਕੰਟੇਨਰਾਂ ਰਾਹੀਂ ਸਧਾਰਨ ਇੰਸਟਾਲੇਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**ਮੁੱਢਲੇ ਕਮਾਂਡ ਅਤੇ ਕਾਰਵਾਈਆਂ**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**ਉੱਚ-ਸਤਹ ਸੰਰਚਨਾ**: Modelfiles ਇੰਟਰਪ੍ਰਾਈਜ਼ ਜ਼ਰੂਰਤਾਂ ਲਈ ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ ਯੋਗ ਬਣਾਉਂਦੇ ਹਨ:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### ਵਿਕਾਸਕਾਰ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਉਦਾਹਰਨ

**Python API ਇੰਟੀਗ੍ਰੇਸ਼ਨ**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript ਇੰਟੀਗ੍ਰੇਸ਼ਨ (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API ਦੀ ਵਰਤੋਂ cURL ਨਾਲ**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### ਪ੍ਰਦਰਸ਼ਨ ਟਿਊਨਿੰਗ ਅਤੇ ਅਨੁਕੂਲਤਾ

**ਮੈਮਰੀ ਅਤੇ ਥ੍ਰੈਡ ਸੰਰਚਨਾ**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਲਈ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਚੋਣ**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: ਇੰਟਰਪ੍ਰਾਈਜ਼ ਐਜ AI ਪਲੇਟਫਾਰਮ

### ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗਰੇਡ ਆਰਕੀਟੈਕਚਰ

Microsoft Foundry Local ਇੱਕ ਵਿਸਤ੍ਰਿਤ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਹੱਲ ਹੈ ਜੋ Microsoft ਈਕੋਸਿਸਟਮ ਵਿੱਚ ਡੂੰਘੀ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਨਾਲ ਉਤਪਾਦਨ ਐਜ AI ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ।

**ONNX-ਅਧਾਰਿਤ ਅਧਾਰ**: ਉਦਯੋਗ-ਮਿਆਰੀ ONNX Runtime 'ਤੇ ਬਣਿਆ, Foundry Local ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਆਰਕੀਟੈਕਚਰਾਂ ਵਿੱਚ ਅਨੁਕੂਲ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ Windows ML ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਦਾ ਲਾਭ ਲੈਂਦਾ ਹੈ ਜੋ Windows ਵਿੱਚ ਸਥਾਨਕ ਅਨੁਕੂਲਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਜਦੋਂ ਕਿ ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਅਨੁਕੂਲਤਾ ਬਣਾਈ ਰੱਖਦਾ ਹੈ।

**ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਵਿੱਚ ਸ਼੍ਰੇਸ਼ਠਤਾ**: Foundry Local CPUs, GPUs, ਅਤੇ NPUs ਵਿੱਚ ਬੁੱਧੀਮਾਨ ਹਾਰਡਵੇਅਰ ਪਛਾਣ ਅਤੇ ਅਨੁਕੂਲਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਹਾਰਡਵੇਅਰ ਵਿਕਰੇਤਾ (AMD, Intel, NVIDIA, Qualcomm) ਨਾਲ ਡੂੰਘੀ ਸਹਿਯੋਗ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ 'ਤੇ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

### ਉੱਚ-ਸਤਹ ਵਿਕਾਸਕਾਰ ਅਨੁਭਵ

**ਮਲਟੀ-ਇੰਟਰਫੇਸ ਪਹੁੰਚ**: Foundry Local ਵਿਸਤ੍ਰਿਤ ਵਿਕਾਸ ਇੰਟਰਫੇਸ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਮਾਡਲ ਪ੍ਰਬੰਧਨ ਅਤੇ ਡਿਪਲੌਇਮੈਂਟ ਲਈ ਇੱਕ ਸ਼ਕਤੀਸ਼ਾਲੀ CLI, ਮਲਟੀ-ਭਾਸ਼ਾ SDKs (Python, NodeJS) ਸਥਾਨਕ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਲਈ, ਅਤੇ RESTful APIs OpenAI ਅਨੁਕੂਲਤਾ ਨਾਲ ਸਹੀ ਮਾਈਗ੍ਰੇਸ਼ਨ ਲਈ ਸ਼ਾਮਲ ਹਨ।

**Visual Studio ਇੰਟੀਗ੍ਰੇਸ਼ਨ**: ਪਲੇਟਫਾਰਮ VS Code ਲਈ AI Toolkit ਨਾਲ ਬੇਹਤਰੀਨ ਤਰੀਕੇ ਨਾਲ ਇੰਟੀਗ੍ਰੇਟ ਹੁੰਦਾ ਹੈ, ਵਿਕਾਸ ਵਾਤਾਵਰਣ ਵਿੱਚ ਮਾਡਲ ਕਨਵਰਜ਼ਨ, ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ, ਅਤੇ ਅਨੁਕੂਲਤਾ ਟੂਲ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਇਹ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਵਿਕਾਸ ਕਾਰਜਨੁਮਾਂ ਨੂੰ ਤੇਜ਼ ਕਰਦਾ ਹੈ ਅਤੇ ਡਿਪਲੌਇਮੈਂਟ ਦੀ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾਉਂਦਾ ਹੈ।

**ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਪਾਈਪਲਾਈਨ**: Microsoft Olive ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਗ੍ਰਾਫ ਅਨੁਕੂਲਤਾ, ਡਾਇਨਾਮਿਕ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ, ਅਤੇ ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਟਿਊਨਿੰਗ ਸਮੇਤ ਵਿਸ਼ੇਸ਼ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਕਾਰਜਨੁਮਾਂ ਯੋਗ ਬਣਾਉਂਦਾ ਹੈ। Azure ML ਰਾਹੀਂ ਕਲਾਉਡ-ਅਧਾਰਿਤ ਕਨਵਰਜ਼ਨ ਯੋਗਤਾਵਾਂ ਵੱਡੇ ਮਾਡਲਾਂ ਲਈ ਸਕੇਲਬਲ ਅਨੁਕੂਲਤਾ ਪ੍ਰਦਾਨ ਕਰਦੀਆਂ ਹਨ।

### ਉਤਪਾਦਨ ਕਾਰਜਨੁਮਾਂ ਰਣਨੀਤੀਆਂ

**ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਸੰਰਚਨਾ**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**ਮਾਡਲ ਪ੍ਰਬੰਧਨ ਕਾਰਵਾਈਆਂ**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**ਉੱਚ-ਸਤਹ ਡਿਪਲੌਇਮੈਂਟ ਸੰਰਚਨਾ**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### ਇੰਟਰਪ੍ਰਾਈਜ਼ ਈਕੋਸਿਸਟਮ ਇੰਟੀਗ੍ਰੇਸ਼ਨ

**ਸੁਰੱਖਿਆ ਅਤੇ ਅਨੁਕੂਲਤਾ**: Foundry Local ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗਰੇਡ ਸੁਰੱਖਿਆ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਰੋਲ-ਅਧਾਰਿਤ ਪਹੁੰਚ ਨਿਯੰਤਰਣ, ਆਡਿਟ ਲੌਗਿੰਗ, ਅਨੁਕੂਲਤਾ ਰਿਪੋਰਟਿੰਗ, ਅਤੇ ਇੰਕ੍ਰਿਪਟ ਕੀਤੀ ਮਾਡਲ ਸਟੋਰੇਜ ਸ਼ਾਮਲ ਹੈ। Microsoft ਸੁਰੱਖਿਆ ਢਾਂਚੇ ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਸੁਰੱਖਿਆ ਨੀਤੀਆਂ ਦੀ ਪਾਲਣਾ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

**ਬਿਲਟ-ਇਨ AI ਸੇਵਾਵਾਂ**: ਪਲੇਟਫਾਰਮ ਸਥਾਨਕ ਭਾਸ਼ਾ ਪ੍ਰੋਸੈਸਿੰਗ ਲਈ Phi Silica, ਚਿੱਤਰ ਵਧਾਈ ਅਤੇ ਵਿਸ਼ਲੇਸ਼ਣ ਲਈ AI Imaging, ਅਤੇ ਆਮ ਇੰਟਰਪ੍ਰਾਈਜ਼ AI ਕਾਰਜਾਂ ਲਈ ਵਿਸ਼ੇਸ਼ APIs ਸਮੇਤ ਤਿਆਰ-ਵਰਤੋਂ ਲਈ AI ਯੋਗਤਾਵਾਂ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

## Ollama ਅਤੇ Foundry Local ਦੀ ਤੁਲਨਾਤਮਕ ਵਿਸ਼ਲੇਸ਼ਣ

### ਤਕਨੀਕੀ ਆਰਕੀਟੈਕਚਰ ਦੀ ਤੁਲਨਾ

| **ਪਹਲੂ** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **ਮਾਡਲ ਫਾਰਮੈਟ** | GGUF (via llama.cpp) | ONNX (via ONNX Runtime) |
| **ਪਲੇਟਫਾਰਮ ਫੋਕਸ** | ਯੂਨੀਵਰਸਲ ਕਰਾਸ-ਪਲੇਟਫਾਰਮ | Windows/ਇੰਟਰਪ੍ਰਾਈਜ਼ ਅਨੁਕੂਲਤਾ |
| **ਹਾਰਡਵੇਅਰ ਇੰਟੀਗ੍ਰੇਸ਼ਨ** | ਜਨਰਲ GPU/CPU ਸਪੋਰਟ | ਡੂੰਘੀ Windows ML, NPU ਸਪੋਰਟ |


---

**ਅਸਵੀਕਤੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਨਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।