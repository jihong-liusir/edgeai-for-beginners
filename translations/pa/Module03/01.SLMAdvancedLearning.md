<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-18T00:02:29+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "pa"
}
-->
# ри╕рйИриХри╕ри╝рии 1: SLM риЙрй▒риЪ-ри╕ридри╣ ри╕ри┐рй▒риЦри┐риЖ - римрйБриири┐риЖрижри╛риВ риЕридрйЗ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии

риЫрйЛриЯрйЗ ринри╛ри╕ри╝ри╛ риори╛рибри▓ (SLMs) EdgeAI ри╡ри┐рй▒риЪ риЗрй▒риХ риори╣рй▒ридри╡рикрйВри░рии ридри░рй▒риХрйА рижри╛ рикрйНри░ридрйАриХ ри╣рии, риЬрйЛ ри╕рй░ри╕ри╛ризриири╛риВ рижрйА риШри╛риЯ ри╡ри╛ри▓рйЗ рибри┐ри╡ри╛риИри╕ри╛риВ 'ридрйЗ ри╕рйБризри╛ри░рид риХрйБрижри░ридрйА ринри╛ри╕ри╝ри╛ рикрйНри░рйЛри╕рйИри╕ри┐рй░риЧ ри╕риори░рй▒риери╛ри╡ри╛риВ риирйВрй░ рипрйЛриЧ римригри╛риЙриВрижрйЗ ри╣рииред SLMs риирйВрй░ рикрйНри░ринри╛ри╡ри╕ри╝ри╛ри▓рйА риврй░риЧ риири╛ри▓ ридрйИриири╛рид, риЕрикриЯри┐риори╛риИриЬри╝ риЕридрйЗ ри╡ри░ридриг рижрйА ри╕риориЭ Edge-риЕризри╛ри░ри┐рид AI ри╣рй▒ри▓ри╛риВ римригри╛риЙриг ри▓риИ риЬри╝ри░рйВри░рйА ри╣рйИред

## рикри░ри┐риЪрип

риЗри╕ рикри╛риа ри╡ри┐рй▒риЪ, риЕри╕рйАриВ риЫрйЛриЯрйЗ ринри╛ри╕ри╝ри╛ риори╛рибри▓ (SLMs) риЕридрйЗ риЙриирйНри╣ри╛риВ рижрйЗ риЙрй▒риЪ-ри╕ридри╣ риЕриори▓ риХри░рии рижрйАриЖриВ ри░ригриирйАридрйАриЖриВ рижрйА рикрйЬридри╛ри▓ риХри░ри╛риВриЧрйЗред риЕри╕рйАриВ SLMs рижрйЗ риорйВри▓ ризри╛ри░риири╛ри╡ри╛риВ, риЙриирйНри╣ри╛риВ рижрйЗ рикрйИри░ри╛риорйАриЯри░ ри╕рйАриори╛ри╡ри╛риВ риЕридрйЗ ри╡ри░риЧрйАриХри░рии, риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ридриХриирйАриХри╛риВ, риЕридрйЗ Edge Computing ри╡ри╛ридри╛ри╡ри░ригри╛риВ ри▓риИ рикрйНри░рипрйЛриЧриХри░ ридрйИриири╛ридрйА ри░ригриирйАридрйАриЖриВ риирйВрй░ риХри╡ри░ риХри░ри╛риВриЧрйЗред

## ри╕ри┐рй▒риЦриг рижрйЗ риЙрижрйЗри╕ри╝

риЗри╕ рикри╛риа рижрйЗ риЕрй░рид ридрй▒риХ, ридрйБри╕рйАриВ ри╕риори░рй▒рие ри╣рйЛри╡рйЛриЧрйЗ:

- ЁЯФв риЫрйЛриЯрйЗ ринри╛ри╕ри╝ри╛ риори╛рибри▓ри╛риВ рижрйЗ рикрйИри░ри╛риорйАриЯри░ ри╕рйАриори╛ри╡ри╛риВ риЕридрйЗ ри╡ри░риЧрйАриХри░рии риирйВрй░ ри╕риориЭрйЛред
- ЁЯЫая╕П Edge рибри┐ри╡ри╛риИри╕ри╛риВ 'ридрйЗ SLM ридрйИриири╛ридрйА ри▓риИ риорйБрй▒риЦ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ридриХриирйАриХри╛риВ рижрйА рикриЫри╛риг риХри░рйЛред
- ЁЯЪА SLMs ри▓риИ риЙрй▒риЪ-ри╕ридри╣ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии риЕридрйЗ риХрй░рикрйНри░рйИри╕ри╝рии ри░ригриирйАридрйАриЖриВ риирйВрй░ ри╕ри┐рй▒риЦрйЛред

## SLM рикрйИри░ри╛риорйАриЯри░ ри╕рйАриори╛ри╡ри╛риВ риЕридрйЗ ри╡ри░риЧрйАриХри░рии риирйВрй░ ри╕риориЭригри╛

риЫрйЛриЯрйЗ ринри╛ри╕ри╝ри╛ риори╛рибри▓ (SLMs) AI риори╛рибри▓ ри╣рии риЬрйЛ риХрйБрижри░ридрйА ринри╛ри╕ри╝ри╛ ри╕риорй▒риЧри░рйА риирйВрй░ рикрйНри░рйЛри╕рйИри╕, ри╕риориЭриг риЕридрйЗ риЬриири░рйЗриЯ риХри░рии ри▓риИ рибри┐риЬри╝ри╛риИрии риХрйАридрйЗ риЧриП ри╣рии, риЬри┐риирйНри╣ри╛риВ ри╡ри┐рй▒риЪ риЙриирйНри╣ри╛риВ рижрйЗ ри╡рй▒рибрйЗ ри╕риори░рйВрикри╛риВ рижрйЗ риорйБриХри╛римри▓рйЗ риХри╛рилри╝рйА риШрй▒риЯ рикрйИри░ри╛риорйАриЯри░ ри╣рйБрй░рижрйЗ ри╣рииред риЬрижрйЛриВ риХри┐ ри╡рй▒рибрйЗ ринри╛ри╕ри╝ри╛ риори╛рибри▓ (LLMs) ри╕рйИриВриХрйЬрйЗ римри┐ри▓рйАриЕрии ридрйЛриВ ри▓рйИ риХрйЗ риЯрйНри░ри┐ри▓рйАриЕрии рикрйИри░ри╛риорйАриЯри░ри╛риВ ридрй▒риХ ри╣рйБрй░рижрйЗ ри╣рии, SLMs риЦри╛ри╕ ридрйМри░ 'ридрйЗ риХрйБри╕ри╝ри▓ридри╛ риЕридрйЗ Edge ридрйИриири╛ридрйА ри▓риИ рибри┐риЬри╝ри╛риИрии риХрйАридрйЗ риЧриП ри╣рииред

рикрйИри░ри╛риорйАриЯри░ ри╡ри░риЧрйАриХри░рии рилри░рйЗриори╡ри░риХ ри╕ри╛риирйВрй░ SLMs рижрйЗ ри╡рй▒риЦ-ри╡рй▒риЦ ри╕ри╝рйНри░рйЗригрйАриЖриВ риЕридрйЗ риЙриирйНри╣ри╛риВ рижрйЗ риЙриЪри┐рид ри╡ри░ридрйЛриВ рижрйЗ риХрйЗри╕ри╛риВ риирйВрй░ ри╕риориЭриг ри╡ри┐рй▒риЪ риорижриж риХри░рижри╛ ри╣рйИред риЗри╣ ри╡ри░риЧрйАриХри░рии риЦри╛ри╕ Edge Computing рижрйНри░ри┐ри╕ри╝ри╛риВ ри▓риИ ри╕ри╣рйА риори╛рибри▓ рижрйА риЪрйЛриг риХри░рии ри▓риИ риори╣рй▒ридри╡рикрйВри░рии ри╣рйИред

### рикрйИри░ри╛риорйАриЯри░ ри╡ри░риЧрйАриХри░рии рилри░рйЗриори╡ри░риХ

рикрйИри░ри╛риорйАриЯри░ ри╕рйАриори╛ри╡ри╛риВ риирйВрй░ ри╕риориЭригри╛ ри╡рй▒риЦ-ри╡рй▒риЦ Edge Computing рижрйНри░ри┐ри╕ри╝ри╛риВ ри▓риИ риЙриЪри┐рид риори╛рибри▓ риЪрйБригрии ри╡ри┐рй▒риЪ риорижриж риХри░рижри╛ ри╣рйИ:

- **ЁЯФм риори╛риИриХрйНри░рйЛ SLMs**: 100M - 1.4B рикрйИри░ри╛риорйАриЯри░ (риорйЛримри╛риИри▓ рибри┐ри╡ри╛риИри╕ри╛риВ ри▓риИ риЕридри┐-ри╣ри▓риХри╛)
- **ЁЯУ▒ риЫрйЛриЯрйЗ SLMs**: 1.5B - 13.9B рикрйИри░ри╛риорйАриЯри░ (ри╕рй░ридрйБри▓ри┐рид рикрйНри░рижри░ри╕ри╝рии риЕридрйЗ риХрйБри╕ри╝ри▓ридри╛)
- **тЪЦя╕П риорй▒ризрио SLMs**: 14B - 30B рикрйИри░ри╛риорйАриЯри░ (LLM ри╕риори░рй▒риери╛ри╡ри╛риВ рижрйЗ риирйЗрйЬрйЗ рикри╣рйБрй░риЪрижрйЗ ри╣рйЛриП риХрйБри╕ри╝ри▓ридри╛ риирйВрй░ римри░риХри░ри╛ри░ ри░рй▒риЦрижрйЗ ри╣рйЛриП)

риЗри╣ ри╕рйАриори╛ риЦрйЛриЬ ринри╛риИриЪри╛ри░рйЗ ри╡ри┐рй▒риЪ ридри░ри▓ ри░ри╣ри┐рй░рижрйА ри╣рйИ, рикри░ риЬри╝ри┐риЖрижри╛ридри░ риЕринри┐риЖри╕риХ 30 римри┐ри▓рйАриЕрии рикрйИри░ри╛риорйАриЯри░ри╛риВ ридрйЛриВ риШрй▒риЯ риори╛рибри▓ри╛риВ риирйВрй░ "риЫрйЛриЯри╛" риорй░риирижрйЗ ри╣рии, риЬрижрйЛриВ риХри┐ риХрйБриЭ ри╕ри░рйЛрид риЗри╕ ри╕рйАриори╛ риирйВрй░ 10 римри┐ри▓рйАриЕрии рикрйИри░ри╛риорйАриЯри░ри╛риВ 'ридрйЗ ри╡рйА риШриЯри╛ рижри┐рй░рижрйЗ ри╣рииред

### SLMs рижрйЗ риорйБрй▒риЦ рилри╛риЗрижрйЗ

SLMs риХрйБриЭ риорйВри▓ рилри╛риЗрижрйЗ рикрйНри░рижри╛рии риХри░рижрйЗ ри╣рии риЬрйЛ риЙриирйНри╣ри╛риВ риирйВрй░ Edge Computing риРрикри▓рйАриХрйЗри╕ри╝риири╛риВ ри▓риИ риЖрижри░ри╕ри╝ римригри╛риЙриВрижрйЗ ри╣рии:

**риЪри╛ри▓рйВ риХрйБри╕ри╝ри▓ридри╛**: SLMs риШрй▒риЯ рикрйИри░ри╛риорйАриЯри░ри╛риВ риирйВрй░ рикрйНри░рйЛри╕рйИри╕ риХри░рии рижрйЗ риХри╛ри░рии ридрйЗриЬри╝ риЗрй░рилри░рйИриВри╕ ри╕риори╛риВ рикрйНри░рижри╛рии риХри░рижрйЗ ри╣рии, риЬрйЛ риЙриирйНри╣ри╛риВ риирйВрй░ ри░рйАриЕри▓-риЯри╛риИрио риРрикри▓рйАриХрйЗри╕ри╝риири╛риВ ри▓риИ риЖрижри░ри╕ри╝ римригри╛риЙриВрижри╛ ри╣рйИред риЗри╣ риШрй▒риЯ риЧригриири╛ридриориХ ри╕рй░ри╕ри╛ризриири╛риВ рижрйА ри▓рйЛрйЬ ри░рй▒риЦрижрйЗ ри╣рии, ри╕рй░ри╕ри╛ризриири╛риВ рижрйА риШри╛риЯ ри╡ри╛ри▓рйЗ рибри┐ри╡ри╛риИри╕ри╛риВ 'ридрйЗ ридрйИриири╛ридрйА риирйВрй░ рипрйЛриЧ римригри╛риЙриВрижрйЗ ри╣рии, риШрй▒риЯ риКри░риЬри╛ рижрйА риЦрикрид риХри░рижрйЗ ри╣рии риЕридрйЗ риШрй▒риЯ риХри╛ри░римрии рилрйБриЯрикрйНри░ри┐рй░риЯ риирйВрй░ римри░риХри░ри╛ри░ ри░рй▒риЦрижрйЗ ри╣рииред

**ридрйИриири╛ридрйА ри▓риЪриХридри╛**: риЗри╣ риори╛рибри▓ риЗрй░риЯри░риирйИриЯ риХриирйИриХриЯри┐ри╡риЯрйА рижрйА ри▓рйЛрйЬ римри┐риири╛риВ рибри┐ри╡ри╛риИри╕-риЕризри╛ри░ри┐рид AI ри╕риори░рй▒риери╛ри╡ри╛риВ рипрйЛриЧ римригри╛риЙриВрижрйЗ ри╣рии, ри╕риери╛риириХ рикрйНри░рйЛри╕рйИри╕ри┐рй░риЧ ри░ри╛ри╣рйАриВ риЧрйЛрикриирйАрипридри╛ риЕридрйЗ ри╕рйБри░рй▒риЦри┐риЖ риирйВрй░ ри╡ризри╛риЙриВрижрйЗ ри╣рии, рибрйЛриорйЗрии-ри╡ри┐ри╕ри╝рйЗри╕ри╝ риРрикри▓рйАриХрйЗри╕ри╝риири╛риВ ри▓риИ риХри╕риЯриори╛риИриЬри╝ риХрйАридри╛ риЬри╛ ри╕риХрижри╛ ри╣рйИ, риЕридрйЗ ри╡рй▒риЦ-ри╡рй▒риЦ Edge Computing ри╡ри╛ридри╛ри╡ри░ригри╛риВ ри▓риИ риЙриЪри┐рид ри╣рииред

**ри▓ри╛риЧрид рижрйА риХрйБри╕ри╝ри▓ридри╛**: SLMs LLMs рижрйЗ риорйБриХри╛римри▓рйЗ ри▓ри╛риЧрид-рикрйНри░ринри╛ри╡ри╕ри╝ри╛ри▓рйА ри╕ри┐риЦри▓ри╛риИ риЕридрйЗ ридрйИриири╛ридрйА рикрйНри░рижри╛рии риХри░рижрйЗ ри╣рии, риШрй▒риЯ риЪри╛ри▓рйВ ри▓ри╛риЧридри╛риВ риЕридрйЗ Edge риРрикри▓рйАриХрйЗри╕ри╝риири╛риВ ри▓риИ риШрй▒риЯ римрйИриВрибри╡ри┐рибрие рижрйА ри▓рйЛрйЬ ри░рй▒риЦрижрйЗ ри╣рииред

## риЙрй▒риЪ-ри╕ридри╣ риори╛рибри▓ рикрйНри░ри╛рикридрйА ри░ригриирйАридрйАриЖриВ

### Hugging Face Ecosystem

Hugging Face риЙрй▒риЪ-ри╕ридри╣ SLMs рижрйА риЦрйЛриЬ риЕридрйЗ рикри╣рйБрй░риЪ ри▓риИ риорйБрй▒риЦ риХрйЗриВрижри░ ри╡риЬрйЛриВ риХрй░рио риХри░рижри╛ ри╣рйИред рикри▓рйЗриЯрилри╛ри░рио риори╛рибри▓ риЦрйЛриЬ риЕридрйЗ ридрйИриири╛ридрйА ри▓риИ ри╡ри┐ри╕ридрйНри░ри┐рид ри╕рй░ри╕ри╛ризрии рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ:

**риори╛рибри▓ риЦрйЛриЬ ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ**: рикри▓рйЗриЯрилри╛ри░рио рикрйИри░ри╛риорйАриЯри░ риЧри┐ригридрйА, ри▓ри╛риЗри╕рй░ри╕ риХри┐ри╕рио, риЕридрйЗ рикрйНри░рижри░ри╕ри╝рии риори╛рикрижрй░рибри╛риВ рижрйБриЖри░ри╛ риЙрй▒риЪ-ри╕ридри╣ рилри┐ри▓риЯри░ри┐рй░риЧ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред риЙрикринрйЛриЧридри╛ ри╕ри╛риИриб-римри╛риИ-ри╕ри╛риИриб риори╛рибри▓ ридрйБри▓риири╛ риЯрйВри▓, ри░рйАриЕри▓-риЯри╛риИрио рикрйНри░рижри░ри╕ри╝рии римрйИриВриЪриори╛ри░риХ риЕридрйЗ риорйБри▓ри╛риВриХриг рииридрйАриЬрйЗ, риЕридрйЗ ридрйБри░рй░рид риЯрйИри╕риЯри┐рй░риЧ ри▓риИ WebGPU рибрйИриорйЛриЬри╝ ридрй▒риХ рикри╣рйБрй░риЪ риХри░ ри╕риХрижрйЗ ри╣рииред

**риХри┐риКри░рйЗриЯ риХрйАридрйЗ SLM риХри▓рйИриХри╕ри╝рии**: рикрйНри░ри╕ри┐рй▒риз риори╛рибри▓ри╛риВ ри╡ри┐рй▒риЪ Phi-4-mini-3.8B риЙрй▒риЪ-ри╕ридри╣ ридри░риХри╕ри╝рйАри▓ риХри╛ри░риЬри╛риВ ри▓риИ, Qwen3 ри╕рйАри░рйАриЬри╝ (0.6B/1.7B/4B) римри╣рйБринри╛ри╕ри╝ри╛риИ риРрикри▓рйАриХрйЗри╕ри╝риири╛риВ ри▓риИ, Google Gemma3 риХрйБри╕ри╝ри▓ риЬриири░ри▓-рикрйНри░рикриЬри╝ риХри╛ри░риЬри╛риВ ри▓риИ, риЕридрйЗ рикрйНри░рипрйЛриЧри╛ридриориХ риори╛рибри▓ риЬри┐ри╡рйЗриВ риХри┐ BitNET риЕридри┐-риШрй▒риЯ ри╕ри╝рйБрй▒ризридри╛ ридрйИриири╛ридрйА ри▓риИ ри╕ри╝ри╛риори▓ ри╣рииред рикри▓рйЗриЯрилри╛ри░рио ри╡ри┐рй▒риЪ риЦри╛ри╕ рибрйЛриорйЗрии ри▓риИ ри╡ри┐ри╕ри╝рйЗри╕ри╝ риори╛рибри▓ри╛риВ рижрйЗ ри╕риорйБрижри╛риЗ-риЪри▓ри┐рид риХри▓рйИриХри╕ри╝рии риЕридрйЗ ри╡рй▒риЦ-ри╡рй▒риЦ ри╡ри░ридрйЛриВ рижрйЗ риХрйЗри╕ри╛риВ ри▓риИ риЕриирйБриХрйВри▓ри┐рид рикрйНри░рйА-риЯрйНри░рйЗриириб риЕридрйЗ риири┐ри░рижрйЗри╕ри╝-риЯри┐риКриириб ри░рйВрикри╛риВ риирйВрй░ ри╡рйА ри╕ри╝ри╛риори▓ риХрйАридри╛ риЧри┐риЖ ри╣рйИред

### Azure AI Foundry Model Catalog

Azure AI Foundry Model Catalog риЙрй▒риЪ-ри╕ридри╣ SLMs ри▓риИ риЙрижрипрйЛриЧ-риЧрйНри░рйЗриб рикри╣рйБрй░риЪ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ риЬри┐ри╕ ри╡ри┐рй▒риЪ ри╡ризрйЗри░рйЗ риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии ри╕риори░рй▒риери╛ри╡ри╛риВ ри╣рии:

**риЙрижрипрйЛриЧ риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии**: риХрйИриЯри╛ри▓ри╛риЧ ри╡ри┐рй▒риЪ риори╛рибри▓ ри╕ри╝ри╛риори▓ ри╣рии риЬрйЛ ри╕ри┐рй▒ризрйЗ Azure рижрйБриЖри░ри╛ ри╡ри┐риХри░рйЗрид риХрйАридрйЗ риЬри╛риВрижрйЗ ри╣рии, риЬри┐риирйНри╣ри╛риВ ри╡ри┐рй▒риЪ риЙрижрипрйЛриЧ-риЧрйНри░рйЗриб ри╕ри╣ри╛риЗридри╛ риЕридрйЗ SLA ри╕ри╝ри╛риори▓ ри╣рииред Phi-4-mini-3.8B риЙрй▒риЪ-ри╕ридри╣ ридри░риХри╕ри╝рйАри▓ ри╕риори░рй▒риери╛ри╡ри╛риВ ри▓риИ риЕридрйЗ Llama 3-8B риЙридрикри╛рижрии ридрйИриири╛ридрйА ри▓риИ ри╕ри╝ри╛риори▓ ри╣рииред риЗри╕ ри╡ри┐рй▒риЪ ринри░рйЛри╕рйЗриорй░риж ридрйАриЬрйА рикри╛ри░риЯрйА риЦрйБрй▒ри▓рйЗ ри╕ри░рйЛрид риори╛рибри▓ ридрйЛриВ Qwen3 8B ри╡рйА ри╕ри╝ри╛риори▓ ри╣рйИред

**риЙрижрипрйЛриЧ рилри╛риЗрижрйЗ**: рилри╛риИрии-риЯри┐риКриири┐рй░риЧ, рижрйНри░ри┐ри╕ри╝рипридри╛, риЕридрйЗ риЬри╝ри┐рй░риорйЗри╡ри╛ри░ AI ри▓риИ римригри╛риП риЧриП риЯрйВри▓ риори╛рибри▓ рикри░ри┐ри╡ри╛ри░ри╛риВ ри╡ри┐рй▒риЪ Fungible Provisioned Throughput рижрйЗ риири╛ри▓ риЗрй░риЯри┐риЧрйНри░рйЗриЯ риХрйАридрйЗ риЧриП ри╣рииред Microsoft рижрйБриЖри░ри╛ ри╕ри┐рй▒ризрйА ри╕ри╣ри╛риЗридри╛, риЙрижрипрйЛриЧ SLA, риЗрй░риЯри┐риЧрйНри░рйЗриЯ риХрйАридрйА риЧриИ ри╕рйБри░рй▒риЦри┐риЖ риЕридрйЗ риЕриирйБриХрйВри▓ридри╛ ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ, риЕридрйЗ ри╡ри┐ри╕ридрйНри░ри┐рид ридрйИриири╛ридрйА ри╡ри░риХрилри▓рйЛриЬри╝ риЙрижрипрйЛриЧ риЕриирйБринри╡ риирйВрй░ ри╡ризри╛риЙриВрижрйЗ ри╣рииред

## риЙрй▒риЪ-ри╕ридри╣ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии риЕридрйЗ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ридриХриирйАриХ

### Llama.cpp риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии рилри░рйЗриори╡ри░риХ

Llama.cpp Edge ридрйИриири╛ридрйА ри╡ри┐рй▒риЪ ри╡рй▒риз ридрйЛриВ ри╡рй▒риз риХрйБри╕ри╝ри▓ридри╛ ри▓риИ риХриЯри┐рй░риЧ-риРриЬ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии ридриХриирйАриХри╛риВ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ:

**риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии ридри░рйАриХрйЗ**: рилри░рйЗриори╡ри░риХ ри╡рй▒риЦ-ри╡рй▒риЦ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии рикрй▒ризри░ри╛риВ рижри╛ ри╕риори░риерии риХри░рижри╛ ри╣рйИ риЬри┐ри╡рйЗриВ риХри┐ Q4_0 (4-римри┐риЯ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии риири╛ри▓ ри╕ри╝ри╛риирижри╛ри░ риЖриХри╛ри░ риШриЯри╛риЙригри╛ - Qwen3-0.6B риорйЛримри╛риИри▓ ридрйИриири╛ридрйА ри▓риИ риЖрижри░ри╕ри╝), Q5_1 (5-римри┐риЯ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии риЧрйБригри╡рй▒ридри╛ риЕридрйЗ риХрй░рикрйНри░рйИри╕ри╝рии рижри╛ ри╕рй░ридрйБри▓рии - Phi-4-mini-3.8B Edge риЗрй░рилри░рйИриВри╕ ри▓риИ риЙриЪри┐рид), риЕридрйЗ Q8_0 (риорйВри▓ риЧрйБригри╡рй▒ридри╛ рижрйЗ риирйЗрйЬрйЗ - Google Gemma3 риЙридрикри╛рижрии ри╡ри░ридрйЛриВ ри▓риИ ри╕ри┐рилри╛ри░ри╕ри╝рйА)ред BitNET риЕридри┐-риШрй▒риЯ риХрй░рикрйНри░рйИри╕ри╝рии рижрйНри░ри┐ри╕ри╝ри╛риВ ри▓риИ 1-римри┐риЯ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии рижрйЗ риири╛ри▓ риХриЯри┐рй░риЧ-риРриЬ рижри╛ рикрйНри░ридрйАриХ ри╣рйИред

**риЕриори▓ рижрйЗ рилри╛риЗрижрйЗ**: SIMD риРриХри╕ри▓рйЗри░рйЗри╕ри╝рии рижрйЗ риири╛ри▓ CPU-риЕрикриЯри┐риори╛риИриЬри╝риб риЗрй░рилри░рйИриВри╕ риори╛рибри▓ ри▓рйЛрибри┐рй░риЧ риЕридрйЗ риХри╛ри░риЬриХри╛ри░рйА ри▓риИ риорйИриори░рйА-риХрйБри╕ри╝ри▓ридри╛ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред x86, ARM, риЕридрйЗ Apple Silicon риЖри░риХрйАриЯрйИриХриЪри░ри╛риВ ри╡ри┐рй▒риЪ рикри▓рйЗриЯрилри╛ри░рио-риЕриЧриирйЛри╕риЯри┐риХ ридрйИриири╛ридрйА ри╕риори░рй▒риери╛ри╡ри╛риВ рипрйЛриЧ римригри╛риЙриВрижри╛ ри╣рйИред

**риЕриори▓рйА риЕриори▓ рижри╛ риЙрижри╛ри╣ри░рии**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**риорйИриори░рйА рилрйБриЯрикрйНри░ри┐рй░риЯ ридрйБри▓риири╛**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ри╕рйВриЯ

Microsoft Olive риЙридрикри╛рижрии ри╡ри╛ридри╛ри╡ри░ригри╛риВ ри▓риИ рибри┐риЬри╝ри╛риИрии риХрйАридрйЗ риЧриП ри╡ри┐ри╕ридрйНри░ри┐рид риори╛рибри▓ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ри╡ри░риХрилри▓рйЛриЬри╝ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ:

**риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ридриХриирйАриХри╛риВ**: ри╕рйВриЯ ри╡ри┐рй▒риЪ риЧрйБригри╡рй▒ридри╛ риори╛рикрижрй░рибри╛риВ рижрйЗ ри╕рй░ри░риЦриг риирйВрй░ риприХрйАриирйА римригри╛риЙриг ри▓риИ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ри░рйВрикри╛риВ рижрйЗ рикри╛ри░ риЖриЯрйЛриорйИриЯри┐риХ римрйИриВриЪриори╛ри░риХри┐рй░риЧ ри╕ри╝ри╛риори▓ ри╣рйИред PyTorch риЕридрйЗ ONNX ри╡ри░риЧрйЗ рикрйНри░ри╕ри┐рй▒риз ML рилри░рйЗриори╡ри░риХри╛риВ риири╛ри▓ риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии риХри▓ри╛риЙриб риЕридрйЗ Edge ридрйИриири╛ридрйА риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ри╕риори░рй▒риери╛ри╡ри╛риВ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред

**риЕриори▓рйА риЕриори▓ рижри╛ риЙрижри╛ри╣ри░рии**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX Framework

Apple MLX риЦри╛ри╕ ридрйМри░ 'ридрйЗ Apple Silicon рибри┐ри╡ри╛риИри╕ри╛риВ ри▓риИ рибри┐риЬри╝ри╛риИрии риХрйАридрйА риЧриИ ри╕риери╛риириХ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ:

**Apple Silicon риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии**: рилри░рйЗриори╡ри░риХ рипрйВриири┐рилри╛риИриб риорйИриори░рйА риЖри░риХрйАриЯрйИриХриЪри░ рижрйЗ риири╛ри▓ Metal Performance Shaders риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии, риЖриЯрйЛриорйИриЯри┐риХ риори┐риХри╕риб рикрйНри░ри┐ри╕рйАри╕ри╝рии риЗрй░рилри░рйИриВри╕ (риЦри╛ри╕ ридрйМри░ 'ридрйЗ Google Gemma3 ри▓риИ рикрйНри░ринри╛ри╡ри╕ри╝ри╛ри▓рйА), риЕридрйЗ риЕрикриЯри┐риори╛риИриЬри╝риб риорйИриори░рйА римрйИриВрибри╡ри┐рибрие рижрйА ри╡ри░ридрйЛриВ рипрйЛриЧ римригри╛риЙриВрижри╛ ри╣рйИред Phi-4-mini-3.8B M-ри╕рйАри░рйАриЬри╝ риЪри┐рикри╕ 'ридрйЗ ри╕ри╝ри╛риирижри╛ри░ рикрйНри░рижри░ри╕ри╝рии рижри┐риЦри╛риЙриВрижри╛ ри╣рйИ, риЬрижрйЛриВ риХри┐ Qwen3-1.7B MacBook Air ридрйИриири╛ридрйА ри▓риИ ри╕рй░ридрйБри▓рии рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред

**рибри┐ри╡рйИри▓рикриорйИриВриЯ ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ**: Python риЕридрйЗ Swift API ри╕ри╣ри╛риЗридри╛ рижрйЗ риири╛ри▓ NumPy-риЕриирйБриХрйВри▓ риРри░рйЗ риХри╛ри░риЬ, риЖриЯрйЛриорйИриЯри┐риХ рибри┐рилри░рйИриВри╕ри╝рйАриПри╕ри╝рии ри╕риори░рй▒риери╛ри╡ри╛риВ, риЕридрйЗ Apple рибри┐ри╡рйИри▓рикриорйИриВриЯ риЯрйВри▓ри╛риВ риири╛ри▓ ри╕ри╣рйА риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии ри╡ри┐ри╕ридрйНри░ри┐рид рибри┐ри╡рйИри▓рикриорйИриВриЯ ри╡ри╛ридри╛ри╡ри░риг рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред

**риЕриори▓рйА риЕриори▓ рижри╛ риЙрижри╛ри╣ри░рии**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## риЙридрикри╛рижрии ридрйИриири╛ридрйА риЕридрйЗ риЗрй░рилри░рйИриВри╕ ри░ригриирйАридрйАриЖриВ

### Ollama: ри╕ризри╛ри░рии ри╕риери╛риириХ ридрйИриири╛ридрйА

Ollama Edge риЕридрйЗ ри╕риери╛риириХ ри╡ри╛ридри╛ри╡ри░ригри╛риВ ри▓риИ риЙрижрипрйЛриЧ-ридри┐риЖри░ ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ рижрйЗ риири╛ри▓ SLM ридрйИриири╛ридрйА риирйВрй░ ри╕ризри╛ри░рии римригри╛риЙриВрижри╛ ри╣рйИ:

**ридрйИриири╛ридрйА ри╕риори░рй▒риери╛ри╡ри╛риВ**: риЗрй▒риХ-риХриори╛риВриб риори╛рибри▓ риЗрй░ри╕риЯри╛ри▓рйЗри╕ри╝рии риЕридрйЗ риХри╛ри░риЬриХри╛ри░рйА рижрйЗ риири╛ри▓ риЖриЯрйЛриорйИриЯри┐риХ риори╛рибри▓ рикрйВри▓ри┐рй░риЧ риЕридрйЗ риХрйИри╕ри╝ри┐рй░риЧред Phi-4-mini-3.8B, рикрйВри░рйА Qwen3 ри╕рйАри░рйАриЬри╝ (0.6B/1.7B/4B), риЕридрйЗ Google Gemma3 ри▓риИ ри╕ри╣ри╛риЗридри╛ REST API рижрйЗ риири╛ри▓ риРрикри▓рйАриХрйЗри╕ри╝рии риЗрй░риЯри┐риЧрйНри░рйЗри╕ри╝рии риЕридрйЗ риори▓риЯрйА-риори╛рибри▓ рикрйНри░римрй░ризрии риЕридрйЗ ри╕ри╡ри┐рй▒риЪри┐рй░риЧ ри╕риори░рй▒риери╛ри╡ри╛риВ рикрйНри░рижри╛рии риХри░рижрйА ри╣рйИред BitNET риори╛рибри▓ 1-римри┐риЯ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии ри╕ри╣ри╛риЗридри╛ ри▓риИ рикрйНри░рипрйЛриЧри╛ридриориХ римри┐ри▓риб ри╕рй░ри░риЪриири╛ри╡ри╛риВ рижрйА ри▓рйЛрйЬ ри░рй▒риЦрижрйЗ ри╣рииред

**риЙрй▒риЪ-ри╕ридри╣ ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ**: риХри╕риЯрио риори╛рибри▓ рилри╛риИрии-риЯри┐риКриири┐рй░риЧ ри╕ри╣ри╛риЗридри╛, риХрй░риЯрйЗриири░ри╛риИриЬри╝риб ридрйИриири╛ридрйА ри▓риИ Dockerfile риЬриири░рйЗри╕ри╝рии, GPU риРриХри╕ри▓рйЗри░рйЗри╕ри╝рии рижрйЗ риири╛ри▓ риЖриЯрйЛриорйИриЯри┐риХ рибри┐риЯрйИриХри╕ри╝рии, риЕридрйЗ риори╛рибри▓ риХри╡ри╛риВриЯрйАриЬри╝рйЗри╕ри╝рии риЕридрйЗ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии ри╡ри┐риХри▓рик ри╡ри┐ри╕ридрйНри░ри┐рид ридрйИриири╛ридрйА ри▓риЪриХридри╛ рикрйНри░рижри╛рии риХри░рижрйЗ ри╣рииред

### VLLM: риЙрй▒риЪ-рикрйНри░рижри░ри╕ри╝рии риЗрй░рилри░рйИриВри╕

VLLM риЙрй▒риЪ-риери░рйВрикрйБрй▒риЯ рижрйНри░ри┐ри╕ри╝ри╛риВ ри▓риИ риЙридрикри╛рижрии-риЧрйНри░рйЗриб риЗрй░рилри░рйИриВри╕ риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИ:

**рикрйНри░рижри░ри╕ри╝рии риЕрикриЯри┐риори╛риИриЬри╝рйЗри╕ри╝рии**: PagedAttention риорйИриори░рйА-риХрйБри╕ри╝ри▓ ризри┐риЖрии риЧригриири╛ ри▓риИ (риЦри╛ри╕ ридрйМри░ 'ридрйЗ Phi-4-mini-3.8B рижрйЗ риЯрйНри░ри╛риВри╕рилри╛ри░риори░ риЖри░риХрйАриЯрйИриХриЪри░ ри▓риИ ри▓ри╛ринрижри╛риЗриХ), рибри╛риЗриири╛риори┐риХ римрйИ

---

**риЕри╕ри╡рйАриХри╛ри░риири╛**:  
риЗри╣ рижри╕ридри╛ри╡рйЗриЬри╝ AI риЕриирйБри╡ри╛риж ри╕рйЗри╡ри╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рижрйА ри╡ри░ридрйЛриВ риХри░риХрйЗ риЕриирйБри╡ри╛риж риХрйАридри╛ риЧри┐риЖ ри╣рйИред риЬрижрйЛриВ риХри┐ риЕри╕рйАриВ ри╕ри╣рйАридри╛ ри▓риИ рипридриири╕ри╝рйАри▓ ри╣ри╛риВ, риХри┐ри░рикри╛ риХри░риХрйЗ ризри┐риЖрии рижри┐риУ риХри┐ ри╕ри╡рйИриЪри╛ри▓ри┐рид риЕриирйБри╡ри╛рижри╛риВ ри╡ри┐рй▒риЪ риЧри▓ридрйАриЖриВ риЬри╛риВ риЕри╕рйБригрйАриЦридри╛риИриЖриВ ри╣рйЛ ри╕риХрижрйАриЖриВ ри╣рииред риорйВри▓ рижри╕ридри╛ри╡рйЗриЬри╝ риирйВрй░ риЗри╕рижрйА риорйВри▓ ринри╛ри╕ри╝ри╛ ри╡ри┐рй▒риЪ риЕризри┐риХри╛ри░рид ри╕ри░рйЛрид риорй░риири┐риЖ риЬри╛ригри╛ риЪри╛ри╣рйАрижри╛ ри╣рйИред риори╣рй▒ридри╡рикрйВри░рии риЬри╛ригриХри╛ри░рйА ри▓риИ, рикрйЗри╕ри╝рйЗри╡ри░ риориирйБрй▒риЦрйА риЕриирйБри╡ри╛риж рижрйА ри╕ри┐рилри╛ри░ри╕ри╝ риХрйАридрйА риЬри╛риВрижрйА ри╣рйИред риЗри╕ риЕриирйБри╡ри╛риж рижрйА ри╡ри░ридрйЛриВ ридрйЛриВ рикрйИрижри╛ ри╣рйЛриг ри╡ри╛ри▓рйЗ риХри┐ри╕рйЗ ри╡рйА риЧри▓ридрилри╣ри┐риорйА риЬри╛риВ риЧри▓рид ри╡ри┐риЖриЦри┐риЖ ри▓риИ риЕри╕рйАриВ риЬри╝ри┐рй░риорйЗри╡ри╛ри░ риири╣рйАриВ ри╣ри╛риВред