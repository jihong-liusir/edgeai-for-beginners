<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T21:10:52+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "pa"
}
-->
# ри╕рйИри╕ри╝рии 4: риЪрйЗриири▓ри┐риЯ риири╛ри▓ рикрйНри░рйЛрибриХри╕ри╝рии риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии римригри╛риЙригри╛

## риЭри▓риХ

риЗри╕ ри╕рйИри╕ри╝рии ри╡ри┐рй▒риЪ риЪрйЗриири▓ри┐риЯ риЕридрйЗ риори╛риИриХри░рйЛри╕ри╛рилриЯ рилри╛риКриВрибри░рйА ри▓рйЛриХри▓ рижрйА ри╡ри░ридрйЛриВ риХри░риХрйЗ рикрйНри░рйЛрибриХри╕ри╝рии-ридри┐риЖри░ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии римригри╛риЙриг 'ридрйЗ ризри┐риЖрии рижри┐рй▒ридри╛ риЧри┐риЖ ри╣рйИред ридрйБри╕рйАриВ AI риЧрй▒ри▓римри╛ридри╛риВ ри▓риИ риЖризрйБриири┐риХ ри╡рйИрй▒рим риЗрй░риЯри░рилрйЗри╕ римригри╛риЙриг, ри╕риЯрйНри░рйАриори┐рй░риЧ риЬри╡ри╛рим ри▓ри╛риЧрйВ риХри░рии риЕридрйЗ риЙрй▒риЪ-риЧрйБригри╡рй▒ридри╛ ри╡ри╛ри▓рйЗ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии риирйВрй░ ридри┐риЖри░ риХри░рии рижрйЗ ридри░рйАриХрйЗ ри╕ри┐рй▒риЦрйЛриЧрйЗред

**ридрйБри╣ри╛рибри╛ римригри╛риЗриЖ ри╣рйЛри╡рйЗриЧри╛:**
- **риЪрйЗриири▓ри┐риЯ риЪрйИриЯ риРрик**: ри╕риЯрйНри░рйАриори┐рй░риЧ риЬри╡ри╛римри╛риВ риири╛ри▓ риЖризрйБриири┐риХ ри╡рйИрй▒рим UI
- **WebGPU рибрйИриорйЛ**: риЧрйЛрикриирйАрипридри╛-рикри╣ри┐ри▓ри╛риВ риРрикри▓рйАриХрйЗри╕ри╝рии ри▓риИ римрйНри░ри╛риКриЬри╝ри░-риЕризри╛ри░ри┐рид риЗрй░рилри░рйИриВри╕  
- **Open WebUI риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии**: Foundry Local риири╛ри▓ рикрйЗри╕ри╝рйЗри╡ри░ риЪрйИриЯ риЗрй░риЯри░рилрйЗри╕
- **рикрйНри░рйЛрибриХри╕ри╝рии рикрйИриЯри░рии**: риЧри▓ридрйА ри╕рй░ринри╛ри▓, риири┐риЧри░ри╛риирйА, риЕридрйЗ рибри┐рикри▓рйМриЗриорйИриВриЯ ри░ригриирйАридрйАриЖриВ

## ри╕ри┐рй▒риЦриг рижрйЗ риЙрижрйЗри╕ри╝

- риЪрйЗриири▓ри┐риЯ риири╛ри▓ рикрйНри░рйЛрибриХри╕ри╝рии-ридри┐риЖри░ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии римригри╛риЙригри╛
- ри╡ризрйЗри░рйЗ риЙрикринрйЛриЧридри╛ риЕриирйБринри╡ ри▓риИ ри╕риЯрйНри░рйАриори┐рй░риЧ риЬри╡ри╛рим ри▓ри╛риЧрйВ риХри░риири╛
- Foundry Local SDK риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии рикрйИриЯри░рии ри╡ри┐рй▒риЪ риори╛ри╣ри░ ри╣рйЛригри╛
- ри╕ри╣рйА риЧри▓ридрйА ри╕рй░ринри╛ри▓ риЕридрйЗ ри╕рйБриЪри╛ри░рйВ рибри┐риЧри░рйЗрибрйЗри╕ри╝рии ри▓ри╛риЧрйВ риХри░риири╛
- ри╡рй▒риЦ-ри╡рй▒риЦ ри╡ри╛ридри╛ри╡ри░ригри╛риВ ри▓риИ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии рибри┐рикри▓рйМриЗ риЕридрйЗ ри╕рй░ри░риЪри┐рид риХри░риири╛
- риЧрй▒ри▓римри╛ридрйА AI ри▓риИ риЖризрйБриири┐риХ ри╡рйИрй▒рим UI рикрйИриЯри░рии риирйВрй░ ри╕риориЭригри╛

## рикрйВри░ри╡ ри╕ри╝ри░ридри╛риВ

- **Foundry Local**: риЗрй░ри╕риЯри╛ри▓ риЕридрйЗ риЪри▓ ри░ри╣рйА ([риЗрй░ри╕риЯри╛ри▓рйЗри╕ри╝рии риЧри╛риИриб](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 риЬри╛риВ риЗри╕ ридрйЛриВ римри╛риЕриж рижрйА ри╡ри░риЬрии, ри╡ри░риЪрйБриЕри▓ риРриири╡ри╛риЗри░риорйИриВриЯ ри╕риори░рй▒риери╛ риири╛ри▓
- **риори╛рибри▓**: риШрй▒риЯрйЛ-риШрй▒риЯ риЗрй▒риХ риори╛рибри▓ ри▓рйЛриб риХрйАридри╛ ри╣рйЛриЗриЖ (`foundry model run phi-4-mini`)
- **римрйНри░ри╛риКриЬри╝ри░**: WebGPU ри╕ри╣ри╛риЗридри╛ риири╛ри▓ риЖризрйБриири┐риХ ри╡рйИрй▒рим римрйНри░ри╛риКриЬри╝ри░ (Chrome/Edge)
- **Docker**: Open WebUI риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии ри▓риИ (ри╡ри┐риХри▓рикри┐риХ)

## ринри╛риЧ 1: риЖризрйБриири┐риХ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии риирйВрй░ ри╕риориЭригри╛

### риЖри░риХрйАриЯрйИриХриЪри░ риЭри▓риХ

```
User Browser тЖРтЖТ Chainlit UI тЖРтЖТ Python Backend тЖРтЖТ Foundry Local тЖРтЖТ AI Model
      тЖУ              тЖУ              тЖУ              тЖУ            тЖУ
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### риорйБрй▒риЦ ридриХриири╛ри▓рйЛриЬрйАриЖриВ

**Foundry Local SDK рикрйИриЯри░рии:**
- `FoundryLocalManager(alias)`: ри╕рйЗри╡ри╛ рикрйНри░римрй░ризрии риирйВрй░ ри╕ри╡рйИриЪри╛ри▓ри┐рид риХри░рижри╛ ри╣рйИ
- `manager.endpoint` риЕридрйЗ `manager.api_key`: риХриирйИриХри╕ри╝рии ри╡рйЗри░ри╡рйЗ
- `manager.get_model_info(alias).id`: риори╛рибри▓ рижрйА рикриЫри╛риг

**риЪрйЗриири▓ри┐риЯ рилри░рйЗриори╡ри░риХ:**
- `@cl.on_chat_start`: риЪрйИриЯ ри╕рйИри╕ри╝рии ри╕ри╝рйБри░рйВ риХри░рйЛ
- `@cl.on_message`: риЖриЙриг ри╡ри╛ри▓рйЗ риЙрикринрйЛриЧридри╛ ри╕рйБриирйЗри╣рйЗ ри╕рй░ринри╛ри▓рйЛ  
- `cl.Message().stream_token()`: ри░рйАриЕри▓-риЯри╛риИрио ри╕риЯрйНри░рйАриори┐рй░риЧ
- риЖриЯрйЛриорйИриЯри┐риХ UI риЬриири░рйЗри╕ри╝рии риЕридрйЗ WebSocket рикрйНри░римрй░ризрии

## ринри╛риЧ 2: ри▓рйЛриХри▓ ри╡ри┐ри░рйБрй▒риз риХри▓ри╛риЙриб рилрйИри╕ри▓ри╛ риорйИриЯрйНри░ри┐риХри╕

### рикрйНри░рижри░ри╕ри╝рии ри╡ри┐ри╕ри╝рйЗри╕ри╝ридри╛ри╡ри╛риВ

| рикри╣ри▓рйВ | ри▓рйЛриХри▓ (Foundry) | риХри▓ри╛риЙриб (Azure OpenAI) |
|--------|-----------------|---------------------|
| **ри▓рйЗриЯрйИриВри╕рйА** | ЁЯЪА 50-200ms (риХрйЛриИ риирйИриЯри╡ри░риХ риири╣рйАриВ) | тП▒я╕П 200-2000ms (риирйИриЯри╡ри░риХ риЙрй▒ридрйЗ риири┐ри░ринри░) |
| **риЧрйЛрикриирйАрипридри╛** | ЁЯФТ рибри╛риЯри╛ риХрижрйЗ ри╡рйА рибри┐ри╡ри╛риИри╕ ридрйЛриВ римри╛ри╣ри░ риири╣рйАриВ риЬри╛риВрижри╛ | тЪая╕П рибри╛риЯри╛ риХри▓ри╛риЙриб риирйВрй░ ринрйЗриЬри┐риЖ риЬри╛риВрижри╛ ри╣рйИ |
| **ри▓ри╛риЧрид** | ЁЯТ░ ри╣ри╛ри░рибри╡рйЗриЕри░ ридрйЛриВ римри╛риЕриж риорйБрилри╝рид | ЁЯТ╕ рикрйНри░ридрйА риЯрйЛриХрии ринрйБриЧридри╛рии риХри░рйЛ |
| **риЖрилри▓ри╛риИрии** | тЬЕ риЗрй░риЯри░риирйИриЯ ридрйЛриВ римри┐риири╛риВ риХрй░рио риХри░рижри╛ ри╣рйИ | тЭМ риЗрй░риЯри░риирйИриЯ рижрйА ри▓рйЛрйЬ ри╣рйИ |
| **риори╛рибри▓ риЖриХри╛ри░** | тЪая╕П ри╣ри╛ри░рибри╡рйЗриЕри░ рижрйБриЖри░ри╛ ри╕рйАриори┐рид | тЬЕ ри╕рин ридрйЛриВ ри╡рй▒рибрйЗ риори╛рибри▓ ридрй▒риХ рикри╣рйБрй░риЪ |
| **ри╕риХрйЗри▓ри┐рй░риЧ** | тЪая╕П ри╣ри╛ри░рибри╡рйЗриЕри░ риЙрй▒ридрйЗ риири┐ри░ринри░ | тЬЕ риЕриирй░рид ри╕риХрйЗри▓ри┐рй░риЧ |

### ри╣ри╛риИримрйНри░ри┐риб ри░ригриирйАридрйА рикрйИриЯри░рии

**ри▓рйЛриХри▓-рикри╣ри┐ри▓ри╛риВ рилри╛ри▓римрйИриХ риири╛ри▓:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**риЯри╛ри╕риХ-риЕризри╛ри░ри┐рид ри░рйВриЯри┐рй░риЧ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## ринри╛риЧ 3: ри╕рйИриВрикри▓ 04 - риЪрйЗриири▓ри┐риЯ риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии

### ридрйБри░рй░рид ри╕ри╝рйБри░рйВриЖрид

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

риРрикри▓рйАриХрйЗри╕ри╝рии риЖриЯрйЛриорйИриЯри┐риХ ридрйМри░ 'ридрйЗ `http://localhost:8080` 'ридрйЗ риЦрйБри▓рйНри╣ риЬри╛риВрижрйА ри╣рйИ риЬри┐ри╕ ри╡ри┐рй▒риЪ риЖризрйБриири┐риХ риЪрйИриЯ риЗрй░риЯри░рилрйЗри╕ ри╣рйБрй░рижри╛ ри╣рйИред

### риорйБрй▒риЦ риХри╛ри░риЬ

ри╕рйИриВрикри▓ 04 риРрикри▓рйАриХрйЗри╕ри╝рии рикрйНри░рйЛрибриХри╕ри╝рии-ридри┐риЖри░ рикрйИриЯри░рии рижри┐риЦри╛риЙриВрижрйА ри╣рйИ:

**риЖриЯрйЛриорйИриЯри┐риХ ри╕рйЗри╡ри╛ риЦрйЛриЬ:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**ри╕риЯрйНри░рйАриори┐рй░риЧ риЪрйИриЯ ри╣рйИриВрибри▓ри░:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### ри╕рй░ри░риЪриири╛ ри╡ри┐риХри▓рик

**ри╡ри╛ридри╛ри╡ри░риг риЪри░:**

| риЪри░ | ри╡рйЗри░ри╡ри╛ | рибри┐рилри╛ри▓риЯ | риЙрижри╛ри╣ри░рии |
|----------|-------------|---------|----------|
| `MODEL` | ри╡ри░ридриг ри▓риИ риори╛рибри▓ риЕри▓рйАриЕри╕ | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local риРриВрибрикрйМриЗрй░риЯ | риЖриЯрйЛ-рибри┐риЯрйИриХриЯ риХрйАридри╛ | `http://localhost:51211` |
| `API_KEY` | API риХрйБрй░риЬрйА (ри▓рйЛриХри▓ ри▓риИ ри╡ри┐риХри▓рикри┐риХ) | `""` | `your-api-key` |

**риЙрй▒риЪ-риЕризри┐риХридрио ри╡ри░ридрйЛриВ:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## ринри╛риЧ 4: Jupyter риирйЛриЯримрйБрй▒риХ римригри╛риЙригри╛ риЕридрйЗ ри╡ри░ридригри╛

### риирйЛриЯримрйБрй▒риХ ри╕ри╣ри╛риЗридри╛ рижри╛ риЭри▓риХ

ри╕рйИриВрикри▓ 04 ри╡ри┐рй▒риЪ риЗрй▒риХ ри╡ри┐ри╕ридрйНри░ри┐рид Jupyter риирйЛриЯримрйБрй▒риХ (`chainlit_app.ipynb`) ри╕ри╝ри╛риори▓ ри╣рйИ риЬрйЛ рикрйНри░рижри╛рии риХри░рижрйА ри╣рйИ:

- **ЁЯУЪ ри╕ри┐рй▒риЦриг ри╕риорй▒риЧри░рйА**: риХрижрио-рижри░-риХрижрио ри╕ри┐рй▒риЦриг ри╕риорй▒риЧри░рйА
- **ЁЯФм риЗрй░риЯри░риРриХриЯри┐ри╡ рикрйЬриЪрйЛри▓**: риХрйЛриб ри╕рйИрй▒ри▓ риЪри▓ри╛риУ риЕридрйЗ риЕриирйБринри╡ риХри░рйЛ
- **ЁЯУК ри╡ри┐риЬри╝рйВриЕри▓ рибрйИриорйЛриири╕риЯри░рйЗри╕ри╝рии**: риЪри╛ри░риЯ, рибри╛риЗриЧрйНри░ри╛рио, риЕридрйЗ риЖриЙриЯрикрйБрй▒риЯ ри╡ри┐риЬри╝рйВриЕри▓ри╛риИриЬри╝рйЗри╕ри╝рии
- **ЁЯЫая╕П ри╡ри┐риХри╛ри╕ риЯрйВри▓**: риЯрйИри╕риЯри┐рй░риЧ риЕридрйЗ рибри┐римрй▒риЧри┐рй░риЧ ри╕риори░рй▒риери╛

### риЖрикригрйАриЖриВ риирйЛриЯримрйБрй▒риХ римригри╛риЙригри╛

#### риХрижрио 1: Jupyter ри╡ри╛ридри╛ри╡ри░риг ри╕рйИриЯриЕрик риХри░рйЛ

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### риХрижрио 2: риири╡рйАриВ риирйЛриЯримрйБрй▒риХ римригри╛риУ

**VS Code рижрйА ри╡ри░ридрйЛриВ риХри░риХрйЗ:**
1. Module08 рибри╛риЗри░рйИриХриЯри░рйА ри╡ри┐рй▒риЪ VS Code риЦрйЛри▓рйНри╣рйЛ
2. `.ipynb` риРриХри╕риЯрйИриВри╕ри╝рии риири╛ри▓ риири╡рйАриВ рилри╛риИри▓ римригри╛риУ
3. "Foundry Local" kernel риЪрйБригрйЛ риЬрижрйЛриВ рикрйБрй▒риЫри┐риЖ риЬри╛ри╡рйЗ
4. риЖрикригрйЗ ри╕риорй▒риЧри░рйА риири╛ри▓ ри╕рйИрй▒ри▓ ри╕ри╝рйБри░рйВ риХри░рйЛ

**Jupyter Lab рижрйА ри╡ри░ридрйЛриВ риХри░риХрйЗ:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### риирйЛриЯримрйБрй▒риХ ри╕рй░ри░риЪриири╛ ри▓риИ ри╕рйНри░рйЗри╕ри╝риа риЕринри┐риЖри╕

#### ри╕рйИрй▒ри▓ ри╕рй░риЧриарии

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("тЬЕ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### риЗрй░риЯри░риРриХриЯри┐ри╡ риЙрижри╛ри╣ри░рии риЕридрйЗ риЕринри┐риЖри╕

#### риЕринри┐риЖри╕ 1: риХри▓ри╛риЗрй░риЯ ри╕рй░ри░риЪриири╛ риЯрйИри╕риЯри┐рй░риЧ

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nЁЯзк Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'тЬЕ Success' if result['status'] == 'ok' else 'тЭМ Failed'}")
```

#### риЕринри┐риЖри╕ 2: ри╕риЯрйНри░рйАриори┐рй░риЧ риЬри╡ри╛рим ри╕ри┐риорйВри▓рйЗри╕ри╝рии

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ЁЯМК Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nтЬЕ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## ринри╛риЧ 5: WebGPU римрйНри░ри╛риКриЬри╝ри░ риЗрй░рилри░рйИриВри╕ рибрйИриорйЛ

### риЭри▓риХ

WebGPU риЧрйЛрикриирйАрипридри╛ риЕридрйЗ риЬри╝рйАри░рйЛ-риЗрй░ри╕риЯри╛ри▓ риЕриирйБринри╡ри╛риВ ри▓риИ риори╛рибри▓ри╛риВ риирйВрй░ ри╕ри┐рй▒ризрйЗ римрйНри░ри╛риКриЬри╝ри░ ри╡ри┐рй▒риЪ риЪри▓ри╛риЙриг рижрйА ри╕риори░рй▒риери╛ рижри┐рй░рижри╛ ри╣рйИред риЗри╣ ри╕рйИриВрикри▓ ONNX Runtime Web риири╛ри▓ WebGPU риРриЧриЬри╝ри┐риХри┐риКри╕ри╝рии рижри┐риЦри╛риЙриВрижри╛ ри╣рйИред

### риХрижрио 1: WebGPU ри╕ри╣ри╛риЗридри╛ рижрйА риЬри╛риВриЪ риХри░рйЛ

**римрйНри░ри╛риКриЬри╝ри░ рижрйАриЖриВ ри▓рйЛрйЬри╛риВ:**
- Chrome/Edge 113+ WebGPU риири╛ри▓ ри╕ри╣ри╛риЗрид
- риЬри╛риВриЪ: `chrome://gpu` тЖТ "WebGPU" ри╕риери┐ридрйА рижрйА рикрйБри╕ри╝риЯрйА риХри░рйЛ
- рикрйНри░рйЛриЧри░ри╛риориорйИриЯри┐риХ риЬри╛риВриЪ: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### риХрижрио 2: WebGPU рибрйИриорйЛ римригри╛риУ

рибри╛риЗри░рйИриХриЯри░рйА римригри╛риУ: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ЁЯЪА WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'тЭМ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ЁЯФН WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('тЬЕ ONNX Runtime session created with WebGPU');
        log(`ЁЯУК Input names: ${session.inputNames.join(', ')}`);
        log(`ЁЯУК Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'тЬЕ WebGPU inference complete!';
        log(`ЁЯОп Predicted class: ${maxIdx}`);
        log(`ЁЯУИ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `тЭМ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### риХрижрио 3: рибрйИриорйЛ риЪри▓ри╛риУ

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## ринри╛риЧ 6: Open WebUI риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии

### риЭри▓риХ

Open WebUI Foundry Local рижрйЗ OpenAI-риЕриирйБриХрйВри▓ API риири╛ри▓ риЬрйБрйЬрии ри▓риИ риЗрй▒риХ рикрйЗри╕ри╝рйЗри╡ри░ ChatGPT-риЬри┐ри╡рйЗриВ риЗрй░риЯри░рилрйЗри╕ рикрйНри░рижри╛рии риХри░рижри╛ ри╣рйИред

### риХрижрио 1: рикрйВри░ри╡ ри╕ри╝ри░ридри╛риВ

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### риХрижрио 2: Docker ри╕рйИриЯриЕрик (ри╕рйБриЭри╛риП риЧриП)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**риирйЛриЯ:** `host.docker.internal` Windows 'ридрйЗ Docker риХрй░риЯрйЗриири░ри╛риВ риирйВрй░ ри╣рйЛри╕риЯ риори╕ри╝рйАрии ридрй▒риХ рикри╣рйБрй░риЪриг рижрйА риЖриЧри┐риЖ рижри┐рй░рижри╛ ри╣рйИред

### риХрижрио 3: ри╕рй░ри░риЪриири╛

1. **римрйНри░ри╛риКриЬри╝ри░ риЦрйЛри▓рйНри╣рйЛ:** `http://localhost:3000` 'ридрйЗ риЬри╛риУ
2. **ри╕ри╝рйБри░рйВриЖридрйА ри╕рйИриЯриЕрик:** риРрибриори┐рии риЦри╛ридри╛ римригри╛риУ
3. **риори╛рибри▓ ри╕рй░ри░риЪриири╛:**
   - Settings тЖТ Models тЖТ OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (риХрйЛриИ ри╡рйА риорйБрй▒ри▓ риЪрй▒ри▓рйЗриЧри╛)
4. **риХриирйИриХри╕ри╝рии риЯрйИри╕риЯ риХри░рйЛ:** риори╛рибри▓ рибрйНри░рйМрикрибри╛риКрии ри╡ри┐рй▒риЪ рижри┐риЦри╛риИ рижрйЗригрйЗ риЪри╛ри╣рйАрижрйЗ ри╣рии

### риЯрйНри░римри▓ри╕ри╝рйВриЯри┐рй░риЧ

**риЖрио ри╕риорй▒ри╕ри┐риЖри╡ри╛риВ:**

1. **риХриирйИриХри╕ри╝рии ри░ри┐рилри┐риКриЬри╝риб:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **риори╛рибри▓ рижри┐риЦри╛риИ риири╣рйАриВ рижрйЗ ри░ри╣рйЗ:**
   - рикрйБри╕ри╝риЯрйА риХри░рйЛ риХри┐ риори╛рибри▓ ри▓рйЛриб ри╣рйИ: `foundry model list`
   - API риЬри╡ри╛рим рижрйА риЬри╛риВриЪ риХри░рйЛ: `curl http://localhost:51211/v1/models`
   - Open WebUI риХрй░риЯрйЗриири░ риирйВрй░ риорйБрйЬ ри╕ри╝рйБри░рйВ риХри░рйЛ

## ринри╛риЧ 7: рикрйНри░рйЛрибриХри╕ри╝рии рибри┐рикри▓рйМриЗриорйИриВриЯ ри╡ри┐риЪри╛ри░

### ри╡ри╛ридри╛ри╡ри░риг ри╕рй░ри░риЪриири╛

**ри╡ри┐риХри╛ри╕ ри╕рйИриЯриЕрик:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**рикрйНри░рйЛрибриХри╕ри╝рии рибри┐рикри▓рйМриЗриорйИриВриЯ:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### риЖрио рикрйЛри░риЯ ри╕риорй▒ри╕ри┐риЖри╡ри╛риВ риЕридрйЗ ри╣рй▒ри▓

**Port 51211 ри╕рй░риШри░ри╕ри╝ ри░рйЛриХриери╛рио:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### рикрйНри░рижри░ри╕ри╝рии риири┐риЧри░ри╛риирйА

**ри╣рйИри▓рие риЪрйИриХ ри▓ри╛риЧрйВ риХри░риири╛:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## ри╕ри╛ри░

ри╕рйИри╕ри╝рии 4 ри╡ри┐рй▒риЪ риЧрй▒ри▓римри╛ридрйА AI ри▓риИ рикрйНри░рйЛрибриХри╕ри╝рии-ридри┐риЖри░ риЪрйЗриири▓ри┐риЯ риРрикри▓рйАриХрйЗри╕ри╝рии римригри╛риЙриг риирйВрй░ риХри╡ри░ риХрйАридри╛ риЧри┐риЖред ридрйБри╕рйАриВ ри╕ри┐рй▒риЦри┐риЖ:

- тЬЕ **риЪрйЗриири▓ри┐риЯ рилри░рйЗриори╡ри░риХ**: риЪрйИриЯ риРрикри▓рйАриХрйЗри╕ри╝рии ри▓риИ риЖризрйБриири┐риХ UI риЕридрйЗ ри╕риЯрйНри░рйАриори┐рй░риЧ ри╕ри╣ри╛риЗридри╛
- тЬЕ **Foundry Local риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии**: SDK рижрйА ри╡ри░ридрйЛриВ риЕридрйЗ ри╕рй░ри░риЪриири╛ рикрйИриЯри░рии  
- тЬЕ **WebGPU риЗрй░рилри░рйИриВри╕**: риЧрйЛрикриирйАрипридри╛ ри▓риИ римрйНри░ри╛риКриЬри╝ри░-риЕризри╛ри░ри┐рид AI
- тЬЕ **Open WebUI ри╕рйИриЯриЕрик**: рикрйЗри╕ри╝рйЗри╡ри░ риЪрйИриЯ риЗрй░риЯри░рилрйЗри╕ рибри┐рикри▓рйМриЗриорйИриВриЯ
- тЬЕ **рикрйНри░рйЛрибриХри╕ри╝рии рикрйИриЯри░рии**: риЧри▓ридрйА ри╕рй░ринри╛ри▓, риири┐риЧри░ри╛риирйА, риЕридрйЗ ри╕риХрйЗри▓ри┐рй░риЧ

ри╕рйИриВрикри▓ 04 риРрикри▓рйАриХрйЗри╕ри╝рии Microsoft Foundry Local рижрйБриЖри░ри╛ ри╕риери╛риириХ AI риори╛рибри▓ри╛риВ риирйВрй░ ри▓рйИри╡ри░рйЗриЬ риХри░рижрйЗ ри╣рйЛриП риориЬри╝римрйВрид риЪрйИриЯ риЗрй░риЯри░рилрйЗри╕ римригри╛риЙриг ри▓риИ ри╕рйНри░рйЗри╕ри╝риа риЕринри┐риЖри╕ рижри┐риЦри╛риЙриВрижрйА ри╣рйИ, риЬрйЛ риЙридриХрйНри░ри┐ри╕ри╝риЯ риЙрикринрйЛриЧридри╛ риЕриирйБринри╡ рикрйНри░рижри╛рии риХри░рижрйА ри╣рйИред

## ри╕рй░рижри░рин

- **[ри╕рйИриВрикри▓ 04: риЪрйЗриири▓ри┐риЯ риРрикри▓рйАриХрйЗри╕ри╝рии](samples/04/README.md)**: рижри╕ридри╛ри╡рйЗриЬри╝рйАриХри░рии риири╛ри▓ рикрйВри░рйА риРрикри▓рйАриХрйЗри╕ри╝рии
- **[риЪрйЗриири▓ри┐риЯ ри╕ри┐рй▒риЦриг риирйЛриЯримрйБрй▒риХ](samples/04/chainlit_app.ipynb)**: риЗрй░риЯри░риРриХриЯри┐ри╡ ри╕ри┐рй▒риЦриг ри╕риорй▒риЧри░рйА
- **[Foundry Local рижри╕ридри╛ри╡рйЗриЬри╝](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: рикрйВри░рйА рикри▓рйЗриЯрилри╛ри░рио рижри╕ридри╛ри╡рйЗриЬри╝рйАриХри░рии
- **[риЪрйЗриири▓ри┐риЯ рижри╕ридри╛ри╡рйЗриЬри╝](https://docs.chainlit.io/)**: риЕризри┐риХри╛ри░риХ рилри░рйЗриори╡ри░риХ рижри╕ридри╛ри╡рйЗриЬри╝
- **[Open WebUI риЗрй░риЯрйАриЧрйНри░рйЗри╕ри╝рии риЧри╛риИриб](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: риЕризри┐риХри╛ри░риХ риЯри┐риКриЯрйЛри░ри┐риЕри▓

---

