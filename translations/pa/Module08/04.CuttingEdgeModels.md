<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T18:31:54+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "pa"
}
-->
# ਸੈਸ਼ਨ 4: ਅਗਲੇ ਪੱਧਰ ਦੇ ਮਾਡਲ – LLMs, SLMs, ਅਤੇ ਡਿਵਾਈਸ 'ਤੇ ਇੰਫਰੈਂਸ

## ਝਲਕ

LLMs ਅਤੇ SLMs ਦੀ ਤੁਲਨਾ ਕਰੋ, ਸਥਾਨਕ ਅਤੇ ਕਲਾਉਡ ਇੰਫਰੈਂਸ ਦੇ ਫਾਇਦੇ-ਨੁਕਸਾਨ ਦਾ ਮੁਲਾਂਕਨ ਕਰੋ, ਅਤੇ EdgeAI ਸਥਿਤੀਆਂ ਨੂੰ ਦਰਸਾਉਣ ਵਾਲੇ ਡੈਮੋਜ਼ ਨੂੰ Phi ਅਤੇ ONNX Runtime ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਲਾਗੂ ਕਰੋ। ਅਸੀਂ Chainlit RAG, WebGPU ਇੰਫਰੈਂਸ ਵਿਕਲਪਾਂ, ਅਤੇ Open WebUI ਇੰਟਿਗ੍ਰੇਸ਼ਨ ਨੂੰ ਵੀ ਹਾਈਲਾਈਟ ਕਰਾਂਗੇ।

ਹਵਾਲੇ:
- Foundry Local ਡੌਕਸ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI ਹਾਊ-ਟੂ (ਚੈਟ ਐਪ ਨਾਲ Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼
- LLM ਅਤੇ SLM ਦੇ ਖਰਚ, ਲੈਟੈਂਸੀ, ਅਤੇ ਸਹੀਤਾ ਦੇ ਫਾਇਦੇ-ਨੁਕਸਾਨ ਨੂੰ ਸਮਝੋ
- ਵਿਸ਼ੇਸ਼ ਕਾਰੋਬਾਰੀ ਜ਼ਰੂਰਤਾਂ ਲਈ ਸਥਾਨਕ ਅਤੇ ਕਲਾਉਡ ਇੰਫਰੈਂਸ ਵਿੱਚ ਚੋਣ ਕਰੋ
- Chainlit ਨਾਲ ਇੱਕ ਛੋਟਾ RAG ਡੈਮੋ ਲਾਗੂ ਕਰੋ
- ਬ੍ਰਾਊਜ਼ਰ-ਸਾਈਡ ਤੇਜ਼ੀ ਲਈ WebGPU ਦੀ ਖੋਜ ਕਰੋ
- Foundry Local ਨਾਲ Open WebUI ਨੂੰ ਕਨੈਕਟ ਕਰੋ

## ਭਾਗ 1: LLM vs SLM – ਫੈਸਲਾ ਮੈਟ੍ਰਿਕਸ

ਵਿਚਾਰ ਕਰੋ:
- ਲੈਟੈਂਸੀ: SLMs ਡਿਵਾਈਸ 'ਤੇ ਅਕਸਰ ਸਬ-ਸੈਕੰਡ ਜਵਾਬ ਦਿੰਦੇ ਹਨ
- ਖਰਚ: ਸਥਾਨਕ ਇੰਫਰੈਂਸ ਕਲਾਉਡ ਖਰਚ ਨੂੰ ਘਟਾਉਂਦਾ ਹੈ
- ਗੋਪਨੀਯਤਾ: ਸੰਵੇਦਨਸ਼ੀਲ ਡਾਟਾ ਡਿਵਾਈਸ 'ਤੇ ਹੀ ਰਹਿੰਦਾ ਹੈ
- ਸਮਰੱਥਾ: LLMs ਸੰਕਲਪਤ ਕਾਰਜਾਂ ਵਿੱਚ SLMs ਤੋਂ ਬਿਹਤਰ ਹੋ ਸਕਦੇ ਹਨ
- ਭਰੋਸੇਯੋਗਤਾ: ਹਾਈਬ੍ਰਿਡ ਰਣਨੀਤੀਆਂ ਡਾਊਨਟਾਈਮ ਦੇ ਖਤਰੇ ਨੂੰ ਘਟਾਉਂਦੀਆਂ ਹਨ

## ਭਾਗ 2: ਸਥਾਨਕ vs ਕਲਾਉਡ – ਹਾਈਬ੍ਰਿਡ ਪੈਟਰਨ

- ਵੱਡੇ/ਜਟਿਲ ਪ੍ਰੋਮਪਟ ਲਈ ਕਲਾਉਡ ਫਾਲਬੈਕ ਨਾਲ ਸਥਾਨਕ-ਪਹਿਲਾਂ
- ਗੋਪਨੀਯਤਾ-ਸੰਵੇਦਨਸ਼ੀਲ ਜਾਂ ਆਫਲਾਈਨ ਸਥਿਤੀਆਂ ਲਈ ਸਥਾਨਕ ਨਾਲ ਕਲਾਉਡ-ਪਹਿਲਾਂ
- ਟਾਸਕ ਦੀ ਕਿਸਮ ਦੁਆਰਾ ਰੂਟ ਕਰੋ (ਕੋਡ-ਜਨਰੇਸ਼ਨ ਲਈ DeepSeek, ਜਨਰਲ ਚੈਟ ਲਈ Phi/Qwen)

## ਭਾਗ 3: RAG ਚੈਟ ਐਪ Chainlit ਨਾਲ (ਨਿਊਨਤਮ)

ਡਿਪੈਂਡੈਂਸੀਜ਼ ਇੰਸਟਾਲ ਕਰੋ:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

ਚਲਾਓ:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

ਵਧਾਓ: ਇੱਕ ਸਧਾਰਨ ਰੀਟਰੀਵਰ (ਸਥਾਨਕ ਫਾਈਲਾਂ) ਸ਼ਾਮਲ ਕਰੋ ਅਤੇ ਯੂਜ਼ਰ ਪ੍ਰੋਮਪਟ ਵਿੱਚ ਰੀਟਰੀਵ ਕੀਤੇ ਸੰਦਰਭ ਨੂੰ ਸ਼ੁਰੂ ਵਿੱਚ ਜੋੜੋ।

## ਭਾਗ 4: WebGPU ਇੰਫਰੈਂਸ (Heads-up)

WebGPU ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਛੋਟੇ ਮਾਡਲਾਂ ਨੂੰ ਸਿੱਧੇ ਬ੍ਰਾਊਜ਼ਰ ਵਿੱਚ ਚਲਾਓ। ਇਹ ਗੋਪਨੀਯਤਾ-ਪਹਿਲਾਂ ਡੈਮੋਜ਼ ਅਤੇ ਜ਼ੀਰੋ-ਇੰਸਟਾਲ ਅਨੁਭਵਾਂ ਲਈ ਆਦਰਸ਼ ਹੈ। ਹੇਠਾਂ ONNX Runtime Web ਨਾਲ WebGPU ਐਗਜ਼ਿਕਿਊਸ਼ਨ ਪ੍ਰੋਵਾਈਡਰ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਨਿਊਨਤਮ, ਕਦਮ-ਦਰ-ਕਦਮ ਉਦਾਹਰਨ ਦਿੱਤੀ ਗਈ ਹੈ।

1) WebGPU ਸਹਾਇਤਾ ਦੀ ਜਾਂਚ ਕਰੋ
- Chromium ਬ੍ਰਾਊਜ਼ਰ: chrome://gpu → “WebGPU” ਸਹਾਇਤਾ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ
- ਪ੍ਰੋਗਰਾਮੈਟਿਕ ਜਾਂਚ (ਅਸੀਂ ਕੋਡ ਵਿੱਚ ਵੀ ਜਾਂਚ ਕਰਾਂਗੇ): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) ਇੱਕ ਨਿਊਨਤਮ ਪ੍ਰੋਜੈਕਟ ਬਣਾਓ
ਇੱਕ ਫੋਲਡਰ ਅਤੇ ਦੋ ਫਾਈਲਾਂ ਬਣਾਓ: `index.html` ਅਤੇ `main.js`।

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) ਸਥਾਨਕ ਸਰਵ ਕਰੋ (Windows cmd.exe)
ਇੱਕ ਸਧਾਰਨ ਸਟੈਟਿਕ ਸਰਵਰ ਦੀ ਵਰਤੋਂ ਕਰੋ ਤਾਂ ਜੋ ਬ੍ਰਾਊਜ਼ਰ ਮਾਡਲ ਨੂੰ ਫੈਚ ਕਰ ਸਕੇ।

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

http://localhost:5173 ਨੂੰ ਆਪਣੇ ਬ੍ਰਾਊਜ਼ਰ ਵਿੱਚ ਖੋਲ੍ਹੋ। ਤੁਹਾਨੂੰ ਸ਼ੁਰੂਆਤੀ ਲੌਗ, WebGPU ਨਾਲ ਸੈਸ਼ਨ ਬਣਾਉਣ, ਅਤੇ argmax ਪ੍ਰਡਿਕਸ਼ਨ ਦੇਖਣਾ ਚਾਹੀਦਾ ਹੈ।

4) ਟਰਬਲਸ਼ੂਟਿੰਗ
- ਜੇ WebGPU ਉਪਲਬਧ ਨਹੀਂ ਹੈ: Chrome/Edge ਨੂੰ ਅਪਡੇਟ ਕਰੋ ਅਤੇ ਯਕੀਨੀ ਬਣਾਓ ਕਿ GPU ਡਰਾਈਵਰ ਮੌਜੂਦ ਹਨ, ਫਿਰ chrome://flags ਵਿੱਚ “Enable WebGPU” ਦੀ ਜਾਂਚ ਕਰੋ।
- ਜੇ CORS ਜਾਂ ਫੈਚ ਐਰਰ ਆਉਂਦੇ ਹਨ: ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਸੀਂ ਫਾਈਲਾਂ http:// (file:// ਨਹੀਂ) 'ਤੇ ਸਰਵ ਕਰਦੇ ਹੋ ਅਤੇ ਮਾਡਲ URL ਕ੍ਰਾਸ-ਓਰਿਜਨ ਰੀਕਵੈਸਟਾਂ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ।
- CPU 'ਤੇ ਫਾਲਬੈਕ: `executionProviders: ['wasm']` ਨੂੰ ਬਦਲੋ ਤਾਂ ਜੋ ਬੇਸਲਾਈਨ ਵਿਹਾਰ ਦੀ ਪੁਸ਼ਟੀ ਕੀਤੀ ਜਾ ਸਕੇ।

5) ਅਗਲੇ ਕਦਮ
- ਡੋਮੇਨ-ਸਪੈਸਿਫਿਕ ONNX ਮਾਡਲ (ਜਿਵੇਂ ਕਿ ਚਿੱਤਰ ਵਰਗੀਕਰਨ ਜਾਂ ਇੱਕ ਛੋਟਾ ਟੈਕਸਟ ਮਾਡਲ) ਸ਼ਾਮਲ ਕਰੋ।
- ਅਸਲ ਇਨਪੁਟਾਂ ਲਈ ਪ੍ਰੀ-ਪ੍ਰੋਸੈਸਿੰਗ/ਪੋਸਟ-ਪ੍ਰੋਸੈਸਿੰਗ ਲੌਜਿਕ ਸ਼ਾਮਲ ਕਰੋ।
- ਵੱਡੇ ਮਾਡਲਾਂ ਜਾਂ ਪ੍ਰੋਡਕਸ਼ਨ ਲੈਟੈਂਸੀ ਲਈ, Foundry Local ਜਾਂ ONNX Runtime Server ਨੂੰ ਤਰਜੀਹ ਦਿਓ।

## ਭਾਗ 5: Open WebUI + Foundry Local (ਕਦਮ-ਦਰ-ਕਦਮ)

ਇਹ Open WebUI ਨੂੰ Foundry Local ਦੇ OpenAI-ਅਨੁਕੂਲ ਐਂਡਪੌਇੰਟ ਨਾਲ ਕਨੈਕਟ ਕਰਦਾ ਹੈ ਇੱਕ ਸਥਾਨਕ ਚੈਟ UI ਲਈ।

1) ਪੂਰਵ ਸ਼ਰਤਾਂ
- Foundry Local ਇੰਸਟਾਲ ਅਤੇ ਚਲ ਰਹੀ (`foundry --version`)
- ਇੱਕ ਮਾਡਲ ਸਥਾਨਕ ਤੌਰ 'ਤੇ ਚਲਾਉਣ ਲਈ ਤਿਆਰ (ਜਿਵੇਂ ਕਿ `phi-4-mini`)
- Docker Desktop ਇੰਸਟਾਲ (Open WebUI ਲਈ ਸਿਫਾਰਸ਼ੀ)

2) Foundry Local ਨਾਲ ਇੱਕ ਮਾਡਲ ਸ਼ੁਰੂ ਕਰੋ
```powershell
foundry model run phi-4-mini
```
ਇਹ `http://localhost:8000` 'ਤੇ OpenAI-ਅਨੁਕੂਲ API ਨੂੰ ਉਪਲਬਧ ਕਰਦਾ ਹੈ।

3) Open WebUI ਸ਼ੁਰੂ ਕਰੋ (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
ਨੋਟਸ:
- Windows 'ਤੇ, `host.docker.internal` ਕੰਟੇਨਰ ਨੂੰ ਤੁਹਾਡੇ ਹੋਸਟ `localhost` 'ਤੇ ਪਹੁੰਚਣ ਦਿੰਦਾ ਹੈ।
- ਅਸੀਂ `OPENAI_API_BASE_URL` ਨੂੰ Foundry Local ਦੇ ਐਂਡਪੌਇੰਟ ਅਤੇ ਇੱਕ ਡਮੀ `OPENAI_API_KEY` 'ਤੇ ਸੈਟ ਕੀਤਾ।

4) Open WebUI UI ਤੋਂ ਸੰਰਚਨਾ ਕਰੋ (ਵਿਕਲਪਕ)
- http://localhost:3000 'ਤੇ ਜਾਓ
- ਸ਼ੁਰੂਆਤੀ ਸੈਟਅੱਪ ਪੂਰਾ ਕਰੋ (ਐਡਮਿਨ ਯੂਜ਼ਰ)
- Settings → Models/Providers 'ਤੇ ਜਾਓ
- Base URL ਸੈਟ ਕਰੋ: `http://host.docker.internal:8000/v1`
- API Key ਸੈਟ ਕਰੋ: `local-key` (placeholder)
- ਸੇਵ ਕਰੋ

5) ਇੱਕ ਟੈਸਟ ਪ੍ਰੋਮਪਟ ਚਲਾਓ
- Open WebUI ਚੈਟ ਵਿੱਚ, ਮਾਡਲ ਦਾ ਨਾਮ `phi-4-mini` ਚੁਣੋ ਜਾਂ ਦਰਜ ਕਰੋ
- ਪ੍ਰੋਮਪਟ: “List five benefits of on-device AI inference.”
- ਤੁਹਾਨੂੰ ਆਪਣੇ ਸਥਾਨਕ ਮਾਡਲ ਤੋਂ ਸਟ੍ਰੀਮ ਕੀਤੀ ਗਈ ਜਵਾਬ ਦੇਖਣੀ ਚਾਹੀਦੀ ਹੈ

6) ਟਰਬਲਸ਼ੂਟਿੰਗ
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) ਵਿਕਲਪਕ: Open WebUI ਡਾਟਾ ਨੂੰ ਸਥਿਰ ਕਰੋ
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## ਹੈਂਡਸ-ਆਨ ਚੈੱਕਲਿਸਟ
- [ ] SLM ਅਤੇ LLM ਦੇ ਸਥਾਨਕ ਜਵਾਬ/ਲੈਟੈਂਸੀ ਦੀ ਤੁਲਨਾ ਕਰੋ
- [ ] ਘੱਟੋ-ਘੱਟ ਦੋ ਮਾਡਲਾਂ ਦੇ ਖਿਲਾਫ Chainlit ਡੈਮੋ ਚਲਾਓ
- [ ] Open WebUI ਨੂੰ ਆਪਣੇ ਸਥਾਨਕ ਐਂਡਪੌਇੰਟ ਨਾਲ ਕਨੈਕਟ ਕਰੋ ਅਤੇ ਟੈਸਟ ਕਰੋ

## ਅਗਲੇ ਕਦਮ
- ਸੈਸ਼ਨ 5 ਵਿੱਚ ਏਜੰਟ ਵਰਕਫਲੋਜ਼ ਲਈ ਤਿਆਰੀ ਕਰੋ
- ਉਹ ਸਥਿਤੀਆਂ ਪਛਾਣੋ ਜਿੱਥੇ ਹਾਈਬ੍ਰਿਡ ਸਥਾਨਕ/ਕਲਾਉਡ ROI ਨੂੰ ਬਿਹਤਰ ਕਰਦਾ ਹੈ

---

