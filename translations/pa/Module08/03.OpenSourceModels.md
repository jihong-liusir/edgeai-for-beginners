<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T18:28:15+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "pa"
}
-->
# ਸੈਸ਼ਨ 3: ਫਾਊਂਡਰੀ ਲੋਕਲ ਨਾਲ ਓਪਨ-ਸੋਰਸ ਮਾਡਲ

## ਝਲਕ

ਇਸ ਸੈਸ਼ਨ ਵਿੱਚ ਇਹ ਸਿੱਖਿਆ ਜਾਵੇਗਾ ਕਿ ਫਾਊਂਡਰੀ ਲੋਕਲ ਵਿੱਚ ਓਪਨ-ਸੋਰਸ ਮਾਡਲ ਕਿਵੇਂ ਲਿਆਂਦੇ ਜਾਣ: ਕਮਿਊਨਿਟੀ ਮਾਡਲ ਚੁਣਨਾ, Hugging Face ਸਮੱਗਰੀ ਨੂੰ ਇੰਟੀਗਰੇਟ ਕਰਨਾ, ਅਤੇ "ਆਪਣਾ ਮਾਡਲ ਲਿਆਓ" (BYOM) ਰਣਨੀਤੀਆਂ ਅਪਣਾਉਣਾ। ਤੁਸੀਂ Model Mondays ਸੀਰੀਜ਼ ਬਾਰੇ ਵੀ ਜਾਣੋਗੇ ਜੋ ਲਗਾਤਾਰ ਸਿੱਖਣ ਅਤੇ ਮਾਡਲ ਖੋਜ ਲਈ ਹੈ।

ਹਵਾਲੇ:
- Foundry Local ਡੌਕਸ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face ਮਾਡਲ ਕੰਪਾਇਲ ਕਰੋ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼
- ਓਪਨ-ਸੋਰਸ ਮਾਡਲਾਂ ਦੀ ਖੋਜ ਅਤੇ ਮੁਲਾਂਕਣ ਕਰੋ ਜੋ ਲੋਕਲ ਇੰਫਰੈਂਸ ਲਈ ਹਨ
- Foundry Local ਵਿੱਚ ਚੁਣੇ ਹੋਏ Hugging Face ਮਾਡਲਾਂ ਨੂੰ ਕੰਪਾਇਲ ਅਤੇ ਚਲਾਓ
- ਸਹੀਤਾ, ਲੈਟੈਂਸੀ, ਅਤੇ ਸਰੋਤਾਂ ਦੀ ਲੋੜ ਲਈ ਮਾਡਲ ਚੋਣ ਰਣਨੀਤੀਆਂ ਲਾਗੂ ਕਰੋ
- ਕੈਸ਼ ਅਤੇ ਵਰਜਨਿੰਗ ਨਾਲ ਮਾਡਲਾਂ ਨੂੰ ਲੋਕਲ ਤੌਰ 'ਤੇ ਪ੍ਰਬੰਧਿਤ ਕਰੋ

## ਭਾਗ 1: ਮਾਡਲ ਖੋਜ ਅਤੇ ਚੋਣ (ਕਦਮ-ਦਰ-ਕਦਮ)

ਕਦਮ 1) ਲੋਕਲ ਕੈਟਾਲੌਗ ਵਿੱਚ ਉਪਲਬਧ ਮਾਡਲਾਂ ਦੀ ਸੂਚੀ ਬਣਾਓ  
```cmd
foundry model list
```
  
ਕਦਮ 2) ਦੋ ਉਮੀਦਵਾਰਾਂ ਦੀ ਤੇਜ਼ੀ ਨਾਲ ਜਾਂਚ ਕਰੋ (ਪਹਿਲੀ ਵਾਰ ਚਲਾਉਣ 'ਤੇ ਆਟੋ-ਡਾਊਨਲੋਡ)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
ਕਦਮ 3) ਮੁੱਖ ਮਾਪਦੰਡਾਂ ਨੂੰ ਨੋਟ ਕਰੋ  
- ਨਿਰਧਾਰਿਤ ਪ੍ਰੋੰਪਟ ਲਈ ਲੈਟੈਂਸੀ (ਵਿਸ਼ੇਸ਼) ਅਤੇ ਗੁਣਵੱਤਾ ਦਾ ਅਵਲੋਕਨ ਕਰੋ  
- ਹਰ ਮਾਡਲ ਚਲਾਉਣ ਦੌਰਾਨ ਟਾਸਕ ਮੈਨੇਜਰ ਰਾਹੀਂ ਮੈਮੋਰੀ ਦੀ ਵਰਤੋਂ ਦੇਖੋ  

## ਭਾਗ 2: ਕੈਟਾਲੌਗ ਮਾਡਲ CLI ਰਾਹੀਂ ਚਲਾਉਣਾ (ਕਦਮ-ਦਰ-ਕਦਮ)

ਕਦਮ 1) ਇੱਕ ਮਾਡਲ ਸ਼ੁਰੂ ਕਰੋ  
```cmd
foundry model run llama-3.2
```
  
ਕਦਮ 2) OpenAI-ਅਨੁਕੂਲ ਐਂਡਪੌਇੰਟ ਰਾਹੀਂ ਇੱਕ ਟੈਸਟ ਪ੍ਰੋੰਪਟ ਭੇਜੋ  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## ਭਾਗ 3: BYOM – Hugging Face ਮਾਡਲ ਕੰਪਾਇਲ ਕਰੋ (ਕਦਮ-ਦਰ-ਕਦਮ)

ਮਾਡਲਾਂ ਨੂੰ ਕੰਪਾਇਲ ਕਰਨ ਲਈ ਅਧਿਕਾਰਤ ਹਦਾਇਤਾਂ ਦੀ ਪਾਲਣਾ ਕਰੋ। ਹੇਠਾਂ ਉੱਚ-ਸਤਹ ਦਾ ਪ੍ਰਵਾਹ—ਸਹੀ ਕਮਾਂਡਾਂ ਅਤੇ ਸਮਰਥਿਤ ਸੰਰਚਨਾਵਾਂ ਲਈ Microsoft Learn ਲੇਖ ਵੇਖੋ।

ਕਦਮ 1) ਇੱਕ ਵਰਕਿੰਗ ਡਾਇਰੈਕਟਰੀ ਤਿਆਰ ਕਰੋ  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
ਕਦਮ 2) ਇੱਕ ਸਮਰਥਿਤ HF ਮਾਡਲ ਕੰਪਾਇਲ ਕਰੋ  
- Learn ਡੌਕ ਦੇ ਕਦਮਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ONNX ਮਾਡਲ ਨੂੰ ਕਨਵਰਟ ਕਰੋ ਅਤੇ ਕੰਪਾਇਲ ਕੀਤੇ ਮਾਡਲ ਨੂੰ ਆਪਣੇ `models` ਡਾਇਰੈਕਟਰੀ ਵਿੱਚ ਰੱਖੋ  
- ਪੁਸ਼ਟੀ ਕਰੋ:  
```cmd
foundry cache ls
```
  
ਤੁਹਾਨੂੰ ਆਪਣੇ ਕੰਪਾਇਲ ਕੀਤੇ ਮਾਡਲ ਦਾ ਨਾਮ ਦੇਖਣਾ ਚਾਹੀਦਾ ਹੈ (ਉਦਾਹਰਨ ਲਈ, `llama-3.2`)।  

ਕਦਮ 3) ਕੰਪਾਇਲ ਕੀਤੇ ਮਾਡਲ ਨੂੰ ਚਲਾਓ  
```cmd
foundry model run llama-3.2 --verbose
```
  
ਨੋਟਸ:  
- ਕੰਪਾਇਲ ਅਤੇ ਚਲਾਉਣ ਲਈ ਯਥਾਯੋਗ ਡਿਸਕ ਅਤੇ RAM ਯਕੀਨੀ ਬਣਾਓ  
- ਪ੍ਰਵਾਹ ਦੀ ਪੁਸ਼ਟੀ ਕਰਨ ਲਈ ਛੋਟੇ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ, ਫਿਰ ਵਧਾਓ  

## ਭਾਗ 4: ਪ੍ਰਯੋਗਿਕ ਮਾਡਲ ਕੁਰੇਸ਼ਨ (ਕਦਮ-ਦਰ-ਕਦਮ)

ਕਦਮ 1) ਇੱਕ `models.json` ਰਜਿਸਟਰੀ ਬਣਾਓ  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
ਕਦਮ 2) ਛੋਟਾ ਚੋਣ ਸਕ੍ਰਿਪਟ  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## ਭਾਗ 5: ਹੈਂਡਸ-ਆਨ ਬੈਂਚਮਾਰਕ (ਕਦਮ-ਦਰ-ਕਦਮ)

ਕਦਮ 1) ਸਧਾਰਨ ਲੈਟੈਂਸੀ ਬੈਂਚਮਾਰਕ  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
ਕਦਮ 2) ਗੁਣਵੱਤਾ ਦੀ ਸਪੌਟ-ਚੈੱਕ  
- ਨਿਰਧਾਰਿਤ ਪ੍ਰੋੰਪਟ ਸੈੱਟ ਦੀ ਵਰਤੋਂ ਕਰੋ, ਆਉਟਪੁੱਟ ਨੂੰ CSV/JSON ਵਿੱਚ ਕੈਪਚਰ ਕਰੋ  
- ਫਲੂਐਂਸੀ, ਸਬੰਧਿਤਤਾ, ਅਤੇ ਸਹੀਤਾ (1–5) ਨੂੰ ਮੈਨੂਅਲੀ ਰੇਟ ਕਰੋ  

## ਭਾਗ 6: ਅਗਲੇ ਕਦਮ
- ਨਵੇਂ ਮਾਡਲਾਂ ਅਤੇ ਸੁਝਾਅ ਲਈ Model Mondays ਦੀ ਸਬਸਕ੍ਰਿਪਸ਼ਨ ਲਓ: https://aka.ms/model-mondays  
- ਆਪਣੇ ਟੀਮ ਦੇ `models.json` ਵਿੱਚ ਖੋਜਾਂ ਸ਼ਾਮਲ ਕਰੋ  
- ਸੈਸ਼ਨ 4 ਲਈ ਤਿਆਰ ਹੋਵੋ: LLMs ਵਿਰੁੱਧ SLMs, ਲੋਕਲ ਵਿਰੁੱਧ ਕਲਾਉਡ ਇੰਫਰੈਂਸ, ਅਤੇ ਹੈਂਡਸ-ਆਨ ਡੈਮੋ

---

