<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-18T12:34:06+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "he"
}
-->
# סעיף 3: מדריך ליישום מעשי

## סקירה כללית

מדריך מקיף זה יעזור לך להתכונן לקורס EdgeAI, שמתמקד בבניית פתרונות AI מעשיים הפועלים ביעילות על מכשירי קצה. הקורס מדגיש פיתוח מעשי באמצעות מסגרות מודרניות ומודלים מתקדמים המותאמים לפריסה בקצה.

## 1. הגדרת סביבת פיתוח

### שפות תכנות ומסגרות

**סביבת Python**
- **גרסה**: Python 3.10 ומעלה (מומלץ: Python 3.11)
- **מנהל חבילות**: pip או conda
- **סביבה וירטואלית**: השתמש ב-venv או בסביבות conda לבידוד
- **ספריות מרכזיות**: נתקין ספריות EdgeAI ספציפיות במהלך הקורס

**סביבת Microsoft .NET**
- **גרסה**: .NET 8 ומעלה
- **IDE**: Visual Studio 2022, Visual Studio Code או JetBrains Rider
- **SDK**: ודא ש-.NET SDK מותקן לפיתוח חוצה פלטפורמות

### כלי פיתוח

**עורכי קוד ו-IDEs**
- Visual Studio Code (מומלץ לפיתוח חוצה פלטפורמות)
- PyCharm או Visual Studio (לפיתוח ספציפי לשפה)
- Jupyter Notebooks לפיתוח אינטראקטיבי ופרוטוטייפינג

**בקרת גרסאות**
- Git (גרסה עדכנית)
- חשבון GitHub לגישה למאגרי קוד ושיתוף פעולה

## 2. דרישות חומרה והמלצות

### דרישות מערכת מינימליות
- **מעבד**: מעבד רב-ליבתי (Intel i5/AMD Ryzen 5 או מקביל)
- **זיכרון RAM**: מינימום 8GB, מומלץ 16GB
- **אחסון**: 50GB פנויים עבור מודלים וכלי פיתוח
- **מערכת הפעלה**: Windows 10/11, macOS 10.15+ או Linux (Ubuntu 20.04+)

### אסטרטגיית משאבי מחשוב
הקורס נועד להיות נגיש על פני תצורות חומרה שונות:

**פיתוח מקומי (מיקוד CPU/NPU)**
- הפיתוח העיקרי יתבסס על האצת CPU ו-NPU
- מתאים לרוב המחשבים הניידים והמחשבים השולחניים המודרניים
- דגש על יעילות ותסריטי פריסה מעשיים

**משאבי GPU בענן (אופציונלי)**
- **Azure Machine Learning**: לאימון וניסויים אינטנסיביים
- **Google Colab**: שכבה חינמית זמינה למטרות חינוכיות
- **Kaggle Notebooks**: פלטפורמת מחשוב ענן חלופית

### שיקולים למכשירי קצה
- הבנה של מעבדים מבוססי ARM
- ידע במגבלות חומרה של מכשירים ניידים ו-IoT
- היכרות עם אופטימיזציה לצריכת חשמל

## 3. משפחות מודלים מרכזיות ומשאבים

### משפחות מודלים עיקריות

**Microsoft Phi-4 Family**
- **תיאור**: מודלים קומפקטיים ויעילים המיועדים לפריסה בקצה
- **חוזקות**: יחס ביצועים-גודל מצוין, מותאם למשימות הסקת מסקנות
- **משאב**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **שימושים**: יצירת קוד, הסקת מסקנות מתמטיות, שיחה כללית

**Qwen-3 Family**
- **תיאור**: הדור האחרון של מודלים רב-לשוניים מבית Alibaba
- **חוזקות**: יכולות רב-לשוניות חזקות, ארכיטקטורה יעילה
- **משאב**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **שימושים**: יישומים רב-לשוניים, פתרונות AI בין-תרבותיים

**Google Gemma-3n Family**
- **תיאור**: מודלים קלים מבית Google המותאמים לפריסה בקצה
- **חוזקות**: הסקה מהירה, ארכיטקטורה ידידותית לנייד
- **משאב**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **שימושים**: יישומים ניידים, עיבוד בזמן אמת

### קריטריונים לבחירת מודלים
- **איזון ביצועים מול גודל**: הבנה מתי לבחור מודלים קטנים מול גדולים
- **אופטימיזציה למשימות ספציפיות**: התאמת מודלים לשימושים מסוימים
- **מגבלות פריסה**: זיכרון, זמן השהיה ושיקולי צריכת חשמל

## 4. כלי כימות ואופטימיזציה

### Llama.cpp Framework
- **מאגר**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **מטרה**: מנוע הסקה בעל ביצועים גבוהים עבור LLMs
- **תכונות מרכזיות**:
  - הסקה מותאמת ל-CPU
  - פורמטים כימות מרובים (Q4, Q5, Q8)
  - תאימות חוצה פלטפורמות
  - ביצוע חסכוני בזיכרון
- **התקנה ושימוש בסיסי**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **מאגר**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **מטרה**: ערכת כלים לאופטימיזציית מודלים לפריסה בקצה
- **תכונות מרכזיות**:
  - תהליכי אופטימיזציה אוטומטיים למודלים
  - אופטימיזציה מודעת לחומרה
  - אינטגרציה עם ONNX Runtime
  - כלי מדידת ביצועים
- **התקנה ושימוש בסיסי**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # הגדרת מודל וקונפיגורציית אופטימיזציה
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # הפעלת תהליך אופטימיזציה
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # שמירת מודל אופטימלי
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # התקנת MLX
  pip install mlx
  
  # דוגמת סקריפט Python לטעינה ואופטימיזציה של מודל
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **מאגר**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **מטרה**: האצת הסקה חוצה פלטפורמות עבור מודלים ONNX
- **תכונות מרכזיות**:
  - אופטימיזציות ספציפיות לחומרה (CPU, GPU, NPU)
  - אופטימיזציות גרף להסקה
  - תמיכה בכימות
  - תמיכה חוצה שפות (Python, C++, C#, JavaScript)
- **התקנה ושימוש בסיסי**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. קריאה ומשאבים מומלצים

### תיעוד חיוני
- **תיעוד ONNX Runtime**: הבנת הסקה חוצה פלטפורמות
- **מדריך Hugging Face Transformers**: טעינת מודלים והסקה
- **תבניות עיצוב Edge AI**: שיטות עבודה מומלצות לפריסה בקצה

### מאמרים טכניים
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### משאבי קהילה
- **EdgeAI Slack/Discord Communities**: תמיכה ודיונים עם עמיתים
- **מאגרי GitHub**: יישומים ודוגמאות הדרכה
- **ערוצי YouTube**: הדרכות מעמיקות טכניות

## 6. הערכה ואימות

### רשימת בדיקה לפני הקורס
- [ ] Python 3.10+ מותקן ומאומת
- [ ] .NET 8+ מותקן ומאומת
- [ ] סביבת פיתוח מוגדרת
- [ ] חשבון Hugging Face נוצר
- [ ] היכרות בסיסית עם משפחות מודלים יעד
- [ ] כלי כימות מותקנים ונבדקו
- [ ] דרישות חומרה מולאו
- [ ] חשבונות מחשוב בענן הוגדרו (אם נדרש)

## מטרות למידה מרכזיות

בסיום מדריך זה, תוכל:

1. להגדיר סביבת פיתוח מלאה לפיתוח יישומי EdgeAI
2. להתקין ולהגדיר את הכלים והמסגרות הנדרשים לאופטימיזציית מודלים
3. לבחור תצורות חומרה ותוכנה מתאימות לפרויקטי EdgeAI שלך
4. להבין את השיקולים המרכזיים לפריסת מודלים AI על מכשירי קצה
5. להכין את המערכת שלך לתרגילים המעשיים בקורס

## משאבים נוספים

### תיעוד רשמי
- **תיעוד Python**: תיעוד רשמי של שפת Python
- **תיעוד Microsoft .NET**: משאבי פיתוח רשמיים של .NET
- **תיעוד ONNX Runtime**: מדריך מקיף ל-ONNX Runtime
- **תיעוד TensorFlow Lite**: תיעוד רשמי של TensorFlow Lite

### כלי פיתוח
- **Visual Studio Code**: עורך קוד קל משקל עם הרחבות לפיתוח AI
- **Jupyter Notebooks**: סביבת מחשוב אינטראקטיבית לניסויי ML
- **Docker**: פלטפורמת קונטיינרים לסביבות פיתוח עקביות
- **Git**: מערכת בקרת גרסאות לניהול קוד

### משאבי למידה
- **EdgeAI Research Papers**: מחקר אקדמי עדכני על מודלים יעילים
- **קורסים מקוונים**: חומרי לימוד משלימים על אופטימיזציית AI
- **פורומי קהילה**: פלטפורמות שאלות ותשובות לאתגרים בפיתוח EdgeAI
- **מאגרי נתונים לבנצ'מרק**: מאגרי נתונים סטנדרטיים להערכת ביצועי מודלים

## תוצאות למידה

לאחר השלמת מדריך ההכנה, תוכל:

1. להגדיר סביבת פיתוח מלאה המוכנה לפיתוח EdgeAI
2. להבין את דרישות החומרה והתוכנה לתרחישי פריסה שונים
3. להכיר את המסגרות והכלים המרכזיים שישמשו במהלך הקורס
4. לבחור מודלים מתאימים בהתבסס על מגבלות המכשיר ודרישות
5. לרכוש ידע חיוני על טכניקות אופטימיזציה לפריסה בקצה

## ➡️ מה הלאה

- [04: חומרה ופריסה של EdgeAI](04.EdgeDeployment.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.