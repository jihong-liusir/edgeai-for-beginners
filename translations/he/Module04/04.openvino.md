<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T12:45:57+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "he"
}
-->
# סעיף 4: ערכת אופטימיזציה של OpenVINO Toolkit

## תוכן עניינים
1. [מבוא](../../../Module04)
2. [מה זה OpenVINO?](../../../Module04)
3. [התקנה](../../../Module04)
4. [מדריך התחלה מהירה](../../../Module04)
5. [דוגמה: המרה ואופטימיזציה של מודלים עם OpenVINO](../../../Module04)
6. [שימוש מתקדם](../../../Module04)
7. [שיטות עבודה מומלצות](../../../Module04)
8. [פתרון בעיות](../../../Module04)
9. [משאבים נוספים](../../../Module04)

## מבוא

OpenVINO (Open Visual Inference and Neural Network Optimization) הוא ערכת כלים בקוד פתוח של אינטל לפריסת פתרונות AI יעילים בענן, בסביבות מקומיות ובקצה. בין אם אתם מכוונים ל-CPU, GPU, VPU או מאיצי AI ייעודיים, OpenVINO מספק יכולות אופטימיזציה מקיפות תוך שמירה על דיוק המודל ואפשרות לפריסה חוצת פלטפורמות.

## מה זה OpenVINO?

OpenVINO הוא ערכת כלים בקוד פתוח שמאפשרת למפתחים לבצע אופטימיזציה, המרה ופריסה של מודלים AI בצורה יעילה על פני פלטפורמות חומרה מגוונות. הוא מורכב משלושה רכיבים עיקריים: OpenVINO Runtime לביצוע, Neural Network Compression Framework (NNCF) לאופטימיזציה של מודלים, ו-OpenVINO Model Server לפריסה בקנה מידה רחב.

### תכונות עיקריות

- **פריסה חוצת פלטפורמות**: תומך ב-Linux, Windows ו-macOS עם APIs ב-Python, C++ ו-C  
- **האצת חומרה**: גילוי אוטומטי של התקנים ואופטימיזציה ל-CPU, GPU, VPU ומאיצי AI  
- **מסגרת דחיסת מודלים**: טכניקות מתקדמות של כימות, גיזום ואופטימיזציה באמצעות NNCF  
- **תאימות למסגרות**: תמיכה ישירה במודלים של TensorFlow, ONNX, PaddlePaddle ו-PyTorch  
- **תמיכה ב-AI גנרטיבי**: OpenVINO GenAI לפריסת מודלים של שפה גדולה ויישומי AI גנרטיבי  

### יתרונות

- **אופטימיזציה לביצועים**: שיפורי מהירות משמעותיים עם אובדן דיוק מינימלי  
- **טביעת רגל פריסה מופחתת**: תלות חיצונית מינימלית מפשטת התקנה ופריסה  
- **זמן אתחול משופר**: טעינת מודלים אופטימלית ואחסון במטמון לאתחול מהיר של יישומים  
- **פריסה בקנה מידה רחב**: ממכשירי קצה ועד תשתית ענן עם APIs עקביים  
- **מוכן לייצור**: אמינות ברמה ארגונית עם תיעוד מקיף ותמיכה קהילתית  

## התקנה

### דרישות מוקדמות

- Python 3.8 ומעלה  
- מנהל חבילות pip  
- סביבה וירטואלית (מומלץ)  
- חומרה תואמת (מומלץ CPU של אינטל, אך תומך בארכיטקטורות שונות)  

### התקנה בסיסית

צרו והפעילו סביבה וירטואלית:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

התקינו את OpenVINO Runtime:

```bash
pip install openvino
```

התקינו את NNCF לאופטימיזציה של מודלים:

```bash
pip install nncf
```

### התקנת OpenVINO GenAI

ליישומי AI גנרטיבי:

```bash
pip install openvino-genai
```

### תלות אופציונליות

חבילות נוספות לשימושים ספציפיים:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### אימות התקנה

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

אם ההתקנה הצליחה, תראו את פרטי גרסת OpenVINO.

## מדריך התחלה מהירה

### אופטימיזציה ראשונה של מודל

בואו נמיר ונבצע אופטימיזציה למודל של Hugging Face באמצעות OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### מה התהליך הזה עושה

תהליך האופטימיזציה כולל: טעינת המודל המקורי מ-Hugging Face, המרה לפורמט Intermediate Representation (IR) של OpenVINO, יישום אופטימיזציות ברירת מחדל, וקומפילציה לחומרה היעד.

### הסבר על פרמטרים מרכזיים

- `export=True`: ממיר את המודל לפורמט IR של OpenVINO  
- `compile=False`: דוחה את הקומפילציה עד זמן הריצה לגמישות  
- `device`: חומרה יעד ("CPU", "GPU", "AUTO" לבחירה אוטומטית)  
- `save_pretrained()`: שומר את המודל האופטימלי לשימוש חוזר  

## דוגמה: המרה ואופטימיזציה של מודלים עם OpenVINO

### שלב 1: המרת מודל עם כימות NNCF

כך תיישמו כימות לאחר אימון באמצעות NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### שלב 2: אופטימיזציה מתקדמת עם דחיסת משקל

למודלים מבוססי טרנספורמרים, יישמו דחיסת משקל:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### שלב 3: ביצוע עם מודל אופטימלי

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### מבנה הפלט

לאחר האופטימיזציה, תיקיית המודל שלכם תכיל:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## שימוש מתקדם

### תצורה עם NNCF YAML

לזרימות עבודה אופטימיזציה מורכבות, השתמשו בקבצי תצורה של NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

יישום תצורה:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### אופטימיזציה ל-GPU

להאצת GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### אופטימיזציה לעיבוד אצווה

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### פריסת שרת מודלים

פרסו מודלים אופטימליים עם OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

קוד לקוח לשרת מודלים:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## שיטות עבודה מומלצות

### 1. בחירת מודל והכנה
- השתמשו במודלים ממסגרות נתמכות (PyTorch, TensorFlow, ONNX)  
- ודאו שלמודל יש צורות קלט קבועות או דינמיות ידועות  
- בדקו עם מערכי נתונים מייצגים לכיול  

### 2. בחירת אסטרטגיית אופטימיזציה
- **כימות לאחר אימון**: התחילו כאן לאופטימיזציה מהירה  
- **דחיסת משקל**: אידיאלי למודלים של שפה גדולה וטרנספורמרים  
- **אימון מודע לכימות**: השתמשו כשדיוק קריטי  

### 3. אופטימיזציה ספציפית לחומרה
- **CPU**: השתמשו בכימות INT8 לאיזון ביצועים  
- **GPU**: נצלו דיוק FP16 ועיבוד אצווה  
- **VPU**: התמקדו בפישוט מודלים ומיזוג שכבות  

### 4. כוונון ביצועים
- **מצב תפוקה**: לעיבוד אצווה בנפח גבוה  
- **מצב השהיה**: ליישומים אינטראקטיביים בזמן אמת  
- **AUTO Device**: תנו ל-OpenVINO לבחור את החומרה האופטימלית  

### 5. ניהול זיכרון
- השתמשו בצורות דינמיות בזהירות כדי להימנע מעומס זיכרון  
- יישמו אחסון במטמון של מודלים לטעינות מהירות יותר  
- עקבו אחר שימוש בזיכרון במהלך האופטימיזציה  

### 6. אימות דיוק
- תמיד אימתו מודלים אופטימליים מול ביצועים מקוריים  
- השתמשו במערכי בדיקה מייצגים להערכה  
- שקלו אופטימיזציה הדרגתית (התחילו עם הגדרות שמרניות)  

## פתרון בעיות

### בעיות נפוצות

#### 1. בעיות התקנה
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. שגיאות המרת מודל
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. בעיות ביצועים
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. בעיות זיכרון
- הפחיתו את גודל האצווה של המודל במהלך האופטימיזציה  
- השתמשו בזרימה למערכי נתונים גדולים  
- הפעילו אחסון במטמון של מודלים: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`  

#### 5. ירידת דיוק
- השתמשו בדיוק גבוה יותר (INT8 במקום INT4)  
- הגדילו את גודל מערך הנתונים לכיול  
- יישמו אופטימיזציה של דיוק מעורב  

### ניטור ביצועים

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### קבלת עזרה

- **תיעוד**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)  
- **בעיות GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)  
- **פורום קהילתי**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)  

## משאבים נוספים

### קישורים רשמיים
- **דף הבית של OpenVINO**: [openvino.ai](https://openvino.ai/)  
- **מאגר GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)  
- **מאגר NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)  
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)  

### משאבי למידה
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)  
- **מדריך התחלה מהירה**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)  
- **מדריך אופטימיזציה**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)  

### כלי אינטגרציה
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)  
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)  
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)  

### מדדי ביצועים
- **מדדים רשמיים**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)  
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)  

### דוגמאות קהילתיות
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - מדריכים מקיפים זמינים במאגר OpenVINO Notebooks  
- **יישומים לדוגמה**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - דוגמאות מעשיות לתחומים שונים (ראייה ממוחשבת, NLP, אודיו)  
- **פוסטים בבלוג**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - פוסטים בבלוג של אינטל AI והקהילה עם מקרי שימוש מפורטים  

### כלים קשורים
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - טכניקות אופטימיזציה נוספות לחומרה של אינטל  
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - להשוואות פריסה במובייל ובקצה  
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - חלופות למנוע ביצוע חוצה פלטפורמות  

## ➡️ מה הלאה

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)  

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.