<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T21:37:44+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "he"
}
-->
# סעיף 3: Microsoft Olive Optimization Suite

## תוכן עניינים
1. [מבוא](../../../Module04)
2. [מה זה Microsoft Olive?](../../../Module04)
3. [התקנה](../../../Module04)
4. [מדריך התחלה מהירה](../../../Module04)
5. [דוגמה: המרת Qwen3 ל-ONNX INT4](../../../Module04)
6. [שימוש מתקדם](../../../Module04)
7. [שיטות עבודה מומלצות](../../../Module04)
8. [פתרון בעיות](../../../Module04)
9. [משאבים נוספים](../../../Module04)

## מבוא

Microsoft Olive הוא כלי עוצמתי וקל לשימוש לאופטימיזציה של מודלים, המותאם לחומרה, שמפשט את תהליך האופטימיזציה של מודלים ללמידת מכונה עבור פריסה על פני פלטפורמות חומרה שונות. בין אם אתם מכוונים ל-CPU, GPU או מאיצי AI ייעודיים, Olive עוזר לכם להשיג ביצועים מיטביים תוך שמירה על דיוק המודל.

## מה זה Microsoft Olive?

Olive הוא כלי אופטימיזציה למודלים, מותאם לחומרה, שמשלב טכניקות מובילות בתעשייה בתחומי דחיסת מודלים, אופטימיזציה וקומפילציה. הוא עובד עם ONNX Runtime כפתרון אופטימיזציה מקצה לקצה עבור ביצועי מודלים.

### תכונות עיקריות

- **אופטימיזציה מותאמת לחומרה**: בוחר באופן אוטומטי את טכניקות האופטימיזציה הטובות ביותר עבור החומרה שלכם
- **40+ רכיבי אופטימיזציה מובנים**: כולל דחיסת מודלים, כימות, אופטימיזציית גרפים ועוד
- **ממשק CLI קל לשימוש**: פקודות פשוטות למשימות אופטימיזציה נפוצות
- **תמיכה בריבוי מסגרות עבודה**: עובד עם PyTorch, מודלים של Hugging Face ו-ONNX
- **תמיכה במודלים פופולריים**: Olive יכול לבצע אופטימיזציה אוטומטית למבני מודלים פופולריים כמו Llama, Phi, Qwen, Gemma ועוד

### יתרונות

- **חיסכון בזמן פיתוח**: אין צורך להתנסות ידנית בטכניקות אופטימיזציה שונות
- **שיפורי ביצועים**: שיפורי מהירות משמעותיים (עד פי 6 במקרים מסוימים)
- **פריסה חוצת פלטפורמות**: מודלים מותאמים עובדים על פני חומרה ומערכות הפעלה שונות
- **שמירה על דיוק**: האופטימיזציות שומרות על איכות המודל תוך שיפור ביצועים

## התקנה

### דרישות מוקדמות

- Python 3.8 ומעלה
- מנהל חבילות pip
- סביבה וירטואלית (מומלץ)

### התקנה בסיסית

צרו והפעילו סביבה וירטואלית:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
התקינו את Olive עם תכונות אופטימיזציה אוטומטיות:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  
### תלות אופציונלית

Olive מציע תלות אופציונלית עבור תכונות נוספות:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  
### אימות התקנה

```bash
olive --help
```
  
אם ההתקנה הצליחה, אתם אמורים לראות את הודעת העזרה של Olive CLI.

## מדריך התחלה מהירה

### האופטימיזציה הראשונה שלכם

בואו נבצע אופטימיזציה למודל שפה קטן באמצעות תכונת האופטימיזציה האוטומטית של Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### מה הפקודה הזו עושה

תהליך האופטימיזציה כולל: הורדת המודל מהמטמון המקומי, לכידת גרף ONNX ושמירת המשקלים בקובץ נתונים של ONNX, אופטימיזציית גרף ONNX וכימות המודל ל-int4 באמצעות שיטת RTN.

### הסבר על פרמטרי הפקודה

- `--model_name_or_path`: מזהה מודל של Hugging Face או נתיב מקומי
- `--output_path`: תיקייה שבה יישמר המודל המותאם
- `--device`: חומרה יעד (cpu, gpu)
- `--provider`: ספק ביצוע (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: שימוש ב-ONNX Runtime Generate AI לביצוע
- `--precision`: דיוק כימות (int4, int8, fp16)
- `--log_level`: רמת פירוט ביומן (0=מינימלי, 1=מפורט)

## דוגמה: המרת Qwen3 ל-ONNX INT4

בהתבסס על הדוגמה שסופקה ב-Hugging Face בכתובת [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), כך ניתן לבצע אופטימיזציה למודל Qwen3:

### שלב 1: הורדת מודל (אופציונלי)

כדי לצמצם את זמן ההורדה, שמרו רק את הקבצים החיוניים:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  
### שלב 2: אופטימיזציה למודל Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### שלב 3: בדיקת המודל המותאם

צרו סקריפט Python פשוט לבדיקת המודל המותאם שלכם:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  
### מבנה הפלט

לאחר האופטימיזציה, תיקיית הפלט שלכם תכיל:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## שימוש מתקדם

### קבצי תצורה

עבור תהליכי אופטימיזציה מורכבים יותר, ניתן להשתמש בקבצי תצורה JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
הרצה עם תצורה:

```bash
olive run --config config.json
```
  
### אופטימיזציה ל-GPU

עבור אופטימיזציה ל-CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
עבור DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### כיוונון עדין עם Olive

Olive תומך גם בכיוונון עדין למודלים:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## שיטות עבודה מומלצות

### 1. בחירת מודל
- התחילו עם מודלים קטנים לבדיקות (למשל, 0.5B-7B פרמטרים)
- ודאו שמבנה המודל שלכם נתמך על ידי Olive

### 2. שיקולי חומרה
- התאימו את יעד האופטימיזציה לחומרה שבה תפרסו את המודל
- השתמשו באופטימיזציה ל-GPU אם יש לכם חומרה תואמת CUDA
- שקלו DirectML עבור מחשבי Windows עם גרפיקה משולבת

### 3. בחירת דיוק
- **INT4**: דחיסה מקסימלית, איבוד דיוק קל
- **INT8**: איזון טוב בין גודל לדיוק
- **FP16**: איבוד דיוק מינימלי, הפחתת גודל מתונה

### 4. בדיקות ואימות
- תמיד בדקו מודלים מותאמים עם מקרי השימוש הספציפיים שלכם
- השוו מדדי ביצועים (זמן תגובה, תפוקה, דיוק)
- השתמשו בנתוני קלט מייצגים להערכה

### 5. אופטימיזציה איטרטיבית
- התחילו עם אופטימיזציה אוטומטית לתוצאות מהירות
- השתמשו בקבצי תצורה לשליטה מדויקת
- נסו שילובים שונים של שלבי אופטימיזציה

## פתרון בעיות

### בעיות נפוצות

#### 1. בעיות התקנה
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  
#### 2. בעיות CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  
#### 3. בעיות זיכרון
- השתמשו בגודל אצווה קטן יותר במהלך האופטימיזציה
- נסו כימות עם דיוק גבוה יותר תחילה (int8 במקום int4)
- ודאו שיש מספיק מקום בדיסק עבור מטמון המודל

#### 4. שגיאות טעינת מודל
- בדקו את נתיב המודל והרשאות הגישה
- בדקו אם המודל דורש `trust_remote_code=True`
- ודאו שכל קבצי המודל הנדרשים הורדו

### קבלת עזרה

- **תיעוד**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **בעיות ב-GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **דוגמאות**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## משאבים נוספים

### קישורים רשמיים
- **מאגר GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **תיעוד ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **דוגמה של Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### דוגמאות קהילתיות
- **מחברות Jupyter**: זמינות במאגר GitHub של Olive — https://github.com/microsoft/Olive/tree/main/examples
- **תוסף VS Code**: סקירה של AI Toolkit ל-VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **פוסטים בבלוג**: בלוג הקוד הפתוח של Microsoft — https://opensource.microsoft.com/blog/

### כלים קשורים
- **ONNX Runtime**: מנוע ביצועים גבוהים לביצוע — https://onnxruntime.ai/
- **Hugging Face Transformers**: מקור למודלים תואמים רבים — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: תהליכי אופטימיזציה מבוססי ענן — https://learn.microsoft.com/azure/machine-learning/

## ➡️ מה הלאה

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

