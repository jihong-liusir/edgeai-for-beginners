<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T12:51:07+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "he"
}
-->
# סעיף 3: Microsoft Olive Optimization Suite

## תוכן העניינים
1. [מבוא](../../../Module04)
2. [מה זה Microsoft Olive?](../../../Module04)
3. [התקנה](../../../Module04)
4. [מדריך התחלה מהירה](../../../Module04)
5. [דוגמה: המרת Qwen3 ל-ONNX INT4](../../../Module04)
6. [שימוש מתקדם](../../../Module04)
7. [שיטות עבודה מומלצות](../../../Module04)
8. [פתרון בעיות](../../../Module04)
9. [משאבים נוספים](../../../Module04)

## מבוא

Microsoft Olive הוא כלי עוצמתי וקל לשימוש לאופטימיזציה של מודלים, המותאם לחומרה. הכלי מפשט את תהליך האופטימיזציה של מודלים ללמידת מכונה עבור פריסה על גבי פלטפורמות חומרה שונות. בין אם אתם מכוונים ל-CPU, GPU או מאיצי AI ייעודיים, Olive מסייע לכם להשיג ביצועים מיטביים תוך שמירה על דיוק המודל.

## מה זה Microsoft Olive?

Olive הוא כלי אופטימיזציה מותאם לחומרה, המשלב טכניקות מובילות בתעשייה בתחומי דחיסת מודלים, אופטימיזציה וקומפילציה. הכלי עובד עם ONNX Runtime כפתרון מקצה לקצה לאופטימיזציית ביצועים.

### תכונות עיקריות

- **אופטימיזציה מותאמת לחומרה**: בוחר אוטומטית את טכניקות האופטימיזציה הטובות ביותר עבור החומרה הייעודית שלכם
- **מעל 40 רכיבי אופטימיזציה מובנים**: כולל דחיסת מודלים, כימות, אופטימיזציית גרפים ועוד
- **ממשק CLI פשוט**: פקודות קלות לביצוע משימות אופטימיזציה נפוצות
- **תמיכה בריבוי מסגרות עבודה**: עובד עם PyTorch, מודלים של Hugging Face ו-ONNX
- **תמיכה במודלים פופולריים**: Olive יכול לאופטימיזציה אוטומטית של ארכיטקטורות מודלים פופולריות כמו Llama, Phi, Qwen, Gemma ועוד

### יתרונות

- **חיסכון בזמן פיתוח**: אין צורך בניסויים ידניים עם טכניקות אופטימיזציה שונות
- **שיפור ביצועים**: שיפורי מהירות משמעותיים (עד פי 6 במקרים מסוימים)
- **פריסה חוצת פלטפורמות**: מודלים מותאמים עובדים על גבי חומרות ומערכות הפעלה שונות
- **שמירה על דיוק**: האופטימיזציות שומרות על איכות המודל תוך שיפור הביצועים

## התקנה

### דרישות מוקדמות

- Python 3.8 ומעלה
- מנהל חבילות pip
- סביבת עבודה וירטואלית (מומלץ)

### התקנה בסיסית

צרו והפעילו סביבת עבודה וירטואלית:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

התקינו את Olive עם תכונות אופטימיזציה אוטומטיות:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### תלויות אופציונליות

Olive מציע תלויות אופציונליות לתכונות נוספות:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### אימות התקנה

```bash
olive --help
```

אם ההתקנה הצליחה, תראו את הודעת העזרה של Olive CLI.

## מדריך התחלה מהירה

### האופטימיזציה הראשונה שלכם

בואו נאופטימיזציה של מודל שפה קטן באמצעות תכונת האופטימיזציה האוטומטית של Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### מה הפקודה הזו עושה

תהליך האופטימיזציה כולל: הורדת המודל מהמטמון המקומי, לכידת גרף ONNX ושמירת המשקולות בקובץ נתונים של ONNX, אופטימיזציית גרף ONNX וכימות המודל ל-int4 באמצעות שיטת RTN.

### הסבר על פרמטרי הפקודה

- `--model_name_or_path`: מזהה מודל של Hugging Face או נתיב מקומי
- `--output_path`: תיקייה שבה יישמר המודל המותאם
- `--device`: התקן יעד (cpu, gpu)
- `--provider`: ספק ביצוע (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: שימוש ב-ONNX Runtime Generate AI לצורך הסקה
- `--precision`: דיוק כימות (int4, int8, fp16)
- `--log_level`: רמת פירוט ביומן (0=מינימלי, 1=מפורט)

## דוגמה: המרת Qwen3 ל-ONNX INT4

בהתבסס על הדוגמה שסופקה ב-Hugging Face בכתובת [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), כך ניתן לאופטימיזציה של מודל Qwen3:

### שלב 1: הורדת מודל (אופציונלי)

כדי למזער את זמן ההורדה, שמרו במטמון רק קבצים חיוניים:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### שלב 2: אופטימיזציה של מודל Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### שלב 3: בדיקת המודל המותאם

צרו סקריפט Python פשוט לבדיקת המודל המותאם:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### מבנה הפלט

לאחר האופטימיזציה, תיקיית הפלט שלכם תכיל:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## שימוש מתקדם

### קבצי תצורה

לזרימות עבודה מורכבות יותר של אופטימיזציה, ניתן להשתמש בקבצי תצורה בפורמט JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

הריצו עם תצורה:

```bash
olive run --config config.json
```

### אופטימיזציה ל-GPU

לאופטימיזציה ל-CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

ל-DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### כוונון עדין עם Olive

Olive תומך גם בכוונון עדין של מודלים:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## שיטות עבודה מומלצות

### 1. בחירת מודל
- התחילו עם מודלים קטנים לבדיקות (למשל, 0.5B-7B פרמטרים)
- ודאו שארכיטקטורת המודל הנתמכת על ידי Olive

### 2. שיקולי חומרה
- התאימו את יעד האופטימיזציה לחומרת הפריסה שלכם
- השתמשו באופטימיזציה ל-GPU אם יש לכם חומרה תואמת CUDA
- שקלו DirectML למחשבים עם Windows וכרטיסי גרפיקה משולבים

### 3. בחירת דיוק
- **INT4**: דחיסה מקסימלית, אובדן דיוק קל
- **INT8**: איזון טוב בין גודל לדיוק
- **FP16**: אובדן דיוק מינימלי, הפחתת גודל מתונה

### 4. בדיקות ואימות
- תמיד בדקו מודלים מותאמים עם מקרי השימוש הספציפיים שלכם
- השוו מדדי ביצועים (זמן תגובה, תפוקה, דיוק)
- השתמשו בנתוני קלט מייצגים להערכה

### 5. אופטימיזציה איטרטיבית
- התחילו עם אופטימיזציה אוטומטית לתוצאות מהירות
- השתמשו בקבצי תצורה לשליטה מדויקת
- נסו טכניקות אופטימיזציה שונות

## פתרון בעיות

### בעיות נפוצות

#### 1. בעיות התקנה
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. בעיות CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. בעיות זיכרון
- השתמשו בגדלי אצווה קטנים יותר במהלך האופטימיזציה
- נסו כימות עם דיוק גבוה יותר תחילה (int8 במקום int4)
- ודאו שיש מספיק מקום בדיסק למטמון המודל

#### 4. שגיאות בטעינת מודל
- ודאו את נתיב המודל והרשאות הגישה
- בדקו אם המודל דורש `trust_remote_code=True`
- ודאו שכל קבצי המודל הנדרשים הורדו

### קבלת עזרה

- **תיעוד**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **בעיות ב-GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **דוגמאות**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## משאבים נוספים

### קישורים רשמיים
- **מאגר GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **תיעוד ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **דוגמה ב-Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### דוגמאות קהילתיות
- **מחברות Jupyter**: זמינות במאגר GitHub של Olive
- **תוסף VS Code**: תוסף AI Toolkit משתמש ב-Olive לאופטימיזציית מודלים
- **פוסטים בבלוג**: הבלוג של Microsoft Open Source כולל מדריכי Olive מפורטים

### כלים קשורים
- **ONNX Runtime**: מנוע הסקה בעל ביצועים גבוהים
- **Hugging Face Transformers**: מקור למודלים תואמים רבים
- **Azure Machine Learning**: זרימות עבודה מבוססות ענן לאופטימיזציה

## ➡️ מה הלאה

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.