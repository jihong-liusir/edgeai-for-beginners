<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T12:54:41+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "he"
}
-->
# פרק 2: מדריך ליישום Llama.cpp

## תוכן עניינים
1. [מבוא](../../../Module04)
2. [מה זה Llama.cpp?](../../../Module04)
3. [התקנה](../../../Module04)
4. [בנייה מקוד מקור](../../../Module04)
5. [כימות מודלים](../../../Module04)
6. [שימוש בסיסי](../../../Module04)
7. [תכונות מתקדמות](../../../Module04)
8. [שילוב עם Python](../../../Module04)
9. [פתרון תקלות](../../../Module04)
10. [שיטות עבודה מומלצות](../../../Module04)

## מבוא

מדריך מקיף זה ילווה אתכם בכל מה שצריך לדעת על Llama.cpp, החל מהתקנה בסיסית ועד לתרחישי שימוש מתקדמים. Llama.cpp הוא יישום עוצמתי ב-C++ שמאפשר ביצוע יעיל של מודלים שפתיים גדולים (LLMs) עם הגדרות מינימליות וביצועים מצוינים על פני מגוון רחב של תצורות חומרה.

## מה זה Llama.cpp?

Llama.cpp הוא מסגרת לביצוע מודלים שפתיים גדולים (LLM) שנכתבה ב-C/C++. היא מאפשרת הרצת מודלים שפתיים גדולים באופן מקומי עם הגדרות מינימליות וביצועים מתקדמים על מגוון רחב של חומרות. תכונות עיקריות כוללות:

### תכונות ליבה
- **יישום ב-C/C++ בלבד** ללא תלות בספריות חיצוניות
- **תאימות בין-פלטפורמות** (Windows, macOS, Linux)
- **אופטימיזציה לחומרה** עבור ארכיטקטורות שונות
- **תמיכה בכימות** (כימות בין 1.5 ביט ל-8 ביט)
- **תמיכה בהאצת CPU ו-GPU**
- **יעילות בזיכרון** לסביבות עם משאבים מוגבלים

### יתרונות
- רץ ביעילות על CPU ללא צורך בחומרה מיוחדת
- תומך במגוון ממשקי GPU (CUDA, Metal, OpenCL, Vulkan)
- קל משקל ונייד
- מותאם במיוחד ל-Apple Silicon באמצעות ARM NEON, Accelerate ו-Metal
- תומך ברמות כימות שונות להפחתת שימוש בזיכרון

## התקנה

### שיטה 1: קבצים בינאריים מוכנים (מומלץ למתחילים)

#### הורדה מ-GitHub Releases
1. בקרו ב-[Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. הורידו את הקובץ הבינארי המתאים למערכת שלכם:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` עבור Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` עבור macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` עבור Linux

3. חלצו את הקובץ והוסיפו את התיקייה ל-PATH של המערכת שלכם

#### שימוש במנהלי חבילות

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (הפצות שונות):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### שיטה 2: חבילת Python (llama-cpp-python)

#### התקנה בסיסית
```bash
pip install llama-cpp-python
```

#### עם האצת חומרה
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## בנייה מקוד מקור

### דרישות מוקדמות

**דרישות מערכת:**
- מהדר C++ (GCC, Clang, או MSVC)
- CMake (גרסה 3.14 או גבוהה יותר)
- Git
- כלי בנייה לפלטפורמה שלכם

**התקנת דרישות מוקדמות:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- התקינו את Visual Studio 2022 עם כלי פיתוח ל-C++
- התקינו את CMake מהאתר הרשמי
- התקינו את Git

### תהליך בנייה בסיסי

1. **שכפול המאגר:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **הגדרת הבנייה:**
```bash
cmake -B build
```

3. **בנייה של הפרויקט:**
```bash
cmake --build build --config Release
```

לבנייה מהירה יותר, השתמשו בעבודות מקבילות:
```bash
cmake --build build --config Release -j 8
```

### בניות מותאמות לחומרה

#### תמיכה ב-CUDA (GPU של NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### תמיכה ב-Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### תמיכה ב-OpenBLAS (אופטימיזציית CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### תמיכה ב-Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### אפשרויות בנייה מתקדמות

#### בניית Debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### עם תכונות נוספות
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## כימות מודלים

### הבנת פורמט GGUF

GGUF (Generalized GGML Unified Format) הוא פורמט קובץ אופטימלי שנועד להרצת מודלים שפתיים גדולים ביעילות באמצעות Llama.cpp ומסגרות אחרות. הוא מספק:

- אחסון סטנדרטי של משקלי מודלים
- תאימות משופרת בין פלטפורמות
- ביצועים משופרים
- ניהול יעיל של מטא-נתונים

### סוגי כימות

Llama.cpp תומך ברמות כימות שונות:

| סוג | ביטים | תיאור | מקרה שימוש |
|-----|-------|-------|------------|
| F16 | 16 | דיוק חצי | איכות גבוהה, שימוש רב בזיכרון |
| Q8_0 | 8 | כימות 8 ביט | איזון טוב |
| Q4_0 | 4 | כימות 4 ביט | איכות בינונית, גודל קטן יותר |
| Q2_K | 2 | כימות 2 ביט | גודל הקטן ביותר, איכות נמוכה יותר |

### המרת מודלים

#### מ-PyTorch ל-GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### הורדה ישירה מ-Hugging Face
מודלים רבים זמינים בפורמט GGUF ב-Hugging Face:
- חפשו מודלים עם "GGUF" בשם
- הורידו את רמת הכימות המתאימה
- השתמשו ישירות עם Llama.cpp

## שימוש בסיסי

### ממשק שורת פקודה

#### יצירת טקסט פשוטה
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### שימוש במודלים מ-Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### מצב שרת
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### פרמטרים נפוצים

| פרמטר | תיאור | דוגמה |
|--------|-------|-------|
| `-m` | נתיב לקובץ המודל | `-m model.gguf` |
| `-p` | טקסט התחלה | `-p "שלום עולם"` |
| `-n` | מספר הטוקנים ליצירה | `-n 100` |
| `-c` | גודל הקשר | `-c 4096` |
| `-t` | מספר הליבות | `-t 8` |
| `-ngl` | שכבות GPU | `-ngl 32` |
| `-temp` | טמפרטורה | `-temp 0.7` |

### מצב אינטראקטיבי

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## תכונות מתקדמות

### API של שרת

#### הפעלת השרת
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### שימוש ב-API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### אופטימיזציית ביצועים

#### ניהול זיכרון
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### ריבוי ליבות
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### האצת GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## שילוב עם Python

### שימוש בסיסי עם llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### ממשק צ'אט

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### תגובות זורמות

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### שילוב עם LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## פתרון תקלות

### בעיות נפוצות ופתרונות

#### שגיאות בנייה

**בעיה: CMake לא נמצא**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**בעיה: מהדר לא נמצא**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### בעיות בזמן ריצה

**בעיה: טעינת מודל נכשלת**
- ודאו את נתיב קובץ המודל
- בדקו הרשאות קובץ
- ודאו שיש מספיק זיכרון RAM
- נסו רמות כימות שונות

**בעיה: ביצועים ירודים**
- הפעילו האצת חומרה
- הגדילו את מספר הליבות
- השתמשו בכימות מתאים
- בדקו שימוש בזיכרון GPU

#### בעיות זיכרון

**בעיה: חוסר בזיכרון**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### בעיות ספציפיות לפלטפורמה

#### Windows
- השתמשו ב-MinGW או במהדר Visual Studio
- ודאו הגדרת PATH נכונה
- בדקו הפרעות של אנטי-וירוס

#### macOS
- הפעילו Metal עבור Apple Silicon
- השתמשו ב-Rosetta 2 לצורך תאימות אם נדרש
- בדקו את כלי הפיתוח של Xcode

#### Linux
- התקינו חבילות פיתוח
- בדקו גרסאות דרייבר GPU
- ודאו התקנת ערכת CUDA

## שיטות עבודה מומלצות

### בחירת מודל
1. **בחרו כימות מתאים** בהתאם לחומרה שלכם
2. **שקלו את גודל המודל** מול איכות
3. **בדקו מודלים שונים** עבור מקרה השימוש שלכם

### אופטימיזציית ביצועים
1. **השתמשו בהאצת GPU** כשאפשר
2. **אופטימיזו את מספר הליבות** עבור ה-CPU שלכם
3. **הגדירו גודל הקשר המתאים** למקרה השימוש שלכם
4. **הפעילו מיפוי זיכרון** עבור מודלים גדולים

### פריסה בייצור
1. **השתמשו במצב שרת** לגישה דרך API
2. **יישמו טיפול בשגיאות**
3. **עקבו אחרי שימוש במשאבים**
4. **הגדירו לוגים וניטור**

### תהליך פיתוח
1. **התחילו עם מודלים קטנים יותר** לצורך בדיקות
2. **השתמשו בבקרת גרסאות** עבור הגדרות מודלים
3. **תעדו את ההגדרות שלכם**
4. **בדקו על פלטפורמות שונות**

### שיקולי אבטחה
1. **אמתו את טקסטי הקלט**
2. **יישמו הגבלת קצב**
3. **אבטחו נקודות קצה של API**
4. **עקבו אחרי דפוסי שימוש לרעה**

## סיכום

Llama.cpp מספק דרך עוצמתית ויעילה להריץ מודלים שפתיים גדולים באופן מקומי על פני מגוון רחב של תצורות חומרה. בין אם אתם מפתחים יישומי AI, עורכים מחקר, או פשוט מתנסים ב-LLMs, מסגרת זו מציעה את הגמישות והביצועים הנדרשים למגוון רחב של מקרים.

נקודות מפתח:
- בחרו את שיטת ההתקנה המתאימה ביותר לצרכים שלכם
- אופטימיזו את ההגדרות בהתאם לחומרה שלכם
- התחילו בשימוש בסיסי והתקדמו לתכונות מתקדמות
- שקלו להשתמש בקישוריות Python לשילוב קל יותר
- עקבו אחרי שיטות עבודה מומלצות לפריסות בייצור

למידע נוסף ועדכונים, בקרו ב-[מאגר הרשמי של Llama.cpp](https://github.com/ggml-org/llama.cpp) ועיינו בתיעוד המקיף ובמשאבי הקהילה הזמינים.

## ➡️ מה הלאה

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.