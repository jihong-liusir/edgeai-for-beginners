<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T12:31:44+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "he"
}
-->
# סעיף 03 - שילוב פרוטוקול הקשר מודל (MCP)

## מבוא ל-MCP (פרוטוקול הקשר מודל)

פרוטוקול הקשר מודל (MCP) הוא מסגרת מהפכנית שמאפשרת למודלים שפה לתקשר עם כלים ומערכות חיצוניות בצורה סטנדרטית. בניגוד לגישות מסורתיות שבהן מודלים מבודדים, MCP יוצר גשר בין מודלים AI לעולם האמיתי באמצעות פרוטוקול מוגדר היטב.

### מהו MCP?

MCP משמש כפרוטוקול תקשורת שמאפשר למודלים שפה:
- להתחבר למקורות נתונים חיצוניים
- להפעיל כלים ופונקציות
- לתקשר עם APIs ושירותים
- לגשת למידע בזמן אמת
- לבצע פעולות מורכבות מרובות שלבים

הפרוטוקול הזה הופך מודלים שפה סטטיים לסוכנים דינמיים המסוגלים לבצע משימות מעשיות מעבר ליצירת טקסט.

## מודלים שפה קטנים (SLMs) ב-MCP

מודלים שפה קטנים מייצגים גישה יעילה לפריסת AI, ומציעים מספר יתרונות:

### יתרונות של SLMs
- **יעילות משאבים**: דרישות חישוביות נמוכות
- **זמני תגובה מהירים**: הפחתת השהייה ליישומים בזמן אמת  
- **חסכוניות**: צרכים מינימליים בתשתית
- **פרטיות**: יכולים לפעול מקומית ללא העברת נתונים
- **התאמה אישית**: קל יותר לכוונן עבור תחומים ספציפיים

### למה SLMs עובדים טוב עם MCP

SLMs בשילוב עם MCP יוצרים שילוב עוצמתי שבו יכולות ההסקה של המודל מוגברות על ידי כלים חיצוניים, מה שמפצה על מספר הפרמטרים הקטן שלהם באמצעות פונקציונליות משופרת.

## סקירה כללית של Python MCP SDK

Python MCP SDK מספק את הבסיס לבניית יישומים המותאמים ל-MCP. ה-SDK כולל:

- **ספריות לקוח**: להתחברות לשרתי MCP
- **מסגרת שרת**: ליצירת שרתי MCP מותאמים אישית
- **מטפלי פרוטוקול**: לניהול תקשורת
- **שילוב כלים**: להפעלת פונקציות חיצוניות

## יישום מעשי: לקוח MCP Phi-4

בואו נחקור יישום בעולם האמיתי באמצעות מודל המיני של Microsoft Phi-4 המשולב עם יכולות MCP.

### ארכיטקטורת מערכת

היישום עוקב אחר ארכיטקטורה שכבתית:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### רכיבים מרכזיים

#### 1. מחלקות לקוח MCP

**BaseMCPClient**: בסיס מופשט המספק פונקציונליות משותפת
- פרוטוקול מנהל הקשר אסינכרוני
- הגדרת ממשק סטנדרטי
- ניהול משאבים

**Phi4MiniMCPClient**: יישום מבוסס STDIO
- תקשורת תהליך מקומי
- טיפול בקלט/פלט סטנדרטי
- ניהול תהליכי משנה

**Phi4MiniSSEMCPClient**: יישום אירועים שנשלחים מהשרת
- תקשורת סטרימינג HTTP
- טיפול באירועים בזמן אמת
- קישוריות שרת מבוססת רשת

#### 2. שילוב LLM

**OllamaClient**: אירוח מודל מקומי
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: שירות ביצועים גבוהים
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. צינור עיבוד כלים

צינור עיבוד הכלים הופך כלים של MCP לפורמטים תואמים למודלים שפה:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## התחלה: מדריך שלב-אחר-שלב

### שלב 1: הגדרת סביבה

התקן את התלויות הנדרשות:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### שלב 2: תצורה בסיסית

הגדר את משתני הסביבה שלך:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### שלב 3: הפעלת לקוח MCP ראשון

**הגדרת Ollama בסיסית:**
```bash
python ghmodel_mcp_demo.py
```

**שימוש ב-vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**חיבור אירועים שנשלחים מהשרת:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**שרת MCP מותאם אישית:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### שלב 4: שימוש תכנותי

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## תכונות מתקדמות

### תמיכה בריבוי Backend

היישום תומך גם ב-Ollama וגם ב-vLLM, ומאפשר לך לבחור בהתאם לדרישות שלך:

- **Ollama**: טוב יותר לפיתוח ובדיקה מקומית
- **vLLM**: מותאם לייצור ותסריטים בעלי תפוקה גבוהה

### פרוטוקולי חיבור גמישים

שני מצבי חיבור נתמכים:

**מצב STDIO**: תקשורת תהליך ישירה
- השהייה נמוכה יותר
- מתאים לכלים מקומיים
- הגדרה פשוטה

**מצב SSE**: סטרימינג מבוסס HTTP
- מתאים לרשת
- טוב יותר למערכות מבוזרות
- עדכונים בזמן אמת

### יכולות שילוב כלים

המערכת יכולה להשתלב עם מגוון כלים:
- אוטומציה של אתרים (Playwright)
- פעולות קבצים
- אינטראקציות API
- פקודות מערכת
- פונקציות מותאמות אישית

## טיפול בשגיאות ונהלים מומלצים

### ניהול שגיאות מקיף

היישום כולל טיפול שגיאות חזק עבור:

**שגיאות חיבור:**
- תקלות שרת MCP
- פסקי זמן ברשת
- בעיות קישוריות

**שגיאות הפעלת כלים:**
- כלים חסרים
- אימות פרמטרים
- תקלות הפעלה

**שגיאות עיבוד תגובות:**
- בעיות ניתוח JSON
- אי עקביות בפורמט
- אנומליות בתגובות LLM

### נהלים מומלצים

1. **ניהול משאבים**: השתמש במנהלי הקשר אסינכרוניים
2. **טיפול בשגיאות**: יישם בלוקים try-catch מקיפים
3. **רישום**: הפעל רמות רישום מתאימות
4. **אבטחה**: אמת קלטים ונקה פלטים
5. **ביצועים**: השתמש בבריכות חיבור ובמטמון

## יישומים בעולם האמיתי

### אוטומציה של אתרים
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### עיבוד נתונים
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### אינטגרציה עם API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## אופטימיזציית ביצועים

### ניהול זיכרון
- טיפול יעיל בהיסטוריית הודעות
- ניקוי משאבים נכון
- בריכות חיבור

### אופטימיזציית רשת
- פעולות HTTP אסינכרוניות
- פסקי זמן ניתנים להגדרה
- התאוששות שגיאות בצורה חלקה

### עיבוד מקבילי
- I/O לא חוסם
- הפעלת כלים במקביל
- דפוסים אסינכרוניים יעילים

## שיקולי אבטחה

### הגנת נתונים
- ניהול מאובטח של מפתחות API
- אימות קלט
- ניקוי פלט

### אבטחת רשת
- תמיכה ב-HTTPS
- ברירות מחדל לנקודות קצה מקומיות
- טיפול מאובטח באסימונים

### בטיחות הפעלה
- סינון כלים
- סביבות מוגנות
- רישום ביקורת

## סיכום

SLMs המשולבים עם MCP מייצגים שינוי פרדיגמה בפיתוח יישומי AI. על ידי שילוב היעילות של מודלים קטנים עם הכוח של כלים חיצוניים, מפתחים יכולים ליצור מערכות חכמות שהן גם יעילות במשאבים וגם בעלות יכולות גבוהות.

יישום לקוח MCP Phi-4 מדגים כיצד ניתן להשיג את השילוב הזה בפועל, ומספק בסיס מוצק לבניית יישומים מתקדמים המופעלים על ידי AI.

נקודות מפתח:
- MCP מגשר על הפער בין מודלים שפה למערכות חיצוניות
- SLMs מציעים יעילות מבלי לוותר על יכולת כאשר הם מוגברים על ידי כלים
- הארכיטקטורה המודולרית מאפשרת הרחבה והתאמה אישית בקלות
- טיפול שגיאות ואמצעי אבטחה נכונים חיוניים לשימוש בייצור

מדריך זה מספק את הבסיס לבניית יישומי MCP המופעלים על ידי SLM משלך, ופותח אפשרויות לאוטומציה, עיבוד נתונים ואינטגרציה של מערכות חכמות.

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.