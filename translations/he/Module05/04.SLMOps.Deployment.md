<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T13:05:16+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "he"
}
-->
# פרק 4: פריסה - יישום מודל מוכן לייצור

## סקירה כללית

מדריך מקיף זה ילווה אתכם בתהליך המלא של פריסת מודלים מכווננים ומקוונטזים באמצעות Foundry Local. נסקור המרה של מודלים, אופטימיזציה של קוונטיזציה, והגדרת פריסה מתחילתה ועד סופה.

## דרישות מוקדמות

לפני שמתחילים, ודאו שיש ברשותכם את הדברים הבאים:

- ✅ מודל onnx מכוונן ומוכן לפריסה
- ✅ מחשב Windows או Mac
- ✅ Python 3.10 או גרסה חדשה יותר
- ✅ לפחות 8GB זיכרון RAM פנוי
- ✅ Foundry Local מותקן במערכת שלכם

## חלק 1: הגדרת סביבה

### התקנת כלים נדרשים

פתחו את הטרמינל (Command Prompt ב-Windows, Terminal ב-Mac) והריצו את הפקודות הבאות ברצף:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **הערה חשובה**: תצטרכו גם את CMake בגרסה 3.31 או חדשה יותר, שניתן להוריד מ-[cmake.org](https://cmake.org/download/).

## חלק 2: המרת מודל וקוונטיזציה

### בחירת הפורמט הנכון

למודלים קטנים מכווננים, אנו ממליצים להשתמש בפורמט **ONNX** מכיוון שהוא מציע:

- 🚀 אופטימיזציית ביצועים טובה יותר
- 🔧 פריסה שאינה תלויה בחומרה
- 🏭 יכולות מוכנות לייצור
- 📱 תאימות בין-פלטפורמות

### שיטה 1: המרה בפקודה אחת (מומלץ)

השתמשו בפקודה הבאה להמרת המודל המכוונן שלכם ישירות:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**הסבר על הפרמטרים:**
- `--model_name_or_path`: הנתיב למודל המכוונן שלכם
- `--device cpu`: שימוש ב-CPU לאופטימיזציה
- `--precision int4`: שימוש בקוונטיזציה INT4 (הפחתת גודל בכ-75%)
- `--output_path`: נתיב הפלט למודל המומר

### שיטה 2: שימוש בקובץ הגדרות (למשתמשים מתקדמים)

צרו קובץ הגדרות בשם `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

לאחר מכן הריצו:

```bash
olive run --config ./finetuned_conversion_config.json
```

### השוואת אפשרויות קוונטיזציה

| דיוק      | גודל קובץ | מהירות הסקה | איכות מודל | שימוש מומלץ       |
|-----------|-----------|-------------|------------|-------------------|
| FP16      | בסיס × 0.5 | מהיר        | הטוב ביותר | חומרה מתקדמת      |
| INT8      | בסיס × 0.25 | מהיר מאוד   | טוב        | בחירה מאוזנת      |
| INT4      | בסיס × 0.125 | המהיר ביותר | סביר       | משאבים מוגבלים    |

💡 **המלצה**: התחילו עם קוונטיזציה INT4 לפריסה הראשונה שלכם. אם האיכות אינה מספקת, נסו INT8 או FP16.

## חלק 3: הגדרת פריסה ב-Foundry Local

### יצירת הגדרות מודל

נווטו לתיקיית המודלים של Foundry Local:

```bash
foundry cache cd ./models/
```

צרו את מבנה התיקיות של המודל:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

צרו את קובץ ההגדרות `inference_model.json` בתיקיית המודל שלכם:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### תבניות הגדרות ייחודיות למודלים

#### עבור מודלים מסדרת Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## חלק 4: בדיקות ואופטימיזציה של המודל

### אימות התקנת המודל

בדקו אם Foundry Local מזהה את המודל שלכם:

```bash
foundry cache ls
```

אתם אמורים לראות את `your-finetuned-model-int4` ברשימה.

### התחלת בדיקות מודל

```bash
foundry model run your-finetuned-model-int4
```

### בדיקת ביצועים

עקבו אחר מדדים מרכזיים במהלך הבדיקות:

1. **זמן תגובה**: מדדו את הזמן הממוצע לכל תגובה
2. **שימוש בזיכרון**: עקבו אחר צריכת ה-RAM
3. **ניצול CPU**: בדקו את עומס המעבד
4. **איכות פלט**: העריכו את הרלוונטיות והעקביות של התגובות

### רשימת בדיקות לאימות איכות

- ✅ המודל מגיב כראוי לשאילתות בתחום המכוונן
- ✅ פורמט התגובה תואם את מבנה הפלט המצופה
- ✅ אין דליפות זיכרון במהלך שימוש ממושך
- ✅ ביצועים עקביים לאורך קלטים באורכים שונים
- ✅ טיפול נכון במקרי קצה וקלטים לא תקינים

## סיכום

ברכות! השלמתם בהצלחה:

- ✅ המרת פורמט של מודל מכוונן
- ✅ אופטימיזציית קוונטיזציה של המודל
- ✅ הגדרת פריסה ב-Foundry Local
- ✅ כוונון ביצועים ופתרון בעיות

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.