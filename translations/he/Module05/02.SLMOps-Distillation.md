<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-18T13:01:19+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "he"
}
-->
# פרק 2: זיקוק מודלים - מתיאוריה לפרקטיקה

## תוכן עניינים
1. [מבוא לזיקוק מודלים](../../../Module05)
2. [למה זיקוק חשוב](../../../Module05)
3. [תהליך הזיקוק](../../../Module05)
4. [יישום מעשי](../../../Module05)
5. [דוגמה לזיקוק ב-Azure ML](../../../Module05)
6. [שיטות עבודה מומלצות ואופטימיזציה](../../../Module05)
7. [יישומים בעולם האמיתי](../../../Module05)
8. [סיכום](../../../Module05)

## מבוא לזיקוק מודלים {#introduction}

זיקוק מודלים הוא טכניקה עוצמתית שמאפשרת לנו ליצור מודלים קטנים ויעילים יותר תוך שמירה על חלק ניכר מהביצועים של מודלים גדולים ומורכבים יותר. התהליך כולל אימון מודל "תלמיד" קומפקטי לחקות את ההתנהגות של מודל "מורה" גדול.

**יתרונות מרכזיים:**
- **דרישות חישוב מופחתות** בזמן חיזוי
- **שימוש מופחת בזיכרון** ודרישות אחסון נמוכות
- **זמני חיזוי מהירים יותר** תוך שמירה על דיוק סביר
- **פריסה חסכונית** בסביבות עם משאבים מוגבלים

## למה זיקוק חשוב {#why-distillation-matters}

מודלים גדולים לשפה (LLMs) הופכים לעוצמתיים יותר, אך גם לדרישות משאבים גבוהות יותר. בעוד שמודל עם מיליארדי פרמטרים עשוי לספק תוצאות מצוינות, הוא לא תמיד פרקטי לשימושים בעולם האמיתי בשל:

### מגבלות משאבים
- **עומס חישובי**: מודלים גדולים דורשים זיכרון GPU רב וכוח עיבוד משמעותי
- **זמני חיזוי ארוכים**: מודלים מורכבים לוקחים יותר זמן לייצר תגובות
- **צריכת אנרגיה**: מודלים גדולים צורכים יותר חשמל, מה שמעלה את עלויות התפעול
- **עלויות תשתית**: אירוח מודלים גדולים דורש חומרה יקרה

### מגבלות מעשיות
- **פריסה במובייל**: מודלים גדולים לא יכולים לפעול ביעילות על מכשירים ניידים
- **יישומים בזמן אמת**: יישומים הדורשים השהיה נמוכה לא יכולים להתמודד עם זמני חיזוי איטיים
- **מחשוב קצה**: מכשירי IoT וקצה מוגבלים במשאבים חישוביים
- **שיקולי עלות**: ארגונים רבים לא יכולים להרשות לעצמם את התשתית הנדרשת לפריסת מודלים גדולים

## תהליך הזיקוק {#the-distillation-process}

זיקוק מודלים מתבצע בשני שלבים שמעבירים ידע ממודל מורה למודל תלמיד:

### שלב 1: יצירת נתונים סינתטיים

מודל המורה מייצר תגובות עבור מערך הנתונים שלך, ויוצר נתונים סינתטיים איכותיים שמכילים את הידע ודפוסי החשיבה של המורה.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**היבטים מרכזיים בשלב זה:**
- מודל המורה מעבד כל דוגמת אימון
- התגובות שנוצרות הופכות ל"אמת המידה" לאימון התלמיד
- התהליך לוכד את דפוסי קבלת ההחלטות של המורה
- איכות הנתונים הסינתטיים משפיעה ישירות על ביצועי מודל התלמיד

### שלב 2: כוונון מודל התלמיד

מודל התלמיד מאומן על מערך הנתונים הסינתטי, ולומד לשחזר את ההתנהגות והתגובות של המורה.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**מטרות האימון:**
- למזער את ההבדל בין פלטי התלמיד לפלטי המורה
- לשמר את הידע של המורה במרחב פרמטרים קטן יותר
- לשמור על ביצועים תוך הפחתת מורכבות המודל

## יישום מעשי {#practical-implementation}

### בחירת מודלי מורה ותלמיד

**בחירת מודל מורה:**
- בחרו מודלים גדולים (100B+ פרמטרים) עם ביצועים מוכחים במשימה שלכם
- מודלים פופולריים למורה כוללים:
  - **DeepSeek V3** (671B פרמטרים) - מצוין להסקה וליצירת קוד
  - **Meta Llama 3.1 405B Instruct** - יכולות כלליות מקיפות
  - **GPT-4** - ביצועים חזקים במגוון משימות
  - **Claude 3.5 Sonnet** - מצוין למשימות הסקה מורכבות
- ודאו שמודל המורה מבצע היטב על נתונים ייחודיים לתחום שלכם

**בחירת מודל תלמיד:**
- איזון בין גודל המודל לדרישות הביצועים
- התמקדות במודלים קטנים ויעילים כמו:
  - **Microsoft Phi-4-mini** - מודל יעיל עם יכולות הסקה חזקות
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (גרסאות 4K ו-128K)
  - Microsoft Phi-3.5 Mini Instruct

### שלבי יישום

1. **הכנת נתונים**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **הגדרת מודל מורה**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **יצירת נתונים סינתטיים**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **אימון מודל תלמיד**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## דוגמה לזיקוק ב-Azure ML {#azure-ml-example}

Azure Machine Learning מספקת פלטפורמה מקיפה ליישום זיקוק מודלים. כך ניתן לנצל את Azure ML בתהליך הזיקוק:

### דרישות מוקדמות

1. **סביבת עבודה ב-Azure ML**: הגדירו את סביבת העבודה באזור המתאים
   - ודאו גישה למודלי מורה גדולים (DeepSeek V3, Llama 405B)
   - הגדירו אזורים בהתאם לזמינות המודלים

2. **משאבי מחשוב**: הגדירו מופעי מחשוב מתאימים לאימון
   - מופעים עם זיכרון גבוה לחיזוי מודל המורה
   - מחשוב עם GPU לכוונון מודל התלמיד

### סוגי משימות נתמכים

Azure ML תומכת בזיקוק למשימות שונות:

- **פרשנות שפה טבעית (NLI)**
- **בינה שיחתית**
- **שאלות ותשובות (QA)**
- **הסקה מתמטית**
- **סיכום טקסטים**

### יישום לדוגמה

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### ניטור והערכה

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## שיטות עבודה מומלצות ואופטימיזציה {#best-practices}

### איכות נתונים

**איכות נתוני האימון היא קריטית:**
- ודאו דוגמאות אימון מגוונות ומייצגות
- השתמשו בנתונים ייחודיים לתחום כשאפשר
- בדקו את פלטי מודל המורה לפני השימוש בהם לאימון התלמיד
- איזנו את מערך הנתונים כדי למנוע הטיות בלמידת מודל התלמיד

### כוונון פרמטרים

**פרמטרים מרכזיים לאופטימיזציה:**
- **קצב למידה**: התחילו בקצבים קטנים (1e-5 עד 5e-5) לכוונון
- **גודל אצווה**: איזון בין מגבלות זיכרון ליציבות האימון
- **מספר אפוקים**: עקבו אחר סימני התאמת יתר; בדרך כלל 2-5 אפוקים מספיקים
- **סקאלת טמפרטורה**: התאימו את רכות פלטי המורה להעברת ידע טובה יותר

### שיקולי ארכיטקטורת מודל

**התאמה בין מורה לתלמיד:**
- ודאו התאמה ארכיטקטונית בין מודלי המורה והתלמיד
- שקלו התאמת שכבות ביניים להעברת ידע טובה יותר
- השתמשו בטכניקות העברת תשומת לב כשמתאים

### אסטרטגיות הערכה

**גישה להערכה מקיפה:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## יישומים בעולם האמיתי {#real-world-applications}

### פריסה במובייל ובקצה

מודלים מזוקקים מאפשרים יכולות AI במכשירים עם משאבים מוגבלים:
- **אפליקציות סמארטפון** עם עיבוד טקסט בזמן אמת
- **מכשירי IoT** המבצעים חיזוי מקומי
- **מערכות משובצות** עם משאבים חישוביים מוגבלים

### מערכות ייצור חסכוניות

ארגונים משתמשים בזיקוק להפחתת עלויות תפעול:
- **צ'אטבוטים לשירות לקוחות** עם זמני תגובה מהירים יותר
- **מערכות סינון תוכן** המעבדות נפחים גבוהים ביעילות
- **שירותי תרגום בזמן אמת** עם דרישות השהיה נמוכות

### יישומים ייחודיים לתחום

זיקוק מסייע ביצירת מודלים מותאמים:
- **סיוע באבחון רפואי** עם חיזוי מקומי השומר על פרטיות
- **ניתוח מסמכים משפטיים** מותאם לתחומים משפטיים ספציפיים
- **הערכת סיכונים פיננסיים** עם קבלת החלטות מהירה

### מקרה בוחן: תמיכה בלקוחות עם DeepSeek V3 → Phi-4-mini

חברת טכנולוגיה יישמה זיקוק במערכת התמיכה בלקוחות שלה:

**פרטי היישום:**
- **מודל מורה**: DeepSeek V3 (671B פרמטרים) - מצוין להסקה בשאלות לקוח מורכבות
- **מודל תלמיד**: Phi-4-mini - מותאם לחיזוי מהיר ולפריסה
- **נתוני אימון**: 50,000 שיחות תמיכה בלקוחות
- **משימה**: תמיכה שיחתית מרובת פניות עם פתרון בעיות טכניות

**תוצאות שהושגו:**
- **85% הפחתה** בזמן החיזוי (מ-3.2 שניות ל-0.48 שניות לתגובה)
- **95% הפחתה** בדרישות הזיכרון (מ-1.2TB ל-60GB)
- **92% שימור** של דיוק המודל המקורי במשימות תמיכה
- **60% הפחתה** בעלויות התפעול
- **שיפור בסקלביליות** - יכולת לטפל ב-10x יותר משתמשים בו-זמנית

**פירוט ביצועים:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## סיכום {#conclusion}

זיקוק מודלים מהווה טכניקה קריטית להנגשת יכולות AI מתקדמות. על ידי יצירת מודלים קטנים ויעילים יותר ששומרים על חלק ניכר מהביצועים של המודלים הגדולים, זיקוק עונה על הצורך הגובר בפריסת AI פרקטית.

### נקודות מפתח

1. **זיקוק מגשר על הפער** בין ביצועי מודלים למגבלות מעשיות
2. **תהליך דו-שלבי** מבטיח העברת ידע יעילה ממורה לתלמיד
3. **Azure ML מספקת תשתית חזקה** ליישום תהליכי זיקוק
4. **הערכה ואופטימיזציה נכונות** חיוניות להצלחת הזיקוק
5. **יישומים בעולם האמיתי** מדגימים יתרונות משמעותיים בעלות, מהירות ונגישות

### כיוונים לעתיד

ככל שהתחום ממשיך להתפתח, ניתן לצפות:
- **טכניקות זיקוק מתקדמות** עם שיטות העברת ידע משופרות
- **זיקוק מרובה מורים** לשיפור יכולות מודל התלמיד
- **אופטימיזציה אוטומטית** של תהליך הזיקוק
- **תמיכה רחבה יותר במודלים** בארכיטקטורות ותחומים שונים

זיקוק מודלים מאפשר לארגונים לנצל יכולות AI מתקדמות תוך שמירה על מגבלות פריסה מעשיות, ומנגיש מודלים לשפה מתקדמים למגוון רחב של יישומים וסביבות.

## ➡️ מה הלאה

- [03: כוונון - התאמת מודלים למשימות ספציפיות](./03.SLMOps-Finetuing.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.