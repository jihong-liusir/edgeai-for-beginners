<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:50:42+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "he"
}
-->
# מפגש 3: גילוי וניהול מודלים בקוד פתוח

## סקירה כללית

המפגש הזה מתמקד בגילוי וניהול מודלים באופן מעשי באמצעות Foundry Local. תלמדו כיצד לרשום מודלים זמינים, לבדוק אפשרויות שונות ולהבין מאפייני ביצועים בסיסיים. הגישה מדגישה חקר מעשי עם ממשק ה-CLI של Foundry כדי לעזור לכם לבחור את המודלים המתאימים לשימושים שלכם.

## מטרות למידה

- לשלוט בפקודות CLI של Foundry לגילוי וניהול מודלים
- להבין את דפוסי המטמון והאחסון המקומי של מודלים
- ללמוד לבדוק ולהשוות מודלים במהירות
- להקים תהליכי עבודה מעשיים לבחירת מודלים וביצוע השוואות
- לחקור את האקוסיסטם הצומח של מודלים הזמינים דרך Foundry Local

## דרישות מקדימות

- סיום מפגש 1: התחלת העבודה עם Foundry Local
- התקנה וגישה ל-CLI של Foundry Local
- שטח אחסון מספיק להורדת מודלים (גודל המודלים נע בין 1GB ל-20GB+)
- הבנה בסיסית של סוגי מודלים ושימושים

## חלק 6: תרגיל מעשי

### תרגיל: גילוי והשוואת מודלים

צרו סקריפט הערכת מודלים משלכם בהתבסס על דוגמה 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### המשימה שלכם

1. **הריצו את סקריפט דוגמה 03**: `samples\03\list_and_bench.cmd`
2. **נסו מודלים שונים**: בדקו לפחות 3 מודלים שונים
3. **השוו ביצועים**: שימו לב להבדלים במהירות ובאיכות התגובה
4. **תעדו ממצאים**: צרו טבלה פשוטה להשוואה

### פורמט השוואה לדוגמה

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## חלק 7: פתרון בעיות וטיפים

### בעיות נפוצות ופתרונות

**המודל לא מתחיל:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**זיכרון לא מספיק:**
- התחילו עם מודלים קטנים יותר (`phi-4-mini`)
- סגרו יישומים אחרים
- שדרגו את הזיכרון אם אתם נתקלים במגבלות לעיתים קרובות

**ביצועים איטיים:**
- ודאו שהמודל נטען במלואו (בדקו פלט מפורט)
- סגרו יישומים מיותרים ברקע
- שקלו אחסון מהיר יותר (SSD)

### טיפים

1. **התחילו בקטן**: התחילו עם `phi-4-mini` כדי לוודא שההגדרות תקינות
2. **מודל אחד בכל פעם**: סגרו מודלים קודמים לפני התחלת חדשים
3. **עקבו אחרי משאבים**: שימו לב לשימוש בזיכרון
4. **בדקו באופן עקבי**: השתמשו באותם פקודות לבדיקה הוגנת
5. **תעדו תוצאות**: שמרו הערות על ביצועי מודלים לשימושים שלכם

## חלק 8: צעדים הבאים ומשאבים

### הכנה למפגש 4

- **מיקוד מפגש 4**: כלים וטכניקות אופטימיזציה
- **דרישות מקדימות**: נוחות עם מעבר בין מודלים ובדיקות ביצועים בסיסיות
- **מומלץ**: לזהות 2-3 מודלים מועדפים מהמפגש הזה

### משאבים נוספים

- **[תיעוד Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: תיעוד רשמי
- **[התייחסות CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: התייחסות מלאה לפקודות
- **[Model Mondays](https://aka.ms/model-mondays)**: סקירות מודלים שבועיות
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: קהילה ותקלות
- **[דוגמה 03: גילוי מודלים](samples/03/README.md)**: סקריפט לדוגמה מעשי

### נקודות מפתח

✅ **גילוי מודלים**: השתמשו ב-`foundry model list` כדי לחקור מודלים זמינים  
✅ **בדיקה מהירה**: דפוס `list_and_bench.cmd` להערכה מהירה  
✅ **מעקב ביצועים**: מדידת שימוש במשאבים וזמן תגובה בסיסי  
✅ **בחירת מודלים**: הנחיות מעשיות לבחירת מודלים לפי שימוש  
✅ **ניהול מטמון**: הבנת אחסון ונהלי ניקוי  

כעת יש לכם את הכישורים המעשיים לגילוי, בדיקה ובחירת מודלים מתאימים ליישומי AI שלכם באמצעות הגישה הפשוטה של CLI של Foundry Local: בחירת מודלים קהילתיים, שילוב תוכן Hugging Face ואימוץ אסטרטגיות "הביאו את המודל שלכם" (BYOM). תגלו גם את סדרת Model Mondays ללמידה מתמשכת וגילוי מודלים.

משאבים:
- תיעוד Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- קומפילציה של מודלים Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## מטרות למידה
- גילוי והערכת מודלים בקוד פתוח להסקה מקומית
- קומפילציה והרצה של מודלים נבחרים מ-Hugging Face בתוך Foundry Local
- יישום אסטרטגיות בחירת מודלים לדיוק, זמן תגובה וצרכי משאבים
- ניהול מודלים מקומי עם מטמון וגרסאות

## חלק 1: גילוי מודלים עם Foundry CLI

### פקודות ניהול מודלים בסיסיות

ממשק ה-CLI של Foundry מספק פקודות פשוטות לגילוי וניהול מודלים:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### הרצת המודלים הראשונים שלכם

התחילו עם מודלים פופולריים ונבדקים היטב כדי להבין מאפייני ביצועים:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**הערה:** הדגל `--verbose` מספק מידע מפורט על ההפעלה, כולל:
- התקדמות הורדת מודלים (בהפעלה הראשונה)
- פרטי הקצאת זיכרון
- מידע על חיבור שירותים
- מדדי אתחול ביצועים

### הבנת קטגוריות מודלים

**מודלים קטנים (SLMs):**
- `phi-4-mini`: מהיר, יעיל, מצוין לשיחות כלליות
- `phi-4`: גרסה מתקדמת עם יכולות הסקה טובות יותר

**מודלים בינוניים:**
- `qwen2.5-7b`: הסקה מצוינת והקשר ארוך יותר
- `deepseek-r1-7b`: מותאם ליצירת קוד

**מודלים גדולים:**
- `llama-3.2`: המודל החדש של Meta בקוד פתוח
- `qwen2.5-14b`: הסקה ברמה ארגונית

## חלק 2: בדיקה והשוואה מהירה של מודלים

### גישת דוגמה 03: רשימה ובדיקה פשוטה

בהתבסס על דפוס דוגמה 03, הנה תהליך מינימלי:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### בדיקת ביצועי מודלים

לאחר שהמודל פועל, בדקו אותו עם פקודות עקביות:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### אלטרנטיבה לבדיקה ב-PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## חלק 3: ניהול מטמון ואחסון מודלים

### הבנת מטמון המודלים

Foundry Local מנהל באופן אוטומטי הורדות ומטמון של מודלים:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### שיקולי אחסון מודלים

**גדלים טיפוסיים של מודלים:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**טיפים לאחסון:**
- שמרו 2-3 מודלים במטמון למעבר מהיר
- הסירו מודלים לא בשימוש כדי לפנות מקום: `foundry cache clean`
- עקבו אחרי שימוש בדיסק, במיוחד ב-SSD קטנים
- שקלו את גודל המודל מול יכולותיו

### מעקב ביצועי מודלים

בזמן שהמודלים פועלים, עקבו אחרי משאבי המערכת:

**מנהל המשימות של Windows:**
- עקבו אחרי שימוש בזיכרון (מודלים נשארים טעונים ב-RAM)
- עקבו אחרי ניצול CPU בזמן הסקה
- בדקו I/O של דיסק בזמן טעינת מודלים ראשונית

**מעקב בשורת הפקודה:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## חלק 4: הנחיות מעשיות לבחירת מודלים

### בחירת מודלים לפי שימוש

**לשיחות כלליות ושאלות ותשובות:**
- התחילו עם: `phi-4-mini` (מהיר, יעיל)
- שדרגו ל: `phi-4` (הסקה טובה יותר)
- מתקדם: `qwen2.5-7b` (הקשר ארוך יותר)

**ליצירת קוד:**
- מומלץ: `deepseek-r1-7b`
- אלטרנטיבה: `qwen2.5-7b` (גם טוב לקוד)

**להסקה מורכבת:**
- הטוב ביותר: `qwen2.5-7b` או `qwen2.5-14b`
- אפשרות חסכונית: `phi-4`

### מדריך דרישות חומרה

**דרישות מערכת מינימליות:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**מומלץ לביצועים מיטביים:**
- 32GB+ RAM למעבר נוח בין מודלים
- אחסון SSD לטעינת מודלים מהירה יותר
- מעבד מודרני עם ביצועי ליבה יחידה טובים
- תמיכה ב-NPU (מחשבי Windows 11 Copilot+) להאצה

### תהליך מעבר בין מודלים

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## חלק 5: השוואת מודלים פשוטה

### בדיקת ביצועים בסיסית

הנה גישה פשוטה להשוואת ביצועי מודלים:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### הערכת איכות ידנית

לכל מודל, בדקו עם פקודות עקביות והעריכו ידנית:

**פקודות בדיקה:**
1. "הסבר על מחשוב קוונטי במונחים פשוטים."
2. "כתוב פונקציה ב-Python למיון רשימה."
3. "מה היתרונות והחסרונות של עבודה מרחוק?"
4. "סכם את היתרונות של AI בקצה."

**קריטריוני הערכה:**
- **דיוק**: האם המידע נכון?
- **בהירות**: האם ההסבר קל להבנה?
- **שלמות**: האם הוא מתייחס לשאלה במלואה?
- **מהירות**: כמה מהר הוא מגיב?

### מעקב אחרי שימוש במשאבים

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## חלק 6: צעדים הבאים
- הירשמו ל-Model Mondays למודלים וטיפים חדשים: https://aka.ms/model-mondays
- תרמו ממצאים ל-`models.json` של הצוות שלכם
- הכינו למפגש 4: השוואת LLMs מול SLMs, הסקה מקומית מול ענן ודמואים מעשיים

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.