<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T21:50:19+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "he"
}
-->
# מפגש 3: מודלים בקוד פתוח עם Foundry Local

## סקירה כללית

במפגש זה נלמד כיצד לשלב מודלים בקוד פתוח ב-Foundry Local: בחירת מודלים מהקהילה, שילוב תוכן מ-Hugging Face, ואימוץ אסטרטגיות "הבא את המודל שלך" (BYOM). בנוסף, תכירו את סדרת Model Mondays ללמידה מתמשכת וגילוי מודלים חדשים.

קישורים:
- תיעוד Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- קומפילציה של מודלים מ-Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## מטרות למידה
- לגלות ולהעריך מודלים בקוד פתוח לצורך הסקה מקומית
- לקמפל ולהריץ מודלים נבחרים מ-Hugging Face בתוך Foundry Local
- ליישם אסטרטגיות לבחירת מודלים בהתבסס על דיוק, זמן תגובה וצריכת משאבים
- לנהל מודלים באופן מקומי עם מטמון וגרסאות

## חלק 1: גילוי ובחירת מודלים (שלב אחר שלב)

שלב 1) רשימת מודלים זמינים בקטלוג המקומי  
```cmd
foundry model list
```
  
שלב 2) ניסיון מהיר של שני מועמדים (הורדה אוטומטית בהרצה הראשונה)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
שלב 3) תיעוד מדדים בסיסיים  
- התבוננות בזמן תגובה (סובייקטיבי) ואיכות עבור פקודה קבועה  
- מעקב אחר שימוש בזיכרון דרך מנהל המשימות בזמן הרצת כל מודל  

## חלק 2: הרצת מודלים מהקטלוג דרך CLI (שלב אחר שלב)

שלב 1) הפעלת מודל  
```cmd
foundry model run llama-3.2
```
  
שלב 2) שליחת פקודת בדיקה דרך נקודת הקצה התואמת ל-OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## חלק 3: BYOM – קומפילציה של מודלים מ-Hugging Face (שלב אחר שלב)

עקבו אחר ההנחיות הרשמיות לקומפילציה של מודלים. להלן זרימה כללית—ראו את המאמר ב-Microsoft Learn לפקודות מדויקות וקונפיגורציות נתמכות.

שלב 1) הכנת ספריית עבודה  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
שלב 2) קומפילציה של מודל HF נתמך  
- השתמשו בשלבים מהתיעוד כדי להמיר ולהניח את המודל המקומפל בפורמט ONNX בספריית `models` שלכם  
- אימות עם:  
```cmd
foundry cache ls
```
  
תראו את שם המודל המקומפל שלכם (לדוגמה, `llama-3.2`).  

שלב 3) הרצת המודל המקומפל  
```cmd
foundry model run llama-3.2 --verbose
```
  
הערות:  
- ודאו שיש מספיק שטח דיסק וזיכרון RAM לקומפילציה והרצה  
- התחילו עם מודלים קטנים כדי לאמת את הזרימה, ואז עברו למודלים גדולים יותר  

## חלק 4: אצירת מודלים מעשית (שלב אחר שלב)

שלב 1) יצירת רישום `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
שלב 2) סקריפט בחירה קטן  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## חלק 5: מדדי ביצועים מעשיים (שלב אחר שלב)

שלב 1) מדד זמן תגובה פשוט  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
שלב 2) בדיקת איכות  
- השתמשו בסט פקודות קבוע, שמרו את התוצאות בקובץ CSV/JSON  
- דרגו באופן ידני את השטף, הרלוונטיות והדיוק (1–5)  

## חלק 6: צעדים הבאים
- הירשמו ל-Model Mondays לקבלת מודלים וטיפים חדשים: https://aka.ms/model-mondays  
- שתפו ממצאים ברישום `models.json` של הצוות שלכם  
- התכוננו למפגש 4: השוואת LLMs מול SLMs, הסקה מקומית מול ענן, ודמואים מעשיים  

---

