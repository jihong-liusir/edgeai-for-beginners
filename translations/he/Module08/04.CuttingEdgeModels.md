<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T21:52:16+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "he"
}
-->
# מפגש 4: מודלים מתקדמים – LLMs, SLMs ואינפרנס מקומי

## סקירה כללית

השוואה בין LLMs ל-SLMs, הערכת יתרונות וחסרונות של אינפרנס מקומי מול ענן, והטמעת הדגמות שמציגות תרחישי EdgeAI באמצעות Phi ו-ONNX Runtime. בנוסף, נדגיש את Chainlit RAG, אפשרויות אינפרנס עם WebGPU, ואינטגרציה עם Open WebUI.

מקורות:
- תיעוד Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- מדריך Open WebUI (אפליקציית צ'אט עם Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## מטרות למידה
- להבין את היתרונות והחסרונות של LLM מול SLM מבחינת עלות, זמן תגובה ודיוק
- לבחור בין אינפרנס מקומי לענן בהתאם לצרכים עסקיים
- להטמיע דמו קטן של RAG עם Chainlit
- לחקור את WebGPU להאצה בצד הדפדפן
- לחבר את Open WebUI ל-Foundry Local

## חלק 1: LLM מול SLM – מטריצת החלטות

שיקולים:
- זמן תגובה: SLMs על המכשיר מספקים לרוב תגובות תוך פחות משנייה
- עלות: אינפרנס מקומי מפחית עלויות ענן
- פרטיות: נתונים רגישים נשארים על המכשיר
- יכולת: LLMs עשויים להצטיין במשימות מורכבות
- אמינות: אסטרטגיות היברידיות מפחיתות סיכון להשבתה

## חלק 2: מקומי מול ענן – דפוסים היברידיים

- מקומי תחילה עם גיבוי בענן עבור פקודות גדולות/מורכבות
- ענן תחילה עם מקומי עבור תרחישים רגישים לפרטיות או ללא חיבור
- ניתוב לפי סוג משימה (יצירת קוד ל-DeepSeek, צ'אט כללי ל-Phi/Qwen)

## חלק 3: אפליקציית צ'אט RAG עם Chainlit (מינימלי)

התקנת תלות:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

הרצה:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

הרחבה: הוספת רכיב אחזור פשוט (קבצים מקומיים) והקדמת ההקשר שנמצא לפקודת המשתמש.

## חלק 4: אינפרנס עם WebGPU (Heads-up)

הרצת מודלים קטנים ישירות בדפדפן באמצעות WebGPU. זה אידיאלי לדמואים שמעדיפים פרטיות ולחוויות ללא התקנה. להלן דוגמה מינימלית, שלב אחר שלב, באמצעות ONNX Runtime Web עם ספק הביצוע WebGPU.

1) בדיקת תמיכה ב-WebGPU
- דפדפני Chromium: chrome://gpu → לוודא ש-“WebGPU” מופעל
- בדיקה תכנותית (נבדוק גם בקוד): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) יצירת פרויקט מינימלי
יצירת תיקייה ושני קבצים: `index.html` ו-`main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) הרצה מקומית (Windows cmd.exe)
שימוש בשרת סטטי פשוט כדי שהדפדפן יוכל לאחזר את המודל.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

פתיחת http://localhost:5173 בדפדפן. אתם אמורים לראות לוגים של אתחול, יצירת סשן עם WebGPU, וניבוי argmax.

4) פתרון בעיות
- אם WebGPU אינו זמין: עדכנו את Chrome/Edge ודאגו שדרייברי ה-GPU מעודכנים, ואז בדקו ב-chrome://flags את “Enable WebGPU”.
- אם יש שגיאות CORS או fetch: ודאו שאתם משרתים קבצים דרך http:// (ולא file://) וכתובת ה-URL של המודל מאפשרת בקשות cross-origin.
- מעבר ל-CPU: שינוי `executionProviders: ['wasm']` כדי לבדוק התנהגות בסיסית.

5) צעדים הבאים
- החלפת המודל למודל ONNX ייעודי (למשל, סיווג תמונות או מודל טקסט קטן).
- הוספת לוגיקת עיבוד מקדים/עיבוד לאחר עבור קלטים אמיתיים.
- עבור מודלים גדולים או זמן תגובה בייצור, העדיפו Foundry Local או ONNX Runtime Server.

## חלק 5: Open WebUI + Foundry Local (שלב אחר שלב)

חיבור Open WebUI לנקודת הקצה של Foundry Local התואמת ל-OpenAI עבור ממשק צ'אט מקומי.

1) דרישות מקדימות
- Foundry Local מותקן ועובד (`foundry --version`)
- מודל אחד מוכן להרצה מקומית (למשל, `phi-4-mini`)
- Docker Desktop מותקן (מומלץ עבור Open WebUI)

2) הפעלת מודל עם Foundry Local
```powershell
foundry model run phi-4-mini
```
זה חושף API תואם ל-OpenAI בכתובת `http://localhost:8000`.

3) הפעלת Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
הערות:
- ב-Windows, `host.docker.internal` מאפשר למיכל להגיע למארח ב-`localhost`.
- הגדרנו `OPENAI_API_BASE_URL` לנקודת הקצה של Foundry Local ומפתח API מדומה `OPENAI_API_KEY`.

4) הגדרה מתוך ממשק Open WebUI (חלופה)
- גלישה ל-http://localhost:3000
- השלמת ההגדרות הראשוניות (משתמש מנהל)
- מעבר ל-Settings → Models/Providers
- הגדרת Base URL: `http://host.docker.internal:8000/v1`
- הגדרת API Key: `local-key` (מפתח מדומה)
- שמירה

5) הרצת פקודת בדיקה
- בצ'אט של Open WebUI, בחירת או הזנת שם מודל `phi-4-mini`
- פקודה: “ציין חמישה יתרונות של אינפרנס AI על המכשיר.”
- אתם אמורים לראות תגובה מוזרמת מהמודל המקומי שלכם

6) פתרון בעיות
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) אופציונלי: שמירת נתוני Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## רשימת בדיקות מעשיות
- [ ] השוואת תגובות/זמן תגובה בין SLM ו-LLM מקומיים
- [ ] הרצת דמו Chainlit מול לפחות שני מודלים
- [ ] חיבור Open WebUI לנקודת הקצה המקומית ובדיקת תקינות

## צעדים הבאים
- הכנה לזרימות עבודה של סוכנים במפגש 5
- זיהוי תרחישים שבהם אסטרטגיה היברידית מקומי/ענן משפרת ROI

---

