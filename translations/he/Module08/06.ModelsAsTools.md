<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T21:51:39+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "he"
}
-->
# מפגש 6: Foundry Local – מודלים ככלים

## סקירה כללית

התייחסו למודלים של AI ככלים מודולריים הניתנים להתאמה אישית, שפועלים ישירות על המכשיר באמצעות Foundry Local. מפגש זה מדגיש תהליכי עבודה מעשיים עבור חיזוי עם שמירה על פרטיות וביצועים מהירים, וכיצד לשלב את הכלים הללו באמצעות SDKs, APIs או CLI. בנוסף, תלמדו כיצד להתרחב ל-Azure AI Foundry בעת הצורך.

מקורות:
- מסמכי Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- שילוב עם SDKs לחיזוי: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- קומפילציה של מודלים של Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## מטרות למידה
- עיצוב תבניות "מודל ככלי" על המכשיר
- שילוב באמצעות REST API תואם OpenAI או SDKs
- התאמת מודלים לשימושים ספציפיים לתחום
- תכנון להתרחבות היברידית ל-Azure AI Foundry

## חלק 1: הפשטת כלים (שלב אחר שלב)

מטרה: לייצג מודלים ככלים עם חוזים ברורים ומנתב פשוט.

שלב 1) הגדרת ממשק כלי ורישום
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```

שלב 2) יישום שני כלים המבוססים על Foundry Local
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```

שלב 3) נתב לפי משימה
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```

## חלק 2: שילוב SDK ו-API (שלב אחר שלב)

מטרה: שימוש ב-SDK של Python של OpenAI מול נקודת הקצה של Foundry Local.

שלב 1) התקנה
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```

שלב 2) הגדרת משתני סביבה
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```

שלב 3) קריאה ל-API של צ'אט
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```

## חלק 3: התאמה לתחום (שלב אחר שלב)

מטרה: התאמת הפלטים לתחום באמצעות תבניות הנחיה וסכמת JSON.

שלב 1) יצירת תבנית הנחיה לתחום
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```

שלב 2) אכיפת פלט JSON
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```

## חלק 4: עבודה לא מקוונת ועמדת אבטחה (שלב אחר שלב)

מטרה: להבטיח פרטיות ועמידות בעת הפעלת מודלים ככלים באופן מקומי.

שלב 1) חימום מוקדם ואימות נקודת הקצה המקומית
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```

שלב 2) ניקוי קלטים
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

שלב 3) דגל מקומי בלבד ורישום
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```

## חלק 5: התרחבות ל-Azure AI Foundry (שלב אחר שלב)

מטרה: שיקוף מודלים מקומיים עם נקודות קצה של Azure לצורך קיבולת נוספת.

שלב 1) החלטה על אסטרטגיית ניתוב
- מקומי תחילה עבור פרטיות/ביצועים, מעבר ל-Azure במקרה של שגיאות או בקשות גדולות

שלב 2) יישום נתב פשוט
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```

## רשימת משימות מעשיות
- [ ] רישום לפחות שני כלים וניתוב בקשות
- [ ] קריאה ל-Foundry Local באמצעות SDK של OpenAI ו-REST גולמי
- [ ] אכיפת פלטי JSON עבור תבנית תחום
- [ ] ניקוי ורישום קריאות באופן מקומי
- [ ] יישום נתב היברידי פשוט עם מעבר ל-Azure

## סיכום

Foundry Local מאפשר AI חזק על המכשיר שבו מודלים הופכים לכלים ניתנים להרכבה. עם ממשקים ברורים, ניהול נכון והתרחבות היברידית, צוותים יכולים לפתח אפליקציות AI בזמן אמת, מאובטחות, שמכבדות את פרטיות המשתמש תוך שמירה על מוכנות ארגונית.

---

