<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T13:13:29+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "he"
}
-->
# פריסת ענן מבוססת קונטיינרים - פתרונות בקנה מידה ייצור

מדריך מקיף זה מכסה שלוש גישות עיקריות לפריסת מודל Phi-4-mini-instruct של Microsoft בסביבות מבוססות קונטיינרים: vLLM, Ollama, ו-SLM Engine עם ONNX Runtime. מודל זה, בעל 3.8 מיליארד פרמטרים, מהווה בחירה אופטימלית למשימות הסקה תוך שמירה על יעילות לפריסה בקצה.

## תוכן עניינים

1. [מבוא לפריסת קונטיינרים של Phi-4-mini](../../../Module03)
2. [מטרות למידה](../../../Module03)
3. [הבנת סיווג Phi-4-mini](../../../Module03)
4. [פריסת קונטיינר vLLM](../../../Module03)
5. [פריסת קונטיינר Ollama](../../../Module03)
6. [SLM Engine עם ONNX Runtime](../../../Module03)
7. [מסגרת השוואה](../../../Module03)
8. [שיטות עבודה מומלצות](../../../Module03)

## מבוא לפריסת קונטיינרים של Phi-4-mini

מודלים שפתיים קטנים (SLMs) מהווים התקדמות משמעותית ב-EdgeAI, ומאפשרים יכולות עיבוד שפה טבעית מתקדמות על מכשירים עם משאבים מוגבלים. מדריך זה מתמקד באסטרטגיות פריסה מבוססות קונטיינרים עבור Phi-4-mini-instruct של Microsoft, מודל הסקה מתקדם שמאזן בין יכולת ליעילות.

### מודל נבחר: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 מיליארד פרמטרים)**: מודל קל משקל חדשני של Microsoft, מותאם להוראות, שתוכנן עבור סביבות עם מגבלות זיכרון/חישוב, עם יכולות יוצאות דופן ב:
- **הסקה מתמטית וחישובים מורכבים**
- **יצירת קוד, איתור באגים וניתוח**
- **פתרון בעיות לוגיות והסקה שלב-אחר-שלב**
- **יישומים חינוכיים הדורשים הסברים מפורטים**
- **קריאה לפונקציות ואינטגרציה עם כלים**

כחלק מקטגוריית "SLMs קטנים" (1.5B - 13.9B פרמטרים), Phi-4-mini משיג איזון אופטימלי בין יכולת הסקה ליעילות משאבים.

### יתרונות פריסת Phi-4-mini בקונטיינרים

- **יעילות תפעולית**: הסקה מהירה למשימות עם דרישות חישוב נמוכות
- **גמישות בפריסה**: יכולות AI על המכשיר עם פרטיות משופרת באמצעות עיבוד מקומי
- **חסכוניות**: עלויות תפעול מופחתות בהשוואה למודלים גדולים יותר תוך שמירה על איכות
- **בידוד**: הפרדה נקייה בין מופעי מודל וסביבות ביצוע מאובטחות
- **יכולת הרחבה**: הרחבה אופקית קלה להגדלת תפוקת ההסקה

## מטרות למידה

בסיום מדריך זה, תוכלו:

- לפרוס ולייעל את Phi-4-mini-instruct בסביבות מבוססות קונטיינרים שונות
- ליישם אסטרטגיות כימות ודחיסה מתקדמות עבור תרחישי פריסה שונים
- להגדיר תזמור קונטיינרים מוכן לייצור עבור עומסי עבודה של הסקה
- להעריך ולבחור מסגרות פריסה מתאימות בהתבסס על דרישות מקרה שימוש ספציפיות
- ליישם שיטות עבודה מומלצות לאבטחה, ניטור והרחבה עבור פריסות SLM מבוססות קונטיינרים

## הבנת סיווג Phi-4-mini

### מפרטי מודל

**פרטים טכניים:**
- **פרמטרים**: 3.8 מיליארד (קטגוריית SLM קטנים)
- **ארכיטקטורה**: Transformer צפוף מבוסס דקודר בלבד עם grouped-query attention
- **אורך הקשר**: 128K טוקנים (32K מומלץ לביצועים מיטביים)
- **אוצר מילים**: 200K טוקנים עם תמיכה רב-לשונית
- **נתוני אימון**: 5T טוקנים של תוכן עשיר בהסקה איכותית

### דרישות משאבים

| סוג פריסה | מינימום RAM | RAM מומלץ | VRAM (GPU) | אחסון | שימושים טיפוסיים |
|-----------|-------------|-----------|------------|-------|-------------------|
| **פיתוח** | 6GB | 8GB | - | 8GB | בדיקות מקומיות, אב טיפוס |
| **ייצור CPU** | 8GB | 12GB | - | 10GB | שרתי קצה, פריסה חסכונית |
| **ייצור GPU** | 6GB | 8GB | 4-6GB | 8GB | שירותי הסקה בעלי תפוקה גבוהה |
| **אופטימיזציה לקצה** | 4GB | 6GB | - | 6GB | פריסה מכומתת, שערי IoT |

### יכולות Phi-4-mini

- **מצוינות מתמטית**: פתרון בעיות מתקדמות באריתמטיקה, אלגברה וחשבון דיפרנציאלי ואינטגרלי
- **אינטליגנציה בקוד**: יצירת קוד ב-Python, JavaScript ושפות נוספות עם איתור באגים
- **הסקה לוגית**: פירוק בעיות שלב-אחר-שלב ובניית פתרונות
- **תמיכה חינוכית**: הסברים מפורטים המתאימים ללמידה והוראה
- **קריאה לפונקציות**: תמיכה מובנית באינטגרציה עם כלים ואינטראקציות API

## פריסת קונטיינר vLLM

vLLM מספק תמיכה מצוינת ל-Phi-4-mini-instruct עם ביצועי הסקה אופטימליים ו-APIs תואמי OpenAI, מה שהופך אותו לאידיאלי עבור שירותי הסקה בייצור.

### דוגמאות התחלה מהירה

#### פריסת CPU בסיסית (פיתוח)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### פריסת ייצור מואצת GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### תצורת ייצור

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### בדיקת יכולות הסקה של Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## פריסת קונטיינר Ollama

Ollama מספק תמיכה מצוינת ל-Phi-4-mini-instruct עם פריסה וניהול פשוטים, מה שהופך אותו לאידיאלי עבור פיתוח ופריסות ייצור מאוזנות.

### התקנה מהירה

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### תצורת ייצור

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### אופטימיזציה של מודל ווריאנטים

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### דוגמאות שימוש ב-API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine עם ONNX Runtime

ONNX Runtime מספק ביצועים מיטביים לפריסת קצה של Phi-4-mini-instruct עם אופטימיזציה מתקדמת ותאימות בין-פלטפורמות.

### התקנה בסיסית

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### יישום שרת פשוט

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### סקריפט המרת מודל

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### תצורת ייצור

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### בדיקת פריסת ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## מסגרת השוואה

### השוואת מסגרות עבור Phi-4-mini

| תכונה | vLLM | Ollama | ONNX Runtime |
|-------|------|--------|--------------|
| **מורכבות התקנה** | בינונית | קלה | מורכבת |
| **ביצועים (GPU)** | מצוינים (~25 טוקנים/שנייה) | טובים מאוד (~20 טוקנים/שנייה) | טובים (~15 טוקנים/שנייה) |
| **ביצועים (CPU)** | טובים (~8 טוקנים/שנייה) | טובים מאוד (~12 טוקנים/שנייה) | מצוינים (~15 טוקנים/שנייה) |
| **שימוש בזיכרון** | 8-12GB | 6-10GB | 4-8GB |
| **תאימות API** | תואם OpenAI | REST מותאם אישית | FastAPI מותאם אישית |
| **קריאה לפונקציות** | ✅ מובנה | ✅ נתמך | ⚠️ יישום מותאם אישית |
| **תמיכה בכימות** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | כימות ONNX |
| **מוכן לייצור** | ✅ מצוין | ✅ טוב מאוד | ✅ טוב |
| **פריסת קצה** | טובה | מצוינת | יוצאת מן הכלל |

## משאבים נוספים

### תיעוד רשמי
- **כרטיס מודל Phi-4 של Microsoft**: מפרטים מפורטים והנחיות שימוש
- **תיעוד vLLM**: אפשרויות תצורה ואופטימיזציה מתקדמות
- **ספריית מודלים Ollama**: מודלים קהילתיים ודוגמאות התאמה אישית
- **מדריכי ONNX Runtime**: אסטרטגיות אופטימיזציה ופריסה

### כלי פיתוח
- **Hugging Face Transformers**: לאינטראקציה והתאמה אישית של מודלים
- **מפרט API של OpenAI**: לבדיקת תאימות vLLM
- **שיטות עבודה מומלצות ב-Docker**: אבטחת קונטיינרים ואופטימיזציה
- **פריסת Kubernetes**: דפוסי תזמור להרחבה בייצור

### משאבי למידה
- **Benchmarking ביצועי SLM**: מתודולוגיות ניתוח השוואתי
- **פריסת Edge AI**: שיטות עבודה מומלצות לסביבות עם משאבים מוגבלים
- **אופטימיזציה למשימות הסקה**: אסטרטגיות הנחיה לבעיות מתמטיות ולוגיות
- **אבטחת קונטיינרים**: שיטות חיזוק לפריסות מודלים AI

## תוצאות למידה

לאחר השלמת מודול זה, תוכלו:

1. לפרוס את מודל Phi-4-mini-instruct בסביבות מבוססות קונטיינרים באמצעות מסגרות שונות
2. להגדיר ולייעל פריסות SLM עבור סביבות חומרה שונות
3. ליישם שיטות עבודה מומלצות לאבטחת פריסות AI מבוססות קונטיינרים
4. להשוות ולבחור מסגרות פריסה מתאימות בהתבסס על דרישות מקרה שימוש ספציפיות
5. ליישם אסטרטגיות ניטור והרחבה עבור שירותי SLM ברמת ייצור

## מה הלאה

- חזרה ל-[מודול 1](../Module01/README.md)
- חזרה ל-[מודול 2](../Module02/README.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור הסמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.