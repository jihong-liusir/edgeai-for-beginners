<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T16:32:34+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "he"
}
-->
# סעיף 1: למידה מתקדמת של SLM - יסודות ואופטימיזציה

מודלים לשוניים קטנים (SLMs) מייצגים התקדמות חשובה ב-EdgeAI, ומאפשרים יכולות עיבוד שפה טבעית מתקדמות על מכשירים עם משאבים מוגבלים. הבנה כיצד לפרוס, לאופטם ולהשתמש ב-SLMs בצורה יעילה היא חיונית לבניית פתרונות AI מבוססי קצה.

## מבוא

בשיעור זה נחקור את מודלים לשוניים קטנים (SLMs) ואת אסטרטגיות היישום המתקדמות שלהם. נסקור את המושגים הבסיסיים של SLMs, גבולות הפרמטרים והסיווגים שלהם, טכניקות אופטימיזציה ואסטרטגיות פריסה מעשיות לסביבות מחשוב קצה.

## מטרות למידה

בסיום השיעור, תוכלו:

- 🔢 להבין את גבולות הפרמטרים והסיווגים של מודלים לשוניים קטנים.
- 🛠️ לזהות טכניקות אופטימיזציה מרכזיות לפריסת SLM על מכשירי קצה.
- 🚀 ללמוד ליישם אסטרטגיות מתקדמות של כימות ודחיסה עבור SLMs.

## הבנת גבולות הפרמטרים וסיווגי SLM

מודלים לשוניים קטנים (SLMs) הם מודלים AI שנועדו לעבד, להבין וליצור תוכן שפה טבעית עם מספר פרמטרים קטן משמעותית בהשוואה למודלים גדולים. בעוד שמודלים לשוניים גדולים (LLMs) מכילים מאות מיליארדים עד טריליונים של פרמטרים, SLMs מתוכננים במיוחד ליעילות ולפריסה בקצה.

מסגרת הסיווג של הפרמטרים עוזרת לנו להבין את הקטגוריות השונות של SLMs ואת השימושים המתאימים להם. סיווג זה חיוני לבחירת המודל הנכון עבור תרחישי מחשוב קצה ספציפיים.

### מסגרת סיווג פרמטרים

הבנת גבולות הפרמטרים עוזרת בבחירת מודלים מתאימים לתרחישי מחשוב קצה שונים:

- **🔬 מיקרו SLMs**: 100M - 1.4B פרמטרים (קל משקל במיוחד למכשירים ניידים)
- **📱 SLMs קטנים**: 1.5B - 13.9B פרמטרים (ביצועים ויעילות מאוזנים)
- **⚖️ SLMs בינוניים**: 14B - 30B פרמטרים (מתקרבים ליכולות LLM תוך שמירה על יעילות)

הגבול המדויק נשאר גמיש בקהילת המחקר, אך רוב העוסקים בתחום רואים במודלים עם פחות מ-30 מיליארד פרמטרים כ"קטנים", כאשר חלק מהמקורות מציבים את הסף אפילו נמוך יותר, ב-10 מיליארד פרמטרים.

### יתרונות מרכזיים של SLMs

ל-SLMs יש מספר יתרונות בסיסיים שהופכים אותם לאידיאליים ליישומי מחשוב קצה:

**יעילות תפעולית**: SLMs מספקים זמני הסקה מהירים יותר בשל מספר פרמטרים קטן יותר לעיבוד, מה שהופך אותם לאידיאליים ליישומים בזמן אמת. הם דורשים פחות משאבים חישוביים, מאפשרים פריסה על מכשירים עם משאבים מוגבלים, צורכים פחות אנרגיה ושומרים על טביעת פחמן מופחתת.

**גמישות בפריסה**: מודלים אלו מאפשרים יכולות AI על המכשיר ללא צורך בחיבור לאינטרנט, משפרים פרטיות ואבטחה באמצעות עיבוד מקומי, ניתנים להתאמה ליישומים ספציפיים לתחום ומתאימים לסביבות מחשוב קצה שונות.

**עלות אפקטיבית**: SLMs מציעים עלויות נמוכות יותר לאימון ולפריסה בהשוואה ל-LLMs, עם עלויות תפעול מופחתות ודרישות רוחב פס נמוכות ליישומי קצה.

## אסטרטגיות מתקדמות לרכישת מודלים

### מערכת Hugging Face

Hugging Face משמשת כמרכז הראשי לגילוי וגישה ל-SLMs מתקדמים. הפלטפורמה מספקת משאבים מקיפים לגילוי מודלים ולפריסה:

**תכונות גילוי מודלים**: הפלטפורמה מציעה סינון מתקדם לפי מספר פרמטרים, סוג רישיון ומדדי ביצועים. משתמשים יכולים לגשת לכלי השוואת מודלים זה לצד זה, תוצאות הערכה וביצועים בזמן אמת ודמואים WebGPU לבדיקות מיידיות.

**אוספי SLM מותאמים**: מודלים פופולריים כוללים Phi-4-mini-3.8B למשימות הסקה מתקדמות, סדרת Qwen3 (0.6B/1.7B/4B) ליישומים רב-לשוניים, Google Gemma3 למשימות כלליות יעילות ומודלים ניסיוניים כמו BitNET לפריסה מדויקת במיוחד. הפלטפורמה כוללת גם אוספים מונעי קהילה עם מודלים מותאמים לתחומים ספציפיים וגרסאות מותאמות מראש ומכוונות הוראות לשימושים שונים.

### קטלוג מודלים של Azure AI Foundry

קטלוג המודלים של Azure AI Foundry מספק גישה ברמה ארגונית ל-SLMs עם יכולות אינטגרציה משופרות:

**אינטגרציה ארגונית**: הקטלוג כולל מודלים הנמכרים ישירות על ידי Azure עם תמיכה ברמה ארגונית ו-SLAs, כולל Phi-4-mini-3.8B ליכולות הסקה מתקדמות ו-Llama 3-8B לפריסה בייצור. הוא כולל גם מודלים כמו Qwen3 8B ממודל קוד פתוח של צד שלישי אמין.

**יתרונות ארגוניים**: כלים מובנים לכוונון עדין, תצפיתיות ו-AI אחראי משולבים עם Provisioned Throughput גמיש בין משפחות מודלים. תמיכה ישירה של Microsoft עם SLAs ארגוניים, תכונות אבטחה וציות משולבות וזרימות עבודה מקיפות לפריסה משפרים את החוויה הארגונית.

## טכניקות מתקדמות לכימות ואופטימיזציה

### מסגרת אופטימיזציה Llama.cpp

Llama.cpp מספקת טכניקות כימות מתקדמות ליעילות מרבית בפריסת קצה:

**שיטות כימות**: המסגרת תומכת ברמות כימות שונות כולל Q4_0 (כימות 4 ביט עם הפחתת גודל מצוינת - אידיאלי לפריסת Qwen3-0.6B במובייל), Q5_1 (כימות 5 ביט המאזן איכות ודחיסה - מתאים ל-Phi-4-mini-3.8B בהסקת קצה) ו-Q8_0 (כימות 8 ביט לאיכות קרובה למקור - מומלץ לשימוש בייצור של Google Gemma3). BitNET מייצג את החזית עם כימות 1 ביט לתרחישי דחיסה קיצוניים.

**יתרונות יישום**: הסקה מותאמת ל-CPU עם האצת SIMD מספקת טעינת מודלים יעילה בזיכרון וביצוע. תאימות בין פלטפורמות על פני ארכיטקטורות x86, ARM ו-Apple Silicon מאפשרת יכולות פריסה ללא תלות בחומרה.

**דוגמה ליישום מעשי**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**השוואת טביעת זיכרון**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Optimization Suite

Microsoft Olive מציעה זרימות עבודה מקיפות לאופטימיזציית מודלים המיועדות לסביבות ייצור:

**טכניקות אופטימיזציה**: הסוויטה כוללת כימות דינמי לבחירת דיוק אוטומטית (יעיל במיוחד עם מודלי סדרת Qwen3), אופטימיזציית גרפים ומיזוג מפעילים (מותאם לארכיטקטורת Google Gemma3), אופטימיזציות ספציפיות לחומרה עבור CPU, GPU ו-NPU (עם תמיכה מיוחדת ב-Phi-4-mini-3.8B על מכשירי ARM) וזרימות עבודה אופטימיזציה רב-שלביות. מודלי BitNET דורשים זרימות עבודה מיוחדות לכימות 1 ביט במסגרת Olive.

**אוטומציה של זרימות עבודה**: ביצועי אוטומציה של השוואות בין גרסאות אופטימיזציה מבטיחים שימור מדדי איכות במהלך האופטימיזציה. אינטגרציה עם מסגרות ML פופולריות כמו PyTorch ו-ONNX מספקת יכולות אופטימיזציה לפריסה בענן ובקצה.

**דוגמה ליישום מעשי**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### מסגרת Apple MLX

Apple MLX מספקת אופטימיזציה מקורית המיועדת במיוחד למכשירי Apple Silicon:

**אופטימיזציה ל-Apple Silicon**: המסגרת משתמשת בארכיטקטורת זיכרון מאוחדת עם אינטגרציה של Metal Performance Shaders, הסקה אוטומטית בדיוק מעורב (יעיל במיוחד עם Google Gemma3) וניצול אופטימלי של רוחב פס זיכרון. Phi-4-mini-3.8B מציג ביצועים יוצאי דופן על שבבי סדרת M, בעוד Qwen3-1.7B מספק איזון אופטימלי לפריסות MacBook Air.

**תכונות פיתוח**: תמיכה ב-API של Python ו-Swift עם פעולות מערך תואמות NumPy, יכולות דיפרנציאציה אוטומטית ואינטגרציה חלקה עם כלי פיתוח של Apple מספקות סביבת פיתוח מקיפה.

**דוגמה ליישום מעשי**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## אסטרטגיות פריסה והסקה בייצור

### Ollama: פריסה מקומית פשוטה

Ollama מפשטת את פריסת SLM עם תכונות מוכנות לארגונים לסביבות מקומיות וקצה:

**יכולות פריסה**: התקנה והפעלה של מודלים בפקודה אחת עם משיכת מודלים אוטומטית ואחסון במטמון. תמיכה ב-Phi-4-mini-3.8B, כל סדרת Qwen3 (0.6B/1.7B/4B) ו-Google Gemma3 עם REST API לאינטגרציה יישומית וניהול והחלפת מודלים מרובים. מודלי BitNET דורשים תצורות בנייה ניסיוניות לתמיכה בכימות 1 ביט.

**תכונות מתקדמות**: תמיכה בכוונון עדין של מודלים מותאמים אישית, יצירת Dockerfile לפריסה במיכלים, האצת GPU עם זיהוי אוטומטי ואפשרויות כימות ואופטימיזציה של מודלים מספקות גמישות פריסה מקיפה.

### VLLM: הסקה בעלת ביצועים גבוהים

VLLM מספקת אופטימיזציית הסקה ברמה ייצורית לתרחישי תפוקה גבוהה:

**אופטימיזציות ביצועים**: PagedAttention לחישוב יעיל של תשומת לב בזיכרון (מועיל במיוחד לארכיטקטורת הטרנספורמר של Phi-4-mini-3.8B), אצווה דינמית לאופטימיזציית תפוקה (מותאם לעיבוד מקבילי של סדרת Qwen3), פרלליזם טנסור להרחבה רב-GPU (תמיכה ב-Google Gemma3) ופענוח ספקולטיבי להפחתת זמן השהיה. מודלי BitNET דורשים ליבות הסקה מיוחדות לפעולות 1 ביט.

**אינטגרציה ארגונית**: נקודות קצה API תואמות OpenAI, תמיכה בפריסה ב-Kubernetes, אינטגרציה של ניטור ותצפיתיות ויכולות התאמה אוטומטית מספקות פתרונות פריסה ברמה ארגונית.

### Foundry Local: פתרון הקצה של Microsoft

Foundry Local מספק יכולות פריסה מקיפות לקצה עבור סביבות ארגוניות:

**תכונות מחשוב קצה**: עיצוב ארכיטקטורה "קודם לא מקוון" עם אופטימיזציה למגבלות משאבים, ניהול רישום מודלים מקומי ויכולות סנכרון קצה-לענן מבטיחים פריסה אמינה בקצה.

**אבטחה וציות**: עיבוד נתונים מקומי לשימור פרטיות, בקרות אבטחה ארגוניות, רישום ביקורת ודיווח ציות וניהול גישה מבוסס תפקידים מספקים אבטחה מקיפה לפריסות קצה.

## שיטות עבודה מומלצות ליישום SLM

### הנחיות לבחירת מודלים

בעת בחירת SLMs לפריסת קצה, שקלו את הגורמים הבאים:

**שיקולי מספר פרמטרים**: בחרו מיקרו SLMs כמו Qwen3-0.6B ליישומי מובייל קלילים במיוחד, SLMs קטנים כמו Qwen3-1.7B או Google Gemma3 לתרחישי ביצועים מאוזנים ו-SLMs בינוניים כמו Phi-4-mini-3.8B או Qwen3-4B כאשר מתקרבים ליכולות LLM תוך שמירה על יעילות. מודלי BitNET מציעים דחיסה אולטרה-ניסיונית ליישומי מחקר ספציפיים.

**התאמה ליישום**: התאימו את יכולות המודל לדרישות יישום ספציפיות, תוך התחשבות בגורמים כמו איכות תגובה, מהירות הסקה, מגבלות זיכרון ודרישות פעולה לא מקוונות.

### בחירת אסטרטגיית אופטימיזציה

**גישה לכימות**: בחרו רמות כימות מתאימות בהתבסס על דרישות איכות ומגבלות חומרה. שקלו Q4_0 לדחיסה מרבית (אידיאלי לפריסת Qwen3-0.6B במובייל), Q5_1 לאיזון איכות-דחיסה (מתאים ל-Phi-4-mini-3.8B ו-Google Gemma3) ו-Q8_0 לשימור איכות קרובה למקור (מומלץ לסביבות ייצור של Qwen3-4B). כימות 1 ביט של BitNET מייצג את חזית הדחיסה הקיצונית ליישומים מיוחדים.

**בחירת מסגרת**: בחרו מסגרות אופטימיזציה בהתבסס על חומרה יעד ודרישות פריסה. השתמשו ב-Llama.cpp לפריסה מותאמת ל-CPU, Microsoft Olive לזרימות עבודה אופטימיזציה מקיפות ו-Apple MLX למכשירי Apple Silicon.

## דוגמאות מודלים מעשיות ושימושים

### תרחישי פריסה בעולם האמיתי

**יישומי מובייל**: Qwen3-0.6B מצטיין ביישומי צ'אטבוט בסמארטפונים עם טביעת זיכרון מינימלית, בעוד Google Gemma3 מספק ביצועים מאוזנים לכלי חינוכי מבוסס טאבלט. Phi-4-mini-3.8B מציע יכולות הסקה מעולות ליישומי פרודוקטיביות במובייל.

**מחשוב שולחני וקצה**: Qwen3-1.7B מספק ביצועים אופטימליים ליישומי עוזר שולחני, Phi-4-mini-3.8B מספק יכולות יצירת קוד מתקדמות לכלי מפתחים ו-Qwen3-4B מאפשר ניתוח מסמכים מתוחכם בסביבות תחנת עבודה.

**מחקר וניסויים**: מודלי BitNET מאפשרים חקר הסקה מדויקת במיוחד למחקר אקדמי ויישומי הוכחת רעיון הדורשים מגבלות משאבים קיצוניות.

### מדדי ביצועים והשוואות

**מהירות הסקה**: Qwen3-0.6B משיג זמני הסקה מהירים ביותר על CPUs ניידים, Google Gemma3 מספק יחס מהירות-איכות מאוזן ליישומים כלליים, Phi-4-mini-3.8B מציע מהירות הסקה מעולה למשימות מורכבות ו-BitNET מספק תפוקה מקסימלית תיאורטית עם חומרה מיוחדת.

**דרישות זיכרון**: טביעות הזיכרון של המודלים נעות בין Qwen3-0.6B (פחות מ-1GB בכימות) ל-Phi-4-mini-3.8B (בערך 3-4GB בכימות), עם BitNET שמגיע לטביעות זיכרון מתחת ל-500MB בתצורות ניסיוניות.

## אתגרים ושיקולים

### פשרות ביצועים

פריסת SLM כרוכה בשיקול זהיר של פשרות בין גודל המודל, מהירות הסקה ואיכות הפלט. לדוגמה, בעוד Qwen3-0.6B מציע מהירות ויעילות יוצאות דופן, Phi-4-mini-3.8B מספק יכולות הסקה מעולות במחיר של דרישות משאבים מוגברות. Google Gemma3 מוצא איזון מתאים לרוב היישומים הכלליים.

### תאימות חומרה

למכשירי קצה שונים יש יכולות ומגבלות משתנות. Qwen3-0.6B פועל ביעילות על מעבדי ARM בסיסיים, Google Gemma3 דורש משאבים חישוביים מתונים ו-Phi-4-mini-3.8B נהנה מחומרת קצה מתקדמת יותר. מודלי BitNET דורשים חומרה או יישומים תוכנתיים מיוחדים לפעולות 1 ביט אופטימליות.

### אבטחה ופרטיות

בעוד ש-SLMs

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.