<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-18T13:11:24+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "he"
}
-->
# פרק 1: למידה מתקדמת עם SLM - יסודות ואופטימיזציה

מודלים לשוניים קטנים (SLMs) מייצגים התקדמות חשובה ב-EdgeAI, ומאפשרים יכולות עיבוד שפה טבעית מתקדמות על גבי מכשירים עם משאבים מוגבלים. הבנת הדרכים לפריסה, אופטימיזציה ושימוש יעיל ב-SLMs היא חיונית לבניית פתרונות AI מעשיים מבוססי קצה.

## מבוא

בשיעור זה נחקור את המודלים הלשוניים הקטנים (SLMs) ואת אסטרטגיות היישום המתקדמות שלהם. נסקור את המושגים הבסיסיים של SLMs, גבולות הפרמטרים והסיווגים שלהם, טכניקות אופטימיזציה ואסטרטגיות פריסה מעשיות לסביבות מחשוב קצה.

## מטרות למידה

בסיום השיעור, תוכלו:

- 🔢 להבין את גבולות הפרמטרים והסיווגים של מודלים לשוניים קטנים.
- 🛠️ לזהות טכניקות אופטימיזציה מרכזיות לפריסת SLM על מכשירי קצה.
- 🚀 ללמוד ליישם אסטרטגיות מתקדמות של כימות ודחיסה עבור SLMs.

## הבנת גבולות הפרמטרים והסיווגים של SLM

מודלים לשוניים קטנים (SLMs) הם מודלים של AI שתוכננו לעיבוד, הבנה ויצירת תוכן שפה טבעית עם מספר פרמטרים קטן משמעותית בהשוואה למודלים הגדולים. בעוד שמודלים לשוניים גדולים (LLMs) מכילים מאות מיליארדים עד טריליונים של פרמטרים, SLMs תוכננו במיוחד ליעילות ולפריסה בקצה.

מסגרת סיווג הפרמטרים עוזרת לנו להבין את הקטגוריות השונות של SLMs ואת מקרי השימוש המתאימים להם. סיווג זה הוא קריטי לבחירת המודל הנכון עבור תרחישי מחשוב קצה ספציפיים.

### מסגרת סיווג פרמטרים

הבנת גבולות הפרמטרים מסייעת בבחירת מודלים מתאימים לתרחישי מחשוב קצה שונים:

- **🔬 Micro SLMs**: 100M - 1.4B פרמטרים (קלילים במיוחד למכשירים ניידים)
- **📱 Small SLMs**: 1.5B - 13.9B פרמטרים (איזון בין ביצועים ליעילות)
- **⚖️ Medium SLMs**: 14B - 30B פרמטרים (מתקרבים ליכולות LLM תוך שמירה על יעילות)

הגבול המדויק נותר גמיש בקהילת המחקר, אך רוב העוסקים בתחום רואים במודלים עם פחות מ-30 מיליארד פרמטרים כ"קטנים", כאשר חלק מהמקורות מציבים את הסף אף נמוך יותר, ב-10 מיליארד פרמטרים.

### יתרונות מרכזיים של SLMs

ל-SLMs יש מספר יתרונות בסיסיים שהופכים אותם לאידיאליים ליישומי מחשוב קצה:

**יעילות תפעולית**: SLMs מספקים זמני הסקה מהירים יותר בשל מספר פרמטרים קטן יותר לעיבוד, מה שהופך אותם לאידיאליים ליישומים בזמן אמת. הם דורשים פחות משאבים חישוביים, מאפשרים פריסה על מכשירים עם משאבים מוגבלים, צורכים פחות אנרגיה ושומרים על טביעת פחמן מופחתת.

**גמישות בפריסה**: מודלים אלו מאפשרים יכולות AI על המכשיר ללא צורך בחיבור לאינטרנט, משפרים פרטיות ואבטחה באמצעות עיבוד מקומי, ניתנים להתאמה ליישומים ייעודיים לתחום, ומתאימים לסביבות מחשוב קצה מגוונות.

**חסכוניות**: SLMs מציעים עלויות נמוכות יותר לאימון ולפריסה בהשוואה ל-LLMs, עם עלויות תפעול מופחתות ודרישות רוחב פס נמוכות ליישומי קצה.

## אסטרטגיות מתקדמות לרכישת מודלים

### מערכת Hugging Face

Hugging Face משמשת כמרכז העיקרי לגילוי וגישה ל-SLMs מתקדמים. הפלטפורמה מספקת משאבים מקיפים לגילוי ופריסת מודלים:

**תכונות גילוי מודלים**: הפלטפורמה מציעה סינון מתקדם לפי מספר פרמטרים, סוג רישיון ומדדי ביצועים. משתמשים יכולים לגשת לכלי השוואת מודלים זה לצד זה, תוצאות הערכה וביצועים בזמן אמת, ודמואים מבוססי WebGPU לבדיקה מיידית.

**אוספי SLM מותאמים**: מודלים פופולריים כוללים את Phi-4-mini-3.8B למשימות הסקה מתקדמות, סדרת Qwen3 (0.6B/1.7B/4B) ליישומים רב-לשוניים, Google Gemma3 למשימות כלליות יעילות, ומודלים ניסיוניים כמו BitNET לפריסה מדויקת במיוחד. הפלטפורמה כוללת גם אוספים מונחי קהילה עם מודלים ייעודיים לתחומים ספציפיים וגרסאות מותאמות מראש ומכוונות הוראות לשימושים שונים.

### קטלוג המודלים של Azure AI Foundry

קטלוג המודלים של Azure AI Foundry מספק גישה ברמה ארגונית ל-SLMs עם יכולות אינטגרציה משופרות:

**אינטגרציה ארגונית**: הקטלוג כולל מודלים הנמכרים ישירות על ידי Azure עם תמיכה ברמה ארגונית ו-SLAs, כולל Phi-4-mini-3.8B ליכולות הסקה מתקדמות ו-Llama 3-8B לפריסה בייצור. הוא כולל גם מודלים כמו Qwen3 8B ממקורות קוד פתוח מהימנים.

**יתרונות ארגוניים**: כלים מובנים לכוונון, תצפיתיות ו-AI אחראי משולבים עם Provisioned Throughput גמיש בין משפחות מודלים. תמיכה ישירה של Microsoft עם SLAs ארגוניים, תכונות אבטחה וציות משולבות, וזרימות עבודה מקיפות לפריסה משפרים את חוויית הארגון.

## טכניקות מתקדמות לכימות ואופטימיזציה

### מסגרת האופטימיזציה Llama.cpp

Llama.cpp מספקת טכניקות כימות מתקדמות ליעילות מרבית בפריסת קצה:

**שיטות כימות**: המסגרת תומכת ברמות כימות שונות, כולל Q4_0 (כימות 4 ביט עם הפחתת גודל מצוינת - אידיאלי לפריסת Qwen3-0.6B במובייל), Q5_1 (כימות 5 ביט המאזן בין איכות לדחיסה - מתאים ל-Phi-4-mini-3.8B בהסקת קצה), ו-Q8_0 (כימות 8 ביט לאיכות קרובה למקור - מומלץ לשימוש בייצור עם Google Gemma3). BitNET מייצג את החזית המתקדמת עם כימות 1 ביט לתרחישי דחיסה קיצוניים.

**יתרונות יישום**: הסקה מותאמת ל-CPU עם האצת SIMD מספקת טעינה וביצוע יעילים בזיכרון. תאימות בין-פלטפורמות על פני ארכיטקטורות x86, ARM ו-Apple Silicon מאפשרת יכולות פריסה שאינן תלויות בחומרה.

**דוגמה ליישום מעשי**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**השוואת טביעת זיכרון**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Optimization Suite

Microsoft Olive מציעה זרימות עבודה מקיפות לאופטימיזציית מודלים המיועדות לסביבות ייצור:

**טכניקות אופטימיזציה**: הסוויטה כוללת כימות דינמי לבחירת דיוק אוטומטית (יעיל במיוחד עם מודלי סדרת Qwen3), אופטימיזציית גרפים ומיזוג אופרטורים (מותאם לארכיטקטורת Google Gemma3), אופטימיזציות ייעודיות לחומרה עבור CPU, GPU ו-NPU (עם תמיכה מיוחדת ב-Phi-4-mini-3.8B על מכשירי ARM), וצינורות אופטימיזציה רב-שלביים. מודלי BitNET דורשים זרימות עבודה מיוחדות לכימות 1 ביט במסגרת Olive.

**אוטומציה של זרימת עבודה**: בדיקות ביצועים אוטומטיות על פני גרסאות אופטימיזציה מבטיחות שימור מדדי איכות במהלך האופטימיזציה. אינטגרציה עם מסגרות ML פופולריות כמו PyTorch ו-ONNX מספקת יכולות אופטימיזציה לפריסה בענן ובקצה.

**דוגמה ליישום מעשי**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### מסגרת Apple MLX

Apple MLX מספקת אופטימיזציה מקורית המיועדת במיוחד למכשירי Apple Silicon:

**אופטימיזציה ל-Apple Silicon**: המסגרת מנצלת ארכיטקטורת זיכרון מאוחדת עם אינטגרציה של Metal Performance Shaders, הסקה אוטומטית בדיוק מעורב (יעיל במיוחד עם Google Gemma3), וניצול אופטימלי של רוחב פס זיכרון. Phi-4-mini-3.8B מציג ביצועים יוצאי דופן על שבבי סדרת M, בעוד Qwen3-1.7B מספק איזון אופטימלי לפריסות MacBook Air.

**תכונות פיתוח**: תמיכה ב-API של Python ו-Swift עם פעולות מערך תואמות NumPy, יכולות דיפרנציאציה אוטומטית, ואינטגרציה חלקה עם כלי פיתוח של Apple מספקות סביבת פיתוח מקיפה.

**דוגמה ליישום מעשי**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## אסטרטגיות פריסה והסקה בייצור

### Ollama: פריסה מקומית פשוטה

Ollama מפשטת את פריסת ה-SLM עם תכונות מוכנות לארגון לסביבות מקומיות וקצה:

**יכולות פריסה**: התקנה והפעלה של מודלים בפקודה אחת עם משיכת מודלים אוטומטית ואחסון במטמון. תמיכה ב-Phi-4-mini-3.8B, כל סדרת Qwen3 (0.6B/1.7B/4B), ו-Google Gemma3 עם REST API לאינטגרציה יישומית וניהול והחלפת מודלים מרובים. מודלי BitNET דורשים תצורות בנייה ניסיוניות לתמיכה בכימות 1 ביט.

**תכונות מתקדמות**: תמיכה בכוונון מודלים מותאם אישית, יצירת Dockerfile לפריסה במכולות, האצת GPU עם זיהוי אוטומטי, ואפשרויות כימות ואופטימיזציה של מודלים מספקות גמישות פריסה מקיפה.

### VLLM: הסקה בעלת ביצועים גבוהים

VLLM מספקת אופטימיזציית הסקה ברמה ייצורית לתרחישים בעלי תפוקה גבוהה:

**אופטימיזציות ביצועים**: PagedAttention לחישוב יעיל בזיכרון של תשומת לב (מועיל במיוחד לארכיטקטורת הטרנספורמר של Phi-4-mini-3.8B), אצווה דינמית לאופטימיזציית תפוקה (מותאם לעיבוד מקבילי של סדרת Qwen3), פרלליזם טנסורי להרחבה על פני מספר GPUs (תמיכה ב-Google Gemma3), ופענוח ספקולטיבי להפחתת השהיה. מודלי BitNET דורשים ליבות הסקה מיוחדות לפעולות 1 ביט.

**אינטגרציה ארגונית**: נקודות קצה API תואמות OpenAI, תמיכה בפריסה ב-Kubernetes, אינטגרציה לניטור ותצפיתיות, ויכולות סקלאביליות אוטומטיות מספקות פתרונות פריסה ברמה ארגונית.

### Foundry Local: פתרון הקצה של Microsoft

Foundry Local מספק יכולות פריסה בקצה מקיפות לסביבות ארגוניות:

**תכונות מחשוב קצה**: עיצוב ארכיטקטורה "קודם כל לא מקוון" עם אופטימיזציה למגבלות משאבים, ניהול רישום מודלים מקומי, ויכולות סנכרון בין הקצה לענן מבטיחים פריסה אמינה בקצה.

**אבטחה וציות**: עיבוד נתונים מקומי לשמירה על פרטיות, בקרות אבטחה ארגוניות, רישום ביקורת ודיווח ציות, וניהול גישה מבוסס תפקידים מספקים אבטחה מקיפה לפריסות קצה.

## שיטות עבודה מומלצות ליישום SLM

### הנחיות לבחירת מודלים

בעת בחירת SLMs לפריסה בקצה, שקלו את הגורמים הבאים:

**שיקולי מספר פרמטרים**: בחרו ב-Micro SLMs כמו Qwen3-0.6B ליישומי מובייל קלילים במיוחד, Small SLMs כמו Qwen3-1.7B או Google Gemma3 לתרחישי ביצועים מאוזנים, ו-Medium SLMs כמו Phi-4-mini-3.8B או Qwen3-4B כאשר מתקרבים ליכולות LLM תוך שמירה על יעילות. מודלי BitNET מציעים דחיסה אולטרה-ניסיונית ליישומי מחקר ספציפיים.

**התאמה למקרי שימוש**: התאימו את יכולות המודל לדרישות היישום הספציפיות, תוך התחשבות בגורמים כמו איכות תגובה, מהירות הסקה, מגבלות זיכרון ודרישות פעולה לא מקוונת.

### בחירת אסטרטגיית אופטימיזציה

**גישה לכימות**: בחרו רמות כימות מתאימות בהתבסס על דרישות איכות ומגבלות חומרה. שקלו Q4_0 לדחיסה מרבית (אידיאלי לפריסת Qwen3-0.6B במובייל), Q5_1 לאיזון בין איכות לדחיסה (מתאים ל-Phi-4-mini-3.8B ו-Google Gemma3), ו-Q8_0 לשימור איכות קרובה למקור (מומלץ לסביבות ייצור עם Qwen3-4B). כימות 1 ביט של BitNET מייצג את חזית הדחיסה הקיצונית ליישומים מיוחדים.

**בחירת מסגרת**: בחרו מסגרות אופטימיזציה בהתבסס על חומרה יעד ודרישות פריסה. השתמשו ב-Llama.cpp לפריסה מותאמת ל-CPU, ב-Microsoft Olive לזרימות עבודה מקיפות לאופטימיזציה, וב-Apple MLX למכשירי Apple Silicon.

## דוגמאות מודלים מעשיות ומקרי שימוש

### תרחישי פריסה בעולם האמיתי

**יישומי מובייל**: Qwen3-0.6B מצטיין ביישומי צ'אטבוט בסמארטפונים עם טביעת זיכרון מינימלית, בעוד ש-Google Gemma3 מספק ביצועים מאוזנים לכלי חינוך מבוססי טאבלט. Phi-4-mini-3.8B מציע יכולות הסקה מתקדמות ליישומי פרודוקטיביות במובייל.

**מחשבים שולחניים ומחשוב קצה**: Qwen3-1.7B מספק ביצועים אופטימליים ליישומי עוזר שולחני, Phi-4-mini-3.8B מספק יכולות מתקדמות ליצירת קוד לכלי מפתחים, ו-Qwen3-4B מאפשר ניתוח מסמכים מתוחכם בסביבות תחנות עבודה.

**מחקר וניסויים**: מודלי BitNET מאפשרים חקר הסקה בדיוק נמוך במיוחד למחקר אקדמי ויישומי הוכחת היתכנות הדורשים מגבלות משאבים קיצוניות.

### מדדי ביצועים והשוואות

**מהירות הסקה**: Qwen3-0.6B משיג זמני הסקה מהירים ביותר על מעבדי מובייל, Google Gemma3 מספק יחס מהירות-איכות מאוזן ליישומים כלליים, Phi-4-mini-3.8B מציע מהירות הסקה מעולה למשימות מורכבות, ו-BitNET מספק תפוקה מקסימלית תיאורטית עם חומרה מיוחדת.

**דרישות זיכרון**: טביעות הזיכרון של המודלים נעות בין Qwen3-0.6B (פחות מ-1GB בכימות) ל-Phi-4-mini-3.8B (כ-3-4GB בכימות), כאשר BitNET משיג טביעות זיכרון מתחת ל-500MB בתצורות ניסיוניות.

## אתגרים ושיקולים

### פשרות ביצועים

פריסת SLM כרוכה בשיקול זהיר של פשרות בין גודל המודל, מהירות ההסקה ואיכות הפלט. לדוגמה, בעוד ש-Qwen3-0.6B מציע מהירות ויעילות יוצאות דופן, Phi-4-mini-3.8B מספק יכולות הסקה מעולות במחיר של דרישות משאבים מוגברות. Google Gemma3 מהווה איזון מתאים לרוב היישומים הכלליים.

### תאימות חומרה

למכשירי קצה שונים יש יכולות ומגבלות משתנות. Qwen3-0.6B פועל ביעילות על מעבדי ARM בסיסיים, Google Gemma3 דורש משאבים חישוביים מתונים, ו-Phi-4-mini-3.8B נהנה מחומרת קצה מתקד

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.