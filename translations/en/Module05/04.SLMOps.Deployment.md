<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-19T01:19:15+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "en"
}
-->
# Section 4: Deployment - Production-Ready Model Implementation

## Overview

This detailed tutorial walks you through the entire process of deploying fine-tuned quantized models using Foundry Local. You'll learn about model conversion, quantization optimization, and deployment configuration from start to finish.

## Prerequisites

Before you begin, make sure you have the following:

- ‚úÖ A fine-tuned ONNX model ready for deployment
- ‚úÖ A Windows or Mac computer
- ‚úÖ Python 3.10 or higher
- ‚úÖ At least 8GB of available RAM
- ‚úÖ Foundry Local installed on your system

## Part 1: Environment Setup

### Installing Required Tools

Open your terminal (Command Prompt for Windows, Terminal for Mac) and execute the following commands in order:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Important Note**: You will also need CMake version 3.31 or newer, which can be downloaded from [cmake.org](https://cmake.org/download/).

## Part 2: Model Conversion and Quantization

### Choosing the Right Format

For fine-tuned small language models, we recommend using the **ONNX format** because it provides:

- üöÄ Improved performance optimization
- üîß Hardware-agnostic deployment
- üè≠ Production-ready capabilities
- üì± Cross-platform compatibility

### Method 1: One-Command Conversion (Recommended)

Use the following command to directly convert your fine-tuned model:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Parameter Explanation:**
- `--model_name_or_path`: Path to your fine-tuned model
- `--device cpu`: Use CPU for optimization
- `--precision int4`: Apply INT4 quantization (reduces size by approximately 75%)
- `--output_path`: Specify the output path for the converted model

### Method 2: Configuration File Approach (Advanced Users)

Create a configuration file named `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Then execute:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Quantization Options Comparison

| Precision | File Size | Inference Speed | Model Quality | Recommended Use |
|-----------|-----------|-----------------|---------------|-----------------|
| FP16      | Baseline √ó 0.5 | Fast | Best | High-end hardware |
| INT8      | Baseline √ó 0.25 | Very Fast | Good | Balanced choice |
| INT4      | Baseline √ó 0.125 | Fastest | Acceptable | Resource-limited |

üí° **Recommendation**: Start with INT4 quantization for your initial deployment. If the quality isn't satisfactory, consider trying INT8 or FP16.

## Part 3: Foundry Local Deployment Configuration

### Creating Model Configuration

Navigate to the Foundry Local models directory:

```bash
foundry cache cd ./models/
```

Set up your model directory structure:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Create the `inference_model.json` configuration file within your model directory:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Model-Specific Template Configurations

#### For Qwen Series Models:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Part 4: Model Testing and Optimization

### Verifying Model Installation

Check if Foundry Local recognizes your model:

```bash
foundry cache ls
```

You should see `your-finetuned-model-int4` listed.

### Starting Model Testing

```bash
foundry model run your-finetuned-model-int4
```

### Performance Benchmarking

Track key metrics during testing:

1. **Response Time**: Measure the average time per response
2. **Memory Usage**: Monitor RAM consumption
3. **CPU Utilization**: Observe processor load
4. **Output Quality**: Assess the relevance and coherence of responses

### Quality Validation Checklist

- ‚úÖ The model responds accurately to fine-tuned domain queries
- ‚úÖ The response format matches the expected output structure
- ‚úÖ No memory leaks occur during extended usage
- ‚úÖ Consistent performance across varying input lengths
- ‚úÖ Proper handling of edge cases and invalid inputs

## Summary

Congratulations! You have successfully completed:

- ‚úÖ Fine-tuned model format conversion
- ‚úÖ Model quantization optimization
- ‚úÖ Foundry Local deployment configuration
- ‚úÖ Performance tuning and troubleshooting

---

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we aim for accuracy, please note that automated translations may include errors or inaccuracies. The original document in its native language should be regarded as the authoritative source. For critical information, professional human translation is advised. We are not responsible for any misunderstandings or misinterpretations resulting from the use of this translation.