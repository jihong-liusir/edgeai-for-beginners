<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T00:59:30+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "en"
}
-->
# Session 3: Open-Source Models with Foundry Local

## Overview

This session focuses on integrating open-source models into Foundry Local. You'll learn how to select community models, incorporate Hugging Face content, and use “bring your own model” (BYOM) strategies. Additionally, you'll be introduced to the Model Mondays series for ongoing learning and model exploration.

References:
- Foundry Local documentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- How to compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub repository: https://github.com/microsoft/Foundry-Local

## Learning Objectives
- Learn how to discover and evaluate open-source models for local inference
- Compile and execute selected Hugging Face models within Foundry Local
- Develop strategies for model selection based on accuracy, latency, and resource requirements
- Manage models locally using caching and version control

## Part 1: Model Discovery and Selection (Step-by-step)

Step 1) List the models available in the local catalog  
```cmd
foundry model list
```
  
Step 2) Test two candidate models (auto-downloads on first use)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Step 3) Record basic metrics  
- Evaluate latency (subjectively) and quality using a fixed prompt  
- Monitor memory usage via Task Manager while running each model  

## Part 2: Running Catalog Models via CLI (Step-by-step)

Step 1) Launch a model  
```cmd
foundry model run llama-3.2
```
  
Step 2) Send a test prompt through the OpenAI-compatible endpoint  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Part 3: BYOM – Compile Hugging Face Models (Step-by-step)

Follow the official guide for compiling models. Below is a high-level overview—refer to the Microsoft Learn article for detailed commands and supported configurations.

Step 1) Set up a working directory  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Step 2) Compile a supported Hugging Face model  
- Follow the steps in the Learn documentation to convert and place the compiled ONNX model in your `models` directory  
- Verify with:  
```cmd
foundry cache ls
```
  
You should see the name of your compiled model (e.g., `llama-3.2`).  

Step 3) Execute the compiled model  
```cmd
foundry model run llama-3.2 --verbose
```
  
Notes:  
- Ensure you have enough disk space and RAM for compiling and running models  
- Start with smaller models to validate the process, then scale up  

## Part 4: Practical Model Curation (Step-by-step)

Step 1) Create a `models.json` registry  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Step 2) Write a small selector script  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Part 5: Hands-On Benchmarks (Step-by-step)

Step 1) Conduct a simple latency benchmark  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Step 2) Perform a quality spot-check  
- Use a fixed set of prompts and save the outputs to a CSV/JSON file  
- Manually rate fluency, relevance, and correctness on a scale of 1–5  

## Part 6: Next Steps
- Subscribe to Model Mondays for updates on new models and tips: https://aka.ms/model-mondays  
- Share your findings in your team’s `models.json` registry  
- Get ready for Session 4, which will cover comparisons between LLMs and SLMs, local vs cloud inference, and hands-on demonstrations  

---

