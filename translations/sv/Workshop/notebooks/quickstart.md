<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T13:21:40+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "sv"
}
-->
# Workshop Notebooks - Snabbstartsguide

## Inneh√•llsf√∂rteckning

- [F√∂ruts√§ttningar](../../../../Workshop/notebooks)
- [Initial inst√§llning](../../../../Workshop/notebooks)
- [Session 04: Modellj√§mf√∂relse](../../../../Workshop/notebooks)
- [Session 05: Multi-Agent Orchestrator](../../../../Workshop/notebooks)
- [Session 06: Intentbaserad modellroutning](../../../../Workshop/notebooks)
- [Milj√∂variabler](../../../../Workshop/notebooks)
- [Vanliga kommandon](../../../../Workshop/notebooks)

---

## F√∂ruts√§ttningar

### 1. Installera Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verifiera installation:**
```bash
foundry --version
```

### 2. Installera Python-beroenden

```bash
cd Workshop
pip install -r requirements.txt
```

Eller installera individuellt:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Initial inst√§llning

### Starta Foundry Local-tj√§nsten

**Kr√§vs innan du k√∂r n√•gon notebook:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

F√∂rv√§ntad output:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### Ladda ner och ladda modeller

Notebooks anv√§nder dessa modeller som standard:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Verifiera inst√§llning

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Session 04: Modellj√§mf√∂relse

### Syfte
J√§mf√∂r prestanda mellan Small Language Models (SLM) och Large Language Models (LLM).

### Snabb inst√§llning

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### K√∂ra notebooken

1. **√ñppna** `session04_model_compare.ipynb` i VS Code eller Jupyter
2. **Starta om k√§rnan** (Kernel ‚Üí Restart Kernel)
3. **K√∂r alla celler** i ordning

### Viktig konfiguration

**Standardmodeller:**
- **SLM:** `phi-4-mini` (~4GB RAM, snabbare)
- **LLM:** `qwen2.5-3b` (~3GB RAM, minnesoptimerad)

**Milj√∂variabler (valfritt):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### F√∂rv√§ntad output

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### Anpassning

**Anv√§nd olika modeller:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Anpassad prompt:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Valideringschecklista

- [ ] Cell 12 visar korrekta modeller (phi-4-mini, qwen2.5-3b)
- [ ] Cell 12 visar korrekt endpoint (port 59959)
- [ ] Cell 16 diagnostik godk√§nd (‚úÖ Tj√§nsten k√∂rs)
- [ ] Cell 20 f√∂rkontroll godk√§nd (b√•da modellerna ok)
- [ ] Cell 22 j√§mf√∂relse slutf√∂rd med latensv√§rden
- [ ] Cell 24 validering visar üéâ ALLA KONTROLLER GODK√ÑNDA!

### Tidsuppskattning
- **F√∂rsta k√∂rning:** 5-10 minuter (inklusive modellnedladdningar)
- **Efterf√∂ljande k√∂rningar:** 1-2 minuter

---

## Session 05: Multi-Agent Orchestrator

### Syfte
Demonstrera samarbete mellan flera agenter med Foundry Local SDK - agenter arbetar tillsammans f√∂r att producera f√∂rfinade resultat.

### Snabb inst√§llning

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### K√∂ra notebooken

1. **√ñppna** `session05_agents_orchestrator.ipynb`
2. **Starta om k√§rnan**
3. **K√∂r alla celler** i ordning

### Viktig konfiguration

**Standardinst√§llning (samma modell f√∂r b√•da agenterna):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Avancerad inst√§llning (olika modeller):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Arkitektur

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### F√∂rv√§ntad output

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Utvidgningar

**L√§gg till fler agenter:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Batchtestning:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Tidsuppskattning
- **F√∂rsta k√∂rning:** 3-5 minuter
- **Efterf√∂ljande k√∂rningar:** 1-2 minuter per fr√•ga

---

## Session 06: Intentbaserad modellroutning

### Syfte
Intelligent routning av prompts till specialiserade modeller baserat p√• uppt√§ckt intent.

### Snabb inst√§llning

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Obs:** Session 06 anv√§nder standardm√§ssigt CPU-modeller f√∂r maximal kompatibilitet.

### K√∂ra notebooken

1. **√ñppna** `session06_models_router.ipynb`
2. **Starta om k√§rnan**
3. **K√∂r alla celler** i ordning

### Viktig konfiguration

**Standardkatalog (CPU-modeller):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternativ (GPU-modeller):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Intentdetektion

Routern anv√§nder regex-m√∂nster f√∂r att uppt√§cka intent:

| Intent | Exempel p√• m√∂nster | Routas till |
|--------|--------------------|-------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | Allt annat | phi-4-mini-cpu |

### F√∂rv√§ntad output

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### Anpassning

**L√§gg till anpassad intent:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Aktivera token-sp√•rning:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### V√§xla till GPU-modeller

Om du har 8GB+ VRAM:

1. I **Cell #6**, kommentera bort CPU-katalogen
2. Avkommentera GPU-katalogen
3. Ladda GPU-modeller:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Starta om k√§rnan och k√∂r notebooken igen

### Tidsuppskattning
- **F√∂rsta k√∂rning:** 5-10 minuter (modellinladdning)
- **Efterf√∂ljande k√∂rningar:** 30-60 sekunder per test

---

## Milj√∂variabler

### Global konfiguration

St√§ll in innan du startar Jupyter/VS Code:

**Windows (Kommandotolken):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Konfiguration i notebook

St√§ll in i b√∂rjan av valfri notebook:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Vanliga kommandon

### Tj√§nstehantering

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Modellhantering

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Testa endpoints

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Diagnostiska kommandon

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## B√§sta praxis

### Innan du startar n√•gon notebook

1. **Kontrollera att tj√§nsten k√∂rs:**
   ```bash
   foundry service status
   ```

2. **Verifiera att modeller √§r laddade:**
   ```bash
   foundry model ls
   ```

3. **Starta om notebook-k√§rnan** om du k√∂r om

4. **Rensa alla outputs** f√∂r en ren k√∂rning

### Resurshantering

1. **Anv√§nd CPU-modeller som standard** f√∂r kompatibilitet
2. **V√§xla till GPU-modeller** endast om du har 8GB+ VRAM
3. **St√§ng andra GPU-applikationer** innan du k√∂r
4. **H√•ll tj√§nsten ig√•ng** mellan notebook-sessioner
5. **√ñvervaka resursanv√§ndning** med Aktivitetshanteraren / nvidia-smi

### Fels√∂kning

1. **Kontrollera alltid tj√§nsten f√∂rst** innan du fels√∂ker kod
2. **Starta om k√§rnan** om du ser f√∂r√•ldrad konfiguration
3. **K√∂r diagnostiska celler igen** efter √§ndringar
4. **Kontrollera modellnamn** matchar det som √§r laddat
5. **Verifiera endpoint-port** matchar tj√§nstens status

---

## Snabbreferens: Modellalias

### Vanliga modeller

| Alias | Storlek | B√§st f√∂r | RAM/VRAM | Varianter |
|-------|---------|----------|----------|-----------|
| `phi-4-mini` | ~4B | Allm√§n chatt, sammanfattning | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Kodgenerering, refaktorering | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Allm√§nna uppgifter, effektiv | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Snabb, l√•g resursanv√§ndning | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Klassificering, minimal resursanv√§ndning | 1-2GB | `-cpu`, `-cuda-gpu` |

### Variantnamngivning

- **Basnamn** (t.ex. `phi-4-mini`): V√§ljer automatiskt b√§sta variant f√∂r din h√•rdvara
- **`-cpu`**: CPU-optimerad, fungerar √∂verallt
- **`-cuda-gpu`**: NVIDIA GPU-optimerad, kr√§ver 8GB+ VRAM
- **`-npu`**: Qualcomm NPU-optimerad, kr√§ver NPU-drivrutiner

**Rekommendation:** Anv√§nd basnamn (utan suffix) och l√•t Foundry Local automatiskt v√§lja b√§sta variant.

---

## Framg√•ngsindikatorer

Du √§r redo n√§r du ser:

‚úÖ `foundry service status` visar "running"
‚úÖ `foundry model ls` visar dina n√∂dv√§ndiga modeller
‚úÖ Tj√§nsten √§r tillg√§nglig p√• r√§tt endpoint
‚úÖ H√§lsokontroll returnerar 200 OK
‚úÖ Notebook-diagnostiska celler godk√§nda
‚úÖ Inga anslutningsfel i output

---

## F√• hj√§lp

### Dokumentation
- **Huvudrepository**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLI-referens**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **Fels√∂kning**: Se `troubleshooting.md` i denna katalog

### GitHub Issues
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Senast uppdaterad:** 8 oktober 2025  
**Version:** Workshop Notebooks 2.0

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiska √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.