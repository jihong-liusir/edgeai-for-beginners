<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:33:42+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "sv"
}
-->
# Session 3: Upptäckt och hantering av öppna modeller

## Översikt

Den här sessionen fokuserar på praktisk upptäckt och hantering av modeller med Foundry Local. Du kommer att lära dig att lista tillgängliga modeller, testa olika alternativ och förstå grundläggande prestandaegenskaper. Tillvägagångssättet betonar praktisk utforskning med Foundry CLI för att hjälpa dig välja rätt modeller för dina användningsområden.

## Lärandemål

- Behärska Foundry CLI-kommandon för upptäckt och hantering av modeller
- Förstå modellcache och lokala lagringsmönster
- Lära dig att snabbt testa och jämföra olika modeller
- Etablera praktiska arbetsflöden för modellval och benchmarking
- Utforska det växande ekosystemet av modeller tillgängliga via Foundry Local

## Förkunskaper

- Slutfört Session 1: Kom igång med Foundry Local
- Foundry Local CLI installerad och tillgänglig
- Tillräckligt med lagringsutrymme för nedladdning av modeller (modeller kan variera från 1GB till 20GB+)
- Grundläggande förståelse för modelltyper och användningsområden

## Del 6: Praktisk övning

### Övning: Upptäckt och jämförelse av modeller

Skapa ditt eget skript för modellutvärdering baserat på Exempel 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Din uppgift

1. **Kör skriptet Exempel 03**: `samples\03\list_and_bench.cmd`
2. **Prova olika modeller**: Testa minst 3 olika modeller
3. **Jämför prestanda**: Notera skillnader i hastighet och svarskvalitet
4. **Dokumentera resultat**: Skapa ett enkelt jämförelsediagram

### Exempel på jämförelseformat

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Del 7: Felsökning och bästa praxis

### Vanliga problem och lösningar

**Modellen startar inte:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Otillräckligt minne:**
- Börja med mindre modeller (`phi-4-mini`)
- Stäng andra applikationer
- Uppgradera RAM om du ofta når gränser

**Långsam prestanda:**
- Kontrollera att modellen är helt laddad (kolla detaljerad output)
- Stäng onödiga bakgrundsapplikationer
- Överväg snabbare lagring (SSD)

### Bästa praxis

1. **Börja smått**: Börja med `phi-4-mini` för att validera inställningen
2. **En modell åt gången**: Stoppa tidigare modeller innan du startar nya
3. **Övervaka resurser**: Håll koll på minnesanvändning
4. **Testa konsekvent**: Använd samma frågor för rättvisa jämförelser
5. **Dokumentera resultat**: Anteckna modellprestanda för dina användningsområden

## Del 8: Nästa steg och referenser

### Förberedelse för Session 4

- **Fokus för Session 4**: Optimeringsverktyg och tekniker
- **Förkunskaper**: Bekväm med modellväxling och grundläggande prestandatestning
- **Rekommenderat**: Identifiera 2-3 favoritmodeller från denna session

### Ytterligare resurser

- **[Foundry Local Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Officiell dokumentation
- **[CLI Referens](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Komplett kommandoöversikt
- **[Model Mondays](https://aka.ms/model-mondays)**: Veckovis modellfokus
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Community och problemhantering
- **[Exempel 03: Modellupptäckt](samples/03/README.md)**: Praktiskt exempel på skript

### Viktiga insikter

✅ **Modellupptäckt**: Använd `foundry model list` för att utforska tillgängliga modeller  
✅ **Snabbtestning**: Mönstret `list_and_bench.cmd` för snabb utvärdering  
✅ **Prestandaövervakning**: Grundläggande resursanvändning och svarstidsmätning  
✅ **Modellval**: Praktiska riktlinjer för att välja modeller baserat på användningsområde  
✅ **Cachehantering**: Förstå lagring och städrutiner  

Du har nu praktiska färdigheter för att upptäcka, testa och välja lämpliga modeller för dina AI-applikationer med Foundry Locals enkla CLI-metod.

## Lärandemål
- Upptäcka och utvärdera öppna modeller för lokal inferens
- Kompilera och köra utvalda Hugging Face-modeller inom Foundry Local
- Tillämpa strategier för modellval baserat på noggrannhet, latens och resursbehov
- Hantera modeller lokalt med cache och versionering

## Del 1: Modellupptäckt med Foundry CLI

### Grundläggande kommandon för modellhantering

Foundry CLI erbjuder enkla kommandon för upptäckt och hantering av modeller:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Köra dina första modeller

Börja med populära, vältestade modeller för att förstå prestandaegenskaper:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Obs:** Flaggan `--verbose` ger detaljerad startinformation, inklusive:
- Nedladdningsprogress för modellen (vid första körningen)
- Detaljer om minnesallokering
- Information om tjänstebindning
- Initieringsmetrik för prestanda

### Förstå modellkategorier

**Små språkmodeller (SLMs):**
- `phi-4-mini`: Snabb, effektiv, bra för allmän chatt
- `phi-4`: Mer kapabel version med bättre resonemang

**Medelstora modeller:**
- `qwen2.5-7b`: Utmärkt resonemang och längre kontext
- `deepseek-r1-7b`: Optimerad för kodgenerering

**Större modeller:**
- `llama-3.2`: Metas senaste öppna modell
- `qwen2.5-14b`: Företagsklassat resonemang

## Del 2: Snabb testning och jämförelse av modeller

### Exempel 03-metoden: Enkel lista och test

Baserat på vårt mönster i Exempel 03, här är det minimala arbetsflödet:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testa modellprestanda

När en modell körs, testa den med konsekventa frågor:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell-alternativ för testning

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Del 3: Hantering av modellcache och lagring

### Förstå modellcache

Foundry Local hanterar automatiskt nedladdning och cache av modeller:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Överväganden kring modelllagring

**Typiska modellstorlekar:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Bästa praxis för lagring:**
- Håll 2-3 modeller cachade för snabb växling
- Ta bort oanvända modeller för att frigöra utrymme: `foundry cache clean`
- Övervaka diskanvändning, särskilt på mindre SSD-enheter
- Överväg avvägningar mellan modellstorlek och kapacitet

### Övervakning av modellprestanda

Medan modeller körs, övervaka systemresurser:

**Windows Aktivitetshanteraren:**
- Kontrollera minnesanvändning (modeller hålls laddade i RAM)
- Övervaka CPU-användning under inferens
- Kontrollera disk I/O under initial modellinladdning

**Kommandoradsövervakning:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Del 4: Praktiska riktlinjer för modellval

### Välja modeller baserat på användningsområde

**För allmän chatt och frågor:**
- Börja med: `phi-4-mini` (snabb, effektiv)
- Uppgradera till: `phi-4` (bättre resonemang)
- Avancerat: `qwen2.5-7b` (längre kontext)

**För kodgenerering:**
- Rekommenderat: `deepseek-r1-7b`
- Alternativ: `qwen2.5-7b` (också bra för kod)

**För komplext resonemang:**
- Bäst: `qwen2.5-7b` eller `qwen2.5-14b`
- Budgetalternativ: `phi-4`

### Guide för hårdvarukrav

**Minimikrav för system:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Rekommenderat för bästa prestanda:**
- 32GB+ RAM för bekväm växling mellan modeller
- SSD-lagring för snabbare modellinladdning
- Modern CPU med bra enkeltrådad prestanda
- NPU-stöd (Windows 11 Copilot+ PC) för acceleration

### Arbetsflöde för modellväxling

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Del 5: Enkel benchmarking av modeller

### Grundläggande prestandatestning

Här är ett enkelt tillvägagångssätt för att jämföra modellprestanda:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuell kvalitetsbedömning

För varje modell, testa med konsekventa frågor och bedöm manuellt:

**Testfrågor:**
1. "Förklara kvantdatorer på ett enkelt sätt."
2. "Skriv en Python-funktion för att sortera en lista."
3. "Vilka är för- och nackdelarna med distansarbete?"
4. "Sammanfatta fördelarna med edge AI."

**Bedömningskriterier:**
- **Noggrannhet**: Är informationen korrekt?
- **Tydlighet**: Är förklaringen lätt att förstå?
- **Fullständighet**: Besvarar den hela frågan?
- **Hastighet**: Hur snabbt svarar den?

### Övervakning av resursanvändning

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Del 6: Nästa steg
- Prenumerera på Model Mondays för nya modeller och tips: https://aka.ms/model-mondays
- Bidra med resultat till ditt teams `models.json`
- Förbered dig för Session 4: jämförelse mellan LLMs och SLMs, lokal vs molninferens, och praktiska demonstrationer

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiska översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.