<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T19:19:08+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "sv"
}
-->
# Session 3: Öppen källkod-modeller med Foundry Local

## Översikt

Den här sessionen utforskar hur man kan använda modeller med öppen källkod i Foundry Local: välja modeller från communityn, integrera innehåll från Hugging Face och använda strategier för "ta med din egen modell" (BYOM). Du kommer också att upptäcka serien Model Mondays för kontinuerligt lärande och modellupptäckt.

Referenser:
- Foundry Local-dokumentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilera Hugging Face-modeller: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Lärandemål
- Upptäcka och utvärdera modeller med öppen källkod för lokal inferens
- Kompilera och köra utvalda Hugging Face-modeller inom Foundry Local
- Använda strategier för modellval baserat på noggrannhet, latens och resursbehov
- Hantera modeller lokalt med cache och versionshantering

## Del 1: Modellupptäckt och urval (Steg-för-steg)

Steg 1) Lista tillgängliga modeller i den lokala katalogen
```cmd
foundry model list
```

Steg 2) Testa snabbt två kandidater (laddas ner automatiskt vid första körningen)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Steg 3) Notera grundläggande mätvärden
- Observera latens (subjektivt) och kvalitet för en fast prompt
- Följ minnesanvändning via Aktivitetshanteraren medan varje modell körs

## Del 2: Köra katalogmodeller via CLI (Steg-för-steg)

Steg 1) Starta en modell
```cmd
foundry model run llama-3.2
```

Steg 2) Skicka en testprompt via OpenAI-kompatibel endpoint
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## Del 3: BYOM – Kompilera Hugging Face-modeller (Steg-för-steg)

Följ den officiella guiden för att kompilera modeller. Översikt nedan—se artikeln på Microsoft Learn för exakta kommandon och stödda konfigurationer.

Steg 1) Förbered en arbetskatalog
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Steg 2) Kompilera en stödd HF-modell
- Använd stegen från Learn-dokumentationen för att konvertera och placera den kompilerade ONNX-modellen i din `models`-katalog
- Bekräfta med:
```cmd
foundry cache ls
```
Du bör se namnet på din kompilerade modell (till exempel `llama-3.2`).

Steg 3) Kör den kompilerade modellen
```cmd
foundry model run llama-3.2 --verbose
```

Noteringar:
- Säkerställ tillräckligt med disk- och RAM-minne för kompilering och körning
- Börja med mindre modeller för att validera flödet, skala sedan upp

## Del 4: Praktisk modellkurering (Steg-för-steg)

Steg 1) Skapa ett `models.json`-register
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Steg 2) Litet urvalsskript
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## Del 5: Praktiska benchmarks (Steg-för-steg)

Steg 1) Enkel latensbenchmark
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Steg 2) Kvalitetskontroll
- Använd en fast promptuppsättning, spara utdata till en CSV/JSON
- Betygsätt manuellt flyt, relevans och korrekthet (1–5)

## Del 6: Nästa steg
- Prenumerera på Model Mondays för nya modeller och tips: https://aka.ms/model-mondays
- Bidra med dina upptäckter till teamets `models.json`
- Förbered dig för Session 4: jämförelse mellan LLMs och SLMs, lokal kontra molninferens, och praktiska demonstrationer

---

