<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T08:16:28+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "sv"
}
-->
# Avsnitt 4: Implementering av produktionsklar modell

## √ñversikt

Denna omfattande handledning guidar dig genom hela processen f√∂r att distribuera finjusterade kvantiserade modeller med Foundry Local. Vi t√§cker modellkonvertering, optimering av kvantisering och konfigurationsinst√§llningar fr√•n b√∂rjan till slut.

## F√∂ruts√§ttningar

Innan du b√∂rjar, se till att du har f√∂ljande:

- ‚úÖ En finjusterad ONNX-modell redo f√∂r distribution
- ‚úÖ Windows- eller Mac-dator
- ‚úÖ Python 3.10 eller h√∂gre
- ‚úÖ Minst 8GB tillg√§ngligt RAM
- ‚úÖ Foundry Local installerat p√• ditt system

## Del 1: Milj√∂inst√§llningar

### Installera n√∂dv√§ndiga verktyg

√ñppna din terminal (Command Prompt p√• Windows, Terminal p√• Mac) och k√∂r f√∂ljande kommandon i ordning:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Viktig notering**: Du beh√∂ver ocks√• CMake version 3.31 eller nyare, som kan laddas ner fr√•n [cmake.org](https://cmake.org/download/).

## Del 2: Modellkonvertering och kvantisering

### V√§lja r√§tt format

F√∂r finjusterade sm√• spr√•kmodeller rekommenderar vi att anv√§nda **ONNX-format** eftersom det erbjuder:

- üöÄ B√§ttre prestandaoptimering
- üîß H√•rdvaruoberoende distribution
- üè≠ Produktionsklara funktioner
- üì± Plattformskompatibilitet

### Metod 1: Konvertering med ett kommando (Rekommenderas)

Anv√§nd f√∂ljande kommando f√∂r att direkt konvertera din finjusterade modell:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**F√∂rklaring av parametrar:**
- `--model_name_or_path`: S√∂kv√§g till din finjusterade modell
- `--device cpu`: Anv√§nd CPU f√∂r optimering
- `--precision int4`: Anv√§nd INT4-kvantisering (ungef√§r 75% storleksreduktion)
- `--output_path`: Utdata-s√∂kv√§g f√∂r den konverterade modellen

### Metod 2: Konfigurationsfil (Avancerade anv√§ndare)

Skapa en konfigurationsfil med namnet `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

K√∂r sedan:

```bash
olive run --config ./finetuned_conversion_config.json
```

### J√§mf√∂relse av kvantiseringsalternativ

| Precision | Filstorlek | Inferenshastighet | Modellkvalitet | Rekommenderad anv√§ndning |
|-----------|------------|-------------------|----------------|--------------------------|
| FP16      | Baslinje √ó 0.5 | Snabb | B√§st | H√∂gpresterande h√•rdvara |
| INT8      | Baslinje √ó 0.25 | Mycket snabb | Bra | Balanserat val |
| INT4      | Baslinje √ó 0.125 | Snabbast | Acceptabel | Resursbegr√§nsade milj√∂er |

üí° **Rekommendation**: B√∂rja med INT4-kvantisering f√∂r din f√∂rsta distribution. Om kvaliteten inte √§r tillfredsst√§llande, prova INT8 eller FP16.

## Del 3: Konfiguration f√∂r Foundry Local-distribution

### Skapa modellkonfiguration

Navigera till Foundry Locals modellkatalog:

```bash
foundry cache cd ./models/
```

Skapa din modellkatalogstruktur:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Skapa konfigurationsfilen `inference_model.json` i din modellkatalog:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Mallkonfigurationer f√∂r specifika modeller

#### F√∂r Qwen-seriens modeller:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Del 4: Modelltestning och optimering

### Verifiera modellinstallation

Kontrollera om Foundry Local kan identifiera din modell:

```bash
foundry cache ls
```

Du b√∂r se `your-finetuned-model-int4` i listan.

### Starta modelltestning

```bash
foundry model run your-finetuned-model-int4
```

### Prestandam√§tning

√ñvervaka nyckelmetrik under testning:

1. **Svarstid**: M√§t genomsnittlig tid per svar
2. **Minnesanv√§ndning**: √ñvervaka RAM-f√∂rbrukning
3. **CPU-anv√§ndning**: Kontrollera processorbelastning
4. **Utdata-kvalitet**: Utv√§rdera svarens relevans och sammanhang

### Kvalitetsvalideringschecklista

- ‚úÖ Modellen svarar korrekt p√• finjusterade dom√§nfr√•gor
- ‚úÖ Svarsformatet matchar f√∂rv√§ntad utdata-struktur
- ‚úÖ Inga minnesl√§ckor vid l√•ngvarig anv√§ndning
- ‚úÖ Konsekvent prestanda √∂ver olika inmatningsl√§ngder
- ‚úÖ Korrekt hantering av kantfall och ogiltiga inmatningar

## Sammanfattning

Grattis! Du har framg√•ngsrikt slutf√∂rt:

- ‚úÖ Konvertering av finjusterat modellformat
- ‚úÖ Optimering av modellkvantisering
- ‚úÖ Konfiguration f√∂r Foundry Local-distribution
- ‚úÖ Prestandajustering och fels√∂kning

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.