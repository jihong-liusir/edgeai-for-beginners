<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T08:14:56+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "sv"
}
-->
# Avsnitt 3: Finjustering - Anpassning av modeller för specifika uppgifter

## Innehållsförteckning
1. [Introduktion till finjustering](../../../Module05)
2. [Varför finjustering är viktigt](../../../Module05)
3. [Typer av finjustering](../../../Module05)
4. [Finjustering med Microsoft Olive](../../../Module05)
5. [Praktiska exempel](../../../Module05)
6. [Bästa praxis och riktlinjer](../../../Module05)
7. [Avancerade tekniker](../../../Module05)
8. [Utvärdering och övervakning](../../../Module05)
9. [Vanliga utmaningar och lösningar](../../../Module05)
10. [Slutsats](../../../Module05)

## Introduktion till finjustering

**Finjustering** är en kraftfull teknik inom maskininlärning som innebär att anpassa en förtränad modell för att utföra specifika uppgifter eller arbeta med specialiserade dataset. Istället för att träna en modell från grunden utnyttjar finjustering den kunskap som redan har lärts in av en förtränad modell och justerar den för ditt specifika användningsområde.

### Vad är finjustering?

Finjustering är en form av **transfer learning** där du:
- Börjar med en förtränad modell som har lärt sig generella mönster från stora dataset
- Justerar modellens interna parametrar med hjälp av ditt specifika dataset
- Behåller värdefull kunskap samtidigt som modellen specialiseras för din uppgift

Tänk på det som att lära en skicklig kock att laga en ny typ av mat – de förstår redan grunderna i matlagning men behöver lära sig specifika tekniker och smaker för den nya stilen.

### Viktiga fördelar

- **Tidsbesparing**: Betydligt snabbare än att träna från grunden
- **Effektiv användning av data**: Kräver mindre dataset för att uppnå bra resultat
- **Kostnadseffektivt**: Lägre krav på beräkningsresurser
- **Bättre prestanda**: Ofta överlägset jämfört med att träna från grunden
- **Resursoptimering**: Gör kraftfull AI tillgänglig för mindre team och organisationer

## Varför finjustering är viktigt

### Användningsområden i verkligheten

Finjustering är avgörande i många scenarier:

**1. Anpassning till domän**
- Medicinsk AI: Anpassning av generella språkmodeller för medicinsk terminologi och kliniska anteckningar
- Juridisk teknik: Specialisering av modeller för analys av juridiska dokument och granskning av kontrakt
- Finansiella tjänster: Anpassning av modeller för analys av finansiella rapporter och riskbedömning

**2. Uppgiftsspecialisering**
- Innehållsgenerering: Finjustering för specifika skrivstilar eller tonlägen
- Kodgenerering: Anpassning av modeller för specifika programmeringsspråk eller ramverk
- Översättning: Förbättring av prestanda för specifika språkpar eller tekniska domäner

**3. Företagsapplikationer**
- Kundservice: Skapande av chatbots som förstår företagsspecifik terminologi
- Intern dokumentation: Bygga AI-assistenter som är bekanta med organisatoriska processer
- Branschspecifika lösningar: Utveckling av modeller som förstår sektorsspecifik jargong och arbetsflöden

## Typer av finjustering

### 1. Fullständig finjustering (Instruktionsfinjustering)

Vid fullständig finjustering uppdateras alla modellparametrar under träningen. Denna metod:
- Ger maximal flexibilitet och prestandapotential
- Kräver betydande beräkningsresurser
- Resulterar i en helt ny version av modellen
- Passar bäst för scenarier där du har omfattande träningsdata och beräkningsresurser

### 2. Parameter-effektiv finjustering (PEFT)

PEFT-metoder uppdaterar endast en liten del av parametrarna, vilket gör processen mer effektiv:

#### Low-Rank Adaptation (LoRA)
- Lägger till små träningsbara matrisdekompositioner till befintliga vikter
- Minskar dramatiskt antalet träningsbara parametrar
- Behåller prestanda nära fullständig finjustering
- Möjliggör enkel växling mellan olika anpassningar

#### QLoRA (Kvantiserad LoRA)
- Kombinerar LoRA med kvantiseringstekniker
- Minskar ytterligare minneskraven
- Möjliggör finjustering av större modeller på konsumenthårdvara
- Balanserar effektivitet med prestanda

#### Adapters
- Infogar små neurala nätverk mellan befintliga lager
- Tillåter riktad finjustering samtidigt som basmodellen förblir oförändrad
- Möjliggör en modulär metod för modellanpassning

### 3. Uppgiftsspecifik finjustering

Fokuserar på att anpassa modeller för specifika nedströmsuppgifter:
- **Klassificering**: Justering av modeller för kategoriseringsuppgifter
- **Generering**: Optimering för innehållsskapande och textgenerering
- **Extraktion**: Finjustering för informationsutvinning och namngiven entity-igenkänning
- **Sammanfattning**: Specialisering av modeller för dokument-sammanfattning

## Finjustering med Microsoft Olive

Microsoft Olive är ett omfattande verktyg för modelloptimering som förenklar finjusteringsprocessen samtidigt som det erbjuder funktioner på företagsnivå.

### Vad är Microsoft Olive?

Microsoft Olive är ett open-source verktyg för modelloptimering som:
- Strömlinjeformar finjusteringsarbetsflöden för olika hårdvarumål
- Ger inbyggt stöd för populära modellarkitekturer (Llama, Phi, Qwen, Gemma)
- Erbjuder både moln- och lokala distributionsalternativ
- Integreras sömlöst med Azure ML och andra Microsoft AI-tjänster
- Stödjer automatisk optimering och kvantisering

### Viktiga funktioner

- **Hårdvaruanpassad optimering**: Optimerar automatiskt modeller för specifik hårdvara (CPU, GPU, NPU)
- **Stöd för flera format**: Fungerar med PyTorch, Hugging Face och ONNX-modeller
- **Automatiserade arbetsflöden**: Minskar behovet av manuell konfiguration och trial-and-error
- **Företagsintegration**: Inbyggt stöd för Azure ML och molndistributioner
- **Utbyggbar arkitektur**: Möjliggör anpassade optimeringstekniker

### Installation och konfiguration

#### Grundläggande installation

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Valfria beroenden

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Verifiera installationen

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktiska exempel

### Exempel 1: Grundläggande finjustering med Olive CLI

Detta exempel visar hur man finjusterar en liten språkmodell för frasklassificering:

#### Steg 1: Förbered din miljö

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Steg 2: Finjustera modellen

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Steg 3: Optimera för distribution

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Exempel 2: Avancerad konfiguration med anpassat dataset

#### Steg 1: Förbered anpassat dataset

Skapa en JSON-fil med dina träningsdata:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Steg 2: Skapa konfigurationsfil

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Steg 3: Utför finjustering

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Exempel 3: QLoRA-finjustering för minneseffektivitet

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Bästa praxis och riktlinjer

### Databeredning

**1. Datakvalitet framför kvantitet**
- Prioritera högkvalitativa, varierade exempel framför stora mängder dåliga data
- Säkerställ att data är representativt för ditt målområde
- Rensa och förbehandla data konsekvent

**2. Dataformat och mallar**
- Använd konsekvent formatering för alla träningsdata
- Skapa tydliga input-output-mallar som matchar ditt användningsområde
- Inkludera lämplig instruktionformatering för instruktionsfinjusterade modeller

**3. Dataset-splitning**
- Reservera 10-20% av data för validering
- Behåll liknande fördelningar mellan tränings- och valideringsuppdelningar
- Överväg stratifierad sampling för klassificeringsuppgifter

### Träningskonfiguration

**1. Val av inlärningshastighet**
- Börja med mindre inlärningshastigheter (1e-5 till 1e-4) för finjustering
- Använd inlärningshastighetsplanering för bättre konvergens
- Övervaka förlustkurvor för att justera hastigheter vid behov

**2. Optimering av batchstorlek**
- Balansera batchstorlek med tillgängligt minne
- Använd gradientackumulering för större effektiva batchstorlekar
- Överväg sambandet mellan batchstorlek och inlärningshastighet

**3. Träningslängd**
- Övervaka valideringsmetrik för att undvika överanpassning
- Använd tidig avstängning när valideringsprestanda planar ut
- Spara checkpoints regelbundet för återhämtning och analys

### Modellval

**1. Val av basmodell**
- Välj modeller som är förtränade på liknande domäner när det är möjligt
- Överväg modellstorlek i förhållande till dina beräkningsbegränsningar
- Utvärdera licenskrav för kommersiell användning

**2. Val av finjusteringsmetod**
- Använd LoRA/QLoRA för resursbegränsade miljöer
- Välj fullständig finjustering när maximal prestanda är avgörande
- Överväg adapterbaserade metoder för flera uppgiftsscenarier

### Resurshantering

**1. Optimering av hårdvara**
- Välj lämplig hårdvara för din modellstorlek och metod
- Utnyttja GPU-minne effektivt med gradientcheckpointing
- Överväg molnbaserade lösningar för större modeller

**2. Minneshantering**
- Använd träning med blandad precision när det är möjligt
- Implementera gradientackumulering för minnesbegränsningar
- Övervaka GPU-minnesanvändning under träningen

## Avancerade tekniker

### Multi-adapterträning

Träna flera adapters för olika uppgifter samtidigt som basmodellen delas:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hyperparameteroptimering

Implementera systematisk optimering av hyperparametrar:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Anpassade förlustfunktioner

Implementera domänspecifika förlustfunktioner:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Utvärdering och övervakning

### Metrik och utvärdering

**1. Standardmetrik**
- **Noggrannhet**: Övergripande korrekthet för klassificeringsuppgifter
- **Perplexity**: Mått på kvaliteten för språkmodellering
- **BLEU/ROUGE**: Kvalitet för textgenerering och sammanfattning
- **F1 Score**: Balanserad precision och återkallelse för klassificering

**2. Domänspecifika metrik**
- **Uppgiftsspecifika benchmarks**: Använd etablerade benchmarks för din domän
- **Mänsklig utvärdering**: Inkludera mänsklig bedömning för subjektiva uppgifter
- **Affärsmetrik**: Anpassa till faktiska affärsmål

**3. Utvärderingsinställning**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Övervakning av träningsframsteg

**1. Förlustspårning**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Valideringsövervakning**
- Spåra valideringsförlust tillsammans med träningsförlust
- Övervaka tecken på överanpassning (valideringsförlust ökar medan träningsförlust minskar)
- Använd tidig avstängning baserat på valideringsmetrik

**3. Resursövervakning**
- Övervaka GPU/CPU-användning
- Spåra minnesanvändningsmönster
- Övervaka träningshastighet och genomströmning

## Vanliga utmaningar och lösningar

### Utmaning 1: Överanpassning

**Symptom:**
- Träningsförlust fortsätter att minska medan valideringsförlust ökar
- Stor skillnad mellan tränings- och valideringsprestanda
- Dålig generalisering till ny data

**Lösningar:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Utmaning 2: Minnesbegränsningar

**Lösningar:**
- Använd gradientcheckpointing
- Implementera gradientackumulering
- Välj parameter-effektiva metoder (LoRA, QLoRA)
- Utnyttja modellparallellism för stora modeller

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Utmaning 3: Långsam träning

**Lösningar:**
- Optimera dataladdningspipelines
- Använd träning med blandad precision
- Implementera effektiva batchningsstrategier
- Överväg distribuerad träning för stora dataset

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Utmaning 4: Dålig prestanda

**Diagnostiksteg:**
1. Verifiera datakvalitet och formatering
2. Kontrollera inlärningshastighet och träningslängd
3. Utvärdera val av basmodell
4. Granska förbehandling och tokenisering

**Lösningar:**
- Öka mångfalden i träningsdata
- Justera inlärningshastighetsplanering
- Testa olika basmodeller
- Implementera tekniker för dataförstärkning

## Slutsats

Finjustering är en kraftfull teknik som demokratiserar tillgången till AI på toppnivå. Genom att använda verktyg som Microsoft Olive kan organisationer effektivt anpassa förtränade modeller till sina specifika behov samtidigt som de optimerar för prestanda och resursbegränsningar.

### Viktiga insikter

1. **Välj rätt metod**: Välj finjusteringsmetoder baserat på dina beräkningsresurser och prestandakrav
2. **Datakvalitet är avgörande**: Investera i högkvalitativa, representativa träningsdata
3. **Övervaka och iterera**: Utvärdera och förbättra dina modeller kontinuerligt
4. **Utnyttja verktyg**: Använd ramverk som Olive för att förenkla och optimera processen
5. **Tänk på distribution**: Planera för modelloptimering och distribution från början

## ➡️ Vad händer härnäst

- [04: Distribution - Implementering av produktionsklara modeller](./04.SLMOps.Deployment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.