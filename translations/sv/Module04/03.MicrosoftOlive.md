<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T19:11:26+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sv"
}
-->
# Avsnitt 3: Microsoft Olive Optimization Suite

## Innehållsförteckning
1. [Introduktion](../../../Module04)
2. [Vad är Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Snabbstartsguide](../../../Module04)
5. [Exempel: Konvertera Qwen3 till ONNX INT4](../../../Module04)
6. [Avancerad användning](../../../Module04)
7. [Bästa praxis](../../../Module04)
8. [Felsökning](../../../Module04)
9. [Ytterligare resurser](../../../Module04)

## Introduktion

Microsoft Olive är ett kraftfullt och användarvänligt verktyg för hårdvaruanpassad modelloptimering som förenklar processen att optimera maskininlärningsmodeller för implementering på olika hårdvaruplattformar. Oavsett om du riktar dig mot CPU:er, GPU:er eller specialiserade AI-acceleratorer hjälper Olive dig att uppnå optimal prestanda samtidigt som modellens noggrannhet bibehålls.

## Vad är Microsoft Olive?

Olive är ett användarvänligt verktyg för hårdvaruanpassad modelloptimering som kombinerar ledande tekniker inom modellkomprimering, optimering och kompilering. Det fungerar med ONNX Runtime som en E2E-lösning för inferensoptimering.

### Nyckelfunktioner

- **Hårdvaruanpassad optimering**: Väljer automatiskt de bästa optimeringsteknikerna för din målplattform
- **40+ inbyggda optimeringskomponenter**: Täcker modellkomprimering, kvantisering, grafoptimering och mer
- **Enkel CLI-gränssnitt**: Smidiga kommandon för vanliga optimeringsuppgifter
- **Stöd för flera ramverk**: Fungerar med PyTorch, Hugging Face-modeller och ONNX
- **Stöd för populära modeller**: Olive kan automatiskt optimera populära modellarkitekturer som Llama, Phi, Qwen, Gemma, etc. direkt

### Fördelar

- **Minskad utvecklingstid**: Ingen behov av att manuellt experimentera med olika optimeringstekniker
- **Prestandaförbättringar**: Betydande hastighetsökningar (upp till 6x i vissa fall)
- **Plattformsoberoende implementering**: Optimerade modeller fungerar på olika hårdvaror och operativsystem
- **Bibehållen noggrannhet**: Optimeringar bevarar modellkvaliteten samtidigt som prestandan förbättras

## Installation

### Förutsättningar

- Python 3.8 eller högre
- pip-pakethanterare
- Virtuell miljö (rekommenderas)

### Grundläggande installation

Skapa och aktivera en virtuell miljö:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installera Olive med funktioner för automatisk optimering:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valfria beroenden

Olive erbjuder olika valfria beroenden för ytterligare funktioner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifiera installationen

```bash
olive --help
```

Om installationen lyckas bör du se Olive CLI-hjälpmeddelandet.

## Snabbstartsguide

### Din första optimering

Låt oss optimera en liten språkmodell med Olives funktion för automatisk optimering:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Vad gör detta kommando?

Optimeringsprocessen innefattar: att hämta modellen från den lokala cachen, fånga ONNX-grafen och lagra vikterna i en ONNX-datafil, optimera ONNX-grafen och kvantisera modellen till int4 med RTN-metoden.

### Förklaring av kommandoparametrar

- `--model_name_or_path`: Hugging Face-modellidentifierare eller lokal sökväg
- `--output_path`: Katalog där den optimerade modellen sparas
- `--device`: Målhårdvara (cpu, gpu)
- `--provider`: Utförandeleverantör (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Använd ONNX Runtime Generate AI för inferens
- `--precision`: Kvantiseringsprecision (int4, int8, fp16)
- `--log_level`: Loggnivå (0=minimal, 1=detaljerad)

## Exempel: Konvertera Qwen3 till ONNX INT4

Baserat på det tillhandahållna Hugging Face-exemplet på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), här är hur du optimerar en Qwen3-modell:

### Steg 1: Ladda ner modellen (valfritt)

För att minimera nedladdningstiden, cachea endast nödvändiga filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Steg 2: Optimera Qwen3-modellen

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Steg 3: Testa den optimerade modellen

Skapa ett enkelt Python-skript för att testa din optimerade modell:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Utgångsstruktur

Efter optimering kommer din utgångskatalog att innehålla:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avancerad användning

### Konfigurationsfiler

För mer komplexa optimeringsarbetsflöden kan du använda JSON-konfigurationsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kör med konfiguration:

```bash
olive run --config config.json
```

### GPU-optimering

För CUDA GPU-optimering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

För DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive stödjer även finjustering av modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bästa praxis

### 1. Modellval
- Börja med mindre modeller för testning (t.ex. 0.5B-7B parametrar)
- Säkerställ att din målmodellarkitektur stöds av Olive

### 2. Hårdvaruöverväganden
- Matcha din optimeringsmål med din implementeringshårdvara
- Använd GPU-optimering om du har CUDA-kompatibel hårdvara
- Överväg DirectML för Windows-maskiner med integrerad grafik

### 3. Val av precision
- **INT4**: Maximal komprimering, viss noggrannhetsförlust
- **INT8**: Bra balans mellan storlek och noggrannhet
- **FP16**: Minimal noggrannhetsförlust, måttlig storleksreduktion

### 4. Testning och validering
- Testa alltid optimerade modeller med dina specifika användningsfall
- Jämför prestandamått (latens, genomströmning, noggrannhet)
- Använd representativ indata för utvärdering

### 5. Iterativ optimering
- Börja med automatisk optimering för snabba resultat
- Använd konfigurationsfiler för detaljerad kontroll
- Experimentera med olika optimeringspass

## Felsökning

### Vanliga problem

#### 1. Installationsproblem
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problem
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Minnesproblem
- Använd mindre batchstorlekar under optimering
- Försök med kvantisering med högre precision först (int8 istället för int4)
- Säkerställ tillräckligt med diskutrymme för modellcache

#### 4. Problem med modellinläsning
- Kontrollera modellens sökväg och åtkomstbehörigheter
- Kontrollera om modellen kräver `trust_remote_code=True`
- Säkerställ att alla nödvändiga modelfiler är nedladdade

### Få hjälp

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exempel**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Ytterligare resurser

### Officiella länkar
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime-dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-exempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community-exempel
- **Jupyter Notebooks**: Finns i Olive GitHub-repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit för VS Code översikt — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogginlägg**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Relaterade verktyg
- **ONNX Runtime**: Högpresterande inferensmotor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Källa till många kompatibla modeller — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Molnbaserade optimeringsarbetsflöden — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Vad händer härnäst

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

