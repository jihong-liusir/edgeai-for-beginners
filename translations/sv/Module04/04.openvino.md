<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T07:52:13+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "sv"
}
-->
# Avsnitt 4: OpenVINO Toolkit Optimeringssvit

## Innehållsförteckning
1. [Introduktion](../../../Module04)
2. [Vad är OpenVINO?](../../../Module04)
3. [Installation](../../../Module04)
4. [Snabbstartsguide](../../../Module04)
5. [Exempel: Konvertera och optimera modeller med OpenVINO](../../../Module04)
6. [Avancerad användning](../../../Module04)
7. [Bästa praxis](../../../Module04)
8. [Felsökning](../../../Module04)
9. [Ytterligare resurser](../../../Module04)

## Introduktion

OpenVINO (Open Visual Inference and Neural Network Optimization) är Intels open-source-verktyg för att implementera högpresterande AI-lösningar i molnet, lokalt och på edge-enheter. Oavsett om du riktar in dig på CPU:er, GPU:er, VPU:er eller specialiserade AI-acceleratorer, erbjuder OpenVINO omfattande optimeringsmöjligheter samtidigt som modellens noggrannhet bibehålls och möjliggör plattformsoberoende distribution.

## Vad är OpenVINO?

OpenVINO är ett open-source-verktyg som gör det möjligt för utvecklare att effektivt optimera, konvertera och distribuera AI-modeller över olika hårdvaruplattformar. Det består av tre huvudkomponenter: OpenVINO Runtime för inferens, Neural Network Compression Framework (NNCF) för modelloptimering och OpenVINO Model Server för skalbar distribution.

### Viktiga funktioner

- **Plattformsoberoende distribution**: Stöd för Linux, Windows och macOS med Python-, C++- och C-API:er
- **Hårdvaruacceleration**: Automatisk enhetsidentifiering och optimering för CPU, GPU, VPU och AI-acceleratorer
- **Modellkomprimeringsramverk**: Avancerade tekniker för kvantisering, beskärning och optimering via NNCF
- **Kompatibilitet med ramverk**: Direkt stöd för TensorFlow, ONNX, PaddlePaddle och PyTorch-modeller
- **Generativ AI-stöd**: Specialiserad OpenVINO GenAI för att distribuera stora språkmodeller och generativa AI-applikationer

### Fördelar

- **Prestandaoptimering**: Betydande hastighetsförbättringar med minimal noggrannhetsförlust
- **Minskad distributionsstorlek**: Minimala externa beroenden förenklar installation och distribution
- **Förbättrad starttid**: Optimerad modellinläsning och caching för snabbare applikationsinitiering
- **Skalbar distribution**: Från edge-enheter till molninfrastruktur med konsekventa API:er
- **Redo för produktion**: Företagsklassad tillförlitlighet med omfattande dokumentation och community-stöd

## Installation

### Förutsättningar

- Python 3.8 eller högre
- pip-pakethanterare
- Virtuell miljö (rekommenderas)
- Kompatibel hårdvara (Intel CPU:er rekommenderas, men stöd för olika arkitekturer finns)

### Grundläggande installation

Skapa och aktivera en virtuell miljö:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installera OpenVINO Runtime:

```bash
pip install openvino
```

Installera NNCF för modelloptimering:

```bash
pip install nncf
```

### Installation av OpenVINO GenAI

För generativa AI-applikationer:

```bash
pip install openvino-genai
```

### Valfria beroenden

Ytterligare paket för specifika användningsfall:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verifiera installationen

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Om installationen lyckas bör du se information om OpenVINO-versionen.

## Snabbstartsguide

### Din första modelloptimering

Låt oss konvertera och optimera en Hugging Face-modell med OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Vad gör denna process?

Optimeringsarbetsflödet innefattar: att ladda den ursprungliga modellen från Hugging Face, konvertera till OpenVINO Intermediate Representation (IR)-format, tillämpa standardoptimeringar och kompilera för målhårdvara.

### Förklaring av nyckelparametrar

- `export=True`: Konverterar modellen till OpenVINO IR-format
- `compile=False`: Fördröjer kompileringen till runtime för flexibilitet
- `device`: Målhårdvara ("CPU", "GPU", "AUTO" för automatisk val)
- `save_pretrained()`: Sparar den optimerade modellen för återanvändning

## Exempel: Konvertera och optimera modeller med OpenVINO

### Steg 1: Modellkonvertering med NNCF-kvantisering

Så här tillämpar du kvantisering efter träning med NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Steg 2: Avancerad optimering med viktkomprimering

För transformerbaserade modeller, tillämpa viktkomprimering:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Steg 3: Inferens med optimerad modell

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Utgångsstruktur

Efter optimering kommer din modellkatalog att innehålla:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Avancerad användning

### Konfiguration med NNCF YAML

För komplexa optimeringsarbetsflöden, använd NNCF-konfigurationsfiler:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Tillämpa konfiguration:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-optimering

För GPU-acceleration:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimering för batchbearbetning

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modellserverdistribution

Distribuera optimerade modeller med OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Klientkod för modellserver:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Bästa praxis

### 1. Modellval och förberedelse
- Använd modeller från stödda ramverk (PyTorch, TensorFlow, ONNX)
- Säkerställ att modellens indata har fasta eller kända dynamiska former
- Testa med representativa dataset för kalibrering

### 2. Val av optimeringsstrategi
- **Kvantisering efter träning**: Börja här för snabb optimering
- **Viktkomprimering**: Perfekt för stora språkmodeller och transformatorer
- **Kvantisering med medvetenhet om träning**: Använd när noggrannhet är avgörande

### 3. Hårdvaruspecifik optimering
- **CPU**: Använd INT8-kvantisering för balanserad prestanda
- **GPU**: Utnyttja FP16-precision och batchbearbetning
- **VPU**: Fokusera på modellsimplifiering och lagerfusion

### 4. Prestandajustering
- **Genomströmningsläge**: För högvolyms batchbearbetning
- **Latensläge**: För realtidsinteraktiva applikationer
- **AUTO-enhet**: Låt OpenVINO välja optimal hårdvara

### 5. Minneshantering
- Använd dynamiska former med försiktighet för att undvika minnesöverbelastning
- Implementera modellcaching för snabbare efterföljande laddningar
- Övervaka minnesanvändning under optimering

### 6. Noggrannhetsvalidering
- Validera alltid optimerade modeller mot ursprunglig prestanda
- Använd representativa testdataset för utvärdering
- Överväg gradvis optimering (börja med konservativa inställningar)

## Felsökning

### Vanliga problem

#### 1. Installationsproblem
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modellkonverteringsfel
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Prestandaproblem
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Minnesproblem
- Minska modellens batchstorlek under optimering
- Använd streaming för stora dataset
- Aktivera modellcaching: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Noggrannhetsförsämring
- Använd högre precision (INT8 istället för INT4)
- Öka storleken på kalibreringsdatasetet
- Tillämpa optimering med blandad precision

### Prestandaövervakning

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Få hjälp

- **Dokumentation**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub-problem**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community-forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Ytterligare resurser

### Officiella länkar
- **OpenVINO Hemsida**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Lärresurser
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Snabbstartsguide**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimeringsguide**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integrationsverktyg
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Prestandajämförelser
- **Officiella jämförelser**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Community-exempel
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Omfattande handledningar tillgängliga i OpenVINO-notebooks repository
- **Exempelapplikationer**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Verkliga exempel för olika domäner (datorseende, NLP, ljud)
- **Blogginlägg**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI och community-blogginlägg med detaljerade användningsfall

### Relaterade verktyg
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Ytterligare optimeringstekniker för Intel-hårdvara
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - För jämförelser av mobil- och edge-distribution
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativa plattformsoberoende inferensmotorer

## ➡️ Vad händer härnäst

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller inexaktheter. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.