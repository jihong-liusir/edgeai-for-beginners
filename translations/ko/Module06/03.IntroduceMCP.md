<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-07-22T04:49:25+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ko"
}
-->
# Section 03 - 모델 컨텍스트 프로토콜(MCP) 통합

## MCP(Model Context Protocol) 소개

모델 컨텍스트 프로토콜(MCP)은 언어 모델이 외부 도구 및 시스템과 표준화된 방식으로 상호작용할 수 있도록 하는 혁신적인 프레임워크입니다. 기존의 모델이 고립된 방식으로 작동하는 것과 달리, MCP는 명확히 정의된 프로토콜을 통해 AI 모델과 현실 세계를 연결하는 다리를 제공합니다.

### MCP란 무엇인가?

MCP는 언어 모델이 다음을 수행할 수 있도록 하는 통신 프로토콜입니다:
- 외부 데이터 소스에 연결
- 도구 및 함수 실행
- API 및 서비스와 상호작용
- 실시간 정보 접근
- 복잡한 다단계 작업 수행

이 프로토콜은 정적인 언어 모델을 텍스트 생성 이상의 실질적인 작업을 수행할 수 있는 동적인 에이전트로 변모시킵니다.

## MCP에서의 소형 언어 모델(SLM)

소형 언어 모델(Small Language Models)은 AI 배포에 있어 효율적인 접근 방식을 제공하며, 여러 가지 이점을 가지고 있습니다.

### SLM의 장점
- **자원 효율성**: 낮은 계산 요구 사항
- **빠른 응답 시간**: 실시간 애플리케이션에서 지연 시간 감소  
- **비용 효율성**: 최소한의 인프라 필요
- **프라이버시**: 데이터 전송 없이 로컬에서 실행 가능
- **맞춤화 용이성**: 특정 도메인에 맞게 쉽게 조정 가능

### SLM과 MCP의 조합이 효과적인 이유

SLM과 MCP를 결합하면 모델의 추론 능력이 외부 도구에 의해 보완되어, 작은 파라미터 수를 가진 모델도 향상된 기능을 통해 강력한 성능을 발휘할 수 있습니다.

## Python MCP SDK 개요

Python MCP SDK는 MCP를 지원하는 애플리케이션을 구축하기 위한 기반을 제공합니다. SDK는 다음을 포함합니다:

- **클라이언트 라이브러리**: MCP 서버에 연결하기 위한 라이브러리
- **서버 프레임워크**: 사용자 정의 MCP 서버를 생성하기 위한 프레임워크
- **프로토콜 핸들러**: 통신 관리
- **도구 통합**: 외부 함수 실행 지원

## 실전 구현: Phi-4 MCP 클라이언트

Microsoft의 Phi-4 미니 모델을 MCP 기능과 통합한 실제 구현 사례를 살펴보겠습니다.

### 시스템 아키텍처

이 구현은 계층형 아키텍처를 따릅니다:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### 핵심 구성 요소

#### 1. MCP 클라이언트 클래스

**BaseMCPClient**: 공통 기능을 제공하는 추상 기반
- 비동기 컨텍스트 관리자 프로토콜
- 표준 인터페이스 정의
- 리소스 관리

**Phi4MiniMCPClient**: STDIO 기반 구현
- 로컬 프로세스 통신
- 표준 입력/출력 처리
- 서브프로세스 관리

**Phi4MiniSSEMCPClient**: 서버-발송 이벤트(Server-Sent Events) 구현
- HTTP 스트리밍 통신
- 실시간 이벤트 처리
- 웹 기반 서버 연결

#### 2. LLM 통합

**OllamaClient**: 로컬 모델 호스팅
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: 고성능 서빙
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. 도구 처리 파이프라인

도구 처리 파이프라인은 MCP 도구를 언어 모델과 호환되는 형식으로 변환합니다:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## 시작하기: 단계별 가이드

### 1단계: 환경 설정

필요한 종속성 설치:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 2단계: 기본 구성

환경 변수를 설정합니다:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 3단계: 첫 MCP 클라이언트 실행

**기본 Ollama 설정:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM 백엔드 사용:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**서버-발송 이벤트 연결:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**사용자 정의 MCP 서버:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 4단계: 프로그래밍 방식 사용

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 고급 기능

### 다중 백엔드 지원

이 구현은 Ollama와 vLLM 백엔드를 모두 지원하며, 요구 사항에 따라 선택할 수 있습니다:

- **Ollama**: 로컬 개발 및 테스트에 적합
- **vLLM**: 프로덕션 및 고처리량 시나리오에 최적화

### 유연한 연결 프로토콜

두 가지 연결 모드를 지원합니다:

**STDIO 모드**: 직접 프로세스 통신
- 낮은 지연 시간
- 로컬 도구에 적합
- 간단한 설정

**SSE 모드**: HTTP 기반 스트리밍
- 네트워크 지원
- 분산 시스템에 적합
- 실시간 업데이트

### 도구 통합 기능

시스템은 다양한 도구와 통합할 수 있습니다:
- 웹 자동화(Playwright)
- 파일 작업
- API 상호작용
- 시스템 명령
- 사용자 정의 함수

## 오류 처리 및 모범 사례

### 포괄적인 오류 관리

이 구현은 다음과 같은 오류에 대한 강력한 처리 기능을 포함합니다:

**연결 오류:**
- MCP 서버 실패
- 네트워크 타임아웃
- 연결 문제

**도구 실행 오류:**
- 누락된 도구
- 매개변수 유효성 검사
- 실행 실패

**응답 처리 오류:**
- JSON 파싱 문제
- 형식 불일치
- LLM 응답 이상

### 모범 사례

1. **리소스 관리**: 비동기 컨텍스트 관리자를 사용하세요.
2. **오류 처리**: 포괄적인 try-catch 블록을 구현하세요.
3. **로깅**: 적절한 로깅 수준을 활성화하세요.
4. **보안**: 입력을 검증하고 출력을 정제하세요.
5. **성능**: 연결 풀링 및 캐싱을 사용하세요.

## 실제 응용 사례

### 웹 자동화
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 데이터 처리
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 통합
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 성능 최적화

### 메모리 관리
- 효율적인 메시지 기록 처리
- 적절한 리소스 정리
- 연결 풀링

### 네트워크 최적화
- 비동기 HTTP 작업
- 구성 가능한 타임아웃
- 우아한 오류 복구

### 동시 처리
- 논블로킹 I/O
- 병렬 도구 실행
- 효율적인 비동기 패턴

## 보안 고려 사항

### 데이터 보호
- 안전한 API 키 관리
- 입력 검증
- 출력 정제

### 네트워크 보안
- HTTPS 지원
- 로컬 엔드포인트 기본값
- 안전한 토큰 처리

### 실행 안전성
- 도구 필터링
- 샌드박스 환경
- 감사 로깅

## 결론

SLM과 MCP의 통합은 AI 애플리케이션 개발에 있어 패러다임 전환을 의미합니다. 소형 모델의 효율성과 외부 도구의 강력함을 결합함으로써, 개발자는 자원 효율적이면서도 높은 성능을 발휘하는 지능형 시스템을 구축할 수 있습니다.

Phi-4 MCP 클라이언트 구현은 이러한 통합을 실현하는 방법을 보여주며, 복잡한 AI 기반 애플리케이션을 구축하기 위한 견고한 기반을 제공합니다.

핵심 요점:
- MCP는 언어 모델과 외부 시스템 간의 격차를 해소합니다.
- SLM은 도구로 보완될 때 효율성을 유지하면서도 높은 성능을 제공합니다.
- 모듈식 아키텍처는 확장성과 사용자 정의를 용이하게 합니다.
- 프로덕션 환경에서는 적절한 오류 처리와 보안 조치가 필수적입니다.

이 튜토리얼은 SLM 기반 MCP 애플리케이션을 구축하기 위한 기초를 제공하며, 자동화, 데이터 처리, 지능형 시스템 통합의 가능성을 열어줍니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.