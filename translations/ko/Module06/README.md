<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b17bf7f849519fac995c24ab9e2d0be8",
  "translation_date": "2025-07-22T04:27:22+00:00",
  "source_file": "Module06/README.md",
  "language_code": "ko"
}
-->
# Chapter 06 : SLM 에이전트 시스템: 종합 개요

인공지능의 세계는 단순한 챗봇에서 소형 언어 모델(SLM)로 구동되는 정교한 AI 에이전트로의 근본적인 변화를 경험하고 있습니다. 이 종합 가이드는 현대 SLM 에이전트 시스템의 세 가지 핵심 측면을 탐구합니다: 기본 개념과 배포 전략, 함수 호출 기능, 그리고 혁신적인 모델 컨텍스트 프로토콜(MCP) 통합.

## [Section 1: AI 에이전트와 소형 언어 모델의 기초](./01.IntroduceAgent.md)

첫 번째 섹션은 AI 에이전트와 소형 언어 모델에 대한 기본 이해를 확립하며, 2023년 챗봇 시대와 2024년 코파일럿 붐 이후 2025년을 AI 에이전트의 해로 자리매김합니다. 이 섹션에서는 **에이전트형 AI 시스템**을 소개하며, 이 시스템은 최소한의 인간 개입으로 사고, 추론, 계획, 도구 사용 및 작업 실행을 수행합니다.

### 주요 개념:
- **에이전트 분류 프레임워크**: 단순 반사형 에이전트에서 학습형 에이전트까지, 다양한 컴퓨팅 시나리오를 위한 포괄적인 분류 체계 제공
- **SLM 기본 사항**: 소형 언어 모델을 소비자 기기에서 실용적인 추론을 수행할 수 있는 100억 개 미만의 매개변수를 가진 모델로 정의
- **고급 최적화 전략**: GGUF 형식 배포, 양자화 기술(Q4_K_M, Q5_K_S, Q8_0), Llama.cpp 및 Apple MLX와 같은 엣지 최적화 프레임워크 다루기
- **SLM vs LLM 비교**: SLM을 통해 10-30배 비용 절감 효과를 보여주며, 일반적인 에이전트 작업의 70-80%를 효과적으로 처리 가능

이 섹션은 Ollama, VLLM, Microsoft의 엣지 솔루션을 활용한 실용적인 배포 전략으로 마무리되며, SLM이 비용 효율적이고 개인정보를 보호하는 에이전트형 AI 배포의 미래로 자리잡는 과정을 설명합니다.

## [Section 2: 소형 언어 모델의 함수 호출](./02.FunctionCalling.md)

두 번째 섹션은 **함수 호출 기능**에 대해 깊이 탐구하며, 정적 언어 모델을 실제 세계와 상호작용할 수 있는 동적 AI 에이전트로 변환하는 메커니즘을 다룹니다. 이 기술적 심층 분석은 의도 감지에서 응답 통합까지의 전체 워크플로를 설명합니다.

### 핵심 구현 영역:
- **체계적인 워크플로**: 도구 통합, 함수 정의, 의도 감지, JSON 출력 생성, 외부 실행에 대한 상세 탐구
- **플랫폼별 구현**: Ollama를 활용한 Phi-4-mini, Qwen3 함수 호출, Microsoft Foundry Local 통합에 대한 종합 가이드
- **고급 예제**: 다중 에이전트 협업 시스템, 동적 도구 선택, 포괄적인 오류 처리와 함께하는 엔터프라이즈 통합 패턴
- **프로덕션 고려사항**: 속도 제한, 감사 로그, 보안 조치, 성능 최적화 전략

이 섹션은 이론적 이해와 실용적인 구현 패턴을 제공하여, 간단한 API 호출부터 복잡한 다단계 엔터프라이즈 워크플로까지 처리할 수 있는 견고한 함수 호출 시스템을 개발할 수 있도록 돕습니다.

## [Section 3: 모델 컨텍스트 프로토콜(MCP) 통합](./03.IntroduceMCP.md)

마지막 섹션은 **모델 컨텍스트 프로토콜(MCP)**을 소개하며, 언어 모델이 외부 도구 및 시스템과 상호작용하는 방식을 표준화하는 혁신적인 프레임워크를 설명합니다. 이 섹션은 MCP가 잘 정의된 프로토콜을 통해 AI 모델과 실제 세계를 연결하는 방법을 보여줍니다.

### 통합 하이라이트:
- **프로토콜 아키텍처**: 애플리케이션, LLM 클라이언트, MCP 클라이언트, 도구 처리 계층을 포함한 계층적 시스템 설계
- **다중 백엔드 지원**: Ollama(로컬 개발) 및 vLLM(프로덕션) 백엔드를 지원하는 유연한 구현
- **연결 프로토콜**: 직접 프로세스 통신을 위한 STDIO 모드와 HTTP 기반 스트리밍을 위한 SSE 모드
- **실제 응용 사례**: 웹 자동화, 데이터 처리, API 통합 예제와 포괄적인 오류 처리

MCP 통합은 SLM이 외부 기능을 통해 매개변수 수의 한계를 보완하면서도 로컬 배포와 자원 효율성의 이점을 유지하는 방법을 보여줍니다.

## 전략적 함의

이 세 섹션은 SLM 에이전트 시스템을 이해하고 구현하기 위한 종합적인 프레임워크를 제공합니다. 기본 개념에서 함수 호출, MCP 통합으로의 진화는 다음과 같은 민주화된 AI 배포로의 명확한 경로를 보여줍니다:

- **효율성과 역량의 결합**: 최적화된 소형 모델을 통해
- **비용 효율성**: 광범위한 채택 가능
- **표준화된 프로토콜**: 상호운용성 보장
- **로컬 배포**: 개인정보 보호 및 지연 시간 감소

이러한 발전은 단순한 기술적 진보를 넘어, 자원 제약 환경에서도 효과적으로 작동하며 정교한 에이전트형 기능을 제공하는 더 접근 가능하고 효율적이며 실용적인 AI 시스템으로의 패러다임 전환을 나타냅니다.

고급 배포 전략, 견고한 함수 호출, 표준화된 도구 통합 프로토콜과 결합된 SLM은 산업 및 응용 분야 전반에서 인공지능과 상호작용하고 혜택을 누리는 방식을 혁신할 차세대 AI 에이전트의 기반으로 자리잡을 것입니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.