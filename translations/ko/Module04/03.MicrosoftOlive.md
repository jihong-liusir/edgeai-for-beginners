<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-15T17:03:48+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ko"
}
-->
# Section 3 : Microsoft Olive Optimization Suite

## 목차
1. [소개](../../../Module04)
2. [Microsoft Olive란 무엇인가?](../../../Module04)
3. [설치](../../../Module04)
4. [빠른 시작 가이드](../../../Module04)
5. [예제: Qwen3를 ONNX INT4로 변환하기](../../../Module04)
6. [고급 사용법](../../../Module04)
7. [모범 사례](../../../Module04)
8. [문제 해결](../../../Module04)
9. [추가 자료](../../../Module04)

## 소개

Microsoft Olive는 강력하고 사용하기 쉬운 하드웨어 인식 모델 최적화 도구로, 다양한 하드웨어 플랫폼에서 머신 러닝 모델을 배포하기 위한 최적화 과정을 간소화합니다. CPU, GPU 또는 AI 가속기와 같은 특수 하드웨어를 대상으로 하든, Olive는 모델 정확도를 유지하면서 최적의 성능을 달성할 수 있도록 도와줍니다.

## Microsoft Olive란 무엇인가?

Olive는 모델 압축, 최적화 및 컴파일과 같은 업계 최고의 기술을 통합한 사용하기 쉬운 하드웨어 인식 모델 최적화 도구입니다. ONNX Runtime과 함께 E2E 추론 최적화 솔루션으로 작동합니다.

### 주요 기능

- **하드웨어 인식 최적화**: 대상 하드웨어에 가장 적합한 최적화 기술을 자동으로 선택
- **40개 이상의 내장 최적화 구성 요소**: 모델 압축, 양자화, 그래프 최적화 등을 포함
- **간단한 CLI 인터페이스**: 일반적인 최적화 작업을 위한 간단한 명령어
- **다중 프레임워크 지원**: PyTorch, Hugging Face 모델 및 ONNX와 호환
- **인기 모델 지원**: Olive는 Llama, Phi, Qwen, Gemma 등과 같은 인기 모델 아키텍처를 자동으로 최적화 가능

### 장점

- **개발 시간 단축**: 다양한 최적화 기술을 수동으로 실험할 필요 없음
- **성능 향상**: 최대 6배의 속도 개선 가능
- **크로스 플랫폼 배포**: 최적화된 모델은 다양한 하드웨어 및 운영 체제에서 작동
- **정확도 유지**: 최적화는 성능을 개선하면서 모델 품질을 유지

## 설치

### 사전 요구 사항

- Python 3.8 이상
- pip 패키지 관리자
- 가상 환경 (권장)

### 기본 설치

가상 환경을 생성하고 활성화합니다:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

자동 최적화 기능이 포함된 Olive를 설치합니다:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### 선택적 종속성

추가 기능을 위한 다양한 선택적 종속성을 제공합니다:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### 설치 확인

```bash
olive --help
```

설치가 성공하면 Olive CLI 도움말 메시지가 표시됩니다.

## 빠른 시작 가이드

### 첫 번째 최적화

Olive의 자동 최적화 기능을 사용하여 작은 언어 모델을 최적화해 봅시다:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 이 명령어가 수행하는 작업

최적화 과정은 다음을 포함합니다: 로컬 캐시에서 모델을 가져오기, ONNX 그래프를 캡처하고 가중치를 ONNX 데이터 파일에 저장, ONNX 그래프 최적화, RTN 방법을 사용하여 모델을 int4로 양자화.

### 명령어 매개변수 설명

- `--model_name_or_path`: Hugging Face 모델 식별자 또는 로컬 경로
- `--output_path`: 최적화된 모델이 저장될 디렉토리
- `--device`: 대상 장치 (cpu, gpu)
- `--provider`: 실행 제공자 (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: 추론을 위한 ONNX Runtime Generate AI 사용
- `--precision`: 양자화 정밀도 (int4, int8, fp16)
- `--log_level`: 로깅 상세 수준 (0=최소, 1=상세)

## 예제: Qwen3를 ONNX INT4로 변환하기

Hugging Face의 [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) 예제를 기반으로 Qwen3 모델을 최적화하는 방법을 살펴봅시다:

### 1단계: 모델 다운로드 (선택 사항)

다운로드 시간을 최소화하기 위해 필수 파일만 캐시합니다:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 2단계: Qwen3 모델 최적화

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 3단계: 최적화된 모델 테스트

최적화된 모델을 테스트하기 위한 간단한 Python 스크립트를 작성합니다:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### 출력 구조

최적화 후 출력 디렉토리에는 다음이 포함됩니다:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## 고급 사용법

### 구성 파일

더 복잡한 최적화 워크플로를 위해 JSON 구성 파일을 사용할 수 있습니다:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

구성을 사용하여 실행:

```bash
olive run --config config.json
```

### GPU 최적화

CUDA GPU 최적화를 위해:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows)의 경우:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive를 사용한 미세 조정

Olive는 모델 미세 조정도 지원합니다:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## 모범 사례

### 1. 모델 선택
- 테스트를 위해 작은 모델로 시작 (예: 0.5B-7B 매개변수)
- 대상 모델 아키텍처가 Olive에서 지원되는지 확인

### 2. 하드웨어 고려 사항
- 최적화 대상이 배포 하드웨어와 일치하도록 설정
- CUDA 호환 하드웨어가 있는 경우 GPU 최적화 사용
- Windows 머신의 통합 그래픽을 사용하는 경우 DirectML 고려

### 3. 정밀도 선택
- **INT4**: 최대 압축, 약간의 정확도 손실
- **INT8**: 크기와 정확도의 균형
- **FP16**: 최소한의 정확도 손실, 중간 크기 감소

### 4. 테스트 및 검증
- 최적화된 모델을 특정 사용 사례로 테스트
- 성능 지표 비교 (지연 시간, 처리량, 정확도)
- 평가를 위한 대표적인 입력 데이터를 사용

### 5. 반복 최적화
- 빠른 결과를 위해 자동 최적화로 시작
- 세부 제어를 위해 구성 파일 사용
- 다양한 최적화 패스를 실험

## 문제 해결

### 일반적인 문제

#### 1. 설치 문제
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU 문제
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. 메모리 문제
- 최적화 중 작은 배치 크기 사용
- 먼저 높은 정밀도로 양자화 시도 (int8 대신 int4)
- 모델 캐싱을 위한 충분한 디스크 공간 확보

#### 4. 모델 로딩 오류
- 모델 경로 및 접근 권한 확인
- 모델이 `trust_remote_code=True`를 요구하는지 확인
- 필요한 모든 모델 파일이 다운로드되었는지 확인

### 도움 받기

- **문서**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub 이슈**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **예제**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## 추가 자료

### 공식 링크
- **GitHub 저장소**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime 문서**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face 예제**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### 커뮤니티 예제
- **Jupyter 노트북**: Olive GitHub 저장소에서 제공
- **VS Code 확장**: AI Toolkit 확장은 모델 최적화를 위해 Olive를 사용
- **블로그 게시물**: Microsoft Open Source Blog에서 자세한 Olive 튜토리얼 제공

### 관련 도구
- **ONNX Runtime**: 고성능 추론 엔진
- **Hugging Face Transformers**: 많은 호환 모델의 출처
- **Azure Machine Learning**: 클라우드 기반 최적화 워크플로

## ➡️ 다음 단계

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.