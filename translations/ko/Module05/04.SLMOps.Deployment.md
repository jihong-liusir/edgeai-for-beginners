<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:14:05+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ko"
}
-->
# 섹션 4: 배포 - 프로덕션 준비 모델 구현

## 개요

이 종합 튜토리얼은 Foundry Local을 사용하여 미세 조정된 양자화 모델을 배포하는 전체 과정을 안내합니다. 모델 변환, 양자화 최적화, 배포 설정을 처음부터 끝까지 다룰 것입니다.

## 사전 요구사항

시작하기 전에 다음을 준비하세요:

- ✅ 배포 준비가 된 미세 조정된 ONNX 모델
- ✅ Windows 또는 Mac 컴퓨터
- ✅ Python 3.10 이상
- ✅ 최소 8GB의 사용 가능한 RAM
- ✅ 시스템에 설치된 Foundry Local

## 1부: 환경 설정

### 필수 도구 설치

터미널(Windows에서는 명령 프롬프트, Mac에서는 터미널)을 열고 다음 명령어를 순서대로 실행하세요:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **중요 참고사항**: CMake 3.31 버전 이상도 필요하며, [cmake.org](https://cmake.org/download/)에서 다운로드할 수 있습니다.

## 2부: 모델 변환 및 양자화

### 올바른 형식 선택

미세 조정된 소형 언어 모델의 경우 **ONNX 형식**을 사용하는 것을 권장합니다. 이유는 다음과 같습니다:

- 🚀 더 나은 성능 최적화
- 🔧 하드웨어에 구애받지 않는 배포
- 🏭 프로덕션 준비 완료 기능
- 📱 크로스 플랫폼 호환성

### 방법 1: 단일 명령어 변환 (권장)

다음 명령어를 사용하여 미세 조정된 모델을 직접 변환하세요:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**매개변수 설명:**
- `--model_name_or_path`: 미세 조정된 모델의 경로
- `--device cpu`: 최적화를 위해 CPU 사용
- `--precision int4`: INT4 양자화 사용 (약 75% 크기 감소)
- `--output_path`: 변환된 모델의 출력 경로

### 방법 2: 구성 파일 접근법 (고급 사용자용)

`finetuned_conversion_config.json`이라는 구성 파일을 생성하세요:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

그런 다음 실행하세요:

```bash
olive run --config ./finetuned_conversion_config.json
```

### 양자화 옵션 비교

| 정밀도   | 파일 크기        | 추론 속도      | 모델 품질      | 권장 사용 사례       |
|-----------|------------------|----------------|----------------|---------------------|
| FP16      | 기준 × 0.5       | 빠름           | 최고           | 고급 하드웨어        |
| INT8      | 기준 × 0.25      | 매우 빠름      | 좋음           | 균형 잡힌 선택       |
| INT4      | 기준 × 0.125     | 가장 빠름      | 수용 가능      | 자원이 제한된 환경   |

💡 **권장사항**: 첫 배포 시 INT4 양자화를 사용하세요. 품질이 만족스럽지 않다면 INT8 또는 FP16을 시도해보세요.

## 3부: Foundry Local 배포 설정

### 모델 구성 생성

Foundry Local 모델 디렉터리로 이동하세요:

```bash
foundry cache cd ./models/
```

모델 디렉터리 구조를 생성하세요:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

모델 디렉터리에 `inference_model.json` 구성 파일을 생성하세요:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### 모델별 템플릿 구성

#### Qwen 시리즈 모델의 경우:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## 4부: 모델 테스트 및 최적화

### 모델 설치 확인

Foundry Local이 모델을 인식할 수 있는지 확인하세요:

```bash
foundry cache ls
```

`your-finetuned-model-int4`가 목록에 표시되어야 합니다.

### 모델 테스트 시작

```bash
foundry model run your-finetuned-model-int4
```

### 성능 벤치마킹

테스트 중 다음 주요 지표를 모니터링하세요:

1. **응답 시간**: 응답당 평균 시간 측정
2. **메모리 사용량**: RAM 소비량 모니터링
3. **CPU 활용도**: 프로세서 부하 확인
4. **출력 품질**: 응답의 관련성과 일관성 평가

### 품질 검증 체크리스트

- ✅ 모델이 미세 조정된 도메인 쿼리에 적절히 응답함
- ✅ 응답 형식이 예상 출력 구조와 일치함
- ✅ 장시간 사용 중 메모리 누수가 없음
- ✅ 다양한 입력 길이에 걸쳐 일관된 성능 제공
- ✅ 엣지 케이스 및 잘못된 입력을 적절히 처리함

## 요약

축하합니다! 다음을 성공적으로 완료했습니다:

- ✅ 미세 조정된 모델 형식 변환
- ✅ 모델 양자화 최적화
- ✅ Foundry Local 배포 설정
- ✅ 성능 튜닝 및 문제 해결

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.