<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-07-22T04:24:57+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "ko"
}
-->
# 섹션 3: 파인 튜닝 - 특정 작업을 위한 모델 맞춤화

## 목차
1. [파인 튜닝 소개](../../../Module05)
2. [파인 튜닝이 중요한 이유](../../../Module05)
3. [파인 튜닝의 유형](../../../Module05)
4. [Microsoft Olive를 활용한 파인 튜닝](../../../Module05)
5. [실습 예제](../../../Module05)
6. [모범 사례 및 가이드라인](../../../Module05)
7. [고급 기술](../../../Module05)
8. [평가 및 모니터링](../../../Module05)
9. [일반적인 문제와 해결책](../../../Module05)
10. [결론](../../../Module05)

## 파인 튜닝 소개

**파인 튜닝**은 사전 학습된 모델을 특정 작업이나 전문화된 데이터셋에 맞게 조정하는 강력한 머신 러닝 기술입니다. 모델을 처음부터 학습시키는 대신, 사전 학습된 모델이 이미 학습한 지식을 활용하여 특정 사용 사례에 맞게 조정합니다.

### 파인 튜닝이란?

파인 튜닝은 **전이 학습**의 한 형태로 다음을 포함합니다:
- 대규모 데이터셋에서 일반적인 패턴을 학습한 사전 학습된 모델을 시작점으로 사용
- 특정 데이터셋을 사용하여 모델의 내부 매개변수를 조정
- 기존 지식을 유지하면서 작업에 맞게 모델을 전문화

이는 숙련된 셰프에게 새로운 요리를 가르치는 것과 비슷합니다. 셰프는 이미 요리의 기본을 이해하고 있지만, 새로운 스타일의 기술과 맛을 배워야 합니다.

### 주요 장점

- **시간 효율성**: 처음부터 학습시키는 것보다 훨씬 빠름
- **데이터 효율성**: 적은 데이터셋으로도 좋은 성능을 달성 가능
- **비용 효율성**: 낮은 계산 요구 사항
- **더 나은 성능**: 처음부터 학습시키는 것보다 우수한 결과를 자주 달성
- **자원 최적화**: 소규모 팀과 조직에서도 강력한 AI를 활용 가능

## 파인 튜닝이 중요한 이유

### 실제 응용 사례

파인 튜닝은 다양한 상황에서 필수적입니다:

**1. 도메인 적응**
- 의료 AI: 일반 언어 모델을 의료 용어와 임상 기록에 맞게 조정
- 법률 기술: 법률 문서 분석 및 계약 검토를 위한 모델 전문화
- 금융 서비스: 금융 보고서 분석 및 위험 평가를 위한 모델 맞춤화

**2. 작업 전문화**
- 콘텐츠 생성: 특정 글쓰기 스타일이나 톤에 맞게 파인 튜닝
- 코드 생성: 특정 프로그래밍 언어나 프레임워크에 맞게 모델 조정
- 번역: 특정 언어 쌍이나 기술 도메인에 대한 성능 향상

**3. 기업 응용**
- 고객 서비스: 회사 고유의 용어를 이해하는 챗봇 생성
- 내부 문서화: 조직 프로세스를 잘 아는 AI 어시스턴트 구축
- 산업별 솔루션: 특정 분야의 전문 용어와 워크플로를 이해하는 모델 개발

## 파인 튜닝의 유형

### 1. 전체 파인 튜닝 (명령어 파인 튜닝)

전체 파인 튜닝에서는 모든 모델 매개변수가 학습 중에 업데이트됩니다. 이 접근법은:
- 최대한의 유연성과 성능 잠재력을 제공
- 상당한 계산 자원이 필요
- 완전히 새로운 버전의 모델 생성
- 충분한 학습 데이터와 계산 자원이 있는 경우에 적합

### 2. 매개변수 효율적 파인 튜닝 (PEFT)

PEFT 방법은 소수의 매개변수만 업데이트하여 효율성을 높입니다:

#### 저랭크 적응 (LoRA)
- 기존 가중치에 작은 학습 가능한 랭크 분해 행렬 추가
- 학습 가능한 매개변수 수를 크게 줄임
- 전체 파인 튜닝에 가까운 성능 유지
- 다양한 적응 간 손쉬운 전환 가능

#### QLoRA (양자화된 LoRA)
- LoRA와 양자화 기술을 결합
- 메모리 요구 사항을 더욱 줄임
- 소비자 하드웨어에서 더 큰 모델의 파인 튜닝 가능
- 효율성과 성능의 균형 유지

#### 어댑터
- 기존 레이어 사이에 작은 신경망 삽입
- 기본 모델을 고정하면서 목표 지향적 파인 튜닝 가능
- 모델 맞춤화에 모듈식 접근 방식 제공

### 3. 작업별 파인 튜닝

특정 다운스트림 작업에 맞게 모델을 조정:
- **분류**: 분류 작업을 위한 모델 조정
- **생성**: 콘텐츠 생성 및 텍스트 생성 최적화
- **추출**: 정보 추출 및 명명된 엔터티 인식에 대한 파인 튜닝
- **요약**: 문서 요약을 위한 모델 전문화

## Microsoft Olive를 활용한 파인 튜닝

Microsoft Olive는 파인 튜닝 프로세스를 간소화하면서 엔터프라이즈급 기능을 제공하는 종합적인 모델 최적화 도구입니다.

### Microsoft Olive란?

Microsoft Olive는 다음을 제공하는 오픈 소스 모델 최적화 도구입니다:
- 다양한 하드웨어 대상에 대한 파인 튜닝 워크플로 간소화
- 인기 있는 모델 아키텍처(Llama, Phi, Qwen, Gemma)에 대한 내장 지원
- 클라우드 및 로컬 배포 옵션 제공
- Azure ML 및 기타 Microsoft AI 서비스와 원활하게 통합
- 자동 최적화 및 양자화를 지원

### 주요 기능

- **하드웨어 인식 최적화**: 특정 하드웨어(CPU, GPU, NPU)에 맞게 모델 자동 최적화
- **다중 형식 지원**: PyTorch, Hugging Face, ONNX 모델과 호환
- **자동화된 워크플로**: 수동 구성 및 시행착오 감소
- **엔터프라이즈 통합**: Azure ML 및 클라우드 배포에 대한 내장 지원
- **확장 가능한 아키텍처**: 사용자 정의 최적화 기술 허용

### 설치 및 설정

#### 기본 설치

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### 선택적 종속성

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### 설치 확인

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## 실습 예제

### 예제 1: Olive CLI를 활용한 기본 파인 튜닝

이 예제는 작은 언어 모델을 구문 분류를 위해 파인 튜닝하는 방법을 보여줍니다:

#### 1단계: 환경 준비

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### 2단계: 모델 파인 튜닝

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### 3단계: 배포를 위한 최적화

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### 예제 2: 사용자 정의 데이터셋을 활용한 고급 설정

#### 1단계: 사용자 정의 데이터셋 준비

학습 데이터를 포함한 JSON 파일 생성:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### 2단계: 구성 파일 생성

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### 3단계: 파인 튜닝 실행

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### 예제 3: 메모리 효율성을 위한 QLoRA 파인 튜닝

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## 모범 사례 및 가이드라인

### 데이터 준비

**1. 데이터 품질 우선**
- 대량의 저품질 데이터보다 고품질, 다양한 예제를 우선시
- 목표 사용 사례를 대표하는 데이터 확보
- 일관된 데이터 정리 및 전처리

**2. 데이터 형식 및 템플릿**
- 모든 학습 예제에서 일관된 형식 사용
- 사용 사례에 맞는 명확한 입력-출력 템플릿 생성
- 명령어 튜닝된 모델에 적합한 명령어 형식 포함

**3. 데이터셋 분할**
- 데이터의 10-20%를 검증용으로 예약
- 학습/검증 분할 간 유사한 분포 유지
- 분류 작업의 경우 계층적 샘플링 고려

### 학습 구성

**1. 학습률 선택**
- 파인 튜닝을 위해 작은 학습률(1e-5 ~ 1e-4)로 시작
- 더 나은 수렴을 위해 학습률 스케줄링 사용
- 손실 곡선을 모니터링하여 학습률 조정

**2. 배치 크기 최적화**
- 사용 가능한 메모리와 배치 크기 균형
- 더 큰 효과적인 배치 크기를 위해 그래디언트 누적 사용
- 배치 크기와 학습률 간 관계 고려

**3. 학습 기간**
- 검증 지표를 모니터링하여 과적합 방지
- 검증 성능이 정체되면 조기 중지 사용
- 복구 및 분석을 위해 정기적으로 체크포인트 저장

### 모델 선택

**1. 기본 모델 선택**
- 가능하면 유사한 도메인에서 사전 학습된 모델 선택
- 계산 제약에 맞는 모델 크기 고려
- 상업적 사용을 위한 라이선스 요구 사항 평가

**2. 파인 튜닝 방법 선택**
- 자원 제약 환경에서는 LoRA/QLoRA 사용
- 최대 성능이 중요한 경우 전체 파인 튜닝 선택
- 여러 작업 시나리오에 대해 어댑터 기반 접근 방식 고려

### 자원 관리

**1. 하드웨어 최적화**
- 모델 크기와 방법에 적합한 하드웨어 선택
- 그래디언트 체크포인트를 활용하여 GPU 메모리 효율적으로 사용
- 더 큰 모델의 경우 클라우드 기반 솔루션 고려

**2. 메모리 관리**
- 가능하면 혼합 정밀 학습 사용
- 메모리 제약을 위해 그래디언트 누적 구현
- 학습 중 GPU 메모리 사용량 모니터링

## 고급 기술

### 다중 어댑터 학습

기본 모델을 공유하면서 다양한 작업을 위한 여러 어댑터 학습:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### 하이퍼파라미터 최적화

체계적인 하이퍼파라미터 튜닝 구현:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### 사용자 정의 손실 함수

도메인별 손실 함수 구현:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## 평가 및 모니터링

### 지표 및 평가

**1. 표준 지표**
- **정확도**: 분류 작업의 전체 정확도
- **Perplexity**: 언어 모델링 품질 측정
- **BLEU/ROUGE**: 텍스트 생성 및 요약 품질
- **F1 점수**: 분류 작업의 정밀도와 재현율의 균형

**2. 도메인별 지표**
- **작업별 벤치마크**: 도메인에 대한 확립된 벤치마크 사용
- **인간 평가**: 주관적인 작업에 인간 평가 포함
- **비즈니스 지표**: 실제 비즈니스 목표와 일치

**3. 평가 설정**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### 학습 진행 모니터링

**1. 손실 추적**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. 검증 모니터링**
- 학습 손실과 함께 검증 손실 추적
- 과적합 징후 모니터링(학습 손실 감소, 검증 손실 증가)
- 검증 지표를 기반으로 조기 중지 사용

**3. 자원 모니터링**
- GPU/CPU 활용도 모니터링
- 메모리 사용 패턴 추적
- 학습 속도 및 처리량 모니터링

## 일반적인 문제와 해결책

### 문제 1: 과적합

**증상:**
- 학습 손실은 계속 감소하지만 검증 손실은 증가
- 학습과 검증 성능 간 큰 차이
- 새로운 데이터에 대한 일반화 부족

**해결책:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### 문제 2: 메모리 제한

**해결책:**
- 그래디언트 체크포인트 사용
- 그래디언트 누적 구현
- 매개변수 효율적 방법(LoRA, QLoRA) 선택
- 큰 모델에 대해 모델 병렬화 활용

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### 문제 3: 느린 학습

**해결책:**
- 데이터 로딩 파이프라인 최적화
- 혼합 정밀 학습 사용
- 효율적인 배칭 전략 구현
- 대규모 데이터셋에 대해 분산 학습 고려

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### 문제 4: 성능 저하

**진단 단계:**
1. 데이터 품질 및 형식 확인
2. 학습률 및 학습 기간 점검
3. 기본 모델 선택 평가
4. 전처리 및 토크나이제이션 검토

**해결책:**
- 학습 데이터 다양성 증가
- 학습률 스케줄 조정
- 다른 기본 모델 시도
- 데이터 증강 기술 구현

## 결론

파인 튜닝은 최첨단 AI 기능에 대한 접근을 민주화하는 강력한 기술입니다. Microsoft Olive와 같은 도구를 활용하면 조직은 사전 학습된 모델을 효율적으로 특정 요구에 맞게 조정하면서 성능과 자원 제약을 최적화할 수 있습니다.

### 주요 요점

1. **적합한 접근법 선택**: 계산 자원과 성능 요구 사항에 따라 파인 튜닝 방법 선택
2. **데이터 품질 중요성**: 고품질, 대표적인 학습 데이터에 투자
3. **모니터링 및 반복**: 모델을 지속적으로 평가하고 개선
4. **도구 활용**: Olive와 같은 프레임워크를 사용하여 프로세스 간소화 및 최적화
5. **배포 고려**: 처음부터 모델 최적화 및 배포를 계획

## ➡️ 다음 단계

- [04: 배포 - 프로덕션 준비 모델 구현](./04.SLMOps.Deployment.md)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.