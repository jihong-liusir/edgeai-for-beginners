<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-07-22T05:00:23+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "ko"
}
-->
# 컨테이너화된 클라우드 배포 - 프로덕션 규모 솔루션

이 종합 튜토리얼은 Microsoft의 Phi-4-mini-instruct 모델을 컨테이너화된 환경에서 배포하는 세 가지 주요 접근 방식을 다룹니다: vLLM, Ollama, SLM Engine with ONNX Runtime. 3.8B 파라미터를 가진 이 모델은 효율성을 유지하면서도 추론 작업에 최적화된 선택지로, 엣지 배포에 적합합니다.

## 목차

1. [Phi-4-mini 컨테이너 배포 소개](../../../Module03)
2. [학습 목표](../../../Module03)
3. [Phi-4-mini 분류 이해하기](../../../Module03)
4. [vLLM 컨테이너 배포](../../../Module03)
5. [Ollama 컨테이너 배포](../../../Module03)
6. [SLM Engine with ONNX Runtime](../../../Module03)
7. [비교 프레임워크](../../../Module03)
8. [모범 사례](../../../Module03)

## Phi-4-mini 컨테이너 배포 소개

소형 언어 모델(SLM)은 EdgeAI에서 중요한 발전을 이루며, 자원이 제한된 장치에서도 정교한 자연어 처리 기능을 제공합니다. 이 튜토리얼은 Microsoft의 Phi-4-mini-instruct 모델을 컨테이너화된 환경에서 배포하기 위한 전략에 중점을 둡니다. 이 모델은 성능과 효율성을 균형 있게 유지하는 최첨단 추론 모델입니다.

### 주요 모델: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B 파라미터)**: Microsoft의 최신 경량화된 명령어 튜닝 모델로, 메모리/연산 자원이 제한된 환경에서도 뛰어난 성능을 발휘합니다:
- **수학적 추론 및 복잡한 계산**
- **코드 생성, 디버깅 및 분석**
- **논리적 문제 해결 및 단계별 추론**
- **자세한 설명이 필요한 교육용 애플리케이션**
- **함수 호출 및 도구 통합**

"소형 SLM" 카테고리(1.5B - 13.9B 파라미터)에 속하는 Phi-4-mini는 추론 능력과 자원 효율성 간의 최적의 균형을 제공합니다.

### Phi-4-mini 컨테이너 배포의 이점

- **운영 효율성**: 낮은 연산 요구사항으로 빠른 추론 제공
- **배포 유연성**: 로컬 처리로 프라이버시를 강화한 온디바이스 AI 기능
- **비용 효율성**: 대형 모델 대비 낮은 운영 비용으로 품질 유지
- **격리성**: 모델 인스턴스 간의 명확한 분리 및 안전한 실행 환경
- **확장성**: 추론 처리량 증가를 위한 수평적 확장 용이

## 학습 목표

이 튜토리얼을 완료하면 다음을 수행할 수 있습니다:

- 다양한 컨테이너화된 환경에서 Phi-4-mini-instruct를 배포 및 최적화
- 다양한 배포 시나리오에 맞는 고급 양자화 및 압축 전략 구현
- 추론 작업을 위한 프로덕션 준비 컨테이너 오케스트레이션 구성
- 특정 사용 사례 요구사항에 따라 적합한 배포 프레임워크 평가 및 선택
- 컨테이너화된 SLM 배포를 위한 보안, 모니터링 및 확장 모범 사례 적용

## Phi-4-mini 분류 이해하기

### 모델 사양

**기술 세부사항:**
- **파라미터**: 3.8억 (소형 SLM 카테고리)
- **아키텍처**: Dense decoder-only Transformer with grouped-query attention
- **컨텍스트 길이**: 128K 토큰 (최적 성능을 위한 권장값: 32K)
- **어휘**: 200K 토큰, 다국어 지원
- **학습 데이터**: 5T 토큰의 고품질 추론 중심 콘텐츠

### 자원 요구사항

| 배포 유형 | 최소 RAM | 권장 RAM | VRAM (GPU) | 스토리지 | 일반적인 사용 사례 |
|-----------|----------|----------|------------|----------|--------------------|
| **개발** | 6GB | 8GB | - | 8GB | 로컬 테스트, 프로토타이핑 |
| **프로덕션 CPU** | 8GB | 12GB | - | 10GB | 엣지 서버, 비용 최적화 배포 |
| **프로덕션 GPU** | 6GB | 8GB | 4-6GB | 8GB | 고처리량 추론 서비스 |
| **엣지 최적화** | 4GB | 6GB | - | 6GB | 양자화 배포, IoT 게이트웨이 |

### Phi-4-mini 기능

- **수학적 우수성**: 고급 산술, 대수학, 미적분 문제 해결
- **코드 지능**: Python, JavaScript 및 다중 언어 코드 생성과 디버깅
- **논리적 추론**: 단계별 문제 분해 및 솔루션 구성
- **교육 지원**: 학습 및 교육 시나리오에 적합한 상세한 설명
- **함수 호출**: 도구 통합 및 API 상호작용에 대한 네이티브 지원

## vLLM 컨테이너 배포

vLLM은 최적화된 추론 성능과 OpenAI 호환 API를 제공하여 Phi-4-mini-instruct의 프로덕션 추론 서비스에 이상적입니다.

### 빠른 시작 예제

#### 기본 CPU 배포 (개발)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU 가속 프로덕션 배포
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### 프로덕션 구성

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Phi-4-mini 추론 기능 테스트

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama 컨테이너 배포

Ollama는 간소화된 배포 및 관리를 제공하여 Phi-4-mini-instruct의 개발 및 균형 잡힌 프로덕션 배포에 적합합니다.

### 빠른 설정

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### 프로덕션 구성

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### 모델 최적화 및 변형

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API 사용 예제

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine with ONNX Runtime

ONNX Runtime은 고급 최적화와 크로스 플랫폼 호환성을 통해 Phi-4-mini-instruct의 엣지 배포에 최적의 성능을 제공합니다.

### 기본 설정

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### 간소화된 서버 구현

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### 모델 변환 스크립트

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### 프로덕션 구성

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ONNX 배포 테스트

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## 비교 프레임워크

### Phi-4-mini를 위한 프레임워크 비교

| 기능 | vLLM | Ollama | ONNX Runtime |
|------|------|--------|--------------|
| **설정 복잡도** | 보통 | 쉬움 | 복잡 |
| **성능 (GPU)** | 우수 (~25 tok/s) | 매우 좋음 (~20 tok/s) | 좋음 (~15 tok/s) |
| **성능 (CPU)** | 좋음 (~8 tok/s) | 매우 좋음 (~12 tok/s) | 우수 (~15 tok/s) |
| **메모리 사용량** | 8-12GB | 6-10GB | 4-8GB |
| **API 호환성** | OpenAI 호환 | 커스텀 REST | 커스텀 FastAPI |
| **함수 호출** | ✅ 네이티브 | ✅ 지원 | ⚠️ 커스텀 구현 |
| **양자화 지원** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX 양자화 |
| **프로덕션 준비** | ✅ 우수 | ✅ 매우 좋음 | ✅ 좋음 |
| **엣지 배포** | 좋음 | 우수 | 뛰어남 |

## 추가 자료

### 공식 문서
- **Microsoft Phi-4 모델 카드**: 상세 사양 및 사용 지침
- **vLLM 문서**: 고급 구성 및 최적화 옵션
- **Ollama 모델 라이브러리**: 커뮤니티 모델 및 사용자 정의 예제
- **ONNX Runtime 가이드**: 성능 최적화 및 배포 전략

### 개발 도구
- **Hugging Face Transformers**: 모델 상호작용 및 사용자 정의
- **OpenAI API 사양**: vLLM 호환성 테스트
- **Docker 모범 사례**: 컨테이너 보안 및 최적화 지침
- **Kubernetes 배포**: 프로덕션 확장을 위한 오케스트레이션 패턴

### 학습 자료
- **SLM 성능 벤치마킹**: 비교 분석 방법론
- **엣지 AI 배포**: 자원 제한 환경을 위한 모범 사례
- **추론 작업 최적화**: 수학 및 논리 문제를 위한 프롬프트 전략
- **컨테이너 보안**: AI 모델 배포를 위한 강화 방법

## 학습 결과

이 모듈을 완료하면 다음을 수행할 수 있습니다:

1. 다양한 프레임워크를 사용하여 컨테이너화된 환경에서 Phi-4-mini-instruct 모델 배포
2. 다양한 하드웨어 환경에 맞게 SLM 배포 구성 및 최적화
3. 컨테이너화된 AI 배포를 위한 보안 모범 사례 구현
4. 특정 사용 사례 요구사항에 따라 적합한 배포 프레임워크 비교 및 선택
5. 프로덕션급 SLM 서비스를 위한 모니터링 및 확장 전략 적용

## 다음 단계

- [모듈 1](../Module01/README.md)로 돌아가기
- [모듈 2](../Module02/README.md)로 돌아가기

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.