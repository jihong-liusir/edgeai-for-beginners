<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-07-22T04:52:03+00:00",
  "source_file": "Module03/README.md",
  "language_code": "ko"
}
-->
# Chapter 03: 소형 언어 모델(SLM) 배포

이 포괄적인 챕터는 소형 언어 모델(SLM)의 배포 전 과정을 탐구하며, 이론적 기초, 실질적인 구현 전략, 그리고 생산 준비가 된 컨테이너화된 솔루션을 다룹니다. 챕터는 독자들을 기본 개념에서 고급 배포 시나리오로 안내하는 세 가지 점진적인 섹션으로 구성되어 있습니다.

## 챕터 구조와 학습 여정

### **[Section 1: SLM 고급 학습 - 기초와 최적화](./01.SLMAdvancedLearning.md)**
첫 번째 섹션은 소형 언어 모델을 이해하고 엣지 AI 배포에서의 전략적 중요성을 확립하기 위한 이론적 기초를 제공합니다. 이 섹션에서는 다음을 다룹니다:

- **파라미터 분류 프레임워크**: Micro SLM(100M-1.4B 파라미터)에서 Medium SLM(14B-30B 파라미터)까지의 SLM 카테고리를 상세히 탐구하며, Phi-4-mini-3.8B, Qwen3 시리즈, Google Gemma3와 같은 모델에 초점을 맞추고 각 모델 계층의 하드웨어 요구 사항 및 메모리 사용 분석을 포함
- **고급 최적화 기술**: Llama.cpp, Microsoft Olive, Apple MLX 프레임워크를 활용한 양자화 방법에 대한 포괄적인 설명, BitNET 1비트 양자화와 같은 최신 기술을 포함한 양자화 파이프라인 및 벤치마킹 결과를 보여주는 실질적인 코드 예제
- **모델 획득 전략**: Hugging Face 생태계와 Azure AI Foundry Model Catalog를 활용한 엔터프라이즈급 SLM 배포를 위한 심층 분석, 프로그래밍 방식으로 모델을 다운로드, 검증 및 형식 변환하는 코드 샘플 포함
- **개발자 API**: Python, C++, C# 코드 예제를 통해 모델 로드, 추론 수행, PyTorch, TensorFlow, ONNX Runtime과 같은 인기 있는 프레임워크와 통합하는 방법을 보여줌

이 기초 섹션은 운영 효율성, 배포 유연성, 비용 효율성 간의 균형을 강조하며, 엣지 컴퓨팅 시나리오에 적합한 SLM의 실질적인 코드 예제를 통해 개발자가 프로젝트에 직접 구현할 수 있도록 합니다.

### **[Section 2: 로컬 환경 배포 - 프라이버시 우선 솔루션](./02.DeployingSLMinLocalEnv.md)**
두 번째 섹션은 이론에서 실질적인 구현으로 전환하며, 데이터 주권과 운영 독립성을 우선시하는 로컬 배포 전략에 초점을 맞춥니다. 주요 내용은 다음과 같습니다:

- **Ollama Universal Platform**: 개발자 친화적인 워크플로우, 모델 라이프사이클 관리, Modelfiles를 통한 맞춤화에 중점을 둔 크로스 플랫폼 배포에 대한 포괄적인 탐구, REST API 통합 예제 및 CLI 자동화 스크립트 포함
- **Microsoft Foundry Local**: ONNX 기반 최적화, Windows ML 통합, 포괄적인 보안 기능을 갖춘 엔터프라이즈급 배포 솔루션, 네이티브 애플리케이션 통합을 위한 C# 및 Python 코드 예제 포함
- **비교 분석**: 기술 아키텍처, 성능 특성, 사용 사례 최적화 지침을 다룬 상세한 프레임워크 비교, 다양한 하드웨어에서 추론 속도와 메모리 사용을 평가하는 벤치마크 코드 포함
- **API 통합**: 로컬 SLM 배포를 사용하여 웹 서비스, 채팅 애플리케이션, 데이터 처리 파이프라인을 구축하는 방법을 보여주는 샘플 애플리케이션, Node.js, Python Flask/FastAPI, ASP.NET Core 코드 예제 포함
- **테스트 프레임워크**: 모델 품질 보증을 위한 자동화된 테스트 접근 방식, SLM 구현을 위한 단위 및 통합 테스트 예제 포함

이 섹션은 조직이 배포 환경에 대한 완전한 통제를 유지하면서 프라이버시를 보호하는 AI 솔루션을 구현하려는 실질적인 지침을 제공하며, 개발자가 특정 요구 사항에 맞게 조정할 수 있는 즉시 사용 가능한 코드 샘플을 제공합니다.

### **[Section 3: 컨테이너화된 클라우드 배포 - 생산 규모 솔루션](./03.DeployingSLMinCloud.md)**
마지막 섹션은 Microsoft의 Phi-4-mini-instruct를 주요 사례 연구로 삼아 고급 컨테이너화된 배포 전략을 다룹니다. 이 섹션에서는 다음을 다룹니다:

- **vLLM 배포**: OpenAI 호환 API, 고급 GPU 가속, 생산 준비 구성으로 고성능 추론 최적화, 완전한 Dockerfile, Kubernetes 매니페스트, 성능 튜닝 매개변수 포함
- **Ollama Container Orchestration**: Docker Compose를 활용한 간소화된 배포 워크플로우, 모델 최적화 변형 및 웹 UI 통합, 자동화된 배포 및 테스트를 위한 CI/CD 파이프라인 예제 포함
- **ONNX Runtime 구현**: 포괄적인 모델 변환, 양자화 전략, 크로스 플랫폼 호환성을 갖춘 엣지 최적화 배포, 모델 최적화 및 배포를 위한 상세한 코드 샘플 포함
- **모니터링 및 관찰성**: SLM 성능 모니터링을 위한 맞춤형 메트릭을 포함한 Prometheus/Grafana 대시보드 구현, 경고 구성 및 로그 집계 포함
- **로드 밸런싱 및 확장**: CPU/GPU 사용량 및 요청 패턴에 기반한 자동 확장 구성을 포함한 수평 및 수직 확장 전략의 실질적인 예제
- **보안 강화**: API 키 및 모델 액세스 자격 증명에 대한 비밀 관리, 네트워크 정책, 권한 축소를 포함한 컨테이너 보안 모범 사례

각 배포 접근 방식은 완전한 구성 예제, 테스트 절차, 생산 준비 체크리스트, 그리고 개발자가 배포 워크플로우에 직접 적용할 수 있는 인프라 코드 템플릿과 함께 제공됩니다.

## 주요 학습 결과

이 챕터를 완료하면 독자는 다음을 숙달하게 됩니다:

1. **전략적 모델 선택**: 자원 제약 및 성능 요구 사항에 따라 적합한 SLM을 선택하는 방법 이해
2. **최적화 숙련도**: 다양한 프레임워크에서 고급 양자화 기술을 구현하여 최적의 성능-효율성 균형 달성
3. **배포 유연성**: 조직의 필요에 따라 로컬 프라이버시 중심 솔루션과 확장 가능한 컨테이너화된 배포 중 선택
4. **생산 준비**: 엔터프라이즈급 SLM 배포를 위한 모니터링, 보안, 확장 시스템 구성

## 실질적인 초점과 실제 응용

챕터는 다음을 포함하여 실질적인 지향성을 유지합니다:

- **실습 예제**: 완전한 구성 파일, API 테스트 절차, 배포 스크립트
- **성능 벤치마킹**: 추론 속도, 메모리 사용, 자원 요구 사항에 대한 상세 비교
- **보안 고려사항**: 엔터프라이즈급 보안 관행, 준수 프레임워크, 데이터 보호 전략
- **모범 사례**: 모니터링, 확장, 유지 관리를 위한 생산 검증 지침

## 미래 지향적 관점

챕터는 다음을 포함한 신흥 트렌드에 대한 통찰력으로 마무리됩니다:

- 효율성 비율이 개선된 고급 모델 아키텍처
- 전문 AI 가속기와의 더 깊은 하드웨어 통합
- 표준화 및 상호 운용성을 향한 생태계 진화
- 프라이버시 및 준수 요구 사항에 의해 주도되는 엔터프라이즈 채택 패턴

이 포괄적인 접근 방식은 독자가 현재 SLM 배포 과제와 미래 기술 개발을 모두 탐색할 수 있도록 준비시키며, 특정 조직 요구 사항 및 제약 조건에 맞는 정보에 입각한 결정을 내릴 수 있도록 합니다.

챕터는 즉각적인 구현을 위한 실질적인 가이드이자 장기적인 AI 배포 계획을 위한 전략적 자원으로서, 성공적인 SLM 배포를 정의하는 역량, 효율성, 운영 우수성 간의 중요한 균형을 강조합니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.