<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a534c7d108d29f908a8f9f693d694664",
  "translation_date": "2025-09-24T10:38:40+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "ko"
}
-->
# 세션 5: Foundry Local로 AI 기반 에이전트 빠르게 구축하기

참고: Foundry Local의 에이전트 기능은 계속 발전하고 있으니, 고급 패턴을 구현하기 전에 최신 릴리스 노트를 확인하세요.

## 개요

Foundry Local을 사용하여 에이전트 애플리케이션을 빠르게 프로토타입화하세요: 시스템 프롬프트, 데이터 기반, 오케스트레이션 패턴. 에이전트 지원이 있는 경우 OpenAI 호환 함수 호출을 표준화하거나 하이브리드 디자인에서 클라우드 측 Azure AI Agents를 사용할 수 있습니다.

> **🔄 최신 SDK에 맞게 업데이트됨**: 이 모듈은 최신 Microsoft Foundry-Local 저장소 패턴에 맞게 조정되었으며 `samples/05/`의 포괄적인 구현과 일치합니다. 예제는 이제 수동 요청 대신 최신 `foundry-local-sdk`와 `OpenAI` 클라이언트를 사용합니다.

**🏗️ 아키텍처 주요 사항:**
- **전문 에이전트**: 검색, 추론, 실행 에이전트로 각각의 고유한 기능 제공
- **코디네이터 패턴**: 피드백 루프를 포함한 다중 에이전트 워크플로를 오케스트레이션
- **최신 SDK 통합**: `FoundryLocalManager`와 OpenAI 클라이언트 사용
- **프로덕션 준비 완료**: 오류 처리, 성능 모니터링, 상태 점검 포함
- **포괄적인 예제**: 고급 기능을 갖춘 대화형 Jupyter 노트북

**📁 로컬 구현:**
- `samples/05/multi_agent_orchestration.ipynb` - 대화형 예제 및 벤치마크
- `samples/05/agents/specialists.py` - 에이전트 구현
- `samples/05/agents/coordinator.py` - 오케스트레이션 로직

참고 자료:
- Foundry Local 문서: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Azure AI Foundry Agents: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- 함수 호출 샘플 (Foundry Local 샘플): https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## 학습 목표
- 신뢰할 수 있는 동작을 위한 시스템 프롬프트 및 데이터 기반 전략 설계
- 함수 호출(도구 사용) 패턴 구현
- 다중 에이전트 워크플로 오케스트레이션(로컬 및 하이브리드)
- 관찰 가능성과 안전성을 위한 계획 수립

## Part 1: 시스템 프롬프트 및 데이터 기반

- 엄격한 역할, 제약 조건, 출력 스키마 정의
- 로컬 또는 엔터프라이즈 데이터를 사용하여 응답 기반 설정
- 다운스트림 자동화를 위한 JSON 출력 강제 적용

## Part 2: 함수 호출 (최신 SDK 접근법)

```python
# tools.py
import json
from typing import List, Dict, Any

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

# Modern tools format for OpenAI API
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a city",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "City name"}
                },
                "required": ["city"]
            }
        }
    }
]
```

```python
# agent.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import json
from tools import TOOLS, get_weather

# Initialize Foundry Local Manager
alias = "phi-4-mini"
manager = FoundryLocalManager(alias)

# Create OpenAI client using Foundry Local endpoint
client = OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key
)

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def process_function_call(messages: List[Dict], tools: List[Dict]) -> str:
    """Process function calling with modern OpenAI API."""
    try:
        response = client.chat.completions.create(
            model=manager.get_model_info(alias).id,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        message = response.choices[0].message
        
        if message.tool_calls:
            # Handle function calls
            messages.append(message)
            
            for tool_call in message.tool_calls:
                if tool_call.function.name == "get_weather":
                    args = json.loads(tool_call.function.arguments)
                    result = get_weather(args["city"])
                    
                    # Add function result to messages
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
            
            # Get final response
            final_response = client.chat.completions.create(
                model=manager.get_model_info(alias).id,
                messages=messages
            )
            return final_response.choices[0].message.content
        else:
            return message.content
            
    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": "What's the weather in Paris?"}
]

result = process_function_call(messages, TOOLS)
print(result)
```

실행:
```powershell
# Ensure Foundry Local is running with a model
foundry model run phi-4-mini
python agent.py
```


## Part 3: 다중 에이전트 오케스트레이션 (패턴)

Foundry Local의 OpenAI 호환 엔드포인트를 사용하여 검색, 추론, 실행 전문 에이전트로 작업을 라우팅하는 코디네이터를 설계하세요.

1단계) 최신 SDK로 전문 에이전트 정의 (`samples/05/agents/specialists.py` 참조)
```python
# agents/specialists.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import List, Dict, Any

class FoundryClient:
    """Shared client for all specialist agents."""
    
    def __init__(self, model_alias: str = "phi-4-mini"):
        self.client = None
        self.model_name = None
        self.model_alias = model_alias
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local."""
        try:
            manager = FoundryLocalManager(self.model_alias)
            model_info = manager.get_model_info(self.model_alias)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.model_name = model_info.id
            print(f"✅ Foundry Local initialized with model: {self.model_name}")
        except Exception as e:
            print(f"❌ Error initializing Foundry Local: {e}")
            raise
    
    def chat(self, messages: List[Dict[str, str]], max_tokens: int = 300, temperature: float = 0.4) -> str:
        """Send chat completion request to the model."""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response: {str(e)}"

# Global client instance
_client = FoundryClient()

class RetrievalAgent:
    """Agent specialized in retrieving relevant information from knowledge sources."""
    
    SYSTEM = """You are a specialized retrieval agent. Your job is to extract and retrieve 
    the most relevant information from knowledge sources based on a given query. Focus on key facts, 
    data points, and contextual information that would be useful for decision-making."""
    
    def run(self, query: str) -> str:
        """Retrieve relevant information based on the query."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Query: {query}\n\nRetrieve the most relevant key facts, data points, and contextual information that would help answer this query or support decision-making around it."}
        ]
        return _client.chat(messages)

class ReasoningAgent:
    """Agent specialized in step-by-step analysis and reasoning."""
    
    SYSTEM = """You are a specialized reasoning agent. Your job is to analyze inputs 
    step-by-step and produce structured, logical conclusions. Break down complex problems 
    into manageable parts and provide clear reasoning for your conclusions."""
    
    def run(self, context: str, question: str) -> str:
        """Analyze context and question to produce structured conclusions."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\n\nAnalyze this step-by-step and provide a structured, logical conclusion with clear reasoning."}
        ]
        return _client.chat(messages, max_tokens=400)

class ExecutionAgent:
    """Agent specialized in creating actionable execution plans."""
    
    SYSTEM = """You are a specialized execution agent. Your job is to transform decisions 
    and conclusions into concrete, actionable steps. Always format your response as valid JSON 
    with an array of action items. Each action should be specific, measurable, and achievable."""
    
    def run(self, decision: str) -> str:
        """Transform decision into actionable steps in JSON format."""
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Decision/Conclusion:\n{decision}\n\nCreate 3-5 specific, actionable steps to implement this decision. Format as JSON with this structure:\n{{\"actions\": [{{\"step\": 1, \"description\": \"...\", \"priority\": \"high/medium/low\", \"timeline\": \"...\"}}]}}"}
        ]
        return _client.chat(messages, max_tokens=400, temperature=0.3)
```

2단계) 고급 기능을 갖춘 코디네이터 구축
```python
# agents/coordinator.py
from .specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent
from typing import Dict, Any
import time
import json

class Coordinator:
    """Multi-agent coordinator that orchestrates specialist agents to handle complex tasks."""
    
    def __init__(self):
        """Initialize the coordinator with specialist agents."""
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()
    
    def handle(self, user_goal: str) -> Dict[str, Any]:
        """
        Orchestrate multiple agents to handle a complex user goal.
        
        Args:
            user_goal: The user's high-level goal or request
            
        Returns:
            Dictionary containing the goal, context, decision, and actions
        """
        print(f"🎯 **Coordinator:** Processing goal: {user_goal}")
        print("=" * 60)
        
        start_time = time.time()
        
        # Step 1: Retrieve relevant context
        print("📚 **Step 1:** Retrieving context...")
        context = self.retrieval.run(user_goal)
        print(f"   ✅ Context retrieved ({len(context)} chars)")
        
        # Step 2: Analyze and reason about the context
        print("🧠 **Step 2:** Analyzing and reasoning...")
        decision = self.reasoning.run(context, user_goal)
        print(f"   ✅ Analysis completed ({len(decision)} chars)")
        
        # Step 3: Create actionable execution plan
        print("⚡ **Step 3:** Creating execution plan...")
        actions = self.execution.run(decision)
        print(f"   ✅ Execution plan created ({len(actions)} chars)")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        result = {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions,
            "agent_flow": ["retrieval", "reasoning", "execution"],
            "processing_time": processing_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"✅ **Coordination Complete** (⏱️ {processing_time:.2f}s)")
        return result
    
    def handle_with_feedback(self, user_goal: str, feedback_rounds: int = 1) -> Dict[str, Any]:
        """
        Handle a goal with multiple feedback rounds for refinement.
        
        Args:
            user_goal: The user's high-level goal or request
            feedback_rounds: Number of feedback rounds to perform
            
        Returns:
            Dictionary containing the refined result
        """
        result = self.handle(user_goal)
        
        for round_num in range(feedback_rounds):
            print(f"\n🔄 **Feedback Round {round_num + 1}:**")
            print("-" * 40)
            
            # Use reasoning agent to refine the execution plan
            refinement_prompt = f"""
            Original Goal: {user_goal}
            Current Decision: {result['decision']}
            Current Actions: {result['actions']}
            
            Review the above and suggest improvements or refinements to make the execution plan more effective.
            """
            
            refined_decision = self.reasoning.run(result['context'], refinement_prompt)
            refined_actions = self.execution.run(refined_decision)
            
            result['decision'] = refined_decision
            result['actions'] = refined_actions
            result['refinement_rounds'] = round_num + 1
            
            print(f"   ✅ Round {round_num + 1} refinement completed")
        
        return result

def main():
    """Main function demonstrating the multi-agent coordinator."""
    print("🤖 **Multi-Agent Coordinator Demo**")
    print("=" * 50)
    
    # Create coordinator
    coord = Coordinator()
    
    # Example goals
    example_goals = [
        "Create a plan to onboard 5 new customers this month",
        "Develop a strategy to improve team productivity by 20%",
        "Design a customer feedback collection system"
    ]
    
    # Process example with feedback
    goal = example_goals[0]
    print(f"🎯 **Processing Goal:** {goal}")
    print("-" * 50)
    
    try:
        # Basic processing
        result = coord.handle(goal)
        
        # With feedback refinement
        refined_result = coord.handle_with_feedback(goal, feedback_rounds=1)
        
        print("\n📊 **Final Result:**")
        print("=" * 50)
        print(f"**Goal:** {refined_result['goal']}")
        print(f"**Processing Time:** {refined_result['processing_time']:.2f}s")
        
        # Try to parse actions as JSON
        try:
            actions_json = json.loads(refined_result['actions'])
            print(f"\n**Formatted Actions:**")
            print(json.dumps(actions_json, indent=2))
        except (json.JSONDecodeError, TypeError):
            print(f"\n**Actions:** {refined_result['actions']}")
            
    except Exception as e:
        print(f"❌ **Error:** {e}")
        print("\nPlease ensure Foundry Local is running with a model loaded.")

if __name__ == "__main__":
    main()
```

3단계) Foundry Local에 대해 검증하고 샘플 실행
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator from Module08 directory
cd Module08
python -m samples.05.agents.coordinator

REM Or explore the comprehensive Jupyter notebook
jupyter notebook samples/05/multi_agent_orchestration.ipynb
```


> **📚 로컬 샘플 참고 자료:**
> - **주요 구현**: `samples/05/agents/specialists.py` 및 `samples/05/agents/coordinator.py`
> - **포괄적인 예제**: `samples/05/multi_agent_orchestration.ipynb`
> - **설치 지침**: `samples/05/README.md`
> 
> **🔗 관련 Foundry Local 샘플:**
> - [함수 호출 샘플](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
> - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)

가이드라인:
- 에이전트 간 재시도 및 타임아웃 구현
- 대화/스레드 상태를 위한 소규모 인메모리 저장소(dict) 추가
- 여러 호출을 연결할 때 속도 제한 도입

## Part 4: 관찰 가능성과 안전성

프롬프트, 응답, 오류를 로컬에서 추적하며 에이전트 스택에서 데이터 위생을 강화하세요.

1단계) 경량 요청 로깅 (선택 사항)

참고: 아래 헬퍼는 기본적으로 포함되지 않습니다. 실험을 위해 로컬 JSON 로깅을 원한다면 `infra/obs.py`를 생성하세요.
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```

에이전트에 로깅 통합 (선택 사항):
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```

2단계) CLI를 통해 가용성과 기본 상태 검증
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```

3단계) 민감 정보 제거 및 PII 위생
- 모델에 메시지를 보내기 전에 민감한 필드(이메일, 전화번호, ID)를 제거하거나 해시 처리
- 원본 데이터를 디바이스에 보관하고 필요한 컨텍스트 문자열만 전달

민감 정보 제거 헬퍼 예제:
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```

에이전트에서 사용:
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```

4단계) 서킷 브레이커 및 오류 처리
- 각 에이전트 호출을 try/except와 지수 백오프로 래핑
- 반복적인 실패 시 파이프라인 단락 처리

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```

5단계) 로컬 감사 기록 및 내보내기
- JSON 로그를 `./agent_logs`에 저장
- 주기적으로 로그 압축 및 회전
- 검토를 위한 요약 내보내기 (카운트, 평균 대기 시간, 오류율)

6단계) Microsoft Learn 문서와 교차 확인
- Foundry Local은 OpenAI 호환 API를 제공합니다 (`curl /v1/models`로 검증됨)
- `foundry model run <name>`을 사용하여 모델 가용성을 확인
- 클라이언트 통합 및 샘플 앱에 대한 공식 지침을 따르세요 (Open WebUI/사용 방법)

참고 자료:
- **Foundry Local 문서**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- **Azure AI Agents**: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- **로컬 샘플**:
  - 다중 에이전트 오케스트레이션: `Module08/samples/05/multi_agent_orchestration.ipynb`
  - 에이전트 구현: `Module08/samples/05/agents/`
  - 샘플 README: `Module08/samples/05/README.md`
- **Microsoft 공식 샘플**:
  - [함수 호출](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling)
  - [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
  - [Foundry Local Python SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **통합 예제**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## 다음 단계
- 클라우드 호스팅 오케스트레이션을 위한 Azure AI Agents 탐색
- 엔터프라이즈 커넥터 추가 (Microsoft Graph, 검색, 데이터베이스)

---

