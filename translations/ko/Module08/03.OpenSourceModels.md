<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T12:24:11+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ko"
}
-->
# 세션 3: Foundry Local과 함께하는 오픈소스 모델

## 개요

이 세션에서는 Foundry Local에 오픈소스 모델을 도입하는 방법을 다룹니다. 커뮤니티 모델 선택, Hugging Face 콘텐츠 통합, "자체 모델 가져오기"(BYOM) 전략을 채택하는 방법을 살펴봅니다. 또한 지속적인 학습과 모델 발견을 위한 Model Mondays 시리즈도 소개합니다.

참고 자료:
- Foundry Local 문서: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face 모델 컴파일: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## 학습 목표
- 로컬 추론을 위한 오픈소스 모델을 발견하고 평가하기
- Foundry Local에서 Hugging Face 모델을 컴파일하고 실행하기
- 정확성, 지연 시간, 리소스 요구 사항에 따른 모델 선택 전략 적용하기
- 캐시 및 버전 관리를 통해 로컬에서 모델 관리하기

## Part 1: 모델 발견 및 선택 (단계별)

Step 1) 로컬 카탈로그에서 사용 가능한 모델 목록 작성
```cmd
foundry model list
```

Step 2) 두 후보 모델을 빠르게 실행해보기 (첫 실행 시 자동 다운로드)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Step 3) 기본 메트릭 기록
- 고정된 프롬프트로 지연 시간(주관적)과 품질 관찰
- 각 모델 실행 중 작업 관리자에서 메모리 사용량 확인

## Part 2: CLI를 통한 카탈로그 모델 실행 (단계별)

Step 1) 모델 시작
```cmd
foundry model run llama-3.2
```

Step 2) OpenAI 호환 엔드포인트를 통해 테스트 프롬프트 전송
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## Part 3: BYOM – Hugging Face 모델 컴파일 (단계별)

모델 컴파일을 위한 공식 가이드를 따르세요. 아래는 고수준 흐름이며, 정확한 명령어와 지원되는 구성은 Microsoft Learn 문서를 참조하세요.

Step 1) 작업 디렉토리 준비
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Step 2) 지원되는 HF 모델 컴파일
- Learn 문서의 단계를 사용하여 컴파일된 ONNX 모델을 `models` 디렉토리에 배치
- 확인:
```cmd
foundry cache ls
```
컴파일된 모델 이름(예: `llama-3.2`)이 표시되어야 합니다.

Step 3) 컴파일된 모델 실행
```cmd
foundry model run llama-3.2 --verbose
```

참고:
- 컴파일 및 실행을 위해 충분한 디스크와 RAM을 확보하세요.
- 작은 모델로 흐름을 검증한 후 점차 확장하세요.

## Part 4: 실용적인 모델 큐레이션 (단계별)

Step 1) `models.json` 레지스트리 생성
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Step 2) 간단한 선택 스크립트
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## Part 5: 실습 벤치마크 (단계별)

Step 1) 간단한 지연 시간 벤치마크
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Step 2) 품질 간단 점검
- 고정된 프롬프트 세트를 사용하여 출력물을 CSV/JSON으로 캡처
- 유창성, 관련성, 정확성을 수동으로 평가 (1–5)

## Part 6: 다음 단계
- 새로운 모델과 팁을 위해 Model Mondays 구독: https://aka.ms/model-mondays
- 팀의 `models.json`에 발견 사항 기여
- 세션 4 준비: LLM과 SLM 비교, 로컬 vs 클라우드 추론, 실습 데모

---

