<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:36:32+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ko"
}
-->
# 세션 3: 오픈소스 모델 탐색 및 관리

## 개요

이번 세션에서는 Foundry Local을 활용한 실질적인 모델 탐색 및 관리 방법에 대해 다룹니다. 사용 가능한 모델을 나열하고, 다양한 옵션을 테스트하며, 기본적인 성능 특성을 이해하는 방법을 배웁니다. Foundry CLI를 활용한 실습 중심 접근법을 통해 사용 사례에 적합한 모델을 선택할 수 있도록 돕습니다.

## 학습 목표

- 모델 탐색 및 관리를 위한 Foundry CLI 명령어 숙달
- 모델 캐시 및 로컬 저장소 패턴 이해
- 다양한 모델을 빠르게 테스트하고 비교하는 방법 학습
- 모델 선택 및 벤치마킹을 위한 실질적인 워크플로우 확립
- Foundry Local을 통해 제공되는 모델 생태계 탐색

## 사전 준비 사항

- 세션 1: Foundry Local 시작하기 완료
- Foundry Local CLI 설치 및 접근 가능
- 모델 다운로드를 위한 충분한 저장 공간 확보 (모델 크기는 1GB에서 20GB 이상까지 다양)
- 모델 유형 및 사용 사례에 대한 기본 이해

## 개요

이번 세션에서는 오픈소스 모델을 Foundry Local에 도입하는 방법을 탐구합니다.

## Part 6: 실습

### 실습: 모델 탐색 및 비교

Sample 03을 기반으로 자신만의 모델 평가 스크립트를 작성하세요:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### 과제

1. **Sample 03 스크립트 실행**: `samples\03\list_and_bench.cmd`
2. **다양한 모델 테스트**: 최소 3개의 다른 모델을 테스트하세요.
3. **성능 비교**: 속도와 응답 품질의 차이를 기록하세요.
4. **결과 문서화**: 간단한 비교 차트를 작성하세요.

### 비교 예시 형식

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Part 7: 문제 해결 및 모범 사례

### 일반적인 문제와 해결 방법

**모델이 시작되지 않음:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**메모리 부족:**
- 작은 모델부터 시작 (`phi-4-mini`)
- 다른 애플리케이션 닫기
- 메모리 부족이 자주 발생하면 RAM 업그레이드

**성능 저하:**
- 모델이 완전히 로드되었는지 확인 (`verbose 출력 확인`)
- 불필요한 백그라운드 애플리케이션 닫기
- 더 빠른 저장 장치 사용 (SSD)

### 모범 사례

1. **작은 모델부터 시작**: `phi-4-mini`로 설정 확인
2. **한 번에 하나의 모델**: 이전 모델을 중지한 후 새 모델 시작
3. **리소스 모니터링**: 메모리 사용량 확인
4. **일관된 테스트**: 공정한 비교를 위해 동일한 프롬프트 사용
5. **결과 문서화**: 사용 사례에 따른 모델 성능 기록

## Part 8: 다음 단계 및 참고 자료

### 세션 4 준비

- **세션 4 주제**: 최적화 도구 및 기술
- **사전 준비 사항**: 모델 전환 및 기본 성능 테스트에 익숙해지기
- **추천 사항**: 이번 세션에서 선호하는 모델 2~3개 선정

### 추가 자료

- **[Foundry Local 문서](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: 공식 문서
- **[CLI 참조](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: 명령어 참조
- **[Model Mondays](https://aka.ms/model-mondays)**: 주간 모델 소개
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: 커뮤니티 및 문제 해결
- **[Sample 03: 모델 탐색](samples/03/README.md)**: 실습 예제 스크립트

### 주요 요점

✅ **모델 탐색**: `foundry model list`를 사용하여 사용 가능한 모델 탐색  
✅ **빠른 테스트**: `list_and_bench.cmd` 패턴을 활용한 신속한 평가  
✅ **성능 모니터링**: 기본 리소스 사용량 및 응답 시간 측정  
✅ **모델 선택**: 사용 사례에 따른 모델 선택을 위한 실질적인 가이드라인  
✅ **캐시 관리**: 저장소 및 정리 절차 이해  

이제 Foundry Local의 간단한 CLI 접근법을 사용하여 AI 애플리케이션에 적합한 모델을 탐색, 테스트 및 선택할 수 있는 실질적인 기술을 갖추게 되었습니다.

## 학습 목표

- 로컬 추론을 위한 오픈소스 모델 탐색 및 평가
- Foundry Local에서 Hugging Face 모델 컴파일 및 실행
- 정확성, 지연 시간 및 리소스 요구 사항에 따른 모델 선택 전략 적용
- 캐시 및 버전 관리를 통해 로컬에서 모델 관리

## Part 1: Foundry CLI를 활용한 모델 탐색

### 기본 모델 관리 명령어

Foundry CLI는 모델 탐색 및 관리를 위한 간단한 명령어를 제공합니다:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### 첫 번째 모델 실행

성능 특성을 이해하기 위해 인기 있고 잘 테스트된 모델부터 시작하세요:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**참고:** `--verbose` 플래그는 다음과 같은 자세한 시작 정보를 제공합니다:
- 첫 실행 시 모델 다운로드 진행 상황
- 메모리 할당 세부 정보
- 서비스 바인딩 정보
- 성능 초기화 메트릭

### 모델 카테고리 이해

**소형 언어 모델 (SLMs):**
- `phi-4-mini`: 빠르고 효율적이며 일반적인 채팅에 적합
- `phi-4`: 더 나은 추론을 제공하는 업그레이드 버전

**중형 모델:**
- `qwen2.5-7b`: 뛰어난 추론 및 긴 컨텍스트 처리
- `deepseek-r1-7b`: 코드 생성에 최적화

**대형 모델:**
- `llama-3.2`: Meta의 최신 오픈소스 모델
- `qwen2.5-14b`: 엔터프라이즈급 추론

## Part 2: 빠른 모델 테스트 및 비교

### Sample 03 접근법: 간단한 목록 및 벤치마크

Sample 03 패턴을 기반으로 한 최소 워크플로우는 다음과 같습니다:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### 모델 성능 테스트

모델이 실행 중일 때 일관된 프롬프트로 테스트하세요:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell 테스트 대안

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Part 3: 모델 캐시 및 저장소 관리

### 모델 캐시 이해

Foundry Local은 모델 다운로드 및 캐시를 자동으로 관리합니다:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### 모델 저장소 고려 사항

**일반적인 모델 크기:**
- `phi-4-mini`: 약 2.5 GB
- `qwen2.5-7b`: 약 4.1 GB  
- `deepseek-r1-7b`: 약 4.3 GB
- `llama-3.2`: 약 4.9 GB
- `qwen2.5-14b`: 약 8.2 GB

**저장소 모범 사례:**
- 빠른 전환을 위해 2~3개의 모델을 캐시에 유지
- 사용하지 않는 모델 제거: `foundry cache clean`
- 작은 SSD에서 디스크 사용량 모니터링
- 모델 크기와 기능 간의 균형 고려

### 모델 성능 모니터링

모델이 실행 중일 때 시스템 리소스를 모니터링하세요:

**Windows 작업 관리자:**
- 메모리 사용량 확인 (모델은 RAM에 로드된 상태 유지)
- 추론 중 CPU 활용도 모니터링
- 초기 모델 로딩 중 디스크 I/O 확인

**명령줄 모니터링:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Part 4: 실질적인 모델 선택 가이드라인

### 사용 사례별 모델 선택

**일반 채팅 및 Q&A:**
- 시작: `phi-4-mini` (빠르고 효율적)
- 업그레이드: `phi-4` (더 나은 추론)
- 고급: `qwen2.5-7b` (긴 컨텍스트)

**코드 생성:**
- 추천: `deepseek-r1-7b`
- 대안: `qwen2.5-7b` (코드에도 적합)

**복잡한 추론:**
- 최고: `qwen2.5-7b` 또는 `qwen2.5-14b`
- 예산 옵션: `phi-4`

### 하드웨어 요구 사항 가이드

**최소 시스템 요구 사항:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**최적 성능을 위한 추천 사양:**
- 32GB 이상의 RAM (다중 모델 전환에 적합)
- SSD 저장소 (더 빠른 모델 로딩)
- 단일 스레드 성능이 우수한 최신 CPU
- NPU 지원 (Windows 11 Copilot+ PC)으로 가속화

### 모델 전환 워크플로우

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Part 5: 간단한 모델 벤치마킹

### 기본 성능 테스트

모델 성능을 비교하기 위한 간단한 접근법은 다음과 같습니다:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### 수동 품질 평가

각 모델에 대해 일관된 프롬프트로 테스트하고 수동으로 평가하세요:

**테스트 프롬프트:**
1. "양자 컴퓨팅을 간단히 설명하세요."
2. "리스트를 정렬하는 Python 함수를 작성하세요."
3. "원격 근무의 장단점은 무엇인가요?"
4. "엣지 AI의 이점을 요약하세요."

**평가 기준:**
- **정확성**: 정보가 올바른가?
- **명확성**: 설명이 이해하기 쉬운가?
- **완전성**: 질문을 충분히 다루고 있는가?
- **속도**: 응답이 얼마나 빠른가?

### 리소스 사용 모니터링

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Part 6: 다음 단계
- 새로운 모델과 팁을 위한 Model Mondays 구독: https://aka.ms/model-mondays
- 팀의 `models.json`에 결과 기여
- 세션 4 준비: LLM과 SLM 비교, 로컬 vs 클라우드 추론, 실습 데모

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 신뢰할 수 있는 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.