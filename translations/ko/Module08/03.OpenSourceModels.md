<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T10:39:28+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ko"
}
-->
# 세션 3: 오픈소스 모델 탐색 및 관리

## 개요

이번 세션에서는 Foundry Local을 활용한 실질적인 모델 탐색 및 관리 방법에 대해 다룹니다. 사용 가능한 모델을 나열하고, 다양한 옵션을 테스트하며, 기본적인 성능 특성을 이해하는 방법을 배웁니다. Foundry CLI를 활용한 실습 중심 접근법을 통해 사용 사례에 적합한 모델을 선택할 수 있도록 돕습니다.

## 학습 목표

- 모델 탐색 및 관리를 위한 Foundry CLI 명령어 숙달
- 모델 캐시 및 로컬 저장소 패턴 이해
- 다양한 모델을 빠르게 테스트하고 비교하는 방법 학습
- 모델 선택 및 벤치마킹을 위한 실질적인 워크플로우 확립
- Foundry Local을 통해 제공되는 모델 생태계 탐색

## 사전 준비 사항

- 세션 1: Foundry Local 시작하기 완료
- Foundry Local CLI 설치 및 접근 가능
- 모델 다운로드를 위한 충분한 저장 공간 확보 (모델 크기는 1GB에서 20GB 이상까지 다양)
- 모델 유형 및 사용 사례에 대한 기본 이해

## 개요

이번 세션에서는 오픈소스 모델을 Foundry Local에 도입하는 방법, Hugging Face 콘텐츠 통합, "자체 모델 가져오기(BYOM)" 전략을 다룹니다. 또한 지속적인 학습과 모델 탐색을 위한 Model Mondays 시리즈를 소개합니다.

## 학습 목표

- 로컬 추론을 위한 오픈소스 모델 탐색 및 평가
- Foundry Local에서 Hugging Face 모델 컴파일 및 실행
- 정확성, 지연 시간, 리소스 요구 사항에 따른 모델 선택 전략 적용
- 캐시 및 버전 관리를 통해 로컬에서 모델 관리

## Part 1: Foundry CLI를 활용한 모델 탐색

### 기본 모델 관리 명령어

Foundry CLI는 모델 탐색 및 관리를 위한 간단한 명령어를 제공합니다:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### 첫 번째 모델 실행하기

성능 특성을 이해하기 위해 인기 있고 잘 테스트된 모델로 시작하세요:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**참고:** `--verbose` 플래그는 다음과 같은 상세한 시작 정보를 제공합니다:
- 첫 실행 시 모델 다운로드 진행 상황
- 메모리 할당 세부 정보
- 서비스 바인딩 정보
- 성능 초기화 지표

### 모델 카테고리 이해하기

**소형 언어 모델(SLMs):**
- `phi-4-mini`: 빠르고 효율적이며 일반적인 대화에 적합
- `phi-4`: 더 나은 추론을 제공하는 업그레이드 버전

**중형 모델:**
- `qwen2.5-7b-instruct`: 뛰어난 추론 및 긴 컨텍스트 처리
- `deepseek-r1-distill-qwen-7b`: 코드 생성에 최적화

**대형 모델:**
- `llama-3.2`: Meta의 최신 오픈소스 모델
- `qwen2.5-14b-instruct`: 엔터프라이즈급 추론

## Part 2: 빠른 모델 테스트 및 비교

### Sample 03 접근법: 간단한 목록 및 벤치마크

Sample 03 패턴을 기반으로 한 최소 워크플로우는 다음과 같습니다:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### 모델 성능 테스트

모델이 실행 중일 때 일관된 프롬프트로 테스트하세요:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell 테스트 대안

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Part 3: 모델 캐시 및 저장소 관리

### 모델 캐시 이해하기

Foundry Local은 모델 다운로드 및 캐싱을 자동으로 관리합니다:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### 모델 저장소 고려 사항

**일반적인 모델 크기:**
- `phi-4-mini`: 약 2.5 GB
- `qwen2.5-7b-instruct`: 약 4.1 GB  
- `deepseek-r1-distill-qwen-7b`: 약 4.3 GB
- `llama-3.2`: 약 4.9 GB
- `qwen2.5-14b-instruct`: 약 8.2 GB

**저장소 관리 모범 사례:**
- 빠른 전환을 위해 2-3개의 모델을 캐시에 유지
- 사용하지 않는 모델 제거: `foundry cache clean`
- 작은 SSD에서 디스크 사용량 모니터링
- 모델 크기와 기능 간의 트레이드오프 고려

### 모델 성능 모니터링

모델이 실행 중일 때 시스템 리소스를 모니터링하세요:

**Windows 작업 관리자:**
- 메모리 사용량 확인 (모델은 RAM에 로드된 상태 유지)
- 추론 중 CPU 활용도 모니터링
- 초기 모델 로딩 중 디스크 I/O 확인

**명령줄 모니터링:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Part 4: 실질적인 모델 선택 가이드라인

### 사용 사례별 모델 선택

**일반 대화 및 Q&A:**
- 시작: `phi-4-mini` (빠르고 효율적)
- 업그레이드: `phi-4` (더 나은 추론)
- 고급: `qwen2.5-7b-instruct` (긴 컨텍스트 처리)

**코드 생성:**
- 추천: `deepseek-r1-distill-qwen-7b`
- 대안: `qwen2.5-7b-instruct` (코드에도 적합)

**복잡한 추론:**
- 최고: `qwen2.5-7b-instruct` 또는 `qwen2.5-14b-instruct`
- 예산 옵션: `phi-4`

### 하드웨어 요구 사항 가이드

**최소 시스템 요구 사항:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**최적 성능을 위한 권장 사항:**
- 32GB 이상의 RAM으로 편안한 다중 모델 전환
- SSD 저장소로 더 빠른 모델 로딩
- 우수한 단일 스레드 성능을 가진 최신 CPU
- 가속을 위한 NPU 지원 (Windows 11 Copilot+ PC)

### 모델 전환 워크플로우

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Part 5: 간단한 모델 벤치마킹

### 기본 성능 테스트

모델 성능을 비교하기 위한 간단한 접근법은 다음과 같습니다:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### 수동 품질 평가

각 모델에 대해 일관된 프롬프트로 테스트하고 수동으로 평가하세요:

**테스트 프롬프트:**
1. "양자 컴퓨팅을 간단히 설명하세요."
2. "리스트를 정렬하는 Python 함수를 작성하세요."
3. "원격 근무의 장단점은 무엇인가요?"
4. "엣지 AI의 이점을 요약하세요."

**평가 기준:**
- **정확성**: 정보가 올바른가?
- **명확성**: 설명이 이해하기 쉬운가?
- **완전성**: 질문을 충분히 다루고 있는가?
- **속도**: 응답이 얼마나 빠른가?

### 리소스 사용 모니터링

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Part 6: 다음 단계
- 새로운 모델과 팁을 위한 Model Mondays 구독: https://aka.ms/model-mondays
- 팀의 `models.json`에 결과 기여
- 세션 4 준비: LLM과 SLM 비교, 로컬 vs 클라우드 추론, 실습 데모

---

