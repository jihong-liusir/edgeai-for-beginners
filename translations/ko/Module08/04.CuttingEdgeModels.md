<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T12:26:54+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ko"
}
-->
# Session 4: 최첨단 모델 – LLM, SLM, 및 온디바이스 추론

## 개요

LLM과 SLM을 비교하고, 로컬 vs 클라우드 추론의 장단점을 평가하며, Phi와 ONNX Runtime을 활용한 EdgeAI 시나리오 데모를 구현합니다. 또한 Chainlit RAG, WebGPU 추론 옵션, Open WebUI 통합을 강조합니다.

참고 자료:
- Foundry Local 문서: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI 사용법 (Open WebUI를 활용한 채팅 앱): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## 학습 목표
- 비용, 지연 시간, 정확성 측면에서 LLM과 SLM의 장단점을 이해하기
- 특정 비즈니스 요구에 맞는 로컬 및 클라우드 추론 선택하기
- Chainlit을 활용한 간단한 RAG 데모 구현하기
- 브라우저 측 가속화를 위한 WebGPU 탐색하기
- Open WebUI를 Foundry Local에 연결하기

## Part 1: LLM vs SLM – 의사결정 매트릭스

고려 사항:
- 지연 시간: SLM은 온디바이스에서 종종 1초 미만의 응답을 제공합니다.
- 비용: 로컬 추론은 클라우드 비용을 줄여줍니다.
- 개인정보 보호: 민감한 데이터가 온디바이스에 유지됩니다.
- 능력: LLM은 복잡한 작업에서 SLM보다 우수할 수 있습니다.
- 신뢰성: 하이브리드 전략은 다운타임 위험을 줄여줍니다.

## Part 2: 로컬 vs 클라우드 – 하이브리드 패턴

- 로컬 우선, 복잡한 프롬프트에 대해 클라우드 백업 사용
- 클라우드 우선, 개인정보 보호 또는 오프라인 시나리오에 대해 로컬 사용
- 작업 유형에 따라 라우팅 (코드 생성은 DeepSeek, 일반 채팅은 Phi/Qwen)

## Part 3: Chainlit을 활용한 RAG 채팅 앱 (최소 구현)

의존성 설치:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

실행:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

확장: 간단한 검색기를 추가하고 검색된 컨텍스트를 사용자 프롬프트에 앞에 추가합니다.

## Part 4: WebGPU 추론 (Heads-up)

WebGPU를 사용하여 브라우저에서 직접 작은 모델을 실행합니다. 이는 개인정보 보호를 우선시하는 데모와 설치가 필요 없는 경험에 이상적입니다. 아래는 ONNX Runtime Web과 WebGPU 실행 제공자를 사용하는 단계별 최소 예제입니다.

1) WebGPU 지원 확인
- Chromium 브라우저: chrome://gpu → “WebGPU” 활성화 확인
- 프로그래밍 확인 (코드에서도 확인): `if (!('gpu' in navigator)) { /* WebGPU 없음 */ }`

2) 최소 프로젝트 생성
폴더와 두 개의 파일 생성: `index.html` 및 `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) 로컬에서 제공 (Windows cmd.exe)
브라우저가 모델을 가져올 수 있도록 간단한 정적 서버를 사용합니다.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

브라우저에서 http://localhost:5173를 엽니다. 초기화 로그, WebGPU를 사용한 세션 생성, argmax 예측을 확인할 수 있습니다.

4) 문제 해결
- WebGPU가 사용 불가능한 경우: Chrome/Edge를 업데이트하고 GPU 드라이버를 최신 상태로 유지한 후 chrome://flags에서 “Enable WebGPU”를 확인합니다.
- CORS 또는 fetch 오류 발생 시: 파일을 http://로 제공하고 모델 URL이 교차 출처 요청을 허용하는지 확인합니다.
- CPU로 대체: `executionProviders: ['wasm']`로 변경하여 기본 동작을 확인합니다.

5) 다음 단계
- 도메인별 ONNX 모델 교체 (예: 이미지 분류 또는 작은 텍스트 모델).
- 실제 입력을 위한 전처리/후처리 로직 추가.
- 더 큰 모델 또는 프로덕션 지연 시간의 경우 Foundry Local 또는 ONNX Runtime Server를 선호합니다.

## Part 5: Open WebUI + Foundry Local (단계별)

Open WebUI를 Foundry Local의 OpenAI 호환 엔드포인트에 연결하여 로컬 채팅 UI를 설정합니다.

1) 사전 준비
- Foundry Local 설치 및 작동 확인 (`foundry --version`)
- 로컬에서 실행 가능한 모델 하나 준비 (예: `phi-4-mini`)
- Docker Desktop 설치 (Open WebUI에 권장)

2) Foundry Local로 모델 시작
```powershell
foundry model run phi-4-mini
```
이 작업은 OpenAI 호환 API를 `http://localhost:8000`에 노출합니다.

3) Open WebUI 시작 (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
참고:
- Windows에서는 `host.docker.internal`을 사용하여 컨테이너가 호스트의 `localhost`에 접근할 수 있습니다.
- `OPENAI_API_BASE_URL`을 Foundry Local의 엔드포인트로 설정하고 `OPENAI_API_KEY`를 더미 값으로 설정합니다.

4) Open WebUI UI에서 설정 (대안)
- http://localhost:3000으로 이동
- 초기 설정 완료 (관리자 사용자)
- 설정 → 모델/제공자 이동
- 기본 URL 설정: `http://host.docker.internal:8000/v1`
- API 키 설정: `local-key` (플레이스홀더)
- 저장

5) 테스트 프롬프트 실행
- Open WebUI 채팅에서 모델 이름 `phi-4-mini` 선택 또는 입력
- 프롬프트: “온디바이스 AI 추론의 다섯 가지 이점을 나열하세요.”
- 로컬 모델에서 스트리밍된 응답을 확인할 수 있습니다.

6) 문제 해결
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Open WebUI 데이터 유지 (선택 사항)
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## 실습 체크리스트
- [ ] 로컬에서 SLM과 LLM의 응답/지연 시간 비교
- [ ] 최소 두 개의 모델로 Chainlit 데모 실행
- [ ] Open WebUI를 로컬 엔드포인트에 연결하고 테스트

## 다음 단계
- Session 5에서 에이전트 워크플로 준비
- 하이브리드 로컬/클라우드가 ROI를 개선하는 시나리오 식별

---

