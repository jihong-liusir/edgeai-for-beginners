<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:36:58+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ko"
}
-->
# 세션 4: Chainlit을 활용한 프로덕션 채팅 애플리케이션 구축

## 개요

이번 세션에서는 Chainlit과 Microsoft Foundry Local을 사용하여 프로덕션 수준의 채팅 애플리케이션을 구축하는 방법을 배웁니다. AI 대화를 위한 현대적인 웹 인터페이스를 만들고, 스트리밍 응답을 구현하며, 오류 처리와 사용자 경험 디자인을 통해 견고한 채팅 애플리케이션을 배포하는 방법을 학습합니다.

**구축할 내용:**
- **Chainlit 채팅 앱**: 스트리밍 응답을 지원하는 현대적인 웹 UI
- **WebGPU 데모**: 브라우저 기반 추론으로 개인정보 보호 중심 애플리케이션  
- **Open WebUI 통합**: Foundry Local을 활용한 전문적인 채팅 인터페이스
- **프로덕션 패턴**: 오류 처리, 모니터링 및 배포 전략

## 학습 목표

- Chainlit을 사용하여 프로덕션 수준의 채팅 애플리케이션 구축
- 사용자 경험을 향상시키는 스트리밍 응답 구현
- Foundry Local SDK 통합 패턴 숙달
- 적절한 오류 처리 및 점진적 성능 저하 적용
- 다양한 환경에서 채팅 애플리케이션 배포 및 구성
- 대화형 AI를 위한 현대적인 웹 UI 패턴 이해

## 사전 준비 사항

- **Foundry Local**: 설치 및 실행 완료 ([설치 가이드](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 이상 및 가상 환경 기능
- **모델**: 최소 하나의 모델 로드 (`foundry model run phi-4-mini`)
- **브라우저**: WebGPU 지원 최신 웹 브라우저 (Chrome/Edge)
- **Docker**: Open WebUI 통합용 (선택 사항)

## Part 1: 현대적인 채팅 애플리케이션 이해하기

### 아키텍처 개요

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### 주요 기술

**Foundry Local SDK 패턴:**
- `FoundryLocalManager(alias)`: 자동 서비스 관리
- `manager.endpoint` 및 `manager.api_key`: 연결 세부 정보
- `manager.get_model_info(alias).id`: 모델 식별

**Chainlit 프레임워크:**
- `@cl.on_chat_start`: 채팅 세션 초기화
- `@cl.on_message`: 사용자 메시지 처리  
- `cl.Message().stream_token()`: 실시간 스트리밍
- 자동 UI 생성 및 WebSocket 관리

## Part 2: 로컬 vs 클라우드 결정 매트릭스

### 성능 특성

| 항목 | 로컬 (Foundry) | 클라우드 (Azure OpenAI) |
|------|----------------|-------------------------|
| **지연 시간** | 🚀 50-200ms (네트워크 없음) | ⏱️ 200-2000ms (네트워크 의존) |
| **개인정보 보호** | 🔒 데이터가 장치를 벗어나지 않음 | ⚠️ 데이터가 클라우드로 전송됨 |
| **비용** | 💰 하드웨어 이후 무료 | 💸 토큰당 비용 발생 |
| **오프라인** | ✅ 인터넷 없이 작동 | ❌ 인터넷 필요 |
| **모델 크기** | ⚠️ 하드웨어 제한 | ✅ 가장 큰 모델 접근 가능 |
| **확장성** | ⚠️ 하드웨어 의존 | ✅ 무제한 확장 가능 |

### 하이브리드 전략 패턴

**로컬 우선 및 폴백:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**작업 기반 라우팅:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Part 3: 샘플 04 - Chainlit 채팅 애플리케이션

### 빠른 시작

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

애플리케이션은 `http://localhost:8080`에서 자동으로 열리며 현대적인 채팅 인터페이스를 제공합니다.

### 핵심 구현

샘플 04 애플리케이션은 프로덕션 수준의 패턴을 보여줍니다:

**자동 서비스 검색:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**스트리밍 채팅 핸들러:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### 구성 옵션

**환경 변수:**

| 변수 | 설명 | 기본값 | 예시 |
|------|------|--------|------|
| `MODEL` | 사용할 모델 별칭 | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local 엔드포인트 | 자동 감지 | `http://localhost:51211` |
| `API_KEY` | API 키 (로컬에서는 선택 사항) | `""` | `your-api-key` |

**고급 사용법:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Part 4: Jupyter 노트북 생성 및 사용

### 노트북 지원 개요

샘플 04에는 포괄적인 Jupyter 노트북(`chainlit_app.ipynb`)이 포함되어 있습니다:

- **📚 교육 콘텐츠**: 단계별 학습 자료
- **🔬 대화형 탐색**: 코드 셀 실행 및 실험
- **📊 시각적 데모**: 차트, 다이어그램 및 출력 시각화
- **🛠️ 개발 도구**: 테스트 및 디버깅 기능

### 나만의 노트북 생성하기

#### Step 1: Jupyter 환경 설정

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Step 2: 새 노트북 생성

**VS Code 사용:**
1. Module08 디렉토리에서 VS Code 열기
2. `.ipynb` 확장자로 새 파일 생성
3. "Foundry Local" 커널 선택
4. 콘텐츠를 추가하며 셀 작성 시작

**Jupyter Lab 사용:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### 노트북 구조 모범 사례

#### 셀 구성

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### 대화형 예제 및 연습

#### 연습 1: 클라이언트 구성 테스트

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### 연습 2: 스트리밍 응답 시뮬레이션

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Part 5: WebGPU 브라우저 추론 데모

### 개요

WebGPU는 AI 모델을 브라우저에서 직접 실행하여 최대 개인정보 보호와 설치 없는 경험을 제공합니다. 이 샘플은 ONNX Runtime Web과 WebGPU 실행을 보여줍니다.

### Step 1: WebGPU 지원 확인

**브라우저 요구 사항:**
- WebGPU가 활성화된 Chrome/Edge 113+
- 확인: `chrome://gpu` → "WebGPU" 상태 확인
- 프로그래밍 확인: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Step 2: WebGPU 데모 생성

디렉토리 생성: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Step 3: 데모 실행

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Part 6: Open WebUI 통합

### 개요

Open WebUI는 Foundry Local의 OpenAI 호환 API에 연결되는 전문적인 ChatGPT 스타일 인터페이스를 제공합니다.

### Step 1: 사전 준비

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Step 2: Docker 설정 (권장)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**참고:** `host.docker.internal`은 Windows에서 Docker 컨테이너가 호스트 머신에 접근할 수 있도록 합니다.

### Step 3: 구성

1. **브라우저 열기:** `http://localhost:3000`으로 이동
2. **초기 설정:** 관리자 계정 생성
3. **모델 구성:**
   - 설정 → 모델 → OpenAI API  
   - 기본 URL: `http://host.docker.internal:51211/v1`
   - API 키: `foundry-local-key` (아무 값이나 사용 가능)
4. **연결 테스트:** 모델이 드롭다운에 표시되어야 함

### 문제 해결

**일반적인 문제:**

1. **연결 거부:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **모델이 표시되지 않음:**
   - 모델이 로드되었는지 확인: `foundry model list`
   - API 응답 확인: `curl http://localhost:51211/v1/models`
   - Open WebUI 컨테이너 재시작

## Part 7: 프로덕션 배포 고려 사항

### 환경 구성

**개발 설정:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**프로덕션 배포:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### 일반적인 포트 문제 및 해결책

**포트 51211 충돌 방지:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### 성능 모니터링

**상태 확인 구현:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## 요약

세션 4에서는 대화형 AI를 위한 프로덕션 수준의 Chainlit 애플리케이션 구축을 다루었습니다. 학습한 내용은 다음과 같습니다:

- ✅ **Chainlit 프레임워크**: 채팅 애플리케이션을 위한 현대적인 UI 및 스트리밍 지원
- ✅ **Foundry Local 통합**: SDK 사용 및 구성 패턴  
- ✅ **WebGPU 추론**: 최대 개인정보 보호를 위한 브라우저 기반 AI
- ✅ **Open WebUI 설정**: 전문적인 채팅 인터페이스 배포
- ✅ **프로덕션 패턴**: 오류 처리, 모니터링 및 확장

샘플 04 애플리케이션은 Microsoft Foundry Local을 통해 로컬 AI 모델을 활용하면서 우수한 사용자 경험을 제공하는 견고한 채팅 인터페이스 구축을 위한 모범 사례를 보여줍니다.

## 참고 자료

- **[샘플 04: Chainlit 애플리케이션](samples/04/README.md)**: 문서가 포함된 완전한 애플리케이션
- **[Chainlit 교육용 노트북](samples/04/chainlit_app.ipynb)**: 대화형 학습 자료
- **[Foundry Local 문서](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: 플랫폼 전체 문서
- **[Chainlit 문서](https://docs.chainlit.io/)**: 공식 프레임워크 문서
- **[Open WebUI 통합 가이드](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: 공식 튜토리얼

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전이 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.