<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "33ecd8ecf0e9347a2b4839a9916e49fb",
  "translation_date": "2025-09-30T23:38:00+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "ko"
}
-->
## 개요

Foundry Local을 사용하여 AI 모델을 모듈화된 맞춤형 도구로 활용하고, 디바이스에서 직접 실행하세요. 이번 세션에서는 개인정보를 보호하면서도 지연 시간이 적은 추론을 위한 실용적인 워크플로우와 SDK, API, CLI를 통해 이러한 도구를 통합하는 방법을 강조합니다. 필요 시 Azure AI Foundry로 확장하는 방법도 배울 수 있습니다.

> **🔄 최신 SDK에 맞게 업데이트됨**: 이 모듈은 최신 Microsoft Foundry-Local 저장소 패턴에 맞게 조정되었으며, `samples/06/`의 지능형 라우팅 구현과 일치합니다. 예제는 이제 최신 `foundry-local-sdk`와 고급 모델 선택 전략을 사용합니다.

**🏗️ 아키텍처 주요 사항:**
- **지능형 모델 라우팅**: 일반, 추론, 코드, 창의적 모델 간 키워드 기반 선택
- **최신 SDK 통합**: 자동 서비스 검색 기능을 갖춘 `FoundryLocalManager` 사용
- **환경 구성**: 환경 변수를 통한 유연한 모델 할당
- **상태 모니터링**: 서비스 유효성 검사 및 모델 가용성 확인
- **프로덕션 준비 완료**: 포괄적인 오류 처리 및 대체 메커니즘

**📁 로컬 구현:**
- `samples/06/router.py` - 키워드 기반 선택을 활용한 지능형 모델 라우터
- `samples/06/model_router.ipynb` - 대화형 예제 및 벤치마크
- `samples/06/README.md` - 구성 및 사용 지침

참고 자료:
- Foundry Local 문서: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- 추론 SDK 통합: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face 모델 컴파일: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## 개요

Foundry Local을 사용하여 AI 모델을 모듈화된 맞춤형 도구로 활용하고, 디바이스에서 직접 실행하세요. 이번 세션에서는 개인정보를 보호하면서도 지연 시간이 적은 추론을 위한 실용적인 워크플로우와 SDK, API, CLI를 통해 이러한 도구를 통합하는 방법을 강조합니다. 필요 시 Azure AI Foundry로 확장하는 방법도 배울 수 있습니다.

참고 자료:
- Foundry Local 문서: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- 추론 SDK 통합: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face 모델 컴파일: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## 학습 목표
- 디바이스에서 모델-도구 패턴 설계
- OpenAI 호환 REST API 또는 SDK를 통한 통합
- 도메인별 사용 사례에 맞춘 모델 맞춤화
- Azure AI Foundry로의 하이브리드 확장 계획

## Part 1: 지능형 모델 라우터 (최신 구현)

목표: 쿼리 내용에 따라 자동 라우팅을 통해 지능형 모델 선택 구현.

> **📋 참고**: 이 구현은 `samples/06/router.py`에서 사용된 패턴과 고급 키워드 기반 모델 선택을 따릅니다.

Step 1) FoundryLocalManager를 사용하여 최신 모델 라우터 정의  
```python
# router/intelligent_router.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
from typing import Dict, Any, Optional
import os
import json

class ModelRouter:
    """Intelligent model router that selects appropriate models for different task types."""
    
    def __init__(self):
        self.client = None
        self.base_url = None
        self.tools = self._load_tool_registry()
        self._initialize_client()
    
    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load tool registry from environment or use defaults."""
        default_tools = {
            "general": {
                "model": os.environ.get("GENERAL_MODEL", "phi-4-mini"),
                "notes": "Fast general-purpose chat and Q&A",
                "temperature": 0.7
            },
            "reasoning": {
                "model": os.environ.get("REASONING_MODEL", "deepseek-r1-7b"),
                "notes": "Step-by-step analysis and logical reasoning",
                "temperature": 0.3
            },
            "code": {
                "model": os.environ.get("CODE_MODEL", "qwen2.5-7b"),
                "notes": "Code generation, debugging, and technical tasks",
                "temperature": 0.2
            },
            "creative": {
                "model": os.environ.get("CREATIVE_MODEL", "phi-4-mini"),
                "notes": "Creative writing and storytelling",
                "temperature": 0.9
            }
        }
        
        # Check for environment override
        tools_env = os.environ.get("TOOL_REGISTRY")
        if tools_env:
            try:
                return json.loads(tools_env)
            except json.JSONDecodeError:
                print("Warning: Invalid TOOL_REGISTRY JSON, using defaults")
        
        return default_tools
```
  
Step 2) 최신 SDK와 서비스 검색을 통해 클라이언트 초기화  
```python
    def _initialize_client(self):
        """Initialize OpenAI client with Foundry Local or fallback configuration."""
        try:
            from foundry_local import FoundryLocalManager
            # Try to use any available model for client initialization
            first_model = next(iter(self.tools.values()))["model"]
            manager = FoundryLocalManager(first_model)
            
            self.client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            self.base_url = manager.endpoint
            print(f"✅ Foundry Local SDK initialized")
        except Exception as e:
            print(f"Warning: Could not use Foundry SDK ({e}), falling back to manual configuration")
            # Fallback to manual configuration
            self.base_url = os.environ.get("BASE_URL", "http://localhost:8000")
            api_key = os.environ.get("API_KEY", "")
            
            self.client = OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key=api_key
            )
            print(f"Initialized manual configuration at {self.base_url}")
    
    def select_tool(self, user_query: str) -> str:
        """Select the most appropriate tool based on the user query."""
        query_lower = user_query.lower()
        
        # Code-related keywords
        code_keywords = ["code", "python", "function", "class", "method", "bug", "debug", 
                        "programming", "script", "algorithm", "implementation", "refactor"]
        if any(keyword in query_lower for keyword in code_keywords):
            return "code"
        
        # Reasoning keywords
        reasoning_keywords = ["why", "how", "explain", "step-by-step", "reason", "analyze", 
                             "think", "logic", "because", "cause", "compare", "evaluate"]
        if any(keyword in query_lower for keyword in reasoning_keywords):
            return "reasoning"
        
        # Creative keywords
        creative_keywords = ["story", "poem", "creative", "imagine", "write", "tale", 
                           "narrative", "fiction", "character", "plot"]
        if any(keyword in query_lower for keyword in creative_keywords):
            return "creative"
        
        # Default to general
        return "general"
    
    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:
        """Send chat completion request to the specified model."""
        try:
            params = {
                "model": model,
                "messages": [{"role": "user", "content": content}],
                "max_tokens": max_tokens
            }
            
            if temperature is not None:
                params["temperature"] = temperature
            
            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating response with model {model}: {str(e)}"
```
  
Step 3) 지능형 라우팅 및 실행 구현 (`samples/06/router.py` 참조)  
```python
    def route_and_run(self, prompt: str) -> Dict[str, Any]:
        """Route the prompt to the appropriate model and generate response."""
        tool_key = self.select_tool(prompt)
        tool_config = self.tools[tool_key]
        model = tool_config["model"]
        temperature = tool_config.get("temperature", 0.7)
        
        print(f"🎯 Selected tool: {tool_key} (model: {model})")
        
        answer = self.chat(
            model=model, 
            content=prompt, 
            max_tokens=400, 
            temperature=temperature
        )
        
        return {
            "tool": tool_key,
            "model": model,
            "tool_description": tool_config["notes"],
            "temperature": temperature,
            "answer": answer
        }
    
    def check_service_health(self) -> Dict[str, Any]:
        """Check Foundry Local service health and available models."""
        try:
            models_response = self.client.models.list()
            available_models = [model.id for model in models_response.data]
            
            return {
                "status": "healthy",
                "base_url": self.base_url,
                "available_models": available_models,
                "tools_configured": list(self.tools.keys())
            }
        except Exception as e:
            return {
                "status": "error",
                "base_url": self.base_url,
                "error": str(e)
            }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    router = ModelRouter()
    
    # Check health
    health = router.check_service_health()
    print(f"Service Health: {json.dumps(health, indent=2)}")
    
    # Test different query types
    queries = [
        "Write a Python function to calculate fibonacci numbers",  # -> code
        "Explain step-by-step why the sky is blue",  # -> reasoning
        "Tell me a creative story about AI",  # -> creative
        "What's the weather like today?"  # -> general
    ]
    
    for query in queries:
        result = router.route_and_run(query)
        print(f"\nQuery: {query}")
        print(f"Selected: {result['tool']} -> {result['model']}")
        print(f"Answer: {result['answer'][:100]}...")
```
  

## Part 2: 최신 SDK 통합 (단계별)

목표: Foundry Local SDK를 OpenAI Python SDK와 함께 사용하여 원활한 통합 구현.

Step 1) 종속성 설치  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install foundry-local-sdk openai
```
  
Step 2) 환경 구성 (선택 사항 - `samples/06/README.md` 참조)  
```cmd
REM Override default models per tool
set GENERAL_MODEL=phi-4-mini
set REASONING_MODEL=deepseek-r1-7b
set CODE_MODEL=qwen2.5-7b
REM Or provide a full JSON registry
set TOOL_REGISTRY={"general":{"model":"phi-4-mini"},"reasoning":{"model":"deepseek-r1-7b"}}
```
  
Step 3) 최신 SDK 통합  
```python
# modern_sdk_demo.py
from foundry_local import FoundryLocalManager
from openai import OpenAI
import sys

def main():
    """Demonstrate modern SDK integration."""
    try:
        # Initialize with FoundryLocalManager
        alias = "phi-4-mini"
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client using Foundry Local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Get model info
        model_info = manager.get_model_info(alias)
        print(f"Using model: {model_info.id}")
        
        # Make request with streaming
        stream = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Explain edge AI benefits in one paragraph."}],
            stream=True,
            max_tokens=200
        )
        
        print("Response: ", end="")
        for chunk in stream:
            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="", flush=True)
        print()
        
    except Exception as e:
        print(f"Error: {e}")
        print("Ensure Foundry Local is running with: foundry model run phi-4-mini")
        sys.exit(1)

if __name__ == "__main__":
    main()
```
  

## Part 3: 도메인 맞춤화 (단계별)

목표: 프롬프트 템플릿과 JSON 스키마를 사용하여 도메인에 맞춘 출력 제공.

Step 1) 도메인 프롬프트 템플릿 생성  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Step 2) JSON 출력 강제 적용  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Part 4: 오프라인 및 보안 태세 (단계별)

목표: 로컬에서 모델을 도구로 실행할 때 개인정보 보호 및 복원력 확보.

Step 1) 로컬 엔드포인트 사전 준비 및 유효성 검사  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Step 2) 입력값 정리  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Step 3) 로컬 전용 플래그 및 로깅  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Part 5: 프로덕션 배포 및 확장

목표: 모니터링 및 Azure AI Foundry 통합을 통해 지능형 라우터 배포.

> **📋 참고**: `samples/06/model_router.ipynb`의 로컬 구현에는 프로덕션 배포 패턴에 대한 포괄적인 예제가 포함되어 있습니다.

Step 1) 모니터링 기능을 갖춘 프로덕션 라우터 (`samples/06/router.py` 참조)  
```python
# production/router.py
from router.intelligent_router import ModelRouter
import json
import time
import sys

class ProductionModelRouter(ModelRouter):
    """Production-ready model router with monitoring and logging."""
    
    def __init__(self):
        super().__init__()
        self.request_count = 0
        self.error_count = 0
        self.start_time = time.time()
    
    def route_and_run_with_monitoring(self, prompt: str) -> Dict[str, Any]:
        """Route with comprehensive monitoring and error handling."""
        start_time = time.time()
        self.request_count += 1
        
        try:
            result = self.route_and_run(prompt)
            processing_time = time.time() - start_time
            
            # Log successful request
            self._log_request({
                "status": "success",
                "tool": result["tool"],
                "model": result["model"],
                "processing_time": processing_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            result["processing_time"] = processing_time
            return result
            
        except Exception as e:
            self.error_count += 1
            error_result = {
                "status": "error",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            self._log_request(error_result)
            return error_result
    
    def _log_request(self, data: Dict[str, Any]):
        """Log request data for monitoring."""
        print(f"📊 {json.dumps(data)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get router statistics."""
        uptime = time.time() - self.start_time
        return {
            "uptime_seconds": uptime,
            "total_requests": self.request_count,
            "error_count": self.error_count,
            "success_rate": (self.request_count - self.error_count) / max(1, self.request_count),
            "requests_per_minute": self.request_count / max(1, uptime / 60)
        }

def main():
    """Production router demo."""
    router = ProductionModelRouter()
    
    # Health check
    health = router.check_service_health()
    if health["status"] == "error":
        print(f"❌ Service health check failed: {health['error']}")
        sys.exit(1)
    
    print(f"✅ Service healthy with {len(health['available_models'])} models")
    
    # Process user query
    user_prompt = " ".join(sys.argv[1:]) or "Write three benefits of on-device AI in JSON format."
    print(f"\n🎯 Processing: {user_prompt}")
    
    result = router.route_and_run_with_monitoring(user_prompt)
    
    if result.get("status") == "error":
        print(f"❌ Error: {result['error']}")
    else:
        print(f"\n📋 Result:")
        print(f"Tool: {result['tool']} -> Model: {result['model']}")
        print(f"Processing Time: {result['processing_time']:.2f}s")
        print(f"Answer: {result['answer']}")
    
    # Show stats
    stats = router.get_stats()
    print(f"\n📊 Statistics: {json.dumps(stats, indent=2)}")

if __name__ == "__main__":
    main()
```
  

## 실습 체크리스트
- [ ] 키워드 기반 선택을 활용한 지능형 모델 라우터 구현 (`samples/06/router.py`)
- [ ] 여러 전문화된 모델 구성 (일반, 추론, 코드, 창의적)
- [ ] 대화형 Jupyter 노트북 테스트 (`samples/06/model_router.ipynb`)
- [ ] 환경 기반 모델 구성 설정
- [ ] 서비스 상태 모니터링 및 오류 처리 구현
- [ ] 포괄적인 로깅을 갖춘 프로덕션 라우터 배포

## 로컬 샘플 통합

전체 구현 실행:  
```cmd
cd Module08
.\.venv\Scripts\activate

REM Start required models
foundry model run phi-4-mini
foundry model run qwen2.5-7b
foundry model run deepseek-r1-7b

REM Test the intelligent router
python samples\06\router.py "Write a Python function to sort a list"
python samples\06\router.py "Explain step-by-step how bubble sort works"
python samples\06\router.py "Tell me a creative story about robots"

REM Explore the interactive notebook
jupyter notebook samples/06/model_router.ipynb
```
  

## 참고 자료 및 다음 단계
- **로컬 구현**: `samples/06/` - 다중 모델 지원을 갖춘 완전한 지능형 라우터
- **Microsoft 샘플**: [Hello Foundry Local](https://github.com/microsoft/Foundry-Local/tree/main/samples/python/hello-foundry-local)
- **통합 문서**: [추론 SDK 통합](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks)
- **고급 패턴**: 모듈 5에서 함수 호출 및 다중 에이전트 오케스트레이션 탐색

## 마무리

Foundry Local은 모델을 지능적이고 전문화된 도구로 변환하여 디바이스에서 강력한 AI를 구현할 수 있도록 합니다. 자동 모델 선택, 포괄적인 모니터링, 프로덕션 준비 패턴을 통해 팀은 다양한 작업 유형에 적응하면서도 개인정보와 성능을 유지하는 정교한 AI 애플리케이션을 배포할 수 있습니다. 여기서 소개된 지능형 라우터 패턴은 로컬 개발에서 프로덕션 배포로 확장 가능한 복잡한 AI 시스템을 구축하기 위한 기반을 제공합니다.

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.