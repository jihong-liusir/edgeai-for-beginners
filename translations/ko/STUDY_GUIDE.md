<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T12:16:08+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "ko"
}
-->
# 초보자를 위한 EdgeAI: 학습 경로와 학습 일정

### 집중 학습 경로 (1주)

| 날짜 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1일차 | 모듈 1: EdgeAI 기본 개념 | 3시간 |
| 2일차 | 모듈 2: SLM 기초 | 3시간 |
| 3일차 | 모듈 3: SLM 배포 | 2시간 |
| 4-5일차 | 모듈 4: 모델 최적화 (6개 프레임워크) | 4시간 |
| 6일차 | 모듈 5: SLMOps | 3시간 |
| 7일차 | 모듈 6-7: AI 에이전트 및 개발 도구 | 5시간 |

### 집중 학습 경로 (2주)

| 날짜 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1-2일차 | 모듈 1: EdgeAI 기본 개념 | 3시간 |
| 3-4일차 | 모듈 2: SLM 기초 | 3시간 |
| 5-6일차 | 모듈 3: SLM 배포 | 2시간 |
| 7-8일차 | 모듈 4: 모델 최적화 | 4시간 |
| 9-10일차 | 모듈 5: SLMOps | 3시간 |
| 11-12일차 | 모듈 6: AI 에이전트 | 2시간 |
| 13-14일차 | 모듈 7: 개발 도구 | 3시간 |

### 파트타임 학습 (4주)

| 주차 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1주차 | 모듈 1-2: 기본 개념 및 SLM 기초 | 6시간 |
| 2주차 | 모듈 3-4: 배포 및 최적화 | 6시간 |
| 3주차 | 모듈 5-6: SLMOps 및 AI 에이전트 | 5시간 |
| 4주차 | 모듈 7: 개발 도구 및 통합 | 3시간 |

| 날짜 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1-2일차 | 모듈 1: EdgeAI 기본 개념 | 3시간 |
| 3-4일차 | 모듈 2: SLM 기초 | 3시간 |
| 5-6일차 | 모듈 3: SLM 배포 | 2시간 |
| 7-8일차 | 모듈 4: 모델 최적화 | 4시간 |
| 9-10일차 | 모듈 5: SLMOps | 3시간 |
| 11-12일차 | 모듈 6: SLM 에이전트 시스템 | 2시간 |
| 13-14일차 | 모듈 7: EdgeAI 구현 샘플 | 2시간 |

| 모듈 | 완료 날짜 | 소요 시간 | 주요 학습 내용 |
|------|----------|-----------|---------------|
| 모듈 1: EdgeAI 기본 개념 | | | |
| 모듈 2: SLM 기초 | | | |
| 모듈 3: SLM 배포 | | | |
| 모듈 4: 모델 최적화 (6개 프레임워크) | | | |
| 모듈 5: SLMOps | | | |
| 모듈 6: SLM 에이전트 시스템 | | | |
| 모듈 7: EdgeAI 구현 샘플 | | | |
| 실습 과제 | | | |
| 미니 프로젝트 | | | |

### 파트타임 학습 (4주)

| 주차 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1주차 | 모듈 1-2: 기본 개념 및 SLM 기초 | 6시간 |
| 2주차 | 모듈 3-4: 배포 및 최적화 | 6시간 |
| 3주차 | 모듈 5-6: SLMOps 및 AI 에이전트 | 5시간 |
| 4주차 | 모듈 7: 개발 도구 및 통합 | 3시간 |

## 소개

EdgeAI 초보자 학습 가이드에 오신 것을 환영합니다! 이 문서는 학습 자료를 효과적으로 활용하고 학습 경험을 극대화할 수 있도록 설계되었습니다. 체계적인 학습 경로, 추천 학습 일정, 주요 개념 요약, 그리고 EdgeAI 기술에 대한 이해를 심화할 수 있는 보조 자료를 제공합니다.

이 과정은 20시간 분량의 간결한 강의로, EdgeAI에 대한 필수 지식을 시간 효율적으로 전달합니다. 바쁜 직장인과 학생들이 이 신흥 분야에서 실질적인 기술을 빠르게 습득할 수 있도록 설계되었습니다.

## 과정 개요

이 과정은 다음의 7개 모듈로 구성된 포괄적인 학습 내용으로 구성되어 있습니다:

1. **EdgeAI 기본 개념 및 기술 변화** - 핵심 개념과 기술 변화 이해
2. **소형 언어 모델(SLM) 기초** - 다양한 SLM 계열과 아키텍처 탐구
3. **소형 언어 모델 배포** - 실질적인 배포 전략 구현
4. **모델 형식 변환 및 양자화** - OpenVINO를 포함한 6개 프레임워크로 고급 최적화
5. **SLMOps - 소형 언어 모델 운영** - 생산 수명 주기 관리 및 배포
6. **SLM 에이전트 시스템** - AI 에이전트, 함수 호출, 모델 컨텍스트 프로토콜
7. **EdgeAI 구현 샘플** - AI 툴킷, Windows 개발, 플랫폼별 구현
8. **Microsoft Foundry Local – 완전한 개발자 툴킷** - Azure 통합을 활용한 로컬 우선 개발 (모듈 08)

## 이 학습 가이드를 활용하는 방법

- **점진적 학습**: 가장 일관된 학습 경험을 위해 모듈 순서대로 진행하세요.
- **지식 체크포인트**: 각 섹션 후 자기 평가 질문을 활용하세요.
- **실습 연습**: 이론적 개념을 강화하기 위해 제안된 연습을 완료하세요.
- **보조 자료**: 관심 있는 주제에 대해 추가 자료를 탐색하세요.

## 학습 일정 추천

### 집중 학습 경로 (1주)

| 날짜 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1-2일차 | 모듈 1: EdgeAI 기본 개념 | 6시간 |
| 3-4일차 | 모듈 2: SLM 기초 | 8시간 |
| 5일차 | 모듈 3: SLM 배포 | 3시간 |
| 6일차 | 모듈 8: Foundry Local Toolkit | 3시간 |

### 파트타임 학습 (3주)

| 주차 | 학습 내용 | 예상 소요 시간 |
|------|----------|----------------|
| 1주차 | 모듈 1: EdgeAI 기본 개념 | 6-7시간 |
| 2주차 | 모듈 2: SLM 기초 | 7-8시간 |
| 3주차 | 모듈 3: SLM 배포 (3시간) + 모듈 8: Foundry Local Toolkit (2-3시간) | 5-6시간 |

## 모듈 1: EdgeAI 기본 개념 및 기술 변화

### 주요 학습 목표

- 클라우드 기반 AI와 엣지 기반 AI의 차이점 이해
- 자원 제약 환경을 위한 핵심 최적화 기술 습득
- EdgeAI 기술의 실제 응용 사례 분석
- EdgeAI 프로젝트를 위한 개발 환경 설정

### 학습 초점 영역

#### 섹션 1: EdgeAI 기본 개념
- **우선 개념**: 
  - 엣지와 클라우드 컴퓨팅 패러다임
  - 모델 양자화 기술
  - 하드웨어 가속 옵션 (NPU, GPU, CPU)
  - 프라이버시 및 보안 이점

- **보조 자료**:
  - [TensorFlow Lite 문서](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse 문서](https://docs.edgeimpulse.com)

#### 섹션 2: 실제 사례 연구
- **우선 개념**: 
  - Microsoft Phi & Mu 모델 생태계
  - 다양한 산업에서의 실제 구현
  - 배포 고려 사항

#### 섹션 3: 실질적 구현 가이드
- **우선 개념**: 
  - 개발 환경 설정
  - 양자화 및 최적화 도구
  - EdgeAI 구현 평가 방법

#### 섹션 4: 엣지 배포 하드웨어
- **우선 개념**: 
  - 하드웨어 플랫폼 비교
  - 특정 하드웨어에 대한 최적화 전략
  - 배포 고려 사항

### 자기 평가 질문

1. 클라우드 기반 AI와 엣지 기반 AI 구현을 비교하고 대조하세요.
2. 엣지 배포를 위한 모델 최적화의 세 가지 주요 기술을 설명하세요.
3. 엣지에서 AI 모델을 실행할 때의 주요 이점은 무엇인가요?
4. 모델을 양자화하는 과정과 성능에 미치는 영향을 설명하세요.
5. NPU, GPU, CPU와 같은 하드웨어 가속기가 EdgeAI 배포에 미치는 영향을 설명하세요.

### 실습 과제

1. **빠른 환경 설정**: 필수 패키지를 사용하여 최소 개발 환경 구성 (30분)
2. **모델 탐색**: 사전 학습된 소형 언어 모델 다운로드 및 분석 (1시간)
3. **기본 양자화**: 소형 모델에 간단한 양자화 적용 (1시간)

## 모듈 2: 소형 언어 모델(SLM) 기초

### 주요 학습 목표

- 다양한 SLM 계열의 아키텍처 원칙 이해
- 모델의 매개변수 규모에 따른 성능 비교
- 효율성, 성능, 배포 요구 사항에 따라 모델 평가
- 각 모델 계열의 적합한 사용 사례 인식

### 학습 초점 영역

#### 섹션 1: Microsoft Phi 모델 계열
- **우선 개념**: 
  - 설계 철학의 진화
  - 효율성 중심 아키텍처
  - 특화된 기능

#### 섹션 2: Qwen 계열
- **우선 개념**: 
  - 오픈 소스 기여
  - 확장 가능한 배포 옵션
  - 고급 추론 아키텍처

#### 섹션 3: Gemma 계열
- **우선 개념**: 
  - 연구 중심 혁신
  - 멀티모달 기능
  - 모바일 최적화

#### 섹션 4: BitNET 계열
- **우선 개념**: 
  - 1비트 양자화 기술
  - 추론 최적화 프레임워크
  - 지속 가능성 고려

#### 섹션 5: Microsoft Mu 모델
- **우선 개념**: 
  - 디바이스 중심 아키텍처
  - Windows와의 시스템 통합
  - 프라이버시 보호 운영

#### 섹션 6: Phi-Silica
- **우선 개념**: 
  - NPU 최적화 아키텍처
  - 성능 지표
  - 개발자 통합

### 자기 평가 질문

1. Phi와 Qwen 모델 계열의 아키텍처 접근 방식을 비교하세요.
2. BitNET의 양자화 기술이 기존 양자화와 어떻게 다른지 설명하세요.
3. Windows 통합을 위한 Mu 모델의 고유한 이점은 무엇인가요?
4. Phi-Silica가 NPU 하드웨어를 활용하여 성능을 최적화하는 방법을 설명하세요.
5. 연결이 제한된 모바일 애플리케이션에 가장 적합한 모델 계열은 무엇이며, 그 이유는 무엇인가요?

### 실습 과제

1. **모델 비교**: 두 가지 다른 SLM 모델의 빠른 벤치마크 수행 (1시간)
2. **간단한 텍스트 생성**: 소형 모델을 사용한 기본 텍스트 생성 구현 (1시간)
3. **빠른 최적화**: 추론 속도를 개선하기 위해 하나의 최적화 기술 적용 (1시간)

## 모듈 3: 소형 언어 모델 배포

### 주요 학습 목표

- 배포 제약 조건에 따라 적합한 모델 선택
- 다양한 배포 시나리오에 대한 최적화 기술 습득
- 로컬 및 클라우드 환경에서 SLM 구현
- EdgeAI 애플리케이션을 위한 프로덕션 준비 구성 설계

### 학습 초점 영역

#### 섹션 1: SLM 고급 학습
- **우선 개념**: 
  - 매개변수 분류 프레임워크
  - 고급 최적화 기술
  - 모델 획득 전략

#### 섹션 2: 로컬 환경 배포
- **우선 개념**: 
  - Ollama 플랫폼 배포
  - Microsoft Foundry 로컬 솔루션
  - 프레임워크 비교 분석

#### 섹션 3: 컨테이너화된 클라우드 배포
- **우선 개념**: 
  - vLLM 고성능 추론
  - 컨테이너 오케스트레이션
  - ONNX Runtime 구현

### 자기 평가 질문

1. 로컬 배포와 클라우드 배포를 선택할 때 고려해야 할 요소는 무엇인가요?
2. Ollama와 Microsoft Foundry Local을 배포 옵션으로 비교하세요.
3. SLM 배포를 위한 컨테이너화의 이점을 설명하세요.
4. 엣지에 배포된 SLM의 주요 성능 지표는 무엇인가요?
5. 모델 선택부터 프로덕션 구현까지의 전체 배포 워크플로를 설명하세요.

### 실습 과제

1. **기본 로컬 배포**: Ollama를 사용하여 간단한 SLM 배포 (1시간)
2. **성능 확인**: 배포된 모델의 빠른 벤치마크 실행 (30분)
3. **간단한 통합**: 배포된 모델을 사용하는 최소 애플리케이션 생성 (1시간)

## 모듈 4: 모델 형식 변환 및 양자화

### 주요 학습 목표

- 1비트에서 8비트 정밀도까지 고급 양자화 기술 습득
- 형식 변환 전략 이해 (GGUF, ONNX)
- 6개 프레임워크(Llama.cpp, Olive, OpenVINO, MLX, 워크플로우 합성)에서 최적화 구현
- Intel, Apple 및 크로스 플랫폼 하드웨어를 위한 프로덕션 엣지 환경에 최적화된 모델 배포

### 학습 초점 영역

#### 섹션 1: 양자화 기초
- **우선 개념**: 
  - 정밀도 분류 프레임워크
  - 성능 대 정확도 트레이드오프
  - 메모리 사용량 최적화

#### 섹션 2: Llama.cpp 구현
- **우선 개념**: 
  - 크로스 플랫폼 배포
  - GGUF 형식 최적화
  - 하드웨어 가속 기술

#### 섹션 3: Microsoft Olive Suite
- **우선 개념**: 
  - 하드웨어 인식 최적화
  - 엔터프라이즈급 배포
  - 자동화된 최적화 워크플로

#### 섹션 4: OpenVINO 툴킷
- **우선 개념**: 
  - Intel 하드웨어 최적화
  - 신경망 압축 프레임워크 (NNCF)
  - 크로스 플랫폼 추론 배포
  - OpenVINO GenAI를 활용한 LLM 배포

#### 섹션 5: Apple MLX 프레임워크
- **우선 개념**:  
  - Apple Silicon 최적화  
  - 통합 메모리 아키텍처  
  - LoRA 미세 조정 기능  

#### 섹션 6: 엣지 AI 개발 워크플로우 종합  
- **우선 개념**:  
  - 통합 워크플로우 아키텍처  
  - 프레임워크 선택 의사결정 트리  
  - 프로덕션 준비 상태 검증  
  - 미래 대비 전략  

### 자기 평가 질문  

1. 다양한 정밀도 수준(1비트에서 8비트)에서 양자화 전략을 비교하세요.  
2. 엣지 배포를 위한 GGUF 형식의 장점을 설명하세요.  
3. Microsoft Olive의 하드웨어 인식 최적화가 배포 효율성을 어떻게 향상시키는지 설명하세요.  
4. 모델 압축을 위한 OpenVINO의 NNCF 주요 이점은 무엇인가요?  
5. Apple MLX가 최적화를 위해 통합 메모리 아키텍처를 어떻게 활용하는지 설명하세요.  
6. 워크플로우 종합이 최적화 프레임워크 선택에 어떻게 도움이 되는지 설명하세요.  

### 실습 과제  

1. **모델 양자화**: 다양한 양자화 수준을 모델에 적용하고 결과를 비교하세요 (1시간)  
2. **OpenVINO 최적화**: Intel 하드웨어를 위해 NNCF를 사용하여 모델을 압축하세요 (1시간)  
3. **프레임워크 비교**: 동일한 모델을 세 가지 다른 최적화 프레임워크에서 테스트하세요 (1시간)  
4. **성능 벤치마킹**: 추론 속도와 메모리 사용량에 대한 최적화 영향을 측정하세요 (1시간)  

## 모듈 5: SLMOps - 소형 언어 모델 운영  

### 주요 학습 목표  

- SLMOps 라이프사이클 관리 원칙 이해  
- 엣지 배포를 위한 증류 및 미세 조정 기술 숙달  
- 모니터링을 포함한 프로덕션 배포 전략 구현  
- 엔터프라이즈급 SLM 운영 및 유지보수 워크플로우 구축  

### 학습 집중 영역  

#### 섹션 1: SLMOps 소개  
- **우선 개념**:  
  - AI 운영에서의 SLMOps 패러다임 변화  
  - 비용 효율성과 프라이버시 우선 아키텍처  
  - 전략적 비즈니스 영향 및 경쟁 우위  

#### 섹션 2: 모델 증류  
- **우선 개념**:  
  - 지식 전이 기술  
  - 2단계 증류 프로세스 구현  
  - Azure ML 증류 워크플로우  

#### 섹션 3: 미세 조정 전략  
- **우선 개념**:  
  - 매개변수 효율적 미세 조정(PEFT)  
  - LoRA 및 QLoRA 고급 방법  
  - 다중 어댑터 훈련 및 하이퍼파라미터 최적화  

#### 섹션 4: 프로덕션 배포  
- **우선 개념**:  
  - 프로덕션을 위한 모델 변환 및 양자화  
  - Foundry Local 배포 구성  
  - 성능 벤치마킹 및 품질 검증  

### 자기 평가 질문  

1. SLMOps가 기존 MLOps와 어떻게 다른가요?  
2. 엣지 배포를 위한 모델 증류의 이점을 설명하세요.  
3. 자원 제약 환경에서 SLM을 미세 조정할 때 주요 고려 사항은 무엇인가요?  
4. 엣지 AI 애플리케이션을 위한 완전한 프로덕션 배포 파이프라인을 설명하세요.  

### 실습 과제  

1. **기본 증류**: 더 큰 교사 모델에서 더 작은 모델을 생성하세요 (1시간)  
2. **미세 조정 실험**: 특정 도메인을 위한 모델을 미세 조정하세요 (1시간)  
3. **배포 파이프라인**: 모델 배포를 위한 기본 CI/CD 파이프라인을 설정하세요 (1시간)  

## 모듈 6: SLM 에이전트 시스템 - AI 에이전트 및 함수 호출  

### 주요 학습 목표  

- 소형 언어 모델을 사용하여 엣지 환경용 지능형 AI 에이전트 구축  
- 체계적인 워크플로우를 통한 함수 호출 기능 구현  
- 표준화된 도구 상호작용을 위한 Model Context Protocol(MCP) 통합 숙달  
- 최소한의 인간 개입으로 정교한 에이전트 시스템 생성  

### 학습 집중 영역  

#### 섹션 1: AI 에이전트 및 SLM 기초  
- **우선 개념**:  
  - 에이전트 분류 프레임워크(반사형, 모델 기반, 목표 기반, 학습 에이전트)  
  - SLM vs LLM 트레이드오프 분석  
  - 엣지 특화 에이전트 설계 패턴  
  - 에이전트 자원 최적화  

#### 섹션 2: 소형 언어 모델에서의 함수 호출  
- **우선 개념**:  
  - 체계적인 워크플로우 구현(의도 감지, JSON 출력, 외부 실행)  
  - 플랫폼별 구현(Phi-4-mini, 선택된 Qwen 모델, Microsoft Foundry Local)  
  - 고급 예제(다중 에이전트 협업, 동적 도구 선택)  
  - 프로덕션 고려 사항(속도 제한, 감사 로그, 보안 조치)  

#### 섹션 3: Model Context Protocol(MCP) 통합  
- **우선 개념**:  
  - 프로토콜 아키텍처 및 계층 시스템 설계  
  - 다중 백엔드 지원(Ollama 개발용, vLLM 프로덕션용)  
  - 연결 프로토콜(STDIO 및 SSE 모드)  
  - 실제 응용 프로그램(웹 자동화, 데이터 처리, API 통합)  

### 자기 평가 질문  

1. 엣지 AI 에이전트를 위한 주요 아키텍처 고려 사항은 무엇인가요?  
2. 함수 호출이 에이전트 기능을 어떻게 향상시키나요?  
3. 에이전트 통신에서 Model Context Protocol의 역할을 설명하세요.  

### 실습 과제  

1. **간단한 에이전트**: 함수 호출 기능이 있는 기본 AI 에이전트를 구축하세요 (1시간)  
2. **MCP 통합**: 에이전트 애플리케이션에 MCP를 구현하세요 (30분)  

## 모듈 7: 엣지 AI 구현 샘플  

### 주요 학습 목표  

- Visual Studio Code용 AI Toolkit을 활용하여 포괄적인 엣지 AI 개발 워크플로우 숙달  
- Windows AI Foundry 플랫폼 및 NPU 최적화 전략에 대한 전문성 확보  
- 다양한 하드웨어 플랫폼 및 배포 시나리오에서 엣지 AI 구현  
- 플랫폼별 최적화를 통해 프로덕션 준비된 엣지 AI 애플리케이션 구축  

### 학습 집중 영역  

#### 섹션 1: Visual Studio Code용 AI Toolkit  
- **우선 개념**:  
  - VS Code 내에서 포괄적인 엣지 AI 개발 환경  
  - 엣지 배포를 위한 모델 카탈로그 및 검색  
  - 로컬 테스트, 최적화 및 에이전트 개발 워크플로우  
  - 엣지 시나리오에 대한 성능 모니터링 및 평가  

#### 섹션 2: Windows 엣지 AI 개발 가이드  
- **우선 개념**:  
  - Windows AI Foundry 플랫폼 포괄적 개요  
  - 효율적인 NPU 추론을 위한 Phi Silica API  
  - 이미지 처리 및 OCR을 위한 컴퓨터 비전 API  
  - 로컬 개발 및 테스트를 위한 Foundry Local CLI  

#### 섹션 3: 플랫폼별 구현  
- **우선 개념**:  
  - NVIDIA Jetson Orin Nano 배포(67 TOPS AI 성능)  
  - .NET MAUI 및 ONNX Runtime GenAI를 활용한 모바일 애플리케이션  
  - 클라우드-엣지 하이브리드 아키텍처를 활용한 Azure 엣지 AI 솔루션  
  - 범용 하드웨어 지원을 통한 Windows ML 최적화  
  - 프라이버시 중심 RAG 구현을 포함한 Foundry Local 애플리케이션  

### 자기 평가 질문  

1. AI Toolkit이 엣지 AI 개발 워크플로우를 어떻게 간소화하나요?  
2. 다양한 하드웨어 플랫폼 간 배포 전략을 비교하세요.  
3. 엣지 개발을 위한 Windows AI Foundry의 장점은 무엇인가요?  
4. 현대 엣지 AI 애플리케이션에서 NPU 최적화의 역할은 무엇인가요?  
5. Phi Silica API가 NPU 하드웨어를 활용하여 성능을 어떻게 최적화하나요?  
6. 프라이버시 민감한 애플리케이션에서 로컬 배포와 클라우드 배포의 이점을 비교하세요.  

### 실습 과제  

1. **AI Toolkit 설정**: AI Toolkit을 구성하고 모델을 최적화하세요 (1시간)  
2. **Windows AI Foundry**: Phi Silica API를 사용하여 간단한 Windows AI 애플리케이션을 구축하세요 (1시간)  
3. **크로스 플랫폼 배포**: 동일한 모델을 두 개의 다른 플랫폼에 배포하세요 (1시간)  
4. **NPU 최적화**: Windows AI Foundry 도구를 사용하여 NPU 성능을 테스트하세요 (30분)  

## 모듈 8: Microsoft Foundry Local – 완전한 개발자 툴킷  

### 주요 학습 목표  

- Windows에서 Foundry Local 설치 및 구성  
- Foundry CLI를 통해 로컬에서 모델 실행, 검색 및 관리  
- OpenAI 호환 REST 및 SDK 클라이언트와 통합  
- 실용적인 샘플 구축: Chainlit 채팅, 에이전트, 모델 라우터  
- Azure AI Foundry와 하이브리드 패턴 이해  

### 학습 집중 영역  

- 설치 및 CLI 필수 사항(모델, 서비스, 캐시)  
- SDK 통합(OpenAI 호환 클라이언트 및 Azure OpenAI)  
- Open WebUI 빠른 검증  
- 에이전트 및 함수 호출 패턴  
- 도구로서의 모델(라우터 및 레지스트리 설계)  

### 자기 평가 질문  

1. 로컬 엔드포인트를 발견하고 사용 가능한 모델을 나열하는 방법은 무엇인가요?  
2. Foundry Local REST와 Azure OpenAI 사용의 차이점은 무엇인가요?  
3. 도구로 모델을 선택하기 위한 간단한 라우터를 어떻게 설계하나요?  
4. 일상적인 개발에 가장 관련 있는 CLI 카테고리는 무엇인가요?  
5. 앱 실행 전에 Foundry Local 준비 상태를 어떻게 검증하나요?  

### 실습 과제  

1. Foundry Local을 설치/업그레이드하고 `phi-4-mini`를 로컬에서 실행하세요 (30분)  
2. `/v1/models`를 호출하고 REST를 통해 간단한 채팅을 실행하세요 (30분)  
3. Chainlit 앱 샘플을 실행하고 로컬에서 채팅하세요 (30분)  
4. 다중 에이전트 코디네이터를 실행하고 출력을 검사하세요 (30분)  
5. 환경 기반 오버라이드를 사용하여 도구로서의 모델 라우터를 시도하세요 (30분)  

## 시간 할당 가이드  

20시간 코스 타임라인을 최대한 활용할 수 있도록 다음과 같은 시간 할당 제안을 제공합니다:  

| 활동 | 시간 할당 | 설명 |  
|------|----------|------|  
| 핵심 자료 읽기 | 9시간 | 각 모듈의 필수 개념에 집중 |  
| 실습 과제 | 6시간 | 주요 기술의 실질적 구현 |  
| 자기 평가 | 2시간 | 질문과 반성을 통해 이해도 테스트 |  
| 미니 프로젝트 | 3시간 | 소규모 실질적 구현에 지식 적용 |  

### 시간 제약에 따른 주요 집중 영역  

**10시간만 있는 경우:**  
- 모듈 1, 2, 3 완료(핵심 엣지 AI 개념)  
- 각 모듈에서 최소한 하나의 실습 과제 수행  
- 구현 세부 사항보다는 핵심 개념 이해에 집중  

**20시간을 모두 사용할 수 있는 경우:**  
- 모든 7개 모듈 완료  
- 각 모듈에서 주요 실습 과제 수행  
- 모듈 7에서 하나의 미니 프로젝트 완료  
- 최소 2-3개의 보충 자료 탐색  

**20시간 이상 있는 경우:**  
- 상세한 실습 과제를 포함하여 모든 모듈 완료  
- 여러 미니 프로젝트 구축  
- 모듈 4에서 고급 최적화 기술 탐색  
- 모듈 5에서 프로덕션 배포 구현  

## 필수 자료  

이 제한된 학습 시간에 가장 큰 가치를 제공하는 신중히 선택된 자료들:  

### 필독 문서  
- [ONNX Runtime 시작하기](https://onnxruntime.ai/docs/get-started/with-python.html) - 가장 효율적인 모델 최적화 도구  
- [Ollama 빠른 시작](https://github.com/ollama/ollama#get-started) - SLM을 로컬에서 빠르게 배포하는 방법  
- [Microsoft Phi 모델 카드](https://huggingface.co/microsoft/phi-2) - 엣지 최적화 모델의 참고 자료  
- [OpenVINO 문서](https://docs.openvino.ai/2025/index.html) - Intel의 포괄적 최적화 툴킷  
- [VS Code용 AI Toolkit](https://code.visualstudio.com/docs/intelligentapps/overview) - 통합 엣지 AI 개발 환경  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows 특화 엣지 AI 개발 플랫폼  

### 시간 절약 도구  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - 빠른 모델 액세스 및 배포  
- [Gradio](https://www.gradio.app/docs/interface) - AI 데모를 위한 빠른 UI 개발  
- [Microsoft Olive](https://github.com/microsoft/Olive) - 간소화된 모델 최적화  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - 효율적인 CPU 추론  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - 신경망 압축 프레임워크  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - 대형 언어 모델 배포 툴킷  

## 진행 추적 템플릿  

20시간 코스를 통해 학습 진행 상황을 추적하기 위해 이 간단한 템플릿을 사용하세요:  

| 모듈 | 완료 날짜 | 소요 시간 | 주요 내용 |  
|------|----------|----------|----------|  
| 모듈 1: 엣지 AI 기본 | | | |  
| 모듈 2: SLM 기초 | | | |  
| 모듈 3: SLM 배포 | | | |  
| 모듈 4: 모델 최적화 | | | |  
| 모듈 5: SLMOps | | | |  
| 모듈 6: AI 에이전트 | | | |  
| 모듈 7: 개발 도구 | | | |  
| 모듈 8: Foundry Local 툴킷 | | | |  
| 실습 과제 | | | |  
| 미니 프로젝트 | | | |  

## 미니 프로젝트 아이디어  

엣지 AI 개념을 연습하기 위해 다음 프로젝트 중 하나를 완료하세요(각각 2-4시간 소요):  

### 초급 프로젝트(각각 2-3시간)  
1. **엣지 텍스트 어시스턴트**: 소형 언어 모델을 사용하여 간단한 오프라인 텍스트 완성 도구를 만드세요.  
2. **모델 비교 대시보드**: 다양한 SLM의 성능 지표를 시각화하는 기본 대시보드를 구축하세요.  
3. **최적화 실험**: 동일한 기본 모델에서 다양한 양자화 수준의 영향을 측정하세요.  

### 중급 프로젝트(각각 3-4시간)  
4. **AI Toolkit 워크플로우**: VS Code AI Toolkit을 사용하여 모델을 처음부터 끝까지 최적화하고 배포하세요.  
5. **Windows AI Foundry 애플리케이션**: Phi Silica API와 NPU 최적화를 사용하여 Windows 앱을 만드세요.  
6. **크로스 플랫폼 배포**: 최적화된 동일한 모델을 Windows(OpenVINO)와 모바일(.NET MAUI)에 배포하세요.  
7. **함수 호출 에이전트**: 엣지 시나리오를 위한 함수 호출 기능이 있는 AI 에이전트를 구축하세요.  

### 고급 통합 프로젝트(각각 4-5시간)  
8. **OpenVINO 최적화 파이프라인**: NNCF와 GenAI 툴킷을 사용하여 완전한 모델 최적화를 구현하기  
9. **SLMOps 파이프라인**: 모델의 학습부터 엣지 배포까지의 전체 라이프사이클을 구현하기  
10. **다중 모델 엣지 시스템**: 엣지 하드웨어에서 여러 개의 특화된 모델을 함께 배포하기  
11. **MCP 통합 시스템**: 도구 상호작용을 위한 Model Context Protocol을 사용하여 에이전트 시스템 구축하기  

## 참고 자료

- Microsoft Learn (Foundry Local)  
  - 개요: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - 시작하기: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - CLI 참조: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - 추론 SDK 통합: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Open WebUI 사용 방법: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Hugging Face 모델 컴파일: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - 개요: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - 에이전트 (개요): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- 최적화 및 추론 도구  
  - Microsoft Olive (문서): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (시작하기): https://onnxruntime.ai/docs/get-started/with-python.html  
  - ONNX Runtime Olive 통합: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (문서): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (문서): https://ml-explore.github.io/mlx/build/html/index.html  
- 배포 프레임워크 및 모델  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (문서): https://docs.vllm.ai/  
  - Ollama (빠른 시작): https://github.com/ollama/ollama#get-started  
- 개발자 도구 (Windows 및 VS Code)  
  - VS Code용 AI 툴킷: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (개요): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## 학습 커뮤니티

토론에 참여하고 다른 학습자들과 교류하세요:  
- [EdgeAI for Beginners 저장소](https://github.com/microsoft/edgeai-for-beginners/discussions)에서 GitHub Discussions  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## 결론

EdgeAI는 인공지능 구현의 최전선에 있으며, 강력한 기능을 디바이스에 직접 제공하면서도 프라이버시, 지연 시간, 연결성에 대한 중요한 문제를 해결합니다. 이 20시간짜리 과정은 EdgeAI 기술을 즉시 활용할 수 있는 필수 지식과 실무 기술을 제공합니다.

이 과정은 가장 중요한 개념에 초점을 맞춰 간결하게 설계되었으며, 과도한 시간 투자 없이도 귀중한 전문 지식을 빠르게 습득할 수 있도록 돕습니다. 간단한 예제라도 직접 실습하는 것이 학습 내용을 강화하는 핵심이라는 점을 기억하세요.

즐거운 학습 되세요!  

---

