<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-15T17:28:46+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "ko"
}
-->
# AI Toolkit for Visual Studio Code - 엣지 AI 개발 가이드

## 소개

Visual Studio Code용 AI Toolkit을 활용한 엣지 AI 개발에 대한 종합 가이드에 오신 것을 환영합니다. 인공지능이 중앙화된 클라우드 컴퓨팅에서 분산된 엣지 디바이스로 이동함에 따라, 개발자는 자원 제약부터 오프라인 작동 요구사항까지 엣지 배포의 고유한 과제를 처리할 수 있는 강력하고 통합된 도구가 필요합니다.

AI Toolkit for Visual Studio Code는 엣지 디바이스에서 효율적으로 실행되는 AI 애플리케이션을 구축, 테스트 및 최적화하기 위해 설계된 완전한 개발 환경을 제공함으로써 이러한 격차를 해소합니다. IoT 센서, 모바일 디바이스, 임베디드 시스템 또는 엣지 서버를 대상으로 개발하든, 이 툴킷은 친숙한 VS Code 환경 내에서 전체 개발 워크플로를 간소화합니다.

이 가이드는 초기 모델 선택부터 프로덕션 배포까지, 엣지 AI 프로젝트에서 AI Toolkit을 활용하기 위한 필수 개념, 도구 및 모범 사례를 안내합니다.

## 개요

AI Toolkit은 VS Code 내에서 엣지 AI 애플리케이션의 전체 라이프사이클을 위한 통합 개발 환경을 제공합니다. OpenAI, Anthropic, Google, GitHub과 같은 주요 AI 모델 제공업체와의 원활한 통합을 제공하며, ONNX 및 Ollama를 통한 로컬 모델 배포를 지원합니다. 이는 온디바이스 추론이 필요한 엣지 AI 애플리케이션에 필수적인 기능입니다.

AI Toolkit이 엣지 AI 개발에서 돋보이는 이유는 전체 엣지 배포 파이프라인에 초점을 맞추고 있기 때문입니다. 클라우드 배포를 주로 목표로 하는 기존 AI 개발 도구와 달리, AI Toolkit은 모델 최적화, 자원 제약 테스트, 엣지 특화 성능 평가를 위한 전문 기능을 포함합니다. 엣지 AI 개발은 더 작은 모델 크기, 더 빠른 추론 시간, 오프라인 기능, 하드웨어 특화 최적화와 같은 다른 고려 사항이 필요하다는 점을 이해합니다.

이 플랫폼은 간단한 온디바이스 추론부터 복잡한 다중 모델 엣지 아키텍처까지 다양한 배포 시나리오를 지원합니다. 성공적인 엣지 배포를 위해 필수적인 모델 변환, 양자화 및 최적화 도구를 제공하며, VS Code가 제공하는 개발자 생산성을 유지합니다.

## 학습 목표

이 가이드를 완료하면 다음을 수행할 수 있습니다:

### 핵심 역량
- **설치 및 구성**: 엣지 AI 개발 워크플로를 위한 Visual Studio Code용 AI Toolkit 설치 및 구성
- **인터페이스 탐색 및 활용**: 모델 카탈로그, 플레이그라운드, 에이전트 빌더를 포함한 AI Toolkit 인터페이스 탐색 및 활용
- **모델 선택 및 평가**: 성능 및 자원 제약을 기반으로 엣지 배포에 적합한 AI 모델 선택 및 평가
- **모델 변환 및 최적화**: ONNX 형식 및 양자화 기술을 사용하여 엣지 디바이스용 모델 변환 및 최적화

### 엣지 AI 개발 기술
- **엣지 AI 애플리케이션 설계 및 구현**: 통합 개발 환경을 사용하여 엣지 AI 애플리케이션 설계 및 구현
- **모델 테스트 수행**: 로컬 추론 및 자원 모니터링을 통해 엣지와 유사한 조건에서 모델 테스트
- **엣지 배포 시나리오에 최적화된 AI 에이전트 생성 및 커스터마이징**
- **엣지 컴퓨팅 관련 메트릭을 사용한 모델 성능 평가** (지연 시간, 메모리 사용량, 정확도)

### 최적화 및 배포
- **양자화 및 가지치기 기술 적용**: 모델 크기를 줄이면서도 적절한 성능 유지
- **특정 엣지 하드웨어 플랫폼에 모델 최적화** (CPU, GPU, NPU 가속 포함)
- **엣지 AI 개발 모범 사례 구현**: 자원 관리 및 대체 전략 포함
- **엣지 디바이스에 모델 및 애플리케이션 준비**: 프로덕션 배포를 위한 준비

### 고급 엣지 AI 개념
- **엣지 AI 프레임워크 통합**: ONNX Runtime, Windows ML, TensorFlow Lite 포함
- **다중 모델 아키텍처 및 연합 학습 시나리오 구현**: 엣지 환경을 위한 설계
- **엣지 AI의 일반적인 문제 해결**: 메모리 제약, 추론 속도, 하드웨어 호환성 포함
- **엣지 AI 애플리케이션을 위한 모니터링 및 로깅 전략 설계**

### 실용적 응용
- **모델 선택부터 배포까지의 엔드 투 엔드 엣지 AI 솔루션 구축**
- **엣지 특화 개발 워크플로 및 최적화 기술에 대한 숙련도 입증**
- **실제 엣지 AI 사용 사례에 학습한 개념 적용**: IoT, 모바일, 임베디드 애플리케이션 포함
- **다양한 엣지 AI 배포 전략 및 그에 따른 트레이드오프 평가 및 비교**

## 엣지 AI 개발을 위한 주요 기능

### 1. 모델 카탈로그 및 검색
- **로컬 모델 지원**: 엣지 배포에 최적화된 AI 모델 검색 및 액세스
- **ONNX 통합**: 엣지 추론을 위한 효율적인 ONNX 형식 모델 액세스
- **Ollama 지원**: 프라이버시 및 오프라인 작동을 위한 로컬 실행 모델 활용
- **모델 비교**: 성능과 자원 소비 간의 최적 균형을 찾기 위해 모델을 나란히 비교

### 2. 인터랙티브 플레이그라운드
- **로컬 테스트 환경**: 엣지 배포 전에 모델을 로컬에서 테스트
- **다중 모달 실험**: 엣지 시나리오에서 일반적인 이미지, 텍스트 및 기타 입력으로 테스트
- **파라미터 튜닝**: 엣지 제약에 최적화하기 위해 다양한 모델 파라미터 실험
- **실시간 성능 모니터링**: 개발 중 추론 속도 및 자원 사용량 관찰

### 3. 엣지 애플리케이션을 위한 에이전트 빌더
- **프롬프트 엔지니어링**: 더 작은 엣지 모델과 효율적으로 작동하는 최적화된 프롬프트 생성
- **MCP 도구 통합**: 향상된 엣지 에이전트 기능을 위한 모델 컨텍스트 프로토콜 도구 통합
- **코드 생성**: 엣지 배포 시나리오에 최적화된 프로덕션 준비 코드 생성
- **구조화된 출력**: 엣지 애플리케이션에 적합한 일관되고 구조화된 응답 설계

### 4. 모델 평가 및 테스트
- **성능 메트릭**: 엣지 배포와 관련된 메트릭을 사용하여 모델 평가 (지연 시간, 메모리 사용량, 정확도)
- **배치 테스트**: 여러 모델 구성 동시에 테스트하여 최적의 엣지 설정 찾기
- **맞춤 평가**: 엣지 AI 사용 사례에 특화된 맞춤 평가 기준 생성
- **자원 프로파일링**: 엣지 배포 계획을 위한 메모리 및 계산 요구사항 분석

### 5. 모델 변환 및 최적화
- **ONNX 변환**: 다양한 형식의 모델을 엣지 호환성을 위해 ONNX로 변환
- **양자화**: 양자화 기술을 통해 모델 크기 축소 및 추론 속도 향상
- **하드웨어 최적화**: 특정 엣지 하드웨어 (CPU, GPU, NPU)에 모델 최적화
- **형식 변환**: Hugging Face 및 기타 소스의 모델을 엣지 배포를 위해 변환

### 6. 엣지 시나리오를 위한 파인튜닝
- **도메인 적응**: 특정 엣지 사용 사례 및 환경에 맞게 모델 커스터마이징
- **로컬 학습**: 엣지 특화 요구사항을 위한 GPU 지원 로컬 학습
- **Azure 통합**: 엣지 배포 전에 클라우드 기반 파인튜닝을 위한 Azure Container Apps 활용
- **전이 학습**: 엣지 특화 작업 및 제약을 위한 사전 학습된 모델 적응

### 7. 성능 모니터링 및 추적
- **엣지 성능 분석**: 엣지와 유사한 조건에서 모델 성능 모니터링
- **추적 데이터 수집**: 최적화를 위한 상세 성능 데이터 수집
- **병목 현상 식별**: 엣지 디바이스 배포 전에 성능 문제 식별
- **자원 사용 추적**: 엣지 최적화를 위한 메모리, CPU 및 추론 시간 모니터링

## 엣지 AI 개발 워크플로

### 1단계: 모델 검색 및 선택
1. **모델 카탈로그 탐색**: 엣지 배포에 적합한 모델 찾기
2. **성능 비교**: 크기, 정확도, 추론 속도를 기준으로 모델 평가
3. **로컬 테스트**: Ollama 또는 ONNX 모델을 사용하여 엣지 배포 전에 로컬 테스트
4. **자원 요구사항 평가**: 대상 엣지 디바이스의 메모리 및 계산 요구사항 결정

### 2단계: 모델 최적화
1. **ONNX로 변환**: 선택한 모델을 엣지 호환성을 위해 ONNX 형식으로 변환
2. **양자화 적용**: INT8 또는 INT4 양자화를 통해 모델 크기 축소
3. **하드웨어 최적화**: 대상 엣지 하드웨어 (ARM, x86, 특화된 가속기)에 최적화
4. **성능 검증**: 최적화된 모델이 적절한 정확도를 유지하는지 확인

### 3단계: 애플리케이션 개발
1. **에이전트 설계**: 에이전트 빌더를 사용하여 엣지 최적화 AI 에이전트 생성
2. **프롬프트 엔지니어링**: 더 작은 엣지 모델과 효과적으로 작동하는 프롬프트 개발
3. **통합 테스트**: 시뮬레이션된 엣지 조건에서 에이전트 테스트
4. **코드 생성**: 엣지 배포에 최적화된 프로덕션 코드 생성

### 4단계: 평가 및 테스트
1. **배치 평가**: 여러 구성 테스트를 통해 최적의 엣지 설정 찾기
2. **성능 프로파일링**: 추론 속도, 메모리 사용량, 정확도 분석
3. **엣지 시뮬레이션**: 대상 엣지 배포 환경과 유사한 조건에서 테스트
4. **스트레스 테스트**: 다양한 부하 조건에서 성능 평가

### 5단계: 배포 준비
1. **최종 최적화**: 테스트 결과를 기반으로 최종 최적화 적용
2. **배포 패키징**: 엣지 배포를 위한 모델 및 코드 패키징
3. **문서화**: 배포 요구사항 및 구성 문서화
4. **모니터링 설정**: 프로덕션 배포를 위한 모니터링 및 로깅 준비

## 엣지 AI 개발 대상 사용자

### 엣지 AI 개발자
- AI 기반 엣지 디바이스 및 IoT 솔루션을 구축하는 애플리케이션 개발자
- 자원 제약 디바이스에 AI 기능을 통합하는 임베디드 시스템 개발자
- 스마트폰 및 태블릿용 온디바이스 AI 애플리케이션을 만드는 모바일 개발자

### 엣지 AI 엔지니어
- 엣지 배포를 위한 모델 최적화 및 추론 파이프라인 관리하는 AI 엔지니어
- 분산된 엣지 인프라에서 AI 모델을 배포 및 관리하는 DevOps 엔지니어
- 엣지 하드웨어 제약을 최적화하는 성능 엔지니어

### 연구자 및 교육자
- 엣지 컴퓨팅을 위한 효율적인 모델 및 알고리즘을 개발하는 AI 연구자
- 엣지 AI 개념을 가르치고 최적화 기술을 시연하는 교육자
- 엣지 AI 배포의 과제와 솔루션을 배우는 학생

## 엣지 AI 사용 사례

### 스마트 IoT 디바이스
- **실시간 이미지 인식**: IoT 카메라 및 센서에서 컴퓨터 비전 모델 배포
- **음성 처리**: 스마트 스피커에서 음성 인식 및 자연어 처리 구현
- **예측 유지보수**: 산업 엣지 디바이스에서 이상 탐지 모델 실행
- **환경 모니터링**: 환경 애플리케이션을 위한 센서 데이터 분석 모델 배포

### 모바일 및 임베디드 애플리케이션
- **온디바이스 번역**: 오프라인으로 작동하는 언어 번역 모델 구현
- **증강 현실**: AR 애플리케이션을 위한 실시간 객체 인식 및 추적 배포
- **건강 모니터링**: 웨어러블 디바이스 및 의료 장비에서 건강 분석 모델 실행
- **자율 시스템**: 드론, 로봇 및 차량을 위한 의사결정 모델 구현

### 엣지 컴퓨팅 인프라
- **엣지 데이터 센터**: 저지연 애플리케이션을 위한 엣지 데이터 센터에서 AI 모델 배포
- **CDN 통합**: 콘텐츠 전달 네트워크에 AI 처리 기능 통합
- **5G 엣지**: AI 기반 애플리케이션을 위한 5G 엣지 컴퓨팅 활용
- **포그 컴퓨팅**: 포그 컴퓨팅 환경에서 AI 처리 구현

## 설치 및 설정

### 빠른 설치
Visual Studio Code Marketplace에서 AI Toolkit 확장을 직접 설치하세요:

```
Install: AI Toolkit for Visual Studio Code (ms-windows-ai-studio.windows-ai-studio)
```

### 엣지 AI 개발을 위한 사전 요구사항
- **ONNX Runtime**: 모델 추론을 위해 ONNX Runtime 설치
- **Ollama** (선택 사항): 로컬 모델 서빙을 위해 Ollama 설치
- **Python 환경**: 필요한 AI 라이브러리가 포함된 Python 설정
- **엣지 하드웨어 도구**: 하드웨어 특화 개발 도구 설치 (CUDA, OpenVINO 등)

### 초기 구성
1. VS Code를 열고 AI Toolkit 확장을 설치합니다.
2. 모델 소스 구성 (ONNX, Ollama, 클라우드 제공업체)
3. 엣지 테스트를 위한 로컬 개발 환경 설정
4. 개발 머신에 대한 하드웨어 가속 옵션 구성

## 엣지 AI 개발 시작하기

### 1단계: 모델 선택
1. 활동 바에서 AI Toolkit 뷰를 엽니다.
2. 모델 카탈로그에서 엣지 호환 모델을 탐색합니다.
3. 모델 크기, 형식 (ONNX), 성능 특성을 기준으로 필터링합니다.
4. 내장된 비교 도구를 사용하여 모델을 비교합니다.

### 2단계: 로컬 테스트
1. 플레이그라운드를 사용하여 선택한 모델을 로컬에서 테스트합니다.
2. 다양한 프롬프트와 파라미터를 실험합니다.
3. 테스트 중 성능 메트릭을 모니터링합니다.
4. 엣지 사용 사례 요구사항에 대한 모델 응답을 평가합니다.

### 3단계: 모델 최적화
1. 모델 변환 도구를 사용하여 엣지 배포를 위해 최적화합니다.
2. 양자화를 적용하여 모델 크기를 줄입니다.
3. 최적화된 모델을 테스트하여 적절한 성능을 보장합니다.
4. 최적화 설정 및 성능 트레이드오프를 문서화합니다.

### 4단계: 에이전트 개발
1. 에이전트 빌더를 사용하여 엣지 최적화 AI 에이전트를 생성합니다.
2. 더 작은 모델과 효과적으로 작동하는 프롬프트를 개발합니다.
3. 엣지 시나리오에 필요한 도구 및 API를 통합합니다.
4. 시뮬레이션된 엣지 조건에서 에이전트를 테스트합니다.

### 5단계: 평가 및 배포
1. 대량 평가를 사용하여 여러 구성을 테스트합니다.
2. 다양한 조건에서 성능을 프로파일링합니다.
3. 대상 엣지 디바이스를 위한 배포 패키지를 준비합니다.
4. 프로덕션 배포를 위한 모니터링 및 로깅을 설정합니다.

## 엣지 AI 개발 모범 사례

### 모델 선택
- **크기 제약**: 대상 디바이스의 메모리 제한 내에 맞는 모델 선택
- **추론 속도**: 실시간 애플리케이션을 위해 빠른 추론 속도를 가진 모델 우선
- **정확도 트레이드오프**: 자원 제약과 모델 정확도 간의 균형 유지
- **형식 호환성**: 엣지 배포를 위해 ONNX 또는 하드웨어 최적화 형식 선호

### 최적화 기술
- **양자화**: INT8 또는 INT4 양자화를 사용하여 모델 크기 축소 및 속도 향상
- **가지치기**
- **보안**: 엣지 AI 애플리케이션을 위한 적절한 보안 조치를 구현하세요

## 엣지 AI 프레임워크와의 통합

### ONNX Runtime
- **크로스 플랫폼 배포**: 다양한 엣지 플랫폼에서 ONNX 모델을 배포하세요
- **하드웨어 최적화**: ONNX Runtime의 하드웨어별 최적화를 활용하세요
- **모바일 지원**: 스마트폰 및 태블릿 애플리케이션을 위해 ONNX Runtime Mobile을 사용하세요
- **IoT 통합**: ONNX Runtime의 경량 배포판을 사용하여 IoT 디바이스에 배포하세요

### Windows ML
- **Windows 디바이스**: Windows 기반 엣지 디바이스와 PC에 최적화하세요
- **NPU 가속**: Windows 디바이스의 Neural Processing Unit을 활용하세요
- **DirectML**: Windows 플랫폼에서 GPU 가속을 위해 DirectML을 사용하세요
- **UWP 통합**: Universal Windows Platform 애플리케이션과 통합하세요

### TensorFlow Lite
- **모바일 최적화**: 모바일 및 임베디드 디바이스에서 TensorFlow Lite 모델을 배포하세요
- **하드웨어 대리자**: 가속화를 위한 특화된 하드웨어 대리자를 사용하세요
- **마이크로컨트롤러**: TensorFlow Lite Micro를 사용하여 마이크로컨트롤러에 배포하세요
- **크로스 플랫폼 지원**: Android, iOS, 임베디드 Linux 시스템에서 배포하세요

### Azure IoT Edge
- **클라우드-엣지 하이브리드**: 클라우드 학습과 엣지 추론을 결합하세요
- **모듈 배포**: AI 모델을 IoT Edge 모듈로 배포하세요
- **디바이스 관리**: 엣지 디바이스와 모델 업데이트를 원격으로 관리하세요
- **텔레메트리**: 엣지 배포에서 성능 데이터와 모델 메트릭을 수집하세요

## 고급 엣지 AI 시나리오

### 다중 모델 배포
- **모델 앙상블**: 정확도 향상 또는 중복성을 위해 여러 모델을 배포하세요
- **A/B 테스트**: 엣지 디바이스에서 서로 다른 모델을 동시에 테스트하세요
- **동적 선택**: 현재 디바이스 상태에 따라 모델을 선택하세요
- **자원 공유**: 여러 배포된 모델 간 자원 사용을 최적화하세요

### 연합 학습
- **분산 학습**: 여러 엣지 디바이스에서 모델을 학습하세요
- **프라이버시 보호**: 학습 데이터를 로컬에 유지하면서 모델 개선 사항을 공유하세요
- **협력 학습**: 디바이스가 집단 경험을 통해 학습할 수 있도록 지원하세요
- **엣지-클라우드 조정**: 엣지 디바이스와 클라우드 인프라 간 학습을 조정하세요

### 실시간 처리
- **스트림 처리**: 엣지 디바이스에서 연속적인 데이터 스트림을 처리하세요
- **저지연 추론**: 최소한의 추론 지연을 위해 최적화하세요
- **배치 처리**: 엣지 디바이스에서 데이터 배치를 효율적으로 처리하세요
- **적응형 처리**: 현재 디바이스 성능에 따라 처리 방식을 조정하세요

## 엣지 AI 개발 문제 해결

### 일반적인 문제
- **메모리 제약**: 모델이 대상 디바이스 메모리에 비해 너무 큼
- **추론 속도**: 모델 추론이 실시간 요구사항에 비해 너무 느림
- **정확도 저하**: 최적화로 인해 모델 정확도가 허용할 수 없을 정도로 감소
- **하드웨어 호환성**: 모델이 대상 하드웨어와 호환되지 않음

### 디버깅 전략
- **성능 프로파일링**: AI Toolkit의 추적 기능을 사용하여 병목 지점을 식별하세요
- **자원 모니터링**: 개발 중 메모리와 CPU 사용량을 모니터링하세요
- **점진적 테스트**: 최적화를 점진적으로 테스트하여 문제를 격리하세요
- **하드웨어 시뮬레이션**: 개발 도구를 사용하여 대상 하드웨어를 시뮬레이션하세요

### 최적화 솔루션
- **추가 양자화**: 더 공격적인 양자화 기술을 적용하세요
- **모델 아키텍처**: 엣지에 최적화된 다른 모델 아키텍처를 고려하세요
- **전처리 최적화**: 엣지 제약에 맞게 데이터 전처리를 최적화하세요
- **추론 최적화**: 하드웨어별 추론 최적화를 사용하세요

## 리소스 및 다음 단계

### 문서
- [AI Toolkit Models Guide](https://code.visualstudio.com/docs/intelligentapps/models)
- [Model Playground Documentation](https://code.visualstudio.com/docs/intelligentapps/playground)
- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Windows ML Documentation](https://docs.microsoft.com/en-us/windows/ai/)

### 커뮤니티 및 지원
- [VS Code AI Toolkit GitHub](https://github.com/microsoft/vscode-ai-toolkit)
- [ONNX Community](https://github.com/onnx/onnx)
- [Edge AI Developer Community](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [VS Code Extension Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 학습 리소스
- [엣지 AI 기본 과정](./Module01/README.md)
- [소형 언어 모델 가이드](./Module02/README.md)
- [엣지 배포 전략](./Module03/README.md)
- [Windows 엣지 AI 개발](./windowdeveloper.md)

## 결론

Visual Studio Code용 AI Toolkit은 모델 탐색과 최적화부터 배포 및 모니터링까지 엣지 AI 개발을 위한 종합적인 플랫폼을 제공합니다. 통합된 도구와 워크플로를 활용하여 개발자는 자원이 제한된 엣지 디바이스에서 효과적으로 실행되는 AI 애플리케이션을 효율적으로 생성, 테스트, 배포할 수 있습니다.

ONNX, Ollama 및 다양한 클라우드 제공업체에 대한 지원과 최적화 및 평가 기능을 결합한 이 툴킷은 엣지 AI 개발에 이상적인 선택입니다. IoT 애플리케이션, 모바일 AI 기능 또는 임베디드 인텔리전스 시스템을 구축하든, AI Toolkit은 성공적인 엣지 AI 배포에 필요한 도구와 워크플로를 제공합니다.

엣지 AI가 계속 발전함에 따라 VS Code용 AI Toolkit은 최첨단 도구와 기능을 제공하며, 차세대 지능형 엣지 애플리케이션을 구축하기 위한 개발자들의 요구를 충족시킵니다.

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.