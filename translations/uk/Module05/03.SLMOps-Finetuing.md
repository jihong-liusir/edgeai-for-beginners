<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-19T01:17:52+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "uk"
}
-->
# Розділ 3: Тонке налаштування - адаптація моделей для конкретних завдань

## Зміст
1. [Вступ до тонкого налаштування](../../../Module05)
2. [Чому тонке налаштування важливе](../../../Module05)
3. [Типи тонкого налаштування](../../../Module05)
4. [Тонке налаштування з Microsoft Olive](../../../Module05)
5. [Практичні приклади](../../../Module05)
6. [Найкращі практики та рекомендації](../../../Module05)
7. [Розширені техніки](../../../Module05)
8. [Оцінка та моніторинг](../../../Module05)
9. [Поширені виклики та рішення](../../../Module05)
10. [Висновок](../../../Module05)

## Вступ до тонкого налаштування

**Тонке налаштування** — це потужна техніка машинного навчання, яка передбачає адаптацію попередньо навченої моделі для виконання конкретних завдань або роботи з спеціалізованими наборами даних. Замість того, щоб навчати модель з нуля, тонке налаштування використовує знання, які вже отримала попередньо навчена модель, і коригує їх для вашого конкретного випадку.

### Що таке тонке налаштування?

Тонке налаштування — це форма **переносного навчання**, яка передбачає:
- Використання попередньо навченої моделі, яка вже вивчила загальні закономірності з великих наборів даних
- Коригування внутрішніх параметрів моделі за допомогою вашого специфічного набору даних
- Збереження цінних знань при спеціалізації моделі для вашого завдання

Це схоже на навчання досвідченого шеф-кухаря новій кухні — він вже розуміє основи кулінарії, але йому потрібно освоїти специфічні техніки та смаки нового стилю.

### Основні переваги

- **Ефективність часу**: Значно швидше, ніж навчання з нуля
- **Ефективність даних**: Потребує менших наборів даних для досягнення хороших результатів
- **Економічність**: Менші вимоги до обчислювальних ресурсів
- **Краща продуктивність**: Часто досягає кращих результатів порівняно з навчанням з нуля
- **Оптимізація ресурсів**: Робить потужний AI доступним для менших команд і організацій

## Чому тонке налаштування важливе

### Застосування в реальному світі

Тонке налаштування є важливим у багатьох сценаріях:

**1. Адаптація до домену**
- Медичний AI: Адаптація загальних мовних моделей для медичної термінології та клінічних записів
- Юридичні технології: Спеціалізація моделей для аналізу юридичних документів та перевірки контрактів
- Фінансові послуги: Налаштування моделей для аналізу фінансових звітів та оцінки ризиків

**2. Спеціалізація завдань**
- Генерація контенту: Тонке налаштування для певних стилів або тонів написання
- Генерація коду: Адаптація моделей для конкретних мов програмування або фреймворків
- Переклад: Покращення продуктивності для певних мовних пар або технічних доменів

**3. Корпоративні застосування**
- Обслуговування клієнтів: Створення чат-ботів, які розуміють специфічну термінологію компанії
- Внутрішня документація: Розробка AI-помічників, знайомих з організаційними процесами
- Галузеві рішення: Розробка моделей, які розуміють специфічний жаргон і робочі процеси галузі

## Типи тонкого налаштування

### 1. Повне тонке налаштування (Instruction Fine-Tuning)

У повному тонкому налаштуванні всі параметри моделі оновлюються під час навчання. Цей підхід:
- Забезпечує максимальну гнучкість і потенціал продуктивності
- Вимагає значних обчислювальних ресурсів
- Призводить до створення повністю нової версії моделі
- Найкраще підходить для сценаріїв, коли у вас є значний обсяг даних для навчання та обчислювальні ресурси

### 2. Ефективне тонке налаштування параметрів (PEFT)

Методи PEFT оновлюють лише невеликий підмножину параметрів, роблячи процес більш ефективним:

#### Low-Rank Adaptation (LoRA)
- Додає невеликі матриці рангової декомпозиції до існуючих ваг
- Значно зменшує кількість параметрів для навчання
- Зберігає продуктивність, близьку до повного тонкого налаштування
- Дозволяє легко перемикатися між різними адаптаціями

#### QLoRA (Quantized LoRA)
- Поєднує LoRA з техніками квантування
- Ще більше зменшує вимоги до пам’яті
- Дозволяє тонке налаштування більших моделей на споживчому обладнанні
- Балансує ефективність і продуктивність

#### Адаптери
- Вставляють невеликі нейронні мережі між існуючими шарами
- Дозволяють цільове тонке налаштування, залишаючи базову модель незмінною
- Забезпечують модульний підхід до налаштування моделі

### 3. Тонке налаштування для конкретних завдань

Зосереджено на адаптації моделей для конкретних завдань:
- **Класифікація**: Налаштування моделей для завдань категоризації
- **Генерація**: Оптимізація для створення контенту та генерації тексту
- **Видобування**: Тонке налаштування для видобування інформації та розпізнавання іменованих сутностей
- **Резюмування**: Спеціалізація моделей для резюмування документів

## Тонке налаштування з Microsoft Olive

Microsoft Olive — це комплексний інструментарій для оптимізації моделей, який спрощує процес тонкого налаштування та забезпечує функції корпоративного рівня.

### Що таке Microsoft Olive?

Microsoft Olive — це інструмент оптимізації моделей з відкритим кодом, який:
- Спрощує робочі процеси тонкого налаштування для різних апаратних платформ
- Забезпечує вбудовану підтримку популярних архітектур моделей (Llama, Phi, Qwen, Gemma)
- Пропонує як хмарні, так і локальні варіанти розгортання
- Інтегрується з Azure ML та іншими AI-сервісами Microsoft
- Підтримує автоматичну оптимізацію та квантування

### Основні функції

- **Оптимізація з урахуванням апаратного забезпечення**: Автоматично оптимізує моделі для конкретного обладнання (CPU, GPU, NPU)
- **Підтримка багатьох форматів**: Працює з моделями PyTorch, Hugging Face та ONNX
- **Автоматизовані робочі процеси**: Зменшує ручну конфігурацію та метод проб і помилок
- **Інтеграція для підприємств**: Вбудована підтримка Azure ML та хмарних розгортань
- **Розширювана архітектура**: Дозволяє використовувати власні техніки оптимізації

### Встановлення та налаштування

#### Основне встановлення

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Додаткові залежності

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Перевірка встановлення

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Практичні приклади

### Приклад 1: Базове тонке налаштування за допомогою Olive CLI

Цей приклад демонструє тонке налаштування невеликої мовної моделі для класифікації фраз:

#### Крок 1: Підготовка середовища

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Крок 2: Тонке налаштування моделі

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Крок 3: Оптимізація для розгортання

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Приклад 2: Розширена конфігурація з власним набором даних

#### Крок 1: Підготовка власного набору даних

Створіть JSON-файл із вашими навчальними даними:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Крок 2: Створення конфігураційного файлу

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Крок 3: Виконання тонкого налаштування

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Приклад 3: Тонке налаштування QLoRA для ефективного використання пам’яті

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Найкращі практики та рекомендації

### Підготовка даних

**1. Якість даних важливіша за кількість**
- Пріоритет надається високоякісним, різноманітним прикладам, а не великим обсягам низькоякісних даних
- Переконайтеся, що дані є репрезентативними для вашого цільового випадку використання
- Очищуйте та попередньо обробляйте дані послідовно

**2. Формат даних і шаблони**
- Використовуйте послідовне форматування для всіх навчальних прикладів
- Створюйте чіткі шаблони вводу-виводу, які відповідають вашому випадку використання
- Включайте відповідне форматування інструкцій для моделей, налаштованих на інструкції

**3. Розподіл набору даних**
- Зарезервуйте 10-20% даних для валідації
- Підтримуйте схожі розподіли між навчальними та валідаційними наборами
- Розгляньте стратифіковану вибірку для завдань класифікації

### Конфігурація навчання

**1. Вибір швидкості навчання**
- Починайте з менших швидкостей навчання (1e-5 до 1e-4) для тонкого налаштування
- Використовуйте планування швидкості навчання для кращої конвергенції
- Моніторьте криві втрат для коригування швидкості

**2. Оптимізація розміру партії**
- Балансуйте розмір партії з доступною пам’яттю
- Використовуйте накопичення градієнтів для збільшення ефективного розміру партії
- Розгляньте взаємозв’язок між розміром партії та швидкістю навчання

**3. Тривалість навчання**
- Моніторьте метрики валідації, щоб уникнути перенавчання
- Використовуйте ранню зупинку, коли продуктивність валідації стабілізується
- Регулярно зберігайте контрольні точки для відновлення та аналізу

### Вибір моделі

**1. Вибір базової моделі**
- Вибирайте моделі, попередньо навчені на схожих доменах, коли це можливо
- Розгляньте розмір моделі відносно ваших обчислювальних обмежень
- Оцініть ліцензійні вимоги для комерційного використання

**2. Вибір методу тонкого налаштування**
- Використовуйте LoRA/QLoRA для середовищ із обмеженими ресурсами
- Вибирайте повне тонке налаштування, коли критично важлива максимальна продуктивність
- Розгляньте підходи на основі адаптерів для сценаріїв із кількома завданнями

### Управління ресурсами

**1. Оптимізація апаратного забезпечення**
- Вибирайте відповідне обладнання для розміру моделі та методу
- Ефективно використовуйте пам’ять GPU за допомогою контрольних точок градієнтів
- Розгляньте хмарні рішення для більших моделей

**2. Управління пам’яттю**
- Використовуйте навчання зі змішаною точністю, коли це можливо
- Реалізуйте накопичення градієнтів для обмежень пам’яті
- Моніторьте використання пам’яті GPU протягом навчання

## Розширені техніки

### Навчання з кількома адаптерами

Навчайте кілька адаптерів для різних завдань, використовуючи одну базову модель:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Оптимізація гіперпараметрів

Реалізуйте систематичне налаштування гіперпараметрів:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Власні функції втрат

Реалізуйте функції втрат, специфічні для домену:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Оцінка та моніторинг

### Метрики та оцінка

**1. Стандартні метрики**
- **Точність**: Загальна правильність для завдань класифікації
- **Перплексія**: Міра якості моделювання мови
- **BLEU/ROUGE**: Якість генерації тексту та резюмування
- **F1 Score**: Збалансована точність і повнота для класифікації

**2. Метрики, специфічні для домену**
- **Еталони завдань**: Використовуйте встановлені еталони для вашого домену
- **Оцінка людиною**: Включайте оцінку людиною для суб’єктивних завдань
- **Бізнес-метрики**: Узгоджуйте з реальними бізнес-цілями

**3. Налаштування оцінки**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Моніторинг прогресу навчання

**1. Відстеження втрат**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Моніторинг валідації**
- Відстежуйте втрати валідації разом із втратами навчання
- Моніторьте ознаки перенавчання (збільшення втрат валідації при зменшенні втрат навчання)
- Використовуйте ранню зупинку на основі метрик валідації

**3. Моніторинг ресурсів**
- Відстежуйте використання GPU/CPU
- Відстежуйте шаблони використання пам’яті
- Моніторьте швидкість навчання та пропускну здатність

## Поширені виклики та рішення

### Виклик 1: Перенавчання

**Симптоми:**
- Втрати навчання продовжують зменшуватися, тоді як втрати валідації збільшуються
- Велика різниця між продуктивністю навчання та валідації
- Погана генералізація на нових даних

**Рішення:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Виклик 2: Обмеження пам’яті

**Рішення:**
- Використовуйте контрольні точки градієнтів
- Реалізуйте накопичення градієнтів
- Вибирайте методи з ефективними параметрами (LoRA, QLoRA)
- Використовуйте паралелізм моделі для великих моделей

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Виклик 3: Повільне навчання

**Рішення:**
- Оптимізуйте конвеєри завантаження даних
- Використовуйте навчання зі змішаною точністю
- Реалізуйте ефективні стратегії пакетування
- Розгляньте розподілене навчання для великих наборів даних

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Виклик 4: Погана продуктивність

**Кроки діагностики:**
1. Перевірте якість і формат даних
2. Перевірте швидкість навчання та тривалість навчання
3. Оцініть вибір базової моделі
4. Перегляньте попередню обробку та токенізацію

**Рішення:**
- Збільште різноманітність навчальних даних
- Налаштуйте графік швидкості навчання
- Спробуйте різні базові моделі
- Реалізуйте техніки розширення даних

## Висновок

Тонке налаштування — це потужна техніка, яка демократизує доступ до передових можливостей AI. Використовуючи інструменти, такі як Microsoft Olive, орган

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.