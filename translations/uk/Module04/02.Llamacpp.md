<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-19T00:52:29+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "uk"
}
-->
# Розділ 2: Посібник з реалізації Llama.cpp

## Зміст
1. [Вступ](../../../Module04)
2. [Що таке Llama.cpp?](../../../Module04)
3. [Встановлення](../../../Module04)
4. [Збірка з вихідного коду](../../../Module04)
5. [Квантування моделей](../../../Module04)
6. [Основне використання](../../../Module04)
7. [Розширені функції](../../../Module04)
8. [Інтеграція з Python](../../../Module04)
9. [Вирішення проблем](../../../Module04)
10. [Найкращі практики](../../../Module04)

## Вступ

Цей детальний посібник допоможе вам освоїти Llama.cpp — від базового встановлення до складних сценаріїв використання. Llama.cpp — це потужна реалізація на C++, яка забезпечує ефективне виконання великих мовних моделей (LLM) з мінімальними налаштуваннями та чудовою продуктивністю на різних апаратних конфігураціях.

## Що таке Llama.cpp?

Llama.cpp — це фреймворк для виконання LLM, написаний на C/C++, який дозволяє запускати великі мовні моделі локально з мінімальними налаштуваннями та передовою продуктивністю на широкому спектрі апаратного забезпечення. Основні функції включають:

### Основні функції
- **Чиста реалізація на C/C++** без залежностей
- **Сумісність між платформами** (Windows, macOS, Linux)
- **Оптимізація апаратного забезпечення** для різних архітектур
- **Підтримка квантування** (від 1.5-бітного до 8-бітного квантування)
- **Прискорення на CPU та GPU**
- **Ефективне використання пам’яті** для обмежених середовищ

### Переваги
- Ефективно працює на CPU без необхідності спеціалізованого обладнання
- Підтримує кілька GPU-бекендів (CUDA, Metal, OpenCL, Vulkan)
- Легкий і портативний
- Apple Silicon — першокласний громадянин, оптимізований через ARM NEON, Accelerate та Metal
- Підтримує різні рівні квантування для зменшення використання пам’яті

## Встановлення

### Метод 1: Готові бінарні файли (рекомендовано для початківців)

#### Завантаження з GitHub Releases
1. Перейдіть на [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Завантажте відповідний бінарний файл для вашої системи:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` для Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` для macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` для Linux

3. Розпакуйте архів і додайте каталог до PATH вашої системи.

#### Використання менеджерів пакетів

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (різні дистрибуції):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Метод 2: Python-пакет (llama-cpp-python)

#### Базове встановлення
```bash
pip install llama-cpp-python
```

#### З апаратним прискоренням
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Збірка з вихідного коду

### Попередні вимоги

**Системні вимоги:**
- Компілятор C++ (GCC, Clang або MSVC)
- CMake (версія 3.14 або новіша)
- Git
- Інструменти для збірки вашої платформи

**Встановлення попередніх вимог:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Встановіть Visual Studio 2022 з інструментами для розробки на C++
- Встановіть CMake з офіційного сайту
- Встановіть Git

### Основний процес збірки

1. **Клонування репозиторію:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Налаштування збірки:**
```bash
cmake -B build
```

3. **Збірка проекту:**
```bash
cmake --build build --config Release
```

Для швидшої компіляції використовуйте паралельні завдання:
```bash
cmake --build build --config Release -j 8
```

### Збірки для конкретного обладнання

#### Підтримка CUDA (GPU NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Підтримка Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Підтримка OpenBLAS (оптимізація CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Підтримка Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Розширені параметри збірки

#### Збірка для налагодження
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### З додатковими функціями
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Квантування моделей

### Розуміння формату GGUF

GGUF (Generalized GGML Unified Format) — це оптимізований формат файлів, розроблений для ефективного виконання великих мовних моделей за допомогою Llama.cpp та інших фреймворків. Він забезпечує:

- Стандартизоване зберігання ваг моделі
- Покращену сумісність між платформами
- Підвищену продуктивність
- Ефективне управління метаданими

### Типи квантування

Llama.cpp підтримує різні рівні квантування:

| Тип | Біти | Опис | Сценарій використання |
|-----|------|------|-----------------------|
| F16 | 16 | Напівточність | Висока якість, великий обсяг пам’яті |
| Q8_0 | 8 | 8-бітне квантування | Хороший баланс |
| Q4_0 | 4 | 4-бітне квантування | Помірна якість, менший розмір |
| Q2_K | 2 | 2-бітне квантування | Найменший розмір, нижча якість |

### Конвертація моделей

#### З PyTorch у GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Пряме завантаження з Hugging Face
Багато моделей доступні у форматі GGUF на Hugging Face:
- Шукайте моделі з "GGUF" у назві
- Завантажте відповідний рівень квантування
- Використовуйте безпосередньо з llama.cpp

## Основне використання

### Інтерфейс командного рядка

#### Просте генерування тексту
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Використання моделей з Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Режим сервера
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Загальні параметри

| Параметр | Опис | Приклад |
|----------|------|---------|
| `-m` | Шлях до файлу моделі | `-m model.gguf` |
| `-p` | Текст запиту | `-p "Hello world"` |
| `-n` | Кількість токенів для генерації | `-n 100` |
| `-c` | Розмір контексту | `-c 4096` |
| `-t` | Кількість потоків | `-t 8` |
| `-ngl` | Шари GPU | `-ngl 32` |
| `-temp` | Температура | `-temp 0.7` |

### Інтерактивний режим

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Розширені функції

### API сервера

#### Запуск сервера
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Використання API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Оптимізація продуктивності

#### Управління пам’яттю
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Багатопоточність
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Прискорення на GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Інтеграція з Python

### Базове використання з llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Інтерфейс чату

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Потокові відповіді

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Інтеграція з LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Вирішення проблем

### Загальні проблеми та рішення

#### Помилки збірки

**Проблема: CMake не знайдено**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Проблема: Компілятор не знайдено**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Проблеми під час виконання

**Проблема: Завантаження моделі не вдалося**
- Перевірте шлях до файлу моделі
- Перевірте права доступу до файлу
- Переконайтеся, що достатньо оперативної пам’яті
- Спробуйте різні рівні квантування

**Проблема: Низька продуктивність**
- Увімкніть апаратне прискорення
- Збільшіть кількість потоків
- Використовуйте відповідне квантування
- Перевірте використання пам’яті GPU

#### Проблеми з пам’яттю

**Проблема: Недостатньо пам’яті**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Проблеми, специфічні для платформи

#### Windows
- Використовуйте компілятор MinGW або Visual Studio
- Переконайтеся у правильній конфігурації PATH
- Перевірте, чи антивірус не заважає

#### macOS
- Увімкніть Metal для Apple Silicon
- Використовуйте Rosetta 2 для сумісності, якщо потрібно
- Перевірте інструменти командного рядка Xcode

#### Linux
- Встановіть пакети для розробки
- Перевірте версії драйверів GPU
- Переконайтеся у встановленні інструментарію CUDA

## Найкращі практики

### Вибір моделі
1. **Вибирайте відповідне квантування** залежно від вашого обладнання
2. **Розглядайте компроміс між розміром моделі** та якістю
3. **Тестуйте різні моделі** для вашого конкретного випадку використання

### Оптимізація продуктивності
1. **Використовуйте прискорення на GPU**, якщо доступно
2. **Оптимізуйте кількість потоків** для вашого CPU
3. **Встановіть відповідний розмір контексту** для вашого випадку використання
4. **Увімкніть відображення пам’яті** для великих моделей

### Розгортання у виробництві
1. **Використовуйте режим сервера** для доступу через API
2. **Реалізуйте належну обробку помилок**
3. **Моніторте використання ресурсів**
4. **Налаштуйте логування та моніторинг**

### Робочий процес розробки
1. **Починайте з менших моделей** для тестування
2. **Використовуйте контроль версій** для конфігурацій моделей
3. **Документуйте ваші конфігурації**
4. **Тестуйте на різних платформах**

### Міркування щодо безпеки
1. **Перевіряйте введені запити**
2. **Реалізуйте обмеження швидкості**
3. **Захищайте API-ендпоінти**
4. **Моніторте шаблони зловживань**

## Висновок

Llama.cpp забезпечує потужний та ефективний спосіб запуску великих мовних моделей локально на різних апаратних конфігураціях. Незалежно від того, чи ви розробляєте AI-додатки, проводите дослідження або просто експериментуєте з LLM, цей фреймворк пропонує гнучкість і продуктивність, необхідні для широкого спектру випадків використання.

Основні висновки:
- Вибирайте метод встановлення, який найкраще відповідає вашим потребам
- Оптимізуйте для вашої конкретної апаратної конфігурації
- Починайте з основного використання і поступово досліджуйте розширені функції
- Розглядайте використання Python-біндінгів для простішої інтеграції
- Дотримуйтесь найкращих практик для розгортання у виробництві

Для отримання додаткової інформації та оновлень відвідайте [офіційний репозиторій Llama.cpp](https://github.com/ggml-org/llama.cpp) та звертайтеся до детальної документації та ресурсів спільноти.

## ➡️ Що далі

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають у результаті використання цього перекладу.