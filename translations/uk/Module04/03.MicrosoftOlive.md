<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-23T00:33:39+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "uk"
}
-->
# Розділ 3: Microsoft Olive Optimization Suite

## Зміст
1. [Вступ](../../../Module04)
2. [Що таке Microsoft Olive?](../../../Module04)
3. [Встановлення](../../../Module04)
4. [Швидкий старт](../../../Module04)
5. [Приклад: Конвертація Qwen3 до ONNX INT4](../../../Module04)
6. [Розширене використання](../../../Module04)
7. [Найкращі практики](../../../Module04)
8. [Вирішення проблем](../../../Module04)
9. [Додаткові ресурси](../../../Module04)

## Вступ

Microsoft Olive — це потужний та простий у використанні інструмент оптимізації моделей, орієнтований на апаратне забезпечення, який спрощує процес оптимізації моделей машинного навчання для розгортання на різних апаратних платформах. Незалежно від того, чи ви працюєте з CPU, GPU або спеціалізованими AI-акселераторами, Olive допомагає досягти оптимальної продуктивності, зберігаючи точність моделі.

## Що таке Microsoft Olive?

Olive — це простий у використанні інструмент оптимізації моделей, орієнтований на апаратне забезпечення, який об'єднує провідні галузеві методи стиснення, оптимізації та компіляції моделей. Він працює з ONNX Runtime як комплексне рішення для оптимізації інференсу.

### Основні функції

- **Оптимізація, орієнтована на апаратне забезпечення**: Автоматично вибирає найкращі методи оптимізації для вашого цільового апаратного забезпечення
- **40+ вбудованих компонентів оптимізації**: Включає стиснення моделей, квантування, оптимізацію графів тощо
- **Простий CLI-інтерфейс**: Легкі команди для виконання загальних завдань оптимізації
- **Підтримка багатьох фреймворків**: Працює з PyTorch, моделями Hugging Face та ONNX
- **Підтримка популярних моделей**: Olive може автоматично оптимізувати популярні архітектури моделей, такі як Llama, Phi, Qwen, Gemma тощо, без додаткових налаштувань

### Переваги

- **Скорочення часу розробки**: Не потрібно вручну експериментувати з різними методами оптимізації
- **Покращення продуктивності**: Значне збільшення швидкості (до 6 разів у деяких випадках)
- **Кросплатформне розгортання**: Оптимізовані моделі працюють на різних апаратних платформах та операційних системах
- **Збереження точності**: Оптимізації зберігають якість моделі, покращуючи її продуктивність

## Встановлення

### Попередні вимоги

- Python 3.8 або новіший
- Менеджер пакетів pip
- Віртуальне середовище (рекомендується)

### Базове встановлення

Створіть та активуйте віртуальне середовище:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Встановіть Olive з функціями автоматичної оптимізації:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Додаткові залежності

Olive пропонує різні додаткові залежності для розширених функцій:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Перевірка встановлення

```bash
olive --help
```

Якщо все успішно, ви побачите повідомлення довідки CLI Olive.

## Швидкий старт

### Перша оптимізація

Оптимізуємо невелику мовну модель за допомогою функції автоматичної оптимізації Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Що робить ця команда

Процес оптимізації включає: отримання моделі з локального кешу, захоплення ONNX-графа та збереження ваг у файлі даних ONNX, оптимізацію ONNX-графа та квантування моделі до int4 за допомогою методу RTN.

### Пояснення параметрів команди

- `--model_name_or_path`: Ідентифікатор моделі Hugging Face або локальний шлях
- `--output_path`: Каталог, де буде збережена оптимізована модель
- `--device`: Цільовий пристрій (cpu, gpu)
- `--provider`: Провайдер виконання (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Використання ONNX Runtime Generate AI для інференсу
- `--precision`: Точність квантування (int4, int8, fp16)
- `--log_level`: Рівень деталізації журналу (0=мінімальний, 1=детальний)

## Приклад: Конвертація Qwen3 до ONNX INT4

На основі прикладу Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ось як оптимізувати модель Qwen3:

### Крок 1: Завантаження моделі (опціонально)

Щоб мінімізувати час завантаження, кешуйте лише необхідні файли:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Крок 2: Оптимізація моделі Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Крок 3: Тестування оптимізованої моделі

Створіть простий Python-скрипт для тестування вашої оптимізованої моделі:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Структура вихідних даних

Після оптимізації ваш каталог вихідних даних міститиме:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Розширене використання

### Файли конфігурації

Для більш складних робочих процесів оптимізації можна використовувати JSON-файли конфігурації:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Запуск з конфігурацією:

```bash
olive run --config config.json
```

### Оптимізація для GPU

Для оптимізації CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Для DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Тонке налаштування з Olive

Olive також підтримує тонке налаштування моделей:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Найкращі практики

### 1. Вибір моделі
- Починайте з менших моделей для тестування (наприклад, 0.5B-7B параметрів)
- Переконайтеся, що архітектура вашої моделі підтримується Olive

### 2. Апаратні аспекти
- Вибирайте ціль оптимізації відповідно до вашого апаратного забезпечення
- Використовуйте оптимізацію для GPU, якщо у вас є апаратне забезпечення, сумісне з CUDA
- Розгляньте DirectML для Windows-машин з інтегрованою графікою

### 3. Вибір точності
- **INT4**: Максимальне стиснення, невелика втрата точності
- **INT8**: Хороший баланс між розміром і точністю
- **FP16**: Мінімальна втрата точності, помірне зменшення розміру

### 4. Тестування та валідація
- Завжди тестуйте оптимізовані моделі для ваших конкретних випадків використання
- Порівнюйте показники продуктивності (затримка, пропускна здатність, точність)
- Використовуйте репрезентативні вхідні дані для оцінки

### 5. Ітеративна оптимізація
- Починайте з автоматичної оптимізації для швидких результатів
- Використовуйте файли конфігурації для детального контролю
- Експериментуйте з різними проходами оптимізації

## Вирішення проблем

### Поширені проблеми

#### 1. Проблеми з встановленням
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Проблеми з CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Проблеми з пам'яттю
- Використовуйте менші розміри пакетів під час оптимізації
- Спробуйте квантування з більшою точністю спочатку (int8 замість int4)
- Переконайтеся, що є достатньо місця на диску для кешування моделі

#### 4. Помилки завантаження моделі
- Перевірте шлях до моделі та права доступу
- Переконайтеся, що модель потребує `trust_remote_code=True`
- Переконайтеся, що всі необхідні файли моделі завантажені

### Отримання допомоги

- **Документація**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Приклади**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Додаткові ресурси

### Офіційні посилання
- **Репозиторій GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Документація ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Приклад Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Приклади від спільноти
- **Jupyter Notebooks**: Доступні в репозиторії Olive на GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Розширення VS Code**: Огляд AI Toolkit для VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Блогові публікації**: Блог Microsoft Open Source — https://opensource.microsoft.com/blog/

### Схожі інструменти
- **ONNX Runtime**: Високопродуктивний механізм інференсу — https://onnxruntime.ai/
- **Hugging Face Transformers**: Джерело багатьох сумісних моделей — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Хмарні робочі процеси оптимізації — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Що далі

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

