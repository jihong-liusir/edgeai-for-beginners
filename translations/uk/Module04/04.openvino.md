<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:29:49+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "uk"
}
-->
# Розділ 4: Набір інструментів оптимізації OpenVINO

## Зміст
1. [Вступ](../../../Module04)
2. [Що таке OpenVINO?](../../../Module04)
3. [Встановлення](../../../Module04)
4. [Швидкий старт](../../../Module04)
5. [Приклад: Конвертація та оптимізація моделей за допомогою OpenVINO](../../../Module04)
6. [Розширене використання](../../../Module04)
7. [Найкращі практики](../../../Module04)
8. [Вирішення проблем](../../../Module04)
9. [Додаткові ресурси](../../../Module04)

## Вступ

OpenVINO (Open Visual Inference and Neural Network Optimization) — це набір інструментів з відкритим кодом від Intel для впровадження високопродуктивних AI-рішень у хмарі, локальних середовищах та на периферійних пристроях. Незалежно від того, чи ви працюєте з CPU, GPU, VPU або спеціалізованими AI-прискорювачами, OpenVINO забезпечує комплексні можливості оптимізації, зберігаючи точність моделі та дозволяючи кросплатформенне впровадження.

## Що таке OpenVINO?

OpenVINO — це набір інструментів з відкритим кодом, який дозволяє розробникам ефективно оптимізувати, конвертувати та впроваджувати AI-моделі на різних апаратних платформах. Він складається з трьох основних компонентів: OpenVINO Runtime для інференсу, Neural Network Compression Framework (NNCF) для оптимізації моделей та OpenVINO Model Server для масштабованого впровадження.

### Основні функції

- **Кросплатформенне впровадження**: Підтримка Linux, Windows та macOS з API для Python, C++ та C
- **Апаратне прискорення**: Автоматичне виявлення пристроїв та оптимізація для CPU, GPU, VPU та AI-прискорювачів
- **Фреймворк компресії моделей**: Розширені техніки квантування, обрізання та оптимізації через NNCF
- **Сумісність з фреймворками**: Пряма підтримка моделей TensorFlow, ONNX, PaddlePaddle та PyTorch
- **Підтримка генеративного AI**: Спеціалізований OpenVINO GenAI для впровадження великих мовних моделей та генеративних AI-додатків

### Переваги

- **Оптимізація продуктивності**: Значне покращення швидкості з мінімальною втратою точності
- **Зменшення розміру впровадження**: Мінімальні зовнішні залежності спрощують встановлення та впровадження
- **Покращений час запуску**: Оптимізоване завантаження моделей та кешування для швидшої ініціалізації додатків
- **Масштабоване впровадження**: Від периферійних пристроїв до хмарної інфраструктури з узгодженими API
- **Готовність до виробництва**: Надійність корпоративного рівня з детальною документацією та підтримкою спільноти

## Встановлення

### Попередні вимоги

- Python 3.8 або новіший
- Менеджер пакетів pip
- Віртуальне середовище (рекомендується)
- Сумісне апаратне забезпечення (рекомендуються процесори Intel, але підтримуються різні архітектури)

### Базове встановлення

Створіть та активуйте віртуальне середовище:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Встановіть OpenVINO Runtime:

```bash
pip install openvino
```

Встановіть NNCF для оптимізації моделей:

```bash
pip install nncf
```

### Встановлення OpenVINO GenAI

Для генеративних AI-додатків:

```bash
pip install openvino-genai
```

### Додаткові залежності

Додаткові пакети для специфічних випадків використання:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Перевірка встановлення

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Якщо все успішно, ви побачите інформацію про версію OpenVINO.

## Швидкий старт

### Ваша перша оптимізація моделі

Давайте конвертуємо та оптимізуємо модель Hugging Face за допомогою OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Що робить цей процес

Процес оптимізації включає: завантаження оригінальної моделі з Hugging Face, конвертацію у формат OpenVINO Intermediate Representation (IR), застосування стандартних оптимізацій та компіляцію для цільового апаратного забезпечення.

### Пояснення ключових параметрів

- `export=True`: Конвертує модель у формат OpenVINO IR
- `compile=False`: Відкладає компіляцію до часу виконання для гнучкості
- `device`: Цільове апаратне забезпечення ("CPU", "GPU", "AUTO" для автоматичного вибору)
- `save_pretrained()`: Зберігає оптимізовану модель для повторного використання

## Приклад: Конвертація та оптимізація моделей за допомогою OpenVINO

### Крок 1: Конвертація моделі з квантуванням NNCF

Як застосувати посттренувальне квантування за допомогою NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Крок 2: Розширена оптимізація з компресією ваг

Для моделей на основі трансформерів застосуйте компресію ваг:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Крок 3: Інференс з оптимізованою моделлю

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Структура вихідних даних

Після оптимізації ваша директорія моделі міститиме:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Розширене використання

### Конфігурація з NNCF YAML

Для складних робочих процесів оптимізації використовуйте конфігураційні файли NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Застосуйте конфігурацію:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Оптимізація для GPU

Для прискорення на GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Оптимізація пакетної обробки

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Впровадження через Model Server

Впроваджуйте оптимізовані моделі за допомогою OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Клієнтський код для Model Server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Найкращі практики

### 1. Вибір та підготовка моделі
- Використовуйте моделі з підтримуваних фреймворків (PyTorch, TensorFlow, ONNX)
- Переконайтеся, що вхідні дані моделі мають фіксовані або відомі динамічні розміри
- Тестуйте з репрезентативними наборами даних для калібрування

### 2. Вибір стратегії оптимізації
- **Посттренувальне квантування**: Почніть з цього для швидкої оптимізації
- **Компресія ваг**: Ідеально для великих мовних моделей та трансформерів
- **Квантування з урахуванням тренування**: Використовуйте, коли точність критична

### 3. Апаратно-специфічна оптимізація
- **CPU**: Використовуйте INT8-квантування для збалансованої продуктивності
- **GPU**: Використовуйте FP16-прецизію та пакетну обробку
- **VPU**: Зосередьтеся на спрощенні моделі та злитті шарів

### 4. Налаштування продуктивності
- **Режим пропускної здатності**: Для обробки великих обсягів даних
- **Режим затримки**: Для інтерактивних додатків у реальному часі
- **AUTO Device**: Дозвольте OpenVINO вибрати оптимальне апаратне забезпечення

### 5. Управління пам'яттю
- Використовуйте динамічні розміри обережно, щоб уникнути перевантаження пам'яті
- Реалізуйте кешування моделі для швидшого завантаження
- Моніторьте використання пам'яті під час оптимізації

### 6. Перевірка точності
- Завжди перевіряйте оптимізовані моделі на відповідність оригінальній продуктивності
- Використовуйте репрезентативні тестові набори даних для оцінки
- Розглядайте поступову оптимізацію (починайте з консервативних налаштувань)

## Вирішення проблем

### Поширені проблеми

#### 1. Проблеми з встановленням
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Помилки конвертації моделі
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Проблеми продуктивності
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Проблеми з пам'яттю
- Зменшіть розмір пакета моделі під час оптимізації
- Використовуйте потокову обробку для великих наборів даних
- Увімкніть кешування моделі: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Зниження точності
- Використовуйте вищу прецизію (INT8 замість INT4)
- Збільшіть розмір калібрувального набору даних
- Застосуйте оптимізацію зі змішаною прецизією

### Моніторинг продуктивності

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Отримання допомоги

- **Документація**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Проблеми на GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Форум спільноти**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Додаткові ресурси

### Офіційні посилання
- **Головна сторінка OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Репозиторій GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Репозиторій NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Навчальні ресурси
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Швидкий старт**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Посібник з оптимізації**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Інтеграційні інструменти
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Оцінка продуктивності
- **Офіційні оцінки**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Приклади спільноти
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Комплексні навчальні матеріали у репозиторії OpenVINO Notebooks
- **Прикладні додатки**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Реальні приклади для різних доменів (комп'ютерне бачення, NLP, аудіо)
- **Блог-пости**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Блог Intel AI та спільноти з детальними кейсами

### Суміжні інструменти
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Додаткові техніки оптимізації для апаратного забезпечення Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Для порівняння мобільного та периферійного впровадження
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Альтернативний кросплатформенний інференс-двигун

## ➡️ Що далі

- [05: Глибокий аналіз Apple MLX Framework](./05.AppleMLX.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.