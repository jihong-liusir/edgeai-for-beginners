<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-19T00:00:08+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "uk"
}
-->
# Розділ 3: Практичний посібник з впровадження

## Огляд

Цей детальний посібник допоможе вам підготуватися до курсу EdgeAI, який зосереджений на створенні практичних AI-рішень, що ефективно працюють на пристроях з обмеженими ресурсами. Курс акцентує увагу на практичній розробці з використанням сучасних фреймворків і передових моделей, оптимізованих для розгортання на edge-пристроях.

## 1. Налаштування середовища розробки

### Мови програмування та фреймворки

**Середовище Python**
- **Версія**: Python 3.10 або новіше (рекомендовано: Python 3.11)
- **Менеджер пакетів**: pip або conda
- **Віртуальне середовище**: Використовуйте venv або conda для ізоляції
- **Основні бібліотеки**: Специфічні бібліотеки EdgeAI будуть встановлені під час курсу

**Середовище Microsoft .NET**
- **Версія**: .NET 8 або новіше
- **IDE**: Visual Studio 2022, Visual Studio Code або JetBrains Rider
- **SDK**: Переконайтеся, що .NET SDK встановлено для кросплатформної розробки

### Інструменти розробки

**Редактори коду та IDE**
- Visual Studio Code (рекомендовано для кросплатформної розробки)
- PyCharm або Visual Studio (для розробки мовою програмування)
- Jupyter Notebooks для інтерактивної розробки та прототипування

**Контроль версій**
- Git (остання версія)
- Обліковий запис GitHub для доступу до репозиторіїв і співпраці

## 2. Вимоги до обладнання та рекомендації

### Мінімальні системні вимоги
- **CPU**: Багатоядерний процесор (Intel i5/AMD Ryzen 5 або еквівалент)
- **RAM**: Мінімум 8GB, рекомендовано 16GB
- **Сховище**: 50GB вільного місця для моделей і інструментів розробки
- **OS**: Windows 10/11, macOS 10.15+ або Linux (Ubuntu 20.04+)

### Стратегія використання обчислювальних ресурсів
Курс розроблений таким чином, щоб бути доступним для різних конфігурацій обладнання:

**Локальна розробка (фокус на CPU/NPU)**
- Основна розробка буде використовувати прискорення CPU і NPU
- Підходить для більшості сучасних ноутбуків і настільних комп'ютерів
- Акцент на ефективності та практичних сценаріях розгортання

**Хмарні GPU-ресурси (опціонально)**
- **Azure Machine Learning**: Для інтенсивного навчання та експериментів
- **Google Colab**: Безкоштовний рівень доступний для освітніх цілей
- **Kaggle Notebooks**: Альтернативна хмарна платформа для обчислень

### Особливості edge-пристроїв
- Розуміння процесорів на базі ARM
- Знання обмежень мобільного та IoT-обладнання
- Ознайомлення з оптимізацією енергоспоживання

## 3. Основні сімейства моделей та ресурси

### Основні сімейства моделей

**Сімейство Microsoft Phi-4**
- **Опис**: Компактні, ефективні моделі, розроблені для розгортання на edge-пристроях
- **Сильні сторони**: Відмінне співвідношення продуктивності до розміру, оптимізовані для задач логічного висновку
- **Ресурс**: [Колекція Phi-4 на Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Сценарії використання**: Генерація коду, математичні розрахунки, загальні розмови

**Сімейство Qwen-3**
- **Опис**: Останнє покоління багатомовних моделей від Alibaba
- **Сильні сторони**: Потужні багатомовні можливості, ефективна архітектура
- **Ресурс**: [Колекція Qwen-3 на Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Сценарії використання**: Багатомовні додатки, кроскультурні AI-рішення

**Сімейство Google Gemma-3n**
- **Опис**: Легкі моделі від Google, оптимізовані для розгортання на edge-пристроях
- **Сильні сторони**: Швидке виконання, архітектура, дружня до мобільних пристроїв
- **Ресурс**: [Колекція Gemma-3n на Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Сценарії використання**: Мобільні додатки, обробка в реальному часі

### Критерії вибору моделі
- **Співвідношення продуктивності до розміру**: Розуміння, коли обирати менші чи більші моделі
- **Оптимізація для конкретних задач**: Відповідність моделей конкретним сценаріям використання
- **Обмеження розгортання**: Пам'ять, затримка та енергоспоживання

## 4. Інструменти для квантування та оптимізації

### Фреймворк Llama.cpp
- **Репозиторій**: [Llama.cpp на GitHub](https://github.com/ggml-org/llama.cpp)
- **Призначення**: Високопродуктивний механізм виконання для LLM
- **Основні функції**:
  - Оптимізоване виконання на CPU
  - Різні формати квантування (Q4, Q5, Q8)
  - Кросплатформна сумісність
  - Ефективне використання пам'яті
- **Встановлення та базове використання**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Репозиторій**: [Microsoft Olive на GitHub](https://github.com/microsoft/olive)
- **Призначення**: Інструментарій для оптимізації моделей для розгортання на edge-пристроях
- **Основні функції**:
  - Автоматизовані робочі процеси оптимізації моделей
  - Оптимізація з урахуванням обладнання
  - Інтеграція з ONNX Runtime
  - Інструменти для оцінки продуктивності
- **Встановлення та базове використання**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Визначення моделі та конфігурації оптимізації
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Запуск робочого процесу оптимізації
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Збереження оптимізованої моделі
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # Встановлення MLX
  pip install mlx
  
  # Приклад скрипта на Python для завантаження та оптимізації моделі
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Репозиторій**: [ONNX Runtime на GitHub](https://github.com/microsoft/onnxruntime)
- **Призначення**: Кросплатформне прискорення виконання для моделей ONNX
- **Основні функції**:
  - Оптимізації, специфічні для обладнання (CPU, GPU, NPU)
  - Графові оптимізації для виконання
  - Підтримка квантування
  - Підтримка різних мов (Python, C++, C#, JavaScript)
- **Встановлення та базове використання**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. Рекомендована література та ресурси

### Основна документація
- **Документація ONNX Runtime**: Розуміння кросплатформного виконання
- **Посібник Hugging Face Transformers**: Завантаження моделей та виконання
- **Шаблони дизайну Edge AI**: Найкращі практики для розгортання на edge-пристроях

### Технічні статті
- "Ефективний Edge AI: Огляд технік квантування"
- "Стиснення моделей для мобільних і edge-пристроїв"
- "Оптимізація моделей Transformer для edge-комп'ютингу"

### Ресурси спільноти
- **Спільноти EdgeAI у Slack/Discord**: Підтримка та обговорення з колегами
- **Репозиторії GitHub**: Прикладні реалізації та навчальні матеріали
- **YouTube-канали**: Технічні розбори та навчальні відео

## 6. Оцінка та перевірка

### Контрольний список перед курсом
- [ ] Встановлено та перевірено Python 3.10+
- [ ] Встановлено та перевірено .NET 8+
- [ ] Налаштовано середовище розробки
- [ ] Створено обліковий запис Hugging Face
- [ ] Базове ознайомлення з цільовими сімействами моделей
- [ ] Встановлено та протестовано інструменти для квантування
- [ ] Відповідають вимоги до обладнання
- [ ] Налаштовано облікові записи для хмарних обчислень (за потреби)

## Основні навчальні цілі

До кінця цього посібника ви зможете:

1. Налаштувати повне середовище розробки для створення додатків EdgeAI
2. Встановити та налаштувати необхідні інструменти та фреймворки для оптимізації моделей
3. Вибрати відповідні конфігурації обладнання та програмного забезпечення для ваших проектів EdgeAI
4. Зрозуміти ключові аспекти розгортання AI-моделей на edge-пристроях
5. Підготувати вашу систему до практичних вправ у рамках курсу

## Додаткові ресурси

### Офіційна документація
- **Документація Python**: Офіційна документація мови Python
- **Документація Microsoft .NET**: Офіційні ресурси для розробки на .NET
- **Документація ONNX Runtime**: Комплексний посібник з ONNX Runtime
- **Документація TensorFlow Lite**: Офіційна документація TensorFlow Lite

### Інструменти розробки
- **Visual Studio Code**: Легкий редактор коду з розширеннями для AI-розробки
- **Jupyter Notebooks**: Інтерактивне середовище для експериментів з ML
- **Docker**: Платформа контейнеризації для стабільних середовищ розробки
- **Git**: Система контролю версій для управління кодом

### Навчальні ресурси
- **Дослідницькі статті EdgeAI**: Останні академічні дослідження ефективних моделей
- **Онлайн-курси**: Додаткові навчальні матеріали з оптимізації AI
- **Форуми спільноти**: Платформи для запитань і відповідей щодо викликів у розробці EdgeAI
- **Еталонні набори даних**: Стандартні набори даних для оцінки продуктивності моделей

## Результати навчання

Після завершення цього підготовчого посібника ви:

1. Матимете повністю налаштоване середовище розробки для створення EdgeAI-додатків
2. Зрозумієте вимоги до обладнання та програмного забезпечення для різних сценаріїв розгортання
3. Ознайомитеся з ключовими фреймворками та інструментами, які використовуються протягом курсу
4. Зможете вибирати відповідні моделі залежно від обмежень пристроїв і вимог
5. Отримаєте базові знання про техніки оптимізації для розгортання на edge-пристроях

## ➡️ Що далі

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматизовані переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.