<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:49:08+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "uk"
}
-->
# Розділ 3: Практичний посібник з впровадження

## Огляд

Цей детальний посібник допоможе вам підготуватися до курсу EdgeAI, який зосереджений на створенні практичних AI-рішень, що ефективно працюють на пристроях з обмеженими ресурсами. Курс акцентує увагу на практичній розробці з використанням сучасних фреймворків і передових моделей, оптимізованих для розгортання на edge-пристроях.

## 1. Налаштування середовища розробки

### Мови програмування та фреймворки

**Середовище Python**
- **Версія**: Python 3.10 або новіше (рекомендовано: Python 3.11)
- **Менеджер пакетів**: pip або conda
- **Віртуальне середовище**: Використовуйте venv або conda для ізоляції
- **Основні бібліотеки**: Специфічні бібліотеки EdgeAI будуть встановлені під час курсу

**Середовище Microsoft .NET**
- **Версія**: .NET 8 або новіше
- **IDE**: Visual Studio 2022, Visual Studio Code або JetBrains Rider
- **SDK**: Переконайтеся, що .NET SDK встановлено для кросплатформної розробки

### Інструменти розробки

**Редактори коду та IDE**
- Visual Studio Code (рекомендовано для кросплатформної розробки)
- PyCharm або Visual Studio (для специфічної розробки мовами)
- Jupyter Notebooks для інтерактивної розробки та прототипування

**Система контролю версій**
- Git (остання версія)
- Обліковий запис GitHub для доступу до репозиторіїв і співпраці

## 2. Вимоги до обладнання та рекомендації

### Мінімальні системні вимоги
- **CPU**: Багатоядерний процесор (Intel i5/AMD Ryzen 5 або еквівалент)
- **RAM**: Мінімум 8 ГБ, рекомендовано 16 ГБ
- **Сховище**: 50 ГБ вільного місця для моделей і інструментів розробки
- **ОС**: Windows 10/11, macOS 10.15+ або Linux (Ubuntu 20.04+)

### Стратегія використання обчислювальних ресурсів
Курс розроблений для доступності на різних апаратних конфігураціях:

**Локальна розробка (фокус на CPU/NPU)**
- Основна розробка буде використовувати прискорення CPU і NPU
- Підходить для більшості сучасних ноутбуків і настільних ПК
- Акцент на ефективності та практичних сценаріях розгортання

**Хмарні GPU-ресурси (опціонально)**
- **Azure Machine Learning**: Для інтенсивного навчання та експериментів
- **Google Colab**: Безкоштовний рівень доступний для освітніх цілей
- **Kaggle Notebooks**: Альтернативна хмарна платформа для обчислень

### Особливості edge-пристроїв
- Розуміння процесорів на базі ARM
- Знання обмежень мобільного та IoT-обладнання
- Ознайомлення з оптимізацією енергоспоживання

## 3. Основні сімейства моделей та ресурси

### Основні сімейства моделей

**Сімейство Microsoft Phi-4**
- **Опис**: Компактні, ефективні моделі, розроблені для розгортання на edge-пристроях
- **Сильні сторони**: Відмінне співвідношення продуктивності до розміру, оптимізовані для задач логічного висновку
- **Ресурс**: [Колекція Phi-4 на Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Сфери застосування**: Генерація коду, математичні розрахунки, загальні розмови

**Сімейство Qwen-3**
- **Опис**: Останнє покоління багатомовних моделей від Alibaba
- **Сильні сторони**: Потужні багатомовні можливості, ефективна архітектура
- **Ресурс**: [Колекція Qwen-3 на Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Сфери застосування**: Багатомовні додатки, міжкультурні AI-рішення

**Сімейство Google Gemma-3n**
- **Опис**: Легкі моделі від Google, оптимізовані для розгортання на edge-пристроях
- **Сильні сторони**: Швидке виконання, архітектура, дружня до мобільних пристроїв
- **Ресурс**: [Колекція Gemma-3n на Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Сфери застосування**: Мобільні додатки, обробка в реальному часі

### Критерії вибору моделі
- **Співвідношення продуктивності до розміру**: Розуміння, коли вибирати менші чи більші моделі
- **Оптимізація для конкретних задач**: Відповідність моделей конкретним сценаріям використання
- **Обмеження розгортання**: Пам'ять, затримка та енергоспоживання

## 4. Інструменти для квантування та оптимізації

### Фреймворк Llama.cpp
- **Репозиторій**: [Llama.cpp на GitHub](https://github.com/ggml-org/llama.cpp)
- **Призначення**: Високопродуктивний механізм виконання для LLM
- **Основні функції**:
  - Оптимізоване виконання на CPU
  - Різні формати квантування (Q4, Q5, Q8)
  - Кросплатформна сумісність
  - Ефективне використання пам'яті
- **Встановлення та базове використання**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Репозиторій**: [Microsoft Olive на GitHub](https://github.com/microsoft/olive)
- **Призначення**: Інструментарій для оптимізації моделей для edge-розгортання
- **Основні функції**:
  - Автоматизовані робочі процеси оптимізації моделей
  - Оптимізація з урахуванням апаратного забезпечення
  - Інтеграція з ONNX Runtime
  - Інструменти для оцінки продуктивності
- **Встановлення та базове використання**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # Приклад скрипта на Python для оптимізації моделі
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (користувачі macOS)
- **Репозиторій**: [Apple MLX на GitHub](https://github.com/ml-explore/mlx)
- **Призначення**: Фреймворк машинного навчання для Apple Silicon
- **Основні функції**:
  - Оптимізація для Apple Silicon
  - Ефективні операції з пам'яттю
  - API, схожий на PyTorch
  - Підтримка архітектури об'єднаної пам'яті
- **Встановлення та базове використання**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Репозиторій**: [ONNX Runtime на GitHub](https://github.com/microsoft/onnxruntime)
- **Призначення**: Кросплатформне прискорення виконання для моделей ONNX
- **Основні функції**:
  - Оптимізації для конкретного апаратного забезпечення (CPU, GPU, NPU)
  - Графові оптимізації для виконання
  - Підтримка квантування
  - Підтримка різних мов (Python, C++, C#, JavaScript)
- **Встановлення та базове використання**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. Рекомендована література та ресурси

### Основна документація
- **Документація ONNX Runtime**: Розуміння кросплатформного виконання
- **Посібник Hugging Face Transformers**: Завантаження моделей і виконання
- **Шаблони дизайну Edge AI**: Найкращі практики для розгортання на edge-пристроях

### Технічні статті
- "Ефективний Edge AI: Огляд технік квантування"
- "Стиснення моделей для мобільних і edge-пристроїв"
- "Оптимізація моделей Transformer для edge-комп'ютингу"

### Ресурси спільноти
- **Спільноти EdgeAI у Slack/Discord**: Підтримка та обговорення з колегами
- **Репозиторії GitHub**: Приклади реалізацій і навчальні матеріали
- **YouTube-канали**: Технічні огляди та навчальні відео

## 6. Оцінка та перевірка

### Контрольний список перед курсом
- [ ] Встановлено та перевірено Python 3.10+
- [ ] Встановлено та перевірено .NET 8+
- [ ] Налаштоване середовище розробки
- [ ] Створено обліковий запис Hugging Face
- [ ] Базове ознайомлення з основними сімействами моделей
- [ ] Встановлено та протестовано інструменти для квантування
- [ ] Відповідають вимоги до обладнання
- [ ] Налаштовані облікові записи для хмарних обчислень (за потреби)

## Основні навчальні цілі

До кінця цього посібника ви зможете:

1. Налаштувати повне середовище розробки для створення додатків EdgeAI
2. Встановити та налаштувати необхідні інструменти та фреймворки для оптимізації моделей
3. Вибрати відповідні апаратні та програмні конфігурації для ваших проектів EdgeAI
4. Зрозуміти ключові аспекти розгортання AI-моделей на edge-пристроях
5. Підготувати вашу систему до практичних вправ у курсі

## Додаткові ресурси

### Офіційна документація
- **Документація Python**: Офіційна документація мови Python
- **Документація Microsoft .NET**: Офіційні ресурси для розробки на .NET
- **Документація ONNX Runtime**: Повний посібник з ONNX Runtime
- **Документація TensorFlow Lite**: Офіційна документація TensorFlow Lite

### Інструменти розробки
- **Visual Studio Code**: Легкий редактор коду з розширеннями для AI-розробки
- **Jupyter Notebooks**: Інтерактивне середовище для експериментів з ML
- **Docker**: Платформа контейнеризації для стабільних середовищ розробки
- **Git**: Система контролю версій для управління кодом

### Навчальні ресурси
- **Дослідницькі статті EdgeAI**: Останні академічні дослідження ефективних моделей
- **Онлайн-курси**: Додаткові навчальні матеріали з оптимізації AI
- **Форуми спільноти**: Платформи для запитань і відповідей щодо викликів у розробці EdgeAI
- **Еталонні набори даних**: Стандартні набори даних для оцінки продуктивності моделей

## Результати навчання

Після завершення цього підготовчого посібника ви:

1. Матимете повністю налаштоване середовище розробки для EdgeAI
2. Зрозумієте вимоги до обладнання та програмного забезпечення для різних сценаріїв розгортання
3. Ознайомитеся з ключовими фреймворками та інструментами, які використовуються в курсі
4. Зможете вибирати відповідні моделі залежно від обмежень пристрою та вимог
5. Отримаєте базові знання про техніки оптимізації для розгортання на edge-пристроях

## ➡️ Що далі

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.