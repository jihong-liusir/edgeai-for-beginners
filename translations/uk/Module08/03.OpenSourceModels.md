<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-25T02:31:01+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "uk"
}
-->
# Сесія 3: Відкриття та управління моделями з відкритим кодом

## Огляд

Ця сесія присвячена практичному відкриттю та управлінню моделями за допомогою Foundry Local. Ви навчитеся перераховувати доступні моделі, тестувати різні варіанти та розуміти основні характеристики продуктивності. Підхід акцентує увагу на практичному дослідженні за допомогою CLI Foundry, щоб допомогти вам вибрати правильні моделі для ваших завдань.

## Цілі навчання

- Опанувати команди CLI Foundry для відкриття та управління моделями
- Зрозуміти шаблони кешування моделей та локального зберігання
- Навчитися швидко тестувати та порівнювати різні моделі
- Встановити практичні робочі процеси для вибору моделей та оцінки продуктивності
- Дослідити зростаючу екосистему моделей, доступних через Foundry Local

## Передумови

- Завершена Сесія 1: Початок роботи з Foundry Local
- Встановлений та доступний CLI Foundry Local
- Достатньо місця для зберігання моделей (розмір моделей може варіюватися від 1GB до 20GB+)
- Базове розуміння типів моделей та їх застосування

## Частина 6: Практичне завдання

### Вправа: Відкриття та порівняння моделей

Створіть власний скрипт для оцінки моделей на основі Зразка 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Ваше завдання

1. **Запустіть скрипт Зразка 03**: `samples\03\list_and_bench.cmd`
2. **Спробуйте різні моделі**: протестуйте щонайменше 3 різні моделі
3. **Порівняйте продуктивність**: зверніть увагу на відмінності у швидкості та якості відповіді
4. **Задокументуйте результати**: створіть просту таблицю порівняння

### Приклад формату порівняння

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Частина 7: Вирішення проблем та найкращі практики

### Поширені проблеми та їх вирішення

**Модель не запускається:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Недостатньо пам'яті:**
- Почніть з менших моделей (`phi-4-mini`)
- Закрийте інші програми
- Оновіть оперативну пам'ять, якщо часто стикаєтеся з обмеженнями

**Повільна продуктивність:**
- Переконайтеся, що модель повністю завантажена (перевірте детальний вивід)
- Закрийте непотрібні фонові програми
- Розгляньте використання швидшого сховища (SSD)

### Найкращі практики

1. **Починайте з малого**: Почніть з `phi-4-mini`, щоб перевірити налаштування
2. **Одна модель за раз**: Зупиняйте попередні моделі перед запуском нових
3. **Моніторинг ресурсів**: Слідкуйте за використанням пам'яті
4. **Тестуйте послідовно**: Використовуйте однакові запити для справедливого порівняння
5. **Документуйте результати**: Ведіть записи про продуктивність моделей для ваших завдань

## Частина 8: Наступні кроки та ресурси

### Підготовка до Сесії 4

- **Фокус Сесії 4**: Інструменти та техніки оптимізації
- **Передумови**: Впевненість у перемиканні моделей та базовому тестуванні продуктивності
- **Рекомендація**: Визначте 2-3 улюблені моделі з цієї сесії

### Додаткові ресурси

- **[Документація Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Офіційна документація
- **[Довідник CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Повний довідник команд
- **[Model Mondays](https://aka.ms/model-mondays)**: Щотижневі огляди моделей
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Спільнота та питання
- **[Зразок 03: Відкриття моделей](samples/03/README.md)**: Практичний приклад скрипту

### Основні висновки

✅ **Відкриття моделей**: Використовуйте `foundry model list` для дослідження доступних моделей  
✅ **Швидке тестування**: Шаблон `list_and_bench.cmd` для швидкої оцінки  
✅ **Моніторинг продуктивності**: Базове використання ресурсів та вимірювання часу відповіді  
✅ **Вибір моделей**: Практичні рекомендації для вибору моделей за завданнями  
✅ **Управління кешем**: Розуміння процедур зберігання та очищення  

Тепер ви маєте практичні навички для відкриття, тестування та вибору відповідних моделей для ваших AI-застосувань, використовуючи простий CLI-підхід Foundry Local.

## Цілі навчання
- Відкривати та оцінювати моделі з відкритим кодом для локального використання
- Компілювати та запускати вибрані моделі Hugging Face у Foundry Local
- Застосовувати стратегії вибору моделей для точності, затримки та потреб у ресурсах
- Управляти моделями локально за допомогою кешу та версій

## Частина 1: Відкриття моделей за допомогою CLI Foundry

### Основні команди управління моделями

CLI Foundry пропонує прості команди для відкриття та управління моделями:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Запуск ваших перших моделей

Почніть з популярних, добре протестованих моделей, щоб зрозуміти їх характеристики продуктивності:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Примітка:** Прапорець `--verbose` надає детальну інформацію про запуск, включаючи:
- Прогрес завантаження моделі (під час першого запуску)
- Деталі розподілу пам'яті
- Інформацію про прив'язку сервісу
- Метрики ініціалізації продуктивності

### Розуміння категорій моделей

**Малі мовні моделі (SLMs):**
- `phi-4-mini`: Швидка, ефективна, чудова для загального спілкування
- `phi-4`: Більш потужна версія з кращими можливостями логічного мислення

**Середні моделі:**
- `qwen2.5-7b-instruct`: Відмінне логічне мислення та довший контекст
- `deepseek-r1-distill-qwen-7b`: Оптимізована для генерації коду

**Великі моделі:**
- `llama-3.2`: Остання модель з відкритим кодом від Meta
- `qwen2.5-14b-instruct`: Рівень підприємства для логічного мислення

## Частина 2: Швидке тестування та порівняння моделей

### Підхід Зразка 03: Простий список та оцінка

На основі шаблону Зразка 03, ось мінімальний робочий процес:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Тестування продуктивності моделей

Після запуску моделі протестуйте її за допомогою послідовних запитів:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Альтернатива тестування в PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Частина 3: Управління кешем та зберіганням моделей

### Розуміння кешу моделей

Foundry Local автоматично управляє завантаженням та кешуванням моделей:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Рекомендації щодо зберігання моделей

**Типові розміри моделей:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Найкращі практики зберігання:**
- Тримайте 2-3 моделі в кеші для швидкого перемикання
- Видаляйте невикористані моделі для звільнення місця: `foundry cache clean`
- Слідкуйте за використанням диска, особливо на менших SSD
- Розглядайте компроміс між розміром моделі та її можливостями

### Моніторинг продуктивності моделей

Під час роботи моделей слідкуйте за системними ресурсами:

**Диспетчер завдань Windows:**
- Слідкуйте за використанням пам'яті (моделі залишаються завантаженими в RAM)
- Моніторинг використання CPU під час виконання запитів
- Перевірка дискового вводу/виводу під час початкового завантаження моделі

**Моніторинг через командний рядок:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Частина 4: Практичні рекомендації щодо вибору моделей

### Вибір моделей за завданнями

**Для загального спілкування та запитань-відповідей:**
- Почніть з: `phi-4-mini` (швидка, ефективна)
- Перейдіть до: `phi-4` (краще логічне мислення)
- Розширений варіант: `qwen2.5-7b-instruct` (довший контекст)

**Для генерації коду:**
- Рекомендовано: `deepseek-r1-distill-qwen-7b`
- Альтернатива: `qwen2.5-7b-instruct` (також добре для коду)

**Для складного логічного мислення:**
- Найкраще: `qwen2.5-7b-instruct` або `qwen2.5-14b-instruct`
- Бюджетний варіант: `phi-4`

### Рекомендації щодо апаратного забезпечення

**Мінімальні системні вимоги:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Рекомендовано для найкращої продуктивності:**
- 32GB+ RAM для комфортного перемикання між моделями
- SSD для швидшого завантаження моделей
- Сучасний CPU з хорошою продуктивністю на одному ядрі
- Підтримка NPU (ПК з Windows 11 Copilot+) для прискорення

### Робочий процес перемикання моделей

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Частина 5: Просте тестування продуктивності моделей

### Базове тестування продуктивності

Ось простий підхід для порівняння продуктивності моделей:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Ручна оцінка якості

Для кожної моделі протестуйте її за допомогою послідовних запитів та оцініть вручну:

**Тестові запити:**
1. "Поясніть квантові обчислення простими словами."
2. "Напишіть функцію Python для сортування списку."
3. "Які плюси та мінуси дистанційної роботи?"
4. "Підсумуйте переваги edge AI."

**Критерії оцінки:**
- **Точність**: Чи правильна інформація?
- **Чіткість**: Чи легко зрозуміти пояснення?
- **Повнота**: Чи відповідає на повне запитання?
- **Швидкість**: Як швидко модель відповідає?

### Моніторинг використання ресурсів

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Частина 6: Наступні кроки
- Підпишіться на Model Mondays для нових моделей та порад: https://aka.ms/model-mondays
- Внесіть результати до `models.json` вашої команди
- Підготуйтеся до Сесії 4: порівняння LLMs vs SLMs, локального та хмарного використання, а також практичних демонстрацій

---

