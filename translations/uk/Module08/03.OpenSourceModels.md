<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:03:31+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "uk"
}
-->
# Сесія 3: Моделі з відкритим кодом у Foundry Local

## Огляд

У цій сесії ми розглянемо, як інтегрувати моделі з відкритим кодом у Foundry Local: вибір моделей спільноти, інтеграція контенту Hugging Face та використання стратегії "принеси свою власну модель" (BYOM). Ви також дізнаєтеся про серію Model Mondays для постійного навчання та відкриття нових моделей.

Посилання:
- Документація Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Компіляція моделей Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Цілі навчання
- Відкривати та оцінювати моделі з відкритим кодом для локального використання
- Компілювати та запускати вибрані моделі Hugging Face у Foundry Local
- Застосовувати стратегії вибору моделей з урахуванням точності, затримки та потреб у ресурсах
- Керувати моделями локально за допомогою кешу та версій

## Частина 1: Відкриття та вибір моделей (покроково)

Крок 1) Складіть список доступних моделей у локальному каталозі  
```cmd
foundry model list
```
  
Крок 2) Швидко протестуйте дві моделі (автоматичне завантаження при першому запуску)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Крок 3) Зверніть увагу на основні метрики  
- Спостерігайте за затримкою (суб'єктивно) та якістю для заданого запиту  
- Перевіряйте використання пам'яті через Task Manager під час роботи кожної моделі  

## Частина 2: Запуск моделей каталогу через CLI (покроково)

Крок 1) Запустіть модель  
```cmd
foundry model run llama-3.2
```
  
Крок 2) Надішліть тестовий запит через сумісний з OpenAI кінцевий пункт  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Частина 3: BYOM – Компіляція моделей Hugging Face (покроково)

Скористайтеся офіційною інструкцією для компіляції моделей. Нижче наведено загальний процес — дивіться статтю Microsoft Learn для точних команд і підтримуваних конфігурацій.

Крок 1) Підготуйте робочу директорію  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Крок 2) Компілюйте підтримувану модель HF  
- Використовуйте кроки з документації Learn для конвертації та розміщення скомпільованої моделі ONNX у вашій директорії `models`  
- Перевірте за допомогою:  
```cmd
foundry cache ls
```
  
Ви повинні побачити назву вашої скомпільованої моделі (наприклад, `llama-3.2`).  

Крок 3) Запустіть скомпільовану модель  
```cmd
foundry model run llama-3.2 --verbose
```
  
Примітки:  
- Переконайтеся, що у вас достатньо дискового простору та оперативної пам'яті для компіляції та запуску  
- Почніть із менших моделей, щоб перевірити процес, а потім переходьте до більших  

## Частина 4: Практична робота з моделями (покроково)

Крок 1) Створіть реєстр `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Крок 2) Невеликий скрипт для вибору  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Частина 5: Практичні бенчмарки (покроково)

Крок 1) Простий бенчмарк затримки  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Крок 2) Перевірка якості  
- Використовуйте фіксований набір запитів, зберігайте результати у форматі CSV/JSON  
- Оцініть вручну плавність, релевантність і правильність (1–5)  

## Частина 6: Наступні кроки
- Підпишіться на Model Mondays для нових моделей і порад: https://aka.ms/model-mondays  
- Діліться результатами з вашою командою через `models.json`  
- Підготуйтеся до Сесії 4: порівняння LLMs і SLMs, локального та хмарного використання, а також практичні демонстрації  

---

