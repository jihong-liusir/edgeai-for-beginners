<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7a474b8e201d5316c0095cdbc3bf0555",
  "translation_date": "2025-09-25T03:11:21+00:00",
  "source_file": "Module08/samples/04/webgpu-demo/README.md",
  "language_code": "uk"
}
-->
# Демонстрація WebGPU + ONNX Runtime

Ця демонстрація показує, як запускати AI-моделі безпосередньо в браузері, використовуючи WebGPU для апаратного прискорення та ONNX Runtime Web.

## Що демонструється

- **AI у браузері**: Запуск моделей повністю в браузері
- **Прискорення WebGPU**: Інференс із апаратним прискоренням, якщо доступно
- **Приватність**: Дані не залишають ваш пристрій
- **Без встановлення**: Працює в будь-якому сумісному браузері
- **Гнучке резервування**: Перемикається на CPU, якщо WebGPU недоступний

## Вимоги

**Сумісність браузера:**
- Chrome/Edge 113+ із увімкненим WebGPU
- Перевірте статус WebGPU: `chrome://gpu`
- Увімкніть WebGPU: `chrome://flags/#enable-unsafe-webgpu`

## Запуск демонстрації

### Варіант 1: Локальний сервер (рекомендовано)

```cmd
# Navigate to the demo directory
cd Module08\samples\04\webgpu-demo

# Start a local server
python -m http.server 5173

# Open browser to http://localhost:5173
```

### Варіант 2: Live Server у VS Code

1. Встановіть розширення "Live Server" у VS Code
2. Клацніть правою кнопкою миші на `index.html` → "Open with Live Server"
3. Демонстрація автоматично відкриється в браузері

## Що ви побачите

1. **Перевірка WebGPU**: Перевіряє сумісність браузера
2. **Завантаження моделі**: Завантажує та ініціалізує класифікатор MNIST
3. **Виконання інференсу**: Виконує прогнозування на зразкових даних
4. **Метрики продуктивності**: Показує час завантаження та швидкість інференсу
5. **Відображення результатів**: Впевненість прогнозу та необроблені результати

## Очікувана продуктивність

| Провайдер виконання | Завантаження моделі | Інференс | Примітки |
|---------------------|---------------------|----------|----------|
| **WebGPU**          | ~2-5с              | ~10-50мс | Апаратне прискорення |
| **CPU (WASM)**      | ~2-5с              | ~50-200мс | Резервне програмне виконання |

## Вирішення проблем

**WebGPU недоступний:**
- Оновіть Chrome/Edge до версії 113+
- Увімкніть WebGPU у `chrome://flags`
- Перевірте актуальність драйверів GPU
- Демонстрація автоматично переключиться на CPU

**Помилки завантаження:**
- Переконайтеся, що ви використовуєте HTTP (не file://)
- Перевірте мережеве з'єднання для завантаження моделі
- Переконайтеся, що CORS не блокує ONNX модель

**Проблеми продуктивності:**
- WebGPU забезпечує значне прискорення порівняно з CPU
- Перше виконання може бути повільнішим через завантаження моделі
- Наступні виконання використовують кеш браузера

## Інтеграція з Foundry Local

Ця демонстрація WebGPU доповнює Foundry Local, показуючи:

- **Інференс на стороні клієнта** для максимальної приватності
- **Офлайн-можливості** при відсутності інтернету  
- **Розгортання на периферії** для середовищ із обмеженими ресурсами
- **Гібридні архітектури**, що поєднують локальний і серверний інференс

Для виробничих застосувань розгляньте:
- Використання Foundry Local для серверного інференсу
- Використання WebGPU для локальної обробки перед/після інференсу
- Реалізацію інтелектуального маршрутизації між локальним і віддаленим інференсом

## Технічні деталі

**Використана модель:**
- Класифікатор цифр MNIST (формат ONNX)
- Вхід: зображення 28x28 у градаціях сірого
- Вихід: розподіл ймовірностей для 10 класів
- Розмір: ~500KB (швидке завантаження)

**ONNX Runtime Web:**
- Провайдер виконання WebGPU для прискорення на GPU
- Провайдер виконання WASM для резервного виконання на CPU
- Автоматична оптимізація та оптимізація графу

**API браузера:**
- WebGPU для доступу до апаратного забезпечення
- Web Workers для фонової обробки (майбутнє покращення)
- WebAssembly для ефективних обчислень

## Наступні кроки

- Спробуйте з власними моделями ONNX
- Реалізуйте завантаження реальних зображень і класифікацію
- Додайте потоковий інференс для більших моделей
- Інтегруйте введення з камери/мікрофона

---

