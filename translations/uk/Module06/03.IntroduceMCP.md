<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T23:54:22+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "uk"
}
-->
# Розділ 03 - Інтеграція протоколу контексту моделі (MCP)

## Вступ до MCP (Протокол контексту моделі)

Протокол контексту моделі (MCP) — це революційна структура, яка дозволяє мовним моделям взаємодіяти із зовнішніми інструментами та системами у стандартизований спосіб. На відміну від традиційних підходів, де моделі ізольовані, MCP створює міст між AI-моделями та реальним світом через чітко визначений протокол.

### Що таке MCP?

MCP слугує комунікаційним протоколом, який дозволяє мовним моделям:
- Підключатися до зовнішніх джерел даних
- Виконувати інструменти та функції
- Взаємодіяти з API та сервісами
- Отримувати інформацію в реальному часі
- Виконувати складні багатокрокові операції

Цей протокол перетворює статичні мовні моделі на динамічних агентів, здатних виконувати практичні завдання, що виходять за межі генерації тексту.

## Малі мовні моделі (SLMs) у MCP

Малі мовні моделі представляють ефективний підхід до впровадження AI, пропонуючи кілька переваг:

### Переваги SLMs
- **Ефективність ресурсів**: Менші вимоги до обчислювальних потужностей
- **Швидший час відгуку**: Зменшена затримка для застосунків у реальному часі  
- **Економічність**: Мінімальні потреби в інфраструктурі
- **Конфіденційність**: Можливість локального запуску без передачі даних
- **Налаштування**: Легше адаптувати до конкретних доменів

### Чому SLMs добре працюють із MCP

SLMs у поєднанні з MCP створюють потужну комбінацію, де можливості моделі щодо логічного мислення доповнюються зовнішніми інструментами, компенсуючи меншу кількість параметрів через розширену функціональність.

## Огляд Python MCP SDK

Python MCP SDK забезпечує основу для створення застосунків із підтримкою MCP. SDK включає:

- **Клієнтські бібліотеки**: Для підключення до серверів MCP
- **Серверний фреймворк**: Для створення власних серверів MCP
- **Обробники протоколу**: Для управління комунікацією
- **Інтеграцію інструментів**: Для виконання зовнішніх функцій

## Практична реалізація: клієнт Phi-4 MCP

Розглянемо реальну реалізацію, використовуючи міні-модель Phi-4 від Microsoft, інтегровану з можливостями MCP.

### Архітектура системи

Реалізація слідує багатошаровій архітектурі:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Основні компоненти

#### 1. Класи клієнтів MCP

**BaseMCPClient**: Абстрактна основа, що забезпечує загальну функціональність
- Протокол асинхронного менеджера контексту
- Визначення стандартного інтерфейсу
- Управління ресурсами

**Phi4MiniMCPClient**: Реалізація на основі STDIO
- Локальна комунікація процесів
- Обробка стандартного вводу/виводу
- Управління підпроцесами

**Phi4MiniSSEMCPClient**: Реалізація на основі подій, що надсилаються сервером
- HTTP-стрімінгова комунікація
- Обробка подій у реальному часі
- Підключення до веб-серверів

#### 2. Інтеграція LLM

**OllamaClient**: Локальний хостинг моделі
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Високопродуктивне обслуговування
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Конвеєр обробки інструментів

Конвеєр обробки інструментів перетворює MCP-інструменти у формати, сумісні з мовними моделями:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Початок роботи: покрокова інструкція

### Крок 1: Налаштування середовища

Встановіть необхідні залежності:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Крок 2: Базова конфігурація

Налаштуйте змінні середовища:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Крок 3: Запуск першого клієнта MCP

**Базове налаштування Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Використання бекенду vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Підключення через події, що надсилаються сервером:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Власний сервер MCP:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Крок 4: Програмне використання

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Розширені функції

### Підтримка кількох бекендів

Реалізація підтримує як Ollama, так і vLLM бекенди, дозволяючи вибирати залежно від ваших потреб:

- **Ollama**: Краще для локальної розробки та тестування
- **vLLM**: Оптимізовано для виробничих сценаріїв із високою пропускною здатністю

### Гнучкі протоколи підключення

Підтримуються два режими підключення:

**Режим STDIO**: Пряма комунікація процесів
- Менша затримка
- Підходить для локальних інструментів
- Просте налаштування

**Режим SSE**: Стрімінг через HTTP
- Можливість роботи в мережі
- Краще для розподілених систем
- Оновлення в реальному часі

### Можливості інтеграції інструментів

Система може інтегруватися з різними інструментами:
- Веб-автоматизація (Playwright)
- Операції з файлами
- Взаємодія з API
- Системні команди
- Власні функції

## Обробка помилок і найкращі практики

### Комплексне управління помилками

Реалізація включає надійне управління помилками для:

**Помилки підключення:**
- Збої серверів MCP
- Тайм-аути мережі
- Проблеми з підключенням

**Помилки виконання інструментів:**
- Відсутні інструменти
- Перевірка параметрів
- Збої виконання

**Помилки обробки відповідей:**
- Проблеми з розбором JSON
- Непослідовності форматів
- Аномалії відповідей LLM

### Найкращі практики

1. **Управління ресурсами**: Використовуйте асинхронні менеджери контексту
2. **Обробка помилок**: Реалізуйте комплексні блоки try-catch
3. **Логування**: Увімкніть відповідні рівні логування
4. **Безпека**: Перевіряйте введення та очищуйте виведення
5. **Продуктивність**: Використовуйте пулінг підключень і кешування

## Реальні застосування

### Веб-автоматизація
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Обробка даних
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Інтеграція API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Оптимізація продуктивності

### Управління пам'яттю
- Ефективна обробка історії повідомлень
- Правильне очищення ресурсів
- Пулінг підключень

### Оптимізація мережі
- Асинхронні HTTP-операції
- Налаштовувані тайм-аути
- Гнучке відновлення після помилок

### Конкурентна обробка
- Неблокуючий ввід/вивід
- Паралельне виконання інструментів
- Ефективні асинхронні шаблони

## Міркування щодо безпеки

### Захист даних
- Безпечне управління ключами API
- Перевірка введення
- Очищення виведення

### Мережева безпека
- Підтримка HTTPS
- Локальні налаштування за замовчуванням
- Безпечне управління токенами

### Безпека виконання
- Фільтрація інструментів
- Пісочниця для середовищ
- Логування аудиту

## Висновок

SLMs, інтегровані з MCP, представляють зміну парадигми у розробці AI-застосунків. Поєднуючи ефективність малих моделей із потужністю зовнішніх інструментів, розробники можуть створювати інтелектуальні системи, які є одночасно ресурсоефективними та високофункціональними.

Реалізація клієнта Phi-4 MCP демонструє, як ця інтеграція може бути досягнута на практиці, забезпечуючи міцну основу для створення складних AI-застосунків.

Основні висновки:
- MCP створює міст між мовними моделями та зовнішніми системами
- SLMs пропонують ефективність без втрати функціональності завдяки інструментам
- Модульна архітектура дозволяє легке розширення та налаштування
- Належна обробка помилок і заходи безпеки є важливими для використання у виробництві

Цей посібник забезпечує основу для створення власних застосунків із підтримкою SLM і MCP, відкриваючи можливості для автоматизації, обробки даних та інтеграції інтелектуальних систем.

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.