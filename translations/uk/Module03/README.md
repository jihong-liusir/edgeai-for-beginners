<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-19T01:24:27+00:00",
  "source_file": "Module03/README.md",
  "language_code": "uk"
}
-->
# Розділ 03: Розгортання Малих Мовних Моделей (SLM)

Цей детальний розділ охоплює повний життєвий цикл розгортання Малих Мовних Моделей (SLM), включаючи теоретичні основи, практичні стратегії реалізації та готові до виробництва контейнеризовані рішення. Розділ структурований у три прогресивні секції, які проводять читачів від базових концепцій до складних сценаріїв розгортання.

## Структура розділу та навчальний шлях

### **[Секція 1: Розширене навчання SLM - Основи та оптимізація](./01.SLMAdvancedLearning.md)**
Початкова секція закладає теоретичну основу для розуміння Малих Мовних Моделей та їх стратегічної важливості в розгортанні AI на периферії. У цій секції розглядаються:

- **Класифікаційна структура параметрів**: Детальне дослідження категорій SLM від Мікро SLM (100M-1.4B параметрів) до Середніх SLM (14B-30B параметрів), з особливим акцентом на моделі, такі як Phi-4-mini-3.8B, серія Qwen3 та Google Gemma3, включаючи аналіз апаратних вимог і пам'яті для кожного рівня моделі
- **Розширені техніки оптимізації**: Всеосяжне висвітлення методів квантування за допомогою Llama.cpp, Microsoft Olive та Apple MLX, включаючи передові методи BitNET 1-бітного квантування з практичними прикладами коду, що демонструють конвеєри квантування та результати тестування
- **Стратегії отримання моделей**: Глибокий аналіз екосистеми Hugging Face та каталогу моделей Azure AI Foundry для корпоративного розгортання SLM, з прикладами коду для програмного завантаження, перевірки та конвертації форматів моделей
- **API для розробників**: Приклади коду на Python, C++ та C#, які показують, як завантажувати моделі, виконувати інференцію та інтегруватися з популярними фреймворками, такими як PyTorch, TensorFlow та ONNX Runtime

Ця базова секція підкреслює баланс між операційною ефективністю, гнучкістю розгортання та економічністю, що робить SLM ідеальними для сценаріїв обчислень на периферії, з практичними прикладами коду, які розробники можуть безпосередньо впроваджувати у свої проєкти.

### **[Секція 2: Розгортання в локальному середовищі - Рішення з пріоритетом конфіденційності](./02.DeployingSLMinLocalEnv.md)**
Друга секція переходить від теорії до практичної реалізації, зосереджуючись на стратегіях локального розгортання, які надають пріоритет суверенітету даних та операційній незалежності. Основні теми включають:

- **Універсальна платформа Ollama**: Всеосяжне дослідження кросплатформного розгортання з акцентом на зручні для розробників робочі процеси, управління життєвим циклом моделей та налаштування через Modelfiles, включаючи повні приклади інтеграції REST API та скрипти автоматизації CLI
- **Microsoft Foundry Local**: Рішення корпоративного рівня для розгортання з оптимізацією на основі ONNX, інтеграцією Windows ML та комплексними функціями безпеки, з прикладами коду на C# та Python для інтеграції в нативні додатки
- **Порівняльний аналіз**: Детальне порівняння фреймворків, що охоплює технічну архітектуру, характеристики продуктивності та рекомендації щодо оптимізації для конкретних випадків використання, з кодом для тестування швидкості інференції та використання пам'яті на різному апаратному забезпеченні
- **Інтеграція API**: Зразки додатків, які показують, як створювати веб-сервіси, чат-додатки та конвеєри обробки даних за допомогою локальних розгортань SLM, з прикладами коду на Node.js, Python Flask/FastAPI та ASP.NET Core
- **Фреймворки тестування**: Автоматизовані підходи до забезпечення якості моделей, включаючи приклади модульного та інтеграційного тестування для реалізацій SLM

Ця секція надає практичні рекомендації для організацій, які прагнуть впроваджувати AI-рішення з пріоритетом конфіденційності, зберігаючи повний контроль над своїм середовищем розгортання, з готовими до використання прикладами коду, які розробники можуть адаптувати до своїх конкретних вимог.

### **[Секція 3: Контейнеризоване хмарне розгортання - Рішення для масштабного виробництва](./03.DeployingSLMinCloud.md)**
Остання секція завершується розширеними стратегіями контейнеризованого розгортання, з використанням Microsoft Phi-4-mini-instruct як основного кейсу. У цій секції розглядаються:

- **Розгортання vLLM**: Оптимізація високопродуктивної інференції з OpenAI-сумісними API, розширеним прискоренням GPU та конфігурацією для виробництва, включаючи повні Dockerfiles, Kubernetes manifests та параметри налаштування продуктивності
- **Оркестрація контейнерів Ollama**: Спрощені робочі процеси розгортання з Docker Compose, варіанти оптимізації моделей та інтеграція веб-інтерфейсу, з прикладами CI/CD конвеєрів для автоматизованого розгортання та тестування
- **Реалізація ONNX Runtime**: Оптимізоване для периферії розгортання з всеосяжною конвертацією моделей, стратегіями квантування та кросплатформною сумісністю, включаючи детальні приклади коду для оптимізації та розгортання моделей
- **Моніторинг та спостереження**: Реалізація панелей Prometheus/Grafana з користувацькими метриками для моніторингу продуктивності SLM, включаючи конфігурації сповіщень та агрегацію логів
- **Балансування навантаження та масштабування**: Практичні приклади горизонтального та вертикального масштабування зі конфігураціями автоматичного масштабування на основі використання CPU/GPU та шаблонів запитів
- **Посилення безпеки**: Найкращі практики безпеки контейнерів, включаючи зменшення привілеїв, політики мережі та управління секретами для ключів API та облікових даних доступу до моделей

Кожен підхід до розгортання представлений з повними прикладами конфігурації, процедурами тестування, контрольними списками готовності до виробництва та шаблонами інфраструктури як коду, які розробники можуть безпосередньо застосовувати у своїх робочих процесах розгортання.

## Основні результати навчання

Після завершення цього розділу читачі опанують:

1. **Стратегічний вибір моделей**: Розуміння меж параметрів та вибір відповідних SLM на основі обмежень ресурсів і вимог до продуктивності
2. **Майстерність оптимізації**: Впровадження розширених технік квантування в різних фреймворках для досягнення оптимального балансу продуктивності та ефективності
3. **Гнучкість розгортання**: Вибір між локальними рішеннями з пріоритетом конфіденційності та масштабованими контейнеризованими розгортаннями залежно від потреб організації
4. **Готовність до виробництва**: Налаштування систем моніторингу, безпеки та масштабування для розгортання SLM корпоративного рівня

## Практична орієнтація та реальні застосування

Розділ зберігає сильну практичну орієнтацію, включаючи:

- **Практичні приклади**: Повні файли конфігурації, процедури тестування API та скрипти розгортання
- **Тестування продуктивності**: Детальні порівняння швидкості інференції, використання пам'яті та вимог до ресурсів
- **Міркування щодо безпеки**: Практики безпеки корпоративного рівня, рамки відповідності та стратегії захисту даних
- **Найкращі практики**: Рекомендації, перевірені у виробництві, для моніторингу, масштабування та обслуговування

## Перспектива готовності до майбутнього

Розділ завершується перспективними інсайтами щодо нових тенденцій, включаючи:

- Розширені архітектури моделей з покращеними коефіцієнтами ефективності
- Глибшу інтеграцію з апаратним забезпеченням, спеціалізованим для AI
- Еволюцію екосистеми у напрямку стандартизації та сумісності
- Шаблони корпоративного впровадження, зумовлені вимогами конфіденційності та відповідності

Цей всеосяжний підхід забезпечує читачів необхідними знаннями для вирішення як поточних викликів розгортання SLM, так і майбутніх технологічних розробок, дозволяючи приймати обґрунтовані рішення, які відповідають їхнім конкретним організаційним вимогам та обмеженням.

Розділ слугує як практичний посібник для негайного впровадження, так і стратегічний ресурс для довгострокового планування розгортання AI, підкреслюючи критичний баланс між можливостями, ефективністю та операційною досконалістю, що визначає успішне розгортання SLM.

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, зверніть увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.