<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-19T01:34:04+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "uk"
}
-->
# Розділ 2: Розгортання локального середовища — рішення з пріоритетом конфіденційності

Локальне розгортання малих мовних моделей (SLM) представляє зміну парадигми у напрямку конфіденційних, економічно ефективних AI-рішень. Цей детальний посібник досліджує два потужні фреймворки — Ollama та Microsoft Foundry Local — які дозволяють розробникам використовувати весь потенціал SLM, зберігаючи повний контроль над середовищем розгортання.

## Вступ

У цьому уроці ми розглянемо передові стратегії розгортання малих мовних моделей у локальних середовищах. Ми охопимо основні концепції локального AI-розгортання, дослідимо дві провідні платформи (Ollama та Microsoft Foundry Local) і надамо практичні рекомендації для впровадження готових до виробництва рішень.

## Цілі навчання

До кінця цього уроку ви зможете:

- Зрозуміти архітектуру та переваги фреймворків локального розгортання SLM.
- Реалізувати готові до виробництва розгортання за допомогою Ollama та Microsoft Foundry Local.
- Порівняти та вибрати відповідну платформу на основі конкретних вимог і обмежень.
- Оптимізувати локальні розгортання для продуктивності, безпеки та масштабованості.

## Розуміння архітектур локального розгортання SLM

Локальне розгортання SLM представляє фундаментальну зміну від хмарних AI-сервісів до локальних рішень, які зберігають конфіденційність. Цей підхід дозволяє організаціям зберігати повний контроль над своєю AI-інфраструктурою, забезпечуючи суверенітет даних і незалежність операцій.

### Класифікація фреймворків розгортання

Розуміння різних підходів до розгортання допомагає вибрати правильну стратегію для конкретних випадків використання:

- **Орієнтовані на розробку**: Спрощене налаштування для експериментів і прототипування.
- **Корпоративного рівня**: Готові до виробництва рішення з можливостями інтеграції в корпоративне середовище.
- **Кросплатформні**: Універсальна сумісність з різними операційними системами та апаратним забезпеченням.

### Основні переваги локального розгортання SLM

Локальне розгортання SLM пропонує кілька фундаментальних переваг, які роблять його ідеальним для корпоративних і конфіденційних застосувань:

**Конфіденційність і безпека**: Локальна обробка гарантує, що конфіденційні дані ніколи не залишають інфраструктуру організації, забезпечуючи відповідність GDPR, HIPAA та іншим нормативним вимогам. Можливі розгортання в ізольованих мережах для класифікованих середовищ, а повні журнали аудиту підтримують контроль безпеки.

**Економічна ефективність**: Усунення моделей ціноутворення за токен значно знижує операційні витрати. Зменшення потреб у пропускній здатності та залежності від хмари забезпечує передбачувані структури витрат для корпоративного бюджетування.

**Продуктивність і надійність**: Швидший час виконання без затримок мережі дозволяє використовувати моделі в реальному часі. Офлайн-функціональність забезпечує безперервну роботу незалежно від підключення до інтернету, а оптимізація локальних ресурсів забезпечує стабільну продуктивність.

## Ollama: Універсальна платформа локального розгортання

### Основна архітектура та філософія

Ollama розроблена як універсальна, зручна для розробників платформа, яка демократизує локальне розгортання LLM на різних апаратних конфігураціях і операційних системах.

**Технічна основа**: Побудована на надійному фреймворку llama.cpp, Ollama використовує ефективний формат моделі GGUF для оптимальної продуктивності. Кросплатформна сумісність забезпечує стабільну роботу в середовищах Windows, macOS і Linux, а інтелектуальне управління ресурсами оптимізує використання CPU, GPU та пам’яті.

**Філософія дизайну**: Ollama надає пріоритет простоті без втрати функціональності, пропонуючи розгортання без налаштувань для негайної продуктивності. Платформа підтримує широку сумісність моделей, забезпечуючи стабільні API для різних архітектур моделей.

### Розширені функції та можливості

**Висока якість управління моделями**: Ollama забезпечує комплексне управління життєвим циклом моделей з автоматичним завантаженням, кешуванням і версіонуванням. Платформа підтримує широкий екосистему моделей, включаючи Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral та спеціалізовані моделі для вбудовування.

**Налаштування через Modelfiles**: Досвідчені користувачі можуть створювати власні конфігурації моделей із конкретними параметрами, системними підказками та модифікаціями поведінки. Це дозволяє оптимізувати моделі для конкретних галузей і спеціалізованих вимог.

**Оптимізація продуктивності**: Ollama автоматично виявляє та використовує доступне апаратне прискорення, включаючи NVIDIA CUDA, Apple Metal і OpenCL. Інтелектуальне управління пам’яттю забезпечує оптимальне використання ресурсів на різних апаратних конфігураціях.

### Стратегії впровадження у виробництво

**Встановлення та налаштування**: Ollama забезпечує спрощене встановлення на різних платформах через рідні інсталятори, менеджери пакетів (WinGet, Homebrew, APT) і Docker-контейнери для контейнеризованих розгортань.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Основні команди та операції**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Розширене налаштування**: Modelfiles дозволяють складне налаштування для корпоративних вимог:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Приклади інтеграції для розробників

**Інтеграція через Python API**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Інтеграція через JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Використання RESTful API через cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Налаштування продуктивності та оптимізація

**Конфігурація пам’яті та потоків**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Вибір квантизації для різного апаратного забезпечення**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Платформа корпоративного AI на периферії

### Архітектура корпоративного рівня

Microsoft Foundry Local представляє комплексне корпоративне рішення, спеціально розроблене для виробничих AI-розгортань на периферії з глибокою інтеграцією в екосистему Microsoft.

**Основа на базі ONNX**: Побудована на стандартному ONNX Runtime, Foundry Local забезпечує оптимізовану продуктивність на різних апаратних архітектурах. Платформа використовує інтеграцію Windows ML для оптимізації роботи на Windows, зберігаючи кросплатформну сумісність.

**Висока якість апаратного прискорення**: Foundry Local забезпечує інтелектуальне виявлення та оптимізацію апаратного забезпечення на CPU, GPU та NPU. Глибока співпраця з постачальниками апаратного забезпечення (AMD, Intel, NVIDIA, Qualcomm) гарантує оптимальну продуктивність на корпоративних конфігураціях.

### Розширений досвід розробників

**Доступ через кілька інтерфейсів**: Foundry Local надає комплексні інтерфейси розробки, включаючи потужний CLI для управління моделями та розгортання, багатомовні SDK (Python, NodeJS) для нативної інтеграції та RESTful API з сумісністю OpenAI для безперешкодної міграції.

**Інтеграція з Visual Studio**: Платформа безперешкодно інтегрується з AI Toolkit для VS Code, надаючи інструменти для конвертації моделей, квантизації та оптимізації в середовищі розробки. Ця інтеграція прискорює робочі процеси розробки та знижує складність розгортання.

**Пайплайн оптимізації моделей**: Інтеграція Microsoft Olive дозволяє складні робочі процеси оптимізації моделей, включаючи динамічну квантизацію, оптимізацію графів і налаштування для конкретного апаратного забезпечення. Можливості хмарної конвертації через Azure ML забезпечують масштабовану оптимізацію для великих моделей.

### Стратегії впровадження у виробництво

**Встановлення та конфігурація**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Операції управління моделями**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Розширене налаштування розгортання**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Інтеграція в корпоративну екосистему

**Безпека та відповідність**: Foundry Local забезпечує функції корпоративного рівня безпеки, включаючи контроль доступу на основі ролей, ведення журналів аудиту, звітність про відповідність і зашифроване зберігання моделей. Інтеграція з інфраструктурою безпеки Microsoft гарантує дотримання корпоративних політик безпеки.

**Вбудовані AI-сервіси**: Платформа пропонує готові до використання AI-можливості, включаючи Phi Silica для локальної обробки мов, AI Imaging для покращення та аналізу зображень, а також спеціалізовані API для поширених корпоративних AI-завдань.

## Порівняльний аналіз: Ollama vs Foundry Local

### Порівняння технічної архітектури

| **Аспект** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Формат моделі** | GGUF (через llama.cpp) | ONNX (через ONNX Runtime) |
| **Фокус платформи** | Універсальна кросплатформність | Оптимізація для Windows/корпоративного середовища |
| **Інтеграція апаратного забезпечення** | Загальна підтримка GPU/CPU | Глибока інтеграція Windows ML, підтримка NPU |
| **Оптимізація** | Квантизація llama.cpp | Microsoft Olive + ONNX Runtime |
| **Корпоративні функції** | Орієнтовані на спільноту | Корпоративного рівня з SLA |

### Характеристики продуктивності

**Сильні сторони продуктивності Ollama**:
- Виняткова продуктивність CPU завдяки оптимізації llama.cpp.
- Стабільна поведінка на різних платформах і апаратному забезпеченні.
- Ефективне використання пам’яті з інтелектуальним завантаженням моделей.
- Швидкий запуск для сценаріїв розробки та тестування.

**Переваги продуктивності Foundry Local**:
- Висока продуктивність NPU на сучасному апаратному забезпеченні Windows.
- Оптимізоване прискорення GPU через партнерство з постачальниками.
- Моніторинг продуктивності корпоративного рівня та оптимізація.
- Масштабовані можливості розгортання для виробничих середовищ.

### Аналіз досвіду розробників

**Досвід розробників Ollama**:
- Мінімальні вимоги до налаштування для негайної продуктивності.
- Інтуїтивно зрозумілий інтерфейс командного рядка для всіх операцій.
- Широка підтримка спільноти та документація.
- Гнучке налаштування через Modelfiles.

**Досвід розробників Foundry Local**:
- Комплексна інтеграція IDE з екосистемою Visual Studio.
- Робочі процеси корпоративного рівня з функціями командної співпраці.
- Професійні канали підтримки з підтримкою Microsoft.
- Розширені інструменти налагодження та оптимізації.

### Оптимізація випадків використання

**Виберіть Ollama, якщо**:
- Розробляєте кросплатформні додатки, які потребують стабільної поведінки.
- Пріоритетом є прозорість відкритого коду та внески спільноти.
- Працюєте з обмеженими ресурсами або бюджетними обмеженнями.
- Створюєте експериментальні або дослідницькі додатки.
- Потрібна широка сумісність моделей для різних архітектур.

**Виберіть Foundry Local, якщо**:
- Розгортаєте корпоративні додатки з суворими вимогами до продуктивності.
- Використовуєте оптимізації апаратного забезпечення, специфічні для Windows (NPU, Windows ML).
- Потрібна корпоративна підтримка, SLA та функції відповідності.
- Створюєте виробничі додатки з інтеграцією в екосистему Microsoft.
- Потрібні розширені інструменти оптимізації та професійні робочі процеси розробки.

## Розширені стратегії розгортання

### Шаблони контейнеризованого розгортання

**Контейнеризація Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Корпоративне розгортання Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Техніки оптимізації продуктивності

**Стратегії оптимізації Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Оптимізація Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Міркування щодо безпеки та відповідності

### Реалізація корпоративної безпеки

**Найкращі практики безпеки Ollama**:
- Ізоляція мережі за допомогою правил брандмауера та доступу через VPN.
- Аутентифікація через інтеграцію з реверс-проксі.
- Перевірка цілісності моделей і безпечне розповсюдження моделей.
- Ведення журналів аудиту для доступу до API та операцій з моделями.

**Корпоративна безпека Foundry Local**:
- Вбудований контроль доступу на основі ролей з інтеграцією Active Directory.
- Комплексні журнали аудиту з звітністю про відповідність.
- Зашифроване зберігання моделей і безпечне розгортання моделей.
- Інтеграція з інфраструктурою безпеки Microsoft.

### Вимоги до відповідності та регулювання

Обидві платформи підтримують відповідність нормативним вимогам через:
- Контроль місцезнаходження даних, що забезпечує локальну обробку.
- Ведення журналів аудиту для вимог звітності.
- Контроль доступу для обробки конфіденційних даних.
- Шифрування даних у стані спокою та під час передачі для захисту даних.

## Найкращі практики для виробничого розгортання

### Моніторинг та спостереження

**Ключові метрики для моніторингу**:
- Затримка та пропускна здатність моделі.
- Використання ресурсів (CPU, GPU, пам’ять).
- Час відповіді API та рівень помилок.
- Точність моделі та дрейф продуктивності.

**Реалізація моніторингу**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Безперервна інтеграція та розгортання

**Інтеграція CI/CD пайплайну**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Майбутні тенденції та міркування

### Новітні технології

Ландшафт локального розгортання SLM продовжує розвиватися з кількома ключовими тенденціями:

**Розширені архітектури моделей**: З’являються моделі наступного покоління з покращеною ефективністю та співвідношенням можливостей, включаючи моделі з експертами для динамічного масштабування та спеціалізовані архітектури для розгортання на периферії.

**Інтеграція апаратного забезпечення**: Глибша інтеграція зі спеціалізованим AI

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.