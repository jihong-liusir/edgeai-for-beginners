<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-19T01:46:02+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "uk"
}
-->
# Контейнеризоване хмарне розгортання - рішення для масштабного виробництва

Цей детальний посібник охоплює три основні підходи до розгортання моделі Microsoft Phi-4-mini-instruct у контейнеризованих середовищах: vLLM, Ollama та SLM Engine з ONNX Runtime. Ця модель з 3.8 мільярдами параметрів є оптимальним вибором для задач логічного мислення, зберігаючи ефективність для розгортання на периферійних пристроях.

## Зміст

1. [Вступ до контейнерного розгортання Phi-4-mini](../../../Module03)
2. [Цілі навчання](../../../Module03)
3. [Розуміння класифікації Phi-4-mini](../../../Module03)
4. [Контейнерне розгортання vLLM](../../../Module03)
5. [Контейнерне розгортання Ollama](../../../Module03)
6. [SLM Engine з ONNX Runtime](../../../Module03)
7. [Порівняльна структура](../../../Module03)
8. [Найкращі практики](../../../Module03)

## Вступ до контейнерного розгортання Phi-4-mini

Малі мовні моделі (SLM) є важливим кроком вперед у EdgeAI, забезпечуючи складні можливості обробки природної мови на пристроях з обмеженими ресурсами. Цей посібник зосереджений на стратегіях контейнерного розгортання моделі Microsoft Phi-4-mini-instruct, сучасної моделі для задач логічного мислення, яка поєднує можливості та ефективність.

### Представлена модель: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 мільярда параметрів)**: новітня легка модель Microsoft, налаштована для інструкцій, розроблена для середовищ з обмеженими пам’яттю та обчислювальними ресурсами, з винятковими можливостями у:
- **Математичному мисленні та складних обчисленнях**
- **Генерації, налагодженні та аналізі коду**
- **Логічному вирішенні задач та покроковому мисленні**
- **Освітніх застосуваннях, що потребують детальних пояснень**
- **Виклику функцій та інтеграції інструментів**

Як частина категорії "Малі SLM" (1.5B - 13.9B параметрів), Phi-4-mini забезпечує оптимальний баланс між можливостями логічного мислення та ефективністю використання ресурсів.

### Переваги контейнерного розгортання Phi-4-mini

- **Операційна ефективність**: Швидке виконання задач логічного мислення з меншими вимогами до обчислювальних ресурсів
- **Гнучкість розгортання**: Можливості AI на пристрої з покращеною конфіденційністю через локальну обробку
- **Економічність**: Зниження операційних витрат порівняно з більшими моделями при збереженні якості
- **Ізоляція**: Чисте розділення між екземплярами моделі та безпечними середовищами виконання
- **Масштабованість**: Легке горизонтальне масштабування для збільшення пропускної здатності задач логічного мислення

## Цілі навчання

Після завершення цього посібника ви зможете:

- Розгорнути та оптимізувати Phi-4-mini-instruct у різних контейнеризованих середовищах
- Реалізувати передові стратегії квантування та стиснення для різних сценаріїв розгортання
- Налаштувати готову до виробництва оркестрацію контейнерів для задач логічного мислення
- Оцінити та вибрати відповідні фреймворки розгортання залежно від конкретних вимог до використання
- Застосувати найкращі практики безпеки, моніторингу та масштабування для контейнеризованих розгортань SLM

## Розуміння класифікації Phi-4-mini

### Специфікації моделі

**Технічні деталі:**
- **Параметри**: 3.8 мільярда (категорія Малих SLM)
- **Архітектура**: Щільний декодерний Transformer з групованою увагою до запитів
- **Довжина контексту**: 128K токенів (32K рекомендовано для оптимальної продуктивності)
- **Словник**: 200K токенів з багатомовною підтримкою
- **Навчальні дані**: 5T токенів високоякісного контенту, насиченого логічним мисленням

### Вимоги до ресурсів

| Тип розгортання | Мінімальна RAM | Рекомендована RAM | VRAM (GPU) | Зберігання | Типові випадки використання |
|-----------------|----------------|--------------------|------------|------------|-----------------------------|
| **Розробка** | 6GB | 8GB | - | 8GB | Локальне тестування, прототипування |
| **Виробництво CPU** | 8GB | 12GB | - | 10GB | Сервери на периферії, економічне розгортання |
| **Виробництво GPU** | 6GB | 8GB | 4-6GB | 8GB | Сервіси логічного мислення з високою пропускною здатністю |
| **Оптимізоване для периферії** | 4GB | 6GB | - | 6GB | Квантування, шлюзи IoT |

### Можливості Phi-4-mini

- **Математична досконалість**: Розв’язання задач з арифметики, алгебри та аналізу
- **Інтелектуальний код**: Генерація коду на Python, JavaScript та інших мовах з налагодженням
- **Логічне мислення**: Покрокова декомпозиція задач та побудова рішень
- **Освітня підтримка**: Детальні пояснення, придатні для навчання та викладання
- **Виклик функцій**: Нативна підтримка інтеграції інструментів та взаємодії з API

## Контейнерне розгортання vLLM

vLLM забезпечує чудову підтримку Phi-4-mini-instruct з оптимізованою продуктивністю виконання та сумісними API OpenAI, що робить його ідеальним для виробничих сервісів логічного мислення.

### Швидкий старт

#### Базове розгортання на CPU (Розробка)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Продуктивне розгортання з GPU
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Конфігурація для виробництва

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестування можливостей логічного мислення Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Контейнерне розгортання Ollama

Ollama забезпечує чудову підтримку Phi-4-mini-instruct зі спрощеним розгортанням та управлінням, що робить його ідеальним для розробки та збалансованих виробничих розгортань.

### Швидке налаштування

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Конфігурація для виробництва

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Оптимізація моделі та варіанти

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Приклади використання API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine з ONNX Runtime

ONNX Runtime забезпечує оптимальну продуктивність для периферійного розгортання Phi-4-mini-instruct з передовою оптимізацією та кросплатформною сумісністю.

### Базове налаштування

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Спрощена реалізація сервера

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Скрипт конвертації моделі

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Конфігурація для виробництва

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Тестування розгортання ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Порівняльна структура

### Порівняння фреймворків для Phi-4-mini

| Особливість | vLLM | Ollama | ONNX Runtime |
|-------------|------|--------|--------------|
| **Складність налаштування** | Середня | Легка | Складна |
| **Продуктивність (GPU)** | Відмінна (~25 ток/с) | Дуже хороша (~20 ток/с) | Хороша (~15 ток/с) |
| **Продуктивність (CPU)** | Хороша (~8 ток/с) | Дуже хороша (~12 ток/с) | Відмінна (~15 ток/с) |
| **Використання пам’яті** | 8-12GB | 6-10GB | 4-8GB |
| **Сумісність API** | Сумісний з OpenAI | Кастомний REST | Кастомний FastAPI |
| **Виклик функцій** | ✅ Нативний | ✅ Підтримується | ⚠️ Кастомна реалізація |
| **Підтримка квантування** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Квантування ONNX |
| **Готовність до виробництва** | ✅ Відмінна | ✅ Дуже хороша | ✅ Хороша |
| **Розгортання на периферії** | Хороша | Відмінна | Чудова |

## Додаткові ресурси

### Офіційна документація
- **Microsoft Phi-4 Model Card**: Детальні специфікації та рекомендації щодо використання
- **vLLM Documentation**: Розширені параметри конфігурації та оптимізації
- **Ollama Model Library**: Моделі спільноти та приклади кастомізації
- **ONNX Runtime Guides**: Стратегії оптимізації продуктивності та розгортання

### Інструменти розробки
- **Hugging Face Transformers**: Для взаємодії з моделями та кастомізації
- **OpenAI API Specification**: Для тестування сумісності з vLLM
- **Docker Best Practices**: Рекомендації щодо безпеки та оптимізації контейнерів
- **Kubernetes Deployment**: Шаблони оркестрації для масштабування у виробництві

### Ресурси для навчання
- **SLM Performance Benchmarking**: Методології порівняльного аналізу
- **Edge AI Deployment**: Найкращі практики для середовищ з обмеженими ресурсами
- **Reasoning Task Optimization**: Стратегії підказок для математичних та логічних задач
- **Container Security**: Практики захисту для розгортання AI моделей

## Результати навчання

Після завершення цього модуля ви зможете:

1. Розгорнути модель Phi-4-mini-instruct у контейнеризованих середовищах за допомогою різних фреймворків
2. Налаштувати та оптимізувати розгортання SLM для різних апаратних середовищ
3. Реалізувати найкращі практики безпеки для контейнеризованих AI розгортань
4. Порівняти та вибрати відповідні фреймворки розгортання залежно від конкретних вимог до використання
5. Застосувати стратегії моніторингу та масштабування для сервісів SLM виробничого рівня

## Що далі

- Повернутися до [Модуля 1](../Module01/README.md)
- Повернутися до [Модуля 2](../Module02/README.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.