<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-19T02:15:51+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "uk"
}
-->
# AI Toolkit для Visual Studio Code - Посібник з розробки Edge AI

## Вступ

Ласкаво просимо до детального посібника з використання AI Toolkit для Visual Studio Code у розробці Edge AI. У той час як штучний інтелект переходить від централізованих хмарних обчислень до розподілених пристроїв на краю мережі, розробникам потрібні потужні інтегровані інструменти, які можуть впоратися з унікальними викликами розгортання на краю — від обмежених ресурсів до вимог до роботи в автономному режимі.

AI Toolkit для Visual Studio Code заповнює цю прогалину, забезпечуючи повноцінне середовище розробки, спеціально створене для побудови, тестування та оптимізації AI-додатків, які ефективно працюють на пристроях краю. Незалежно від того, чи ви розробляєте для IoT-сенсорів, мобільних пристроїв, вбудованих систем або серверів на краю, цей набір інструментів спрощує весь ваш робочий процес розробки у знайомому середовищі VS Code.

Цей посібник проведе вас через основні концепції, інструменти та найкращі практики використання AI Toolkit у ваших проектах Edge AI — від вибору моделі до розгортання у виробництві.

## Огляд

AI Toolkit забезпечує інтегроване середовище розробки для повного життєвого циклу Edge AI-додатків у VS Code. Він пропонує безшовну інтеграцію з популярними AI-моделями від таких провайдерів, як OpenAI, Anthropic, Google і GitHub, а також підтримує локальне розгортання моделей через ONNX і Ollama — важливі функції для Edge AI-додатків, які потребують інференсу на пристрої.

Що відрізняє AI Toolkit для розробки Edge AI, так це його фокус на всьому процесі розгортання на краю. На відміну від традиційних інструментів розробки AI, які в основному орієнтовані на хмарне розгортання, AI Toolkit включає спеціалізовані функції для оптимізації моделей, тестування в умовах обмежених ресурсів і оцінки продуктивності, специфічної для краю. Набір інструментів враховує, що розробка Edge AI вимагає інших підходів — менших розмірів моделей, швидшого часу інференсу, автономної роботи та оптимізації для конкретного обладнання.

Платформа підтримує різні сценарії розгортання — від простого інференсу на пристрої до складних архітектур з кількома моделями на краю. Вона надає інструменти для конвертації, квантування та оптимізації моделей, які є необхідними для успішного розгортання на краю, зберігаючи продуктивність розробника, якою славиться VS Code.

## Навчальні цілі

До кінця цього посібника ви зможете:

### Основні компетенції
- **Встановити та налаштувати** AI Toolkit для Visual Studio Code для робочих процесів розробки Edge AI
- **Орієнтуватися та використовувати** інтерфейс AI Toolkit, включаючи Model Catalog, Playground і Agent Builder
- **Вибирати та оцінювати** AI-моделі, придатні для розгортання на краю, з урахуванням продуктивності та обмежень ресурсів
- **Конвертувати та оптимізувати** моделі у формат ONNX і використовувати техніки квантування для пристроїв краю

### Навички розробки Edge AI
- **Проектувати та впроваджувати** Edge AI-додатки за допомогою інтегрованого середовища розробки
- **Тестувати моделі** в умовах, схожих на край, використовуючи локальний інференс і моніторинг ресурсів
- **Створювати та налаштовувати** AI-агентів, оптимізованих для сценаріїв розгортання на краю
- **Оцінювати продуктивність моделей** за допомогою метрик, релевантних для обчислень на краю (затримка, використання пам’яті, точність)

### Оптимізація та розгортання
- **Застосовувати техніки квантування та обрізання** для зменшення розміру моделі при збереженні прийнятної продуктивності
- **Оптимізувати моделі** для конкретних апаратних платформ краю, включаючи прискорення на CPU, GPU і NPU
- **Впроваджувати найкращі практики** для розробки Edge AI, включаючи управління ресурсами та резервні стратегії
- **Готувати моделі та додатки** до розгортання у виробництві на пристроях краю

### Розширені концепції Edge AI
- **Інтегрувати з фреймворками Edge AI**, такими як ONNX Runtime, Windows ML і TensorFlow Lite
- **Реалізовувати архітектури з кількома моделями** та сценарії федеративного навчання для середовищ краю
- **Вирішувати поширені проблеми Edge AI**, включаючи обмеження пам’яті, швидкість інференсу та сумісність обладнання
- **Проектувати стратегії моніторингу та логування** для Edge AI-додатків у виробництві

### Практичне застосування
- **Створювати комплексні рішення Edge AI** від вибору моделі до розгортання
- **Демонструвати компетентність** у робочих процесах розробки та техніках оптимізації, специфічних для краю
- **Застосовувати отримані знання** до реальних сценаріїв використання Edge AI, включаючи IoT, мобільні та вбудовані додатки
- **Оцінювати та порівнювати** різні стратегії розгортання Edge AI та їх компроміси

## Основні функції для розробки Edge AI

### 1. Каталог моделей і пошук
- **Підтримка локальних моделей**: Знаходьте та отримуйте доступ до AI-моделей, спеціально оптимізованих для розгортання на краю
- **Інтеграція ONNX**: Доступ до моделей у форматі ONNX для ефективного інференсу на краю
- **Підтримка Ollama**: Використовуйте моделі, що працюють локально через Ollama, для забезпечення конфіденційності та роботи в автономному режимі
- **Порівняння моделей**: Порівнюйте моделі, щоб знайти оптимальний баланс між продуктивністю та споживанням ресурсів для пристроїв краю

### 2. Інтерактивний Playground
- **Локальне тестове середовище**: Тестуйте моделі локально перед розгортанням на краю
- **Мультимодальні експерименти**: Тестуйте зображення, текст та інші типові для краю входи
- **Налаштування параметрів**: Експериментуйте з різними параметрами моделі для оптимізації під обмеження краю
- **Моніторинг продуктивності в реальному часі**: Спостерігайте за швидкістю інференсу та використанням ресурсів під час розробки

### 3. Конструктор агентів для додатків на краю
- **Інженерія запитів**: Створюйте оптимізовані запити, які ефективно працюють з меншими моделями краю
- **Інтеграція MCP Tool**: Інтегруйте інструменти Model Context Protocol для розширених можливостей агентів краю
- **Генерація коду**: Генеруйте готовий до виробництва код, оптимізований для сценаріїв розгортання на краю
- **Структуровані виходи**: Проектуйте агентів, які забезпечують послідовні, структуровані відповіді, придатні для додатків на краю

### 4. Оцінка та тестування моделей
- **Метрики продуктивності**: Оцінюйте моделі за метриками, релевантними для розгортання на краю (затримка, використання пам’яті, точність)
- **Пакетне тестування**: Тестуйте кілька конфігурацій моделей одночасно, щоб знайти оптимальні налаштування для краю
- **Користувацька оцінка**: Створюйте критерії оцінки, специфічні для сценаріїв використання Edge AI
- **Профілювання ресурсів**: Аналізуйте вимоги до пам’яті та обчислень для планування розгортання на краю

### 5. Конвертація та оптимізація моделей
- **Конвертація в ONNX**: Конвертуйте моделі з різних форматів у ONNX для сумісності з краєм
- **Квантування**: Зменшуйте розмір моделі та покращуйте швидкість інференсу за допомогою технік квантування
- **Оптимізація обладнання**: Оптимізуйте моделі для конкретного обладнання краю (CPU, GPU, NPU)
- **Трансформація форматів**: Перетворюйте моделі з Hugging Face та інших джерел для розгортання на краю

### 6. Тонке налаштування для сценаріїв краю
- **Адаптація до домену**: Налаштовуйте моделі для конкретних сценаріїв використання та середовищ краю
- **Локальне навчання**: Навчайте моделі локально з підтримкою GPU для специфічних вимог краю
- **Інтеграція Azure**: Використовуйте Azure Container Apps для хмарного тонкого налаштування перед розгортанням на краю
- **Трансферне навчання**: Адаптуйте попередньо навчені моделі для завдань і обмежень, специфічних для краю

### 7. Моніторинг продуктивності та трасування
- **Аналіз продуктивності на краю**: Моніторинг продуктивності моделі в умовах, схожих на край
- **Збір трасування**: Збирайте детальні дані про продуктивність для оптимізації
- **Ідентифікація вузьких місць**: Виявляйте проблеми продуктивності перед розгортанням на пристроях краю
- **Відстеження використання ресурсів**: Моніторинг пам’яті, CPU та часу інференсу для оптимізації краю

## Робочий процес розробки Edge AI

### Фаза 1: Вибір і пошук моделі
1. **Дослідження каталогу моделей**: Використовуйте каталог моделей для пошуку моделей, придатних для розгортання на краю
2. **Порівняння продуктивності**: Оцінюйте моделі за розміром, точністю та швидкістю інференсу
3. **Локальне тестування**: Використовуйте моделі Ollama або ONNX для локального тестування перед розгортанням на краю
4. **Оцінка вимог до ресурсів**: Визначте потреби в пам’яті та обчисленнях для цільових пристроїв краю

### Фаза 2: Оптимізація моделі
1. **Конвертація в ONNX**: Конвертуйте вибрані моделі у формат ONNX для сумісності з краєм
2. **Застосування квантування**: Зменшуйте розмір моделі за допомогою квантування INT8 або INT4
3. **Оптимізація обладнання**: Оптимізуйте для цільового обладнання краю (ARM, x86, спеціалізовані прискорювачі)
4. **Валідація продуктивності**: Переконайтеся, що оптимізовані моделі зберігають прийнятну точність

### Фаза 3: Розробка додатків
1. **Проектування агентів**: Використовуйте Agent Builder для створення AI-агентів, оптимізованих для краю
2. **Інженерія запитів**: Розробляйте запити, які ефективно працюють з меншими моделями
3. **Інтеграційне тестування**: Тестуйте агентів у симульованих умовах краю
4. **Генерація коду**: Генеруйте код для виробництва, оптимізований для розгортання на краю

### Фаза 4: Оцінка та тестування
1. **Пакетна оцінка**: Тестуйте кілька конфігурацій для пошуку оптимальних налаштувань краю
2. **Профілювання продуктивності**: Аналізуйте швидкість інференсу, використання пам’яті та точність
3. **Симуляція краю**: Тестуйте в умовах, схожих на цільове середовище розгортання на краю
4. **Стрес-тестування**: Оцінюйте продуктивність за різних умов навантаження

### Фаза 5: Підготовка до розгортання
1. **Фінальна оптимізація**: Застосовуйте фінальні оптимізації на основі результатів тестування
2. **Пакування для розгортання**: Пакуйте моделі та код для розгортання на краю
3. **Документація**: Документуйте вимоги до розгортання та конфігурацію
4. **Налаштування моніторингу**: Готуйте моніторинг і логування для розгортання у виробництві

## Цільова аудиторія для розробки Edge AI

### Розробники Edge AI
- Розробники додатків, які створюють пристрої на краю з підтримкою AI та IoT-рішення
- Розробники вбудованих систем, які інтегрують AI-функції у пристрої з обмеженими ресурсами
- Мобільні розробники, які створюють AI-додатки для смартфонів і планшетів

### Інженери Edge AI
- Інженери AI, які оптимізують моделі для розгортання на краю та керують інференс-пайплайнами
- Інженери DevOps, які розгортають і керують AI-моделями у розподіленій інфраструктурі краю
- Інженери продуктивності, які оптимізують AI-навантаження для обмежень апаратного забезпечення краю

### Дослідники та викладачі
- Дослідники AI, які розробляють ефективні моделі та алгоритми для обчислень на краю
- Викладачі, які навчають концепціям Edge AI та демонструють техніки оптимізації
- Студенти, які вивчають виклики та рішення у розгортанні Edge AI

## Сценарії використання Edge AI

### Розумні IoT-пристрої
- **Реальний час розпізнавання зображень**: Розгортання моделей комп’ютерного зору на IoT-камерах і сенсорах
- **Обробка голосу**: Реалізація розпізнавання мови та обробки природної мови на розумних колонках
- **Прогнозне обслуговування**: Запуск моделей виявлення аномалій на промислових пристроях краю
- **Моніторинг навколишнього середовища**: Розгортання моделей аналізу даних сенсорів для екологічних додатків

### Мобільні та вбудовані додатки
- **Переклад на пристрої**: Реалізація моделей перекладу мов, які працюють в автономному режимі
- **Доповнена реальність**: Розгортання моделей розпізнавання та відстеження об’єктів у реальному часі для AR-додатків
- **Моніторинг здоров’я**: Запуск моделей аналізу здоров’я на носимих пристроях і медичному обладнанні
- **Автономні системи**: Реалізація моделей прийняття рішень для дронів, роботів і транспортних засобів

### Інфраструктура обчислень на краю
- **Центри даних на краю**: Розгортання AI-моделей у центрах даних на краю для додатків з низькою затримкою
- **Інтеграція CDN**: Інтеграція можливостей обробки AI у мережі доставки контенту
- **5G Edge**: Використання обчислень на краю 5G для додатків з підтримкою AI
- **Fog Computing**: Реалізація обробки AI у середовищах fog computing

## Встановлення та налаштування

### Швид
- **Безпека**: Реалізуйте відповідні заходи безпеки для застосунків Edge AI

## Інтеграція з фреймворками Edge AI

### ONNX Runtime
- **Кросплатформенне розгортання**: Розгортайте моделі ONNX на різних платформах Edge
- **Оптимізація обладнання**: Використовуйте апаратно-специфічні оптимізації ONNX Runtime
- **Підтримка мобільних пристроїв**: Використовуйте ONNX Runtime Mobile для застосунків на смартфонах і планшетах
- **Інтеграція з IoT**: Розгортайте на IoT-пристроях за допомогою легких дистрибутивів ONNX Runtime

### Windows ML
- **Пристрої Windows**: Оптимізуйте для пристроїв Edge на базі Windows і ПК
- **Прискорення NPU**: Використовуйте нейронні процесори на пристроях Windows
- **DirectML**: Використовуйте DirectML для прискорення GPU на платформах Windows
- **Інтеграція з UWP**: Інтегруйте з застосунками Universal Windows Platform

### TensorFlow Lite
- **Оптимізація для мобільних пристроїв**: Розгортайте моделі TensorFlow Lite на мобільних і вбудованих пристроях
- **Апаратні делегати**: Використовуйте спеціалізовані апаратні делегати для прискорення
- **Мікроконтролери**: Розгортайте на мікроконтролерах за допомогою TensorFlow Lite Micro
- **Кросплатформна підтримка**: Розгортайте на Android, iOS і вбудованих системах Linux

### Azure IoT Edge
- **Гібрид хмара-край**: Поєднуйте навчання в хмарі з інференсом на краю
- **Розгортання модулів**: Розгортайте AI-моделі як модулі IoT Edge
- **Управління пристроями**: Дистанційно керуйте пристроями Edge і оновленнями моделей
- **Телеметрія**: Збирайте дані про продуктивність і метрики моделей з розгортань на краю

## Розширені сценарії Edge AI

### Розгортання кількох моделей
- **Ансамблі моделей**: Розгортайте кілька моделей для покращення точності або резервування
- **A/B тестування**: Одночасно тестуйте різні моделі на пристроях Edge
- **Динамічний вибір**: Вибирайте моделі залежно від поточних умов пристрою
- **Спільне використання ресурсів**: Оптимізуйте використання ресурсів між кількома розгорнутими моделями

### Федеративне навчання
- **Розподілене навчання**: Навчайте моделі на кількох пристроях Edge
- **Збереження конфіденційності**: Залишайте дані навчання локальними, обмінюючись лише покращеннями моделей
- **Спільне навчання**: Дозволяйте пристроям навчатися на основі колективного досвіду
- **Координація Edge-хмара**: Координуйте навчання між пристроями Edge і хмарною інфраструктурою

### Обробка в реальному часі
- **Обробка потоків**: Обробляйте безперервні потоки даних на пристроях Edge
- **Інференс з низькою затримкою**: Оптимізуйте для мінімальної затримки інференсу
- **Пакетна обробка**: Ефективно обробляйте пакети даних на пристроях Edge
- **Адаптивна обробка**: Коригуйте обробку залежно від поточних можливостей пристрою

## Вирішення проблем розробки Edge AI

### Поширені проблеми
- **Обмеження пам'яті**: Модель занадто велика для пам'яті цільового пристрою
- **Швидкість інференсу**: Інференс моделі занадто повільний для вимог реального часу
- **Погіршення точності**: Оптимізація неприйнятно знижує точність моделі
- **Сумісність обладнання**: Модель несумісна з цільовим обладнанням

### Стратегії налагодження
- **Профілювання продуктивності**: Використовуйте функції трасування AI Toolkit для виявлення вузьких місць
- **Моніторинг ресурсів**: Моніторьте використання пам'яті та CPU під час розробки
- **Інкрементальне тестування**: Тестуйте оптимізації поступово, щоб ізолювати проблеми
- **Симуляція обладнання**: Використовуйте інструменти розробки для симуляції цільового обладнання

### Рішення для оптимізації
- **Додаткова квантизація**: Застосовуйте більш агресивні техніки квантизації
- **Архітектура моделі**: Розгляньте різні архітектури моделей, оптимізовані для Edge
- **Оптимізація попередньої обробки**: Оптимізуйте попередню обробку даних для обмежень Edge
- **Оптимізація інференсу**: Використовуйте апаратно-специфічні оптимізації інференсу

## Ресурси та наступні кроки

### Документація
- [AI Toolkit Models Guide](https://code.visualstudio.com/docs/intelligentapps/models)
- [Model Playground Documentation](https://code.visualstudio.com/docs/intelligentapps/playground)
- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Windows ML Documentation](https://docs.microsoft.com/en-us/windows/ai/)

### Спільнота та підтримка
- [VS Code AI Toolkit GitHub](https://github.com/microsoft/vscode-ai-toolkit)
- [ONNX Community](https://github.com/onnx/onnx)
- [Edge AI Developer Community](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [VS Code Extension Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### Навчальні ресурси
- [Курс основ Edge AI](./Module01/README.md)
- [Посібник з малих мовних моделей](./Module02/README.md)
- [Стратегії розгортання Edge](./Module03/README.md)
- [Розробка Edge AI для Windows](./windowdeveloper.md)

## Висновок

AI Toolkit для Visual Studio Code забезпечує комплексну платформу для розробки Edge AI, від пошуку та оптимізації моделей до їх розгортання та моніторингу. Використовуючи інтегровані інструменти та робочі процеси, розробники можуть ефективно створювати, тестувати та розгортати AI-застосунки, які працюють на пристроях з обмеженими ресурсами.

Підтримка ONNX, Ollama та різних хмарних провайдерів, у поєднанні з можливостями оптимізації та оцінки, робить цей інструментарій ідеальним вибором для розробки Edge AI. Незалежно від того, чи створюєте ви IoT-застосунки, функції мобільного AI або вбудовані інтелектуальні системи, AI Toolkit надає необхідні інструменти та робочі процеси для успішного розгортання Edge AI.

Оскільки Edge AI продовжує розвиватися, AI Toolkit для VS Code залишається на передовій, забезпечуючи розробників найсучаснішими інструментами та можливостями для створення наступного покоління інтелектуальних застосунків на краю.

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.