<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T15:44:58+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "hi"
}
-->
# अनुभाग 2 : Llama.cpp कार्यान्वयन गाइड

## सामग्री तालिका
1. [परिचय](../../../Module04)
2. [Llama.cpp क्या है?](../../../Module04)
3. [इंस्टॉलेशन](../../../Module04)
4. [सोर्स से निर्माण](../../../Module04)
5. [मॉडल क्वांटाइजेशन](../../../Module04)
6. [मूल उपयोग](../../../Module04)
7. [उन्नत सुविधाएँ](../../../Module04)
8. [Python एकीकरण](../../../Module04)
9. [समस्या निवारण](../../../Module04)
10. [सर्वोत्तम प्रथाएँ](../../../Module04)

## परिचय

यह व्यापक ट्यूटोरियल आपको Llama.cpp के बारे में सब कुछ सिखाएगा, बुनियादी इंस्टॉलेशन से लेकर उन्नत उपयोग परिदृश्यों तक। Llama.cpp एक शक्तिशाली C++ कार्यान्वयन है जो न्यूनतम सेटअप और विभिन्न हार्डवेयर कॉन्फ़िगरेशन पर उत्कृष्ट प्रदर्शन के साथ बड़े भाषा मॉडल (LLMs) का कुशल अनुमान सक्षम करता है।

## Llama.cpp क्या है?

Llama.cpp एक LLM अनुमान फ्रेमवर्क है जो C/C++ में लिखा गया है और बड़े भाषा मॉडल को स्थानीय रूप से न्यूनतम सेटअप और अत्याधुनिक प्रदर्शन के साथ चलाने में सक्षम बनाता है। इसकी मुख्य विशेषताएँ हैं:

### मुख्य विशेषताएँ
- **साधारण C/C++ कार्यान्वयन** बिना किसी निर्भरता के
- **क्रॉस-प्लेटफ़ॉर्म संगतता** (Windows, macOS, Linux)
- **विभिन्न आर्किटेक्चर के लिए हार्डवेयर अनुकूलन**
- **क्वांटाइजेशन समर्थन** (1.5-बिट से 8-बिट इंटिजर क्वांटाइजेशन)
- **CPU और GPU त्वरण** समर्थन
- **स्मृति दक्षता** सीमित वातावरण के लिए

### लाभ
- CPU पर कुशलता से चलता है, विशेष हार्डवेयर की आवश्यकता नहीं होती
- कई GPU बैकएंड का समर्थन करता है (CUDA, Metal, OpenCL, Vulkan)
- हल्का और पोर्टेबल
- Apple सिलिकॉन को प्राथमिकता दी गई है - ARM NEON, Accelerate और Metal फ्रेमवर्क के माध्यम से अनुकूलित
- कम स्मृति उपयोग के लिए विभिन्न क्वांटाइजेशन स्तरों का समर्थन करता है

## इंस्टॉलेशन

### विधि 1: प्री-बिल्ट बायनरी (शुरुआती लोगों के लिए अनुशंसित)

#### GitHub रिलीज़ से डाउनलोड करें
1. [Llama.cpp GitHub रिलीज़](https://github.com/ggml-org/llama.cpp/releases) पर जाएँ
2. अपने सिस्टम के लिए उपयुक्त बायनरी डाउनलोड करें:
   - Windows के लिए `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS के लिए `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux के लिए `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. आर्काइव को निकालें और अपने सिस्टम के PATH में डायरेक्टरी जोड़ें

#### पैकेज मैनेजर का उपयोग करना

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (विभिन्न वितरण):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### विधि 2: Python पैकेज (llama-cpp-python)

#### बुनियादी इंस्टॉलेशन
```bash
pip install llama-cpp-python
```

#### हार्डवेयर त्वरण के साथ
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## सोर्स से निर्माण

### आवश्यकताएँ

**सिस्टम आवश्यकताएँ:**
- C++ कंपाइलर (GCC, Clang, या MSVC)
- CMake (संस्करण 3.14 या उच्चतर)
- Git
- आपके प्लेटफ़ॉर्म के लिए बिल्ड टूल्स

**आवश्यकताओं को स्थापित करना:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022 को C++ विकास टूल्स के साथ इंस्टॉल करें
- आधिकारिक वेबसाइट से CMake इंस्टॉल करें
- Git इंस्टॉल करें

### बुनियादी निर्माण प्रक्रिया

1. **रिपॉजिटरी क्लोन करें:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **बिल्ड को कॉन्फ़िगर करें:**
```bash
cmake -B build
```

3. **प्रोजेक्ट को बिल्ड करें:**
```bash
cmake --build build --config Release
```

तेज़ संकलन के लिए, समानांतर जॉब्स का उपयोग करें:
```bash
cmake --build build --config Release -j 8
```

### हार्डवेयर-विशिष्ट निर्माण

#### CUDA समर्थन (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal समर्थन (Apple सिलिकॉन)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS समर्थन (CPU अनुकूलन)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan समर्थन
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### उन्नत निर्माण विकल्प

#### डिबग बिल्ड
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### अतिरिक्त सुविधाओं के साथ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## मॉडल क्वांटाइजेशन

### GGUF प्रारूप को समझना

GGUF (Generalized GGML Unified Format) एक अनुकूलित फ़ाइल प्रारूप है जिसे Llama.cpp और अन्य फ्रेमवर्क का उपयोग करके बड़े भाषा मॉडल को कुशलतापूर्वक चलाने के लिए डिज़ाइन किया गया है। यह प्रदान करता है:

- मानकीकृत मॉडल वेट स्टोरेज
- प्लेटफ़ॉर्म के बीच बेहतर संगतता
- उन्नत प्रदर्शन
- कुशल मेटाडेटा हैंडलिंग

### क्वांटाइजेशन प्रकार

Llama.cpp विभिन्न क्वांटाइजेशन स्तरों का समर्थन करता है:

| प्रकार | बिट्स | विवरण | उपयोग का मामला |
|-------|-------|--------|----------------|
| F16 | 16 | हाफ प्रिसिजन | उच्च गुणवत्ता, बड़ी स्मृति |
| Q8_0 | 8 | 8-बिट क्वांटाइजेशन | अच्छा संतुलन |
| Q4_0 | 4 | 4-बिट क्वांटाइजेशन | मध्यम गुणवत्ता, छोटा आकार |
| Q2_K | 2 | 2-बिट क्वांटाइजेशन | सबसे छोटा आकार, कम गुणवत्ता |

### मॉडल को परिवर्तित करना

#### PyTorch से GGUF में
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face से सीधे डाउनलोड करें
कई मॉडल GGUF प्रारूप में Hugging Face पर उपलब्ध हैं:
- "GGUF" नाम वाले मॉडल खोजें
- उपयुक्त क्वांटाइजेशन स्तर डाउनलोड करें
- Llama.cpp के साथ सीधे उपयोग करें

## मूल उपयोग

### कमांड लाइन इंटरफ़ेस

#### सरल टेक्स्ट जनरेशन
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face से मॉडल का उपयोग करना
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### सर्वर मोड
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### सामान्य पैरामीटर

| पैरामीटर | विवरण | उदाहरण |
|----------|--------|--------|
| `-m` | मॉडल फ़ाइल पथ | `-m model.gguf` |
| `-p` | प्रॉम्प्ट टेक्स्ट | `-p "Hello world"` |
| `-n` | उत्पन्न करने के लिए टोकन की संख्या | `-n 100` |
| `-c` | संदर्भ आकार | `-c 4096` |
| `-t` | थ्रेड्स की संख्या | `-t 8` |
| `-ngl` | GPU लेयर्स | `-ngl 32` |
| `-temp` | तापमान | `-temp 0.7` |

### इंटरैक्टिव मोड

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## उन्नत सुविधाएँ

### सर्वर API

#### सर्वर शुरू करना
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API उपयोग
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### प्रदर्शन अनुकूलन

#### स्मृति प्रबंधन
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### मल्टी-थ्रेडिंग
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU त्वरण
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python एकीकरण

### llama-cpp-python के साथ बुनियादी उपयोग

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### चैट इंटरफ़ेस

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### स्ट्रीमिंग प्रतिक्रियाएँ

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain के साथ एकीकरण

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## समस्या निवारण

### सामान्य समस्याएँ और समाधान

#### निर्माण त्रुटियाँ

**समस्या: CMake नहीं मिला**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**समस्या: कंपाइलर नहीं मिला**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### रनटाइम समस्याएँ

**समस्या: मॉडल लोडिंग विफल**
- मॉडल फ़ाइल पथ सत्यापित करें
- फ़ाइल अनुमतियाँ जांचें
- पर्याप्त RAM सुनिश्चित करें
- विभिन्न क्वांटाइजेशन स्तरों को आज़माएँ

**समस्या: खराब प्रदर्शन**
- हार्डवेयर त्वरण सक्षम करें
- थ्रेड्स की संख्या बढ़ाएँ
- उपयुक्त क्वांटाइजेशन का उपयोग करें
- GPU मेमोरी उपयोग जांचें

#### स्मृति समस्याएँ

**समस्या: मेमोरी समाप्त**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### प्लेटफ़ॉर्म-विशिष्ट समस्याएँ

#### Windows
- MinGW या Visual Studio कंपाइलर का उपयोग करें
- उचित PATH कॉन्फ़िगरेशन सुनिश्चित करें
- एंटीवायरस हस्तक्षेप की जांच करें

#### macOS
- Apple सिलिकॉन के लिए Metal सक्षम करें
- यदि आवश्यक हो तो संगतता के लिए Rosetta 2 का उपयोग करें
- Xcode कमांड लाइन टूल्स जांचें

#### Linux
- विकास पैकेज इंस्टॉल करें
- GPU ड्राइवर संस्करण जांचें
- CUDA टूलकिट इंस्टॉलेशन सत्यापित करें

## सर्वोत्तम प्रथाएँ

### मॉडल चयन
1. **अपने हार्डवेयर के आधार पर उपयुक्त क्वांटाइजेशन चुनें**
2. **मॉडल आकार बनाम गुणवत्ता समझौता पर विचार करें**
3. **अपने विशिष्ट उपयोग के मामले के लिए विभिन्न मॉडलों का परीक्षण करें**

### प्रदर्शन अनुकूलन
1. **GPU त्वरण का उपयोग करें** जब उपलब्ध हो
2. **अपने CPU के लिए थ्रेड्स की संख्या अनुकूलित करें**
3. **अपने उपयोग के मामले के लिए उपयुक्त संदर्भ आकार सेट करें**
4. **बड़े मॉडलों के लिए मेमोरी मैपिंग सक्षम करें**

### उत्पादन परिनियोजन
1. **API एक्सेस के लिए सर्वर मोड का उपयोग करें**
2. **उचित त्रुटि हैंडलिंग लागू करें**
3. **संसाधन उपयोग की निगरानी करें**
4. **लॉगिंग और मॉनिटरिंग सेट करें**

### विकास कार्यप्रवाह
1. **परीक्षण के लिए छोटे मॉडलों से शुरू करें**
2. **मॉडल कॉन्फ़िगरेशन के लिए संस्करण नियंत्रण का उपयोग करें**
3. **अपनी कॉन्फ़िगरेशन का दस्तावेज़ीकरण करें**
4. **विभिन्न प्लेटफ़ॉर्म पर परीक्षण करें**

### सुरक्षा विचार
1. **इनपुट प्रॉम्प्ट्स को मान्य करें**
2. **रेट लिमिटिंग लागू करें**
3. **API एंडपॉइंट्स को सुरक्षित करें**
4. **दुरुपयोग पैटर्न की निगरानी करें**

## निष्कर्ष

Llama.cpp विभिन्न हार्डवेयर कॉन्फ़िगरेशन पर बड़े भाषा मॉडल को स्थानीय रूप से चलाने का एक शक्तिशाली और कुशल तरीका प्रदान करता है। चाहे आप AI एप्लिकेशन विकसित कर रहे हों, शोध कर रहे हों, या LLMs के साथ प्रयोग कर रहे हों, यह फ्रेमवर्क आपके उपयोग के मामलों की विस्तृत श्रृंखला के लिए आवश्यक लचीलापन और प्रदर्शन प्रदान करता है।

मुख्य बातें:
- अपनी आवश्यकताओं के अनुसार इंस्टॉलेशन विधि चुनें
- अपने विशिष्ट हार्डवेयर कॉन्फ़िगरेशन के लिए अनुकूलित करें
- बुनियादी उपयोग से शुरू करें और धीरे-धीरे उन्नत सुविधाओं का अन्वेषण करें
- आसान एकीकरण के लिए Python बाइंडिंग का उपयोग करने पर विचार करें
- उत्पादन परिनियोजन के लिए सर्वोत्तम प्रथाओं का पालन करें

अधिक जानकारी और अपडेट के लिए, [आधिकारिक Llama.cpp रिपॉजिटरी](https://github.com/ggml-org/llama.cpp) पर जाएँ और उपलब्ध व्यापक दस्तावेज़ और सामुदायिक संसाधनों का संदर्भ लें।

## ➡️ आगे क्या करें

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।