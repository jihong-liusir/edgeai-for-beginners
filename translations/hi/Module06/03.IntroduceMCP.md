<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T15:21:04+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "hi"
}
-->
# अनुभाग 03 - मॉडल संदर्भ प्रोटोकॉल (MCP) एकीकरण

## MCP (मॉडल संदर्भ प्रोटोकॉल) का परिचय

मॉडल संदर्भ प्रोटोकॉल (MCP) एक क्रांतिकारी ढांचा है जो भाषा मॉडलों को बाहरी उपकरणों और प्रणालियों के साथ एक मानकीकृत तरीके से संवाद करने की अनुमति देता है। पारंपरिक तरीकों के विपरीत, जहां मॉडल अलग-थलग होते हैं, MCP एक सुव्यवस्थित प्रोटोकॉल के माध्यम से AI मॉडल और वास्तविक दुनिया के बीच पुल बनाता है।

### MCP क्या है?

MCP एक संचार प्रोटोकॉल के रूप में कार्य करता है जो भाषा मॉडलों को सक्षम बनाता है:
- बाहरी डेटा स्रोतों से जुड़ने के लिए
- उपकरण और कार्यों को निष्पादित करने के लिए
- APIs और सेवाओं के साथ संवाद करने के लिए
- वास्तविक समय की जानकारी तक पहुंचने के लिए
- जटिल बहु-चरणीय संचालन करने के लिए

यह प्रोटोकॉल स्थिर भाषा मॉडलों को गतिशील एजेंटों में बदल देता है, जो केवल टेक्स्ट जनरेशन से परे व्यावहारिक कार्य कर सकते हैं।

## छोटे भाषा मॉडल (SLMs) में MCP

छोटे भाषा मॉडल AI तैनाती के लिए एक कुशल दृष्टिकोण का प्रतिनिधित्व करते हैं, जो कई लाभ प्रदान करते हैं:

### SLMs के लाभ
- **संसाधन दक्षता**: कम कंप्यूटेशनल आवश्यकताएं  
- **तेज प्रतिक्रिया समय**: वास्तविक समय अनुप्रयोगों के लिए कम विलंबता  
- **लागत प्रभावशीलता**: न्यूनतम बुनियादी ढांचे की आवश्यकता  
- **गोपनीयता**: स्थानीय रूप से चल सकते हैं, बिना डेटा ट्रांसमिशन के  
- **अनुकूलन**: विशिष्ट क्षेत्रों के लिए आसानी से ट्यून किए जा सकते हैं  

### MCP के साथ SLMs क्यों प्रभावी हैं

SLMs और MCP का संयोजन एक शक्तिशाली समाधान बनाता है, जहां मॉडल की तर्क क्षमता बाहरी उपकरणों द्वारा बढ़ाई जाती है, उनके छोटे पैरामीटर काउंट को उन्नत कार्यक्षमता के माध्यम से संतुलित किया जाता है।

## Python MCP SDK का अवलोकन

Python MCP SDK MCP-सक्षम अनुप्रयोगों के निर्माण के लिए आधार प्रदान करता है। SDK में शामिल हैं:

- **क्लाइंट लाइब्रेरी**: MCP सर्वरों से जुड़ने के लिए  
- **सर्वर फ्रेमवर्क**: कस्टम MCP सर्वर बनाने के लिए  
- **प्रोटोकॉल हैंडलर**: संचार प्रबंधन के लिए  
- **टूल इंटीग्रेशन**: बाहरी कार्यों को निष्पादित करने के लिए  

## व्यावहारिक कार्यान्वयन: Phi-4 MCP क्लाइंट

Microsoft के Phi-4 मिनी मॉडल का उपयोग करके MCP क्षमताओं के साथ एक वास्तविक-जीवन कार्यान्वयन का अन्वेषण करें।

### सिस्टम आर्किटेक्चर

इस कार्यान्वयन में एक स्तरित आर्किटेक्चर का पालन किया गया है:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### मुख्य घटक

#### 1. MCP क्लाइंट क्लासेस

**BaseMCPClient**: सामान्य कार्यक्षमता प्रदान करने वाला अमूर्त आधार  
- Async संदर्भ प्रबंधक प्रोटोकॉल  
- मानक इंटरफ़ेस परिभाषा  
- संसाधन प्रबंधन  

**Phi4MiniMCPClient**: STDIO-आधारित कार्यान्वयन  
- स्थानीय प्रक्रिया संचार  
- मानक इनपुट/आउटपुट हैंडलिंग  
- सबप्रोसेस प्रबंधन  

**Phi4MiniSSEMCPClient**: सर्वर-सेंट इवेंट्स कार्यान्वयन  
- HTTP स्ट्रीमिंग संचार  
- वास्तविक समय इवेंट हैंडलिंग  
- वेब-आधारित सर्वर कनेक्टिविटी  

#### 2. LLM इंटीग्रेशन

**OllamaClient**: स्थानीय मॉडल होस्टिंग  
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: उच्च-प्रदर्शन सर्विंग  
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. टूल प्रोसेसिंग पाइपलाइन

टूल प्रोसेसिंग पाइपलाइन MCP टूल्स को भाषा मॉडलों के साथ संगत स्वरूपों में बदलती है:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## शुरुआत करें: चरण-दर-चरण मार्गदर्शिका

### चरण 1: पर्यावरण सेटअप

आवश्यक निर्भरताएं स्थापित करें:  
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### चरण 2: बुनियादी कॉन्फ़िगरेशन

अपने पर्यावरण वेरिएबल्स सेट करें:  
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### चरण 3: अपना पहला MCP क्लाइंट चलाना

**बुनियादी Ollama सेटअप:**  
```bash
python ghmodel_mcp_demo.py
```

**vLLM बैकएंड का उपयोग:**  
```bash
python ghmodel_mcp_demo.py --env vllm
```

**सर्वर-सेंट इवेंट्स कनेक्शन:**  
```bash
python ghmodel_mcp_demo.py --run sse
```

**कस्टम MCP सर्वर:**  
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### चरण 4: प्रोग्रामेटिक उपयोग

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## उन्नत सुविधाएं

### मल्टी-बैकएंड समर्थन

यह कार्यान्वयन Ollama और vLLM दोनों बैकएंड का समर्थन करता है, जिससे आप अपनी आवश्यकताओं के अनुसार चयन कर सकते हैं:

- **Ollama**: स्थानीय विकास और परीक्षण के लिए बेहतर  
- **vLLM**: उत्पादन और उच्च-थ्रूपुट परिदृश्यों के लिए अनुकूलित  

### लचीले कनेक्शन प्रोटोकॉल

दो कनेक्शन मोड समर्थित हैं:

**STDIO मोड**: डायरेक्ट प्रक्रिया संचार  
- कम विलंबता  
- स्थानीय उपकरणों के लिए उपयुक्त  
- सरल सेटअप  

**SSE मोड**: HTTP-आधारित स्ट्रीमिंग  
- नेटवर्क-सक्षम  
- वितरित प्रणालियों के लिए बेहतर  
- वास्तविक समय अपडेट  

### टूल इंटीग्रेशन क्षमताएं

सिस्टम विभिन्न उपकरणों के साथ एकीकृत हो सकता है:  
- वेब ऑटोमेशन (Playwright)  
- फ़ाइल संचालन  
- API इंटरैक्शन  
- सिस्टम कमांड  
- कस्टम कार्य  

## त्रुटि प्रबंधन और सर्वोत्तम प्रथाएं

### व्यापक त्रुटि प्रबंधन

इस कार्यान्वयन में मजबूत त्रुटि प्रबंधन शामिल है:

**कनेक्शन त्रुटियां:**  
- MCP सर्वर विफलताएं  
- नेटवर्क टाइमआउट  
- कनेक्टिविटी समस्याएं  

**टूल निष्पादन त्रुटियां:**  
- गायब टूल्स  
- पैरामीटर सत्यापन  
- निष्पादन विफलताएं  

**प्रतिक्रिया प्रसंस्करण त्रुटियां:**  
- JSON पार्सिंग समस्याएं  
- स्वरूप असंगतियां  
- LLM प्रतिक्रिया विसंगतियां  

### सर्वोत्तम प्रथाएं

1. **संसाधन प्रबंधन**: Async संदर्भ प्रबंधकों का उपयोग करें  
2. **त्रुटि प्रबंधन**: व्यापक try-catch ब्लॉक लागू करें  
3. **लॉगिंग**: उपयुक्त लॉगिंग स्तर सक्षम करें  
4. **सुरक्षा**: इनपुट को मान्य करें और आउटपुट को स्वच्छ करें  
5. **प्रदर्शन**: कनेक्शन पूलिंग और कैशिंग का उपयोग करें  

## वास्तविक-जीवन अनुप्रयोग

### वेब ऑटोमेशन  
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### डेटा प्रसंस्करण  
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API इंटीग्रेशन  
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## प्रदर्शन अनुकूलन

### मेमोरी प्रबंधन  
- कुशल संदेश इतिहास हैंडलिंग  
- उचित संसाधन सफाई  
- कनेक्शन पूलिंग  

### नेटवर्क अनुकूलन  
- Async HTTP संचालन  
- कॉन्फ़िगर करने योग्य टाइमआउट  
- ग्रेसफुल त्रुटि रिकवरी  

### समवर्ती प्रसंस्करण  
- गैर-अवरोधक I/O  
- समानांतर टूल निष्पादन  
- कुशल async पैटर्न  

## सुरक्षा विचार

### डेटा सुरक्षा  
- सुरक्षित API कुंजी प्रबंधन  
- इनपुट सत्यापन  
- आउटपुट स्वच्छता  

### नेटवर्क सुरक्षा  
- HTTPS समर्थन  
- स्थानीय एंडपॉइंट डिफॉल्ट्स  
- सुरक्षित टोकन हैंडलिंग  

### निष्पादन सुरक्षा  
- टूल फ़िल्टरिंग  
- सैंडबॉक्स वातावरण  
- ऑडिट लॉगिंग  

## निष्कर्ष

MCP के साथ एकीकृत SLMs AI अनुप्रयोग विकास में एक नया दृष्टिकोण प्रस्तुत करते हैं। छोटे मॉडलों की दक्षता को बाहरी उपकरणों की शक्ति के साथ जोड़कर, डेवलपर्स संसाधन-कुशल और अत्यधिक सक्षम बुद्धिमान प्रणालियां बना सकते हैं।

Phi-4 MCP क्लाइंट कार्यान्वयन दिखाता है कि इस एकीकरण को व्यावहारिक रूप से कैसे प्राप्त किया जा सकता है, और यह परिष्कृत AI-संचालित अनुप्रयोगों के निर्माण के लिए एक ठोस आधार प्रदान करता है।

मुख्य बातें:
- MCP भाषा मॉडलों और बाहरी प्रणालियों के बीच की खाई को पाटता है  
- SLMs उपकरणों द्वारा संवर्धित होने पर दक्षता के साथ क्षमता प्रदान करते हैं  
- मॉड्यूलर आर्किटेक्चर आसान विस्तार और अनुकूलन को सक्षम बनाता है  
- उत्पादन उपयोग के लिए उचित त्रुटि प्रबंधन और सुरक्षा उपाय आवश्यक हैं  

यह ट्यूटोरियल आपके स्वयं के SLM-संचालित MCP अनुप्रयोगों के निर्माण के लिए आधार प्रदान करता है, जो स्वचालन, डेटा प्रसंस्करण और बुद्धिमान प्रणाली एकीकरण के लिए संभावनाओं को खोलता है।

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।