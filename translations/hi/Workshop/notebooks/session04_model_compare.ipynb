{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# рд╕рддреНрд░ 4 тАУ SLM рдмрдирд╛рдо LLM рддреБрд▓рдирд╛\n",
    "\n",
    "рдПрдХ рдЫреЛрдЯреЗ рднрд╛рд╖рд╛ рдореЙрдбрд▓ рдФрд░ Foundry Local рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдЪрд▓рдиреЗ рд╡рд╛рд▓реЗ рдмрдбрд╝реЗ рдореЙрдбрд▓ рдХреЗ рдмреАрдЪ рд╡рд┐рд▓рдВрдмрддрд╛ рдФрд░ рдирдореВрдирд╛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдЧреБрдгрд╡рддреНрддрд╛ рдХреА рддреБрд▓рдирд╛ рдХрд░реЗрдВред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## тЪб рддреНрд╡рд░рд┐рдд рд╢реБрд░реБрдЖрдд\n",
    "\n",
    "**рдореЗрдореЛрд░реА-рдЕрдиреБрдХреВрд▓рд┐рдд рд╕реЗрдЯрдЕрдк (рдЕрдкрдбреЗрдЯреЗрдб):**\n",
    "1. рдореЙрдбрд▓реНрд╕ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ CPU рд╡реЗрд░рд┐рдПрдВрдЯ рдЪреБрдирддреЗ рд╣реИрдВ (рдХрд┐рд╕реА рднреА рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдкрд░ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ)\n",
    "2. `qwen2.5-3b` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ рдмрдЬрд╛рдп 7B рдХреЗ (рд▓рдЧрднрдЧ 4GB RAM рдмрдЪрд╛рддрд╛ рд╣реИ)\n",
    "3. рдкреЛрд░реНрдЯ рдХрд╛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдкрддрд╛ рд▓рдЧрд╛рдирд╛ (рдХреЛрдИ рдореИрдиреБрдЕрд▓ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдирд╣реАрдВ)\n",
    "4. рдХреБрд▓ рдЖрд╡рд╢реНрдпрдХ RAM: ~8GB рдЕрдиреБрд╢рдВрд╕рд┐рдд (рдореЙрдбрд▓реНрд╕ + OS)\n",
    "\n",
    "**рдЯрд░реНрдорд┐рдирд▓ рд╕реЗрдЯрдЕрдк (30 рд╕реЗрдХрдВрдб):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "рдЗрд╕рдХреЗ рдмрд╛рдж рдЗрд╕ рдиреЛрдЯрдмреБрдХ рдХреЛ рдЪрд▓рд╛рдПрдВ! ЁЯЪА\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдирд┐рд░реНрднрд░рддрд╛ рд╕реНрдерд╛рдкрдирд╛\n",
    "рдиреНрдпреВрдирддрдо рдкреИрдХреЗрдЬ (`foundry-local-sdk`, `openai`, `numpy`) рд╕реНрдерд╛рдкрд┐рдд рдХрд░рддрд╛ рд╣реИ рдЬреЛ рд╕рдордп рдирд┐рд░реНрдзрд╛рд░рдг рдФрд░ рдЪреИрдЯ рдЕрдиреБрд░реЛрдзреЛрдВ рдХреЗ рд▓рд┐рдП рдЖрд╡рд╢реНрдпрдХ рд╣реИрдВред рдЗрд╕реЗ рд╕реБрд░рдХреНрд╖рд┐рдд рд░реВрдк рд╕реЗ рдмрд╛рд░-рдмрд╛рд░ рдЪрд▓рд╛рдпрд╛ рдЬрд╛ рд╕рдХрддрд╛ рд╣реИред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# рдкрд░рд┐рджреГрд╢реНрдп\n",
    "рдПрдХ рдЫреЛрдЯреЗ рднрд╛рд╖рд╛ рдореЙрдбрд▓ (SLM) рдХреА рддреБрд▓рдирд╛ рдПрдХ рдмрдбрд╝реЗ рдореЙрдбрд▓ рд╕реЗ рдПрдХ рд╣реА рдкреНрд░реЙрдореНрдкреНрдЯ рдкрд░ рдХрд░реЗрдВ рддрд╛рдХрд┐ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд┐рдпрд╛ рдЬрд╛ рд╕рдХреЗ:\n",
    "- **рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рд╕рдордп рдХрд╛ рдЕрдВрддрд░** (рджреАрд╡рд╛рд░ рдШрдбрд╝реА рд╕реЗрдХрдВрдб рдореЗрдВ)\n",
    "- **рдЯреЛрдХрди рдЙрдкрдпреЛрдЧ** (рдпрджрд┐ рдЙрдкрд▓рдмреНрдз рд╣реЛ) рдереНрд░реВрдкреБрдЯ рдХреЗ рд▓рд┐рдП рдПрдХ рд╕рдВрдХреЗрддрдХ рдХреЗ рд░реВрдк рдореЗрдВ\n",
    "- **рдЧреБрдгрд╛рддреНрдордХ рдЖрдЙрдЯрдкреБрдЯ рдХрд╛ рдирдореВрдирд╛** рддреНрд╡рд░рд┐рдд рдореВрд▓реНрдпрд╛рдВрдХрди рдХреЗ рд▓рд┐рдП\n",
    "- **рдЧрддрд┐ рд╡реГрджреНрдзрд┐ рдХреА рдЧрдгрдирд╛** рдкреНрд░рджрд░реНрд╢рди рд▓рд╛рдн рдХреЛ рдорд╛рдкрдиреЗ рдХреЗ рд▓рд┐рдП\n",
    "\n",
    "**рдкрд░реНрдпрд╛рд╡рд░рдгреАрдп рд╡реЗрд░рд┐рдПрдмрд▓реНрд╕:**\n",
    "- `SLM_ALIAS` - рдЫреЛрдЯрд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓ (рдбрд┐рдлрд╝реЙрд▓реНрдЯ: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - рдмрдбрд╝рд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓ (рдбрд┐рдлрд╝реЙрд▓реНрдЯ: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - рддреБрд▓рдирд╛ рдХреЗ рд▓рд┐рдП рдкрд░реАрдХреНрд╖рдг рдкреНрд░реЙрдореНрдкреНрдЯ\n",
    "- `COMPARE_RETRIES` - рд▓рдЪреАрд▓рд╛рдкрди рдХреЗ рд▓рд┐рдП рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ (рдбрд┐рдлрд╝реЙрд▓реНрдЯ: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - рд╕реЗрд╡рд╛ рдПрдВрдбрдкреЙрдЗрдВрдЯ рдХреЛ рдУрд╡рд░рд░рд╛рдЗрдб рдХрд░реЗрдВ (рдпрджрд┐ рд╕реЗрдЯ рдирд╣реАрдВ рд╣реИ рддреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдкрддрд╛ рд▓рдЧрд╛рдпрд╛ рдЬрд╛рдПрдЧрд╛)\n",
    "\n",
    "**рдпрд╣ рдХреИрд╕реЗ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ (рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдкреИрдЯрд░реНрди):**\n",
    "1. **FoundryLocalManager** Foundry Local рд╕реЗрд╡рд╛ рдХреЛ рдкреНрд░рд╛рд░рдВрдн рдФрд░ рдкреНрд░рдмрдВрдзрд┐рдд рдХрд░рддрд╛ рд╣реИ\n",
    "2. рдпрджрд┐ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ рддреЛ рдпрд╣ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд╢реБрд░реВ рд╣реЛ рдЬрд╛рддреА рд╣реИ (рдХреЛрдИ рдореИрдиреБрдЕрд▓ рд╕реЗрдЯрдЕрдк рдЖрд╡рд╢реНрдпрдХ рдирд╣реАрдВ)\n",
    "3. рдореЙрдбрд▓ рдХреЛ рдЙрдкрдирд╛рдореЛрдВ рд╕реЗ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдареЛрд╕ рдЖрдИрдбреА рдореЗрдВ рд╣рд▓ рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ\n",
    "4. рд╣рд╛рд░реНрдбрд╡реЗрдпрд░-рдЕрдиреБрдХреВрд▓рд┐рдд рд╡реЗрд░рд┐рдПрдВрдЯ рдХрд╛ рдЪрдпрди рдХрд┐рдпрд╛ рдЬрд╛рддрд╛ рд╣реИ (CUDA, NPU, рдпрд╛ CPU)\n",
    "5. OpenAI-рд╕рдВрдЧрдд рдХреНрд▓рд╛рдЗрдВрдЯ рдЪреИрдЯ рдкреВрд░реНрдгрддрд╛ рдХрд░рддрд╛ рд╣реИ\n",
    "6. рдореЗрдЯреНрд░рд┐рдХреНрд╕ рдХреИрдкреНрдЪрд░ рдХрд┐рдП рдЬрд╛рддреЗ рд╣реИрдВ: рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рд╕рдордп, рдЯреЛрдХрди, рдЖрдЙрдЯрдкреБрдЯ рдЧреБрдгрд╡рддреНрддрд╛\n",
    "7. рдкрд░рд┐рдгрд╛рдореЛрдВ рдХреА рддреБрд▓рдирд╛ рдХрд░рдХреЗ рдЧрддрд┐ рд╡реГрджреНрдзрд┐ рдЕрдиреБрдкрд╛рдд рдХреА рдЧрдгрдирд╛ рдХреА рдЬрд╛рддреА рд╣реИ\n",
    "\n",
    "рдпрд╣ рд╕реВрдХреНрд╖реНрдо рддреБрд▓рдирд╛ рдпрд╣ рддрдп рдХрд░рдиреЗ рдореЗрдВ рдорджрдж рдХрд░рддреА рд╣реИ рдХрд┐ рдЖрдкрдХреЗ рдЙрдкрдпреЛрдЧ рдХреЗ рдорд╛рдорд▓реЗ рдХреЗ рд▓рд┐рдП рдмрдбрд╝реЗ рдореЙрдбрд▓ рдХреА рдУрд░ рд░реВрдЯрд┐рдВрдЧ рдХрдм рдЙрдЪрд┐рдд рд╣реИред\n",
    "\n",
    "**SDK рд╕рдВрджрд░реНрдн:** \n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- рд╡рд░реНрдХрд╢реЙрдк рдпреВрдЯрд┐рд▓реНрд╕: ../samples/workshop_utils.py рд╕реЗ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдкреИрдЯрд░реНрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ\n",
    "\n",
    "**рдореБрдЦреНрдп рд▓рд╛рдн:**\n",
    "- тЬЕ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕реЗрд╡рд╛ рдЦреЛрдЬ рдФрд░ рдкреНрд░рд╛рд░рдВрднрд┐рдХрдХрд░рдг\n",
    "- тЬЕ рдпрджрд┐ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ рддреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд╢реБрд░реВ рд╣реЛрддреА рд╣реИ\n",
    "- тЬЕ рдЕрдВрддрд░реНрдирд┐рд╣рд┐рдд рдореЙрдбрд▓ рд╕рдорд╛рдзрд╛рди рдФрд░ рдХреИрд╢рд┐рдВрдЧ\n",
    "- тЬЕ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдЕрдиреБрдХреВрд▓рди (CUDA/NPU/CPU)\n",
    "- тЬЕ OpenAI SDK рд╕рдВрдЧрддрддрд╛\n",
    "- тЬЕ рдкреБрдирдГ рдкреНрд░рдпрд╛рд╕ рдХреЗ рд╕рд╛рде рдордЬрдмреВрдд рддреНрд░реБрдЯрд┐ рдкреНрд░рдмрдВрдзрди\n",
    "- тЬЕ рд╕реНрдерд╛рдиреАрдп рдЕрдиреБрдорд╛рди (рдХреЛрдИ рдХреНрд▓рд╛рдЙрдб API рдЖрд╡рд╢реНрдпрдХ рдирд╣реАрдВ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ЁЯЪи рдЖрд╡рд╢реНрдпрдХ рд╢рд░реНрддреЗрдВ: Foundry Local рдЪрд╛рд▓реВ рд╣реЛрдирд╛ рдЪрд╛рд╣рд┐рдП!\n",
    "\n",
    "**рдЗрд╕ рдиреЛрдЯрдмреБрдХ рдХреЛ рдЪрд▓рд╛рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ**, рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ Foundry Local рд╕реЗрд╡рд╛ рд╕реЗрдЯрдЕрдк рд╣реЛ рдЪреБрдХреА рд╣реИ:\n",
    "\n",
    "### рддреНрд╡рд░рд┐рдд рдкреНрд░рд╛рд░рдВрдн рдХрдорд╛рдВрдб (рдЯрд░реНрдорд┐рдирд▓ рдореЗрдВ рдЪрд▓рд╛рдПрдВ):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### рд╡реИрдХрд▓реНрдкрд┐рдХ рдореЙрдбрд▓ (рдпрджрд┐ рдбрд┐рдлрд╝реЙрд▓реНрдЯ рдЙрдкрд▓рдмреНрдз рдирд╣реАрдВ рд╣реИрдВ):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "тЪая╕П **рдпрджрд┐ рдЖрдк рдЗрди рдЪрд░рдгреЛрдВ рдХреЛ рдЫреЛрдбрд╝рддреЗ рд╣реИрдВ**, рддреЛ рдиреАрдЪреЗ рдиреЛрдЯрдмреБрдХ рд╕реЗрд▓ рдЪрд▓рд╛рдиреЗ рдкрд░ рдЖрдкрдХреЛ `APIConnectionError` рджрд┐рдЦрд╛рдИ рджреЗрдЧрд╛ред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдХреЛрд░ рдЖрдпрд╛рдд\n",
    "рд╕рдордп рдЙрдкрдпреЛрдЧрд┐рддрд╛рдУрдВ рдФрд░ Foundry Local / OpenAI рдХреНрд▓рд╛рдЗрдВрдЯреНрд╕ рдХреЛ рд╢рд╛рдорд┐рд▓ рдХрд░рддрд╛ рд╣реИ, рдЬреЛ рдореЙрдбрд▓ рдЬрд╛рдирдХрд╛рд░реА рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдФрд░ рдЪреИрдЯ рдкреВрд░реНрдгрддрд╛ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреЛрдЧ рдХрд┐рдП рдЬрд╛рддреЗ рд╣реИрдВред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдЙрдкрдирд╛рдо рдФрд░ рдкреНрд░реЙрдореНрдкреНрдЯ рд╕реЗрдЯрдЕрдк  \n",
    "рдЫреЛрдЯреЗ рдФрд░ рдмрдбрд╝реЗ рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП рдкрд░реНрдпрд╛рд╡рд░рдг-рдХреЙрдиреНрдлрд╝рд┐рдЧрд░ рдХрд░рдиреЗ рдпреЛрдЧреНрдп рдЙрдкрдирд╛рдореЛрдВ рдХреЛ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдХрд░рддрд╛ рд╣реИ, рд╕рд╛рде рд╣реА рддреБрд▓рдирд╛ рдкреНрд░реЙрдореНрдкреНрдЯ рднреАред рд╡рд┐рднрд┐рдиреНрди рдореЙрдбрд▓ рдкрд░рд┐рд╡рд╛рд░реЛрдВ рдпрд╛ рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд╕рд╛рде рдкреНрд░рдпреЛрдЧ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдкрд░реНрдпрд╛рд╡рд░рдг рд╡реЗрд░рд┐рдПрдмрд▓реНрд╕ рдХреЛ рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░реЗрдВред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ЁЯТб рдореЗрдореЛрд░реА-рдЕрдиреБрдХреВрд▓рд┐рдд рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди\n",
    "\n",
    "**рдпрд╣ рдиреЛрдЯрдмреБрдХ рдбрд┐рдлрд╝реЙрд▓реНрдЯ рд░реВрдк рд╕реЗ рдореЗрдореЛрд░реА-рдХреБрд╢рд▓ рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ:**\n",
    "- `phi-4-mini` тЖТ ~4GB RAM (Foundry Local рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ CPU рд╕рдВрд╕реНрдХрд░рдг рдЪреБрдирддрд╛ рд╣реИ)\n",
    "- `qwen2.5-3b` тЖТ ~3GB RAM (7B рдХреЗ рдмрдЬрд╛рдп рдЬреЛ ~7GB+ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╣реЛрддреА рд╣реИ)\n",
    "\n",
    "**рдкреЛрд░реНрдЯ рдСрдЯреЛ-рдбрд┐рдЯреЗрдХреНрд╢рди:**\n",
    "- Foundry Local рд╡рд┐рднрд┐рдиреНрди рдкреЛрд░реНрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд╕рдХрддрд╛ рд╣реИ (рдЖрдорддреМрд░ рдкрд░ 55769 рдпрд╛ 59959)\n",
    "- рдиреАрдЪреЗ рджрд┐рдпрд╛ рдЧрдпрд╛ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓ рд╕рд╣реА рдкреЛрд░реНрдЯ рдХреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рдкрд╣рдЪрд╛рдирддрд╛ рд╣реИ\n",
    "- рдХреЛрдИ рдореИрдиреБрдЕрд▓ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░реЗрд╢рди рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВ рд╣реИ!\n",
    "\n",
    "**рдпрджрд┐ рдЖрдкрдХреЗ рдкрд╛рд╕ рд╕реАрдорд┐рдд RAM (<8GB) рд╣реИ, рддреЛ рдФрд░ рднреА рдЫреЛрдЯреЗ рдореЙрдбрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ЁЯТб Using official Foundry SDK pattern from workshop_utils\n",
      "   тЖТ FoundryLocalManager handles service lifecycle\n",
      "   тЖТ Automatic model resolution and hardware optimization\n",
      "   тЖТ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nЁЯТб Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   тЖТ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   тЖТ Automatic model resolution and hardware optimization\")\n",
    "print(\"   тЖТ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдирд┐рд╖реНрдкрд╛рджрди рд╕рд╣рд╛рдпрдХ (Foundry SDK рдкреИрдЯрд░реНрди)\n",
    "рдпрд╣ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Foundry Local SDK рдкреИрдЯрд░реНрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ, рдЬреИрд╕рд╛ рдХрд┐ Workshop рдирдореВрдиреЛрдВ рдореЗрдВ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝рд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИ:\n",
    "\n",
    "**рджреГрд╖реНрдЯрд┐рдХреЛрдг:**\n",
    "- **FoundryLocalManager** - Foundry Local рд╕реЗрд╡рд╛ рдХреЛ рдкреНрд░рд╛рд░рдВрдн рдФрд░ рдкреНрд░рдмрдВрдзрд┐рдд рдХрд░рддрд╛ рд╣реИ\n",
    "- **рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдкрд╣рдЪрд╛рди** - рд╕реНрд╡рддрдГ рд╣реА рдПрдВрдбрдкреЙрдЗрдВрдЯ рдХрд╛ рдкрддрд╛ рд▓рдЧрд╛рддрд╛ рд╣реИ рдФрд░ рд╕реЗрд╡рд╛ рдЬреАрд╡рдирдЪрдХреНрд░ рдХреЛ рд╕рдВрднрд╛рд▓рддрд╛ рд╣реИ\n",
    "- **рдореЙрдбрд▓ рд╕рдорд╛рдзрд╛рди** - рдЙрдкрдирд╛рдореЛрдВ рдХреЛ рдкреВрд░реНрдг рдореЙрдбрд▓ рдЖрдИрдбреА рдореЗрдВ рдмрджрд▓рддрд╛ рд╣реИ (рдЬреИрд╕реЗ, phi-4-mini тЖТ phi-4-mini-instruct-cpu)\n",
    "- **рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдЕрдиреБрдХреВрд▓рди** - рдЙрдкрд▓рдмреНрдз рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдХреЗ рд▓рд┐рдП рд╕рдмрд╕реЗ рдЕрдЪреНрдЫрд╛ рд╕рдВрд╕реНрдХрд░рдг рдЪреБрдирддрд╛ рд╣реИ (CUDA, NPU, рдпрд╛ CPU)\n",
    "- **OpenAI рдХреНрд▓рд╛рдЗрдВрдЯ** - OpenAI-рд╕рдВрдЧрдд API рдПрдХреНрд╕реЗрд╕ рдХреЗ рд▓рд┐рдП рдореИрдиреЗрдЬрд░ рдХреЗ рдПрдВрдбрдкреЙрдЗрдВрдЯ рдХреЗ рд╕рд╛рде рдХреЙрдиреНрдлрд╝рд┐рдЧрд░ рдХрд┐рдпрд╛ рдЧрдпрд╛\n",
    "\n",
    "**рд▓рдЪреАрд▓рд╛рдкрди рд╡рд┐рд╢реЗрд╖рддрд╛рдПрдБ:**\n",
    "- рдПрдХреНрд╕рдкреЛрдиреЗрдВрд╢рд┐рдпрд▓ рдмреИрдХрдСрдл рд░реАрдЯреНрд░рд╛рдИ рд▓реЙрдЬрд┐рдХ (рдкрд░реНрдпрд╛рд╡рд░рдг рдХреЗ рдорд╛рдзреНрдпрдо рд╕реЗ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░ рдХрд░рдиреЗ рдпреЛрдЧреНрдп)\n",
    "- рдпрджрд┐ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ рддреЛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдк рд╕реЗ рд╕реЗрд╡рд╛ рдкреНрд░рд╛рд░рдВрдн\n",
    "- рдкреНрд░рд╛рд░рдВрднрд┐рдХрдХрд░рдг рдХреЗ рдмрд╛рдж рдХрдиреЗрдХреНрд╢рди рд╕рддреНрдпрд╛рдкрди\n",
    "- рд╡рд┐рд╕реНрддреГрдд рддреНрд░реБрдЯрд┐ рд░рд┐рдкреЛрд░реНрдЯрд┐рдВрдЧ рдХреЗ рд╕рд╛рде рд╕рд╣рдЬ рддреНрд░реБрдЯрд┐ рдкреНрд░рдмрдВрдзрди\n",
    "- рдмрд╛рд░-рдмрд╛рд░ рдкреНрд░рд╛рд░рдВрднрд┐рдХрдХрд░рдг рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдП рдореЙрдбрд▓ рдХреИрд╢рд┐рдВрдЧ\n",
    "\n",
    "**рдкрд░рд┐рдгрд╛рдо рд╕рдВрд░рдЪрдирд╛:**\n",
    "- рд╡рд┐рд▓рдВрдмрддрд╛ рдорд╛рдкрди (рджреАрд╡рд╛рд░ рдШрдбрд╝реА рд╕рдордп)\n",
    "- рдЯреЛрдХрди рдЙрдкрдпреЛрдЧ рдЯреНрд░реИрдХрд┐рдВрдЧ (рдпрджрд┐ рдЙрдкрд▓рдмреНрдз рд╣реЛ)\n",
    "- рдирдореВрдирд╛ рдЖрдЙрдЯрдкреБрдЯ (рдкрдврд╝рдиреЗ рдореЗрдВ рдЖрд╕рд╛рдиреА рдХреЗ рд▓рд┐рдП рдЫреЛрдЯрд╛ рдХрд┐рдпрд╛ рдЧрдпрд╛)\n",
    "- рдЕрд╕рдлрд▓ рдЕрдиреБрд░реЛрдзреЛрдВ рдХреЗ рд▓рд┐рдП рддреНрд░реБрдЯрд┐ рд╡рд┐рд╡рд░рдг\n",
    "\n",
    "рдпрд╣ рдкреИрдЯрд░реНрди workshop_utils рдореЙрдбреНрдпреВрд▓ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ, рдЬреЛ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдкреИрдЯрд░реНрди рдХрд╛ рдЕрдиреБрд╕рд░рдг рдХрд░рддрд╛ рд╣реИред\n",
    "\n",
    "**SDK рд╕рдВрджрд░реНрдн:**\n",
    "- рдореБрдЦреНрдп рд░рд┐рдкреЙрдЬрд┐рдЯрд░реА: https://github.com/microsoft/Foundry-Local\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- Workshop Utils: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Execution helpers defined: setup(), run()\n",
      "   тЖТ Uses workshop_utils for proper SDK integration\n",
      "   тЖТ setup() initializes with FoundryLocalManager\n",
      "   тЖТ run() executes inference via OpenAI-compatible API\n",
      "   тЖТ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        тЖТ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        тЖТ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        тЖТ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        тЖТ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nтЭМ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nЁЯТб Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      тЖТ foundry service status\")\n",
    "    print(f\"      тЖТ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      тЖТ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      тЖТ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"тЬЕ Execution helpers defined: setup(), run()\")\n",
    "print(\"   тЖТ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   тЖТ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   тЖТ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   тЖТ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдкреНрд░реА-рдлреНрд▓рд╛рдЗрдЯ рд╕реЗрд▓реНрдл-рдЯреЗрд╕реНрдЯ\n",
    "FoundryLocalManager рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рджреЛрдиреЛрдВ рдореЙрдбрд▓реЛрдВ рдХреЗ рд▓рд┐рдП рдПрдХ рд╣рд▓реНрдХрд╛ рдХрдиреЗрдХреНрдЯрд┐рд╡рд┐рдЯреА рдЪреЗрдХ рдЪрд▓рд╛рддрд╛ рд╣реИред рдпрд╣ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддрд╛ рд╣реИ:\n",
    "- рд╕реЗрд╡рд╛ рд╕реБрд▓рдн рд╣реИ\n",
    "- рдореЙрдбрд▓ рдкреНрд░рд╛рд░рдВрдн рдХрд┐рдП рдЬрд╛ рд╕рдХрддреЗ рд╣реИрдВ\n",
    "- рдЙрдкрдирд╛рдо рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдореЙрдбрд▓ рдЖрдИрдбреА рдореЗрдВ рдмрджрд▓рддреЗ рд╣реИрдВ\n",
    "- рддреБрд▓рдирд╛ рдЪрд▓рд╛рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ рдХрдиреЗрдХреНрд╢рди рд╕реНрдерд┐рд░ рд╣реИ\n",
    "\n",
    "setup() рдлрд╝рдВрдХреНрд╢рди workshop_utils рд╕реЗ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдкреИрдЯрд░реНрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "тЭМ Foundry Local service not found!\n",
      "\n",
      "ЁЯТб To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "тЪая╕П  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"тЬЕ Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"тЬЕ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"тЪая╕П  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"тЪая╕П  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nтЭМ Foundry Local service not found!\")\n",
    "    print(\"\\nЁЯТб To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nтЬЕ Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nтЪая╕П  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪая╕П  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ЁЯТб Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"тЪая╕П  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ЁЯТб Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ЁЯЫая╕П рддреНрд╡рд░рд┐рдд рд╕рдорд╛рдзрд╛рди: рдиреЛрдЯрдмреБрдХ рд╕реЗ рдлрд╛рдЙрдВрдбреНрд░реА рд▓реЛрдХрд▓ рд╢реБрд░реВ рдХрд░реЗрдВ (рд╡реИрдХрд▓реНрдкрд┐рдХ)\n",
    "\n",
    "рдпрджрд┐ рдКрдкрд░ рдХреЗ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗ рдкрддрд╛ рдЪрд▓рддрд╛ рд╣реИ рдХрд┐ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ, рддреЛ рдЖрдк рдЗрд╕реЗ рдпрд╣рд╛рдВ рд╕реЗ рд╢реБрд░реВ рдХрд░рдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ:\n",
    "\n",
    "**рдиреЛрдЯ:** рдпрд╣ рд╡рд┐рдВрдбреЛрдЬрд╝ рдкрд░ рд╕рдмрд╕реЗ рдЕрдЪреНрдЫрд╛ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИред рдЕрдиреНрдп рдкреНрд▓реЗрдЯрдлрд╛рд░реНрдореЛрдВ рдкрд░, рдЯрд░реНрдорд┐рдирд▓ рдХрдорд╛рдВрдб рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### тЪая╕П рдХрдиреЗрдХреНрд╢рди рддреНрд░реБрдЯрд┐рдпреЛрдВ рдХрд╛ рд╕рдорд╛рдзрд╛рди\n",
    "\n",
    "рдпрджрд┐ рдЖрдк `APIConnectionError` рджреЗрдЦ рд░рд╣реЗ рд╣реИрдВ, рддреЛ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ рдХрд┐ Foundry Local рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рди рд╣реЛ рдпрд╛ рдореЙрдбрд▓ рд▓реЛрдб рди рдХрд┐рдП рдЧрдП рд╣реЛрдВред рдЗрди рдЪрд░рдгреЛрдВ рдХреЛ рдЖрдЬрд╝рдорд╛рдПрдВ:\n",
    "\n",
    "**1. рд╕реЗрд╡рд╛ рдХреА рд╕реНрдерд┐рддрд┐ рдЬрд╛рдВрдЪреЗрдВ:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**2. рд╕реЗрд╡рд╛ рд╢реБрд░реВ рдХрд░реЗрдВ (рдпрджрд┐ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**3. рдЖрд╡рд╢реНрдпрдХ рдореЙрдбрд▓ рд▓реЛрдб рдХрд░реЗрдВ:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**4. рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ рдореЙрдбрд▓ рдЙрдкрд▓рдмреНрдз рд╣реИрдВ:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛рдПрдВ:**\n",
    "- тЭМ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╣реАрдВ рд╣реИ тЖТ `foundry service start` рдЪрд▓рд╛рдПрдВ\n",
    "- тЭМ рдореЙрдбрд▓ рд▓реЛрдб рдирд╣реАрдВ рдХрд┐рдП рдЧрдП рд╣реИрдВ тЖТ `foundry model run <model-name>` рдЪрд▓рд╛рдПрдВ\n",
    "- тЭМ рдкреЛрд░реНрдЯ рд╕рдВрдШрд░реНрд╖ тЖТ рдЬрд╛рдВрдЪреЗрдВ рдХрд┐ рдХреЛрдИ рдЕрдиреНрдп рд╕реЗрд╡рд╛ рдкреЛрд░реНрдЯ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд░рд╣реА рд╣реИ\n",
    "- тЭМ рдлрд╝рд╛рдпрд░рд╡реЙрд▓ рдмреНрд▓реЙрдХ рдХрд░ рд░рд╣рд╛ рд╣реИ тЖТ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ рд╕реНрдерд╛рдиреАрдп рдХрдиреЗрдХреНрд╢рди рдХреА рдЕрдиреБрдорддрд┐ рд╣реИ\n",
    "\n",
    "**рддреНрд╡рд░рд┐рдд рд╕рдорд╛рдзрд╛рди:** рдкреНрд░реА-рдлреНрд▓рд╛рдЗрдЯ рдЪреЗрдХ рд╕реЗ рдкрд╣рд▓реЗ рдиреАрдЪреЗ рджрд┐рдпрд╛ рдЧрдпрд╛ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓ рдЪрд▓рд╛рдПрдВред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  тЬЕ phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  тЬЕ qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'тЬЕ' if details['status'] == 'success' else 'тЭМ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### тЬЕ рдкреНрд░реА-рдлреНрд▓рд╛рдЗрдЯ рдЪреЗрдХ: рдореЙрдбрд▓ рдЙрдкрд▓рдмреНрдзрддрд╛\n",
    "\n",
    "рдпрд╣ рд╕реЗрд▓ рддреБрд▓рдирд╛ рд╢реБрд░реВ рдХрд░рдиреЗ рд╕реЗ рдкрд╣рд▓реЗ рдпрд╣ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддрд╛ рд╣реИ рдХрд┐ рджреЛрдиреЛрдВ рдореЙрдбрд▓ рдХреЙрдиреНрдлрд╝рд┐рдЧрд░ рдХрд┐рдП рдЧрдП рдПрдВрдбрдкреЙрдЗрдВрдЯ рдкрд░ рдЙрдкрд▓рдмреНрдз рд╣реИрдВред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рд░рди рддреБрд▓рдирд╛ рдФрд░ рдкрд░рд┐рдгрд╛рдо рд╕рдВрдЧреНрд░рд╣ рдХрд░реЗрдВ\n",
    "рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Foundry SDK рдкреИрдЯрд░реНрди рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддреЗ рд╣реБрдП рджреЛрдиреЛрдВ рдЙрдкрдирд╛рдореЛрдВ рдкрд░ рдкреБрдирд░рд╛рд╡реГрддреНрддрд┐ рдХрд░рддрд╛ рд╣реИ:\n",
    "1. рдкреНрд░рддреНрдпреЗрдХ рдореЙрдбрд▓ рдХреЛ setup() рдХреЗ рд╕рд╛рде рдкреНрд░рд╛рд░рдВрдн рдХрд░реЗрдВ (FoundryLocalManager рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рддрд╛ рд╣реИ)\n",
    "2. OpenAI-рд╕рдВрдЧрдд API рдХреЗ рд╕рд╛рде рдЕрдиреБрдорд╛рди рд▓рдЧрд╛рдПрдВ\n",
    "3. рд╡рд┐рд▓рдВрдмрддрд╛, рдЯреЛрдХрди рдФрд░ рдирдореВрдирд╛ рдЖрдЙрдЯрдкреБрдЯ рдХреИрдкреНрдЪрд░ рдХрд░реЗрдВ\n",
    "4. рддреБрд▓рдирд╛рддреНрдордХ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХреЗ рд╕рд╛рде JSON рд╕рд╛рд░рд╛рдВрд╢ рддреИрдпрд╛рд░ рдХрд░реЗрдВ\n",
    "\n",
    "рдпрд╣ session04/model_compare.py рдореЗрдВ рд╡рд░реНрдХрд╢реЙрдк рдирдореВрдиреЛрдВ рдХреЗ рд╕рдорд╛рди рдкреИрдЯрд░реНрди рдХрд╛ рдЕрдиреБрд╕рд░рдг рдХрд░рддрд╛ рд╣реИред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "тЬЕ phi-4-mini         success         51.975          ~247 (est.)    \n",
      "тЬЕ qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ЁЯТб SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'тЬЕ' if status == 'success' else 'тЭМ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nЁЯТб SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nтЪая╕П  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### рдкрд░рд┐рдгрд╛рдореЛрдВ рдХреА рд╡реНрдпрд╛рдЦреНрдпрд╛\n",
    "\n",
    "**рдореБрдЦреНрдп рдорд╛рдкрджрдВрдб:**\n",
    "- **рд▓реЗрдЯреЗрдВрд╕реА**: рдХрдо рд╣реЛрдирд╛ рдмреЗрд╣рддрд░ рд╣реИ - рддреЗрдЬ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рд╕рдордп рдХреЛ рджрд░реНрд╢рд╛рддрд╛ рд╣реИ\n",
    "- **рдЯреЛрдХрди**: рдЕрдзрд┐рдХ рдереНрд░реВрдкреБрдЯ = рдЕрдзрд┐рдХ рдЯреЛрдХрди рд╕рдВрд╕рд╛рдзрд┐рдд рдХрд┐рдП рдЧрдП\n",
    "- **рд░реВрдЯ**: рдкреБрд╖реНрдЯрд┐ рдХрд░рддрд╛ рд╣реИ рдХрд┐ рдХреМрди рд╕рд╛ API рдПрдВрдбрдкреЙрдЗрдВрдЯ рдЙрдкрдпреЛрдЧ рдХрд┐рдпрд╛ рдЧрдпрд╛\n",
    "\n",
    "**SLM рдмрдирд╛рдо LLM рдХрдм рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ:**\n",
    "- **SLM (рдЫреЛрдЯрд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓)**: рддреЗрдЬ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛рдПрдВ, рдХрдо рд╕рдВрд╕рд╛рдзрди рдЙрдкрдпреЛрдЧ, рд╕рд░рд▓ рдХрд╛рд░реНрдпреЛрдВ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд\n",
    "- **LLM (рдмрдбрд╝рд╛ рднрд╛рд╖рд╛ рдореЙрдбрд▓)**: рдЙрдЪреНрдЪ рдЧреБрдгрд╡рддреНрддрд╛, рдмреЗрд╣рддрд░ рддрд░реНрдХ рдХреНрд╖рдорддрд╛, рдЬрдм рдЧреБрдгрд╡рддреНрддрд╛ рд╕рдмрд╕реЗ рдорд╣рддреНрд╡рдкреВрд░реНрдг рд╣реЛ рддрдм рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ\n",
    "\n",
    "**рдЕрдЧрд▓реЗ рдХрджрдо:**\n",
    "1. рд╡рд┐рднрд┐рдиреНрди рдкреНрд░реЙрдореНрдкреНрдЯ рдЖрдЬрд╝рдорд╛рдПрдВ рдФрд░ рджреЗрдЦреЗрдВ рдХрд┐ рдЬрдЯрд┐рд▓рддрд╛ рддреБрд▓рдирд╛ рдХреЛ рдХреИрд╕реЗ рдкреНрд░рднрд╛рд╡рд┐рдд рдХрд░рддреА рд╣реИ\n",
    "2. рдЕрдиреНрдп рдореЙрдбрд▓ рдЬреЛрдбрд╝реЛрдВ рдХреЗ рд╕рд╛рде рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВ\n",
    "3. рдХрд╛рд░реНрдп рдХреА рдЬрдЯрд┐рд▓рддрд╛ рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рдмреБрджреНрдзрд┐рдорд╛рдиреА рд╕реЗ рд░реВрдЯ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╡рд░реНрдХрд╢реЙрдк рд░рд╛рдЙрдЯрд░ рд╕реИрдВрдкрд▓ (рд╕реЗрд╢рди 06) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "тЬЕ SLM Model: phi-4-mini\n",
      "тЬЕ LLM Model: qwen2.5-7b\n",
      "тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "тЬЕ Pre-flight passed: True\n",
      "тЬЕ Comparison completed: True\n",
      "тЬЕ Both models responded: True\n",
      "======================================================================\n",
      "ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"тЬЕ SLM Model: {SLM}\")\n",
    "print(f\"тЬЕ LLM Model: {LLM}\")\n",
    "print(f\"тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"тЬЕ Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"тЪая╕П  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"тЪая╕П  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"тЪая╕П  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nтЪая╕П  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nЁЯТб Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nЁЯТб Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрдпрд╣ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ AI рдЕрдиреБрд╡рд╛рдж рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░рдХреЗ рдЕрдиреБрд╡рд╛рджрд┐рдд рдХрд┐рдпрд╛ рдЧрдпрд╛ рд╣реИред рдЬрдмрдХрд┐ рд╣рдо рд╕рдЯреАрдХрддрд╛ рдХреЗ рд▓рд┐рдП рдкреНрд░рдпрд╛рд╕ рдХрд░рддреЗ рд╣реИрдВ, рдХреГрдкрдпрд╛ рдзреНрдпрд╛рди рджреЗрдВ рдХрд┐ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЕрдиреБрд╡рд╛рдж рдореЗрдВ рддреНрд░реБрдЯрд┐рдпрд╛рдВ рдпрд╛ рдЕрд╢реБрджреНрдзрд┐рдпрд╛рдВ рд╣реЛ рд╕рдХрддреА рд╣реИрдВред рдореВрд▓ рднрд╛рд╖рд╛ рдореЗрдВ рдЙрдкрд▓рдмреНрдз рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдХреЛ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рд╕реНрд░реЛрдд рдорд╛рдирд╛ рдЬрд╛рдирд╛ рдЪрд╛рд╣рд┐рдПред рдорд╣рддреНрд╡рдкреВрд░реНрдг рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП, рдкреЗрд╢реЗрд╡рд░ рдорд╛рдирд╡ рдЕрдиреБрд╡рд╛рдж рдХреА рд╕рд┐рдлрд╛рд░рд┐рд╢ рдХреА рдЬрд╛рддреА рд╣реИред рдЗрд╕ рдЕрдиреБрд╡рд╛рдж рдХреЗ рдЙрдкрдпреЛрдЧ рд╕реЗ рдЙрддреНрдкрдиреНрди рдХрд┐рд╕реА рднреА рдЧрд▓рддрдлрд╣рдореА рдпрд╛ рдЧрд▓рдд рд╡реНрдпрд╛рдЦреНрдпрд╛ рдХреЗ рд▓рд┐рдП рд╣рдо рдЙрддреНрддрд░рджрд╛рдпреА рдирд╣реАрдВ рд╣реИрдВред\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-08T22:15:34+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "hi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}