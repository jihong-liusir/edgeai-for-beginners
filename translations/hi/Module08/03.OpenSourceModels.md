<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T13:36:39+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "hi"
}
-->
# सत्र 3: Foundry Local के साथ ओपन-सोर्स मॉडल

## परिचय

इस सत्र में यह जानेंगे कि Foundry Local में ओपन-सोर्स मॉडल कैसे लाए जाएं: समुदाय के मॉडल चुनना, Hugging Face सामग्री को एकीकृत करना, और "अपना खुद का मॉडल लाएं" (BYOM) रणनीतियों को अपनाना। आप Model Mondays श्रृंखला के बारे में भी जानेंगे, जो निरंतर सीखने और मॉडल खोज के लिए है।

संदर्भ:
- Foundry Local दस्तावेज़: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face मॉडल को संकलित करें: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## सीखने के उद्देश्य
- स्थानीय इनफेरेंस के लिए ओपन-सोर्स मॉडल की खोज और मूल्यांकन करें
- Foundry Local में चुनिंदा Hugging Face मॉडल को संकलित और चलाएं
- सटीकता, विलंबता, और संसाधन आवश्यकताओं के लिए मॉडल चयन रणनीतियों को लागू करें
- कैश और संस्करण प्रबंधन के साथ स्थानीय रूप से मॉडल प्रबंधित करें

## भाग 1: मॉडल खोज और चयन (स्टेप-बाय-स्टेप)

चरण 1) स्थानीय कैटलॉग में उपलब्ध मॉडल सूचीबद्ध करें  
```cmd
foundry model list
```
  
चरण 2) दो उम्मीदवारों का त्वरित परीक्षण करें (पहली बार चलाने पर स्वचालित डाउनलोड)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
चरण 3) बुनियादी मेट्रिक्स नोट करें  
- एक निश्चित प्रॉम्प्ट के लिए विलंबता (सामान्य) और गुणवत्ता का अवलोकन करें  
- प्रत्येक मॉडल के चलने के दौरान टास्क मैनेजर के माध्यम से मेमोरी उपयोग देखें  

## भाग 2: CLI के माध्यम से कैटलॉग मॉडल चलाना (स्टेप-बाय-स्टेप)

चरण 1) एक मॉडल शुरू करें  
```cmd
foundry model run llama-3.2
```
  
चरण 2) OpenAI-संगत एंडपॉइंट के माध्यम से एक परीक्षण प्रॉम्प्ट भेजें  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## भाग 3: BYOM – Hugging Face मॉडल संकलित करें (स्टेप-बाय-स्टेप)

मॉडल संकलित करने के लिए आधिकारिक निर्देशों का पालन करें। नीचे उच्च-स्तरीय प्रवाह दिया गया है—Microsoft Learn लेख में सटीक कमांड और समर्थित कॉन्फ़िगरेशन देखें।

चरण 1) एक कार्यशील निर्देशिका तैयार करें  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
चरण 2) एक समर्थित HF मॉडल संकलित करें  
- Learn दस्तावेज़ से चरणों का उपयोग करके संकलित ONNX मॉडल को अपने `models` निर्देशिका में रखें  
- पुष्टि करें:  
```cmd
foundry cache ls
```
  
आपको अपने संकलित मॉडल का नाम देखना चाहिए (उदाहरण के लिए, `llama-3.2`)।  

चरण 3) संकलित मॉडल चलाएं  
```cmd
foundry model run llama-3.2 --verbose
```
  
नोट्स:  
- संकलन और चलाने के लिए पर्याप्त डिस्क और RAM सुनिश्चित करें  
- प्रवाह को मान्य करने के लिए छोटे मॉडल से शुरू करें, फिर स्केल करें  

## भाग 4: व्यावहारिक मॉडल क्यूरेशन (स्टेप-बाय-स्टेप)

चरण 1) एक `models.json` रजिस्ट्री बनाएं  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
चरण 2) छोटा चयनकर्ता स्क्रिप्ट  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## भाग 5: हैंड्स-ऑन बेंचमार्क (स्टेप-बाय-स्टेप)

चरण 1) सरल विलंबता बेंचमार्क  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
चरण 2) गुणवत्ता स्पॉट-चेक  
- एक निश्चित प्रॉम्प्ट सेट का उपयोग करें, आउटपुट को CSV/JSON में कैप्चर करें  
- प्रवाह, प्रासंगिकता, और शुद्धता को मैन्युअल रूप से रेट करें (1–5)  

## भाग 6: अगले कदम
- नए मॉडल और सुझावों के लिए Model Mondays की सदस्यता लें: https://aka.ms/model-mondays  
- अपनी टीम के `models.json` में निष्कर्ष साझा करें  
- सत्र 4 के लिए तैयारी करें: LLMs बनाम SLMs, स्थानीय बनाम क्लाउड इनफेरेंस, और हैंड्स-ऑन डेमो की तुलना  

---

