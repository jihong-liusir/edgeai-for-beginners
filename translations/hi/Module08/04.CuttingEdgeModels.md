<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T13:39:31+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "hi"
}
-->
# सत्र 4: अत्याधुनिक मॉडल – LLMs, SLMs, और ऑन-डिवाइस इन्फरेंस

## परिचय

LLMs और SLMs की तुलना करें, स्थानीय बनाम क्लाउड इन्फरेंस के फायदे-नुकसान का मूल्यांकन करें, और EdgeAI परिदृश्यों को प्रदर्शित करने वाले डेमो को Phi और ONNX Runtime का उपयोग करके लागू करें। हम Chainlit RAG, WebGPU इन्फरेंस विकल्प, और Open WebUI इंटीग्रेशन पर भी प्रकाश डालेंगे।

संदर्भ:
- Foundry Local दस्तावेज़: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI कैसे करें (Open WebUI के साथ चैट ऐप): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## सीखने के उद्देश्य
- लागत, विलंबता, और सटीकता के लिए LLM बनाम SLM के फायदे-नुकसान को समझें
- विशिष्ट व्यावसायिक आवश्यकताओं के लिए स्थानीय और क्लाउड इन्फरेंस में से चुनें
- Chainlit के साथ एक छोटा RAG डेमो लागू करें
- ब्राउज़र-साइड एक्सेलेरेशन के लिए WebGPU का अन्वेषण करें
- Open WebUI को Foundry Local से कनेक्ट करें

## भाग 1: LLM बनाम SLM – निर्णय मैट्रिक्स

विचार करें:
- विलंबता: SLMs ऑन-डिवाइस अक्सर सेकंड से कम समय में प्रतिक्रिया देते हैं
- लागत: स्थानीय इन्फरेंस क्लाउड लागत को कम करता है
- गोपनीयता: संवेदनशील डेटा ऑन-डिवाइस रहता है
- क्षमता: LLMs जटिल कार्यों में SLMs से बेहतर प्रदर्शन कर सकते हैं
- विश्वसनीयता: हाइब्रिड रणनीतियाँ डाउनटाइम जोखिम को कम करती हैं

## भाग 2: स्थानीय बनाम क्लाउड – हाइब्रिड पैटर्न

- बड़े/जटिल प्रॉम्प्ट के लिए क्लाउड बैकअप के साथ स्थानीय-प्रथम
- गोपनीयता-संवेदनशील या ऑफलाइन परिदृश्यों के लिए स्थानीय के साथ क्लाउड-प्रथम
- कार्य प्रकार के अनुसार रूट करें (कोड-जेन के लिए DeepSeek, सामान्य चैट के लिए Phi/Qwen)

## भाग 3: Chainlit के साथ RAG चैट ऐप (मिनिमल)

डिपेंडेंसी इंस्टॉल करें:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

रन करें:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

विस्तार करें: एक साधारण रिट्रीवर (स्थानीय फाइलें) जोड़ें और प्राप्त संदर्भ को उपयोगकर्ता प्रॉम्प्ट में जोड़ें।

## भाग 4: WebGPU इन्फरेंस (Heads-up)

WebGPU का उपयोग करके छोटे मॉडल को सीधे ब्राउज़र में चलाएं। यह गोपनीयता-प्रथम डेमो और शून्य-इंस्टॉल अनुभवों के लिए आदर्श है। नीचे ONNX Runtime Web का उपयोग करके WebGPU निष्पादन प्रदाता के साथ एक चरण-दर-चरण न्यूनतम उदाहरण दिया गया है।

1) WebGPU समर्थन की जांच करें
- Chromium ब्राउज़र: chrome://gpu → पुष्टि करें कि “WebGPU” सक्षम है
- प्रोग्रामेटिक जांच (हम कोड में भी जांच करेंगे): `if (!('gpu' in navigator)) { /* कोई WebGPU नहीं */ }`

2) एक न्यूनतम प्रोजेक्ट बनाएं
एक फ़ोल्डर और दो फाइलें बनाएं: `index.html` और `main.js`।

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) स्थानीय रूप से सर्व करें (Windows cmd.exe)
एक साधारण स्थिर सर्वर का उपयोग करें ताकि ब्राउज़र मॉडल को प्राप्त कर सके।

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

अपने ब्राउज़र में http://localhost:5173 खोलें। आपको प्रारंभिक लॉग, WebGPU के साथ सत्र निर्माण, और एक argmax भविष्यवाणी दिखाई देनी चाहिए।

4) समस्या निवारण
- यदि WebGPU अनुपलब्ध है: Chrome/Edge को अपडेट करें और सुनिश्चित करें कि GPU ड्राइवर वर्तमान हैं, फिर chrome://flags में “Enable WebGPU” जांचें।
- यदि CORS या fetch त्रुटियाँ होती हैं: सुनिश्चित करें कि आप फाइलों को http:// (file:// नहीं) पर सर्व करते हैं और मॉडल URL क्रॉस-ओरिजिन अनुरोधों की अनुमति देता है।
- CPU पर वापस जाएं: `executionProviders: ['wasm']` बदलें और बेसलाइन व्यवहार की पुष्टि करें।

5) अगले चरण
- एक डोमेन-विशिष्ट ONNX मॉडल (जैसे, इमेज क्लासिफिकेशन या एक छोटा टेक्स्ट मॉडल) स्वैप करें।
- वास्तविक इनपुट के लिए प्रीप्रोसेसिंग/पोस्टप्रोसेसिंग लॉजिक जोड़ें।
- बड़े मॉडल या उत्पादन विलंबता के लिए, Foundry Local या ONNX Runtime Server को प्राथमिकता दें।

## भाग 5: Open WebUI + Foundry Local (चरण-दर-चरण)

यह Open WebUI को Foundry Local के OpenAI-संगत एंडपॉइंट से जोड़ता है ताकि एक स्थानीय चैट UI बनाया जा सके।

1) आवश्यकताएँ
- Foundry Local इंस्टॉल और कार्यशील (`foundry --version`)
- एक मॉडल स्थानीय रूप से चलाने के लिए तैयार (जैसे, `phi-4-mini`)
- Docker Desktop इंस्टॉल (Open WebUI के लिए अनुशंसित)

2) Foundry Local के साथ एक मॉडल शुरू करें
```powershell
foundry model run phi-4-mini
```
यह `http://localhost:8000` पर एक OpenAI-संगत API को एक्सपोज़ करता है।

3) Open WebUI शुरू करें (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
नोट्स:
- Windows पर, `host.docker.internal` कंटेनर को आपके होस्ट तक `localhost` पर पहुँचने देता है।
- हम `OPENAI_API_BASE_URL` को Foundry Local के एंडपॉइंट और एक डमी `OPENAI_API_KEY` सेट करते हैं।

4) Open WebUI UI से कॉन्फ़िगर करें (वैकल्पिक)
- http://localhost:3000 पर ब्राउज़ करें
- प्रारंभिक सेटअप पूरा करें (एडमिन उपयोगकर्ता)
- सेटिंग्स → मॉडल/प्रोवाइडर्स पर जाएं
- बेस URL सेट करें: `http://host.docker.internal:8000/v1`
- API Key सेट करें: `local-key` (प्लेसहोल्डर)
- सेव करें

5) एक टेस्ट प्रॉम्प्ट चलाएं
- Open WebUI चैट में, मॉडल नाम `phi-4-mini` चुनें या दर्ज करें
- प्रॉम्प्ट: “ऑन-डिवाइस AI इन्फरेंस के पाँच लाभ सूचीबद्ध करें।”
- आपको अपने स्थानीय मॉडल से एक प्रतिक्रिया स्ट्रीम होती हुई दिखाई देनी चाहिए

6) समस्या निवारण
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) वैकल्पिक: Open WebUI डेटा को स्थायी बनाएं
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## हैंड्स-ऑन चेकलिस्ट
- [ ] स्थानीय रूप से SLM और LLM के बीच प्रतिक्रिया/विलंबता की तुलना करें
- [ ] कम से कम दो मॉडलों के खिलाफ Chainlit डेमो चलाएं
- [ ] Open WebUI को अपने स्थानीय एंडपॉइंट से कनेक्ट करें और परीक्षण करें

## अगले चरण
- सत्र 5 में एजेंट वर्कफ़्लो के लिए तैयारी करें
- उन परिदृश्यों की पहचान करें जहाँ हाइब्रिड स्थानीय/क्लाउड ROI में सुधार करता है

---

