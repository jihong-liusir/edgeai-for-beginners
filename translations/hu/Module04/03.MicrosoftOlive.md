<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T23:25:28+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "hu"
}
-->
# 3. szakasz: Microsoft Olive Optimalizációs Eszközkészlet

## Tartalomjegyzék
1. [Bevezetés](../../../Module04)
2. [Mi az a Microsoft Olive?](../../../Module04)
3. [Telepítés](../../../Module04)
4. [Gyors kezdési útmutató](../../../Module04)
5. [Példa: Qwen3 átalakítása ONNX INT4 formátumba](../../../Module04)
6. [Haladó használat](../../../Module04)
7. [Legjobb gyakorlatok](../../../Module04)
8. [Hibaelhárítás](../../../Module04)
9. [További források](../../../Module04)

## Bevezetés

A Microsoft Olive egy erőteljes, könnyen használható, hardverre optimalizált modelloptimalizációs eszközkészlet, amely leegyszerűsíti a gépi tanulási modellek különböző hardverplatformokra történő telepítésének optimalizálását. Legyen szó CPU-król, GPU-król vagy speciális AI gyorsítókról, az Olive segít elérni az optimális teljesítményt a modell pontosságának megőrzése mellett.

## Mi az a Microsoft Olive?

Az Olive egy könnyen használható, hardverre optimalizált modelloptimalizációs eszköz, amely iparágvezető technikákat ötvöz a modellkompresszió, optimalizáció és fordítás terén. Az ONNX Runtime-mal működik, mint egy teljes körű inferencia optimalizációs megoldás.

### Főbb jellemzők

- **Hardverre optimalizált megoldások**: Automatikusan kiválasztja a legjobb optimalizációs technikákat a célhardverhez
- **40+ beépített optimalizációs komponens**: Modellkompressziót, kvantálást, gráfoptimalizációt és még sok mást tartalmaz
- **Egyszerű CLI felület**: Könnyen használható parancsok a gyakori optimalizációs feladatokhoz
- **Több keretrendszer támogatása**: Működik PyTorch, Hugging Face modellekkel és ONNX-szel
- **Népszerű modellek támogatása**: Az Olive automatikusan optimalizálja a népszerű modellarchitektúrákat, mint például Llama, Phi, Qwen, Gemma stb.

### Előnyök

- **Fejlesztési idő csökkentése**: Nem kell manuálisan kísérletezni különböző optimalizációs technikákkal
- **Teljesítményjavulás**: Jelentős sebességnövekedés (akár 6x bizonyos esetekben)
- **Keresztplatformos telepítés**: Az optimalizált modellek különböző hardvereken és operációs rendszereken működnek
- **Pontosság megőrzése**: Az optimalizációk megőrzik a modell minőségét, miközben javítják a teljesítményt

## Telepítés

### Előfeltételek

- Python 3.8 vagy újabb
- pip csomagkezelő
- Virtuális környezet (ajánlott)

### Alapvető telepítés

Hozzon létre és aktiváljon egy virtuális környezetet:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Telepítse az Olive-ot automatikus optimalizációs funkciókkal:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcionális függőségek

Az Olive különféle opcionális függőségeket kínál további funkciókhoz:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Telepítés ellenőrzése

```bash
olive --help
```

Ha sikeres, az Olive CLI súgóüzenetét kell látnia.

## Gyors kezdési útmutató

### Első optimalizáció

Optimalizáljunk egy kis nyelvi modellt az Olive automatikus optimalizációs funkciójával:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mit csinál ez a parancs?

Az optimalizációs folyamat magában foglalja: a modell helyi gyorsítótárból történő beszerzését, az ONNX gráf rögzítését és a súlyok ONNX adatfájlba történő mentését, az ONNX gráf optimalizálását, valamint a modell int4 formátumba történő kvantálását az RTN módszerrel.

### Parancsparaméterek magyarázata

- `--model_name_or_path`: Hugging Face modellazonosító vagy helyi elérési út
- `--output_path`: Az optimalizált modell mentési könyvtára
- `--device`: Célhardver (cpu, gpu)
- `--provider`: Végrehajtási szolgáltató (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI használata inferenciához
- `--precision`: Kvantálási pontosság (int4, int8, fp16)
- `--log_level`: Naplózási részletesség (0=minimális, 1=részletes)

## Példa: Qwen3 átalakítása ONNX INT4 formátumba

A Hugging Face példája alapján: [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), így optimalizálhat egy Qwen3 modellt:

### 1. lépés: Modell letöltése (opcionális)

A letöltési idő minimalizálása érdekében csak a szükséges fájlokat gyorsítótárazza:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 2. lépés: Qwen3 modell optimalizálása

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 3. lépés: Az optimalizált modell tesztelése

Hozzon létre egy egyszerű Python szkriptet az optimalizált modell teszteléséhez:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Kimeneti struktúra

Az optimalizáció után a kimeneti könyvtár tartalmazza:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Haladó használat

### Konfigurációs fájlok

Komplexebb optimalizációs munkafolyamatokhoz JSON konfigurációs fájlokat használhat:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Futtatás konfigurációval:

```bash
olive run --config config.json
```

### GPU optimalizáció

CUDA GPU optimalizációhoz:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) esetén:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finomhangolás Olive-val

Az Olive támogatja a modellek finomhangolását is:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Legjobb gyakorlatok

### 1. Modell kiválasztása
- Kezdje kisebb modellekkel teszteléshez (pl. 0.5B-7B paraméterek)
- Győződjön meg róla, hogy a célmodell architektúrája támogatott az Olive által

### 2. Hardver szempontok
- Igazítsa az optimalizációs célt a telepítési hardverhez
- Használjon GPU optimalizációt, ha CUDA-kompatibilis hardvere van
- Fontolja meg a DirectML használatát Windows gépeken integrált grafikával

### 3. Pontosság kiválasztása
- **INT4**: Maximális kompresszió, enyhe pontosságvesztés
- **INT8**: Jó egyensúly méret és pontosság között
- **FP16**: Minimális pontosságvesztés, mérsékelt méretcsökkentés

### 4. Tesztelés és validáció
- Mindig tesztelje az optimalizált modelleket saját konkrét felhasználási eseteivel
- Hasonlítsa össze a teljesítménymutatókat (késleltetés, átbocsátás, pontosság)
- Használjon reprezentatív bemeneti adatokat az értékeléshez

### 5. Iteratív optimalizáció
- Kezdje automatikus optimalizációval gyors eredményekért
- Használjon konfigurációs fájlokat a finomhangolt vezérléshez
- Kísérletezzen különböző optimalizációs lépésekkel

## Hibaelhárítás

### Gyakori problémák

#### 1. Telepítési problémák
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU problémák
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Memóriaproblémák
- Használjon kisebb batch méreteket az optimalizáció során
- Próbálja ki a kvantálást magasabb pontossággal először (int8 az int4 helyett)
- Biztosítson elegendő lemezterületet a modell gyorsítótárazásához

#### 4. Modell betöltési hibák
- Ellenőrizze a modell elérési útját és hozzáférési jogosultságait
- Ellenőrizze, hogy a modell `trust_remote_code=True` beállítást igényel-e
- Győződjön meg róla, hogy minden szükséges modellfájl le van töltve

### Segítség kérése

- **Dokumentáció**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub hibák**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Példák**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## További források

### Hivatalos linkek
- **GitHub repó**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime dokumentáció**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face példa**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Közösségi példák
- **Jupyter jegyzetfüzetek**: Elérhetők az Olive GitHub repóban — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code bővítmény**: AI Toolkit for VS Code áttekintés — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogbejegyzések**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Kapcsolódó eszközök
- **ONNX Runtime**: Nagy teljesítményű inferencia motor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Számos kompatibilis modell forrása — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Felhőalapú optimalizációs munkafolyamatok — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Mi következik

- [04: OpenVINO Toolkit Optimalizációs Eszközkészlet](./04.openvino.md)

---

