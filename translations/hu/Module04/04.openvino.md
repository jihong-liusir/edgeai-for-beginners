<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T16:53:00+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "hu"
}
-->
# 4. szakasz: OpenVINO Toolkit Optimalizációs Csomag

## Tartalomjegyzék
1. [Bevezetés](../../../Module04)
2. [Mi az OpenVINO?](../../../Module04)
3. [Telepítés](../../../Module04)
4. [Gyors kezdési útmutató](../../../Module04)
5. [Példa: Modellek konvertálása és optimalizálása OpenVINO-val](../../../Module04)
6. [Haladó használat](../../../Module04)
7. [Legjobb gyakorlatok](../../../Module04)
8. [Hibaelhárítás](../../../Module04)
9. [További források](../../../Module04)

## Bevezetés

Az OpenVINO (Open Visual Inference and Neural Network Optimization) az Intel nyílt forráskódú eszközkészlete, amely lehetővé teszi hatékony AI megoldások telepítését felhőben, helyszíni környezetben és edge eszközökön. Akár CPU-kat, GPU-kat, VPU-kat vagy speciális AI gyorsítókat céloz meg, az OpenVINO átfogó optimalizációs képességeket kínál, miközben megőrzi a modellek pontosságát és lehetővé teszi a platformok közötti telepítést.

## Mi az OpenVINO?

Az OpenVINO egy nyílt forráskódú eszközkészlet, amely lehetővé teszi a fejlesztők számára, hogy hatékonyan optimalizálják, konvertálják és telepítsék AI modelleket különböző hardverplatformokon. Három fő komponensből áll: OpenVINO Runtime az inferenciához, Neural Network Compression Framework (NNCF) a modellek optimalizálásához, és OpenVINO Model Server a skálázható telepítéshez.

### Főbb jellemzők

- **Platformok közötti telepítés**: Támogatja a Linuxot, Windowst és macOS-t Python, C++ és C API-kkal
- **Hardvergyorsítás**: Automatikus eszközfelismerés és optimalizáció CPU, GPU, VPU és AI gyorsítók számára
- **Modellek tömörítési keretrendszere**: Fejlett kvantálási, metszési és optimalizációs technikák az NNCF segítségével
- **Keretrendszer-kompatibilitás**: Közvetlen támogatás TensorFlow, ONNX, PaddlePaddle és PyTorch modellekhez
- **Generatív AI támogatás**: Speciális OpenVINO GenAI nagy nyelvi modellek és generatív AI alkalmazások telepítéséhez

### Előnyök

- **Teljesítményoptimalizáció**: Jelentős sebességnövekedés minimális pontosságvesztéssel
- **Csökkentett telepítési lábnyom**: Minimális külső függőségek egyszerűsítik a telepítést
- **Gyorsabb indítási idő**: Optimalizált modellbetöltés és gyorsítótárazás a gyorsabb alkalmazásindításhoz
- **Skálázható telepítés**: Edge eszközöktől a felhő infrastruktúráig következetes API-kkal
- **Kész a gyártásra**: Vállalati szintű megbízhatóság átfogó dokumentációval és közösségi támogatással

## Telepítés

### Előfeltételek

- Python 3.8 vagy újabb
- pip csomagkezelő
- Virtuális környezet (ajánlott)
- Kompatibilis hardver (Intel CPU-k ajánlottak, de különböző architektúrákat támogat)

### Alapvető telepítés

Hozzon létre és aktiváljon egy virtuális környezetet:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Telepítse az OpenVINO Runtime-ot:

```bash
pip install openvino
```

Telepítse az NNCF-et a modellek optimalizálásához:

```bash
pip install nncf
```

### OpenVINO GenAI telepítése

Generatív AI alkalmazásokhoz:

```bash
pip install openvino-genai
```

### Opcionális függőségek

További csomagok specifikus felhasználási esetekhez:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Telepítés ellenőrzése

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Sikeres telepítés esetén az OpenVINO verzióinformációinak meg kell jelenniük.

## Gyors kezdési útmutató

### Az első modell optimalizálása

Konvertáljunk és optimalizáljunk egy Hugging Face modellt az OpenVINO segítségével:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Mit csinál ez a folyamat?

Az optimalizációs munkafolyamat magában foglalja: az eredeti modell betöltését a Hugging Face-től, konvertálást OpenVINO Intermediate Representation (IR) formátumba, alapértelmezett optimalizációk alkalmazását, és a célhardverre való fordítást.

### Fontos paraméterek magyarázata

- `export=True`: A modellt OpenVINO IR formátumba konvertálja
- `compile=False`: A fordítást futásidőre halasztja a rugalmasság érdekében
- `device`: Célhardver ("CPU", "GPU", "AUTO" az automatikus kiválasztáshoz)
- `save_pretrained()`: Az optimalizált modellt menti újrafelhasználásra

## Példa: Modellek konvertálása és optimalizálása OpenVINO-val

### 1. lépés: Modell konvertálása NNCF kvantálással

Így alkalmazhatunk utólagos kvantálást az NNCF segítségével:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### 2. lépés: Haladó optimalizáció súlytömörítéssel

Transzformer-alapú modellekhez alkalmazzon súlytömörítést:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### 3. lépés: Inferencia optimalizált modellel

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Kimeneti struktúra

Az optimalizáció után a modell könyvtára tartalmazza:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Haladó használat

### Konfiguráció NNCF YAML-lel

Komplex optimalizációs munkafolyamatokhoz használjon NNCF konfigurációs fájlokat:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Konfiguráció alkalmazása:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU optimalizáció

GPU gyorsításhoz:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Batch feldolgozás optimalizációja

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modell szerver telepítése

Optimalizált modellek telepítése az OpenVINO Model Server segítségével:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Klienskód a modell szerverhez:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Legjobb gyakorlatok

### 1. Modell kiválasztása és előkészítése
- Használjon támogatott keretrendszerekből származó modelleket (PyTorch, TensorFlow, ONNX)
- Biztosítsa, hogy a modell bemenetei fix vagy ismert dinamikus alakúak legyenek
- Teszteljen reprezentatív adathalmazokkal kalibrációhoz

### 2. Optimalizációs stratégia kiválasztása
- **Utólagos kvantálás**: Kezdje itt gyors optimalizációhoz
- **Súlytömörítés**: Ideális nagy nyelvi modellekhez és transzformerekhez
- **Kvantálás-tudatos tanítás**: Használja, ha a pontosság kritikus

### 3. Hardver-specifikus optimalizáció
- **CPU**: Használjon INT8 kvantálást kiegyensúlyozott teljesítményhez
- **GPU**: Használja az FP16 precíziót és batch feldolgozást
- **VPU**: Koncentráljon a modell egyszerűsítésére és rétegek összevonására

### 4. Teljesítményhangolás
- **Átviteli mód**: Nagy mennyiségű batch feldolgozáshoz
- **Késleltetési mód**: Valós idejű interaktív alkalmazásokhoz
- **AUTO eszköz**: Hagyja, hogy az OpenVINO válassza ki az optimális hardvert

### 5. Memóriakezelés
- Használja a dinamikus alakokat körültekintően, hogy elkerülje a memória túlterhelést
- Valósítson meg modell gyorsítótárazást a gyorsabb betöltésekhez
- Figyelje a memóriahasználatot az optimalizáció során

### 6. Pontosság ellenőrzése
- Mindig ellenőrizze az optimalizált modelleket az eredeti teljesítményhez képest
- Használjon reprezentatív tesztadatokat az értékeléshez
- Fontolja meg a fokozatos optimalizációt (kezdjen konzervatív beállításokkal)

## Hibaelhárítás

### Gyakori problémák

#### 1. Telepítési problémák
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modell konvertálási hibák
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Teljesítményproblémák
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Memóriaproblémák
- Csökkentse a modell batch méretét az optimalizáció során
- Használjon streaminget nagy adathalmazokhoz
- Engedélyezze a modell gyorsítótárazást: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Pontosságromlás
- Használjon magasabb precíziót (INT8 helyett INT4)
- Növelje a kalibrációs adathalmaz méretét
- Alkalmazzon vegyes precíziós optimalizációt

### Teljesítményfigyelés

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Segítségkérés

- **Dokumentáció**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub hibák**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Közösségi fórum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## További források

### Hivatalos linkek
- **OpenVINO honlap**: [openvino.ai](https://openvino.ai/)
- **GitHub repó**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF repó**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Tanulási források
- **OpenVINO jegyzetfüzetek**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Gyors kezdési útmutató**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimalizációs útmutató**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integrációs eszközök
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Teljesítményértékelések
- **Hivatalos értékelések**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Közösségi példák
- **Jupyter jegyzetfüzetek**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Átfogó oktatóanyagok az OpenVINO jegyzetfüzetek repójában
- **Mintapéldák**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Valós példák különböző területeken (számítógépes látás, NLP, hang)
- **Blogbejegyzések**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI és közösségi blogbejegyzések részletes esettanulmányokkal

### Kapcsolódó eszközök
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - További optimalizációs technikák Intel hardverhez
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Mobil és edge telepítési összehasonlításokhoz
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Platformok közötti inferencia motor alternatívák

## ➡️ Mi következik

- [05: Apple MLX Framework mélyreható elemzés](./05.AppleMLX.md)

---

**Felelősség kizárása**:  
Ez a dokumentum az AI fordítási szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.