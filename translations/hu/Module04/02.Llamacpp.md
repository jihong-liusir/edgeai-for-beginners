<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T17:02:30+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "hu"
}
-->
# 2. szakasz: Llama.cpp megvalósítási útmutató

## Tartalomjegyzék
1. [Bevezetés](../../../Module04)
2. [Mi az a Llama.cpp?](../../../Module04)
3. [Telepítés](../../../Module04)
4. [Forrásból való építés](../../../Module04)
5. [Modell kvantálás](../../../Module04)
6. [Alapvető használat](../../../Module04)
7. [Haladó funkciók](../../../Module04)
8. [Python integráció](../../../Module04)
9. [Hibakeresés](../../../Module04)
10. [Legjobb gyakorlatok](../../../Module04)

## Bevezetés

Ez az átfogó útmutató mindent bemutat, amit a Llama.cpp-ről tudni kell, az alapvető telepítéstől kezdve a haladó használati forgatókönyvekig. A Llama.cpp egy erőteljes C++ megvalósítás, amely lehetővé teszi a Nagy Nyelvi Modellek (LLM-ek) hatékony futtatását minimális beállítással és kiváló teljesítménnyel különböző hardverkonfigurációk esetén.

## Mi az a Llama.cpp?

A Llama.cpp egy C/C++ nyelven írt LLM inferencia keretrendszer, amely lehetővé teszi nagy nyelvi modellek helyi futtatását minimális beállítással és csúcstechnológiás teljesítménnyel számos hardveren. Főbb jellemzői:

### Főbb funkciók
- **Egyszerű C/C++ megvalósítás** függőségek nélkül
- **Platformfüggetlen kompatibilitás** (Windows, macOS, Linux)
- **Hardveroptimalizáció** különböző architektúrákhoz
- **Kvantálás támogatása** (1,5 bit-től 8 bitig terjedő egész kvantálás)
- **CPU és GPU gyorsítás** támogatása
- **Memóriahatékonyság** korlátozott környezetekhez

### Előnyök
- Hatékonyan fut CPU-n, speciális hardver nélkül
- Több GPU backend támogatása (CUDA, Metal, OpenCL, Vulkan)
- Könnyű és hordozható
- Az Apple silicon kiemelt támogatást élvez - optimalizálva ARM NEON, Accelerate és Metal keretrendszerekkel
- Különböző kvantálási szintek támogatása a csökkentett memóriahasználat érdekében

## Telepítés

### 1. módszer: Előre elkészített binárisok (Kezdőknek ajánlott)

#### Letöltés a GitHub Releases oldalról
1. Látogasson el a [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) oldalra
2. Töltse le a rendszerének megfelelő binárist:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` Windowsra
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` macOS-re
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` Linuxra

3. Csomagolja ki az archívumot, és adja hozzá a könyvtárat a rendszer PATH-jához

#### Csomagkezelők használata

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Különböző disztribúciók):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### 2. módszer: Python csomag (llama-cpp-python)

#### Alapvető telepítés
```bash
pip install llama-cpp-python
```

#### Hardvergyorsítással
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Forrásból való építés

### Előfeltételek

**Rendszerkövetelmények:**
- C++ fordító (GCC, Clang vagy MSVC)
- CMake (3.14-es vagy újabb verzió)
- Git
- Építési eszközök a platformjához

**Előfeltételek telepítése:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Telepítse a Visual Studio 2022-t C++ fejlesztési eszközökkel
- Telepítse a CMake-et a hivatalos weboldalról
- Telepítse a Git-et

### Alapvető építési folyamat

1. **Tároló klónozása:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Építés konfigurálása:**
```bash
cmake -B build
```

3. **Projekt építése:**
```bash
cmake --build build --config Release
```

Gyorsabb fordításhoz használjon párhuzamos feladatokat:
```bash
cmake --build build --config Release -j 8
```

### Hardver-specifikus építések

#### CUDA támogatás (NVIDIA GPU-k)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal támogatás (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS támogatás (CPU optimalizáció)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan támogatás
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Haladó építési opciók

#### Hibakeresési építés
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### További funkciókkal
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Modell kvantálás

### GGUF formátum megértése

A GGUF (Generalized GGML Unified Format) egy optimalizált fájlformátum, amelyet nagy nyelvi modellek hatékony futtatására terveztek a Llama.cpp és más keretrendszerek segítségével. Előnyei:

- Standardizált modell súlytárolás
- Javított kompatibilitás a platformok között
- Fokozott teljesítmény
- Hatékony metaadat-kezelés

### Kvantálási típusok

A Llama.cpp különböző kvantálási szinteket támogat:

| Típus | Bitek | Leírás | Felhasználási eset |
|-------|-------|--------|--------------------|
| F16 | 16 | Fél precizitás | Magas minőség, nagy memória |
| Q8_0 | 8 | 8 bites kvantálás | Jó egyensúly |
| Q4_0 | 4 | 4 bites kvantálás | Mérsékelt minőség, kisebb méret |
| Q2_K | 2 | 2 bites kvantálás | Legkisebb méret, alacsonyabb minőség |

### Modellek konvertálása

#### PyTorch-ból GGUF-ba
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Közvetlen letöltés a Hugging Face-ről
Számos modell elérhető GGUF formátumban a Hugging Face-en:
- Keressen "GGUF" nevű modelleket
- Töltse le a megfelelő kvantálási szintet
- Használja közvetlenül a llama.cpp-vel

## Alapvető használat

### Parancssori felület

#### Egyszerű szöveg generálás
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Modellek használata a Hugging Face-ről
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Szerver mód
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Gyakori paraméterek

| Paraméter | Leírás | Példa |
|-----------|--------|-------|
| `-m` | Modell fájl elérési útja | `-m model.gguf` |
| `-p` | Szöveges prompt | `-p "Hello world"` |
| `-n` | Generálandó tokenek száma | `-n 100` |
| `-c` | Kontextus mérete | `-c 4096` |
| `-t` | Szálak száma | `-t 8` |
| `-ngl` | GPU rétegek | `-ngl 32` |
| `-temp` | Hőmérséklet | `-temp 0.7` |

### Interaktív mód

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Haladó funkciók

### Szerver API

#### Szerver indítása
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API használata
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Teljesítmény optimalizálás

#### Memóriakezelés
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Többszálúság
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU gyorsítás
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python integráció

### Alapvető használat a llama-cpp-python-nal

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chat felület

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Folyamatos válaszok

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integráció a LangChain-nel

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Hibakeresés

### Gyakori problémák és megoldások

#### Építési hibák

**Hiba: CMake nem található**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Hiba: Fordító nem található**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Futási hibák

**Hiba: Modell betöltése sikertelen**
- Ellenőrizze a modell fájl elérési útját
- Ellenőrizze a fájl jogosultságait
- Győződjön meg elegendő RAM-ról
- Próbáljon ki más kvantálási szinteket

**Hiba: Gyenge teljesítmény**
- Engedélyezze a hardvergyorsítást
- Növelje a szálak számát
- Használjon megfelelő kvantálást
- Ellenőrizze a GPU memóriahasználatot

#### Memória problémák

**Hiba: Memóriahiány**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platform-specifikus problémák

#### Windows
- Használjon MinGW vagy Visual Studio fordítót
- Biztosítsa a PATH megfelelő konfigurációját
- Ellenőrizze az antivírus interferenciát

#### macOS
- Engedélyezze a Metal-t Apple Silicon esetén
- Használja a Rosetta 2-t kompatibilitás érdekében, ha szükséges
- Ellenőrizze az Xcode parancssori eszközöket

#### Linux
- Telepítse a fejlesztési csomagokat
- Ellenőrizze a GPU driver verziókat
- Ellenőrizze a CUDA toolkit telepítését

## Legjobb gyakorlatok

### Modell kiválasztása
1. **Válasszon megfelelő kvantálást** a hardverének alapján
2. **Vegye figyelembe a modell méretét** és a minőség kompromisszumait
3. **Teszteljen különböző modelleket** az adott felhasználási esethez

### Teljesítmény optimalizálás
1. **Használjon GPU gyorsítást**, ha elérhető
2. **Optimalizálja a szálak számát** a CPU-jához
3. **Állítson be megfelelő kontextus méretet** az adott felhasználási esethez
4. **Engedélyezze a memória leképezést** nagy modellekhez

### Éles környezetben való telepítés
1. **Használja a szerver módot** API hozzáféréshez
2. **Valósítson meg megfelelő hibakezelést**
3. **Figyelje az erőforrás-használatot**
4. **Állítson be naplózást és monitorozást**

### Fejlesztési munkafolyamat
1. **Kezdje kisebb modellekkel** teszteléshez
2. **Használjon verziókezelést** a modell konfigurációkhoz
3. **Dokumentálja a konfigurációit**
4. **Teszteljen különböző platformokon**

### Biztonsági szempontok
1. **Ellenőrizze a bemeneti promptokat**
2. **Valósítson meg sebességkorlátozást**
3. **Biztosítsa az API végpontokat**
4. **Figyelje a visszaélési mintákat**

## Következtetés

A Llama.cpp hatékony és rugalmas módot kínál nagy nyelvi modellek helyi futtatására különböző hardverkonfigurációk esetén. Akár AI alkalmazásokat fejleszt, kutatást végez, vagy egyszerűen csak kísérletezik LLM-ekkel, ez a keretrendszer biztosítja a szükséges rugalmasságot és teljesítményt számos felhasználási esethez.

Főbb tanulságok:
- Válassza ki az igényeinek leginkább megfelelő telepítési módot
- Optimalizálja a hardverkonfigurációjához
- Kezdje az alapvető használattal, majd fokozatosan fedezze fel a haladó funkciókat
- Fontolja meg a Python kötéseket az egyszerűbb integráció érdekében
- Kövesse a legjobb gyakorlatokat az éles telepítésekhez

További információkért és frissítésekért látogasson el a [hivatalos Llama.cpp tárolóba](https://github.com/ggml-org/llama.cpp), és tekintse meg az átfogó dokumentációt és a közösségi forrásokat.

## ➡️ Mi következik?

- [03: Microsoft Olive Optimalizációs Suite](./03.MicrosoftOlive.md)

---

**Felelősség kizárása**:  
Ez a dokumentum az AI fordítási szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.