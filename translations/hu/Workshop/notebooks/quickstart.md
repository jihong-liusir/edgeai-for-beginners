<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T21:52:57+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "hu"
}
-->
# Workshop Jegyzetek - Gyorsind√≠t√°si √ötmutat√≥

## Tartalomjegyz√©k

- [El≈ëfelt√©telek](../../../../Workshop/notebooks)
- [Kezdeti Be√°ll√≠t√°s](../../../../Workshop/notebooks)
- [4. Szekci√≥: Modell√∂sszehasonl√≠t√°s](../../../../Workshop/notebooks)
- [5. Szekci√≥: T√∂bb√ºgyn√∂k√∂s Orkesztr√°tor](../../../../Workshop/notebooks)
- [6. Szekci√≥: Sz√°nd√©k-alap√∫ Modellir√°ny√≠t√°s](../../../../Workshop/notebooks)
- [K√∂rnyezeti V√°ltoz√≥k](../../../../Workshop/notebooks)
- [Gyakori Parancsok](../../../../Workshop/notebooks)

---

## El≈ëfelt√©telek

### 1. Foundry Local telep√≠t√©se

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Telep√≠t√©s ellen≈ërz√©se:**
```bash
foundry --version
```

### 2. Python f√ºgg≈ës√©gek telep√≠t√©se

```bash
cd Workshop
pip install -r requirements.txt
```

Vagy telep√≠tse egyenk√©nt:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Kezdeti Be√°ll√≠t√°s

### Foundry Local Szolg√°ltat√°s ind√≠t√°sa

**Sz√ºks√©ges minden jegyzet futtat√°sa el≈ëtt:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

V√°rhat√≥ kimenet:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### Modellek let√∂lt√©se √©s bet√∂lt√©se

A jegyzetek alap√©rtelmez√©s szerint ezeket a modelleket haszn√°lj√°k:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Be√°ll√≠t√°s ellen≈ërz√©se

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## 4. Szekci√≥: Modell√∂sszehasonl√≠t√°s

### C√©l
Kis Nyelvi Modellek (SLM) √©s Nagy Nyelvi Modellek (LLM) teljes√≠tm√©ny√©nek √∂sszehasonl√≠t√°sa.

### Gyors Be√°ll√≠t√°s

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Jegyzet futtat√°sa

1. **Nyissa meg** a `session04_model_compare.ipynb` f√°jlt VS Code-ban vagy Jupyterben
2. **Ind√≠tsa √∫jra a kernelt** (Kernel ‚Üí Restart Kernel)
3. **Futtassa az √∂sszes cell√°t** sorrendben

### F≈ë Konfigur√°ci√≥

**Alap√©rtelmezett modellek:**
- **SLM:** `phi-4-mini` (~4GB RAM, gyorsabb)
- **LLM:** `qwen2.5-3b` (~3GB RAM, mem√≥ria-optimaliz√°lt)

**K√∂rnyezeti v√°ltoz√≥k (opcion√°lis):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### V√°rhat√≥ kimenet

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### Testreszab√°s

**M√°s modellek haszn√°lata:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Egyedi prompt:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Ellen≈ërz≈ëlista

- [ ] A 12. cella a megfelel≈ë modelleket mutatja (phi-4-mini, qwen2.5-3b)
- [ ] A 12. cella a megfelel≈ë v√©gpontot mutatja (port 59959)
- [ ] A 16. cella diagnosztik√°ja sikeres (‚úÖ Szolg√°ltat√°s fut)
- [ ] A 20. cella el≈ëzetes ellen≈ërz√©se sikeres (mindk√©t modell rendben)
- [ ] A 22. cella √∂sszehasonl√≠t√°sa befejez≈ëdik k√©sleltet√©si √©rt√©kekkel
- [ ] A 24. cella valid√°ci√≥ja mutatja üéâ MINDEN ELLEN≈êRZ√âS SIKERES!

### Id≈ëbecsl√©s
- **Els≈ë futtat√°s:** 5-10 perc (bele√©rtve a modellek let√∂lt√©s√©t)
- **K√∂vetkez≈ë futtat√°sok:** 1-2 perc

---

## 5. Szekci√≥: T√∂bb√ºgyn√∂k√∂s Orkesztr√°tor

### C√©l
T√∂bb√ºgyn√∂k√∂s egy√ºttm≈±k√∂d√©s bemutat√°sa a Foundry Local SDK seg√≠ts√©g√©vel - az √ºgyn√∂k√∂k egy√ºtt dolgoznak a kifinomultabb eredm√©nyek √©rdek√©ben.

### Gyors Be√°ll√≠t√°s

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Jegyzet futtat√°sa

1. **Nyissa meg** a `session05_agents_orchestrator.ipynb` f√°jlt
2. **Ind√≠tsa √∫jra a kernelt**
3. **Futtassa az √∂sszes cell√°t** sorrendben

### F≈ë Konfigur√°ci√≥

**Alap√©rtelmezett be√°ll√≠t√°s (ugyanaz a modell mindk√©t √ºgyn√∂kn√©l):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Halad√≥ be√°ll√≠t√°s (k√ºl√∂nb√∂z≈ë modellek):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Architekt√∫ra

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### V√°rhat√≥ kimenet

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Kiterjeszt√©sek

**Tov√°bbi √ºgyn√∂k√∂k hozz√°ad√°sa:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Batch tesztel√©s:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Id≈ëbecsl√©s
- **Els≈ë futtat√°s:** 3-5 perc
- **K√∂vetkez≈ë futtat√°sok:** 1-2 perc k√©rd√©senk√©nt

---

## 6. Szekci√≥: Sz√°nd√©k-alap√∫ Modellir√°ny√≠t√°s

### C√©l
Promptok intelligens ir√°ny√≠t√°sa specializ√°lt modellekhez a sz√°nd√©k felismer√©se alapj√°n.

### Gyors Be√°ll√≠t√°s

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Megjegyz√©s:** A 6. szekci√≥ alap√©rtelmez√©s szerint CPU modelleket haszn√°l a maxim√°lis kompatibilit√°s √©rdek√©ben.

### Jegyzet futtat√°sa

1. **Nyissa meg** a `session06_models_router.ipynb` f√°jlt
2. **Ind√≠tsa √∫jra a kernelt**
3. **Futtassa az √∂sszes cell√°t** sorrendben

### F≈ë Konfigur√°ci√≥

**Alap√©rtelmezett katal√≥gus (CPU modellek):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternat√≠va (GPU modellek):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Sz√°nd√©k Felismer√©s

Az ir√°ny√≠t√≥ regex mint√°kat haszn√°l a sz√°nd√©k felismer√©s√©re:

| Sz√°nd√©k | P√©lda mint√°k | Ir√°ny√≠tott modell |
|---------|--------------|-------------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | Minden m√°s | phi-4-mini-cpu |

### V√°rhat√≥ kimenet

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### Testreszab√°s

**Egyedi sz√°nd√©k hozz√°ad√°sa:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Token k√∂vet√©s enged√©lyez√©se:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### GPU modellekre v√°lt√°s

Ha van 8GB+ VRAM:

1. A **6. cell√°ban** kommentelje ki a CPU katal√≥gust
2. Hagyja j√≥v√° a GPU katal√≥gust
3. T√∂ltse be a GPU modelleket:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Ind√≠tsa √∫jra a kernelt √©s futtassa √∫jra a jegyzetet

### Id≈ëbecsl√©s
- **Els≈ë futtat√°s:** 5-10 perc (modellek bet√∂lt√©se)
- **K√∂vetkez≈ë futtat√°sok:** 30-60 m√°sodperc tesztenk√©nt

---

## K√∂rnyezeti V√°ltoz√≥k

### Glob√°lis Konfigur√°ci√≥

√Åll√≠tsa be Jupyter/VS Code ind√≠t√°sa el≈ëtt:

**Windows (Command Prompt):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Jegyzeten bel√ºli konfigur√°ci√≥

√Åll√≠tsa be b√°rmely jegyzet elej√©n:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Gyakori Parancsok

### Szolg√°ltat√°skezel√©s

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Modellkezel√©s

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### V√©gpontok tesztel√©se

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Diagnosztikai parancsok

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Legjobb Gyakorlatok

### Miel≈ëtt b√°rmely jegyzetet elind√≠tana

1. **Ellen≈ërizze, hogy a szolg√°ltat√°s fut-e:**
   ```bash
   foundry service status
   ```

2. **Gy≈ëz≈ëdj√∂n meg r√≥la, hogy a modellek bet√∂ltve vannak:**
   ```bash
   foundry model ls
   ```

3. **Ind√≠tsa √∫jra a jegyzet kernelt**, ha √∫jra futtatja

4. **T√∂r√∂lje az √∂sszes kimenetet** a tiszta futtat√°s √©rdek√©ben

### Er≈ëforr√°s-kezel√©s

1. **Haszn√°ljon CPU modelleket alap√©rtelmez√©s szerint** a kompatibilit√°s √©rdek√©ben
2. **V√°ltson GPU modellekre**, ha van 8GB+ VRAM
3. **Z√°rja be m√°s GPU alkalmaz√°sokat** futtat√°s el≈ëtt
4. **Tartsa a szolg√°ltat√°st futva** a jegyzetek k√∂z√∂tt
5. **Figyelje az er≈ëforr√°s-haszn√°latot** a Feladatkezel≈ëvel / nvidia-smi-vel

### Hibakeres√©s

1. **Mindig ellen≈ërizze el≈ësz√∂r a szolg√°ltat√°st**, miel≈ëtt a k√≥dot hibakeresn√©
2. **Ind√≠tsa √∫jra a kernelt**, ha elavult konfigur√°ci√≥t l√°t
3. **Futtassa √∫jra a diagnosztikai cell√°kat** b√°rmilyen v√°ltoztat√°s ut√°n
4. **Ellen≈ërizze, hogy a modellnevek** megegyeznek a bet√∂lt√∂tt modellekkel
5. **Gy≈ëz≈ëdj√∂n meg r√≥la, hogy a v√©gpont portja** megegyezik a szolg√°ltat√°s √°llapot√°val

---

## Gyors Referencia: Modell Aliasok

### Gyakori Modellek

| Alias | M√©ret | Legjobb Felhaszn√°l√°s | RAM/VRAM | Vari√°nsok |
|-------|-------|-----------------------|----------|----------|
| `phi-4-mini` | ~4B | √Åltal√°nos chat, √∂sszegz√©s | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | K√≥dgener√°l√°s, refaktor√°l√°s | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | √Åltal√°nos feladatok, hat√©kony | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Gyors, alacsony er≈ëforr√°s | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Oszt√°lyoz√°s, minim√°lis er≈ëforr√°s | 1-2GB | `-cpu`, `-cuda-gpu` |

### Vari√°ns Elnevez√©s

- **Alapn√©v** (pl. `phi-4-mini`): Automatikusan kiv√°lasztja a legjobb vari√°nst a hardver√©hez
- **`-cpu`**: CPU-optimaliz√°lt, mindenhol m≈±k√∂dik
- **`-cuda-gpu`**: NVIDIA GPU optimaliz√°lt, 8GB+ VRAM sz√ºks√©ges
- **`-npu`**: Qualcomm NPU optimaliz√°lt, NPU driverek sz√ºks√©gesek

**Aj√°nl√°s:** Haszn√°lja az alapneveket (suffix n√©lk√ºl), √©s hagyja, hogy a Foundry Local automatikusan kiv√°lassza a legjobb vari√°nst.

---

## Siker Mutat√≥k

K√©szen √°ll, ha:

‚úÖ `foundry service status` "running"-ot mutat  
‚úÖ `foundry model ls` megjelen√≠ti a sz√ºks√©ges modelleket  
‚úÖ Szolg√°ltat√°s el√©rhet≈ë a megfelel≈ë v√©gponton  
‚úÖ Eg√©szs√©g√ºgyi ellen≈ërz√©s 200 OK-t ad vissza  
‚úÖ Jegyzet diagnosztikai cell√°i sikeresek  
‚úÖ Nincs kapcsolat hiba a kimenetben  

---

## Seg√≠ts√©g K√©r√©se

### Dokument√°ci√≥
- **F≈ë Repository**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLI Referencia**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **Hibakeres√©s**: L√°sd `troubleshooting.md` ebben a k√∂nyvt√°rban

### GitHub Hib√°k
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Utols√≥ friss√≠t√©s:** 2025. okt√≥ber 8.  
**Verzi√≥:** Workshop Jegyzetek 2.0

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI ford√≠t√°si szolg√°ltat√°s seg√≠ts√©g√©vel ker√ºlt leford√≠t√°sra. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.