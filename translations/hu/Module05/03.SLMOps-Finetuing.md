<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T17:12:07+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "hu"
}
-->
# 3. szakasz: Finomhangolás - Modellek testreszabása specifikus feladatokra

## Tartalomjegyzék
1. [Bevezetés a finomhangolásba](../../../Module05)
2. [Miért fontos a finomhangolás?](../../../Module05)
3. [A finomhangolás típusai](../../../Module05)
4. [Finomhangolás a Microsoft Olive segítségével](../../../Module05)
5. [Gyakorlati példák](../../../Module05)
6. [Legjobb gyakorlatok és irányelvek](../../../Module05)
7. [Haladó technikák](../../../Module05)
8. [Értékelés és nyomon követés](../../../Module05)
9. [Gyakori kihívások és megoldások](../../../Module05)
10. [Összegzés](../../../Module05)

## Bevezetés a finomhangolásba

**Finomhangolás** egy hatékony gépi tanulási technika, amely lehetővé teszi, hogy egy előre betanított modellt specifikus feladatokra vagy speciális adathalmazokra adaptáljunk. Ahelyett, hogy nulláról kezdenénk a modell tanítását, a finomhangolás kihasználja az előre betanított modell által már elsajátított tudást, és azt az adott felhasználási területhez igazítja.

### Mi az a finomhangolás?

A finomhangolás a **transzfer tanulás** egyik formája, amely során:
- Egy előre betanított modellel kezdünk, amely általános mintázatokat tanult nagy adathalmazokból
- A modell belső paramétereit az adott adathalmazunkkal módosítjuk
- Megtartjuk az értékes tudást, miközben a modellt az adott feladatra specializáljuk

Gondoljunk rá úgy, mint egy tapasztalt séf tanítására egy új konyha stílusára – már érti a főzés alapjait, de meg kell tanulnia az új technikákat és ízeket.

### Fő előnyök

- **Időhatékonyság**: Jelentősen gyorsabb, mint nulláról kezdeni a tanítást
- **Adathatékonyság**: Kisebb adathalmazokkal is jó teljesítményt érhetünk el
- **Költséghatékonyság**: Alacsonyabb számítási igények
- **Jobb teljesítmény**: Gyakran jobb eredményeket ér el, mint a nulláról történő tanítás
- **Erőforrás-optimalizálás**: Erőteljes AI-t tesz elérhetővé kisebb csapatok és szervezetek számára

## Miért fontos a finomhangolás?

### Valós alkalmazások

A finomhangolás számos helyzetben elengedhetetlen:

**1. Domain adaptáció**
- Orvosi AI: Általános nyelvi modellek adaptálása orvosi terminológiára és klinikai jegyzetekre
- Jogi technológia: Modellek specializálása jogi dokumentumok elemzésére és szerződések áttekintésére
- Pénzügyi szolgáltatások: Modellek testreszabása pénzügyi jelentések elemzésére és kockázatértékelésre

**2. Feladat specializáció**
- Tartalomgenerálás: Finomhangolás specifikus írásstílusokra vagy hangnemekre
- Kódgenerálás: Modellek adaptálása bizonyos programozási nyelvekre vagy keretrendszerekre
- Fordítás: Teljesítmény javítása specifikus nyelvpárok vagy technikai területek esetén

**3. Vállalati alkalmazások**
- Ügyfélszolgálat: Chatbotok létrehozása, amelyek megértik a vállalati specifikus terminológiát
- Belső dokumentáció: AI asszisztensek építése, amelyek ismerik a szervezeti folyamatokat
- Iparspecifikus megoldások: Modellek fejlesztése, amelyek értik az iparági zsargont és munkafolyamatokat

## A finomhangolás típusai

### 1. Teljes finomhangolás (Instrukció alapú finomhangolás)

Teljes finomhangolás során a modell összes paraméterét frissítjük a tanítás során. Ez a megközelítés:
- Maximális rugalmasságot és teljesítmény potenciált biztosít
- Jelentős számítási erőforrásokat igényel
- Teljesen új modell verziót eredményez
- Ideális, ha nagy mennyiségű tanítási adat és számítási kapacitás áll rendelkezésre

### 2. Paraméter-hatékony finomhangolás (PEFT)

A PEFT módszerek csak a paraméterek egy kis részét frissítik, így hatékonyabbá teszik a folyamatot:

#### Low-Rank Adaptation (LoRA)
- Kis tanítható mátrixokat ad hozzá a meglévő súlyokhoz
- Jelentősen csökkenti a tanítható paraméterek számát
- Teljesítményt közel tart a teljes finomhangoláshoz
- Könnyű váltást tesz lehetővé különböző adaptációk között

#### QLoRA (Quantized LoRA)
- Kombinálja a LoRA-t kvantálási technikákkal
- Tovább csökkenti a memóriaigényt
- Lehetővé teszi nagyobb modellek finomhangolását fogyasztói hardveren
- Egyensúlyt teremt a hatékonyság és a teljesítmény között

#### Adapterek
- Kis neurális hálózatokat illesztenek a meglévő rétegek közé
- Célzott finomhangolást tesznek lehetővé, miközben az alapmodell érintetlen marad
- Moduláris megközelítést kínálnak a modell testreszabásához

### 3. Feladatspecifikus finomhangolás

Modellek adaptálása specifikus downstream feladatokra:
- **Kategorizálás**: Modellek finomhangolása osztályozási feladatokra
- **Generálás**: Optimalizálás tartalomkészítésre és szövegalkotásra
- **Kinyerés**: Finomhangolás információkinyerésre és névjegyfelismerésre
- **Összefoglalás**: Modellek specializálása dokumentumösszefoglalásra

## Finomhangolás a Microsoft Olive segítségével

A Microsoft Olive egy átfogó modelloptimalizáló eszközkészlet, amely egyszerűsíti a finomhangolási folyamatot, miközben vállalati szintű funkciókat kínál.

### Mi az a Microsoft Olive?

A Microsoft Olive egy nyílt forráskódú modelloptimalizáló eszköz, amely:
- Egyszerűsíti a finomhangolási munkafolyamatokat különböző hardverekhez
- Beépített támogatást nyújt népszerű modellarchitektúrákhoz (Llama, Phi, Qwen, Gemma)
- Felhő- és helyi telepítési lehetőségeket kínál
- Zökkenőmentesen integrálódik az Azure ML és más Microsoft AI szolgáltatásokkal
- Támogatja az automatikus optimalizálást és kvantálást

### Főbb jellemzők

- **Hardver-tudatos optimalizálás**: Automatikusan optimalizálja a modelleket specifikus hardverekhez (CPU, GPU, NPU)
- **Többformátumú támogatás**: Működik PyTorch, Hugging Face és ONNX modellekkel
- **Automatizált munkafolyamatok**: Csökkenti a manuális konfigurációt és próbálgatást
- **Vállalati integráció**: Beépített támogatás az Azure ML-hez és felhőalapú telepítésekhez
- **Kiterjeszthető architektúra**: Lehetővé teszi egyedi optimalizálási technikák alkalmazását

### Telepítés és beállítás

#### Alaptelepítés

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Opcionális függőségek

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Telepítés ellenőrzése

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Gyakorlati példák

### Példa 1: Alapvető finomhangolás Olive CLI-vel

Ez a példa bemutatja egy kis nyelvi modell finomhangolását kifejezésosztályozásra:

#### 1. lépés: Környezet előkészítése

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### 2. lépés: Modell finomhangolása

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### 3. lépés: Optimalizálás telepítéshez

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Példa 2: Haladó konfiguráció egyedi adathalmazzal

#### 1. lépés: Egyedi adathalmaz előkészítése

Hozzon létre egy JSON fájlt a tanítási adatokkal:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### 2. lépés: Konfigurációs fájl létrehozása

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### 3. lépés: Finomhangolás végrehajtása

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Példa 3: QLoRA finomhangolás memóriahatékonyság érdekében

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Legjobb gyakorlatok és irányelvek

### Adatelőkészítés

**1. Adatminőség a mennyiség felett**
- Részesítse előnyben a magas minőségű, sokszínű példákat a nagy mennyiségű gyenge adat helyett
- Biztosítsa, hogy az adatok reprezentálják a célfelhasználási esetet
- Tisztítsa és előfeldolgozza az adatokat következetesen

**2. Adatformátum és sablonok**
- Használjon következetes formázást az összes tanítási példában
- Hozzon létre világos input-output sablonokat, amelyek illeszkednek a felhasználási esethez
- Tartalmazzon megfelelő instrukcióformázást az instrukció-alapú modellekhez

**3. Adathalmaz felosztása**
- Tartson fenn 10-20%-ot validációra
- Biztosítsa, hogy a tanítási és validációs adatok hasonló eloszlásúak legyenek
- Fontolja meg a rétegzett mintavételt osztályozási feladatokhoz

### Tanítási konfiguráció

**1. Tanulási ráta kiválasztása**
- Kezdjen kisebb tanulási rátákkal (1e-5-től 1e-4-ig) finomhangoláshoz
- Használjon tanulási ráta ütemezést a jobb konvergencia érdekében
- Figyelje a veszteség görbéket, hogy szükség esetén módosítson

**2. Batch méret optimalizálása**
- Egyensúlyozza a batch méretet a rendelkezésre álló memória alapján
- Használjon gradiens felhalmozást nagyobb effektív batch méretekhez
- Vegye figyelembe a batch méret és tanulási ráta közötti kapcsolatot

**3. Tanítási időtartam**
- Figyelje a validációs metrikákat, hogy elkerülje a túltanulást
- Használjon korai leállítást, ha a validációs teljesítmény stagnál
- Mentse el rendszeresen a checkpointokat helyreállításhoz és elemzéshez

### Modellválasztás

**1. Alapmodell kiválasztása**
- Válasszon előre betanított modelleket, amelyek hasonló domaineken tanultak
- Vegye figyelembe a modell méretét a számítási korlátokhoz képest
- Értékelje a licencelési követelményeket kereskedelmi használatra

**2. Finomhangolási módszer kiválasztása**
- Használjon LoRA/QLoRA-t erőforrás-korlátozott környezetekben
- Válassza a teljes finomhangolást, ha a maximális teljesítmény kritikus
- Fontolja meg adapter-alapú megközelítéseket több feladat esetén

### Erőforrás-kezelés

**1. Hardver optimalizálás**
- Válasszon megfelelő hardvert a modell méretéhez és módszeréhez
- Használja ki hatékonyan a GPU memóriát gradiens checkpointinggal
- Fontolja meg felhőalapú megoldásokat nagyobb modellekhez

**2. Memóriakezelés**
- Használjon vegyes precíziós tanítást, ha elérhető
- Valósítson meg gradiens felhalmozást memória korlátok esetén
- Figyelje a GPU memóriahasználatot a tanítás során

## Haladó technikák

### Több adapteres tanítás

Több adapter tanítása különböző feladatokra, miközben az alapmodell közös marad:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hiperparaméter optimalizálás

Rendszeres hiperparaméter hangolás megvalósítása:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Egyedi veszteségfüggvények

Domain-specifikus veszteségfüggvények implementálása:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Értékelés és nyomon követés

### Metrikák és értékelés

**1. Standard metrikák**
- **Pontosság**: Az osztályozási feladatok általános helyessége
- **Perplexitás**: Nyelvi modellezési minőség mértéke
- **BLEU/ROUGE**: Szövegalkotás és összefoglalás minősége
- **F1 pontszám**: Kiegyensúlyozott precizitás és visszahívás osztályozáshoz

**2. Domain-specifikus metrikák**
- **Feladatspecifikus benchmarkok**: Használjon bevált benchmarkokat a domainhez
- **Emberi értékelés**: Vonjon be emberi értékelést szubjektív feladatokhoz
- **Üzleti metrikák**: Igazítsa az üzleti célokhoz

**3. Értékelési beállítás**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Tanítási folyamat nyomon követése

**1. Veszteség követése**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Validáció nyomon követése**
- Kövesse a validációs veszteséget a tanítási veszteség mellett
- Figyelje a túltanulás jeleit (validációs veszteség növekszik, miközben a tanítási veszteség csökken)
- Használjon korai leállítást validációs metrikák alapján

**3. Erőforrás nyomon követése**
- Kövesse a GPU/CPU kihasználtságot
- Figyelje a memóriahasználati mintákat
- Kövesse a tanítási sebességet és áteresztőképességet

## Gyakori kihívások és megoldások

### Kihívás 1: Túltanulás

**Tünetek:**
- A tanítási veszteség tovább csökken, miközben a validációs veszteség növekszik
- Nagy különbség a tanítási és validációs teljesítmény között
- Gyenge általánosítás új adatokra

**Megoldások:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Kihívás 2: Memória korlátok

**Megoldások:**
- Használjon gradiens checkpointingot
- Valósítson meg gradiens felhalmozást
- Válasszon paraméter-hatékony módszereket (LoRA, QLoRA)
- Használjon modell párhuzamosítást nagy modellekhez

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Kihívás 3: Lassú tanítás

**Megoldások:**
- Optimalizálja az adatbetöltési folyamatokat
- Használjon vegyes precíziós tanítást
- Valósítson meg hatékony batch stratégiákat
- Fontolja meg elosztott tanítást nagy adathalmazokhoz

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Kihívás 4: Gyenge teljesítmény

**Diagnosztikai lépések:**
1. Ellenőrizze az adatok minőségét és formázását
2. Vizsgálja meg a tanulási rátát és a tanítási időtartamot
3. Értékelje az alapmodell választását
4. Tekintse át az előfeldolgozást és tokenizálást

**Megoldások:**
- Növelje az adatok sokszínűségét
- Á

---

**Felelősség kizárása**:  
Ez a dokumentum az AI fordítási szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.