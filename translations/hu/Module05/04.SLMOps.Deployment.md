<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T17:14:12+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "hu"
}
-->
# 4. szakasz: Telep√≠t√©s - Gy√°rt√°sra K√©sz Modell Implement√°ci√≥ja

## √Åttekint√©s

Ez az √°tfog√≥ √∫tmutat√≥ v√©gigvezet a finomhangolt kvant√°lt modellek Foundry Local seg√≠ts√©g√©vel t√∂rt√©n≈ë telep√≠t√©s√©nek teljes folyamat√°n. Foglalkozunk a modell konverzi√≥j√°val, kvant√°l√°si optimaliz√°l√°ssal √©s telep√≠t√©si konfigur√°ci√≥val az elej√©t≈ël a v√©g√©ig.

## El≈ëfelt√©telek

Miel≈ëtt elkezden√©d, gy≈ëz≈ëdj meg arr√≥l, hogy rendelkezel az al√°bbiakkal:

- ‚úÖ Telep√≠t√©sre k√©sz finomhangolt ONNX modell
- ‚úÖ Windows vagy Mac sz√°m√≠t√≥g√©p
- ‚úÖ Python 3.10 vagy √∫jabb verzi√≥
- ‚úÖ Legal√°bb 8 GB szabad RAM
- ‚úÖ Foundry Local telep√≠tve a rendszereden

## 1. r√©sz: K√∂rnyezet Be√°ll√≠t√°sa

### Sz√ºks√©ges Eszk√∂z√∂k Telep√≠t√©se

Nyisd meg a termin√°lt (Windows eset√©n Command Prompt, Mac eset√©n Terminal), √©s futtasd az al√°bbi parancsokat sorrendben:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Fontos Megjegyz√©s**: Sz√ºks√©ged lesz a CMake 3.31-es vagy √∫jabb verzi√≥j√°ra is, amely let√∂lthet≈ë innen: [cmake.org](https://cmake.org/download/).

## 2. r√©sz: Modell Konverzi√≥ √©s Kvant√°l√°s

### Megfelel≈ë Form√°tum Kiv√°laszt√°sa

Finomhangolt kis nyelvi modellek eset√©n az **ONNX form√°tum** haszn√°lat√°t javasoljuk, mert:

- üöÄ Jobb teljes√≠tm√©nyoptimaliz√°l√°st k√≠n√°l
- üîß Hardverf√ºggetlen telep√≠t√©st tesz lehet≈ëv√©
- üè≠ Gy√°rt√°sra k√©sz k√©pess√©gekkel rendelkezik
- üì± Platformf√ºggetlen kompatibilit√°st biztos√≠t

### 1. M√≥dszer: Egyparancsos Konverzi√≥ (Aj√°nlott)

Haszn√°ld az al√°bbi parancsot a finomhangolt modell k√∂zvetlen konvert√°l√°s√°hoz:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Param√©terek Magyar√°zata:**
- `--model_name_or_path`: A finomhangolt modell el√©r√©si √∫tja
- `--device cpu`: CPU haszn√°lata optimaliz√°l√°shoz
- `--precision int4`: INT4 kvant√°l√°s haszn√°lata (kb. 75%-os m√©retcs√∂kkent√©s)
- `--output_path`: A konvert√°lt modell kimeneti √∫tvonala

### 2. M√≥dszer: Konfigur√°ci√≥s F√°jl Megk√∂zel√≠t√©s (Halad√≥ Felhaszn√°l√≥knak)

Hozz l√©tre egy `finetuned_conversion_config.json` nev≈± konfigur√°ci√≥s f√°jlt:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Majd futtasd:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Kvant√°l√°si Opci√≥k √ñsszehasonl√≠t√°sa

| Pontoss√°g | F√°jlm√©ret | Inference Sebess√©g | Modell Min≈ës√©g | Aj√°nlott Haszn√°lat |
|-----------|-----------|--------------------|----------------|--------------------|
| FP16      | Alap √ó 0.5 | Gyors | Legjobb | Magas szint≈± hardver |
| INT8      | Alap √ó 0.25 | Nagyon Gyors | J√≥ | Kiegyens√∫lyozott v√°laszt√°s |
| INT4      | Alap √ó 0.125 | Leggyorsabb | Elfogadhat√≥ | Er≈ëforr√°s-korl√°tozott |

üí° **Aj√°nl√°s**: Kezdd az INT4 kvant√°l√°ssal az els≈ë telep√≠t√©shez. Ha a min≈ës√©g nem kiel√©g√≠t≈ë, pr√≥b√°ld ki az INT8 vagy FP16 opci√≥kat.

## 3. r√©sz: Foundry Local Telep√≠t√©si Konfigur√°ci√≥

### Modell Konfigur√°ci√≥ L√©trehoz√°sa

Navig√°lj a Foundry Local modellek k√∂nyvt√°r√°ba:

```bash
foundry cache cd ./models/
```

Hozd l√©tre a modell k√∂nyvt√°rstrukt√∫r√°j√°t:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Hozd l√©tre az `inference_model.json` konfigur√°ci√≥s f√°jlt a modell k√∂nyvt√°r√°ban:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Modell-Specifikus Sablon Konfigur√°ci√≥k

#### Qwen Sorozat Modellekhez:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## 4. r√©sz: Modell Tesztel√©se √©s Optimaliz√°l√°sa

### Modell Telep√≠t√©s Ellen≈ërz√©se

Ellen≈ërizd, hogy a Foundry Local felismeri-e a modelledet:

```bash
foundry cache ls
```

A list√°ban meg kell jelennie a `your-finetuned-model-int4` elemnek.

### Modell Tesztel√©s Ind√≠t√°sa

```bash
foundry model run your-finetuned-model-int4
```

### Teljes√≠tm√©ny M√©r√©se

Figyeld meg a kulcsfontoss√°g√∫ mutat√≥kat a tesztel√©s sor√°n:

1. **V√°laszid≈ë**: √Åtlagos v√°laszid≈ë m√©r√©se
2. **Mem√≥riahaszn√°lat**: RAM fogyaszt√°s figyel√©se
3. **CPU Kihaszn√°lts√°g**: Processzor terhel√©s ellen≈ërz√©se
4. **Kimeneti Min≈ës√©g**: V√°lasz relevanci√°j√°nak √©s koherenci√°j√°nak √©rt√©kel√©se

### Min≈ës√©g Ellen≈ërz√©si Lista

- ‚úÖ A modell megfelel≈ëen v√°laszol a finomhangolt ter√ºlet k√©rd√©seire
- ‚úÖ A v√°lasz form√°tuma megfelel az elv√°rt kimeneti strukt√∫r√°nak
- ‚úÖ Nincs mem√≥ria sziv√°rg√°s hosszabb haszn√°lat sor√°n
- ‚úÖ Konzisztens teljes√≠tm√©ny k√ºl√∂nb√∂z≈ë bemeneti hossz√∫s√°gok eset√©n
- ‚úÖ Sz√©ls≈ës√©ges esetek √©s √©rv√©nytelen bemenetek megfelel≈ë kezel√©se

## √ñsszefoglal√°s

Gratul√°lunk! Sikeresen elv√©gezted:

- ‚úÖ Finomhangolt modell form√°tum konverzi√≥j√°t
- ‚úÖ Modell kvant√°l√°si optimaliz√°l√°s√°t
- ‚úÖ Foundry Local telep√≠t√©si konfigur√°ci√≥j√°t
- ‚úÖ Teljes√≠tm√©nyhangol√°st √©s hibakeres√©st

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Fontos inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.