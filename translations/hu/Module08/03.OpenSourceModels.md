<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:00:14+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "hu"
}
-->
# 3. szekció: Nyílt forráskódú modellek Foundry Local segítségével

## Áttekintés

Ebben a szekcióban azt vizsgáljuk, hogyan lehet nyílt forráskódú modelleket integrálni a Foundry Local-ba: közösségi modellek kiválasztása, Hugging Face tartalmak integrálása, valamint a „hozd a saját modelledet” (BYOM) stratégiák alkalmazása. Emellett megismerheted a Model Mondays sorozatot, amely folyamatos tanulást és modellfelfedezést kínál.

Hivatkozások:
- Foundry Local dokumentáció: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face modellek fordítása: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Tanulási célok
- Nyílt forráskódú modellek felfedezése és értékelése helyi futtatáshoz
- Hugging Face modellek fordítása és futtatása Foundry Local környezetben
- Modellválasztási stratégiák alkalmazása pontosság, késleltetés és erőforrásigény alapján
- Modellek helyi kezelése gyorsítótár és verziózás segítségével

## 1. rész: Modellek felfedezése és kiválasztása (Lépésről lépésre)

1. lépés) A helyi katalógusban elérhető modellek listázása  
```cmd
foundry model list
```
  
2. lépés) Két jelölt gyors kipróbálása (az első futtatáskor automatikusan letöltődik)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
3. lépés) Alapvető metrikák megfigyelése  
- Figyeld meg a késleltetést (szubjektív) és a minőséget egy fix prompt esetén  
- Kövesd a memóriahasználatot a Feladatkezelőben, miközben minden modell fut  

## 2. rész: Katalógusmodellek futtatása CLI segítségével (Lépésről lépésre)

1. lépés) Modell indítása  
```cmd
foundry model run llama-3.2
```
  
2. lépés) Tesztprompt küldése az OpenAI-kompatibilis végpontra  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## 3. rész: BYOM – Hugging Face modellek fordítása (Lépésről lépésre)

Kövesd a hivatalos útmutatót a modellek fordításához. Az alábbiakban egy magas szintű folyamatot találsz – a pontos parancsokért és támogatott konfigurációkért lásd a Microsoft Learn cikket.

1. lépés) Munkakönyvtár előkészítése  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
2. lépés) Támogatott HF modell fordítása  
- Használd az útmutató lépéseit az ONNX modell konvertálásához és elhelyezéséhez a `models` könyvtárban  
- Ellenőrizd:  
```cmd
foundry cache ls
```
  
Látnod kell a fordított modell nevét (például `llama-3.2`).  

3. lépés) Fordított modell futtatása  
```cmd
foundry model run llama-3.2 --verbose
```
  
Megjegyzések:  
- Biztosíts elegendő lemez- és RAM-kapacitást a fordításhoz és futtatáshoz  
- Kezdd kisebb modellekkel a folyamat validálásához, majd lépj nagyobbakra  

## 4. rész: Gyakorlati modellkuráció (Lépésről lépésre)

1. lépés) `models.json` regisztráció létrehozása  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
2. lépés) Kis kiválasztó szkript  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## 5. rész: Gyakorlati benchmarkok (Lépésről lépésre)

1. lépés) Egyszerű késleltetési benchmark  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
2. lépés) Minőségi ellenőrzés  
- Használj egy fix prompt készletet, rögzítsd az eredményeket CSV/JSON formátumban  
- Kézzel értékeld a folyékonyságot, relevanciát és helyességet (1–5)  

## 6. rész: Következő lépések
- Iratkozz fel a Model Mondays sorozatra új modellekért és tippekért: https://aka.ms/model-mondays  
- Oszd meg az eredményeket a csapatod `models.json` fájljában  
- Készülj a 4. szekcióra: LLM-ek és SLM-ek összehasonlítása, helyi és felhőalapú futtatás, valamint gyakorlati bemutatók  

---

