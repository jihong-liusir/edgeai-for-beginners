<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T01:12:27+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "hu"
}
-->
# 4. szekci√≥: Termel√©sre k√©sz chatalkalmaz√°sok √©p√≠t√©se Chainlit seg√≠ts√©g√©vel

## √Åttekint√©s

Ebben a szekci√≥ban a Chainlit √©s a Microsoft Foundry Local haszn√°lat√°val termel√©sre k√©sz chatalkalmaz√°sok √©p√≠t√©s√©re koncentr√°lunk. Megtanulhatod, hogyan hozz l√©tre modern webes fel√ºleteket AI besz√©lget√©sekhez, hogyan val√≥s√≠ts meg streaming v√°laszokat, √©s hogyan telep√≠ts robusztus chatalkalmaz√°sokat megfelel≈ë hibakezel√©ssel √©s felhaszn√°l√≥i √©lm√©nytervez√©ssel.

**Amit √©p√≠teni fogsz:**
- **Chainlit Chat App**: Modern webes UI streaming v√°laszokkal
- **WebGPU Demo**: B√∂ng√©sz≈ëalap√∫ inference adatv√©delmi szempontb√≥l el≈ëny√∂s alkalmaz√°sokhoz  
- **Open WebUI Integr√°ci√≥**: Professzion√°lis chatfel√ºlet Foundry Local seg√≠ts√©g√©vel
- **Termel√©si mint√°k**: Hibakezel√©s, monitoroz√°s √©s telep√≠t√©si strat√©gi√°k

## Tanul√°si c√©lok

- Termel√©sre k√©sz chatalkalmaz√°sok √©p√≠t√©se Chainlit seg√≠ts√©g√©vel
- Streaming v√°laszok megval√≥s√≠t√°sa a felhaszn√°l√≥i √©lm√©ny jav√≠t√°sa √©rdek√©ben
- Foundry Local SDK integr√°ci√≥s mint√°k elsaj√°t√≠t√°sa
- Megfelel≈ë hibakezel√©s √©s fokozatos le√©p√≠t√©s alkalmaz√°sa
- Chatalkalmaz√°sok telep√≠t√©se √©s konfigur√°l√°sa k√ºl√∂nb√∂z≈ë k√∂rnyezetekhez
- Modern webes UI mint√°k meg√©rt√©se besz√©lget√©si AI-hoz

## El≈ëfelt√©telek

- **Foundry Local**: Telep√≠tve √©s futtatva ([Telep√≠t√©si √∫tmutat√≥](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 vagy √∫jabb, virtu√°lis k√∂rnyezet t√°mogat√°ssal
- **Model**: Legal√°bb egy modell bet√∂ltve (`foundry model run phi-4-mini`)
- **B√∂ng√©sz≈ë**: Modern webes b√∂ng√©sz≈ë WebGPU t√°mogat√°ssal (Chrome/Edge)
- **Docker**: Open WebUI integr√°ci√≥hoz (opcion√°lis)

## 1. r√©sz: Modern chatalkalmaz√°sok meg√©rt√©se

### Architekt√∫ra √°ttekint√©se

```
User Browser ‚Üê‚Üí Chainlit UI ‚Üê‚Üí Python Backend ‚Üê‚Üí Foundry Local ‚Üê‚Üí AI Model
      ‚Üì              ‚Üì              ‚Üì              ‚Üì            ‚Üì
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Kulcstechnol√≥gi√°k

**Foundry Local SDK mint√°k:**
- `FoundryLocalManager(alias)`: Automatikus szolg√°ltat√°skezel√©s
- `manager.endpoint` √©s `manager.api_key`: Kapcsolati adatok
- `manager.get_model_info(alias).id`: Modellazonos√≠t√°s

**Chainlit keretrendszer:**
- `@cl.on_chat_start`: Chat szekci√≥k inicializ√°l√°sa
- `@cl.on_message`: Be√©rkez≈ë √ºzenetek kezel√©se  
- `cl.Message().stream_token()`: Val√≥s idej≈± streaming
- Automatikus UI gener√°l√°s √©s WebSocket kezel√©s

## 2. r√©sz: Lok√°lis vs felh≈ë d√∂nt√©si m√°trix

### Teljes√≠tm√©nyjellemz≈ëk

| Szempont | Lok√°lis (Foundry) | Felh≈ë (Azure OpenAI) |
|----------|-------------------|---------------------|
| **K√©sleltet√©s** | üöÄ 50-200ms (nincs h√°l√≥zat) | ‚è±Ô∏è 200-2000ms (h√°l√≥zatf√ºgg≈ë) |
| **Adatv√©delem** | üîí Az adatok nem hagyj√°k el az eszk√∂zt | ‚ö†Ô∏è Adatok a felh≈ëbe ker√ºlnek |
| **K√∂lts√©g** | üí∞ Ingyenes hardver ut√°n | üí∏ Fizet√©s tokenenk√©nt |
| **Offline** | ‚úÖ Internet n√©lk√ºl m≈±k√∂dik | ‚ùå Internet sz√ºks√©ges |
| **Modellm√©ret** | ‚ö†Ô∏è Hardver korl√°tozza | ‚úÖ Hozz√°f√©r√©s a legnagyobb modellekhez |
| **Sk√°l√°z√°s** | ‚ö†Ô∏è Hardverf√ºgg≈ë | ‚úÖ Korl√°tlan sk√°l√°z√°s |

### Hibrid strat√©giai mint√°k

**Lok√°lis els≈ëdleges, visszaes√©ssel:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Feladat-alap√∫ ir√°ny√≠t√°s:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## 3. r√©sz: Minta 04 - Chainlit chatalkalmaz√°s

### Gyors kezd√©s

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Az alkalmaz√°s automatikusan megny√≠lik a `http://localhost:8080` c√≠men modern chatfel√ºlettel.

### Alapvet≈ë megval√≥s√≠t√°s

A Minta 04 alkalmaz√°s termel√©sre k√©sz mint√°kat mutat be:

**Automatikus szolg√°ltat√°sfelismer√©s:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Streaming chatkezel≈ë:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Konfigur√°ci√≥s lehet≈ës√©gek

**K√∂rnyezeti v√°ltoz√≥k:**

| V√°ltoz√≥ | Le√≠r√°s | Alap√©rtelmezett | P√©lda |
|---------|--------|-----------------|-------|
| `MODEL` | Haszn√°land√≥ modell alias | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local v√©gpont | Automatikusan felismerve | `http://localhost:51211` |
| `API_KEY` | API kulcs (opcion√°lis lok√°lis haszn√°lathoz) | `""` | `your-api-key` |

**Halad√≥ haszn√°lat:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## 4. r√©sz: Jupyter notebookok l√©trehoz√°sa √©s haszn√°lata

### Notebook t√°mogat√°s √°ttekint√©se

A Minta 04 tartalmaz egy √°tfog√≥ Jupyter notebookot (`chainlit_app.ipynb`), amely biztos√≠tja:

- **üìö Oktat√°si tartalom**: L√©p√©sr≈ël l√©p√©sre tanul√°si anyagok
- **üî¨ Interakt√≠v felfedez√©s**: K√≥dcell√°k futtat√°sa √©s k√≠s√©rletez√©se
- **üìä Vizualiz√°ci√≥k**: Diagramok, √°br√°k √©s kimeneti megjelen√≠t√©s
- **üõ†Ô∏è Fejleszt√©si eszk√∂z√∂k**: Tesztel√©si √©s hibakeres√©si lehet≈ës√©gek

### Saj√°t notebookok l√©trehoz√°sa

#### 1. l√©p√©s: Jupyter k√∂rnyezet be√°ll√≠t√°sa

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### 2. l√©p√©s: √öj notebook l√©trehoz√°sa

**VS Code haszn√°lat√°val:**
1. Nyisd meg a VS Code-ot a Module08 k√∂nyvt√°rban
2. Hozz l√©tre egy √∫j f√°jlt `.ipynb` kiterjeszt√©ssel
3. V√°laszd ki a "Foundry Local" kernelt, amikor megjelenik a k√©rd√©s
4. Kezdj el cell√°kat hozz√°adni a tartalmaddal

**Jupyter Lab haszn√°lat√°val:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook strukt√∫ra legjobb gyakorlatai

#### Cell√°k szervez√©se

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("‚úÖ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Interakt√≠v p√©ld√°k √©s gyakorlatok

#### Gyakorlat 1: √úgyf√©lkonfigur√°ci√≥ tesztel√©se

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nüß™ Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'‚úÖ Success' if result['status'] == 'ok' else '‚ùå Failed'}")
```

#### Gyakorlat 2: Streaming v√°lasz szimul√°ci√≥

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("üåä Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n‚úÖ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## 5. r√©sz: WebGPU b√∂ng√©sz≈ë inference demo

### √Åttekint√©s

A WebGPU lehet≈ëv√© teszi AI modellek futtat√°s√°t k√∂zvetlen√ºl a b√∂ng√©sz≈ëben, maxim√°lis adatv√©delem √©s telep√≠t√©s n√©lk√ºli √©lm√©ny √©rdek√©ben. Ez a minta az ONNX Runtime Web-et mutatja be WebGPU v√©grehajt√°ssal.

### 1. l√©p√©s: WebGPU t√°mogat√°s ellen≈ërz√©se

**B√∂ng√©sz≈ë k√∂vetelm√©nyek:**
- Chrome/Edge 113+ WebGPU enged√©lyezve
- Ellen≈ërz√©s: `chrome://gpu` ‚Üí "WebGPU" st√°tusz meger≈ës√≠t√©se
- Programozott ellen≈ërz√©s: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### 2. l√©p√©s: WebGPU demo l√©trehoz√°sa

K√∂nyvt√°r l√©trehoz√°sa: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>üöÄ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '‚ùå WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'üîç WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('‚úÖ ONNX Runtime session created with WebGPU');
        log(`üìä Input names: ${session.inputNames.join(', ')}`);
        log(`üìä Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '‚úÖ WebGPU inference complete!';
        log(`üéØ Predicted class: ${maxIdx}`);
        log(`üìà Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `‚ùå Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### 3. l√©p√©s: Demo futtat√°sa

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## 6. r√©sz: Open WebUI integr√°ci√≥

### √Åttekint√©s

Az Open WebUI professzion√°lis ChatGPT-szer≈± fel√ºletet biztos√≠t, amely csatlakozik a Foundry Local OpenAI-kompatibilis API-j√°hoz.

### 1. l√©p√©s: El≈ëfelt√©telek

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### 2. l√©p√©s: Docker be√°ll√≠t√°s (aj√°nlott)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Megjegyz√©s:** A `host.docker.internal` lehet≈ëv√© teszi, hogy a Docker kont√©nerek hozz√°f√©rjenek a gazdag√©phez Windows rendszeren.

### 3. l√©p√©s: Konfigur√°ci√≥

1. **B√∂ng√©sz≈ë megnyit√°sa:** Navig√°lj a `http://localhost:3000` c√≠mre
2. **Kezdeti be√°ll√≠t√°s:** Admin fi√≥k l√©trehoz√°sa
3. **Modell konfigur√°ci√≥:**
   - Be√°ll√≠t√°sok ‚Üí Modellek ‚Üí OpenAI API  
   - Alap URL: `http://host.docker.internal:51211/v1`
   - API kulcs: `foundry-local-key` (b√°rmilyen √©rt√©k m≈±k√∂dik)
4. **Kapcsolat tesztel√©se:** A modelleknek meg kell jelenni√ºk a leg√∂rd√ºl≈ë men√ºben

### Hibakeres√©s

**Gyakori probl√©m√°k:**

1. **Kapcsolat megtagadva:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modellek nem jelennek meg:**
   - Ellen≈ërizd, hogy a modell bet√∂ltve van-e: `foundry model list`
   - Ellen≈ërizd az API v√°lasz√°t: `curl http://localhost:51211/v1/models`
   - Ind√≠tsd √∫jra az Open WebUI kont√©nert

## 7. r√©sz: Termel√©si telep√≠t√©si szempontok

### K√∂rnyezet konfigur√°ci√≥

**Fejleszt√©si be√°ll√≠t√°s:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Termel√©si telep√≠t√©s:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Gyakori portprobl√©m√°k √©s megold√°sok

**Port 51211 konfliktus megel≈ëz√©se:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Teljes√≠tm√©ny monitoroz√°s

**Eg√©szs√©g√ºgyi ellen≈ërz√©s megval√≥s√≠t√°sa:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## √ñsszefoglal√≥

A 4. szekci√≥ban a termel√©sre k√©sz Chainlit alkalmaz√°sok √©p√≠t√©s√©t tanultuk meg besz√©lget√©si AI-hoz. Megismerkedt√©l:

- ‚úÖ **Chainlit keretrendszerrel**: Modern UI √©s streaming t√°mogat√°s chatalkalmaz√°sokhoz
- ‚úÖ **Foundry Local integr√°ci√≥val**: SDK haszn√°lat √©s konfigur√°ci√≥s mint√°k  
- ‚úÖ **WebGPU inference**: B√∂ng√©sz≈ëalap√∫ AI maxim√°lis adatv√©delem √©rdek√©ben
- ‚úÖ **Open WebUI be√°ll√≠t√°ssal**: Professzion√°lis chatfel√ºlet telep√≠t√©se
- ‚úÖ **Termel√©si mint√°kkal**: Hibakezel√©s, monitoroz√°s √©s sk√°l√°z√°s

A Minta 04 alkalmaz√°s bemutatja a legjobb gyakorlatokat robusztus chatfel√ºletek √©p√≠t√©s√©hez, amelyek helyi AI modelleket haszn√°lnak a Microsoft Foundry Local seg√≠ts√©g√©vel, mik√∂zben kiv√°l√≥ felhaszn√°l√≥i √©lm√©nyt ny√∫jtanak.

## Hivatkoz√°sok

- **[Minta 04: Chainlit alkalmaz√°s](samples/04/README.md)**: Teljes alkalmaz√°s dokument√°ci√≥val
- **[Chainlit oktat√°si notebook](samples/04/chainlit_app.ipynb)**: Interakt√≠v tanul√°si anyagok
- **[Foundry Local dokument√°ci√≥](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Teljes platform dokument√°ci√≥
- **[Chainlit dokument√°ci√≥](https://docs.chainlit.io/)**: Hivatalos keretrendszer dokument√°ci√≥
- **[Open WebUI integr√°ci√≥s √∫tmutat√≥](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Hivatalos √∫tmutat√≥

---

