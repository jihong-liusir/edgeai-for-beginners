<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T14:03:20+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ar"
}
-->
# القسم 3: مجموعة أدوات تحسين Microsoft Olive

## جدول المحتويات
1. [المقدمة](../../../Module04)
2. [ما هو Microsoft Olive؟](../../../Module04)
3. [التثبيت](../../../Module04)
4. [دليل البدء السريع](../../../Module04)
5. [مثال: تحويل Qwen3 إلى ONNX INT4](../../../Module04)
6. [الاستخدام المتقدم](../../../Module04)
7. [أفضل الممارسات](../../../Module04)
8. [استكشاف الأخطاء وإصلاحها](../../../Module04)
9. [موارد إضافية](../../../Module04)

## المقدمة

Microsoft Olive هي أداة قوية وسهلة الاستخدام لتحسين النماذج مع مراعاة الأجهزة، حيث تبسط عملية تحسين نماذج التعلم الآلي لنشرها عبر منصات الأجهزة المختلفة. سواء كنت تستهدف وحدات المعالجة المركزية (CPU)، أو وحدات معالجة الرسومات (GPU)، أو مسرعات الذكاء الاصطناعي المتخصصة، تساعدك Olive في تحقيق الأداء الأمثل مع الحفاظ على دقة النموذج.

## ما هو Microsoft Olive؟

Olive هي أداة سهلة الاستخدام لتحسين النماذج مع مراعاة الأجهزة، تجمع بين تقنيات رائدة في الصناعة مثل ضغط النماذج، والتحسين، والتجميع. تعمل مع ONNX Runtime كحل شامل لتحسين الاستدلال.

### الميزات الرئيسية

- **تحسين مع مراعاة الأجهزة**: تختار تلقائيًا أفضل تقنيات التحسين للأجهزة المستهدفة.
- **أكثر من 40 مكون تحسين مدمج**: تشمل ضغط النماذج، التكميم، تحسين الرسوم البيانية، والمزيد.
- **واجهة CLI سهلة**: أوامر بسيطة للمهام الشائعة في التحسين.
- **دعم متعدد الأطر**: تعمل مع PyTorch، نماذج Hugging Face، وONNX.
- **دعم النماذج الشهيرة**: يمكن لـ Olive تحسين بنى النماذج الشهيرة مثل Llama، Phi، Qwen، Gemma، وغيرها تلقائيًا.

### الفوائد

- **تقليل وقت التطوير**: لا حاجة لتجربة تقنيات التحسين المختلفة يدويًا.
- **تحسين الأداء**: تحسينات كبيرة في السرعة (حتى 6 أضعاف في بعض الحالات).
- **النشر عبر المنصات**: النماذج المحسنة تعمل عبر أجهزة وأنظمة تشغيل مختلفة.
- **الحفاظ على الدقة**: التحسينات تحافظ على جودة النموذج مع تحسين الأداء.

## التثبيت

### المتطلبات الأساسية

- Python 3.8 أو أعلى
- مدير الحزم pip
- بيئة افتراضية (موصى بها)

### التثبيت الأساسي

قم بإنشاء وتفعيل بيئة افتراضية:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
قم بتثبيت Olive مع ميزات التحسين التلقائي:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  

### التبعيات الاختيارية

يوفر Olive تبعيات اختيارية لميزات إضافية:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  

### التحقق من التثبيت

```bash
olive --help
```
  
إذا نجح التثبيت، يجب أن ترى رسالة مساعدة CLI الخاصة بـ Olive.

## دليل البدء السريع

### أول عملية تحسين لك

لنقم بتحسين نموذج لغة صغير باستخدام ميزة التحسين التلقائي لـ Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### ماذا يفعل هذا الأمر؟

تشمل عملية التحسين: الحصول على النموذج من ذاكرة التخزين المحلية، التقاط الرسم البياني لـ ONNX وتخزين الأوزان في ملف بيانات ONNX، تحسين الرسم البياني لـ ONNX، وتكميم النموذج إلى int4 باستخدام طريقة RTN.

### شرح معلمات الأمر

- `--model_name_or_path`: معرف نموذج Hugging Face أو المسار المحلي.
- `--output_path`: الدليل الذي سيتم حفظ النموذج المحسن فيه.
- `--device`: الجهاز المستهدف (cpu، gpu).
- `--provider`: مزود التنفيذ (CPUExecutionProvider، CUDAExecutionProvider، DmlExecutionProvider).
- `--use_ort_genai`: استخدام ONNX Runtime Generate AI للاستدلال.
- `--precision`: دقة التكميم (int4، int8، fp16).
- `--log_level`: مستوى تفصيل السجلات (0=الحد الأدنى، 1=تفصيلي).

## مثال: تحويل Qwen3 إلى ONNX INT4

استنادًا إلى المثال المقدم من Hugging Face في [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)، إليك كيفية تحسين نموذج Qwen3:

### الخطوة 1: تنزيل النموذج (اختياري)

لتقليل وقت التنزيل، قم بتخزين الملفات الأساسية فقط:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  

### الخطوة 2: تحسين نموذج Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### الخطوة 3: اختبار النموذج المحسن

قم بإنشاء برنامج Python بسيط لاختبار النموذج المحسن:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  

### هيكل الإخراج

بعد التحسين، سيحتوي دليل الإخراج على:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## الاستخدام المتقدم

### ملفات التكوين

لعمليات تحسين أكثر تعقيدًا، يمكنك استخدام ملفات JSON للتكوين:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
قم بالتشغيل باستخدام التكوين:

```bash
olive run --config config.json
```
  

### تحسين GPU

لتحسين CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
لـ DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### التخصيص باستخدام Olive

يدعم Olive أيضًا تخصيص النماذج:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## أفضل الممارسات

### 1. اختيار النموذج
- ابدأ بالنماذج الصغيرة للاختبار (مثل 0.5B-7B معلمات).
- تأكد من أن بنية النموذج المستهدف مدعومة من Olive.

### 2. اعتبارات الأجهزة
- طابق هدف التحسين مع الأجهزة التي سيتم النشر عليها.
- استخدم تحسين GPU إذا كان لديك أجهزة متوافقة مع CUDA.
- ضع في اعتبارك DirectML لأجهزة Windows مع الرسومات المدمجة.

### 3. اختيار الدقة
- **INT4**: أقصى ضغط، فقدان طفيف في الدقة.
- **INT8**: توازن جيد بين الحجم والدقة.
- **FP16**: فقدان دقة طفيف، تقليل متوسط للحجم.

### 4. الاختبار والتحقق
- اختبر دائمًا النماذج المحسنة مع حالات الاستخدام الخاصة بك.
- قارن مقاييس الأداء (زمن الاستجابة، الإنتاجية، الدقة).
- استخدم بيانات إدخال ممثلة للتقييم.

### 5. التحسين التكراري
- ابدأ بالتحسين التلقائي للحصول على نتائج سريعة.
- استخدم ملفات التكوين للتحكم الدقيق.
- جرب تمريرات تحسين مختلفة.

## استكشاف الأخطاء وإصلاحها

### المشكلات الشائعة

#### 1. مشاكل التثبيت
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  

#### 2. مشاكل CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  

#### 3. مشاكل الذاكرة
- استخدم أحجام دفعات أصغر أثناء التحسين.
- جرب التكميم بدقة أعلى أولاً (int8 بدلاً من int4).
- تأكد من وجود مساحة كافية على القرص لتخزين النموذج.

#### 4. أخطاء تحميل النموذج
- تحقق من مسار النموذج وأذونات الوصول.
- تحقق مما إذا كان النموذج يتطلب `trust_remote_code=True`.
- تأكد من تنزيل جميع ملفات النموذج المطلوبة.

### الحصول على المساعدة

- **التوثيق**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **مشكلات GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **أمثلة**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## موارد إضافية

### الروابط الرسمية
- **مستودع GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **توثيق ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **مثال Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### أمثلة المجتمع
- **دفاتر Jupyter**: متوفرة في مستودع GitHub الخاص بـ Olive — https://github.com/microsoft/Olive/tree/main/examples
- **إضافة VS Code**: نظرة عامة على أدوات الذكاء الاصطناعي لـ VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **مقالات المدونة**: مدونة Microsoft Open Source — https://opensource.microsoft.com/blog/

### الأدوات ذات الصلة
- **ONNX Runtime**: محرك استدلال عالي الأداء — https://onnxruntime.ai/
- **Hugging Face Transformers**: مصدر العديد من النماذج المتوافقة — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: سير عمل تحسين قائم على السحابة — https://learn.microsoft.com/azure/machine-learning/

## ➡️ ما التالي

- [04: مجموعة أدوات تحسين OpenVINO](./04.openvino.md)

---

