<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T17:49:26+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ar"
}
-->
# القسم 4: مجموعة أدوات تحسين OpenVINO

## جدول المحتويات
1. [المقدمة](../../../Module04)
2. [ما هو OpenVINO؟](../../../Module04)
3. [التثبيت](../../../Module04)
4. [دليل البدء السريع](../../../Module04)
5. [مثال: تحويل وتحسين النماذج باستخدام OpenVINO](../../../Module04)
6. [الاستخدام المتقدم](../../../Module04)
7. [أفضل الممارسات](../../../Module04)
8. [استكشاف الأخطاء وإصلاحها](../../../Module04)
9. [موارد إضافية](../../../Module04)

## المقدمة

OpenVINO (اختصار لـ Open Visual Inference and Neural Network Optimization) هو مجموعة أدوات مفتوحة المصدر من Intel لتطبيق حلول الذكاء الاصطناعي عالية الأداء عبر السحابة، والبيئات المحلية، وأجهزة الحافة. سواء كنت تستهدف وحدات المعالجة المركزية (CPU)، أو وحدات معالجة الرسومات (GPU)، أو وحدات معالجة الفيديو (VPU)، أو مسرعات الذكاء الاصطناعي المتخصصة، يوفر OpenVINO إمكانيات تحسين شاملة مع الحفاظ على دقة النموذج وتمكين النشر عبر المنصات المختلفة.

## ما هو OpenVINO؟

OpenVINO هو مجموعة أدوات مفتوحة المصدر تتيح للمطورين تحسين وتحويل ونشر نماذج الذكاء الاصطناعي بكفاءة عبر منصات الأجهزة المختلفة. يتكون من ثلاثة مكونات رئيسية: OpenVINO Runtime للتنفيذ، Neural Network Compression Framework (NNCF) لتحسين النماذج، وOpenVINO Model Server للنشر القابل للتوسع.

### الميزات الرئيسية

- **النشر عبر المنصات**: يدعم Linux، Windows، وmacOS مع واجهات برمجية Python، C++، وC
- **تسريع الأجهزة**: اكتشاف الأجهزة تلقائيًا وتحسين الأداء لوحدات CPU، GPU، VPU، ومسرعات الذكاء الاصطناعي
- **إطار ضغط الشبكات العصبية**: تقنيات متقدمة للتكميم، التشذيب، والتحسين عبر NNCF
- **التوافق مع الأطر**: دعم مباشر لنماذج TensorFlow، ONNX، PaddlePaddle، وPyTorch
- **دعم الذكاء الاصطناعي التوليدي**: OpenVINO GenAI متخصص لنشر نماذج اللغة الكبيرة وتطبيقات الذكاء الاصطناعي التوليدي

### الفوائد

- **تحسين الأداء**: تحسينات كبيرة في السرعة مع خسارة طفيفة في الدقة
- **تقليل بصمة النشر**: تقليل التبعيات الخارجية لتبسيط التثبيت والنشر
- **تحسين وقت البدء**: تحميل النماذج وتحسين التخزين المؤقت لتسريع بدء التطبيقات
- **النشر القابل للتوسع**: من أجهزة الحافة إلى البنية التحتية السحابية مع واجهات برمجية متسقة
- **جاهزية للإنتاج**: موثوقية على مستوى المؤسسات مع وثائق شاملة ودعم المجتمع

## التثبيت

### المتطلبات الأساسية

- Python 3.8 أو أعلى
- مدير الحزم pip
- بيئة افتراضية (موصى بها)
- أجهزة متوافقة (يوصى بوحدات Intel CPU، ولكن يدعم معماريات متنوعة)

### التثبيت الأساسي

إنشاء وتفعيل بيئة افتراضية:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

تثبيت OpenVINO Runtime:

```bash
pip install openvino
```

تثبيت NNCF لتحسين النماذج:

```bash
pip install nncf
```

### تثبيت OpenVINO GenAI

لتطبيقات الذكاء الاصطناعي التوليدي:

```bash
pip install openvino-genai
```

### التبعيات الاختيارية

حزم إضافية لحالات الاستخدام المحددة:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### التحقق من التثبيت

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

إذا نجح التثبيت، يجب أن ترى معلومات إصدار OpenVINO.

## دليل البدء السريع

### أول عملية تحسين للنموذج

لنقم بتحويل وتحسين نموذج من Hugging Face باستخدام OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### ماذا يفعل هذا الإجراء؟

يتضمن سير العمل لتحسين النموذج: تحميل النموذج الأصلي من Hugging Face، تحويله إلى صيغة OpenVINO Intermediate Representation (IR)، تطبيق تحسينات افتراضية، وتجميعه للأجهزة المستهدفة.

### شرح المعلمات الرئيسية

- `export=True`: يحول النموذج إلى صيغة OpenVINO IR
- `compile=False`: يؤجل التجميع حتى وقت التشغيل لتوفير المرونة
- `device`: الجهاز المستهدف ("CPU"، "GPU"، "AUTO" للاختيار التلقائي)
- `save_pretrained()`: يحفظ النموذج المحسن لإعادة الاستخدام

## مثال: تحويل وتحسين النماذج باستخدام OpenVINO

### الخطوة 1: تحويل النموذج باستخدام تكميم NNCF

إليك كيفية تطبيق التكميم بعد التدريب باستخدام NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### الخطوة 2: تحسين متقدم باستخدام ضغط الأوزان

للنماذج القائمة على المحولات، قم بتطبيق ضغط الأوزان:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### الخطوة 3: التنفيذ باستخدام النموذج المحسن

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### هيكل الإخراج

بعد التحسين، سيحتوي دليل النموذج الخاص بك على:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## الاستخدام المتقدم

### التكوين باستخدام NNCF YAML

لعمليات تحسين معقدة، استخدم ملفات تكوين NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

تطبيق التكوين:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### تحسين GPU

لتسريع GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### تحسين معالجة الدُفعات

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### نشر خادم النموذج

قم بنشر النماذج المحسنة باستخدام OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

كود العميل لخادم النموذج:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## أفضل الممارسات

### 1. اختيار النموذج وإعداده
- استخدم نماذج من الأطر المدعومة (PyTorch، TensorFlow، ONNX)
- تأكد من أن مدخلات النموذج لها أشكال ثابتة أو ديناميكية معروفة
- اختبر باستخدام مجموعات بيانات تمثيلية للمعايرة

### 2. اختيار استراتيجية التحسين
- **التكميم بعد التدريب**: ابدأ هنا لتحسين سريع
- **ضغط الأوزان**: مثالي للنماذج اللغوية الكبيرة والمحولات
- **التدريب الواعي للتكميم**: استخدمه عندما تكون الدقة حرجة

### 3. تحسين خاص بالأجهزة
- **CPU**: استخدم تكميم INT8 لتحقيق توازن بين الأداء
- **GPU**: استفد من دقة FP16 ومعالجة الدُفعات
- **VPU**: ركز على تبسيط النموذج ودمج الطبقات

### 4. ضبط الأداء
- **وضع الإنتاجية**: لمعالجة الدُفعات بكميات كبيرة
- **وضع الاستجابة**: للتطبيقات التفاعلية في الوقت الفعلي
- **الجهاز التلقائي**: دع OpenVINO يختار الأجهزة المثلى

### 5. إدارة الذاكرة
- استخدم الأشكال الديناميكية بحذر لتجنب زيادة استهلاك الذاكرة
- قم بتنفيذ تخزين مؤقت للنماذج لتسريع التحميلات اللاحقة
- راقب استخدام الذاكرة أثناء التحسين

### 6. التحقق من الدقة
- تحقق دائمًا من النماذج المحسنة مقابل الأداء الأصلي
- استخدم مجموعات بيانات اختبار تمثيلية للتقييم
- فكر في التحسين التدريجي (ابدأ بإعدادات محافظة)

## استكشاف الأخطاء وإصلاحها

### المشكلات الشائعة

#### 1. مشاكل التثبيت
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. أخطاء تحويل النموذج
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. مشاكل الأداء
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. مشاكل الذاكرة
- تقليل حجم الدُفعة للنموذج أثناء التحسين
- استخدام البث للبيانات الكبيرة
- تمكين تخزين النموذج المؤقت: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. تدهور الدقة
- استخدم دقة أعلى (INT8 بدلاً من INT4)
- زيادة حجم مجموعة بيانات المعايرة
- تطبيق تحسين الدقة المختلطة

### مراقبة الأداء

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### الحصول على المساعدة

- **الوثائق**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **مشكلات GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **منتدى المجتمع**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## موارد إضافية

### الروابط الرسمية
- **صفحة OpenVINO الرئيسية**: [openvino.ai](https://openvino.ai/)
- **مستودع GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **مستودع NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### موارد التعلم
- **دفاتر OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **دليل البدء السريع**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **دليل التحسين**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### أدوات التكامل
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### معايير الأداء
- **المعايير الرسمية**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### أمثلة المجتمع
- **دفاتر Jupyter**: [مستودع دفاتر OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - دروس شاملة متوفرة في مستودع دفاتر OpenVINO
- **تطبيقات نموذجية**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - أمثلة واقعية لمجالات متنوعة (الرؤية الحاسوبية، معالجة اللغة الطبيعية، الصوت)
- **مقالات المدونة**: [مدونة Intel AI](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - مقالات مدونة Intel AI والمجتمع مع حالات استخدام مفصلة

### الأدوات ذات الصلة
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - تقنيات تحسين إضافية لأجهزة Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - للمقارنة مع النشر على الأجهزة المحمولة والحافة
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - بدائل محركات التنفيذ عبر المنصات

## ➡️ ما التالي

- [05: نظرة عميقة على إطار عمل Apple MLX](./05.AppleMLX.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.