<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-17T17:17:23+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "ar"
}
-->
# القسم 3: أساسيات عائلة Gemma

تمثل عائلة نماذج Gemma نهج Google الشامل لتطوير نماذج اللغة المفتوحة المصدر والذكاء الاصطناعي متعدد الوسائط، مما يثبت أن النماذج المتاحة يمكن أن تحقق أداءً مذهلاً مع إمكانية نشرها عبر سيناريوهات متنوعة من الأجهزة المحمولة إلى محطات العمل المؤسسية. من المهم فهم كيف تمكن عائلة Gemma من تحقيق قدرات ذكاء اصطناعي قوية مع خيارات نشر مرنة مع الحفاظ على أداء تنافسي وممارسات ذكاء اصطناعي مسؤولة.

## المقدمة

في هذا الدليل، سنستكشف عائلة نماذج Gemma من Google ومفاهيمها الأساسية. سنغطي تطور عائلة Gemma، منهجيات التدريب المبتكرة التي تجعل نماذج Gemma فعالة، المتغيرات الرئيسية في العائلة، والتطبيقات العملية عبر سيناريوهات نشر مختلفة.

## أهداف التعلم

بنهاية هذا الدليل، ستكون قادرًا على:

- فهم فلسفة التصميم وتطور عائلة نماذج Gemma من Google
- التعرف على الابتكارات الرئيسية التي تمكن نماذج Gemma من تحقيق أداء عالي عبر أحجام مختلفة من المعلمات
- التعرف على فوائد وقيود المتغيرات المختلفة لنماذج Gemma
- تطبيق المعرفة بنماذج Gemma لاختيار المتغيرات المناسبة للسيناريوهات الواقعية

## فهم مشهد نماذج الذكاء الاصطناعي الحديث

شهد مشهد الذكاء الاصطناعي تطورًا كبيرًا، حيث تتبع المنظمات المختلفة نهجًا متنوعًا لتطوير نماذج اللغة. بينما يركز البعض على النماذج المغلقة المصدر التي يمكن الوصول إليها فقط عبر واجهات برمجية، يركز آخرون على إمكانية الوصول المفتوح والشفافية. النهج التقليدي يتضمن إما نماذج ضخمة مغلقة المصدر مع تكاليف مستمرة أو نماذج مفتوحة المصدر التي قد تتطلب خبرة تقنية كبيرة للنشر.

هذا النموذج يخلق تحديات للمنظمات التي تسعى للحصول على قدرات ذكاء اصطناعي قوية مع الحفاظ على التحكم في بياناتها، تكاليفها، ومرونة النشر. غالبًا ما يتطلب النهج التقليدي الاختيار بين الأداء المتقدم واعتبارات النشر العملية.

## تحدي التميز في الذكاء الاصطناعي المتاح

أصبحت الحاجة إلى ذكاء اصطناعي عالي الجودة ومتاح أكثر أهمية عبر سيناريوهات متنوعة. فكر في التطبيقات التي تتطلب خيارات نشر مرنة لتلبية احتياجات المنظمات المختلفة، تنفيذات فعالة من حيث التكلفة حيث يمكن أن تصبح تكاليف واجهات البرمجية كبيرة، قدرات متعددة الوسائط لفهم شامل، أو نشر متخصص على الأجهزة المحمولة وأجهزة الحافة.

### متطلبات النشر الرئيسية

تواجه عمليات نشر الذكاء الاصطناعي الحديثة عدة متطلبات أساسية تحد من التطبيق العملي:

- **إمكانية الوصول**: توفر المصدر المفتوح للشفافية والتخصيص
- **فعالية التكلفة**: متطلبات حسابية معقولة لتناسب ميزانيات مختلفة
- **المرونة**: أحجام نماذج متعددة لسيناريوهات نشر مختلفة
- **فهم متعدد الوسائط**: قدرات معالجة الرؤية والنص والصوت
- **نشر على الحافة**: أداء محسن على الأجهزة المحمولة والمحدودة الموارد

## فلسفة نماذج Gemma

تمثل عائلة نماذج Gemma نهج Google الشامل لتطوير نماذج الذكاء الاصطناعي، مع التركيز على إمكانية الوصول المفتوح، القدرات متعددة الوسائط، والنشر العملي مع الحفاظ على خصائص الأداء التنافسي. تحقق نماذج Gemma ذلك من خلال أحجام نماذج متنوعة، منهجيات تدريب عالية الجودة مشتقة من أبحاث Gemini، ومتغيرات متخصصة لمجالات وسيناريوهات نشر مختلفة.

تشمل عائلة Gemma نهجًا متنوعًا مصممًا لتوفير خيارات عبر طيف الأداء والكفاءة، مما يمكن النشر من الأجهزة المحمولة إلى خوادم المؤسسات مع توفير قدرات ذكاء اصطناعي ذات معنى. الهدف هو ديمقراطية الوصول إلى تقنية الذكاء الاصطناعي عالية الجودة مع توفير المرونة في خيارات النشر.

### مبادئ تصميم Gemma الأساسية

تم بناء نماذج Gemma على عدة مبادئ أساسية تميزها عن عائلات نماذج اللغة الأخرى:

- **المصدر المفتوح أولاً**: شفافية كاملة وإمكانية الوصول للاستخدام البحثي والتجاري
- **التطوير المدفوع بالبحث**: مبني باستخدام نفس البحث والتكنولوجيا التي تدعم نماذج Gemini
- **هيكلية قابلة للتوسع**: أحجام نماذج متعددة لتناسب متطلبات حسابية مختلفة
- **ذكاء اصطناعي مسؤول**: تدابير أمان مدمجة وممارسات تطوير مسؤولة

## التقنيات الرئيسية التي تمكن عائلة Gemma

### منهجيات تدريب متقدمة

أحد الجوانب المميزة لعائلة Gemma هو نهج التدريب المتطور المشتق من أبحاث Gemini من Google. تستفيد نماذج Gemma من التقطير من نماذج أكبر، التعلم المعزز من ردود الفعل البشرية (RLHF)، وتقنيات دمج النماذج لتحقيق أداء محسن في الرياضيات، البرمجة، واتباع التعليمات.

### التكامل والفهم متعدد الوسائط

تتضمن نماذج Gemma الحديثة قدرات متعددة الوسائط متطورة تمكن من الفهم الشامل عبر أنواع مختلفة من المدخلات:

**تكامل الرؤية والنص (Gemma 3)**: يمكن لـ Gemma 3 معالجة النصوص والصور في وقت واحد، مما يسمح لها بتحليل الصور، الإجابة على الأسئلة حول المحتوى البصري، استخراج النصوص من الصور، وفهم البيانات البصرية المعقدة.

**معالجة الصوت (Gemma 3n)**: يتميز Gemma 3n بقدرات صوتية متقدمة تشمل التعرف التلقائي على الكلام (ASR) والترجمة التلقائية للكلام (AST)، مع أداء قوي بشكل خاص في الترجمة بين الإنجليزية والإسبانية، الفرنسية، الإيطالية، والبرتغالية.

**معالجة المدخلات المتداخلة**: تدعم نماذج Gemma المدخلات المتداخلة عبر الوسائط، مما يمكن من فهم التفاعلات متعددة الوسائط المعقدة حيث يمكن معالجة النصوص والصور والصوت معًا.

### ابتكارات هيكلية

تتضمن عائلة Gemma العديد من التحسينات الهيكلية المصممة لتحقيق الأداء والكفاءة:

**توسيع نافذة السياق**: تتميز نماذج Gemma 3 بنافذة سياق بحجم 128K رمز، أكبر بـ16 مرة من نماذج Gemma السابقة، مما يمكن من معالجة كميات هائلة من المعلومات بما في ذلك مستندات متعددة أو مئات الصور.

**هيكلية موجهة للأجهزة المحمولة (Gemma 3n)**: يستفيد Gemma 3n من تقنية Per-Layer Embeddings (PLE) وهيكلية MatFormer، مما يسمح للنماذج الأكبر بالعمل مع بصمات ذاكرة قابلة للمقارنة مع النماذج التقليدية الأصغر.

**قدرات استدعاء الوظائف**: يدعم Gemma 3 استدعاء الوظائف، مما يمكن المطورين من بناء واجهات لغة طبيعية لواجهات البرمجة وإنشاء أنظمة أتمتة ذكية.

## حجم النموذج وخيارات النشر

تستفيد بيئات النشر الحديثة من مرونة نماذج Gemma عبر متطلبات حسابية متنوعة:

### نماذج صغيرة (0.6B-4B)

توفر Gemma نماذج صغيرة فعالة مناسبة للنشر على الحافة، التطبيقات المحمولة، والبيئات المحدودة الموارد مع الحفاظ على قدرات مذهلة. النموذج 1B مثالي للتطبيقات الصغيرة، بينما يوفر النموذج 4B أداءً متوازنًا ومرونة مع دعم متعدد الوسائط.

### نماذج متوسطة (8B-14B)

توفر النماذج متوسطة الحجم قدرات محسنة للتطبيقات المهنية، مما يوفر توازنًا ممتازًا بين الأداء والمتطلبات الحسابية للنشر على محطات العمل والخوادم.

### نماذج كبيرة (27B+)

توفر النماذج كاملة النطاق أداءً متقدمًا للتطبيقات المتطلبة، البحث، والنشر المؤسسي الذي يتطلب أقصى قدر من القدرات. النموذج 27B يمثل الخيار الأكثر قدرة الذي يمكن تشغيله على وحدة معالجة رسومات واحدة.

### نماذج محسنة للأجهزة المحمولة (Gemma 3n)

تم تصميم نماذج Gemma 3n E2B وE4B خصيصًا للنشر على الأجهزة المحمولة والحافة، مع عدد فعال من المعلمات يبلغ 2B و4B على التوالي، مع استخدام هيكلية مبتكرة لتقليل بصمة الذاكرة إلى 2GB فقط لـ E2B و3GB لـ E4B.

## فوائد عائلة نماذج Gemma

### إمكانية الوصول المفتوح

توفر نماذج Gemma شفافية كاملة وقدرات تخصيص مع أوزان مفتوحة تسمح بالاستخدام التجاري المسؤول، مما يمكن المنظمات من ضبطها ونشرها في مشاريعها وتطبيقاتها الخاصة.

### مرونة النشر

مجموعة أحجام النماذج تمكن النشر عبر تكوينات أجهزة متنوعة، من الأجهزة المحمولة إلى الخوادم عالية الأداء، مع تحسينات لمنصات مختلفة بما في ذلك Google Cloud TPUs، وحدات معالجة الرسومات NVIDIA، وحدات معالجة الرسومات AMD عبر ROCm، وتنفيذ وحدة المعالجة المركزية عبر Gemma.cpp.

### التميز متعدد اللغات

تتفوق نماذج Gemma في الفهم والتوليد متعدد اللغات، حيث تدعم أكثر من 140 لغة مع قدرات متعددة اللغات لا مثيل لها، مما يجعلها مناسبة للتطبيقات العالمية.

### الأداء التنافسي

تحقق نماذج Gemma نتائج تنافسية باستمرار على المعايير، حيث تحتل Gemma 3 مرتبة عالية بين النماذج المغلقة والمفتوحة الشهيرة في تقييمات تفضيلات المستخدم.

### قدرات متخصصة

تستفيد التطبيقات الخاصة بالمجالات من فهم Gemma متعدد الوسائط، قدرات استدعاء الوظائف، والأداء المحسن عبر منصات الأجهزة المختلفة.

## أمثلة عملية وحالات استخدام

### مثال على الفهم متعدد الوسائط

يمكن لـ Gemma 3 معالجة النصوص والصور في وقت واحد لتحليل شامل:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### مثال على الدعم متعدد اللغات

تظهر نماذج Gemma قدرات قوية متعددة اللغات عبر أكثر من 140 لغة:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### مثال على استدعاء الوظائف

يدعم Gemma 3 استدعاء الوظائف لبناء واجهات لغة طبيعية:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### مثال على النشر المحمول (Gemma 3n)

تم تحسين Gemma 3n للنشر على الأجهزة المحمولة والحافة مع استخدام ذاكرة فعال:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### مثال على معالجة الصوت (Gemma 3n)

يتضمن Gemma 3n قدرات صوتية متقدمة للتعرف على الكلام والترجمة:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## تطور عائلة Gemma

### Gemma 1.0 و2.0: نماذج الأساس

وضعت نماذج Gemma المبكرة المبادئ الأساسية لإمكانية الوصول المفتوح والنشر العملي:

- **Gemma-2B و7B**: الإصدار الأول يركز على فهم اللغة الفعال
- **سلسلة Gemma 1.5**: معالجة سياق موسعة وأداء محسّن
- **عائلة Gemma 2**: تقديم قدرات متعددة الوسائط وأحجام نماذج موسعة

### Gemma 3: التميز متعدد الوسائط

تميزت سلسلة Gemma 3 بتقدم كبير في القدرات متعددة الوسائط والأداء. تم بناء Gemma 3 باستخدام نفس البحث والتكنولوجيا التي تدعم نماذج Gemini 2.0، وقدمت فهم الرؤية والنص، نوافذ سياق بحجم 128K رمز، استدعاء الوظائف، ودعم لأكثر من 140 لغة.

### Gemma 3n: الابتكار الموجه للأجهزة المحمولة

يمثل Gemma 3n تقدمًا في هيكلية الذكاء الاصطناعي الموجهة للأجهزة المحمولة، مع تقنية Per-Layer Embeddings (PLE) وهيكلية MatFormer للمرونة الحسابية، وقدرات متعددة الوسائط شاملة بما في ذلك معالجة الصوت.

## تطبيقات نماذج Gemma

### التطبيقات المؤسسية

تستخدم المنظمات نماذج Gemma لتحليل المستندات مع محتوى بصري، أتمتة خدمة العملاء بدعم متعدد الوسائط، مساعدات البرمجة الذكية، وتطبيقات ذكاء الأعمال.

### الحوسبة المحمولة والحافة

تستفيد التطبيقات المحمولة من Gemma 3n للذكاء الاصطناعي في الوقت الفعلي الذي يعمل مباشرة على الأجهزة.

### التكنولوجيا التعليمية

تستخدم المنصات التعليمية نماذج Gemma لتجارب تعليمية متعددة الوسائط، مساعدات تعلم اللغة، وتجارب تعليمية تفاعلية.

### التطبيقات العالمية

تستفيد التطبيقات الدولية من قدرات Gemma متعددة اللغات والثقافات.

## التحديات والقيود

### المتطلبات الحسابية

تتطلب النماذج الأكبر موارد حسابية كبيرة لتحقيق الأداء الأمثل.

### الأداء في المجالات المتخصصة

قد تستفيد التطبيقات المتخصصة من ضبط النماذج أو تحسينها لمهام محددة.

### تعقيد اختيار النموذج

قد يكون اختيار النموذج المناسب تحديًا للمستخدمين الجدد.

### تحسين الأجهزة

قد يختلف الأداء عبر تكوينات الأجهزة المختلفة.

## مستقبل عائلة نماذج Gemma

تمثل عائلة نماذج Gemma التطور المستمر نحو ديمقراطية الذكاء الاصطناعي عالي الجودة.

## أمثلة التطوير والدمج

### البدء السريع مع Transformers

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3-8B model
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation
messages = [
    {"role": "user", "content": "Explain quantum computing and its potential applications."}
]

# Generate response
input_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### الاستخدام متعدد الوسائط مع Gemma 3

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

# Load Gemma 3 vision model
model_name = "google/gemma-3-4b-it"
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# Process image and text input
image = Image.open("chart.jpg")
messages = [
    {
        "role": "user", 
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Analyze this chart and explain the key trends you observe."}
        ]
    }
]

# Prepare inputs
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

# Generate multimodal response
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

response = processor.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### تنفيذ استدعاء الوظائف

```python
import json

# Define available functions
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    },
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    }
]

def gemma_function_calling(user_query, available_functions):
    """Implement function calling with Gemma 3"""
    
    # Create system prompt with function definitions
    system_prompt = f"""You are a helpful assistant with access to the following functions:

{json.dumps(available_functions, indent=2)}

When the user asks for something that requires a function call, respond with a JSON object containing:
- "function_name": the name of the function to call
- "parameters": the parameters to pass to the function

If no function call is needed, respond normally."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    # Process with Gemma
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        temperature=0.3  # Lower temperature for more structured output
    )
    
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    # Parse function call if present
    try:
        function_call = json.loads(response.strip())
        if "function_name" in function_call:
            return function_call
    except json.JSONDecodeError:
        pass
    
    return {"response": response}

# Example usage
user_request = "What's the weather like in Tokyo and calculate 15% of 850"
result = gemma_function_calling(user_request, functions)
print(result)
```

### 📱 النشر المحمول مع Gemma 3n

```python
# Optimized mobile deployment
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class MobileGemmaService:
    """Optimized Gemma 3n service for mobile deployment"""
    
    def __init__(self, model_name="google/gemma-3n-E2B-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._load_optimized_model()
    
    def _load_optimized_model(self):
        """Load model with mobile optimizations"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load with optimizations for mobile
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # Quantization for efficiency
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if torch.cuda.is_available():
            self.model = torch.jit.trace(self.model, example_inputs=...)  # JIT optimization
    
    def mobile_chat(self, user_input, max_tokens=150):
        """Optimized chat for mobile devices"""
        messages = [{"role": "user", "content": user_input}]
        
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            max_length=512,
            truncation=True
        )
        
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(input_text, "").strip()
    
    def process_audio(self, audio_path, task="transcribe"):
        """Process audio input with Gemma 3n"""
        if task == "transcribe":
            prompt = f"<audio>{audio_path}</audio>\nTranscribe:"
        elif task == "translate":
            prompt = f"<audio>{audio_path}</audio>\nTranslate to English:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return result.replace(prompt, "").strip()

# Initialize mobile service
mobile_gemma = MobileGemmaService()

# Example mobile usage
quick_response = mobile_gemma.mobile_chat("Summarize renewable energy benefits in 2 sentences")
print(f"Mobile Response: {quick_response}")

# Audio processing example
# audio_transcript = mobile_gemma.process_audio("voice_note.wav", task="transcribe")
# print(f"Audio Transcript: {audio_transcript}")
```

### النشر عبر API باستخدام vLLM

```python
from vllm import LLM, SamplingParams
import asyncio
from typing import List, Dict

class GemmaAPIService:
    """High-performance Gemma API service using vLLM"""
    
    def __init__(self, model_name="google/gemma-3-8b-it"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            trust_remote_code=True
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop_token_ids=[self.llm.get_tokenizer().eos_token_id]
        )
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for API processing"""
        tokenizer = self.llm.get_tokenizer()
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_batch(self, batch_messages: List[List[Dict[str, str]]]) -> List[str]:
        """Generate responses for batch of conversations"""
        formatted_prompts = [self.format_messages(messages) for messages in batch_messages]
        
        # Generate responses
        outputs = self.llm.generate(formatted_prompts, self.sampling_params)
        
        # Extract responses
        responses = []
        for output in outputs:
            response = output.outputs[0].text.strip()
            responses.append(response)
        
        return responses
    
    async def stream_generate(self, messages: List[Dict[str, str]]):
        """Stream generation for real-time applications"""
        formatted_prompt = self.format_messages(messages)
        
        # Note: vLLM streaming implementation would go here
        # This is a simplified example
        outputs = self.llm.generate([formatted_prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        # Simulate streaming by yielding chunks
        words = response.split()
        for i in range(0, len(words), 3):  # Yield 3 words at a time
            chunk = " ".join(words[i:i+3])
            yield chunk
            await asyncio.sleep(0.1)  # Simulate streaming delay

# Example API usage
async def api_example():
    api_service = GemmaAPIService()
    
    # Batch processing
    batch_conversations = [
        [{"role": "user", "content": "Explain machine learning briefly"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}],
        [{"role": "user", "content": "How does photosynthesis work?"}]
    ]
    
    responses = await api_service.generate_batch(batch_conversations)
    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response}")
    
    # Streaming example
    print("\nStreaming response:")
    stream_messages = [{"role": "user", "content": "Write a short story about AI and humans working together"}]
    async for chunk in api_service.stream_generate(stream_messages):
        print(chunk, end=" ", flush=True)

# Run API example
# asyncio.run(api_example())
```

## معايير الأداء والإنجازات

### أبرز الأداء

**التميز متعدد الوسائط:**
- تقدم Gemma 3 قدرات قوية للمطورين مع إمكانيات متقدمة لفهم النصوص والصور، حيث تدعم إدخال الصور والنصوص لفهم متعدد الوسائط  
- تحتل Gemma 3n مرتبة عالية بين النماذج المفتوحة والمملوكة في درجات Elo في Chatbot Arena، مما يشير إلى تفضيل قوي من المستخدمين  

**إنجازات الكفاءة:**  
- يمكن لنماذج Gemma 3 التعامل مع مدخلات تصل إلى 128 ألف رمز، وهو نافذة سياق أكبر بـ16 مرة مقارنة بالنماذج السابقة  
- تستفيد Gemma 3n من تقنية Per-Layer Embeddings (PLE) التي تقلل بشكل كبير من استخدام ذاكرة الوصول العشوائي مع الحفاظ على قدرات النماذج الكبيرة  

**تحسينات الهواتف المحمولة:**  
- تعمل Gemma 3n E2B بذاكرة تصل إلى 2 جيجابايت فقط، بينما تحتاج E4B إلى 3 جيجابايت فقط، على الرغم من أن عدد المعاملات الخام يبلغ 5 مليار و8 مليار على التوالي  
- قدرات الذكاء الاصطناعي في الوقت الفعلي مباشرة على الأجهزة المحمولة مع تشغيل يحترم الخصوصية ويعمل دون اتصال  

**مقياس التدريب:**  
- تم تدريب Gemma 3 على 2 تريليون رمز لنماذج 1 مليار، و4 تريليون لنماذج 4 مليار، و12 تريليون لنماذج 12 مليار، و14 تريليون رمز لنماذج 27 مليار باستخدام Google TPUs وإطار عمل JAX  

### مقارنة النماذج  

| سلسلة النماذج | نطاق المعاملات | طول السياق | نقاط القوة الرئيسية | أفضل حالات الاستخدام |
|---------------|----------------|-------------|----------------------|-----------------------|
| **Gemma 3** | 1 مليار - 27 مليار | 128 ألف | فهم متعدد الوسائط، استدعاء الوظائف | التطبيقات العامة، مهام اللغة والرؤية |
| **Gemma 3n** | E2B (5 مليار)، E4B (8 مليار) | متغير | تحسين الهواتف المحمولة، معالجة الصوت | تطبيقات الهواتف المحمولة، الحوسبة الطرفية، الذكاء الاصطناعي في الوقت الفعلي |
| **Gemma 2.5** | 0.5 مليار - 72 مليار | 32 ألف - 128 ألف | أداء متوازن، متعدد اللغات | النشر الإنتاجي، سير العمل الحالي |
| **Gemma-VL** | متنوع | متغير | تخصص في اللغة والرؤية | تحليل الصور، الإجابة على الأسئلة البصرية |

## دليل اختيار النموذج  

### للتطبيقات الأساسية  
- **Gemma 3-1B**: مهام النصوص الخفيفة، تطبيقات الهواتف المحمولة البسيطة  
- **Gemma 3-4B**: أداء متوازن مع دعم متعدد الوسائط للاستخدام العام  

### للتطبيقات متعددة الوسائط  
- **Gemma 3-4B/12B**: فهم الصور، الإجابة على الأسئلة البصرية  
- **Gemma 3n**: تطبيقات الهواتف المحمولة متعددة الوسائط مع قدرات معالجة الصوت  

### للنشر على الهواتف المحمولة والحوسبة الطرفية  
- **Gemma 3n E2B**: الأجهزة ذات الموارد المحدودة، الذكاء الاصطناعي في الوقت الفعلي على الهواتف المحمولة  
- **Gemma 3n E4B**: أداء محسن للهواتف المحمولة مع قدرات الصوت  

### للنشر المؤسسي  
- **Gemma 3-12B/27B**: فهم عالي الأداء للغة والرؤية  
- **قدرات استدعاء الوظائف**: بناء أنظمة أتمتة ذكية  

### للتطبيقات العالمية  
- **أي إصدار من Gemma 3**: دعم لأكثر من 140 لغة مع فهم ثقافي  
- **Gemma 3n**: تطبيقات عالمية تعتمد على الهواتف المحمولة مع ترجمة صوتية  

## منصات النشر وإمكانية الوصول  

### منصات السحابة  
- **Vertex AI**: قدرات MLOps شاملة مع تجربة بدون خوادم  
- **Google Kubernetes Engine (GKE)**: نشر الحاويات القابل للتوسع للمهام المعقدة  
- **Google GenAI API**: وصول مباشر إلى API للتجارب السريعة  
- **NVIDIA API Catalog**: أداء محسن على وحدات معالجة الرسومات من NVIDIA  

### أطر العمل للتطوير المحلي  
- **Hugging Face Transformers**: تكامل قياسي للتطوير  
- **Ollama**: نشر وإدارة محلية مبسطة  
- **vLLM**: تقديم عالي الأداء للإنتاج  
- **Gemma.cpp**: تنفيذ محسن لوحدات المعالجة المركزية  
- **Google AI Edge**: تحسين النشر على الهواتف المحمولة والحوسبة الطرفية  

### موارد التعلم  
- **Google AI Studio**: جرب نماذج Gemma بنقرات قليلة  
- **Kaggle وHugging Face**: تنزيل أوزان النماذج وأمثلة المجتمع  
- **التقارير الفنية**: وثائق شاملة وأوراق بحثية  
- **منتديات المجتمع**: دعم المجتمع النشط والمناقشات  

### البدء باستخدام نماذج Gemma  

#### منصات التطوير  
1. **Google AI Studio**: ابدأ بالتجارب عبر الويب  
2. **Hugging Face Hub**: استكشاف النماذج وتنفيذات المجتمع  
3. **النشر المحلي**: استخدم Ollama أو Transformers للتطوير  

#### مسار التعلم  
1. **فهم المفاهيم الأساسية**: دراسة القدرات متعددة الوسائط وخيارات النشر  
2. **تجربة الإصدارات المختلفة**: جرب أحجام النماذج المختلفة والإصدارات المتخصصة  
3. **ممارسة التنفيذ**: نشر النماذج في بيئات التطوير  
4. **التحسين للإنتاج**: ضبط النماذج لحالات الاستخدام والمنصات المحددة  

#### أفضل الممارسات  
- **ابدأ صغيرًا**: ابدأ بـ Gemma 3-4B للتطوير والاختبار الأولي  
- **استخدم القوالب الرسمية**: تطبيق قوالب المحادثة المناسبة للحصول على أفضل النتائج  
- **راقب الموارد**: تتبع استخدام الذاكرة وأداء الاستدلال  
- **فكر في التخصص**: اختر الإصدارات المناسبة للاحتياجات متعددة الوسائط أو الهواتف المحمولة  

## أنماط الاستخدام المتقدمة  

### أمثلة على التخصيص  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### هندسة التوجيه المتخصصة  

**للمهام متعددة الوسائط:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**لاستدعاء الوظائف مع السياق:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### التطبيقات متعددة اللغات مع السياق الثقافي  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### أنماط النشر الإنتاجي  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## استراتيجيات تحسين الأداء  

### تحسين الذاكرة  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### تحسين الاستدلال  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## أفضل الممارسات والإرشادات  

### الأمن والخصوصية  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### المراقبة والتقييم  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## الخاتمة  

تمثل عائلة نماذج Gemma نهج Google الشامل نحو دمقرطة تقنية الذكاء الاصطناعي مع الحفاظ على الأداء التنافسي عبر التطبيقات المختلفة وسيناريوهات النشر. من خلال التزامها بإمكانية الوصول المفتوح المصدر، والقدرات متعددة الوسائط، والتصميمات المعمارية المبتكرة، تمكن Gemma المؤسسات والمطورين من الاستفادة من قدرات الذكاء الاصطناعي القوية بغض النظر عن مواردهم أو متطلباتهم المحددة.  

### النقاط الرئيسية  

**التميز المفتوح المصدر**: تثبت Gemma أن النماذج المفتوحة المصدر يمكن أن تحقق أداءً تنافسيًا مع البدائل المملوكة مع توفير الشفافية والتخصيص والتحكم في نشر الذكاء الاصطناعي.  

**الابتكار متعدد الوسائط**: يمثل دمج قدرات النصوص والرؤية والصوت في Gemma 3 وGemma 3n تقدمًا كبيرًا في الذكاء الاصطناعي متعدد الوسائط، مما يتيح فهمًا شاملاً لأنواع المدخلات المختلفة.  

**الهندسة الموجهة للهواتف المحمولة**: تظهر تقنية Per-Layer Embeddings (PLE) المبتكرة وتحسين الهواتف المحمولة في Gemma 3n أن الذكاء الاصطناعي القوي يمكن أن يعمل بكفاءة على الأجهزة ذات الموارد المحدودة دون التضحية بالقدرات.  

**النشر القابل للتوسع**: يتيح النطاق من 1 مليار إلى 27 مليار معاملات، مع إصدارات الهواتف المحمولة المتخصصة، النشر عبر مجموعة واسعة من البيئات الحاسوبية مع الحفاظ على الجودة والأداء المتسق.  

**دمج الذكاء الاصطناعي المسؤول**: تضمن التدابير الأمنية المدمجة من خلال ShieldGemma 2 وممارسات التطوير المسؤولة أن يتم نشر قدرات الذكاء الاصطناعي القوية بأمان وأخلاقية.  

### النظرة المستقبلية  

مع استمرار تطور عائلة Gemma، يمكننا توقع:  

**قدرات محسنة للهواتف المحمولة**: تحسين إضافي للنشر على الهواتف المحمولة والحوسبة الطرفية مع دمج هندسة Gemma 3n في منصات رئيسية مثل Android وChrome.  

**فهم متعدد الوسائط موسع**: تقدم مستمر في دمج النصوص والرؤية والصوت لتجارب ذكاء اصطناعي أكثر شمولية.  

**كفاءة محسنة**: ابتكارات معمارية مستمرة لتقديم نسب أداء أفضل لكل معاملة ومتطلبات حسابية أقل.  

**تكامل أوسع للنظام البيئي**: دعم محسّن عبر أطر العمل التطويرية، ومنصات السحابة، وأدوات النشر للتكامل السلس في سير العمل الحالي.  

**نمو المجتمع**: توسع مستمر في Gemmaverse مع نماذج وأدوات وتطبيقات تم إنشاؤها بواسطة المجتمع والتي توسع القدرات الأساسية.  

### الخطوات التالية  

سواء كنت تبني تطبيقات الهواتف المحمولة بقدرات ذكاء اصطناعي في الوقت الفعلي، أو تطور أدوات تعليمية متعددة الوسائط، أو تنشئ أنظمة أتمتة ذكية، أو تعمل على تطبيقات عالمية تتطلب دعمًا متعدد اللغات، توفر عائلة Gemma حلولًا قابلة للتوسع مع دعم قوي من المجتمع ووثائق شاملة.  

**توصيات البدء:**  
1. **جرب Google AI Studio** للحصول على تجربة عملية فورية  
2. **قم بتنزيل النماذج من Hugging Face** للتطوير المحلي والتخصيص  
3. **استكشاف الإصدارات المتخصصة** مثل Gemma 3n لتطبيقات الهواتف المحمولة  
4. **تنفيذ القدرات متعددة الوسائط** لتجارب ذكاء اصطناعي شاملة  
5. **اتباع أفضل ممارسات الأمان** للنشر الإنتاجي  

**للتطوير على الهواتف المحمولة**: ابدأ بـ Gemma 3n E2B للنشر الفعال من حيث الموارد مع قدرات الصوت والرؤية.  

**للتطبيقات المؤسسية**: ضع في اعتبارك نماذج Gemma 3-12B أو 27B للحصول على أقصى قدر من القدرات مع استدعاء الوظائف والتفكير المتقدم.  

**للتطبيقات العالمية**: استفد من دعم Gemma لأكثر من 140 لغة مع هندسة توجيه تراعي الثقافة.  

**لحالات الاستخدام المتخصصة**: استكشاف طرق التخصيص وتقنيات التحسين الخاصة بالمجال.  

### 🔮 دمقرطة الذكاء الاصطناعي  

تمثل عائلة Gemma مستقبل تطوير الذكاء الاصطناعي حيث تكون النماذج القوية والقادرة متاحة للجميع من المطورين الأفراد إلى المؤسسات الكبيرة. من خلال الجمع بين البحث المتقدم وإمكانية الوصول المفتوح المصدر، أنشأت Google أساسًا يمكّن الابتكار عبر جميع القطاعات والمقاييس.  

نجاح Gemma مع أكثر من 100 مليون تنزيل و60,000+ إصدار مجتمعي يوضح قوة التعاون المفتوح في تقدم تقنية الذكاء الاصطناعي. مع تقدمنا، ستستمر عائلة Gemma في العمل كمحفز للابتكار في الذكاء الاصطناعي، مما يتيح تطوير تطبيقات كانت ممكنة سابقًا فقط باستخدام نماذج مملوكة ومكلفة.  

مستقبل الذكاء الاصطناعي مفتوح، ومتاح، وقوي – وعائلة Gemma تقود الطريق لجعل هذه الرؤية حقيقة.  

## موارد إضافية  

**الوثائق الرسمية والنماذج:**  
- **Google AI Studio**: [جرب نماذج Gemma مباشرة](https://aistudio.google.com)  
- **مجموعات Hugging Face**:  
  - [إصدار Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [معاينة Gemma 3n](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **وثائق مطوري Google AI**: [أدلة شاملة لـ Gemma](https://ai.google.dev/gemma)  
- **وثائق Vertex AI**: [أدلة النشر المؤسسية](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**الموارد التقنية:**  
- **الأوراق البحثية والتقارير الفنية**: [منشورات Google DeepMind](https://deepmind.google/models/gemma/)  
- **مدونات المطورين**: [الإعلانات والدروس التعليمية الأخيرة](https://developers.googleblog.com)  
- **بطاقات النماذج**: مواصفات تقنية مفصلة ومعايير الأداء  

**المجتمع والدعم:**  
- **مجتمع Hugging Face**: مناقشات نشطة وأمثلة المجتمع  
- **مستودعات GitHub**: تنفيذات وأدوات مفتوحة المصدر  
- **منتديات المطورين**: دعم مجتمع مطوري Google AI  
- **Stack Overflow**: أسئلة موسومة وحلول المجتمع  

**أدوات التطوير:**  
- **Ollama**: [نشر محلي بسيط](https://ollama.ai)  
- **vLLM**: [تقديم عالي الأداء](https://github.com/vllm-project/vllm)  
- **مكتبة Transformers**: [تكامل Hugging Face](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: تحسين النشر على الهواتف المحمولة والحوسبة الطرفية  

**مسارات التعلم:**  
- **مبتدئ**: ابدأ بـ Google AI Studio → أمثلة Hugging Face → النشر المحلي  
- **مطور**: تكامل Transformers → تطبيقات مخصصة → النشر الإنتاجي  
- **باحث**: أوراق تقنية → تخصيص → تطبيقات جديدة  
- **مؤسسة**: نشر Vertex AI → تنفيذ الأمان → تحسين النطاق  

تمثل عائلة نماذج Gemma ليس فقط مجموعة من نماذج الذكاء الاصطناعي، بل نظامًا بيئيًا كاملًا لبناء مستقبل التطبيقات الذكية، القوية، والمسؤولة. ابدأ الاستكشاف اليوم وانضم إلى مجتمع المطورين والباحثين المتنامي الذي يدفع حدود الممكن مع الذكاء الاصطناعي المفتوح المصدر.

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.