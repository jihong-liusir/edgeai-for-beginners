<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T17:36:33+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ar"
}
-->
# القسم 03 - دمج بروتوكول سياق النموذج (MCP)

## مقدمة إلى MCP (بروتوكول سياق النموذج)

بروتوكول سياق النموذج (MCP) هو إطار عمل ثوري يتيح للنماذج اللغوية التفاعل مع الأدوات والأنظمة الخارجية بطريقة موحدة. على عكس الأساليب التقليدية التي تكون فيها النماذج معزولة، يقوم MCP بإنشاء جسر بين نماذج الذكاء الاصطناعي والعالم الحقيقي من خلال بروتوكول محدد جيدًا.

### ما هو MCP؟

يعمل MCP كبروتوكول اتصال يسمح للنماذج اللغوية بـ:
- الاتصال بمصادر البيانات الخارجية
- تنفيذ الأدوات والوظائف
- التفاعل مع واجهات برمجة التطبيقات والخدمات
- الوصول إلى المعلومات في الوقت الفعلي
- تنفيذ عمليات متعددة الخطوات المعقدة

هذا البروتوكول يحول النماذج اللغوية الثابتة إلى وكلاء ديناميكيين قادرين على أداء مهام عملية تتجاوز توليد النصوص.

## نماذج اللغة الصغيرة (SLMs) في MCP

تمثل نماذج اللغة الصغيرة نهجًا فعالًا لنشر الذكاء الاصطناعي، وتوفر العديد من المزايا:

### فوائد SLMs
- **كفاءة الموارد**: متطلبات حسابية أقل
- **أوقات استجابة أسرع**: تقليل التأخير للتطبيقات في الوقت الفعلي  
- **فعالية التكلفة**: احتياجات بنية تحتية بسيطة
- **الخصوصية**: يمكن تشغيلها محليًا دون نقل البيانات
- **التخصيص**: أسهل في التعديل لتناسب المجالات المحددة

### لماذا تعمل SLMs بشكل جيد مع MCP

عند دمج SLMs مع MCP، يتم إنشاء مزيج قوي حيث يتم تعزيز قدرات التفكير للنموذج باستخدام الأدوات الخارجية، مما يعوض عن العدد الأصغر من المعلمات من خلال الوظائف المحسنة.

## نظرة عامة على Python MCP SDK

يوفر Python MCP SDK الأساس لبناء تطبيقات تدعم MCP. يتضمن SDK:

- **مكتبات العميل**: للاتصال بخوادم MCP
- **إطار عمل الخادم**: لإنشاء خوادم MCP مخصصة
- **معالجات البروتوكول**: لإدارة الاتصال
- **دمج الأدوات**: لتنفيذ الوظائف الخارجية

## التنفيذ العملي: عميل Phi-4 MCP

دعونا نستكشف تنفيذًا عمليًا باستخدام نموذج Phi-4 المصغر من Microsoft المدمج مع قدرات MCP.

### بنية النظام

يتبع التنفيذ بنية طبقية:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### المكونات الأساسية

#### 1. فئات عميل MCP

**BaseMCPClient**: الأساس المجرد الذي يوفر الوظائف المشتركة
- بروتوكول مدير السياق غير المتزامن
- تعريف واجهة قياسية
- إدارة الموارد

**Phi4MiniMCPClient**: تنفيذ يعتمد على STDIO
- اتصال العملية المحلية
- معالجة الإدخال/الإخراج القياسي
- إدارة العمليات الفرعية

**Phi4MiniSSEMCPClient**: تنفيذ أحداث الخادم المرسلة
- اتصال HTTP المتدفق
- معالجة الأحداث في الوقت الفعلي
- الاتصال بالخوادم عبر الويب

#### 2. دمج LLM

**OllamaClient**: استضافة النموذج محليًا
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: تقديم عالي الأداء
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. خط أنابيب معالجة الأدوات

يقوم خط أنابيب معالجة الأدوات بتحويل أدوات MCP إلى تنسيقات متوافقة مع النماذج اللغوية:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## البدء: دليل خطوة بخطوة

### الخطوة 1: إعداد البيئة

قم بتثبيت التبعيات المطلوبة:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### الخطوة 2: التكوين الأساسي

قم بإعداد متغيرات البيئة الخاصة بك:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### الخطوة 3: تشغيل أول عميل MCP

**إعداد Ollama الأساسي:**
```bash
python ghmodel_mcp_demo.py
```

**استخدام vLLM كخلفية:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**اتصال أحداث الخادم المرسلة:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**خادم MCP مخصص:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### الخطوة 4: الاستخدام البرمجي

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## الميزات المتقدمة

### دعم متعدد الخلفيات

يدعم التنفيذ كلًا من خلفيات Ollama وvLLM، مما يتيح لك الاختيار بناءً على احتياجاتك:

- **Ollama**: أفضل للتطوير المحلي والاختبار
- **vLLM**: مُحسّن للإنتاج والسيناريوهات ذات الإنتاجية العالية

### بروتوكولات اتصال مرنة

يتم دعم وضعين للاتصال:

**وضع STDIO**: اتصال مباشر بالعملية
- تأخير أقل
- مناسب للأدوات المحلية
- إعداد بسيط

**وضع SSE**: بث يعتمد على HTTP
- قابلية العمل عبر الشبكة
- أفضل للأنظمة الموزعة
- تحديثات في الوقت الفعلي

### قدرات دمج الأدوات

يمكن للنظام دمج أدوات متنوعة:
- أتمتة الويب (Playwright)
- عمليات الملفات
- التفاعلات مع واجهات برمجة التطبيقات
- أوامر النظام
- وظائف مخصصة

## معالجة الأخطاء وأفضل الممارسات

### إدارة شاملة للأخطاء

يتضمن التنفيذ معالجة قوية للأخطاء لـ:

**أخطاء الاتصال:**
- فشل خادم MCP
- انتهاء مهلة الشبكة
- مشاكل الاتصال

**أخطاء تنفيذ الأدوات:**
- أدوات مفقودة
- التحقق من صحة المعلمات
- فشل التنفيذ

**أخطاء معالجة الاستجابة:**
- مشاكل تحليل JSON
- تناقضات التنسيق
- شذوذ في استجابة LLM

### أفضل الممارسات

1. **إدارة الموارد**: استخدم مديري السياق غير المتزامنين
2. **معالجة الأخطاء**: قم بتنفيذ كتل try-catch شاملة
3. **التسجيل**: قم بتمكين مستويات التسجيل المناسبة
4. **الأمان**: تحقق من المدخلات ونظف المخرجات
5. **الأداء**: استخدم تجميع الاتصالات والتخزين المؤقت

## التطبيقات العملية

### أتمتة الويب
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### معالجة البيانات
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### دمج واجهات برمجة التطبيقات
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## تحسين الأداء

### إدارة الذاكرة
- معالجة فعالة لتاريخ الرسائل
- تنظيف الموارد بشكل صحيح
- تجميع الاتصالات

### تحسين الشبكة
- عمليات HTTP غير المتزامنة
- إعدادات مهلة قابلة للتكوين
- استرداد الأخطاء بسلاسة

### المعالجة المتزامنة
- إدخال/إخراج غير محظور
- تنفيذ الأدوات بالتوازي
- أنماط غير متزامنة فعالة

## اعتبارات الأمان

### حماية البيانات
- إدارة آمنة لمفاتيح واجهات برمجة التطبيقات
- التحقق من صحة المدخلات
- تنظيف المخرجات

### أمان الشبكة
- دعم HTTPS
- إعدادات افتراضية لنقاط النهاية المحلية
- إدارة الرموز بشكل آمن

### سلامة التنفيذ
- تصفية الأدوات
- بيئات معزولة
- تسجيل التدقيق

## الخاتمة

تمثل نماذج اللغة الصغيرة المدمجة مع MCP تحولًا جذريًا في تطوير تطبيقات الذكاء الاصطناعي. من خلال الجمع بين كفاءة النماذج الصغيرة وقوة الأدوات الخارجية، يمكن للمطورين إنشاء أنظمة ذكية تكون فعالة من حيث الموارد وقادرة للغاية.

يُظهر تنفيذ عميل Phi-4 MCP كيف يمكن تحقيق هذا الدمج عمليًا، مما يوفر أساسًا قويًا لبناء تطبيقات مدعومة بالذكاء الاصطناعي.

النقاط الرئيسية:
- MCP يربط الفجوة بين النماذج اللغوية والأنظمة الخارجية
- تقدم SLMs كفاءة دون التضحية بالقدرات عند تعزيزها بالأدوات
- تتيح البنية المعيارية التوسيع والتخصيص بسهولة
- تعتبر معالجة الأخطاء وتدابير الأمان ضرورية للاستخدام في الإنتاج

يوفر هذا الدليل الأساس لبناء تطبيقات MCP مدعومة بـ SLM، مما يفتح إمكانيات للأتمتة ومعالجة البيانات ودمج الأنظمة الذكية.

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.