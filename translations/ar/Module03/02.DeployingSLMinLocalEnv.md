<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "308873206bc5ec5d87e34ca2907d2c9c",
  "translation_date": "2025-09-17T18:09:21+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "ar"
}
-->
# القسم 2: نشر البيئة المحلية - حلول تحافظ على الخصوصية

يمثل نشر نماذج اللغة الصغيرة (SLMs) محليًا تحولًا جذريًا نحو حلول الذكاء الاصطناعي التي تحافظ على الخصوصية وتخفض التكاليف. يستعرض هذا الدليل الشامل إطارين قويين—Ollama وMicrosoft Foundry Local—لتمكين المطورين من استغلال الإمكانيات الكاملة لـ SLMs مع الحفاظ على السيطرة الكاملة على بيئة النشر.

## المقدمة

في هذه الدرس، سنستكشف استراتيجيات النشر المتقدمة لنماذج اللغة الصغيرة في البيئات المحلية. سنغطي المفاهيم الأساسية لنشر الذكاء الاصطناعي محليًا، ونستعرض منصتين رائدتين (Ollama وMicrosoft Foundry Local)، ونقدم إرشادات عملية لتطبيق حلول جاهزة للإنتاج.

## أهداف التعلم

بنهاية هذا الدرس، ستكون قادرًا على:

- فهم بنية وفوائد أطر نشر SLM المحلية.
- تنفيذ عمليات نشر جاهزة للإنتاج باستخدام Ollama وMicrosoft Foundry Local.
- مقارنة واختيار المنصة المناسبة بناءً على المتطلبات والقيود المحددة.
- تحسين عمليات النشر المحلية من حيث الأداء والأمان وقابلية التوسع.

## فهم بنية نشر SLM المحلية

يمثل نشر SLM محليًا تحولًا أساسيًا من خدمات الذكاء الاصطناعي المعتمدة على السحابة إلى حلول تحافظ على الخصوصية وتعمل داخل المؤسسة. يتيح هذا النهج للمؤسسات السيطرة الكاملة على بنيتها التحتية للذكاء الاصطناعي مع ضمان سيادة البيانات واستقلالية العمليات.

### تصنيفات أطر النشر

فهم النهج المختلفة للنشر يساعد في اختيار الاستراتيجية المناسبة لحالات الاستخدام المحددة:

- **موجه للتطوير**: إعداد مبسط للتجارب والنماذج الأولية.
- **مستوى المؤسسات**: حلول جاهزة للإنتاج مع قدرات تكامل للمؤسسات.
- **متعدد المنصات**: توافق عالمي عبر أنظمة التشغيل المختلفة والأجهزة.

### المزايا الرئيسية لنشر SLM المحلي

يوفر نشر SLM محليًا العديد من المزايا الأساسية التي تجعله مثاليًا للتطبيقات الحساسة للخصوصية والمؤسسات:

**الخصوصية والأمان**: يضمن المعالجة المحلية عدم مغادرة البيانات الحساسة لبنية المؤسسة، مما يتيح الامتثال لـ GDPR وHIPAA ومتطلبات تنظيمية أخرى. يمكن تنفيذ عمليات نشر معزولة للبيئات المصنفة، بينما توفر سجلات التدقيق الكاملة مراقبة أمنية.

**خفض التكاليف**: القضاء على نماذج التسعير لكل رمز يقلل بشكل كبير من تكاليف التشغيل. متطلبات النطاق الترددي المنخفضة والاعتماد الأقل على السحابة توفر هياكل تكلفة متوقعة لميزانيات المؤسسات.

**الأداء والموثوقية**: أوقات استنتاج أسرع بدون تأخير الشبكة تمكن التطبيقات في الوقت الفعلي. تضمن الوظائف غير المتصلة بالإنترنت التشغيل المستمر بغض النظر عن الاتصال بالإنترنت، بينما يوفر تحسين الموارد المحلية أداءً ثابتًا.

## Ollama: منصة نشر محلية عالمية

### البنية الأساسية والفلسفة

تم تصميم Ollama كمنصة عالمية سهلة الاستخدام للمطورين، تتيح نشر نماذج اللغة الكبيرة محليًا عبر تكوينات الأجهزة وأنظمة التشغيل المختلفة.

**الأساس التقني**: يعتمد Ollama على إطار العمل القوي llama.cpp ويستخدم تنسيق النموذج الفعال GGUF لتحقيق الأداء الأمثل. يضمن التوافق عبر الأنظمة الأساسية السلوك المتسق عبر Windows وmacOS وLinux، بينما تعمل إدارة الموارد الذكية على تحسين استخدام وحدة المعالجة المركزية ووحدة معالجة الرسومات والذاكرة.

**فلسفة التصميم**: يركز Ollama على البساطة دون التضحية بالوظائف، حيث يقدم نشرًا بدون إعدادات لتحقيق الإنتاجية الفورية. تحافظ المنصة على توافق واسع مع النماذج مع توفير واجهات برمجية متسقة عبر بنى النماذج المختلفة.

### الميزات والقدرات المتقدمة

**إدارة النماذج بكفاءة**: يوفر Ollama إدارة شاملة لدورة حياة النماذج مع السحب التلقائي والتخزين المؤقت وإدارة الإصدارات. تدعم المنصة نظامًا بيئيًا واسعًا للنماذج بما في ذلك Llama 3.2 وGoogle Gemma 2 وMicrosoft Phi-4 وQwen 2.5 وDeepSeek وMistral ونماذج التضمين المتخصصة.

**التخصيص عبر ملفات النماذج**: يمكن للمستخدمين المتقدمين إنشاء تكوينات نماذج مخصصة مع معلمات محددة، ومطالبات النظام، وتعديلات السلوك. يتيح ذلك تحسينات خاصة بالمجال ومتطلبات التطبيقات المتخصصة.

**تحسين الأداء**: يكتشف Ollama تلقائيًا ويستخدم تسريع الأجهزة المتاح بما في ذلك NVIDIA CUDA وApple Metal وOpenCL. تضمن إدارة الذاكرة الذكية الاستخدام الأمثل للموارد عبر تكوينات الأجهزة المختلفة.

### استراتيجيات التنفيذ للإنتاج

**التثبيت والإعداد**: يوفر Ollama تثبيتًا مبسطًا عبر الأنظمة الأساسية من خلال المثبتات الأصلية، ومديري الحزم (WinGet، Homebrew، APT)، وحاويات Docker للنشر المعبأ.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**الأوامر والعمليات الأساسية**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**التكوين المتقدم**: تتيح ملفات النماذج تخصيصًا متطورًا لمتطلبات المؤسسات:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### أمثلة تكامل المطورين

**تكامل واجهة برمجة التطبيقات في Python**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**تكامل JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**استخدام واجهة برمجة التطبيقات RESTful مع cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### تحسين الأداء والتخصيص

**تكوين الذاكرة والخيوط**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**اختيار التكميم للأجهزة المختلفة**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: منصة الذكاء الاصطناعي الطرفية للمؤسسات

### بنية على مستوى المؤسسات

يمثل Microsoft Foundry Local حلاً شاملاً للمؤسسات مصممًا خصيصًا لنشر الذكاء الاصطناعي الطرفي للإنتاج مع تكامل عميق في نظام Microsoft البيئي.

**الأساس القائم على ONNX**: يعتمد Foundry Local على ONNX Runtime القياسي في الصناعة، مما يوفر أداءً محسنًا عبر بنى الأجهزة المختلفة. تستفيد المنصة من تكامل Windows ML لتحسين Windows الأصلي مع الحفاظ على التوافق عبر الأنظمة الأساسية.

**تميز تسريع الأجهزة**: يتميز Foundry Local بالكشف الذكي عن الأجهزة وتحسينها عبر وحدات المعالجة المركزية ووحدات معالجة الرسومات ووحدات المعالجة العصبية. يضمن التعاون العميق مع موردي الأجهزة (AMD، Intel، NVIDIA، Qualcomm) الأداء الأمثل لتكوينات أجهزة المؤسسات.

### تجربة مطور متقدمة

**الوصول متعدد الواجهات**: يوفر Foundry Local واجهات تطوير شاملة بما في ذلك واجهة سطر أوامر قوية لإدارة النماذج والنشر، وSDKs متعددة اللغات (Python، NodeJS) للتكامل الأصلي، وواجهات برمجية RESTful متوافقة مع OpenAI للهجرة السلسة.

**تكامل Visual Studio**: تندمج المنصة بسلاسة مع أدوات الذكاء الاصطناعي لـ VS Code، مما يوفر أدوات تحويل النماذج والتكميم والتحسين داخل بيئة التطوير. يسرع هذا التكامل سير العمل التطويري ويقلل من تعقيد النشر.

**خط أنابيب تحسين النماذج**: يتيح تكامل Microsoft Olive سير عمل تحسين النماذج المتقدم بما في ذلك التكميم الديناميكي، وتحسين الرسوم البيانية، وضبط الأجهزة. توفر قدرات التحويل المستندة إلى السحابة عبر Azure ML تحسينًا قابلاً للتوسع للنماذج الكبيرة.

### استراتيجيات التنفيذ للإنتاج

**التثبيت والتكوين**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**عمليات إدارة النماذج**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**تكوين النشر المتقدم**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### تكامل نظام المؤسسات

**الأمان والامتثال**: يوفر Foundry Local ميزات أمان على مستوى المؤسسات بما في ذلك التحكم في الوصول القائم على الأدوار، وسجلات التدقيق، وتقارير الامتثال، وتخزين النماذج المشفر. يضمن التكامل مع بنية أمان Microsoft الالتزام بسياسات أمان المؤسسات.

**خدمات الذكاء الاصطناعي المدمجة**: تقدم المنصة قدرات ذكاء اصطناعي جاهزة للاستخدام بما في ذلك Phi Silica لمعالجة اللغة المحلية، وAI Imaging لتحسين الصور وتحليلها، وواجهات برمجية متخصصة لمهام الذكاء الاصطناعي الشائعة في المؤسسات.

## التحليل المقارن: Ollama مقابل Foundry Local

### مقارنة البنية التقنية

| **الجانب** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **تنسيق النموذج** | GGUF (عبر llama.cpp) | ONNX (عبر ONNX Runtime) |
| **تركيز المنصة** | متعدد المنصات عالميًا | تحسين Windows/المؤسسات |
| **تكامل الأجهزة** | دعم وحدة معالجة الرسومات/وحدة المعالجة المركزية العامة | دعم Windows ML العميق، وحدة المعالجة العصبية |
| **التحسين** | تكميم llama.cpp | Microsoft Olive + ONNX Runtime |
| **ميزات المؤسسات** | مدفوعة بالمجتمع | على مستوى المؤسسات مع اتفاقيات مستوى الخدمة |

### خصائص الأداء

**نقاط قوة أداء Ollama**:
- أداء وحدة المعالجة المركزية الاستثنائي من خلال تحسين llama.cpp.
- سلوك متسق عبر المنصات والأجهزة المختلفة.
- استخدام ذاكرة فعال مع تحميل ذكي للنماذج.
- أوقات بدء تشغيل سريعة للتطوير وسيناريوهات الاختبار.

**مزايا أداء Foundry Local**:
- استخدام وحدة المعالجة العصبية الفائق على أجهزة Windows الحديثة.
- تسريع وحدة معالجة الرسومات المحسن من خلال شراكات الموردين.
- مراقبة الأداء على مستوى المؤسسات وتحسينه.
- قدرات نشر قابلة للتوسع لبيئات الإنتاج.

### تحليل تجربة التطوير

**تجربة مطور Ollama**:
- متطلبات إعداد بسيطة مع إنتاجية فورية.
- واجهة سطر أوامر بديهية لجميع العمليات.
- دعم مجتمعي واسع ووثائق شاملة.
- تخصيص مرن عبر ملفات النماذج.

**تجربة مطور Foundry Local**:
- تكامل شامل مع بيئة Visual Studio.
- سير عمل تطوير المؤسسات مع ميزات التعاون الجماعي.
- قنوات دعم احترافية مدعومة من Microsoft.
- أدوات تصحيح الأخطاء والتحسين المتقدمة.

### تحسين حالات الاستخدام

**اختر Ollama عندما**:
- تطوير تطبيقات متعددة المنصات تتطلب سلوكًا متسقًا.
- إعطاء الأولوية للشفافية مفتوحة المصدر ومساهمات المجتمع.
- العمل بموارد محدودة أو قيود الميزانية.
- بناء تطبيقات تجريبية أو موجهة للبحث.
- الحاجة إلى توافق واسع مع النماذج عبر البنى المختلفة.

**اختر Foundry Local عندما**:
- نشر تطبيقات المؤسسات مع متطلبات أداء صارمة.
- الاستفادة من تحسينات الأجهزة الخاصة بـ Windows (وحدة المعالجة العصبية، Windows ML).
- الحاجة إلى دعم المؤسسات، اتفاقيات مستوى الخدمة، وميزات الامتثال.
- بناء تطبيقات الإنتاج مع تكامل نظام Microsoft البيئي.
- الحاجة إلى أدوات تحسين متقدمة وسير عمل تطوير احترافي.

## استراتيجيات النشر المتقدمة

### أنماط النشر المعبأة

**تعبئة Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**نشر المؤسسات لـ Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### تقنيات تحسين الأداء

**استراتيجيات تحسين Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**تحسين Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## اعتبارات الأمان والامتثال

### تنفيذ الأمان على مستوى المؤسسات

**أفضل ممارسات أمان Ollama**:
- عزل الشبكة باستخدام قواعد الجدار الناري والوصول عبر VPN.
- المصادقة من خلال تكامل الوكيل العكسي.
- التحقق من سلامة النموذج وتوزيع النموذج الآمن.
- تسجيل التدقيق للوصول إلى واجهات برمجية النماذج والعمليات.

**أمان المؤسسات لـ Foundry Local**:
- التحكم في الوصول القائم على الأدوار مع تكامل Active Directory.
- سجلات تدقيق شاملة مع تقارير الامتثال.
- تخزين النماذج المشفر ونشر النماذج الآمن.
- التكامل مع بنية أمان Microsoft.

### الامتثال والمتطلبات التنظيمية

يدعم كلا النظامين الامتثال التنظيمي من خلال:
- ضوابط إقامة البيانات التي تضمن المعالجة المحلية.
- تسجيل التدقيق لمتطلبات تقارير الامتثال.
- ضوابط الوصول للتعامل مع البيانات الحساسة.
- التشفير أثناء الراحة وفي النقل لحماية البيانات.

## أفضل الممارسات للنشر الإنتاجي

### المراقبة وقابلية الملاحظة

**المقاييس الرئيسية للمراقبة**:
- زمن استنتاج النموذج ومعدل الإنتاجية.
- استخدام الموارد (وحدة المعالجة المركزية، وحدة معالجة الرسومات، الذاكرة).
- أوقات استجابة واجهات برمجية التطبيقات ومعدلات الأخطاء.
- دقة النموذج وانحراف الأداء.

**تنفيذ المراقبة**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### التكامل المستمر والنشر

**تكامل خط أنابيب CI/CD**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## الاتجاهات المستقبلية والاعتبارات

### التقنيات الناشئة

يستمر مشهد نشر SLM المحلي في التطور مع عدة اتجاهات رئيسية:

**بنى النماذج المتقدمة**: تظهر نماذج SLM من الجيل التالي بكفاءة محسنة ونسب قدرات أعلى، بما في ذلك نماذج الخبراء المختلطة للتوسع الديناميكي والبنى المتخصصة للنشر الطرفي.

**تكامل الأجهزة**: يوفر التكامل الأعمق مع أجهزة الذكاء الاصطناعي المتخصصة بما في ذلك وحدات المعالجة العصبية والسيليكون المخصص ومسرعات الحوسبة الطرفية قدرات أداء محسنة.

**تطور النظام البيئي**: ستبسط جهود التوحيد عبر منصات النشر وتحسين التوافق بين الأطر المختلفة عمليات النشر متعددة المنصات.

### أنماط تبني الصناعة

**تبني المؤسسات**: زيادة تبني المؤسسات مدفوعة بمتطلبات الخصوصية، تحسين التكاليف، واحتياجات الامتثال التنظيمي. تركز قطاعات الحكومة والدفاع بشكل خاص على عمليات النشر المعزولة.

**الاعتبارات العالمية**: تدفع متطلبات سيادة البيانات الدولية تبني النشر المحلي، خاصة في المناطق ذات اللوائح الصارمة لحماية البيانات.

## التحديات والاعتبارات

### التحديات التقنية

**متطلبات البنية التحتية**: يتطلب النشر المحلي تخطيطًا دقيقًا للقدرة واختيار الأجهزة. يجب على المؤسسات تحقيق التوازن بين متطلبات الأداء وقيود التكلفة مع ضمان قابلية التوسع لأعباء العمل المتزايدة.

**🔧 الصيانة والتحديثات**: تتطلب تحديثات النماذج المنتظمة، وتصحيحات الأمان، وتحسين الأداء موارد وخبرات مخصصة. تصبح خطوط أنابيب النشر الآلية ضرورية لبيئات الإنتاج.

### اعتبارات الأمان

**أمان النموذج**: حماية النماذج الخاصة من الوصول غير المصرح به أو الاستخراج يتطلب تدابير أمان شاملة بما في ذلك التشفير، ضوابط الوصول، وتسجيل التدقيق.

**حماية البيانات**: ضمان التعامل الآمن مع البيانات طوال خط أنابيب الاستنتاج مع الحفاظ على معايير الأداء وقابلية الاستخدام.

## قائمة التحقق للتنفيذ العملي

### ✅ تقييم ما قبل النشر

- [ ] تحليل متطلبات الأجهزة وتخطيط القدرة.
- [ ] تعريف بنية الشبكة ومتطلبات الأمان.
- [ ] اختيار النموذج ومعايير الأداء.
- [ ] التحقق من الامتثال والمتطلبات التنظيمية.

### ✅ تنفيذ النشر

- [ ] اختيار المنصة بناءً على تحليل المتطلبات.
- [ ] تثبيت وتكوين المنصة المختارة.
- [ ] تنفيذ تحسين النموذج والتكميم.
- [ ] إكمال تكامل واجهات برمجية التطبيقات والاختبار.

### ✅ جاهزية الإنتاج

- [ ] تكوين نظام المراقبة والتنبيه.
- [ ] إنشاء إجراءات النسخ الاحتياطي واستعادة الكوارث.
- [ ] إكمال تحسين الأداء وضبطه.
- [ ] تطوير الوثائق ومواد التدريب.

## الخاتمة

يعتمد الاختيار بين Ollama وMicrosoft Foundry Local على متطلبات المؤسسة المحددة، القيود التقنية، والأهداف الاستراتيجية. تقدم كلا المنصتين مزايا مقنعة لنشر SLM المحلي، حيث يتفوق Ollama في التوافق متعدد المنصات وسهولة الاستخدام، بينما يوفر Foundry Local تحسينًا على مستوى المؤسسات وتكاملًا مع نظام Microsoft البيئي.

يكمن مستقبل نشر الذكاء الاصطناعي في النهج الهجينة التي تجمع بين فوائد المعالجة المحلية وقدرات السحابة. المؤسسات التي تتقن نشر SLM المحلي ستكون في وضع جيد للاستفادة من تقنيات الذكاء الاصطناعي مع الحفاظ على السيطرة على بياناتها وبنيتها التحتية.

يتطلب النجاح في نشر SLM المحلي النظر بعناية في المتطلبات التقنية، الآثار الأمنية، والإجراءات التشغيلية. من خلال اتباع أفضل الممارسات والاستفادة من نقاط القوة لهذه المنصات، يمكن للمؤسسات بناء حلول ذكاء اصطناعي قوية وقابلة للتوسع وآمنة تلبي احتياجاتها وقيودها المحددة.

## ➡️ الخطوة التالية

- [03: التنفيذ العملي لـ SLM](03.SLMPracticalImplementation.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.