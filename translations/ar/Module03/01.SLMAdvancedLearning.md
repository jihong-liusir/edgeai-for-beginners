<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T06:31:09+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "ar"
}
-->
# القسم 1: التعلم المتقدم لنماذج اللغة الصغيرة - الأسس والتحسين

نماذج اللغة الصغيرة (SLMs) تمثل تقدمًا مهمًا في EdgeAI، حيث تتيح قدرات معالجة اللغة الطبيعية المتقدمة على الأجهزة ذات الموارد المحدودة. فهم كيفية نشر وتحسين واستخدام SLMs بشكل فعال ضروري لبناء حلول ذكاء اصطناعي عملية تعتمد على الحافة.

## المقدمة

في هذه الدرس، سنستكشف نماذج اللغة الصغيرة (SLMs) واستراتيجيات تنفيذها المتقدمة. سنغطي المفاهيم الأساسية لـ SLMs، حدود المعلمات وتصنيفاتها، تقنيات التحسين، واستراتيجيات النشر العملية لبيئات الحوسبة الطرفية.

## أهداف التعلم

بنهاية هذا الدرس، ستكون قادرًا على:

- 🔢 فهم حدود المعلمات وتصنيفات نماذج اللغة الصغيرة.
- 🛠️ تحديد تقنيات التحسين الرئيسية لنشر SLMs على الأجهزة الطرفية.
- 🚀 تعلم تنفيذ استراتيجيات التكميم والضغط المتقدمة لـ SLMs.

## فهم حدود المعلمات وتصنيفات SLM

نماذج اللغة الصغيرة (SLMs) هي نماذج ذكاء اصطناعي مصممة لمعالجة وفهم وإنشاء محتوى اللغة الطبيعية بعدد أقل بكثير من المعلمات مقارنة بنظيراتها الكبيرة. بينما تحتوي نماذج اللغة الكبيرة (LLMs) على مئات المليارات إلى تريليونات من المعلمات، تم تصميم SLMs خصيصًا لتحقيق الكفاءة والنشر على الحافة.

يساعد إطار تصنيف المعلمات في فهم الفئات المختلفة لـ SLMs واستخداماتها المناسبة. هذا التصنيف ضروري لاختيار النموذج المناسب لسيناريوهات الحوسبة الطرفية المحددة.

### إطار تصنيف المعلمات

فهم حدود المعلمات يساعد في اختيار النماذج المناسبة لسيناريوهات الحوسبة الطرفية المختلفة:

- **🔬 نماذج صغيرة جدًا (Micro SLMs)**: 100 مليون - 1.4 مليار معلمة (خفيفة للغاية للأجهزة المحمولة)
- **📱 نماذج صغيرة (Small SLMs)**: 1.5 مليار - 13.9 مليار معلمة (أداء متوازن وكفاءة)
- **⚖️ نماذج متوسطة (Medium SLMs)**: 14 مليار - 30 مليار معلمة (تقترب من قدرات LLM مع الحفاظ على الكفاءة)

يبقى الحد الدقيق مرنًا في مجتمع البحث، لكن معظم الممارسين يعتبرون النماذج التي تحتوي على أقل من 30 مليار معلمة "صغيرة"، مع تحديد بعض المصادر الحد الأدنى حتى 10 مليارات معلمة.

### المزايا الرئيسية لـ SLMs

تقدم SLMs العديد من المزايا الأساسية التي تجعلها مثالية لتطبيقات الحوسبة الطرفية:

**الكفاءة التشغيلية**: توفر SLMs أوقات استنتاج أسرع بسبب عدد أقل من المعلمات للمعالجة، مما يجعلها مثالية للتطبيقات في الوقت الفعلي. تتطلب موارد حسابية أقل، مما يتيح النشر على الأجهزة ذات الموارد المحدودة مع استهلاك أقل للطاقة وتقليل البصمة الكربونية.

**مرونة النشر**: تتيح هذه النماذج قدرات الذكاء الاصطناعي على الأجهزة دون الحاجة إلى الاتصال بالإنترنت، وتعزز الخصوصية والأمان من خلال المعالجة المحلية، ويمكن تخصيصها للتطبيقات الخاصة بالمجال، وهي مناسبة لبيئات الحوسبة الطرفية المختلفة.

**فعالية التكلفة**: تقدم SLMs تدريبًا ونشرًا بتكلفة أقل مقارنة بـ LLMs، مع تقليل تكاليف التشغيل ومتطلبات النطاق الترددي لتطبيقات الحافة.

## استراتيجيات الحصول على النماذج المتقدمة

### نظام Hugging Face

يعد Hugging Face المركز الأساسي لاكتشاف والوصول إلى نماذج SLMs المتقدمة. يوفر النظام الأساسي موارد شاملة لاكتشاف النماذج ونشرها:

**ميزات اكتشاف النماذج**: يقدم النظام الأساسي تصفية متقدمة حسب عدد المعلمات، نوع الترخيص، ومقاييس الأداء. يمكن للمستخدمين الوصول إلى أدوات مقارنة النماذج جنبًا إلى جنب، معايير الأداء والتقييم في الوقت الفعلي، وعروض WebGPU للتجربة الفورية.

**مجموعات SLMs المختارة**: تشمل النماذج الشهيرة Phi-4-mini-3.8B للمهام الاستدلالية المتقدمة، سلسلة Qwen3 (0.6B/1.7B/4B) للتطبيقات متعددة اللغات، Google Gemma3 للمهام العامة بكفاءة، والنماذج التجريبية مثل BitNET للنشر بدقة منخفضة للغاية. كما يضم النظام الأساسي مجموعات مدفوعة من المجتمع مع نماذج متخصصة لمجالات محددة ومتغيرات مدربة مسبقًا ومضبوطة على التعليمات لتحسين الاستخدامات المختلفة.

### كتالوج نماذج Azure AI Foundry

يوفر كتالوج نماذج Azure AI Foundry وصولًا على مستوى المؤسسات إلى SLMs مع قدرات تكامل محسنة:

**تكامل المؤسسات**: يتضمن الكتالوج نماذج تُباع مباشرة من Azure مع دعم على مستوى المؤسسات واتفاقيات مستوى الخدمة (SLAs)، مثل Phi-4-mini-3.8B للقدرات الاستدلالية المتقدمة وLlama 3-8B للنشر الإنتاجي. كما يضم نماذج مثل Qwen3 8B من نماذج مفتوحة المصدر موثوقة من طرف ثالث.

**فوائد المؤسسات**: أدوات مدمجة للتخصيص، المراقبة، والذكاء الاصطناعي المسؤول مع توفير قابل للتخصيص عبر عائلات النماذج. دعم مباشر من Microsoft مع اتفاقيات مستوى الخدمة على مستوى المؤسسات، ميزات الأمان والامتثال المدمجة، وعمليات نشر شاملة تعزز تجربة المؤسسات.

## تقنيات التكميم والتحسين المتقدمة

### إطار تحسين Llama.cpp

يوفر Llama.cpp تقنيات تكميم متقدمة لتحقيق أقصى كفاءة في النشر الطرفي:

**طرق التكميم**: يدعم الإطار مستويات تكميم مختلفة بما في ذلك Q4_0 (تكميم 4 بت مع تقليل الحجم بشكل ممتاز - مثالي لنشر Qwen3-0.6B على الأجهزة المحمولة)، Q5_1 (تكميم 5 بت يوازن بين الجودة والضغط - مناسب للاستنتاج الطرفي لـ Phi-4-mini-3.8B)، وQ8_0 (تكميم 8 بت للحفاظ على الجودة الأصلية تقريبًا - موصى به للاستخدام الإنتاجي لـ Google Gemma3). يمثل BitNET الحد الأقصى مع تكميم 1 بت للسيناريوهات ذات الضغط الشديد.

**فوائد التنفيذ**: استنتاج محسن لوحدة المعالجة المركزية مع تسريع SIMD يوفر تحميل وتنفيذ النماذج بكفاءة في الذاكرة. التوافق عبر الأنظمة الأساسية بين معماريات x86، ARM، وApple Silicon يتيح قدرات نشر غير معتمدة على الأجهزة.

**مثال عملي للتنفيذ**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**مقارنة بصمة الذاكرة**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### مجموعة تحسين Microsoft Olive

تقدم Microsoft Olive سير عمل شامل لتحسين النماذج المصممة لبيئات الإنتاج:

**تقنيات التحسين**: تتضمن المجموعة التكميم الديناميكي لاختيار الدقة تلقائيًا (فعال بشكل خاص مع نماذج سلسلة Qwen3)، تحسين الرسوم البيانية ودمج المشغلين (محسن لهندسة Google Gemma3)، تحسينات خاصة بالأجهزة لوحدة المعالجة المركزية، وحدة معالجة الرسومات، ووحدة المعالجة العصبية (مع دعم خاص لـ Phi-4-mini-3.8B على أجهزة ARM)، وخطوط تحسين متعددة المراحل. تتطلب نماذج BitNET سير عمل تكميم 1 بت متخصص داخل إطار Olive.

**أتمتة سير العمل**: يضمن القياس التلقائي عبر متغيرات التحسين الحفاظ على مقاييس الجودة أثناء التحسين. يوفر التكامل مع أطر التعلم الآلي الشهيرة مثل PyTorch وONNX قدرات تحسين للنشر السحابي والطرفي.

**مثال عملي للتنفيذ**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### إطار Apple MLX

يوفر Apple MLX تحسينًا أصليًا مصممًا خصيصًا لأجهزة Apple Silicon:

**تحسين Apple Silicon**: يستخدم الإطار بنية ذاكرة موحدة مع تكامل Metal Performance Shaders، استنتاج دقة مختلطة تلقائيًا (فعال بشكل خاص مع Google Gemma3)، وتحسين استخدام عرض النطاق الترددي للذاكرة. يظهر Phi-4-mini-3.8B أداءً استثنائيًا على رقائق سلسلة M، بينما يوفر Qwen3-1.7B توازنًا مثاليًا لنشر MacBook Air.

**ميزات التطوير**: دعم واجهات برمجة التطبيقات Python وSwift مع عمليات صفيف متوافقة مع NumPy، قدرات التمايز التلقائي، وتكامل سلس مع أدوات تطوير Apple يوفر بيئة تطوير شاملة.

**مثال عملي للتنفيذ**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## استراتيجيات النشر والإستنتاج الإنتاجي

### Ollama: نشر محلي مبسط

يُبسط Ollama نشر SLMs مع ميزات جاهزة للمؤسسات لبيئات المحلية والطرفية:

**قدرات النشر**: تثبيت وتشغيل النماذج بأمر واحد مع سحب النماذج وتخزينها تلقائيًا. دعم Phi-4-mini-3.8B، سلسلة Qwen3 بالكامل (0.6B/1.7B/4B)، وGoogle Gemma3 مع واجهة REST API لتكامل التطبيقات وإدارة النماذج المتعددة وقدرات التبديل. تتطلب نماذج BitNET تكوينات بناء تجريبية لدعم التكميم 1 بت.

**ميزات متقدمة**: دعم التخصيص الدقيق للنماذج، إنشاء ملفات Docker للنشر في الحاويات، تسريع وحدة معالجة الرسومات مع الكشف التلقائي، وخيارات تكميم وتحسين النماذج توفر مرونة شاملة للنشر.

### VLLM: استنتاج عالي الأداء

يوفر VLLM تحسينات استنتاج إنتاجية لسيناريوهات عالية الإنتاجية:

**تحسينات الأداء**: PagedAttention لحساب الانتباه بكفاءة في الذاكرة (مفيد بشكل خاص لهندسة Phi-4-mini-3.8B)، التجميع الديناميكي لتحسين الإنتاجية (محسن لمعالجة سلسلة Qwen3 بالتوازي)، التوازي التنسوري لتوسيع النطاق متعدد وحدات معالجة الرسومات (دعم Google Gemma3)، والتشفير التخميني لتقليل زمن الاستجابة. تتطلب نماذج BitNET نوى استنتاج متخصصة لعمليات 1 بت.

**تكامل المؤسسات**: نقاط نهاية API متوافقة مع OpenAI، دعم نشر Kubernetes، تكامل المراقبة والملاحظة، وقدرات التوسع التلقائي توفر حلول نشر على مستوى المؤسسات.

### Foundry Local: حل الحافة من Microsoft

يوفر Foundry Local قدرات نشر طرفية شاملة لبيئات المؤسسات:

**ميزات الحوسبة الطرفية**: تصميم معماري يعتمد على العمل دون اتصال مع تحسين قيود الموارد، إدارة سجل النماذج المحلية، وقدرات المزامنة بين الحافة والسحابة تضمن نشرًا موثوقًا على الحافة.

**الأمان والامتثال**: معالجة البيانات المحلية للحفاظ على الخصوصية، ضوابط أمان المؤسسات، تسجيل التدقيق وإعداد تقارير الامتثال، وإدارة الوصول بناءً على الأدوار توفر أمانًا شاملاً للنشر الطرفي.

## أفضل الممارسات لتنفيذ SLM

### إرشادات اختيار النماذج

عند اختيار SLMs للنشر الطرفي، ضع في اعتبارك العوامل التالية:

**اعتبارات عدد المعلمات**: اختر نماذج صغيرة جدًا مثل Qwen3-0.6B للتطبيقات المحمولة خفيفة الوزن للغاية، نماذج صغيرة مثل Qwen3-1.7B أو Google Gemma3 لسيناريوهات الأداء المتوازن، ونماذج متوسطة مثل Phi-4-mini-3.8B أو Qwen3-4B عند الاقتراب من قدرات LLM مع الحفاظ على الكفاءة. توفر نماذج BitNET ضغطًا فائقًا تجريبيًا لتطبيقات البحث المحددة.

**توافق الاستخدام**: طابق قدرات النموذج مع متطلبات التطبيق المحددة، مع مراعاة عوامل مثل جودة الاستجابة، سرعة الاستنتاج، قيود الذاكرة، ومتطلبات التشغيل دون اتصال.

### اختيار استراتيجية التحسين

**نهج التكميم**: اختر مستويات التكميم المناسبة بناءً على متطلبات الجودة وقيود الأجهزة. ضع في اعتبارك Q4_0 لتحقيق أقصى ضغط (مثالي لنشر Qwen3-0.6B على الأجهزة المحمولة)، Q5_1 لتحقيق توازن بين الجودة والضغط (مناسب لـ Phi-4-mini-3.8B وGoogle Gemma3)، وQ8_0 للحفاظ على الجودة الأصلية تقريبًا (موصى به لبيئات الإنتاج لـ Qwen3-4B). يمثل تكميم 1 بت لـ BitNET الحد الأقصى للضغط للتطبيقات المتخصصة.

**اختيار الإطار**: اختر أطر التحسين بناءً على الأجهزة المستهدفة ومتطلبات النشر. استخدم Llama.cpp للنشر المحسن لوحدة المعالجة المركزية، Microsoft Olive لسير عمل التحسين الشامل، وApple MLX لأجهزة Apple Silicon.

## أمثلة عملية للنماذج وحالات الاستخدام

### سيناريوهات النشر الواقعية

**التطبيقات المحمولة**: يتفوق Qwen3-0.6B في تطبيقات الدردشة على الهواتف الذكية مع بصمة ذاكرة صغيرة، بينما يوفر Google Gemma3 أداءً متوازنًا لأدوات التعليم على الأجهزة اللوحية. يقدم Phi-4-mini-3.8B قدرات استدلال فائقة لتطبيقات الإنتاجية المحمولة.

**الحوسبة المكتبية والطرفية**: يوفر Qwen3-1.7B أداءً مثاليًا لتطبيقات المساعد المكتبي، يقدم Phi-4-mini-3.8B قدرات إنشاء أكواد متقدمة لأدوات المطورين، ويتيح Qwen3-4B تحليلًا متقدمًا للوثائق في بيئات محطات العمل.

**البحث والتجريب**: تتيح نماذج BitNET استكشاف الاستنتاج بدقة منخفضة للغاية للبحث الأكاديمي وتطبيقات إثبات المفهوم التي تتطلب قيود موارد شديدة.

### معايير الأداء والمقارنات

**سرعة الاستنتاج**: يحقق Qwen3-0.6B أسرع أوقات الاستنتاج على وحدات المعالجة المركزية المحمولة، يوفر Google Gemma3 نسبة سرعة-جودة متوازنة للتطبيقات العامة، يقدم Phi-4-mini-3.8B سرعة استدلال فائقة للمهام المعقدة، ويحقق BitNET أقصى إنتاجية نظرية مع الأجهزة المتخصصة.

**متطلبات الذاكرة**: تتراوح بصمات ذاكرة النماذج من Qwen3-0.6B (أقل من 1 جيجابايت بعد التكميم) إلى Phi-4-mini-3.8B (حوالي 3-4 جيجابايت بعد التكميم)، مع تحقيق BitNET بصمات أقل من 500 ميجابايت في التكوينات التجريبية.

## التحديات والاعتبارات

### التوازن بين الأداء

يتطلب نشر SLMs النظر بعناية في التوازن بين حجم النموذج، سرعة الاستنتاج، وجودة المخرجات. على سبيل المثال، بينما يقدم Qwen3-0.6B سرعة وكفاءة استثنائية، يوفر Phi-4-mini-3.8B قدرات استدلال فائقة على حساب زيادة متطلبات الموارد. يمثل Google Gemma3 خيارًا وسطًا مناسبًا لمعظم التطبيقات العامة.

### توافق الأجهزة

تتمتع الأجهزة الطرفية المختلفة بقدرات وقيود متنوعة. يعمل Qwen3-0.6B بكفاءة على معالجات ARM الأساسية، يتطلب Google Gemma3 موارد حسابية معتدلة، ويستفيد Phi-4-mini-3.8B من الأجهزة الطرفية عالية الأداء. تتطلب نماذج BitNET أجهزة أو تطبيقات برمجية متخصصة لتحقيق عمليات 1 بت المثلى.

### الأمان والخصوصية

بينما تتيح SLMs المعالجة المحلية لتعزيز الخصوصية، يجب تنفيذ تدابير أمان مناسبة لحماية النماذج والبيانات في البيئات الطرفية. هذا مهم بشكل خاص عند نشر نماذج مثل Phi-4-mini-3.8B في بيئات المؤسسات أو سلسلة Qwen3 في التطبيقات متعددة اللغات التي تتعامل مع بيانات حساسة.

## الاتجاهات المستقبلية في تطوير SLM

يستمر مشهد SLM في التطور مع التقدم في بنى النماذج، تقنيات التحسين، واستراتيجيات النشر. تشمل التطورات المستقبلية بنى أكثر كفاءة، طرق تكميم محسنة، وتكامل أفضل مع مسرعات الأجهزة الطرفية.

فهم هذه الاتجاهات والحفاظ على الوعي بالتقنيات الناشئة سيكون أمرًا حاسمًا للبقاء على اطلاع بأفضل الممارسات لتطوير ونشر SLM.

## ➡️ ما التالي

- [02: نشر SLM في البيئة المحلية](02.DeployingSLMinLocalEnv.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.