<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:12:58+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ar"
}
-->
# القسم 3: دليل التنفيذ العملي

## نظرة عامة

هذا الدليل الشامل سيساعدك على التحضير لدورة EdgeAI، التي تركز على بناء حلول الذكاء الاصطناعي العملية التي تعمل بكفاءة على الأجهزة الطرفية. الدورة تركز على التطوير العملي باستخدام الأطر الحديثة والنماذج المتقدمة المصممة خصيصًا للنشر على الأجهزة الطرفية.

## 1. إعداد بيئة التطوير

### لغات البرمجة والأطر

**بيئة Python**
- **الإصدار**: Python 3.10 أو أعلى (الموصى به: Python 3.11)
- **مدير الحزم**: pip أو conda
- **البيئة الافتراضية**: استخدم venv أو بيئات conda للعزل
- **المكتبات الأساسية**: سنقوم بتثبيت مكتبات EdgeAI المحددة خلال الدورة

**بيئة Microsoft .NET**
- **الإصدار**: .NET 8 أو أعلى
- **بيئة التطوير**: Visual Studio 2022، Visual Studio Code، أو JetBrains Rider
- **مجموعة الأدوات**: تأكد من تثبيت .NET SDK للتطوير عبر الأنظمة

### أدوات التطوير

**محررات الأكواد وبيئات التطوير**
- Visual Studio Code (موصى به للتطوير عبر الأنظمة)
- PyCharm أو Visual Studio (للتطوير الخاص بلغة معينة)
- Jupyter Notebooks للتطوير التفاعلي والنماذج الأولية

**إدارة الإصدارات**
- Git (أحدث إصدار)
- حساب GitHub للوصول إلى المستودعات والتعاون

## 2. متطلبات الأجهزة والتوصيات

### الحد الأدنى لمتطلبات النظام
- **المعالج**: معالج متعدد النواة (Intel i5/AMD Ryzen 5 أو ما يعادله)
- **الذاكرة**: 8GB كحد أدنى، يوصى بـ 16GB
- **التخزين**: مساحة متوفرة 50GB للنماذج وأدوات التطوير
- **نظام التشغيل**: Windows 10/11، macOS 10.15+، أو Linux (Ubuntu 20.04+)

### استراتيجية موارد الحوسبة
الدورة مصممة لتكون متاحة عبر تكوينات الأجهزة المختلفة:

**التطوير المحلي (تركيز على CPU/NPU)**
- سيتم استخدام المعالج المركزي (CPU) وتسريع NPU بشكل أساسي
- مناسب لمعظم أجهزة الكمبيوتر المحمولة والمكتبية الحديثة
- التركيز على الكفاءة وسيناريوهات النشر العملية

**موارد GPU السحابية (اختياري)**
- **Azure Machine Learning**: للتدريب المكثف والتجارب
- **Google Colab**: الطبقة المجانية متاحة للأغراض التعليمية
- **Kaggle Notebooks**: منصة حوسبة سحابية بديلة

### اعتبارات الأجهزة الطرفية
- فهم المعالجات القائمة على ARM
- معرفة قيود الأجهزة المحمولة وإنترنت الأشياء
- الإلمام بتحسين استهلاك الطاقة

## 3. العائلات النموذجية الأساسية والموارد

### العائلات النموذجية الأساسية

**عائلة Microsoft Phi-4**
- **الوصف**: نماذج مدمجة وفعالة مصممة للنشر على الأجهزة الطرفية
- **نقاط القوة**: نسبة أداء إلى حجم ممتازة، مصممة لمهام التفكير
- **المصدر**: [مجموعة Phi-4 على Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **الاستخدامات**: توليد الأكواد، التفكير الرياضي، المحادثات العامة

**عائلة Qwen-3**
- **الوصف**: أحدث جيل من النماذج متعددة اللغات من Alibaba
- **نقاط القوة**: قدرات متعددة اللغات قوية، بنية فعالة
- **المصدر**: [مجموعة Qwen-3 على Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **الاستخدامات**: التطبيقات متعددة اللغات، حلول الذكاء الاصطناعي عبر الثقافات

**عائلة Google Gemma-3n**
- **الوصف**: نماذج خفيفة الوزن من Google مصممة للنشر على الأجهزة الطرفية
- **نقاط القوة**: استدلال سريع، بنية صديقة للأجهزة المحمولة
- **المصدر**: [مجموعة Gemma-3n على Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **الاستخدامات**: التطبيقات المحمولة، المعالجة في الوقت الفعلي

### معايير اختيار النموذج
- **التوازن بين الأداء والحجم**: فهم متى تختار النماذج الصغيرة مقابل الكبيرة
- **تحسين المهام المحددة**: مطابقة النماذج مع الاستخدامات المحددة
- **قيود النشر**: الذاكرة، التأخير، واستهلاك الطاقة

## 4. أدوات التكميم والتحسين

### إطار عمل Llama.cpp
- **المستودع**: [Llama.cpp على GitHub](https://github.com/ggml-org/llama.cpp)
- **الغرض**: محرك استدلال عالي الأداء للنماذج اللغوية الكبيرة
- **الميزات الرئيسية**:
  - استدلال محسن للمعالج المركزي (CPU)
  - تنسيقات تكميم متعددة (Q4، Q5، Q8)
  - توافق عبر الأنظمة
  - تنفيذ فعال للذاكرة
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```


### Microsoft Olive
- **المستودع**: [Microsoft Olive على GitHub](https://github.com/microsoft/olive)
- **الغرض**: مجموعة أدوات تحسين النماذج للنشر على الأجهزة الطرفية
- **الميزات الرئيسية**:
  - سير عمل تحسين النماذج تلقائيًا
  - تحسينات موجهة للأجهزة
  - تكامل مع ONNX Runtime
  - أدوات قياس الأداء
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # مثال على سكربت Python لتحسين النماذج
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```


### Apple MLX (لمستخدمي macOS)
- **المستودع**: [Apple MLX على GitHub](https://github.com/ml-explore/mlx)
- **الغرض**: إطار عمل التعلم الآلي لـ Apple Silicon
- **الميزات الرئيسية**:
  - تحسين أصلي لـ Apple Silicon
  - عمليات فعالة للذاكرة
  - واجهة برمجية مشابهة لـ PyTorch
  - دعم بنية الذاكرة الموحدة
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```


### ONNX Runtime
- **المستودع**: [ONNX Runtime على GitHub](https://github.com/microsoft/onnxruntime)
- **الغرض**: تسريع الاستدلال عبر الأنظمة لنماذج ONNX
- **الميزات الرئيسية**:
  - تحسينات موجهة للأجهزة (CPU، GPU، NPU)
  - تحسينات الرسم البياني للاستدلال
  - دعم التكميم
  - دعم متعدد اللغات (Python، C++، C#، JavaScript)
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. القراءة والموارد الموصى بها

### الوثائق الأساسية
- **وثائق ONNX Runtime**: فهم الاستدلال عبر الأنظمة
- **دليل Hugging Face Transformers**: تحميل النماذج والاستدلال
- **أنماط تصميم Edge AI**: أفضل الممارسات للنشر على الأجهزة الطرفية

### الأوراق التقنية
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### موارد المجتمع
- **مجتمعات EdgeAI على Slack/Discord**: دعم الأقران والنقاش
- **مستودعات GitHub**: أمثلة على التنفيذ والدروس
- **قنوات YouTube**: شروحات تقنية ودروس تعليمية

## 6. التقييم والتحقق

### قائمة التحقق قبل الدورة
- [ ] تثبيت Python 3.10+ والتحقق منه
- [ ] تثبيت .NET 8+ والتحقق منه
- [ ] تكوين بيئة التطوير
- [ ] إنشاء حساب على Hugging Face
- [ ] الإلمام الأساسي بالعائلات النموذجية المستهدفة
- [ ] تثبيت أدوات التكميم واختبارها
- [ ] تلبية متطلبات الأجهزة
- [ ] إعداد حسابات الحوسبة السحابية (إذا لزم الأمر)

## الأهداف التعليمية الرئيسية

بنهاية هذا الدليل، ستكون قادرًا على:

1. إعداد بيئة تطوير كاملة لتطوير تطبيقات EdgeAI
2. تثبيت وتكوين الأدوات والأطر اللازمة لتحسين النماذج
3. اختيار تكوينات الأجهزة والبرامج المناسبة لمشاريع EdgeAI الخاصة بك
4. فهم الاعتبارات الرئيسية لنشر نماذج الذكاء الاصطناعي على الأجهزة الطرفية
5. تجهيز نظامك للتمارين العملية في الدورة

## موارد إضافية

### الوثائق الرسمية
- **وثائق Python**: الوثائق الرسمية للغة Python
- **وثائق Microsoft .NET**: موارد التطوير الرسمية لـ .NET
- **وثائق ONNX Runtime**: دليل شامل لـ ONNX Runtime
- **وثائق TensorFlow Lite**: الوثائق الرسمية لـ TensorFlow Lite

### أدوات التطوير
- **Visual Studio Code**: محرر أكواد خفيف مع إضافات لتطوير الذكاء الاصطناعي
- **Jupyter Notebooks**: بيئة حوسبة تفاعلية لتجارب التعلم الآلي
- **Docker**: منصة الحاويات لتوفير بيئات تطوير متسقة
- **Git**: نظام التحكم بالإصدارات لإدارة الأكواد

### موارد التعلم
- **أوراق بحثية عن EdgeAI**: أحدث الأبحاث الأكاديمية حول النماذج الفعالة
- **دورات عبر الإنترنت**: مواد تعليمية إضافية حول تحسين الذكاء الاصطناعي
- **منتديات المجتمع**: منصات الأسئلة والأجوبة لتحديات تطوير EdgeAI
- **مجموعات البيانات المرجعية**: مجموعات بيانات قياسية لتقييم أداء النماذج

## نتائج التعلم

بعد إكمال دليل التحضير هذا، ستكون:

1. قد أعددت بيئة تطوير كاملة جاهزة لتطوير EdgeAI
2. فهمت متطلبات الأجهزة والبرامج لسيناريوهات النشر المختلفة
3. ملمًا بالأطر والأدوات الرئيسية المستخدمة طوال الدورة
4. قادرًا على اختيار النماذج المناسبة بناءً على قيود الأجهزة والمتطلبات
5. لديك معرفة أساسية بتقنيات التحسين للنشر على الأجهزة الطرفية

## ➡️ ما التالي

- [04: أجهزة EdgeAI والنشر](04.EdgeDeployment.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.