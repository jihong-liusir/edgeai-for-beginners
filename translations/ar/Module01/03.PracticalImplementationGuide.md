<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-17T17:38:30+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ar"
}
-->
# القسم 3: دليل التنفيذ العملي

## نظرة عامة

هذا الدليل الشامل سيساعدك على التحضير لدورة EdgeAI، التي تركز على بناء حلول الذكاء الاصطناعي العملية التي تعمل بكفاءة على الأجهزة الطرفية. الدورة تركز على التطوير العملي باستخدام أطر عمل حديثة ونماذج متقدمة مُحسّنة للنشر على الأجهزة الطرفية.

## 1. إعداد بيئة التطوير

### لغات البرمجة وأطر العمل

**بيئة Python**
- **الإصدار**: Python 3.10 أو أعلى (الموصى به: Python 3.11)
- **مدير الحزم**: pip أو conda
- **البيئة الافتراضية**: استخدم venv أو بيئات conda للعزل
- **المكتبات الأساسية**: سنقوم بتثبيت مكتبات EdgeAI المحددة خلال الدورة

**بيئة Microsoft .NET**
- **الإصدار**: .NET 8 أو أعلى
- **بيئة التطوير (IDE)**: Visual Studio 2022، Visual Studio Code، أو JetBrains Rider
- **مجموعة أدوات التطوير (SDK)**: تأكد من تثبيت .NET SDK للتطوير عبر الأنظمة

### أدوات التطوير

**محررات الأكواد وبيئات التطوير (IDE)**
- Visual Studio Code (موصى به للتطوير عبر الأنظمة)
- PyCharm أو Visual Studio (للتطوير الخاص بلغة معينة)
- Jupyter Notebooks للتطوير التفاعلي والنماذج الأولية

**إدارة الإصدارات**
- Git (أحدث إصدار)
- حساب GitHub للوصول إلى المستودعات والتعاون

## 2. متطلبات الأجهزة والتوصيات

### الحد الأدنى لمتطلبات النظام
- **المعالج (CPU)**: معالج متعدد النوى (Intel i5/AMD Ryzen 5 أو ما يعادله)
- **الذاكرة (RAM)**: الحد الأدنى 8GB، الموصى به 16GB
- **التخزين**: مساحة متوفرة 50GB للنماذج وأدوات التطوير
- **نظام التشغيل (OS)**: Windows 10/11، macOS 10.15+، أو Linux (Ubuntu 20.04+)

### استراتيجية موارد الحوسبة
تم تصميم الدورة لتكون متاحة عبر تكوينات الأجهزة المختلفة:

**التطوير المحلي (تركيز على CPU/NPU)**
- سيتم استخدام المعالج (CPU) وتسريع وحدة المعالجة العصبية (NPU) بشكل أساسي
- مناسب لمعظم أجهزة الكمبيوتر المحمولة والمكتبية الحديثة
- التركيز على الكفاءة وسيناريوهات النشر العملية

**موارد GPU السحابية (اختياري)**
- **Azure Machine Learning**: للتدريب المكثف والتجارب
- **Google Colab**: الطبقة المجانية متاحة للأغراض التعليمية
- **Kaggle Notebooks**: منصة حوسبة سحابية بديلة

### اعتبارات الأجهزة الطرفية
- فهم المعالجات القائمة على ARM
- معرفة قيود الأجهزة المحمولة وإنترنت الأشياء (IoT)
- الإلمام بتحسين استهلاك الطاقة

## 3. العائلات النموذجية الأساسية والموارد

### العائلات النموذجية الأساسية

**عائلة Microsoft Phi-4**
- **الوصف**: نماذج مدمجة وفعالة مصممة للنشر على الأجهزة الطرفية
- **نقاط القوة**: نسبة أداء إلى حجم ممتازة، مُحسّنة لمهام الاستنتاج
- **المصدر**: [مجموعة Phi-4 على Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **الاستخدامات**: توليد الأكواد، الاستنتاج الرياضي، المحادثات العامة

**عائلة Qwen-3**
- **الوصف**: أحدث جيل من النماذج متعددة اللغات من Alibaba
- **نقاط القوة**: قدرات متعددة اللغات قوية، بنية فعالة
- **المصدر**: [مجموعة Qwen-3 على Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **الاستخدامات**: التطبيقات متعددة اللغات، حلول الذكاء الاصطناعي عبر الثقافات

**عائلة Google Gemma-3n**
- **الوصف**: نماذج خفيفة الوزن من Google مُحسّنة للنشر على الأجهزة الطرفية
- **نقاط القوة**: استنتاج سريع، بنية مناسبة للأجهزة المحمولة
- **المصدر**: [مجموعة Gemma-3n على Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **الاستخدامات**: التطبيقات المحمولة، المعالجة في الوقت الفعلي

### معايير اختيار النماذج
- **التوازن بين الأداء والحجم**: فهم متى تختار النماذج الصغيرة مقابل الكبيرة
- **تحسين المهام المحددة**: مطابقة النماذج مع الاستخدامات المحددة
- **قيود النشر**: اعتبارات الذاكرة، التأخير، واستهلاك الطاقة

## 4. أدوات التكميم والتحسين

### إطار عمل Llama.cpp
- **المستودع**: [Llama.cpp على GitHub](https://github.com/ggml-org/llama.cpp)
- **الغرض**: محرك استنتاج عالي الأداء للنماذج اللغوية الكبيرة (LLMs)
- **الميزات الرئيسية**:
  - استنتاج مُحسّن للمعالج (CPU)
  - تنسيقات تكميم متعددة (Q4، Q5، Q8)
  - توافق عبر الأنظمة
  - تنفيذ فعال للذاكرة
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **المستودع**: [Microsoft Olive على GitHub](https://github.com/microsoft/olive)
- **الغرض**: مجموعة أدوات تحسين النماذج للنشر على الأجهزة الطرفية
- **الميزات الرئيسية**:
  - سير عمل تحسين النماذج تلقائيًا
  - تحسينات واعية بالأجهزة
  - تكامل مع ONNX Runtime
  - أدوات قياس الأداء
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # تعريف النموذج وتكوين التحسين
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # تشغيل سير عمل التحسين
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # حفظ النموذج المحسن
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # تثبيت MLX
  pip install mlx
  
  # مثال على سكربت Python لتحميل وتحسين نموذج
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **المستودع**: [ONNX Runtime على GitHub](https://github.com/microsoft/onnxruntime)
- **الغرض**: تسريع الاستنتاج عبر الأنظمة لنماذج ONNX
- **الميزات الرئيسية**:
  - تحسينات خاصة بالأجهزة (CPU، GPU، NPU)
  - تحسينات الرسم البياني للاستنتاج
  - دعم التكميم
  - دعم متعدد اللغات (Python، C++، C#، JavaScript)
- **التثبيت والاستخدام الأساسي**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## 5. القراءة والموارد الموصى بها

### الوثائق الأساسية
- **وثائق ONNX Runtime**: فهم الاستنتاج عبر الأنظمة
- **دليل Hugging Face Transformers**: تحميل النماذج والاستنتاج
- **أنماط تصميم Edge AI**: أفضل الممارسات للنشر على الأجهزة الطرفية

### الأوراق التقنية
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### موارد المجتمع
- **مجتمعات EdgeAI على Slack/Discord**: دعم الأقران والنقاش
- **مستودعات GitHub**: أمثلة على التنفيذ والدروس
- **قنوات YouTube**: شروحات تقنية متعمقة ودروس

## 6. التقييم والتحقق

### قائمة التحقق قبل الدورة
- [ ] تثبيت Python 3.10+ والتحقق منه
- [ ] تثبيت .NET 8+ والتحقق منه
- [ ] تكوين بيئة التطوير
- [ ] إنشاء حساب Hugging Face
- [ ] الإلمام الأساسي بالعائلات النموذجية المستهدفة
- [ ] تثبيت أدوات التكميم واختبارها
- [ ] تلبية متطلبات الأجهزة
- [ ] إعداد حسابات الحوسبة السحابية (إذا لزم الأمر)

## الأهداف التعليمية الرئيسية

بنهاية هذا الدليل، ستكون قادرًا على:

1. إعداد بيئة تطوير كاملة لتطوير تطبيقات EdgeAI
2. تثبيت وتكوين الأدوات والأطر اللازمة لتحسين النماذج
3. اختيار تكوينات الأجهزة والبرامج المناسبة لمشاريع EdgeAI الخاصة بك
4. فهم الاعتبارات الرئيسية لنشر نماذج الذكاء الاصطناعي على الأجهزة الطرفية
5. تجهيز نظامك للتمارين العملية في الدورة

## موارد إضافية

### الوثائق الرسمية
- **وثائق Python**: وثائق لغة Python الرسمية
- **وثائق Microsoft .NET**: موارد تطوير .NET الرسمية
- **وثائق ONNX Runtime**: دليل شامل لـ ONNX Runtime
- **وثائق TensorFlow Lite**: وثائق TensorFlow Lite الرسمية

### أدوات التطوير
- **Visual Studio Code**: محرر أكواد خفيف مع إضافات لتطوير الذكاء الاصطناعي
- **Jupyter Notebooks**: بيئة حوسبة تفاعلية لتجارب التعلم الآلي
- **Docker**: منصة الحاويات لبيئات التطوير المتسقة
- **Git**: نظام التحكم بالإصدارات لإدارة الأكواد

### موارد التعلم
- **أوراق بحثية عن EdgeAI**: أحدث الأبحاث الأكاديمية حول النماذج الفعالة
- **دورات عبر الإنترنت**: مواد تعليمية إضافية حول تحسين الذكاء الاصطناعي
- **منتديات المجتمع**: منصات الأسئلة والأجوبة لتحديات تطوير EdgeAI
- **مجموعات البيانات المرجعية**: مجموعات بيانات قياسية لتقييم أداء النماذج

## نتائج التعلم

بعد إكمال دليل التحضير هذا، ستكون:

1. قد أعددت بيئة تطوير كاملة جاهزة لتطوير EdgeAI
2. فهمت متطلبات الأجهزة والبرامج لسيناريوهات النشر المختلفة
3. ملمًا بالأطر والأدوات الرئيسية المستخدمة طوال الدورة
4. قادرًا على اختيار النماذج المناسبة بناءً على قيود الأجهزة والمتطلبات
5. لديك معرفة أساسية بتقنيات التحسين للنشر على الأجهزة الطرفية

## ➡️ ما التالي

- [04: أجهزة EdgeAI والنشر](04.EdgeDeployment.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.