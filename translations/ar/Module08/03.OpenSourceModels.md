<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T14:21:12+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ar"
}
-->
# الجلسة 3: النماذج مفتوحة المصدر مع Foundry Local

## نظرة عامة

تتناول هذه الجلسة كيفية استخدام النماذج مفتوحة المصدر في Foundry Local: اختيار نماذج المجتمع، دمج محتوى Hugging Face، واعتماد استراتيجيات "إحضار النموذج الخاص بك" (BYOM). ستتعرف أيضًا على سلسلة Model Mondays للتعلم المستمر واكتشاف النماذج.

المراجع:
- وثائق Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- تجميع نماذج Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- GitHub الخاص بـ Foundry Local: https://github.com/microsoft/Foundry-Local

## أهداف التعلم
- اكتشاف وتقييم النماذج مفتوحة المصدر للاستدلال المحلي
- تجميع وتشغيل نماذج Hugging Face المختارة داخل Foundry Local
- تطبيق استراتيجيات اختيار النماذج بناءً على الدقة، التأخير، واحتياجات الموارد
- إدارة النماذج محليًا باستخدام التخزين المؤقت وإصدارات النماذج

## الجزء 1: اكتشاف النماذج واختيارها (خطوة بخطوة)

الخطوة 1) قم بإدراج النماذج المتاحة في الكتالوج المحلي  
```cmd
foundry model list
```
  
الخطوة 2) تجربة سريعة لاثنين من المرشحين (يتم التنزيل تلقائيًا عند التشغيل الأول)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
الخطوة 3) ملاحظة المقاييس الأساسية  
- مراقبة التأخير (بشكل ذاتي) والجودة لموجه ثابت  
- مراقبة استخدام الذاكرة عبر مدير المهام أثناء تشغيل كل نموذج  

## الجزء 2: تشغيل نماذج الكتالوج عبر CLI (خطوة بخطوة)

الخطوة 1) بدء تشغيل نموذج  
```cmd
foundry model run llama-3.2
```
  
الخطوة 2) إرسال موجه اختبار عبر نقطة النهاية المتوافقة مع OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## الجزء 3: BYOM – تجميع نماذج Hugging Face (خطوة بخطوة)

اتبع الإرشادات الرسمية لتجميع النماذج. التدفق العام أدناه—راجع مقال Microsoft Learn للحصول على الأوامر الدقيقة والتكوينات المدعومة.

الخطوة 1) إعداد دليل العمل  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
الخطوة 2) تجميع نموذج HF مدعوم  
- استخدم الخطوات من وثيقة التعلم لتحويل ووضع نموذج ONNX المجمع في دليل `models` الخاص بك  
- تأكيد باستخدام:  
```cmd
foundry cache ls
```
  
يجب أن ترى اسم النموذج المجمع الخاص بك (على سبيل المثال، `llama-3.2`).  

الخطوة 3) تشغيل النموذج المجمع  
```cmd
foundry model run llama-3.2 --verbose
```
  
ملاحظات:  
- تأكد من وجود مساحة كافية على القرص وذاكرة RAM للتجميع والتشغيل  
- ابدأ بنماذج أصغر للتحقق من التدفق، ثم قم بالتوسع  

## الجزء 4: تنظيم النماذج عمليًا (خطوة بخطوة)

الخطوة 1) إنشاء سجل `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
الخطوة 2) نص اختيار صغير  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## الجزء 5: معايير الأداء العملية (خطوة بخطوة)

الخطوة 1) معيار تأخير بسيط  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
الخطوة 2) فحص الجودة  
- استخدم مجموعة موجهات ثابتة، وقم بالتقاط المخرجات إلى CSV/JSON  
- قم بتقييم الطلاقة، الصلة، والصحة يدويًا (1–5)  

## الجزء 6: الخطوات التالية
- اشترك في Model Mondays للحصول على نماذج جديدة ونصائح: https://aka.ms/model-mondays  
- شارك النتائج مع فريقك في `models.json`  
- استعد للجلسة 4: مقارنة LLMs مقابل SLMs، الاستدلال المحلي مقابل السحابي، وعروض عملية

---

