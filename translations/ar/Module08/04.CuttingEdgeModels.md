<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T14:24:08+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ar"
}
-->
# الجلسة الرابعة: النماذج المتقدمة – LLMs، SLMs، والتنبؤ على الجهاز

## نظرة عامة

قارن بين LLMs و SLMs، قيّم الموازنة بين التنبؤ المحلي والسحابي، وطبّق عروضًا توضيحية تُبرز سيناريوهات EdgeAI باستخدام Phi و ONNX Runtime. سنسلط الضوء أيضًا على Chainlit RAG، خيارات التنبؤ باستخدام WebGPU، وتكامل Open WebUI.

المراجع:
- وثائق Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- دليل Open WebUI (تطبيق دردشة باستخدام Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## أهداف التعلم
- فهم الموازنة بين LLM و SLM من حيث التكلفة، التأخير، والدقة
- اختيار بين التنبؤ المحلي والسحابي بناءً على احتياجات العمل
- تنفيذ عرض توضيحي صغير لـ RAG باستخدام Chainlit
- استكشاف WebGPU لتسريع الأداء على المتصفح
- ربط Open WebUI بـ Foundry Local

## الجزء الأول: LLM مقابل SLM – مصفوفة اتخاذ القرار

اعتبارات:
- التأخير: غالبًا ما تقدم SLMs على الجهاز استجابات في أقل من ثانية
- التكلفة: التنبؤ المحلي يقلل من تكاليف السحابة
- الخصوصية: تبقى البيانات الحساسة على الجهاز
- القدرة: قد تتفوق LLMs على SLMs في المهام المعقدة
- الموثوقية: الاستراتيجيات الهجينة تقلل من مخاطر التوقف

## الجزء الثاني: المحلي مقابل السحابي – أنماط هجينة

- الأولوية للمحلي مع الاعتماد على السحابة للطلبات الكبيرة/المعقدة
- الأولوية للسحابة مع الاعتماد على المحلي للسيناريوهات الحساسة للخصوصية أو غير المتصلة
- التوجيه حسب نوع المهمة (توليد الكود إلى DeepSeek، الدردشة العامة إلى Phi/Qwen)

## الجزء الثالث: تطبيق دردشة RAG باستخدام Chainlit (بسيط)

تثبيت التبعيات:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

التشغيل:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

التوسيع: أضف مسترجعًا بسيطًا (ملفات محلية) وقم بإضافة السياق المسترجع إلى طلب المستخدم.

## الجزء الرابع: التنبؤ باستخدام WebGPU (تنبيه)

قم بتشغيل نماذج صغيرة مباشرة في المتصفح باستخدام WebGPU. هذا مثالي للعروض التوضيحية التي تركز على الخصوصية وتجارب بدون تثبيت. أدناه مثال بسيط خطوة بخطوة باستخدام ONNX Runtime Web مع موفر التنفيذ WebGPU.

1) تحقق من دعم WebGPU
- متصفحات Chromium: chrome://gpu → تأكد من تمكين "WebGPU"
- تحقق برمجيًا (سنقوم أيضًا بالتحقق في الكود): `if (!('gpu' in navigator)) { /* لا يوجد WebGPU */ }`

2) إنشاء مشروع بسيط
قم بإنشاء مجلد وملفين: `index.html` و `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) التشغيل محليًا (Windows cmd.exe)
استخدم خادم ثابت بسيط حتى يتمكن المتصفح من جلب النموذج.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

افتح http://localhost:5173 في متصفحك. يجب أن ترى سجلات التهيئة، إنشاء جلسة باستخدام WebGPU، وتنبؤ argmax.

4) استكشاف الأخطاء وإصلاحها
- إذا كان WebGPU غير متاح: قم بتحديث Chrome/Edge وتأكد من تحديث برامج تشغيل GPU، ثم تحقق من chrome://flags لـ "Enable WebGPU".
- إذا حدثت أخطاء CORS أو fetch: تأكد من تقديم الملفات عبر http:// (وليس file://) وأن عنوان URL للنموذج يسمح بطلبات عبر الأصل.
- الرجوع إلى CPU: قم بتغيير `executionProviders: ['wasm']` للتحقق من السلوك الأساسي.

5) الخطوات التالية
- استبدل النموذج بـ ONNX خاص بمجال معين (مثل تصنيف الصور أو نموذج نص صغير).
- أضف منطق المعالجة المسبقة/اللاحقة للمدخلات الحقيقية.
- للنماذج الأكبر أو التأخير في الإنتاج، يُفضل استخدام Foundry Local أو ONNX Runtime Server.

## الجزء الخامس: Open WebUI + Foundry Local (خطوة بخطوة)

يربط هذا Open WebUI بنقطة النهاية المتوافقة مع OpenAI لـ Foundry Local للحصول على واجهة دردشة محلية.

1) المتطلبات الأساسية
- تثبيت وتشغيل Foundry Local (`foundry --version`)
- نموذج جاهز للتشغيل محليًا (مثل `phi-4-mini`)
- تثبيت Docker Desktop (موصى به لـ Open WebUI)

2) تشغيل نموذج باستخدام Foundry Local
```powershell
foundry model run phi-4-mini
```
هذا يكشف عن واجهة برمجة تطبيقات متوافقة مع OpenAI على `http://localhost:8000`.

3) تشغيل Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
ملاحظات:
- على Windows، يتيح `host.docker.internal` للحاوية الوصول إلى مضيفك على `localhost`.
- قمنا بتعيين `OPENAI_API_BASE_URL` إلى نقطة النهاية لـ Foundry Local ومفتاح API وهمي `OPENAI_API_KEY`.

4) التكوين من واجهة Open WebUI (بديل)
- تصفح إلى http://localhost:3000
- أكمل الإعداد الأولي (مستخدم مسؤول)
- انتقل إلى الإعدادات → النماذج/المزودين
- تعيين عنوان URL الأساسي: `http://host.docker.internal:8000/v1`
- تعيين مفتاح API: `local-key` (عنصر نائب)
- حفظ

5) تشغيل طلب اختبار
- في دردشة Open WebUI، اختر أو أدخل اسم النموذج `phi-4-mini`
- الطلب: "اذكر خمس فوائد للتنبؤ بالذكاء الاصطناعي على الجهاز."
- يجب أن ترى استجابة يتم بثها من النموذج المحلي الخاص بك

6) استكشاف الأخطاء وإصلاحها
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) اختياري: الاحتفاظ ببيانات Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## قائمة التحقق العملية
- [ ] قارن بين الاستجابات/التأخير بين SLM و LLM محليًا
- [ ] تشغيل عرض Chainlit التوضيحي ضد نموذجين على الأقل
- [ ] ربط Open WebUI بنقطة النهاية المحلية الخاصة بك واختبارها

## الخطوات التالية
- التحضير لسير عمل الوكلاء في الجلسة الخامسة
- تحديد السيناريوهات التي تحسن فيها الاستراتيجيات الهجينة المحلية/السحابية العائد على الاستثمار

---

