<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93c0b94f58ff29d3227dc67797b93d23",
  "translation_date": "2025-09-22T14:22:08+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ar"
}
-->
# الجلسة الأولى: البدء مع Foundry Local

## نظرة عامة

يقدم Microsoft Foundry Local إمكانيات Azure AI Foundry مباشرة إلى بيئة تطوير Windows 11 الخاصة بك، مما يتيح تطوير الذكاء الاصطناعي مع الحفاظ على الخصوصية وزمن استجابة منخفض باستخدام أدوات على مستوى المؤسسات. تغطي هذه الجلسة التثبيت الكامل، الإعداد، والنشر العملي لنماذج شهيرة مثل phi، qwen، deepseek، وGPT-OSS-20B.

## أهداف التعلم

بنهاية هذه الجلسة، ستتمكن من:
- تثبيت وإعداد Foundry Local على Windows 11
- إتقان أوامر CLI وخيارات الإعداد
- فهم استراتيجيات تخزين النماذج لتحسين الأداء
- تشغيل نماذج phi، qwen، deepseek، وGPT-OSS-20B بنجاح
- إنشاء أول تطبيق ذكاء اصطناعي باستخدام Foundry Local

## المتطلبات الأساسية

### متطلبات النظام
- **Windows 11**: الإصدار 22H2 أو أحدث
- **RAM**: الحد الأدنى 16GB، الموصى به 32GB
- **التخزين**: مساحة خالية 50GB للنماذج والتخزين المؤقت
- **الأجهزة**: يفضل جهاز مزود بـ NPU أو GPU (مثل Copilot+ PC أو NVIDIA GPU)
- **الشبكة**: إنترنت عالي السرعة لتنزيل النماذج

### بيئة التطوير
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion
```

## الجزء الأول: التثبيت والإعداد

### الخطوة الأولى: تثبيت Foundry Local

قم بتثبيت Foundry Local باستخدام Winget أو قم بتنزيل المثبت من GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### الخطوة الثانية: التحقق من التثبيت

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## الجزء الثاني: فهم CLI

### هيكل الأوامر الأساسية

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## الجزء الثالث: إدارة وتخزين النماذج

يستخدم Foundry Local تخزينًا ذكيًا للنماذج لتحسين الأداء والتخزين:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## الجزء الرابع: النشر العملي للنماذج

### تشغيل نماذج Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### العمل مع نماذج Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### تشغيل نماذج DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### تشغيل GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## الجزء الخامس: إنشاء أول تطبيق

### واجهة دردشة بسيطة (متوافقة مع API الخاص بـ OpenAI)

قم بإنشاء تطبيق دردشة أساسي باستخدام REST API المتوافق مع OpenAI الخاص بـ Foundry Local. تأكد من تشغيل نموذج في نافذة طرفية أخرى.

```python
# chat_app.py
import requests
import os

class FoundryLocalChat:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000")
        self.api_key = os.getenv("OPENAI_API_KEY", "local-key")
        self.conversation_history = []
    
    def send_message(self, message):
        payload = {
            "model": self.model_name,
            "messages": self.conversation_history + [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        response = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        assistant_message = result["choices"][0]["message"]["content"]
        self.conversation_history.append({"role": "user", "content": message})
        self.conversation_history.append({"role": "assistant", "content": assistant_message})
        return assistant_message

if __name__ == "__main__":
    chat = FoundryLocalChat()
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        try:
            response = chat.send_message(user_input)
            print(f"Assistant: {response}\n")
        except Exception as e:
            print(f"Error: {e}\n")
```

### تشغيل تطبيق الدردشة

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Configure environment for OpenAI-compatible SDKs/clients (optional)
setx OPENAI_BASE_URL http://localhost:8000
setx OPENAI_API_KEY local-key

# Run the chat app
python chat_app.py
```

## الجزء السادس: استكشاف الأخطاء وإصلاحها وأفضل الممارسات

### المشكلات الشائعة وحلولها

```powershell
# Issue: Model fails to start
foundry model list
foundry model run phi-4-mini --verbose

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal
```

### مراقبة موارد النظام (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### أفضل الممارسات

- يفضل استخدام أوامر `foundry model ...`، `foundry cache ...`، و`foundry service ...` (راجع مرجع CLI)
- قم بالترقية بانتظام للوصول إلى نماذج وإصلاحات جديدة
- ابدأ بالنماذج الصغيرة (Phi mini، Qwen 7B) ثم قم بالتوسع
- راقب وحدة المعالجة المركزية/وحدة معالجة الرسومات/الذاكرة أثناء ضبط المطالبات والإعدادات

## الجزء السابع: التمارين العملية

### التمرين الأول: تشغيل سريع متعدد النماذج

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### التمرين الثاني: قياس زمن الاستجابة الأساسي

```python
# benchmark.py
import time
import requests

def test_model(model_name, prompt="Explain machine learning in 50 words."):
    start = time.time()
    r = requests.post("http://localhost:8000/v1/completions", json={
        "model": model_name,
        "prompt": prompt,
        "max_tokens": 128
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model_name, "latency_sec": round(elapsed, 3)}

for m in ["phi-4-mini", "qwen2.5-7b-instruct"]:
    print(test_model(m))
```

## المراجع

- البدء مع Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- مرجع CLI ونظرة عامة على الأوامر: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- تجميع نماذج Hugging Face لـ Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- GitHub الخاص بـ Microsoft Foundry Local: https://github.com/microsoft/Foundry-Local

---

