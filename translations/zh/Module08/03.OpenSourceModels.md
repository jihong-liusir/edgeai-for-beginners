<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:18:53+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "zh"
}
-->
# 第三节：开源模型的发现与管理

## 概述

本节课重点介绍如何使用 Foundry Local 进行实际的模型发现与管理。您将学习如何列出可用模型、测试不同选项，并了解基本性能特性。课程强调通过 Foundry CLI 的动手探索，帮助您为自己的使用场景选择合适的模型。

## 学习目标

- 掌握使用 Foundry CLI 进行模型发现与管理的命令
- 理解模型缓存和本地存储模式
- 学习快速测试和比较不同模型的方法
- 建立模型选择和基准测试的实用工作流程
- 探索通过 Foundry Local 可用的不断增长的模型生态系统

## 前置条件

- 完成第一节课：Foundry Local 入门
- 已安装并可访问 Foundry Local CLI
- 有足够的存储空间下载模型（模型大小从 1GB 到 20GB+ 不等）
- 对模型类型和使用场景有基本了解

## 概述

本节课探讨如何将开源模型引入 Foundry Local，选择社区模型、集成 Hugging Face 内容，以及采用“自带模型”（BYOM）策略。您还将了解 Model Mondays 系列，以持续学习和发现模型。

## 第六部分：动手练习

### 练习：模型发现与比较

基于示例 03 创建自己的模型评估脚本：

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```


### 您的任务

1. **运行示例 03 脚本**：`samples\03\list_and_bench.cmd`
2. **尝试不同模型**：至少测试 3 个不同的模型
3. **比较性能**：记录速度和响应质量的差异
4. **记录发现**：创建一个简单的比较图表

### 示例比较格式

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```


## 第七部分：故障排除与最佳实践

### 常见问题及解决方案

**模型无法启动：**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```


**内存不足：**
- 从较小的模型开始（如 `phi-4-mini`）
- 关闭其他应用程序
- 如果频繁遇到限制，考虑升级 RAM

**性能较慢：**
- 确保模型完全加载（检查详细输出）
- 关闭不必要的后台应用程序
- 考虑使用更快的存储设备（如 SSD）

### 最佳实践

1. **从小开始**：使用 `phi-4-mini` 验证设置
2. **一次运行一个模型**：在启动新模型前停止之前的模型
3. **监控资源**：关注内存使用情况
4. **一致测试**：使用相同的提示进行公平比较
5. **记录结果**：记录模型性能以便于您的使用场景

## 第八部分：后续步骤与参考资料

### 为第四节课做准备

- **第四节课重点**：优化工具和技术
- **前置条件**：熟悉模型切换和基本性能测试
- **推荐**：从本节课中确定 2-3 个您喜欢的模型

### 额外资源

- **[Foundry Local 文档](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**：官方文档  
- **[CLI 参考](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**：完整命令参考  
- **[Model Mondays](https://aka.ms/model-mondays)**：每周模型亮点  
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**：社区与问题反馈  
- **[示例 03：模型发现](samples/03/README.md)**：动手示例脚本  

### 关键要点

✅ **模型发现**：使用 `foundry model list` 探索可用模型  
✅ **快速测试**：使用 `list_and_bench.cmd` 模式进行快速评估  
✅ **性能监控**：基本资源使用和响应时间测量  
✅ **模型选择**：根据使用场景选择模型的实用指南  
✅ **缓存管理**：理解存储和清理流程  

您现在掌握了使用 Foundry Local 的简单 CLI 方法发现、测试和选择适合您 AI 应用的模型的实用技能。

参考资料：
- Foundry Local 文档：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- 编译 Hugging Face 模型：https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays：https://aka.ms/model-mondays
- Foundry Local GitHub：https://github.com/microsoft/Foundry-Local

## 学习目标

- 发现并评估用于本地推理的开源模型
- 在 Foundry Local 中编译并运行选定的 Hugging Face 模型
- 根据准确性、延迟和资源需求应用模型选择策略
- 使用缓存和版本管理本地管理模型

## 第一部分：使用 Foundry CLI 进行模型发现

### 基本模型管理命令

Foundry CLI 提供了简单的命令用于模型发现与管理：

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```


### 运行您的第一个模型

从流行且经过测试的模型开始，了解其性能特性：

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```


**注意**：`--verbose` 标志提供详细的启动信息，包括：
- 模型下载进度（首次运行时）
- 内存分配详情
- 服务绑定信息
- 性能初始化指标

### 理解模型类别

**小型语言模型 (SLMs)：**
- `phi-4-mini`：快速、高效，适合一般聊天
- `phi-4`：更强大的版本，推理能力更佳

**中型模型：**
- `qwen2.5-7b`：优秀的推理能力和更长的上下文
- `deepseek-r1-7b`：针对代码生成优化

**大型模型：**
- `llama-3.2`：Meta 最新的开源模型
- `qwen2.5-14b`：企业级推理能力

## 第二部分：快速模型测试与比较

### 示例 03 方法：简单的列表与基准测试

基于示例 03 模式，以下是最小化工作流程：

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```


### 测试模型性能

模型运行后，使用一致的提示进行测试：

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```


### PowerShell 测试替代方案

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```


## 第三部分：模型缓存与存储管理

### 理解模型缓存

Foundry Local 自动管理模型下载和缓存：

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```


### 模型存储注意事项

**典型模型大小：**
- `phi-4-mini`：约 2.5 GB
- `qwen2.5-7b`：约 4.1 GB  
- `deepseek-r1-7b`：约 4.3 GB
- `llama-3.2`：约 4.9 GB
- `qwen2.5-14b`：约 8.2 GB

**存储最佳实践：**
- 保留 2-3 个模型缓存以便快速切换
- 删除未使用的模型以释放空间：`foundry cache clean`
- 监控磁盘使用情况，尤其是在较小的 SSD 上
- 考虑模型大小与能力的权衡

### 模型性能监控

模型运行时，监控系统资源：

**Windows 任务管理器：**
- 观察内存使用情况（模型保持加载在 RAM 中）
- 推理时监控 CPU 利用率
- 初始模型加载时检查磁盘 I/O

**命令行监控：**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```


## 第四部分：实用模型选择指南

### 根据使用场景选择模型

**用于一般聊天和问答：**
- 起步：`phi-4-mini`（快速、高效）
- 升级：`phi-4`（推理能力更强）
- 高级：`qwen2.5-7b`（更长的上下文）

**用于代码生成：**
- 推荐：`deepseek-r1-7b`
- 替代：`qwen2.5-7b`（代码生成也不错）

**用于复杂推理：**
- 最佳选择：`qwen2.5-7b` 或 `qwen2.5-14b`
- 经济选项：`phi-4`

### 硬件需求指南

**最低系统要求：**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```


**推荐以获得最佳性能：**
- 32GB+ RAM，便于舒适地切换多个模型
- SSD 存储以加快模型加载
- 具有良好单线程性能的现代 CPU
- 支持 NPU（Windows 11 Copilot+ PC）以加速推理

### 模型切换工作流程

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```


## 第五部分：简单模型基准测试

### 基本性能测试

以下是比较模型性能的简单方法：

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```


### 手动质量评估

对于每个模型，使用一致的提示进行测试并手动评估：

**测试提示：**
1. “用简单的语言解释量子计算。”
2. “写一个 Python 函数来排序列表。”
3. “远程工作的优缺点是什么？”
4. “总结边缘 AI 的优势。”

**评估标准：**
- **准确性**：信息是否正确？
- **清晰度**：解释是否易于理解？
- **完整性**：是否全面回答了问题？
- **速度**：响应速度如何？

### 资源使用监控

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```


## 第六部分：后续步骤

- 订阅 Model Mondays，获取新模型和技巧：https://aka.ms/model-mondays
- 将发现结果贡献到团队的 `models.json`
- 为第四节课做准备：比较 LLMs 与 SLMs、本地与云推理，以及动手演示

---

**免责声明**：  
本文档使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。