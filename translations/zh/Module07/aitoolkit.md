<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-15T17:22:34+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "zh"
}
-->
# AI工具包用于Visual Studio Code - 边缘AI开发指南

## 介绍

欢迎使用全面指南，了解如何在边缘AI开发中使用AI工具包与Visual Studio Code。随着人工智能从集中式云计算转向分布式边缘设备，开发者需要强大的集成工具来应对边缘部署的独特挑战——从资源限制到离线操作需求。

AI工具包为Visual Studio Code提供了一个完整的开发环境，专门用于构建、测试和优化能够在边缘设备上高效运行的AI应用程序。无论您是为物联网传感器、移动设备、嵌入式系统还是边缘服务器开发，这款工具包都能在熟悉的VS Code环境中简化您的整个开发工作流程。

本指南将带您了解如何在边缘AI项目中利用AI工具包的核心概念、工具和最佳实践，从初始模型选择到生产部署。

## 概述

AI工具包在VS Code中提供了一个集成开发环境，用于边缘AI应用程序的完整生命周期。它与OpenAI、Anthropic、Google和GitHub等提供商的流行AI模型无缝集成，同时支持通过ONNX和Ollama进行本地模型部署——这是边缘AI应用程序需要设备端推理的关键功能。

AI工具包在边缘AI开发中的独特之处在于它专注于整个边缘部署管道。与主要针对云部署的传统AI开发工具不同，AI工具包包含专门的功能，用于模型优化、资源受限测试和边缘特定性能评估。该工具包深知边缘AI开发需要不同的考虑因素——更小的模型尺寸、更快的推理时间、离线能力以及硬件特定优化。

该平台支持多种部署场景，从简单的设备端推理到复杂的多模型边缘架构。它提供了模型转换、量化和优化工具，这些工具对于成功的边缘部署至关重要，同时保持了VS Code所知的开发者生产力。

## 学习目标

通过本指南，您将能够：

### 核心能力
- **安装和配置** AI工具包以支持边缘AI开发工作流程
- **导航和使用** AI工具包界面，包括模型目录、实验室和代理构建器
- **选择和评估**适合边缘部署的AI模型，基于性能和资源限制
- **转换和优化**模型，使用ONNX格式和量化技术以适配边缘设备

### 边缘AI开发技能
- **设计和实现**使用集成开发环境的边缘AI应用程序
- **在边缘条件下测试模型**，使用本地推理和资源监控
- **创建和定制**针对边缘部署场景优化的AI代理
- **使用边缘计算相关指标**评估模型性能（延迟、内存使用、准确性）

### 优化与部署
- **应用量化和剪枝技术**以减少模型尺寸，同时保持可接受的性能
- **优化模型**以适配特定边缘硬件平台，包括CPU、GPU和NPU加速
- **实施最佳实践**用于边缘AI开发，包括资源管理和备用策略
- **为边缘设备准备模型和应用程序**以进行生产部署

### 高级边缘AI概念
- **集成边缘AI框架**，包括ONNX Runtime、Windows ML和TensorFlow Lite
- **实现多模型架构**和边缘环境中的联邦学习场景
- **解决常见边缘AI问题**，包括内存限制、推理速度和硬件兼容性
- **设计监控和日志记录策略**用于生产中的边缘AI应用程序

### 实际应用
- **构建端到端边缘AI解决方案**，从模型选择到部署
- **展示熟练度**于边缘特定开发工作流程和优化技术
- **将学到的概念应用于实际边缘AI用例**，包括物联网、移动和嵌入式应用
- **评估和比较**不同的边缘AI部署策略及其权衡

## 边缘AI开发的关键功能

### 1. 模型目录与发现
- **本地模型支持**：发现并访问专门为边缘部署优化的AI模型
- **ONNX集成**：访问ONNX格式的模型以实现高效边缘推理
- **Ollama支持**：通过Ollama使用本地运行的模型以确保隐私和离线操作
- **模型比较**：并排比较模型，以找到性能与资源消耗之间的最佳平衡

### 2. 交互式实验室
- **本地测试环境**：在边缘部署之前本地测试模型
- **多模态实验**：使用图像、文本和其他典型边缘场景输入进行测试
- **参数调优**：实验不同的模型参数以优化边缘限制
- **实时性能监控**：在开发过程中观察推理速度和资源使用情况

### 3. 用于边缘应用的代理构建器
- **提示工程**：创建与较小边缘模型高效协作的优化提示
- **MCP工具集成**：集成模型上下文协议工具以增强边缘代理功能
- **代码生成**：生成针对边缘部署场景优化的生产级代码
- **结构化输出**：设计提供一致、结构化响应的代理，适用于边缘应用

### 4. 模型评估与测试
- **性能指标**：使用与边缘部署相关的指标评估模型（延迟、内存使用、准确性）
- **批量测试**：同时测试多个模型配置以找到最佳边缘设置
- **自定义评估**：创建特定于边缘AI用例的自定义评估标准
- **资源分析**：分析内存和计算需求以进行边缘部署规划

### 5. 模型转换与优化
- **ONNX转换**：将模型从各种格式转换为ONNX以适配边缘兼容性
- **量化**：通过量化技术减少模型尺寸并提高推理速度
- **硬件优化**：针对特定边缘硬件（CPU、GPU、NPU）优化模型
- **格式转换**：将来自Hugging Face等来源的模型转换为边缘部署

### 6. 针对边缘场景的微调
- **领域适配**：根据特定边缘用例和环境定制模型
- **本地训练**：使用GPU支持进行本地训练以满足边缘特定需求
- **Azure集成**：在边缘部署之前利用Azure容器应用进行云端微调
- **迁移学习**：将预训练模型适配于边缘特定任务和限制

### 7. 性能监控与追踪
- **边缘性能分析**：在类似边缘的条件下监控模型性能
- **追踪收集**：收集详细的性能数据以进行优化
- **瓶颈识别**：在部署到边缘设备之前识别性能问题
- **资源使用跟踪**：监控内存、CPU和推理时间以进行边缘优化

## 边缘AI开发工作流程

### 阶段1：模型发现与选择
1. **探索模型目录**：使用模型目录找到适合边缘部署的模型
2. **比较性能**：根据尺寸、准确性和推理速度评估模型
3. **本地测试**：使用Ollama或ONNX模型在边缘部署之前进行本地测试
4. **评估资源需求**：确定目标边缘设备的内存和计算需求

### 阶段2：模型优化
1. **转换为ONNX**：将选定模型转换为ONNX格式以适配边缘兼容性
2. **应用量化**：通过INT8或INT4量化减少模型尺寸
3. **硬件优化**：针对目标边缘硬件（ARM、x86、专用加速器）优化
4. **性能验证**：验证优化后的模型保持可接受的准确性

### 阶段3：应用开发
1. **代理设计**：使用代理构建器创建边缘优化的AI代理
2. **提示工程**：开发与较小边缘模型有效协作的提示
3. **集成测试**：在模拟边缘条件下测试代理
4. **代码生成**：生成针对边缘部署优化的生产代码

### 阶段4：评估与测试
1. **批量评估**：测试多个配置以找到最佳边缘设置
2. **性能分析**：分析推理速度、内存使用和准确性
3. **边缘模拟**：在类似目标边缘部署环境的条件下测试
4. **压力测试**：在各种负载条件下评估性能

### 阶段5：部署准备
1. **最终优化**：根据测试结果应用最终优化
2. **部署打包**：将模型和代码打包以进行边缘部署
3. **文档编写**：记录部署需求和配置
4. **监控设置**：为边缘部署准备监控和日志记录

## 边缘AI开发的目标受众

### 边缘AI开发者
- 构建AI驱动边缘设备和物联网解决方案的应用开发者
- 将AI功能集成到资源受限设备中的嵌入式系统开发者
- 为智能手机和平板电脑创建设备端AI应用的移动开发者

### 边缘AI工程师
- 优化模型以进行边缘部署并管理推理管道的AI工程师
- 部署和管理分布式边缘基础设施中AI模型的DevOps工程师
- 优化AI工作负载以适配边缘硬件限制的性能工程师

### 研究人员与教育者
- 开发高效模型和算法以适配边缘计算的AI研究人员
- 教授边缘AI概念并演示优化技术的教育者
- 学习边缘AI部署中的挑战和解决方案的学生

## 边缘AI用例

### 智能物联网设备
- **实时图像识别**：在物联网摄像头和传感器上部署计算机视觉模型
- **语音处理**：在智能音箱上实现语音识别和自然语言处理
- **预测性维护**：在工业边缘设备上运行异常检测模型
- **环境监测**：部署传感器数据分析模型以进行环境应用

### 移动与嵌入式应用
- **设备端翻译**：实现离线工作的语言翻译模型
- **增强现实**：部署实时对象识别和跟踪以支持AR应用
- **健康监测**：在可穿戴设备和医疗设备上运行健康分析模型
- **自主系统**：为无人机、机器人和车辆实现决策模型

### 边缘计算基础设施
- **边缘数据中心**：在边缘数据中心部署AI模型以支持低延迟应用
- **CDN集成**：将AI处理能力集成到内容分发网络中
- **5G边缘**：利用5G边缘计算支持AI驱动的应用
- **雾计算**：在雾计算环境中实现AI处理

## 安装与设置

### 快速安装
直接从Visual Studio Code Marketplace安装AI工具包扩展：

```
Install: AI Toolkit for Visual Studio Code (ms-windows-ai-studio.windows-ai-studio)
```

### 边缘AI开发的先决条件
- **ONNX Runtime**：安装ONNX Runtime以进行模型推理
- **Ollama**（可选）：安装Ollama以进行本地模型服务
- **Python环境**：设置Python并安装所需的AI库
- **边缘硬件工具**：安装硬件特定的开发工具（CUDA、OpenVINO等）

### 初始配置
1. 打开VS Code并安装AI工具包扩展
2. 配置模型来源（ONNX、Ollama、云提供商）
3. 设置本地开发环境以进行边缘测试
4. 配置开发机器的硬件加速选项

## 开始边缘AI开发

### 步骤1：模型选择
1. 在活动栏中打开AI工具包视图
2. 浏览模型目录以寻找边缘兼容模型
3. 按模型尺寸、格式（ONNX）和性能特性进行筛选
4. 使用内置比较工具比较模型

### 步骤2：本地测试
1. 使用实验室在本地测试选定模型
2. 实验不同的提示和参数
3. 在测试期间监控性能指标
4. 评估模型响应以满足边缘用例需求

### 步骤3：模型优化
1. 使用模型转换工具优化边缘部署
2. 应用量化以减少模型尺寸
3. 测试优化后的模型以确保可接受性能
4. 记录优化设置和性能权衡

### 步骤4：代理开发
1. 使用代理构建器创建边缘优化的AI代理
2. 开发与较小模型有效协作的提示
3. 集成必要的工具和API以适配边缘场景
4. 在模拟边缘条件下测试代理

### 步骤5：评估与部署
1. 使用批量评估测试多个配置
2. 在各种条件下分析性能
3. 为目标边缘设备准备部署包
4. 设置生产部署的监控和日志记录

## 边缘AI开发的最佳实践

### 模型选择
- **尺寸限制**：选择适合目标设备内存限制的模型
- **推理速度**：优先选择推理速度快的模型以支持实时应用
- **准确性权衡**：在模型准确性与资源限制之间找到平衡
- **格式兼容性**：优先选择ONNX或硬件优化格式以适配边缘部署

### 优化技术
- **量化**：使用INT8或INT4量化减少模型尺寸并提高速度
- **剪枝**：移除不必要的模型参数以减少计算需求
- **知识蒸馏**：创建保持大模型性能的小模型
- **硬件加速**：在可用时利用NPU、GPU或专用加速器

### 开发工作流程
- **迭代测试**：在开发过程中频繁在类似边缘条件下测试
- **性能监控**：持续监控资源使用和推理速度
- **版本控制**：跟踪模型版本和优化设置
- **文档编写**：记录所有优化决策和性能权衡

### 部署考虑
- **资源监控**：在生产中监控内存、CPU和功耗使用
- **备用策略**：为模型故障实施备用机制
- **更新机制**：规划模型更新和版本管理
- **安全性**: 为边缘AI应用实施适当的安全措施

## 与边缘AI框架的集成

### ONNX Runtime
- **跨平台部署**: 在不同的边缘平台上部署ONNX模型
- **硬件优化**: 利用ONNX Runtime的硬件特定优化
- **移动支持**: 使用ONNX Runtime Mobile支持智能手机和平板应用
- **物联网集成**: 使用ONNX Runtime的轻量级版本在物联网设备上部署

### Windows ML
- **Windows设备**: 针对基于Windows的边缘设备和PC进行优化
- **NPU加速**: 利用Windows设备上的神经处理单元
- **DirectML**: 在Windows平台上使用DirectML进行GPU加速
- **UWP集成**: 与通用Windows平台应用集成

### TensorFlow Lite
- **移动优化**: 在移动和嵌入式设备上部署TensorFlow Lite模型
- **硬件代理**: 使用专用硬件代理进行加速
- **微控制器**: 使用TensorFlow Lite Micro在微控制器上部署
- **跨平台支持**: 在Android、iOS和嵌入式Linux系统上部署

### Azure IoT Edge
- **云-边缘混合**: 结合云端训练与边缘推理
- **模块部署**: 将AI模型作为IoT Edge模块进行部署
- **设备管理**: 远程管理边缘设备和模型更新
- **遥测**: 收集边缘部署的性能数据和模型指标

## 高级边缘AI场景

### 多模型部署
- **模型集成**: 部署多个模型以提高准确性或冗余性
- **A/B测试**: 在边缘设备上同时测试不同模型
- **动态选择**: 根据当前设备条件选择模型
- **资源共享**: 优化多个部署模型的资源使用

### 联邦学习
- **分布式训练**: 在多个边缘设备上训练模型
- **隐私保护**: 保持训练数据本地化，同时共享模型改进
- **协作学习**: 使设备能够从集体经验中学习
- **边缘-云协调**: 在边缘设备和云基础设施之间协调学习

### 实时处理
- **流处理**: 在边缘设备上处理连续数据流
- **低延迟推理**: 优化以实现最小推理延迟
- **批处理**: 在边缘设备上高效处理数据批次
- **自适应处理**: 根据当前设备能力调整处理方式

## 边缘AI开发故障排除

### 常见问题
- **内存限制**: 模型过大，超出目标设备内存
- **推理速度**: 模型推理速度无法满足实时需求
- **准确性下降**: 优化导致模型准确性不可接受地降低
- **硬件兼容性**: 模型与目标硬件不兼容

### 调试策略
- **性能分析**: 使用AI工具包的追踪功能识别瓶颈
- **资源监控**: 在开发过程中监控内存和CPU使用情况
- **增量测试**: 逐步测试优化以隔离问题
- **硬件模拟**: 使用开发工具模拟目标硬件

### 优化解决方案
- **进一步量化**: 应用更激进的量化技术
- **模型架构**: 考虑针对边缘优化的不同模型架构
- **预处理优化**: 优化数据预处理以适应边缘限制
- **推理优化**: 使用硬件特定的推理优化技术

## 资源与下一步

### 文档
- [AI工具包模型指南](https://code.visualstudio.com/docs/intelligentapps/models)
- [模型游乐场文档](https://code.visualstudio.com/docs/intelligentapps/playground)
- [ONNX Runtime文档](https://onnxruntime.ai/)
- [Windows ML文档](https://docs.microsoft.com/en-us/windows/ai/)

### 社区与支持
- [VS Code AI工具包GitHub](https://github.com/microsoft/vscode-ai-toolkit)
- [ONNX社区](https://github.com/onnx/onnx)
- [边缘AI开发者社区](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [VS Code扩展市场](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 学习资源
- [边缘AI基础课程](./Module01/README.md)
- [小型语言模型指南](./Module02/README.md)
- [边缘部署策略](./Module03/README.md)
- [Windows边缘AI开发](./windowdeveloper.md)

## 结论

Visual Studio Code的AI工具包为边缘AI开发提供了一个全面的平台，从模型发现和优化到部署和监控。通过利用其集成工具和工作流程，开发者可以高效地创建、测试和部署能够在资源受限的边缘设备上有效运行的AI应用。

该工具包对ONNX、Ollama以及各种云提供商的支持，加上其优化和评估能力，使其成为边缘AI开发的理想选择。无论您是在构建物联网应用、移动AI功能还是嵌入式智能系统，AI工具包都提供了成功部署边缘AI所需的工具和工作流程。

随着边缘AI的不断发展，VS Code的AI工具包始终处于前沿，为开发者提供构建下一代智能边缘应用的先进工具和能力。

---

**免责声明**：  
本文档使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们不对因使用此翻译而产生的任何误解或误读承担责任。