<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "65a22ed38b95f334dd8a893bf2c55806",
  "translation_date": "2025-10-02T11:19:31+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "zh"
}
-->
# Visual Studio Code 的 AI 工具包 - 边缘 AI 开发指南

## 介绍

欢迎阅读使用 Visual Studio Code 的 AI 工具包进行边缘 AI 开发的全面指南。随着人工智能从集中式云计算向分布式边缘设备转移，开发者需要强大且集成的工具来应对边缘部署的独特挑战——从资源限制到离线操作需求。

Visual Studio Code 的 AI 工具包通过提供一个专门设计的完整开发环境，弥合了这一差距，使开发者能够构建、测试和优化在边缘设备上高效运行的 AI 应用程序。无论您是为物联网传感器、移动设备、嵌入式系统还是边缘服务器开发应用程序，这款工具包都能在熟悉的 VS Code 环境中简化您的整个开发工作流程。

本指南将带您了解如何在边缘 AI 项目中利用 AI 工具包的核心概念、工具和最佳实践，从初始模型选择到生产部署。

## 概述

Visual Studio Code 的 AI 工具包是一款强大的扩展程序，可简化智能体开发和 AI 应用程序的创建。该工具包提供了全面的功能，支持从多种提供商（包括 Anthropic、OpenAI、GitHub、Google 等）探索、评估和部署 AI 模型，同时支持使用 ONNX 和 Ollama 进行本地模型执行。

AI 工具包的独特之处在于其对整个 AI 开发生命周期的全面覆盖。与传统的 AI 开发工具仅关注单一方面不同，AI 工具包提供了一个集成环境，涵盖了模型发现、实验、智能体开发、评估和部署——所有这些都在熟悉的 VS Code 环境中完成。

该平台专为快速原型设计和生产部署而设计，具有提示生成、快速入门、无缝 MCP（模型上下文协议）工具集成以及广泛的评估功能等特点。对于边缘 AI 开发，这意味着您可以高效地开发、测试和优化适用于边缘部署场景的 AI 应用程序，同时保持完整的开发工作流程。

## 学习目标

通过学习本指南，您将能够：

### 核心能力
- **安装和配置** Visual Studio Code 的 AI 工具包以支持边缘 AI 开发工作流程
- **导航和使用** AI 工具包界面，包括模型目录、Playground 和智能体构建器
- **选择和评估** 适合边缘部署的 AI 模型，基于性能和资源限制
- **转换和优化** 模型，使用 ONNX 格式和量化技术以适配边缘设备

### 边缘 AI 开发技能
- **设计和实现** 使用集成开发环境的边缘 AI 应用程序
- **在边缘条件下测试模型**，通过本地推理和资源监控
- **创建和定制** 针对边缘部署场景优化的 AI 智能体
- **评估模型性能**，使用与边缘计算相关的指标（如延迟、内存使用、准确性）

### 优化与部署
- **应用量化和剪枝技术**，在保持性能的同时减少模型大小
- **优化模型**，以适配特定的边缘硬件平台，包括 CPU、GPU 和 NPU 加速
- **实施最佳实践**，包括资源管理和回退策略
- **为生产部署准备模型和应用程序**，适配边缘设备

### 高级边缘 AI 概念
- **集成边缘 AI 框架**，包括 ONNX Runtime、Windows ML 和 TensorFlow Lite
- **实现多模型架构**和联邦学习场景，适配边缘环境
- **排查常见边缘 AI 问题**，如内存限制、推理速度和硬件兼容性
- **设计监控和日志策略**，用于生产中的边缘 AI 应用程序

### 实践应用
- **构建端到端的边缘 AI 解决方案**，从模型选择到部署
- **展示熟练掌握** 边缘特定的开发工作流程和优化技术
- **将学到的概念应用于** 现实世界的边缘 AI 用例，包括物联网、移动和嵌入式应用
- **评估和比较** 不同的边缘 AI 部署策略及其权衡

## 边缘 AI 开发的关键功能

### 1. 模型目录与发现
- **多提供商支持**：浏览并访问来自 Anthropic、OpenAI、GitHub、Google 等提供商的 AI 模型
- **本地模型集成**：简化 ONNX 和 Ollama 模型的发现，适配边缘部署
- **GitHub 模型**：与 GitHub 的模型托管直接集成，便于访问
- **模型比较**：并排比较模型，以找到适合边缘设备限制的最佳平衡

### 2. 交互式 Playground
- **交互式测试环境**：在受控环境中快速实验模型能力
- **多模态支持**：测试图像、文本及其他边缘场景中的典型输入
- **实时实验**：即时反馈模型响应和性能
- **参数优化**：微调模型参数以满足边缘部署需求

### 3. 提示（智能体）构建器
- **自然语言生成**：通过自然语言描述生成初始提示
- **迭代优化**：根据模型响应和性能改进提示
- **任务分解**：通过提示链和结构化输出分解复杂任务
- **变量支持**：在提示中使用变量实现动态智能体行为
- **生产代码生成**：生成生产就绪代码以快速开发应用程序

### 4. 批量运行与评估
- **多模型测试**：同时对多个提示在选定模型上运行
- **高效大规模测试**：高效测试各种输入和配置
- **自定义测试用例**：运行智能体测试用例以验证功能
- **性能比较**：比较不同模型和配置的结果

### 5. 使用数据集进行模型评估
- **标准指标**：使用内置评估器（如 F1 分数、相关性、相似性、一致性）测试 AI 模型
- **自定义评估器**：为特定用例创建自定义评估指标
- **数据集集成**：针对全面的数据集测试模型
- **性能测量**：量化模型性能以支持边缘部署决策

### 6. 微调功能
- **模型定制**：根据特定用例和领域定制模型
- **专业化适配**：将模型适配于特定领域和需求
- **边缘优化**：专门为边缘部署限制微调模型
- **领域特定训练**：创建适配特定边缘用例的模型

### 7. MCP 工具集成
- **外部工具连接**：通过模型上下文协议服务器连接智能体到外部工具
- **现实世界操作**：使智能体能够查询数据库、访问 API 或执行自定义逻辑
- **现有 MCP 服务器**：使用命令（stdio）或 HTTP（服务器发送事件）协议的工具
- **自定义 MCP 开发**：在智能体构建器中构建和测试新的 MCP 服务器

### 8. 智能体开发与测试
- **函数调用支持**：使智能体能够动态调用外部函数
- **实时集成测试**：通过实时运行和工具使用测试集成
- **智能体版本控制**：对智能体进行版本控制，并比较评估结果
- **调试与跟踪**：本地跟踪和调试功能支持智能体开发

## 边缘 AI 开发工作流程

### 阶段 1：模型发现与选择
1. **探索模型目录**：使用模型目录查找适合边缘部署的模型
2. **比较性能**：根据模型大小、准确性和推理速度评估模型
3. **本地测试**：使用 Ollama 或 ONNX 模型在本地测试
4. **评估资源需求**：确定目标边缘设备的内存和计算需求

### 阶段 2：模型优化
1. **转换为 ONNX**：将选定模型转换为 ONNX 格式以适配边缘兼容性
2. **应用量化**：通过 INT8 或 INT4 量化减少模型大小
3. **硬件优化**：针对目标边缘硬件（ARM、x86、专用加速器）进行优化
4. **性能验证**：验证优化后的模型是否保持可接受的准确性

### 阶段 3：应用程序开发
1. **智能体设计**：使用智能体构建器创建边缘优化的 AI 智能体
2. **提示工程**：开发适配小型边缘模型的提示
3. **集成测试**：在模拟边缘条件下测试智能体
4. **代码生成**：生成适配边缘部署的生产代码

### 阶段 4：评估与测试
1. **批量评估**：测试多种配置以找到最佳边缘设置
2. **性能分析**：分析推理速度、内存使用和准确性
3. **边缘模拟**：在类似目标边缘部署环境的条件下测试
4. **压力测试**：在各种负载条件下评估性能

### 阶段 5：部署准备
1. **最终优化**：根据测试结果进行最终优化
2. **部署打包**：将模型和代码打包以适配边缘部署
3. **文档编写**：记录部署需求和配置
4. **监控设置**：为边缘部署准备监控和日志记录

## 边缘 AI 开发的目标受众

### 边缘 AI 开发者
- 构建 AI 驱动边缘设备和物联网解决方案的应用开发者
- 将 AI 功能集成到资源受限设备中的嵌入式系统开发者
- 为智能手机和平板电脑创建设备端 AI 应用程序的移动开发者

### 边缘 AI 工程师
- 优化模型以适配边缘部署并管理推理管道的 AI 工程师
- 部署和管理分布式边缘基础设施中 AI 模型的 DevOps 工程师
- 优化 AI 工作负载以适配边缘硬件限制的性能工程师

### 研究人员与教育者
- 开发高效模型和算法以适配边缘计算的 AI 研究人员
- 教授边缘 AI 概念并演示优化技术的教育者
- 学习边缘 AI 部署挑战和解决方案的学生

## 边缘 AI 用例

### 智能物联网设备
- **实时图像识别**：在物联网摄像头和传感器上部署计算机视觉模型
- **语音处理**：在智能音箱上实现语音识别和自然语言处理
- **预测性维护**：在工业边缘设备上运行异常检测模型
- **环境监测**：部署传感器数据分析模型用于环境应用

### 移动和嵌入式应用
- **设备端翻译**：实现离线工作的语言翻译模型
- **增强现实**：部署实时对象识别和跟踪以支持 AR 应用
- **健康监测**：在可穿戴设备和医疗设备上运行健康分析模型
- **自主系统**：为无人机、机器人和车辆实现决策模型

### 边缘计算基础设施
- **边缘数据中心**：在边缘数据中心部署 AI 模型以支持低延迟应用
- **CDN 集成**：将 AI 处理能力集成到内容分发网络中
- **5G 边缘**：利用 5G 边缘计算支持 AI 驱动的应用
- **雾计算**：在雾计算环境中实现 AI 处理

## 安装与设置

### 扩展安装
直接从 Visual Studio Code Marketplace 安装 AI 工具包扩展：

**扩展 ID**：`ms-windows-ai-studio.windows-ai-studio`

**安装方法**：
1. **VS Code Marketplace**：在扩展视图中搜索“AI Toolkit”
2. **命令行**：`code --install-extension ms-windows-ai-studio.windows-ai-studio`
3. **直接安装**：从 [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) 下载

### 边缘 AI 开发的先决条件
- **Visual Studio Code**：建议使用最新版本
- **Python 环境**：Python 3.8+，安装必要的 AI 库
- **ONNX Runtime**（可选）：用于 ONNX 模型推理
- **Ollama**（可选）：用于本地模型服务
- **硬件加速工具**：CUDA、OpenVINO 或平台特定加速器

### 初始配置
1. **扩展激活**：打开 VS Code，确认 AI 工具包出现在活动栏中
2. **模型提供商设置**：配置对 GitHub、OpenAI、Anthropic 或其他模型提供商的访问
3. **本地环境**：设置 Python 环境并安装必要的包
4. **硬件加速**：配置 GPU/NPU 加速（如果可用）
5. **MCP 集成**：根据需要设置模型上下文协议服务器

### 初次设置检查清单
- [ ] 已安装并激活 AI 工具包扩展
- [ ] 模型目录可访问，模型可发现
- [ ] Playground 可用于模型测试
- [ ] 智能体构建器可用于提示开发
- [ ] 本地开发环境已配置
- [ ] 硬件加速（如果可用）已正确配置

## 开始使用 AI 工具包

### 快速入门指南

我们建议从 GitHub 托管的模型开始，以获得最简化的体验：

1. **安装**：按照 [安装指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup) 设置您的设备上的 AI 工具包
2. **模型发现**：从扩展树视图中选择 **CATALOG > Models**，探索可用模型
3. **GitHub 模型**：从 GitHub 托管的模型开始，以获得最佳集成
4. **Playground 测试**：从任意模型卡片中选择 **Try in Playground**，开始实验模型能力

### 边缘 AI 开发分步指南

#### 第 1 步：模型探索与选择
1. 在 VS Code 活动栏中打开 AI 工具包视图
2. 浏览模型目录，查找适合边缘部署的模型
3. 根据边缘需求按提供商（GitHub、ONNX、Ollama）筛选
4. 使用 **Try in Playground** 立即测试模型能力

#### 第 2 步：智能体开发
1. 使用 **提示（智能体）构建器** 创建边缘优化的 AI 智能体
2. 使用自然语言描述生成初始提示  
3. 根据模型响应迭代并优化提示  
4. 集成 MCP 工具以增强代理功能  

#### 第三步：测试与评估  
1. 使用 **Bulk Run** 测试多个提示在选定模型上的表现  
2. 运行代理并通过测试用例验证功能  
3. 使用内置或自定义指标评估准确性和性能  
4. 比较不同模型和配置  

#### 第四步：微调与优化  
1. 为特定边缘用例定制模型  
2. 应用领域特定的微调  
3. 针对边缘部署限制进行优化  
4. 对不同代理配置进行版本管理和比较  

#### 第五步：部署准备  
1. 使用 Agent Builder 生成生产就绪代码  
2. 设置 MCP 服务器连接以支持生产环境  
3. 为边缘设备准备部署包  
4. 配置监控和评估指标  

## 边缘 AI 开发最佳实践  

### 模型选择  
- **尺寸限制**：选择适合目标设备内存限制的模型  
- **推理速度**：优先选择推理速度快的模型以支持实时应用  
- **准确性权衡**：在模型准确性和资源限制之间找到平衡  
- **格式兼容性**：优先选择 ONNX 或硬件优化格式以支持边缘部署  

### 优化技术  
- **量化**：使用 INT8 或 INT4 量化以减小模型尺寸并提高速度  
- **剪枝**：移除不必要的模型参数以降低计算需求  
- **知识蒸馏**：创建小型模型，同时保持大型模型的性能  
- **硬件加速**：利用 NPU、GPU 或专用加速器（如有）  

### 开发工作流  
- **迭代测试**：在开发过程中频繁在边缘环境下测试  
- **性能监控**：持续监控资源使用和推理速度  
- **版本控制**：跟踪模型版本和优化设置  
- **文档记录**：记录所有优化决策和性能权衡  

### 部署注意事项  
- **资源监控**：在生产环境中监控内存、CPU 和功耗使用情况  
- **回退策略**：为模型故障实施回退机制  
- **更新机制**：规划模型更新和版本管理  
- **安全性**：为边缘 AI 应用实施适当的安全措施  

## 与边缘 AI 框架的集成  

### ONNX Runtime  
- **跨平台部署**：在不同边缘平台上部署 ONNX 模型  
- **硬件优化**：利用 ONNX Runtime 的硬件特定优化  
- **移动支持**：使用 ONNX Runtime Mobile 支持智能手机和平板应用  
- **物联网集成**：通过 ONNX Runtime 的轻量级分发在物联网设备上部署  

### Windows ML  
- **Windows 设备**：优化 Windows 系统的边缘设备和 PC  
- **NPU 加速**：利用 Windows 设备上的神经处理单元  
- **DirectML**：在 Windows 平台上使用 DirectML 进行 GPU 加速  
- **UWP 集成**：与通用 Windows 平台应用集成  

### TensorFlow Lite  
- **移动优化**：在移动和嵌入式设备上部署 TensorFlow Lite 模型  
- **硬件委托**：使用专用硬件委托进行加速  
- **微控制器**：通过 TensorFlow Lite Micro 部署到微控制器  
- **跨平台支持**：支持 Android、iOS 和嵌入式 Linux 系统  

### Azure IoT Edge  
- **云-边混合**：结合云端训练与边缘推理  
- **模块化部署**：将 AI 模型作为 IoT Edge 模块部署  
- **设备管理**：远程管理边缘设备和模型更新  
- **遥测**：收集边缘部署的性能数据和模型指标  

## 高级边缘 AI 场景  

### 多模型部署  
- **模型集成**：部署多个模型以提高准确性或提供冗余  
- **A/B 测试**：在边缘设备上同时测试不同模型  
- **动态选择**：根据当前设备条件选择模型  
- **资源共享**：优化多个部署模型的资源使用  

### 联邦学习  
- **分布式训练**：在多个边缘设备上训练模型  
- **隐私保护**：保留本地训练数据，同时共享模型改进  
- **协作学习**：使设备能够从集体经验中学习  
- **边缘-云协调**：协调边缘设备与云基础设施之间的学习  

### 实时处理  
- **流处理**：在边缘设备上处理连续数据流  
- **低延迟推理**：优化以实现最小推理延迟  
- **批量处理**：高效处理边缘设备上的数据批量  
- **自适应处理**：根据当前设备能力调整处理方式  

## 边缘 AI 开发故障排除  

### 常见问题  
- **内存限制**：模型过大，无法适应目标设备内存  
- **推理速度**：模型推理速度过慢，无法满足实时需求  
- **准确性下降**：优化导致模型准确性不可接受地降低  
- **硬件兼容性**：模型与目标硬件不兼容  

### 调试策略  
- **性能分析**：使用 AI Toolkit 的跟踪功能识别瓶颈  
- **资源监控**：在开发过程中监控内存和 CPU 使用情况  
- **增量测试**：逐步测试优化以隔离问题  
- **硬件模拟**：使用开发工具模拟目标硬件  

### 优化解决方案  
- **进一步量化**：应用更激进的量化技术  
- **模型架构**：考虑为边缘优化的不同模型架构  
- **预处理优化**：优化数据预处理以适应边缘限制  
- **推理优化**：使用硬件特定的推理优化  

## 资源与后续步骤  

### 官方文档  
- [AI Toolkit 开发者文档](https://aka.ms/AIToolkit/doc)  
- [安装与设置指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)  
- [VS Code 智能应用文档](https://code.visualstudio.com/docs/intelligentapps)  
- [Model Context Protocol (MCP) 文档](https://modelcontextprotocol.io/)  

### 社区与支持  
- [AI Toolkit GitHub 仓库](https://github.com/microsoft/vscode-ai-toolkit)  
- [GitHub 问题与功能请求](https://aka.ms/AIToolkit/feedback)  
- [Azure AI Foundry Discord 社区](https://aka.ms/azureaifoundry/discord)  
- [VS Code 扩展市场](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)  

### 技术资源  
- [ONNX Runtime 文档](https://onnxruntime.ai/)  
- [Ollama 文档](https://ollama.ai/)  
- [Windows ML 文档](https://docs.microsoft.com/en-us/windows/ai/)  
- [Azure AI Foundry 文档](https://learn.microsoft.com/en-us/azure/ai-foundry/)  

### 学习路径  
- [边缘 AI 基础课程](../Module01/README.md)  
- [小型语言模型指南](../Module02/README.md)  
- [边缘部署策略](../Module03/README.md)  
- [Windows 边缘 AI 开发](./windowdeveloper.md)  

### 其他资源  
- **仓库统计**：1.8k+ stars，150+ forks，18+ 贡献者  
- **许可证**：MIT 许可证  
- **安全性**：适用 Microsoft 安全策略  
- **遥测**：遵循 VS Code 遥测设置  

## 结论  

Visual Studio Code 的 AI Toolkit 是一个全面的现代 AI 开发平台，提供了特别适合边缘 AI 应用的流线型代理开发能力。其广泛的模型目录支持 Anthropic、OpenAI、GitHub 和 Google 等提供商，并通过 ONNX 和 Ollama 提供本地执行功能，为多样化的边缘部署场景提供了所需的灵活性。  

该工具包的优势在于其集成式方法——从 Playground 中的模型发现和实验，到 Prompt Builder 的高级代理开发、全面的评估能力，以及无缝的 MCP 工具集成。对于边缘 AI 开发者来说，这意味着在边缘部署前快速原型设计和测试 AI 代理，并能够快速迭代和优化以适应资源受限的环境。  

边缘 AI 开发的关键优势包括：  
- **快速实验**：在边缘部署前快速测试模型和代理  
- **多提供商灵活性**：从多种来源访问模型以找到最佳边缘解决方案  
- **本地开发**：使用 ONNX 和 Ollama 进行离线和隐私保护开发  
- **生产就绪**：生成生产就绪代码并通过 MCP 集成外部工具  
- **全面评估**：使用内置和自定义指标验证边缘 AI 性能  

随着 AI 向边缘部署场景的不断发展，VS Code 的 AI Toolkit 提供了构建、测试和优化智能应用所需的开发环境和工作流，特别适合资源受限环境。无论您是在开发物联网解决方案、移动 AI 应用还是嵌入式智能系统，该工具包的全面功能集和集成工作流支持整个边缘 AI 开发生命周期。  

凭借持续的开发和活跃的社区（1.8k+ GitHub stars），AI Toolkit 始终处于 AI 开发工具的前沿，不断演进以满足现代 AI 开发者在边缘部署场景中的需求。  

[Next Foundry Local](./foundrylocal.md)  

---

**免责声明**：  
本文档使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。