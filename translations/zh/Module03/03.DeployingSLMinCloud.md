<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-07-22T04:57:54+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "zh"
}
-->
# 容器化云部署 - 生产级解决方案

本教程全面介绍了在容器化环境中部署微软的 Phi-4-mini-instruct 模型的三种主要方法：vLLM、Ollama 和基于 ONNX Runtime 的 SLM Engine。这款拥有 3.8B 参数的模型是进行推理任务的理想选择，同时在边缘部署中保持高效。

## 目录

1. [Phi-4-mini 容器部署简介](../../../Module03)
2. [学习目标](../../../Module03)
3. [Phi-4-mini 分类解析](../../../Module03)
4. [vLLM 容器部署](../../../Module03)
5. [Ollama 容器部署](../../../Module03)
6. [基于 ONNX Runtime 的 SLM Engine](../../../Module03)
7. [框架对比](../../../Module03)
8. [最佳实践](../../../Module03)

## Phi-4-mini 容器部署简介

小型语言模型（SLM）是 EdgeAI 的重要进步，使资源受限设备能够实现复杂的自然语言处理功能。本教程重点介绍微软的 Phi-4-mini-instruct 的容器化部署策略，这是一款平衡能力与效率的先进推理模型。

### 特色模型：Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B 参数)**：微软最新的轻量化指令调优模型，专为内存/计算受限环境设计，具备以下卓越能力：
- **数学推理与复杂计算**
- **代码生成、调试与分析**
- **逻辑问题解决与逐步推理**
- **需要详细解释的教育应用**
- **函数调用与工具集成**

作为“小型 SLM”类别（1.5B - 13.9B 参数）的一部分，Phi-4-mini 在推理能力与资源效率之间达到了最佳平衡。

### Phi-4-mini 容器化部署的优势

- **操作效率**：以较低的计算需求快速完成推理任务
- **部署灵活性**：设备端 AI 功能，通过本地处理增强隐私
- **成本效益**：与大型模型相比，运营成本更低，同时保持质量
- **隔离性**：模型实例之间的清晰分离与安全执行环境
- **可扩展性**：轻松实现水平扩展以提高推理吞吐量

## 学习目标

完成本教程后，您将能够：

- 在各种容器化环境中部署和优化 Phi-4-mini-instruct
- 针对不同部署场景实施高级量化与压缩策略
- 配置生产级容器编排以支持推理工作负载
- 根据具体用例需求评估并选择适合的部署框架
- 应用容器化 SLM 部署的安全性、监控与扩展最佳实践

## Phi-4-mini 分类解析

### 模型规格

**技术细节：**
- **参数量**：3.8 亿（小型 SLM 类别）
- **架构**：密集型解码器 Transformer，带分组查询注意力机制
- **上下文长度**：128K tokens（推荐 32K 以获得最佳性能）
- **词汇表**：20 万 tokens，支持多语言
- **训练数据**：5T 高质量推理密集型内容

### 资源需求

| 部署类型 | 最低 RAM | 推荐 RAM | VRAM (GPU) | 存储 | 典型用例 |
|----------|----------|----------|------------|------|----------|
| **开发环境** | 6GB | 8GB | - | 8GB | 本地测试、原型设计 |
| **生产 CPU** | 8GB | 12GB | - | 10GB | 边缘服务器、成本优化部署 |
| **生产 GPU** | 6GB | 8GB | 4-6GB | 8GB | 高吞吐量推理服务 |
| **边缘优化** | 4GB | 6GB | - | 6GB | 量化部署、物联网网关 |

### Phi-4-mini 能力

- **数学卓越**：高级算术、代数与微积分问题解决
- **代码智能**：Python、JavaScript 等多语言代码生成与调试
- **逻辑推理**：逐步问题分解与解决方案构建
- **教育支持**：适合学习与教学场景的详细解释
- **函数调用**：原生支持工具集成与 API 交互

## vLLM 容器部署

vLLM 为 Phi-4-mini-instruct 提供了卓越支持，优化了推理性能并兼容 OpenAI API，非常适合生产级推理服务。

### 快速入门示例

#### 基础 CPU 部署（开发环境）
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU 加速生产部署
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### 生产配置

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### 测试 Phi-4-mini 推理能力

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama 容器部署

Ollama 为 Phi-4-mini-instruct 提供了简化的部署与管理，非常适合开发与平衡的生产部署。

### 快速设置

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### 生产配置

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### 模型优化与变体

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API 使用示例

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## 基于 ONNX Runtime 的 SLM Engine

ONNX Runtime 为 Phi-4-mini-instruct 的边缘部署提供了最佳性能，具备高级优化与跨平台兼容性。

### 基础设置

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### 简化的服务器实现

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### 模型转换脚本

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### 生产配置

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### 测试 ONNX 部署

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## 框架对比

### Phi-4-mini 的框架对比

| 特性 | vLLM | Ollama | ONNX Runtime |
|------|------|--------|--------------|
| **设置复杂度** | 中等 | 简单 | 复杂 |
| **性能（GPU）** | 优秀 (~25 tok/s) | 很好 (~20 tok/s) | 良好 (~15 tok/s) |
| **性能（CPU）** | 良好 (~8 tok/s) | 很好 (~12 tok/s) | 优秀 (~15 tok/s) |
| **内存使用** | 8-12GB | 6-10GB | 4-8GB |
| **API 兼容性** | OpenAI 兼容 | 自定义 REST | 自定义 FastAPI |
| **函数调用** | ✅ 原生支持 | ✅ 支持 | ⚠️ 自定义实现 |
| **量化支持** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX 量化 |
| **生产就绪** | ✅ 优秀 | ✅ 很好 | ✅ 良好 |
| **边缘部署** | 良好 | 优秀 | 出色 |

## 其他资源

### 官方文档
- **微软 Phi-4 模型卡**：详细规格与使用指南
- **vLLM 文档**：高级配置与优化选项
- **Ollama 模型库**：社区模型与定制化示例
- **ONNX Runtime 指南**：性能优化与部署策略

### 开发工具
- **Hugging Face Transformers**：用于模型交互与定制化
- **OpenAI API 规范**：用于 vLLM 兼容性测试
- **Docker 最佳实践**：容器安全与优化指南
- **Kubernetes 部署**：生产扩展的编排模式

### 学习资源
- **SLM 性能基准测试**：比较分析方法
- **边缘 AI 部署**：资源受限环境的最佳实践
- **推理任务优化**：数学与逻辑问题的提示策略
- **容器安全**：AI 模型部署的加固实践

## 学习成果

完成本模块后，您将能够：

1. 使用多种框架在容器化环境中部署 Phi-4-mini-instruct 模型
2. 针对不同硬件环境配置与优化 SLM 部署
3. 实施容器化 AI 部署的安全最佳实践
4. 根据具体用例需求比较并选择适合的部署框架
5. 应用监控与扩展策略以支持生产级 SLM 服务

## 下一步

- 返回 [模块 1](../Module01/README.md)
- 返回 [模块 2](../Module02/README.md)

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们不对因使用此翻译而产生的任何误解或误读承担责任。