<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:12:43+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "zh"
}
-->
# 第四部分：部署 - 生产级模型实施

## 概述

本教程将全面指导您使用 Foundry Local 部署微调量化模型的完整流程。我们将从头到尾涵盖模型转换、量化优化以及部署配置。

## 前提条件

在开始之前，请确保您具备以下条件：

- ✅ 一个已微调的 ONNX 模型，准备部署
- ✅ Windows 或 Mac 电脑
- ✅ Python 3.10 或更高版本
- ✅ 至少 8GB 可用内存
- ✅ 已在系统上安装 Foundry Local

## 第一部分：环境设置

### 安装所需工具

打开终端（Windows 上的命令提示符，Mac 上的终端），按顺序运行以下命令：

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **重要提示**：您还需要 CMake 3.31 或更高版本，可从 [cmake.org](https://cmake.org/download/) 下载。

## 第二部分：模型转换与量化

### 选择合适的格式

对于微调的小型语言模型，我们推荐使用 **ONNX 格式**，因为它具有以下优势：

- 🚀 更好的性能优化
- 🔧 硬件无关的部署
- 🏭 生产级能力
- 📱 跨平台兼容性

### 方法一：一键转换（推荐）

使用以下命令直接转换您的微调模型：

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**参数说明：**
- `--model_name_or_path`：微调模型的路径
- `--device cpu`：使用 CPU 进行优化
- `--precision int4`：使用 INT4 量化（约 75% 的大小缩减）
- `--output_path`：转换后模型的输出路径

### 方法二：配置文件方式（高级用户）

创建一个名为 `finetuned_conversion_config.json` 的配置文件：

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

然后运行：

```bash
olive run --config ./finetuned_conversion_config.json
```

### 量化选项对比

| 精度      | 文件大小       | 推理速度       | 模型质量       | 推荐用途         |
|-----------|---------------|---------------|---------------|-----------------|
| FP16      | 基线 × 0.5    | 快速          | 最佳          | 高端硬件         |
| INT8      | 基线 × 0.25   | 非常快        | 良好          | 平衡选择         |
| INT4      | 基线 × 0.125  | 最快          | 可接受        | 资源有限         |

💡 **推荐**：首次部署建议使用 INT4 量化。如果质量不满意，可尝试 INT8 或 FP16。

## 第三部分：Foundry Local 部署配置

### 创建模型配置

导航到 Foundry Local 的模型目录：

```bash
foundry cache cd ./models/
```

创建模型目录结构：

```bash
mkdir -p ./models/custom/your-finetuned-model
```

在模型目录中创建 `inference_model.json` 配置文件：

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### 特定模型模板配置

#### 针对 Qwen 系列模型：

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## 第四部分：模型测试与优化

### 验证模型安装

检查 Foundry Local 是否能识别您的模型：

```bash
foundry cache ls
```

您应该在列表中看到 `your-finetuned-model-int4`。

### 开始模型测试

```bash
foundry model run your-finetuned-model-int4
```

### 性能基准测试

在测试期间监控以下关键指标：

1. **响应时间**：测量每次响应的平均时间
2. **内存使用**：监控 RAM 消耗
3. **CPU 利用率**：检查处理器负载
4. **输出质量**：评估响应的相关性和连贯性

### 质量验证检查表

- ✅ 模型对微调领域的查询响应适当
- ✅ 响应格式符合预期的输出结构
- ✅ 长时间使用期间无内存泄漏
- ✅ 在不同输入长度下性能一致
- ✅ 正确处理边界情况和无效输入

## 总结

恭喜！您已成功完成以下内容：

- ✅ 微调模型格式转换
- ✅ 模型量化优化
- ✅ Foundry Local 部署配置
- ✅ 性能调优与故障排除

**免责声明**：  
本文档使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。