<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:02:16+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "hr"
}
-->
# Sesija 3: Open-source modeli s Foundry Local

## Pregled

Ova sesija istražuje kako integrirati open-source modele u Foundry Local: odabir modela iz zajednice, integracija sadržaja s Hugging Face platforme i primjena strategija "donesi svoj model" (BYOM). Također ćete otkriti seriju Model Mondays za kontinuirano učenje i otkrivanje modela.

Reference:
- Dokumentacija za Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilacija Hugging Face modela: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Ciljevi učenja
- Otkriti i procijeniti open-source modele za lokalnu inferenciju
- Kompilirati i pokrenuti odabrane Hugging Face modele unutar Foundry Local
- Primijeniti strategije odabira modela prema točnosti, kašnjenju i potrebama resursa
- Upravljati modelima lokalno uz pomoć predmemorije i verzioniranja

## Dio 1: Otkrivanje i odabir modela (Korak po korak)

Korak 1) Popis dostupnih modela u lokalnom katalogu  
```cmd
foundry model list
```
  
Korak 2) Brzo isprobavanje dva kandidata (automatsko preuzimanje pri prvom pokretanju)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Korak 3) Zabilježite osnovne metrike  
- Promatrajte kašnjenje (subjektivno) i kvalitetu za zadani upit  
- Pratite korištenje memorije putem Task Managera dok svaki model radi  

## Dio 2: Pokretanje modela iz kataloga putem CLI-a (Korak po korak)

Korak 1) Pokrenite model  
```cmd
foundry model run llama-3.2
```
  
Korak 2) Pošaljite testni upit putem OpenAI-kompatibilnog endpointa  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Dio 3: BYOM – Kompilacija Hugging Face modela (Korak po korak)

Slijedite službene upute za kompilaciju modela. Opći tijek je dolje naveden—pogledajte Microsoft Learn članak za točne naredbe i podržane konfiguracije.

Korak 1) Pripremite radni direktorij  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Korak 2) Kompilirajte podržani HF model  
- Koristite korake iz Learn dokumentacije za konverziju i postavljanje kompiliranog ONNX modela u vaš `models` direktorij  
- Potvrdite s:  
```cmd
foundry cache ls
```
  
Trebali biste vidjeti naziv vašeg kompiliranog modela (na primjer, `llama-3.2`).  

Korak 3) Pokrenite kompilirani model  
```cmd
foundry model run llama-3.2 --verbose
```
  
Napomene:  
- Osigurajte dovoljno prostora na disku i RAM-a za kompilaciju i pokretanje  
- Započnite s manjim modelima kako biste provjerili tijek, a zatim se proširite  

## Dio 4: Praktična kuracija modela (Korak po korak)

Korak 1) Kreirajte `models.json` registar  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Korak 2) Mali selektorski skript  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Dio 5: Benchmarking u praksi (Korak po korak)

Korak 1) Jednostavni benchmark kašnjenja  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Korak 2) Provjera kvalitete  
- Koristite fiksni set upita, zabilježite izlaze u CSV/JSON  
- Ručno ocijenite tečnost, relevantnost i točnost (1–5)  

## Dio 6: Sljedeći koraci
- Pretplatite se na Model Mondays za nove modele i savjete: https://aka.ms/model-mondays  
- Podijelite svoja otkrića s timom putem `models.json`  
- Pripremite se za Sesiju 4: usporedba LLM-ova i SLM-ova, lokalna vs cloud inferencija, te praktične demonstracije  

---

