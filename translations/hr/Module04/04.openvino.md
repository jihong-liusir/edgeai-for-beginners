<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:28:02+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "hr"
}
-->
# Sekcija 4: OpenVINO Toolkit Optimizacijski Paket

## Sadržaj
1. [Uvod](../../../Module04)
2. [Što je OpenVINO?](../../../Module04)
3. [Instalacija](../../../Module04)
4. [Vodič za brzi početak](../../../Module04)
5. [Primjer: Pretvaranje i optimizacija modela s OpenVINO](../../../Module04)
6. [Napredno korištenje](../../../Module04)
7. [Najbolje prakse](../../../Module04)
8. [Rješavanje problema](../../../Module04)
9. [Dodatni resursi](../../../Module04)

## Uvod

OpenVINO (Open Visual Inference and Neural Network Optimization) je Intelov open-source alat za implementaciju visokoučinkovitih AI rješenja u oblaku, lokalnim sustavima i rubnim uređajima. Bez obzira ciljate li na CPU, GPU, VPU ili specijalizirane AI akceleratore, OpenVINO pruža sveobuhvatne mogućnosti optimizacije uz očuvanje točnosti modela i omogućuje implementaciju na različitim platformama.

## Što je OpenVINO?

OpenVINO je open-source alat koji omogućuje programerima da učinkovito optimiziraju, pretvaraju i implementiraju AI modele na raznim hardverskim platformama. Sastoji se od tri glavne komponente: OpenVINO Runtime za inferenciju, Neural Network Compression Framework (NNCF) za optimizaciju modela i OpenVINO Model Server za skalabilnu implementaciju.

### Ključne značajke

- **Implementacija na više platformi**: Podrška za Linux, Windows i macOS s Python, C++ i C API-jem
- **Hardverska akceleracija**: Automatsko otkrivanje uređaja i optimizacija za CPU, GPU, VPU i AI akceleratore
- **Okvir za kompresiju modela**: Napredne tehnike kvantizacije, obrezivanja i optimizacije putem NNCF-a
- **Kompatibilnost s okvirima**: Direktna podrška za TensorFlow, ONNX, PaddlePaddle i PyTorch modele
- **Podrška za generativni AI**: Specijalizirani OpenVINO GenAI za implementaciju velikih jezičnih modela i generativnih AI aplikacija

### Prednosti

- **Optimizacija performansi**: Značajna poboljšanja brzine uz minimalan gubitak točnosti
- **Smanjen prostor za implementaciju**: Minimalne vanjske ovisnosti pojednostavljuju instalaciju i implementaciju
- **Poboljšano vrijeme pokretanja**: Optimizirano učitavanje modela i predmemoriranje za bržu inicijalizaciju aplikacija
- **Skalabilna implementacija**: Od rubnih uređaja do infrastrukture u oblaku uz dosljedne API-je
- **Spremno za proizvodnju**: Pouzdanost na razini poduzeća uz sveobuhvatnu dokumentaciju i podršku zajednice

## Instalacija

### Preduvjeti

- Python 3.8 ili noviji
- pip upravitelj paketa
- Virtualno okruženje (preporučeno)
- Kompatibilan hardver (preporučuju se Intel CPU-ovi, ali podržava različite arhitekture)

### Osnovna instalacija

Kreirajte i aktivirajte virtualno okruženje:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Instalirajte OpenVINO Runtime:

```bash
pip install openvino
```

Instalirajte NNCF za optimizaciju modela:

```bash
pip install nncf
```

### Instalacija OpenVINO GenAI

Za generativne AI aplikacije:

```bash
pip install openvino-genai
```

### Opcionalne ovisnosti

Dodatni paketi za specifične slučajeve korištenja:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Provjera instalacije

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Ako je uspješno, trebali biste vidjeti informacije o verziji OpenVINO-a.

## Vodič za brzi početak

### Vaša prva optimizacija modela

Pretvorimo i optimizirajmo Hugging Face model koristeći OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Što ovaj proces radi

Radni tijek optimizacije uključuje: učitavanje originalnog modela s Hugging Face-a, pretvaranje u OpenVINO Intermediate Representation (IR) format, primjenu osnovnih optimizacija i kompilaciju za ciljani hardver.

### Objašnjenje ključnih parametara

- `export=True`: Pretvara model u OpenVINO IR format
- `compile=False`: Odgađa kompilaciju do vremena izvođenja radi fleksibilnosti
- `device`: Ciljani hardver ("CPU", "GPU", "AUTO" za automatski odabir)
- `save_pretrained()`: Sprema optimizirani model za ponovnu upotrebu

## Primjer: Pretvaranje i optimizacija modela s OpenVINO

### Korak 1: Pretvaranje modela s NNCF kvantizacijom

Kako primijeniti kvantizaciju nakon treninga koristeći NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Korak 2: Napredna optimizacija s kompresijom težina

Za modele temeljene na transformatorima, primijenite kompresiju težina:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Korak 3: Inferencija s optimiziranim modelom

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struktura izlaza

Nakon optimizacije, vaš direktorij modela sadržavat će:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Napredno korištenje

### Konfiguracija s NNCF YAML-om

Za složene radne tijekove optimizacije koristite NNCF konfiguracijske datoteke:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Primijenite konfiguraciju:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimizacija za GPU

Za GPU akceleraciju:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimizacija obrade u serijama

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Implementacija modela na serveru

Implementirajte optimizirane modele s OpenVINO Model Serverom:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Klijentski kod za model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Najbolje prakse

### 1. Odabir i priprema modela
- Koristite modele iz podržanih okvira (PyTorch, TensorFlow, ONNX)
- Osigurajte da ulazi modela imaju fiksne ili poznate dinamičke oblike
- Testirajte s reprezentativnim skupovima podataka za kalibraciju

### 2. Odabir strategije optimizacije
- **Kvantizacija nakon treninga**: Počnite ovdje za brzu optimizaciju
- **Kompresija težina**: Idealno za velike jezične modele i transformatore
- **Trening svjestan kvantizacije**: Koristite kada je točnost ključna

### 3. Optimizacija specifična za hardver
- **CPU**: Koristite INT8 kvantizaciju za uravnotežene performanse
- **GPU**: Iskoristite FP16 preciznost i obradu u serijama
- **VPU**: Fokusirajte se na pojednostavljenje modela i fuziju slojeva

### 4. Podešavanje performansi
- **Način propusnosti**: Za obradu velikih količina podataka u serijama
- **Način kašnjenja**: Za interaktivne aplikacije u stvarnom vremenu
- **AUTO uređaj**: Dopustite OpenVINO-u da odabere optimalni hardver

### 5. Upravljanje memorijom
- Koristite dinamičke oblike pažljivo kako biste izbjegli prekomjernu potrošnju memorije
- Implementirajte predmemoriranje modela za brže učitavanje
- Pratite potrošnju memorije tijekom optimizacije

### 6. Validacija točnosti
- Uvijek validirajte optimizirane modele u odnosu na originalne performanse
- Koristite reprezentativne testne skupove podataka za evaluaciju
- Razmotrite postupnu optimizaciju (počnite s konzervativnim postavkama)

## Rješavanje problema

### Uobičajeni problemi

#### 1. Problemi s instalacijom
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Pogreške pri pretvaranju modela
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Problemi s performansama
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Problemi s memorijom
- Smanjite veličinu serije modela tijekom optimizacije
- Koristite streaming za velike skupove podataka
- Omogućite predmemoriranje modela: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Smanjenje točnosti
- Koristite veću preciznost (INT8 umjesto INT4)
- Povećajte veličinu kalibracijskog skupa podataka
- Primijenite optimizaciju s mješovitom preciznošću

### Praćenje performansi

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Dobivanje pomoći

- **Dokumentacija**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub problemi**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum zajednice**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Dodatni resursi

### Službeni linkovi
- **OpenVINO početna stranica**: [openvino.ai](https://openvino.ai/)
- **GitHub repozitorij**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF repozitorij**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Resursi za učenje
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Vodič za brzi početak**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Vodič za optimizaciju**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Alati za integraciju
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmark performansi
- **Službeni benchmarkovi**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Primjeri iz zajednice
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Sveobuhvatni vodiči dostupni u OpenVINO repozitoriju za bilježnice
- **Primjeri aplikacija**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Primjeri iz stvarnog svijeta za različite domene (računalni vid, NLP, audio)
- **Blog postovi**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI i blogovi zajednice s detaljnim slučajevima korištenja

### Povezani alati
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Dodatne tehnike optimizacije za Intel hardver
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Za usporedbe mobilne i rubne implementacije
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativni mehanizmi za inferenciju na više platformi

## ➡️ Što slijedi

- [05: Apple MLX Framework Dubinska analiza](./05.AppleMLX.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.