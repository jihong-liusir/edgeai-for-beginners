<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-19T00:46:07+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "hr"
}
-->
# Poglavlje 4: Detaljno o Apple MLX Frameworku

## Sadržaj
1. [Uvod u Apple MLX](../../../Module04)
2. [Ključne značajke za razvoj LLM-a](../../../Module04)
3. [Vodič za instalaciju](../../../Module04)
4. [Početak rada s MLX-om](../../../Module04)
5. [MLX-LM: Jezični modeli](../../../Module04)
6. [Rad s velikim jezičnim modelima](../../../Module04)
7. [Integracija s Hugging Face](../../../Module04)
8. [Konverzija i kvantizacija modela](../../../Module04)
9. [Fino podešavanje jezičnih modela](../../../Module04)
10. [Napredne značajke LLM-a](../../../Module04)
11. [Najbolje prakse za LLM](../../../Module04)
12. [Rješavanje problema](../../../Module04)
13. [Dodatni resursi](../../../Module04)

## Uvod u Apple MLX

Apple MLX je okvir za strojno učenje dizajniran za učinkovito i fleksibilno korištenje na Apple Siliconu, razvijen od strane Apple Machine Learning Research tima. Objavljen u prosincu 2023., MLX predstavlja Appleov odgovor na okvire poput PyTorcha i TensorFlowa, s posebnim naglaskom na omogućavanje moćnih mogućnosti velikih jezičnih modela na Mac računalima.

### Što MLX čini posebnim za LLM-ove?

MLX je dizajniran da u potpunosti iskoristi arhitekturu objedinjene memorije Apple Silicona, što ga čini posebno pogodnim za lokalno pokretanje i fino podešavanje velikih jezičnih modela na Mac računalima. Okvir uklanja mnoge probleme kompatibilnosti s kojima su se Mac korisnici tradicionalno suočavali pri radu s LLM-ovima.

### Tko bi trebao koristiti MLX za LLM-ove?

- **Mac korisnici** koji žele lokalno pokretati LLM-ove bez ovisnosti o oblaku
- **Istraživači** koji eksperimentiraju s fino podešavanjem i prilagodbom jezičnih modela
- **Razvojni programeri** koji grade AI aplikacije s mogućnostima jezičnih modela
- **Svi** koji žele iskoristiti Apple Silicon za generiranje teksta, chat i jezične zadatke

## Ključne značajke za razvoj LLM-a

### 1. Arhitektura objedinjene memorije
Objedinjena memorija Apple Silicona omogućuje MLX-u učinkovito upravljanje velikim jezičnim modelima bez prekomjernog kopiranja memorije, što znači da možete raditi s većim modelima na istom hardveru.

### 2. Optimizacija za Apple Silicon
MLX je od temelja izgrađen za M-seriju čipova Applea, pružajući optimalne performanse za transformacijske arhitekture koje se često koriste u jezičnim modelima.

### 3. Podrška za kvantizaciju
Ugrađena podrška za kvantizaciju od 4-bit i 8-bit smanjuje zahtjeve za memorijom uz očuvanje kvalitete modela, omogućujući pokretanje većih modela na potrošačkom hardveru.

### 4. Integracija s Hugging Face
Besprijekorna integracija s Hugging Face ekosustavom omogućuje pristup tisućama unaprijed treniranih jezičnih modela uz jednostavne alate za konverziju.

### 5. LoRA fino podešavanje
Podrška za Low-Rank Adaptation (LoRA) omogućuje učinkovito fino podešavanje velikih modela uz minimalne računalne resurse.

## Vodič za instalaciju

### Sistemski zahtjevi
- **macOS 13.0+** (za optimizaciju Apple Silicona)
- **Python 3.8+**
- **Apple Silicon** (M1, M2, M3, M4 serija)
- **Izvorno ARM okruženje** (ne pokretanje pod Rosettom)
- **8GB+ RAM-a** (preporučeno 16GB+ za veće modele)

### Brza instalacija za LLM-ove

Najlakši način za početak rada s jezičnim modelima je instalacija MLX-LM:

```bash
pip install mlx-lm
```

Ova jedinstvena naredba instalira i osnovni MLX okvir i alate za jezične modele.

### Postavljanje virtualnog okruženja (preporučeno)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Dodatne ovisnosti za audio modele

Ako planirate raditi s govornim modelima poput Whispera:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## Početak rada s MLX-om

### Vaš prvi jezični model

Započnimo s jednostavnim primjerom generiranja teksta:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Primjer Python API-ja

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Razumijevanje učitavanja modela

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: Jezični modeli

### Podržane arhitekture modela

MLX-LM podržava širok raspon popularnih arhitektura jezičnih modela:

- **LLaMA i LLaMA 2** - Meta-ovi temeljni modeli
- **Mistral i Mixtral** - Učinkoviti i moćni modeli
- **Phi-3** - Microsoftovi kompaktni jezični modeli
- **Qwen** - Alibaba-ovi višejezični modeli
- **Code Llama** - Specijalizirani za generiranje koda
- **Gemma** - Googleovi otvoreni jezični modeli

### Sučelje naredbenog retka

Sučelje naredbenog retka MLX-LM pruža moćne alate za rad s jezičnim modelima:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Python API za napredne slučajeve korištenja

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Rad s velikim jezičnim modelima

### Obrasci generiranja teksta

#### Generiranje u jednom koraku
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Slijeđenje uputa
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Kreativno pisanje
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Višekratni razgovori

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Integracija s Hugging Face

### Pronalaženje modela kompatibilnih s MLX-om

MLX besprijekorno radi s Hugging Face ekosustavom:

- **Pregled MLX modela**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX zajednica**: https://huggingface.co/mlx-community (prekonvertirani modeli)
- **Izvorni modeli**: Većina LLaMA, Mistral, Phi i Qwen modela radi s konverzijom

### Učitavanje modela s Hugging Face-a

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Preuzimanje modela za offline korištenje

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Konverzija i kvantizacija modela

### Konverzija Hugging Face modela u MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Razumijevanje kvantizacije

Kvantizacija smanjuje veličinu modela i korištenje memorije uz minimalan gubitak kvalitete:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Prilagođena kvantizacija

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Fino podešavanje jezičnih modela

### LoRA (Low-Rank Adaptation) fino podešavanje

MLX podržava učinkovito fino podešavanje pomoću LoRA, što omogućuje prilagodbu velikih modela uz minimalne računalne resurse:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Priprema podataka za treniranje

Izradite JSON datoteku s primjerima za treniranje:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Naredba za fino podešavanje

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Korištenje fino podešenih modela

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Napredne značajke LLM-a

### Keširanje upita za učinkovitost

Za ponovnu upotrebu istog konteksta, MLX podržava keširanje upita radi poboljšanja performansi:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Generiranje teksta u stvarnom vremenu

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Rad s modelima za generiranje koda

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Rad s chat modelima

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Najbolje prakse za LLM

### Upravljanje memorijom

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Smjernice za odabir modela

**Za eksperimentiranje i učenje:**
- Koristite 4-bit kvantizirane modele (npr. `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Započnite s manjim modelima poput Phi-3-mini

**Za produkcijske aplikacije:**
- Razmotrite kompromis između veličine modela i kvalitete
- Testirajte kvantizirane i modele pune preciznosti
- Provedite benchmark testove za svoje specifične slučajeve korištenja

**Za specifične zadatke:**
- **Generiranje koda**: CodeLlama, Code Llama Instruct
- **Opći chat**: Mistral-7B-Instruct, Phi-3
- **Višejezičnost**: Qwen modeli
- **Kreativno pisanje**: Više postavke temperature s Mistralom ili LLaMA-om

### Najbolje prakse za inženjering upita

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Optimizacija performansi

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Rješavanje problema

### Uobičajeni problemi i rješenja

#### Problemi s instalacijom

**Problem**: "Nema odgovarajuće distribucije za mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Rješenje**: Koristite izvorni ARM Python ili Miniconda:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Problemi s memorijom

**Problem**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Problemi s učitavanjem modela

**Problem**: Model se ne učitava ili generira loš izlaz
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Problemi s performansama

**Problem**: Sporo generiranje
- Zatvorite druge aplikacije koje intenzivno koriste memoriju
- Koristite kvantizirane modele kad je to moguće
- Provjerite da ne radite pod Rosettom
- Provjerite dostupnu memoriju prije učitavanja modela

### Savjeti za otklanjanje grešaka

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Dodatni resursi

### Službena dokumentacija i repozitoriji

- **MLX GitHub repozitorij**: https://github.com/ml-explore/mlx
- **MLX-LM primjeri**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX dokumentacija**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX integracija**: https://huggingface.co/docs/hub/en/mlx

### Kolekcije modela

- **MLX zajednički modeli**: https://huggingface.co/mlx-community
- **Trendovski MLX modeli**: https://huggingface.co/models?library=mlx&sort=trending

### Primjeri aplikacija

1. **Osobni AI asistent**: Izradite lokalni chatbot s memorijom razgovora
2. **Pomoćnik za kodiranje**: Kreirajte asistenta za kodiranje za svoj razvojni tijek
3. **Generator sadržaja**: Razvijte alate za pisanje, sažimanje i kreiranje sadržaja
4. **Prilagođeni fino podešeni modeli**: Prilagodite modele za zadatke specifične za domenu
5. **Multimodalne aplikacije**: Kombinirajte generiranje teksta s drugim MLX mogućnostima

### Zajednica i učenje

- **MLX zajedničke rasprave**: GitHub Issues i Discussions
- **Hugging Face forumi**: Podrška zajednice i dijeljenje modela
- **Apple Developer dokumentacija**: Službeni Apple ML resursi

### Citiranje

Ako koristite MLX u svom istraživanju, molimo citirajte:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Zaključak

Apple MLX je revolucionirao način pokretanja velikih jezičnih modela na Mac računalima. Pružajući nativnu optimizaciju za Apple Silicon, besprijekornu integraciju s Hugging Face-om i moćne značajke poput kvantizacije i LoRA fino podešavanja, MLX omogućuje lokalno pokretanje sofisticiranih jezičnih modela uz izvrsne performanse.

Bilo da gradite chatbote, asistente za kodiranje, generatore sadržaja ili prilagođene fino podešene modele, MLX pruža alate i performanse potrebne za iskorištavanje punog potencijala vašeg Apple Silicon Maca za aplikacije jezičnih modela. Fokus okvira na učinkovitost i jednostavnost korištenja čini ga izvrsnim izborom za istraživanje i produkcijske aplikacije.

Započnite s osnovnim primjerima u ovom vodiču, istražite bogat ekosustav unaprijed konvertiranih modela na Hugging Face-u i postupno napredujte prema naprednijim značajkama poput fino podešavanja i razvoja prilagođenih modela. Kako MLX ekosustav nastavlja rasti, postaje sve moćnija platforma za razvoj jezičnih modela na Apple hardveru.

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za nesporazume ili pogrešne interpretacije koje mogu proizaći iz korištenja ovog prijevoda.