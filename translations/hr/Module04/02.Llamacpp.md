<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-19T00:50:45+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "hr"
}
-->
# Sekcija 2: Vodič za implementaciju Llama.cpp

## Sadržaj
1. [Uvod](../../../Module04)
2. [Što je Llama.cpp?](../../../Module04)
3. [Instalacija](../../../Module04)
4. [Izgradnja iz izvornog koda](../../../Module04)
5. [Kvantizacija modela](../../../Module04)
6. [Osnovna upotreba](../../../Module04)
7. [Napredne značajke](../../../Module04)
8. [Integracija s Pythonom](../../../Module04)
9. [Rješavanje problema](../../../Module04)
10. [Najbolje prakse](../../../Module04)

## Uvod

Ovaj sveobuhvatni vodič pružit će vam sve što trebate znati o Llama.cpp, od osnovne instalacije do naprednih scenarija upotrebe. Llama.cpp je moćna C++ implementacija koja omogućuje učinkovito izvođenje velikih jezičnih modela (LLM) uz minimalnu konfiguraciju i izvrsne performanse na različitim hardverskim konfiguracijama.

## Što je Llama.cpp?

Llama.cpp je okvir za izvođenje LLM-a napisan u C/C++ koji omogućuje lokalno pokretanje velikih jezičnih modela uz minimalnu konfiguraciju i vrhunske performanse na širokom rasponu hardvera. Ključne značajke uključuju:

### Osnovne značajke
- **Implementacija u čistom C/C++** bez ovisnosti
- **Kompatibilnost na više platformi** (Windows, macOS, Linux)
- **Optimizacija hardvera** za različite arhitekture
- **Podrška za kvantizaciju** (od 1,5-bitne do 8-bitne kvantizacije cijelih brojeva)
- **Podrška za ubrzanje putem CPU-a i GPU-a**
- **Učinkovito korištenje memorije** za ograničena okruženja

### Prednosti
- Učinkovito radi na CPU-u bez potrebe za specijaliziranim hardverom
- Podržava više GPU backendova (CUDA, Metal, OpenCL, Vulkan)
- Lagano i prenosivo
- Apple Silicon je prioritet - optimizirano putem ARM NEON-a, Accelerate i Metal okvira
- Podržava različite razine kvantizacije za smanjenu potrošnju memorije

## Instalacija

### Metoda 1: Prethodno izgrađene binarne datoteke (preporučeno za početnike)

#### Preuzimanje s GitHub Releases
1. Posjetite [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Preuzmite odgovarajuću binarnu datoteku za vaš sustav:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` za Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` za macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` za Linux

3. Raspakirajte arhivu i dodajte direktorij u PATH vašeg sustava

#### Korištenje upravitelja paketa

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (razne distribucije):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metoda 2: Python paket (llama-cpp-python)

#### Osnovna instalacija
```bash
pip install llama-cpp-python
```

#### S hardverskim ubrzanjem
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Izgradnja iz izvornog koda

### Preduvjeti

**Sistemski zahtjevi:**
- C++ kompajler (GCC, Clang ili MSVC)
- CMake (verzija 3.14 ili novija)
- Git
- Alati za izgradnju za vašu platformu

**Instalacija preduvjeta:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Instalirajte Visual Studio 2022 s alatima za razvoj C++
- Instalirajte CMake s službene web stranice
- Instalirajte Git

### Osnovni proces izgradnje

1. **Klonirajte repozitorij:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurirajte izgradnju:**
```bash
cmake -B build
```

3. **Izgradite projekt:**
```bash
cmake --build build --config Release
```

Za bržu kompilaciju koristite paralelne zadatke:
```bash
cmake --build build --config Release -j 8
```

### Izgradnje specifične za hardver

#### Podrška za CUDA (NVIDIA GPU-ovi)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Podrška za Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Podrška za OpenBLAS (optimizacija CPU-a)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Podrška za Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Napredne opcije izgradnje

#### Debug izgradnja
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### S dodatnim značajkama
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kvantizacija modela

### Razumijevanje GGUF formata

GGUF (Generalizirani GGML Unified Format) je optimizirani format datoteke dizajniran za učinkovito izvođenje velikih jezičnih modela koristeći Llama.cpp i druge okvire. Pruža:

- Standardizirano pohranjivanje težina modela
- Poboljšanu kompatibilnost na različitim platformama
- Povećane performanse
- Učinkovito rukovanje metapodacima

### Vrste kvantizacije

Llama.cpp podržava različite razine kvantizacije:

| Tip | Bitovi | Opis | Primjena |
|-----|--------|------|----------|
| F16 | 16 | Polupreciznost | Visoka kvaliteta, velika memorija |
| Q8_0 | 8 | 8-bitna kvantizacija | Dobar balans |
| Q4_0 | 4 | 4-bitna kvantizacija | Umjerena kvaliteta, manja veličina |
| Q2_K | 2 | 2-bitna kvantizacija | Najmanja veličina, niža kvaliteta |

### Pretvaranje modela

#### Iz PyTorcha u GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Direktno preuzimanje s Hugging Face
Mnogi modeli dostupni su u GGUF formatu na Hugging Face:
- Potražite modele s "GGUF" u nazivu
- Preuzmite odgovarajuću razinu kvantizacije
- Koristite direktno s llama.cpp

## Osnovna upotreba

### Sučelje naredbenog retka

#### Jednostavno generiranje teksta
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Korištenje modela s Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Način rada servera
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Uobičajeni parametri

| Parametar | Opis | Primjer |
|-----------|------|---------|
| `-m` | Putanja do datoteke modela | `-m model.gguf` |
| `-p` | Tekst upita | `-p "Hello world"` |
| `-n` | Broj generiranih tokena | `-n 100` |
| `-c` | Veličina konteksta | `-c 4096` |
| `-t` | Broj niti | `-t 8` |
| `-ngl` | GPU slojevi | `-ngl 32` |
| `-temp` | Temperatura | `-temp 0.7` |

### Interaktivni način rada

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Napredne značajke

### API servera

#### Pokretanje servera
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Korištenje API-ja
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optimizacija performansi

#### Upravljanje memorijom
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Višenitnost
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Ubrzanje putem GPU-a
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integracija s Pythonom

### Osnovna upotreba s llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Sučelje za chat

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streaming odgovora

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integracija s LangChainom

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Rješavanje problema

### Uobičajeni problemi i rješenja

#### Greške pri izgradnji

**Problem: CMake nije pronađen**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problem: Kompajler nije pronađen**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Problemi pri izvođenju

**Problem: Učitavanje modela ne uspijeva**
- Provjerite putanju do datoteke modela
- Provjerite dozvole datoteke
- Osigurajte dovoljno RAM-a
- Isprobajte različite razine kvantizacije

**Problem: Loše performanse**
- Omogućite hardversko ubrzanje
- Povećajte broj niti
- Koristite odgovarajuću kvantizaciju
- Provjerite korištenje GPU memorije

#### Problemi s memorijom

**Problem: Nedostatak memorije**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problemi specifični za platformu

#### Windows
- Koristite MinGW ili Visual Studio kompajler
- Osigurajte pravilnu konfiguraciju PATH-a
- Provjerite interferenciju antivirusnog softvera

#### macOS
- Omogućite Metal za Apple Silicon
- Koristite Rosetta 2 za kompatibilnost ako je potrebno
- Provjerite alate naredbenog retka Xcodea

#### Linux
- Instalirajte razvojne pakete
- Provjerite verzije GPU upravljačkih programa
- Provjerite instalaciju CUDA alata

## Najbolje prakse

### Odabir modela
1. **Odaberite odgovarajuću kvantizaciju** prema vašem hardveru
2. **Razmotrite veličinu modela** u odnosu na kompromis kvalitete
3. **Testirajte različite modele** za vaš specifični slučaj upotrebe

### Optimizacija performansi
1. **Koristite GPU ubrzanje** kad je dostupno
2. **Optimizirajte broj niti** za vaš CPU
3. **Postavite odgovarajuću veličinu konteksta** za vaš slučaj upotrebe
4. **Omogućite mapiranje memorije** za velike modele

### Implementacija u produkciji
1. **Koristite način rada servera** za pristup API-ju
2. **Implementirajte pravilno rukovanje greškama**
3. **Pratite korištenje resursa**
4. **Postavite zapisivanje i praćenje**

### Radni tijek razvoja
1. **Započnite s manjim modelima** za testiranje
2. **Koristite kontrolu verzija** za konfiguracije modela
3. **Dokumentirajte svoje konfiguracije**
4. **Testirajte na različitim platformama**

### Sigurnosni aspekti
1. **Provjerite ulazne upite**
2. **Implementirajte ograničenje brzine**
3. **Osigurajte API krajnje točke**
4. **Pratite obrasce zloupotrebe**

## Zaključak

Llama.cpp pruža moćan i učinkovit način za lokalno pokretanje velikih jezičnih modela na različitim hardverskim konfiguracijama. Bilo da razvijate AI aplikacije, provodite istraživanja ili jednostavno eksperimentirate s LLM-ovima, ovaj okvir nudi fleksibilnost i performanse potrebne za širok raspon slučajeva upotrebe.

Ključni zaključci:
- Odaberite metodu instalacije koja najbolje odgovara vašim potrebama
- Optimizirajte za svoju specifičnu hardversku konfiguraciju
- Započnite s osnovnom upotrebom i postupno istražujte napredne značajke
- Razmotrite korištenje Python vezivanja za lakšu integraciju
- Slijedite najbolje prakse za implementaciju u produkciji

Za više informacija i ažuriranja posjetite [službeni Llama.cpp repozitorij](https://github.com/ggml-org/llama.cpp) i konzultirajte sveobuhvatnu dokumentaciju i dostupne resurse zajednice.

## ➡️ Što dalje

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane čovjeka. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja mogu proizaći iz korištenja ovog prijevoda.