<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-23T00:08:55+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "hr"
}
-->
# Odjeljak 3: Microsoft Olive Optimization Suite

## Sadržaj
1. [Uvod](../../../Module04)
2. [Što je Microsoft Olive?](../../../Module04)
3. [Instalacija](../../../Module04)
4. [Vodič za brzi početak](../../../Module04)
5. [Primjer: Pretvaranje Qwen3 u ONNX INT4](../../../Module04)
6. [Napredno korištenje](../../../Module04)
7. [Najbolje prakse](../../../Module04)
8. [Rješavanje problema](../../../Module04)
9. [Dodatni resursi](../../../Module04)

## Uvod

Microsoft Olive je moćan i jednostavan alat za optimizaciju modela koji je svjestan hardvera. Olakšava proces optimizacije modela strojnog učenja za implementaciju na različitim hardverskim platformama. Bez obzira ciljate li na CPU, GPU ili specijalizirane AI akceleratore, Olive vam pomaže postići optimalne performanse uz očuvanje točnosti modela.

## Što je Microsoft Olive?

Olive je jednostavan alat za optimizaciju modela koji je svjestan hardvera i objedinjuje vodeće tehnike u industriji za kompresiju, optimizaciju i kompilaciju modela. Radi s ONNX Runtime kao E2E rješenjem za optimizaciju inferencije.

### Ključne značajke

- **Optimizacija svjesna hardvera**: Automatski odabire najbolje tehnike optimizacije za ciljani hardver
- **40+ ugrađenih komponenti za optimizaciju**: Pokriva kompresiju modela, kvantizaciju, optimizaciju grafa i više
- **Jednostavno CLI sučelje**: Jednostavne naredbe za uobičajene zadatke optimizacije
- **Podrška za više okvira**: Radi s PyTorch, Hugging Face modelima i ONNX
- **Podrška za popularne modele**: Olive automatski optimizira popularne arhitekture modela poput Llama, Phi, Qwen, Gemma itd.

### Prednosti

- **Smanjeno vrijeme razvoja**: Nema potrebe za ručnim eksperimentiranjem s različitim tehnikama optimizacije
- **Poboljšane performanse**: Značajna ubrzanja (do 6x u nekim slučajevima)
- **Implementacija na više platformi**: Optimizirani modeli rade na različitim hardverima i operativnim sustavima
- **Očuvana točnost**: Optimizacije čuvaju kvalitetu modela uz poboljšanje performansi

## Instalacija

### Preduvjeti

- Python 3.8 ili noviji
- pip upravitelj paketa
- Virtualno okruženje (preporučeno)

### Osnovna instalacija

Kreirajte i aktivirajte virtualno okruženje:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instalirajte Olive s funkcijama automatske optimizacije:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcionalne ovisnosti

Olive nudi razne opcionalne ovisnosti za dodatne značajke:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Provjera instalacije

```bash
olive --help
```

Ako je uspješno, trebali biste vidjeti poruku pomoći za Olive CLI.

## Vodič za brzi početak

### Vaša prva optimizacija

Optimizirajmo mali jezični model koristeći Oliveovu funkciju automatske optimizacije:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Što ova naredba radi

Proces optimizacije uključuje: preuzimanje modela iz lokalne predmemorije, hvatanje ONNX grafa i spremanje težina u ONNX datoteku, optimizaciju ONNX grafa i kvantizaciju modela na int4 koristeći RTN metodu.

### Objašnjenje parametara naredbe

- `--model_name_or_path`: Hugging Face identifikator modela ili lokalna putanja
- `--output_path`: Direktorij u kojem će optimizirani model biti spremljen
- `--device`: Ciljani uređaj (cpu, gpu)
- `--provider`: Izvršni pružatelj (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Koristi ONNX Runtime Generate AI za inferenciju
- `--precision`: Kvantizacijska preciznost (int4, int8, fp16)
- `--log_level`: Razina detaljnosti zapisivanja (0=minimalno, 1=detaljno)

## Primjer: Pretvaranje Qwen3 u ONNX INT4

Na temelju pruženog Hugging Face primjera na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), evo kako optimizirati Qwen3 model:

### Korak 1: Preuzimanje modela (opcionalno)

Kako biste smanjili vrijeme preuzimanja, predmemorirajte samo ključne datoteke:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Korak 2: Optimizacija Qwen3 modela

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Korak 3: Testiranje optimiziranog modela

Kreirajte jednostavan Python skript za testiranje vašeg optimiziranog modela:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura izlaza

Nakon optimizacije, vaš izlazni direktorij sadržavat će:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Napredno korištenje

### Konfiguracijske datoteke

Za složenije tijekove optimizacije možete koristiti JSON konfiguracijske datoteke:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Pokrenite s konfiguracijom:

```bash
olive run --config config.json
```

### Optimizacija za GPU

Za CUDA GPU optimizaciju:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Za DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fino podešavanje s Olive

Olive također podržava fino podešavanje modela:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najbolje prakse

### 1. Odabir modela
- Započnite s manjim modelima za testiranje (npr. 0.5B-7B parametara)
- Provjerite je li arhitektura ciljanog modela podržana od strane Olive

### 2. Razmatranje hardvera
- Prilagodite cilj optimizacije svom hardveru za implementaciju
- Koristite GPU optimizaciju ako imate CUDA-kompatibilan hardver
- Razmotrite DirectML za Windows računala s integriranom grafikom

### 3. Odabir preciznosti
- **INT4**: Maksimalna kompresija, blagi gubitak točnosti
- **INT8**: Dobar balans veličine i točnosti
- **FP16**: Minimalan gubitak točnosti, umjereno smanjenje veličine

### 4. Testiranje i validacija
- Uvijek testirajte optimizirane modele s vašim specifičnim slučajevima korištenja
- Usporedite metrike performansi (kašnjenje, propusnost, točnost)
- Koristite reprezentativne ulazne podatke za evaluaciju

### 5. Iterativna optimizacija
- Započnite s automatskom optimizacijom za brze rezultate
- Koristite konfiguracijske datoteke za preciznu kontrolu
- Eksperimentirajte s različitim prolazima optimizacije

## Rješavanje problema

### Uobičajeni problemi

#### 1. Problemi s instalacijom
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemi s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemi s memorijom
- Koristite manje veličine serija tijekom optimizacije
- Pokušajte kvantizaciju s višom preciznošću prvo (int8 umjesto int4)
- Osigurajte dovoljno prostora na disku za predmemoriranje modela

#### 4. Pogreške pri učitavanju modela
- Provjerite putanju modela i dozvole za pristup
- Provjerite zahtijeva li model `trust_remote_code=True`
- Osigurajte da su sve potrebne datoteke modela preuzete

### Dobivanje pomoći

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Primjeri**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Dodatni resursi

### Službeni linkovi
- **GitHub repozitorij**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face primjer**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Primjeri iz zajednice
- **Jupyter bilježnice**: Dostupne u Olive GitHub repozitoriju — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code ekstenzija**: Pregled AI Toolkit za VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog postovi**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Povezani alati
- **ONNX Runtime**: Visokoučinkoviti motor za inferenciju — https://onnxruntime.ai/
- **Hugging Face Transformers**: Izvor mnogih kompatibilnih modela — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Optimizacijski tijekovi temeljeni na oblaku — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Što dalje

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

