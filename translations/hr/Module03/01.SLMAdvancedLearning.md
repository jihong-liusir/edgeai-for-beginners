<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-19T01:38:54+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "hr"
}
-->
# Odjeljak 1: Napredno učenje SLM-a - Osnove i optimizacija

Mali jezični modeli (SLM) predstavljaju ključni napredak u EdgeAI-u, omogućujući sofisticirane sposobnosti obrade prirodnog jezika na uređajima s ograničenim resursima. Razumijevanje kako učinkovito implementirati, optimizirati i koristiti SLM-ove ključno je za izgradnju praktičnih AI rješenja temeljenih na rubnim uređajima.

## Uvod

U ovoj lekciji istražit ćemo male jezične modele (SLM) i njihove napredne strategije implementacije. Pokrit ćemo osnovne koncepte SLM-ova, njihove parametarske granice i klasifikacije, tehnike optimizacije te praktične strategije implementacije za okruženja rubnog računalstva.

## Ciljevi učenja

Na kraju ove lekcije moći ćete:

- 🔢 Razumjeti parametarske granice i klasifikacije malih jezičnih modela.
- 🛠️ Identificirati ključne tehnike optimizacije za implementaciju SLM-ova na rubnim uređajima.
- 🚀 Naučiti primijeniti napredne strategije kvantizacije i kompresije za SLM-ove.

## Razumijevanje parametarskih granica i klasifikacija SLM-ova

Mali jezični modeli (SLM) su AI modeli dizajnirani za obradu, razumijevanje i generiranje sadržaja prirodnog jezika s značajno manje parametara u usporedbi s njihovim velikim pandanima. Dok veliki jezični modeli (LLM) sadrže stotine milijardi do trilijuna parametara, SLM-ovi su posebno dizajnirani za učinkovitost i implementaciju na rubnim uređajima.

Okvir klasifikacije parametara pomaže nam razumjeti različite kategorije SLM-ova i njihove odgovarajuće slučajeve upotrebe. Ova klasifikacija je ključna za odabir pravog modela za specifične scenarije rubnog računalstva.

### Okvir klasifikacije parametara

Razumijevanje parametarskih granica pomaže u odabiru odgovarajućih modela za različite scenarije rubnog računalstva:

- **🔬 Mikro SLM-ovi**: 100M - 1.4B parametara (ultra-lagani za mobilne uređaje)
- **📱 Mali SLM-ovi**: 1.5B - 13.9B parametara (uravnotežene performanse i učinkovitost)
- **⚖️ Srednji SLM-ovi**: 14B - 30B parametara (približavanje sposobnostima LLM-a uz održavanje učinkovitosti)

Točna granica ostaje fluidna u istraživačkoj zajednici, ali većina praktičara smatra modele s manje od 30 milijardi parametara "malima", dok neki izvori postavljaju prag čak niže, na 10 milijardi parametara.

### Ključne prednosti SLM-ova

SLM-ovi nude nekoliko temeljnih prednosti koje ih čine idealnima za aplikacije rubnog računalstva:

**Operativna učinkovitost**: SLM-ovi omogućuju brže vrijeme izvođenja zbog manjeg broja parametara za obradu, što ih čini idealnima za aplikacije u stvarnom vremenu. Zahtijevaju manje računalnih resursa, omogućujući implementaciju na uređajima s ograničenim resursima uz manju potrošnju energije i smanjen ugljični otisak.

**Fleksibilnost implementacije**: Ovi modeli omogućuju AI sposobnosti na uređaju bez potrebe za internetskom povezanošću, poboljšavaju privatnost i sigurnost lokalnom obradom, mogu se prilagoditi za aplikacije specifične za određene domene i prikladni su za različita okruženja rubnog računalstva.

**Isplativost**: SLM-ovi nude isplativu obuku i implementaciju u usporedbi s LLM-ovima, uz smanjene operativne troškove i niže zahtjeve za propusnost za aplikacije na rubu.

## Napredne strategije stjecanja modela

### Ekosustav Hugging Face

Hugging Face služi kao primarno središte za otkrivanje i pristup najnaprednijim SLM-ovima. Platforma pruža sveobuhvatne resurse za otkrivanje i implementaciju modela:

**Značajke otkrivanja modela**: Platforma nudi napredno filtriranje prema broju parametara, vrsti licence i metrikama performansi. Korisnici mogu pristupiti alatima za usporedbu modela, rezultatima evaluacije u stvarnom vremenu i WebGPU demonstracijama za trenutna testiranja.

**Kolekcije SLM-ova**: Popularni modeli uključuju Phi-4-mini-3.8B za zadatke naprednog zaključivanja, seriju Qwen3 (0.6B/1.7B/4B) za višejezične aplikacije, Google Gemma3 za učinkovite zadatke opće namjene te eksperimentalne modele poput BitNET-a za ultra-nisku preciznost implementacije. Platforma također sadrži kolekcije vođene zajednicom sa specijaliziranim modelima za specifične domene te unaprijed obučene i varijante prilagođene instrukcijama optimizirane za različite slučajeve upotrebe.

### Katalog modela Azure AI Foundry

Katalog modela Azure AI Foundry pruža pristup SLM-ovima na razini poduzeća s poboljšanim mogućnostima integracije:

**Integracija za poduzeća**: Katalog uključuje modele koje Azure prodaje izravno s podrškom na razini poduzeća i SLA-ovima, uključujući Phi-4-mini-3.8B za napredne sposobnosti zaključivanja i Llama 3-8B za produkcijsku implementaciju. Također sadrži modele poput Qwen3 8B od pouzdanih trećih strana otvorenog koda.

**Prednosti za poduzeća**: Ugrađeni alati za fino podešavanje, promatranje i odgovorni AI integrirani su s fleksibilnim Provisioned Throughputom među obiteljima modela. Izravna Microsoftova podrška s SLA-ovima na razini poduzeća, integrirane značajke sigurnosti i usklađenosti te sveobuhvatni tijekovi implementacije poboljšavaju iskustvo poduzeća.

## Napredne tehnike kvantizacije i optimizacije

### Llama.cpp okvir za optimizaciju

Llama.cpp pruža najnaprednije tehnike kvantizacije za maksimalnu učinkovitost u implementaciji na rubu:

**Metode kvantizacije**: Okvir podržava različite razine kvantizacije, uključujući Q4_0 (4-bitna kvantizacija s izvrsnim smanjenjem veličine - idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 (5-bitna kvantizacija koja balansira kvalitetu i kompresiju - prikladno za Phi-4-mini-3.8B rubnu inferenciju) i Q8_0 (8-bitna kvantizacija za gotovo originalnu kvalitetu - preporučeno za produkcijsku upotrebu Google Gemma3). BitNET predstavlja vrhunac s 1-bitnom kvantizacijom za ekstremne scenarije kompresije.

**Prednosti implementacije**: Optimizirana inferencija za CPU s SIMD ubrzanjem omogućuje učinkovito učitavanje i izvođenje modela. Kompatibilnost na više platformi, uključujući x86, ARM i Apple Silicon arhitekture, omogućuje hardverski neovisne mogućnosti implementacije.

**Praktični primjer implementacije**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Usporedba memorijskog otiska**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Suite za optimizaciju

Microsoft Olive nudi sveobuhvatne tijekove optimizacije modela dizajnirane za produkcijska okruženja:

**Tehnike optimizacije**: Suite uključuje dinamičku kvantizaciju za automatski odabir preciznosti (posebno učinkovito s modelima serije Qwen3), optimizaciju grafa i fuziju operatora (optimizirano za arhitekturu Google Gemma3), hardverske specifične optimizacije za CPU, GPU i NPU (s posebnom podrškom za Phi-4-mini-3.8B na ARM uređajima) te višestupanjske tijekove optimizacije. BitNET modeli zahtijevaju specijalizirane tijekove rada za 1-bitnu kvantizaciju unutar Olive okvira.

**Automatizacija tijeka rada**: Automatizirano testiranje među varijantama optimizacije osigurava očuvanje kvalitete metrika tijekom optimizacije. Integracija s popularnim ML okvirima poput PyTorcha i ONNX-a pruža optimizaciju za implementaciju u oblaku i na rubu.

**Praktični primjer implementacije**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX okvir

Apple MLX pruža nativnu optimizaciju posebno dizajniranu za Apple Silicon uređaje:

**Optimizacija za Apple Silicon**: Okvir koristi arhitekturu ujedinjene memorije s integracijom Metal Performance Shaders, automatsku inferenciju mješovite preciznosti (posebno učinkovito s Google Gemma3) i optimiziranu iskorištenost memorijske širine. Phi-4-mini-3.8B pokazuje iznimne performanse na M-seriji čipova, dok Qwen3-1.7B pruža optimalnu ravnotežu za implementaciju na MacBook Air uređajima.

**Značajke razvoja**: Podrška za Python i Swift API s operacijama kompatibilnim s NumPy-jem, mogućnosti automatske diferencijacije i besprijekorna integracija s Appleovim alatima za razvoj pružaju sveobuhvatno razvojno okruženje.

**Praktični primjer implementacije**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategije implementacije i inferencije u produkciji

### Ollama: Pojednostavljena lokalna implementacija

Ollama pojednostavljuje implementaciju SLM-ova s funkcijama spremnim za poduzeća u lokalnim i rubnim okruženjima:

**Mogućnosti implementacije**: Instalacija i izvođenje modela jednim naredbom s automatskim povlačenjem i predmemoriranjem modela. Podrška za Phi-4-mini-3.8B, cijelu seriju Qwen3 (0.6B/1.7B/4B) i Google Gemma3 s REST API-jem za integraciju aplikacija te mogućnostima upravljanja i prebacivanja između više modela. BitNET modeli zahtijevaju eksperimentalne konfiguracije za podršku 1-bitne kvantizacije.

**Napredne značajke**: Podrška za prilagođeno fino podešavanje modela, generiranje Dockerfile-a za implementaciju u kontejnerima, GPU ubrzanje s automatskim otkrivanjem te opcije kvantizacije i optimizacije modela pružaju sveobuhvatnu fleksibilnost implementacije.

### VLLM: Inferencija visokih performansi

VLLM omogućuje optimizaciju inferencije na razini produkcije za scenarije visokog protoka:

**Optimizacije performansi**: PagedAttention za memorijski učinkovitu obradu pažnje (posebno korisno za transformacijsku arhitekturu Phi-4-mini-3.8B), dinamičko grupiranje za optimizaciju protoka (optimizirano za paralelnu obradu serije Qwen3), paralelizam tenzora za skaliranje na više GPU-ova (podrška za Google Gemma3) i spekulativno dekodiranje za smanjenje kašnjenja. BitNET modeli zahtijevaju specijalizirane jezgre za inferenciju za 1-bitne operacije.

**Integracija za poduzeća**: API krajnje točke kompatibilne s OpenAI-jem, podrška za implementaciju na Kubernetesu, integracija za praćenje i promatranje te mogućnosti automatskog skaliranja pružaju rješenja za implementaciju na razini poduzeća.

### Foundry Local: Microsoftovo rješenje za rub

Foundry Local pruža sveobuhvatne mogućnosti implementacije na rubu za okruženja poduzeća:

**Značajke rubnog računalstva**: Dizajn arhitekture s prioritetom offline rada uz optimizaciju za ograničene resurse, upravljanje lokalnim registrima modela i mogućnosti sinkronizacije između ruba i oblaka osiguravaju pouzdanu implementaciju na rubu.

**Sigurnost i usklađenost**: Lokalna obrada podataka za očuvanje privatnosti, kontrole sigurnosti na razini poduzeća, zapisivanje revizija i izvještavanje o usklađenosti te upravljanje pristupom na temelju uloga pružaju sveobuhvatnu sigurnost za implementacije na rubu.

## Najbolje prakse za implementaciju SLM-ova

### Smjernice za odabir modela

Pri odabiru SLM-ova za implementaciju na rubu, razmotrite sljedeće faktore:

**Razmatranja o broju parametara**: Odaberite mikro SLM-ove poput Qwen3-0.6B za ultra-lagane mobilne aplikacije, male SLM-ove poput Qwen3-1.7B ili Google Gemma3 za scenarije uravnoteženih performansi te srednje SLM-ove poput Phi-4-mini-3.8B ili Qwen3-4B kada se približavate sposobnostima LLM-a uz održavanje učinkovitosti. BitNET modeli nude eksperimentalnu ultra-kompresiju za specifične istraživačke aplikacije.

**Usklađenost s slučajem upotrebe**: Uskladite sposobnosti modela sa specifičnim zahtjevima aplikacije, uzimajući u obzir faktore poput kvalitete odgovora, brzine inferencije, ograničenja memorije i zahtjeva za offline radom.

### Odabir strategije optimizacije

**Pristup kvantizaciji**: Odaberite odgovarajuće razine kvantizacije na temelju zahtjeva za kvalitetom i hardverskih ograničenja. Razmotrite Q4_0 za maksimalnu kompresiju (idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 za uravnotežen omjer kvalitete i kompresije (prikladno za Phi-4-mini-3.8B i Google Gemma3) te Q8_0 za očuvanje gotovo originalne kvalitete (preporučeno za produkcijska okruženja Qwen3-4B). BitNET-ova 1-bitna kvantizacija predstavlja ekstremnu granicu kompresije za specijalizirane aplikacije.

**Odabir okvira**: Odaberite okvire za optimizaciju na temelju ciljanog hardvera i zahtjeva za implementacijom. Koristite Llama.cpp za optimiziranu implementaciju na CPU-u, Microsoft Olive za sveobuhvatne tijekove optimizacije i Apple MLX za Apple Silicon uređaje.

## Praktični primjeri modela i slučajevi upotrebe

### Scenariji implementacije u stvarnom svijetu

**Mobilne aplikacije**: Qwen3-0.6B izvrsno se snalazi u aplikacijama za chatbotove na pametnim telefonima s minimalnim memorijskim otiskom, dok Google Gemma3 pruža uravnotežene performanse za obrazovne alate na tabletima. Phi-4-mini-3.8B nudi vrhunske sposobnosti zaključivanja za mobilne aplikacije produktivnosti.

**Desktop i rubno računalstvo**: Qwen3-1.7B pruža optimalne performanse za aplikacije pomoćnika na desktopu, Phi-4-mini-3.8B omogućuje napredne sposobnosti generiranja koda za alate za razvojne programere, a Qwen3-4B omogućuje sofisticiranu analizu dokumenata na radnim stanicama.

**Istraživanje i eksperimentiranje**: BitNET modeli omogućuju istraživanje ultra-niske preciznosti inferencije za akademska istraživanja i aplikacije dokazivanja koncepta koje zahtijevaju ekstremna ograničenja resursa.

### Benchmarking performansi i usporedbe

**Brzina inferencije**: Qwen3-0.6B postiže najbrže vrijeme inferencije na mobilnim CPU-ima, Google Gemma3 pruža uravnotežen omjer brzine i kvalitete za opće aplikacije, Phi-4-mini-3.8B nudi vrhunsku brzinu zaključivanja za složene zadatke, a BitNET pruža teorijski maksimalan protok uz specijalizirani hardver.

**Zahtjevi za memorijom**: Memorijski otisci modela kreću se od Qwen3-0.6B (manje od 1GB kvantizirano) do Phi-4-mini-3.8B (otprilike 3-4GB kvantizirano), dok BitNET postiže otiske ispod 500MB u eksperimentalnim konfiguracijama.

## Izazovi i razmatranja

### Kompromisi performansi

Implementacija SLM-ova uključuje pažljivo razmatranje kompromisa između veličine modela, brzine inferencije i kvalitete izlaza. Na primjer, dok Qwen3-0.6B nudi iznimnu brzinu i učinkovitost, Phi-4-mini-3.8B pruža vrhunske sposobnosti zaključivanja uz povećane zahtjeve za resursima. Google Gemma3 predstavlja sredinu prikladnu za većinu općih aplikacija.

### Kompatibilnost hardvera

Različiti rubni

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritične informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.