<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T20:54:19+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "hr"
}
-->
# Poglavlje 1: Napredno učenje o SLM-ovima - Osnove i optimizacija

Mali jezični modeli (SLM-ovi) predstavljaju ključni napredak u EdgeAI-u, omogućujući sofisticirane sposobnosti obrade prirodnog jezika na uređajima s ograničenim resursima. Razumijevanje kako učinkovito implementirati, optimizirati i koristiti SLM-ove ključno je za izgradnju praktičnih AI rješenja temeljenih na rubnim uređajima.

## Uvod

U ovoj lekciji istražit ćemo male jezične modele (SLM-ove) i njihove napredne strategije implementacije. Pokrit ćemo osnovne koncepte SLM-ova, njihove granice parametara i klasifikacije, tehnike optimizacije te praktične strategije implementacije za okruženja rubnog računalstva.

## Ciljevi učenja

Na kraju ove lekcije moći ćete:

- 🔢 Razumjeti granice parametara i klasifikacije malih jezičnih modela.
- 🛠️ Identificirati ključne tehnike optimizacije za implementaciju SLM-ova na rubnim uređajima.
- 🚀 Naučiti primijeniti napredne strategije kvantizacije i kompresije za SLM-ove.

## Razumijevanje granica parametara i klasifikacije SLM-ova

Mali jezični modeli (SLM-ovi) su AI modeli dizajnirani za obradu, razumijevanje i generiranje sadržaja prirodnog jezika s mnogo manje parametara u usporedbi s njihovim velikim pandanima. Dok veliki jezični modeli (LLM-ovi) sadrže stotine milijardi do trilijuna parametara, SLM-ovi su posebno dizajnirani za učinkovitost i implementaciju na rubnim uređajima.

Okvir klasifikacije parametara pomaže nam razumjeti različite kategorije SLM-ova i njihove odgovarajuće primjene. Ova klasifikacija ključna je za odabir pravog modela za specifične scenarije rubnog računalstva.

### Okvir klasifikacije parametara

Razumijevanje granica parametara pomaže u odabiru odgovarajućih modela za različite scenarije rubnog računalstva:

- **🔬 Mikro SLM-ovi**: 100M - 1,4B parametara (ultralagani za mobilne uređaje)
- **📱 Mali SLM-ovi**: 1,5B - 13,9B parametara (uravnotežena izvedba i učinkovitost)
- **⚖️ Srednji SLM-ovi**: 14B - 30B parametara (približavanje sposobnostima LLM-ova uz održavanje učinkovitosti)

Točna granica ostaje fluidna u istraživačkoj zajednici, ali većina praktičara smatra modele s manje od 30 milijardi parametara "malima", dok neki izvori postavljaju prag čak i niže, na 10 milijardi parametara.

### Ključne prednosti SLM-ova

SLM-ovi nude nekoliko temeljnih prednosti koje ih čine idealnima za primjene u rubnom računalstvu:

**Operativna učinkovitost**: SLM-ovi omogućuju brže vrijeme izvođenja zbog manjeg broja parametara za obradu, što ih čini idealnima za aplikacije u stvarnom vremenu. Zahtijevaju manje računalnih resursa, omogućujući implementaciju na uređajima s ograničenim resursima uz manju potrošnju energije i smanjenje ugljičnog otiska.

**Fleksibilnost implementacije**: Ovi modeli omogućuju AI sposobnosti na uređaju bez potrebe za internetskom povezanošću, poboljšavaju privatnost i sigurnost lokalnom obradom, mogu se prilagoditi za aplikacije specifične za određene domene i prikladni su za različita okruženja rubnog računalstva.

**Isplativost**: SLM-ovi nude isplativije treniranje i implementaciju u usporedbi s LLM-ovima, uz smanjene operativne troškove i niže zahtjeve za propusnost za rubne aplikacije.

## Napredne strategije nabave modela

### Ekosustav Hugging Face

Hugging Face služi kao primarno središte za otkrivanje i pristup najnaprednijim SLM-ovima. Platforma pruža sveobuhvatne resurse za otkrivanje i implementaciju modela:

**Značajke otkrivanja modela**: Platforma nudi napredno filtriranje prema broju parametara, vrsti licence i metrikama izvedbe. Korisnici mogu pristupiti alatima za usporedbu modela, rezultatima evaluacije u stvarnom vremenu i WebGPU demonstracijama za neposredno testiranje.

**Kolekcije SLM-ova**: Popularni modeli uključuju Phi-4-mini-3.8B za zadatke naprednog zaključivanja, seriju Qwen3 (0.6B/1.7B/4B) za višejezične aplikacije, Google Gemma3 za učinkovite zadatke opće namjene i eksperimentalne modele poput BitNET-a za implementaciju s ultra-niskom preciznošću. Platforma također sadrži kolekcije koje su kreirali članovi zajednice sa specijaliziranim modelima za određene domene te unaprijed trenirane i varijante prilagođene instrukcijama optimizirane za različite primjene.

### Katalog modela Azure AI Foundry

Katalog modela Azure AI Foundry pruža pristup SLM-ovima na razini poduzeća s poboljšanim mogućnostima integracije:

**Integracija za poduzeća**: Katalog uključuje modele koje Azure izravno prodaje s podrškom na razini poduzeća i SLA-ovima, uključujući Phi-4-mini-3.8B za napredne sposobnosti zaključivanja i Llama 3-8B za implementaciju u produkciji. Također uključuje modele poput Qwen3 8B od pouzdanih trećih strana otvorenog koda.

**Prednosti za poduzeća**: Ugrađeni alati za fino podešavanje, promatranje i odgovorni AI integrirani su s fleksibilnim Provisioned Throughputom među obiteljima modela. Izravna Microsoftova podrška s SLA-ovima na razini poduzeća, integrirane značajke sigurnosti i usklađenosti te sveobuhvatni tijekovi implementacije poboljšavaju iskustvo za poduzeća.

## Napredne tehnike kvantizacije i optimizacije

### Okvir za optimizaciju Llama.cpp

Llama.cpp pruža najnovije tehnike kvantizacije za maksimalnu učinkovitost u implementaciji na rubnim uređajima:

**Metode kvantizacije**: Okvir podržava različite razine kvantizacije, uključujući Q4_0 (4-bitna kvantizacija s izvrsnim smanjenjem veličine - idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 (5-bitna kvantizacija koja balansira kvalitetu i kompresiju - prikladno za Phi-4-mini-3.8B za rubnu inferenciju) i Q8_0 (8-bitna kvantizacija za gotovo originalnu kvalitetu - preporučeno za Google Gemma3 u produkciji). BitNET predstavlja vrhunac s 1-bitnom kvantizacijom za scenarije ekstremne kompresije.

**Prednosti implementacije**: Inferencija optimizirana za CPU s SIMD ubrzanjem omogućuje učinkovito učitavanje i izvođenje modela. Kompatibilnost na više platformi, uključujući x86, ARM i Apple Silicon arhitekture, omogućuje hardverski neovisne mogućnosti implementacije.

**Praktični primjer implementacije**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Usporedba memorijskog otiska**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Suite za optimizaciju

Microsoft Olive nudi sveobuhvatne tijekove optimizacije modela dizajnirane za produkcijska okruženja:

**Tehnike optimizacije**: Suite uključuje dinamičku kvantizaciju za automatski odabir preciznosti (posebno učinkovito za modele serije Qwen3), optimizaciju grafova i fuziju operatora (optimizirano za arhitekturu Google Gemma3), optimizacije specifične za hardver za CPU, GPU i NPU (s posebnom podrškom za Phi-4-mini-3.8B na ARM uređajima) te višestupanjske tijekove optimizacije. BitNET modeli zahtijevaju specijalizirane tijekove kvantizacije s 1-bitnom preciznošću unutar Olive okvira.

**Automatizacija tijekova rada**: Automatizirano testiranje među varijantama optimizacije osigurava očuvanje kvalitativnih metrika tijekom optimizacije. Integracija s popularnim ML okvirima poput PyTorcha i ONNX-a pruža mogućnosti optimizacije za oblak i rubne uređaje.

**Praktični primjer implementacije**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX Framework

Apple MLX pruža nativnu optimizaciju posebno dizajniranu za uređaje s Apple Silicon čipovima:

**Optimizacija za Apple Silicon**: Okvir koristi arhitekturu unificirane memorije s integracijom Metal Performance Shaders, automatsku inferenciju s mješovitom preciznošću (posebno učinkovito za Google Gemma3) i optimizirano korištenje memorijske širine. Phi-4-mini-3.8B pokazuje iznimne performanse na M-seriji čipova, dok Qwen3-1.7B pruža optimalnu ravnotežu za implementaciju na MacBook Air uređajima.

**Značajke razvoja**: Podrška za Python i Swift API-je s operacijama kompatibilnim s NumPy-jem, mogućnosti automatske diferencijacije i besprijekorna integracija s Appleovim alatima za razvoj pružaju sveobuhvatno razvojno okruženje.

**Praktični primjer implementacije**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Strategije implementacije i inferencije u produkciji

### Ollama: Pojednostavljena lokalna implementacija

Ollama pojednostavljuje implementaciju SLM-ova s funkcijama spremnim za poduzeća u lokalnim i rubnim okruženjima:

**Mogućnosti implementacije**: Instalacija i izvođenje modela jednim naredbom s automatskim povlačenjem i predmemoriranjem modela. Podrška za Phi-4-mini-3.8B, cijelu seriju Qwen3 (0.6B/1.7B/4B) i Google Gemma3 s REST API-jem za integraciju aplikacija te mogućnost upravljanja i prebacivanja između više modela. BitNET modeli zahtijevaju eksperimentalne konfiguracije za podršku kvantizacije s 1-bitnom preciznošću.

**Napredne značajke**: Podrška za prilagodbu modela, generiranje Dockerfile-a za implementaciju u kontejnerima, ubrzanje putem GPU-a s automatskim otkrivanjem te opcije kvantizacije i optimizacije modela pružaju sveobuhvatnu fleksibilnost implementacije.

### VLLM: Inferencija visokih performansi

VLLM omogućuje optimizaciju inferencije na razini produkcije za scenarije visokog protoka:

**Optimizacije performansi**: PagedAttention za memorijski učinkovitu obradu pažnje (posebno korisno za transformacijsku arhitekturu Phi-4-mini-3.8B), dinamičko grupiranje za optimizaciju protoka (optimizirano za paralelnu obradu serije Qwen3), paralelizam tenzora za skaliranje na više GPU-a (podrška za Google Gemma3) i spekulativno dekodiranje za smanjenje kašnjenja. BitNET modeli zahtijevaju specijalizirane jezgre za inferenciju za operacije s 1-bitnom preciznošću.

**Integracija za poduzeća**: API krajnje točke kompatibilne s OpenAI-jem, podrška za implementaciju na Kubernetesu, integracija za praćenje i promatranje te mogućnosti automatskog skaliranja pružaju rješenja za implementaciju na razini poduzeća.

### Foundry Local: Microsoftovo rješenje za rubne uređaje

Foundry Local pruža sveobuhvatne mogućnosti implementacije na rubnim uređajima za okruženja poduzeća:

**Značajke rubnog računalstva**: Dizajn arhitekture s prioritetom offline rada uz optimizaciju za ograničene resurse, upravljanje lokalnim registrima modela i mogućnosti sinkronizacije između rubnih uređaja i oblaka osiguravaju pouzdanu implementaciju na rubnim uređajima.

**Sigurnost i usklađenost**: Lokalna obrada podataka za očuvanje privatnosti, kontrole sigurnosti na razini poduzeća, zapisivanje audita i izvještavanje o usklađenosti te upravljanje pristupom na temelju uloga pružaju sveobuhvatnu sigurnost za implementacije na rubnim uređajima.

## Najbolje prakse za implementaciju SLM-ova

### Smjernice za odabir modela

Pri odabiru SLM-ova za implementaciju na rubnim uređajima, razmotrite sljedeće faktore:

**Razmatranja o broju parametara**: Odaberite mikro SLM-ove poput Qwen3-0.6B za ultralagane mobilne aplikacije, male SLM-ove poput Qwen3-1.7B ili Google Gemma3 za scenarije uravnotežene izvedbe te srednje SLM-ove poput Phi-4-mini-3.8B ili Qwen3-4B kada se približavate sposobnostima LLM-ova uz održavanje učinkovitosti. BitNET modeli nude eksperimentalnu ultra-kompresiju za specifične istraživačke primjene.

**Usklađenost s primjenom**: Uskladite sposobnosti modela sa specifičnim zahtjevima aplikacije, uzimajući u obzir faktore poput kvalitete odgovora, brzine inferencije, ograničenja memorije i zahtjeva za offline radom.

### Odabir strategije optimizacije

**Pristup kvantizaciji**: Odaberite odgovarajuće razine kvantizacije na temelju zahtjeva za kvalitetom i hardverskih ograničenja. Razmotrite Q4_0 za maksimalnu kompresiju (idealno za mobilnu implementaciju Qwen3-0.6B), Q5_1 za uravnotežen odnos kvalitete i kompresije (prikladno za Phi-4-mini-3.8B i Google Gemma3) te Q8_0 za očuvanje gotovo originalne kvalitete (preporučeno za Qwen3-4B u produkcijskim okruženjima). BitNET-ova 1-bitna kvantizacija predstavlja ekstremnu granicu kompresije za specijalizirane primjene.

**Odabir okvira**: Odaberite okvire za optimizaciju na temelju ciljanog hardvera i zahtjeva za implementaciju. Koristite Llama.cpp za implementaciju optimiziranu za CPU, Microsoft Olive za sveobuhvatne tijekove optimizacije i Apple MLX za uređaje s Apple Silicon čipovima.

## Praktični primjeri modela i primjene

### Scenariji implementacije u stvarnom svijetu

**Mobilne aplikacije**: Qwen3-0.6B izvrsno se snalazi u aplikacijama za chatbotove na pametnim telefonima s minimalnim memorijskim otiskom, dok Google Gemma3 pruža uravnoteženu izvedbu za obrazovne alate na tabletima. Phi-4-mini-3.8B nudi vrhunske sposobnosti zaključivanja za aplikacije produktivnosti na mobilnim uređajima.

**Desktop i rubno računalstvo**: Qwen3-1.7B pruža optimalnu izvedbu za aplikacije pomoćnika na stolnim računalima, Phi-4-mini-3.8B omogućuje napredne sposobnosti generiranja koda za alate za razvojne programere, a Qwen3-4B omogućuje sofisticiranu analizu dokumenata na radnim stanicama.

**Istraživanje i eksperimentalne primjene**: BitNET modeli omogućuju istraživanje inferencije s ultra-niskom preciznošću za akademska istraživanja i aplikacije dokazivanja koncepta koje zahtijevaju ekstremna ograničenja resursa.

### Usporedbe performansi i mjerila

**Brzina inferencije**: Qwen3-0.6B postiže najbrže vrijeme inferencije na mobilnim CPU-ima, Google Gemma3 pruža uravnotežen omjer brzine i kvalitete za opće aplikacije, Phi-4-mini-3.8B nudi vrhunsku brzinu zaključivanja za složene zadatke, a BitNET postiže teorijski maksimalan protok uz specijalizirani hardver.

**Zahtjevi za memorijom**: Memorijski otisci modela kreću se od Qwen3-0.6B (ispod 1GB kvantiziran) do Phi-4-mini-3.8B (otprilike 3-4GB kvantiziran), dok BitNET postiže otiske ispod 500MB u eksperimentalnim konfiguracijama.

## Izazovi i razmatranja

### Kompromisi u performansama

Implementacija SLM-ova uključuje pažljivo razmatranje kompromisa između veličine modela, brzine inferencije i kvalitete izlaza. Na primjer, dok Qwen3-0.6B nudi iznimnu brzinu i učinkovitost, Phi-4-mini-3.8B pruža vrhuns

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja mogu proizaći iz korištenja ovog prijevoda.