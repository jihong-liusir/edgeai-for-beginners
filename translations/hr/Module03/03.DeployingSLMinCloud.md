<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-19T01:44:33+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "hr"
}
-->
# Kontejnersko Oblak Rješenje - Produkcijska Razina

Ovaj sveobuhvatni vodič pokriva tri glavna pristupa za implementaciju Microsoftovog Phi-4-mini-instruct modela u kontejnerskim okruženjima: vLLM, Ollama i SLM Engine s ONNX Runtime. Ovaj model s 3.8 milijardi parametara predstavlja optimalan izbor za zadatke zaključivanja uz održavanje učinkovitosti za implementaciju na rubnim uređajima.

## Sadržaj

1. [Uvod u Phi-4-mini kontejnersku implementaciju](../../../Module03)
2. [Ciljevi učenja](../../../Module03)
3. [Razumijevanje Phi-4-mini klasifikacije](../../../Module03)
4. [vLLM kontejnerska implementacija](../../../Module03)
5. [Ollama kontejnerska implementacija](../../../Module03)
6. [SLM Engine s ONNX Runtime](../../../Module03)
7. [Okvir za usporedbu](../../../Module03)
8. [Najbolje prakse](../../../Module03)

## Uvod u Phi-4-mini kontejnersku implementaciju

Mali jezični modeli (SLM) predstavljaju ključni napredak u EdgeAI tehnologiji, omogućujući sofisticirane sposobnosti obrade prirodnog jezika na uređajima s ograničenim resursima. Ovaj vodič fokusira se na strategije kontejnerske implementacije za Microsoftov Phi-4-mini-instruct, vrhunski model za zaključivanje koji balansira sposobnosti i učinkovitost.

### Istaknuti model: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 milijardi parametara)**: Najnoviji Microsoftov lagani model prilagođen za upute, dizajniran za okruženja s ograničenom memorijom i računalnim resursima, s izvanrednim sposobnostima u:
- **Matematičkom zaključivanju i složenim izračunima**
- **Generiranju, otklanjanju grešaka i analizi koda**
- **Logičkom rješavanju problema i zaključivanju korak po korak**
- **Obrazovnim aplikacijama koje zahtijevaju detaljna objašnjenja**
- **Pozivanju funkcija i integraciji alata**

Dio kategorije "Mali SLM-ovi" (1.5B - 13.9B parametara), Phi-4-mini postiže optimalnu ravnotežu između sposobnosti zaključivanja i učinkovitosti resursa.

### Prednosti kontejnerske implementacije Phi-4-mini

- **Operativna učinkovitost**: Brza inferencija za zadatke zaključivanja uz niže računalne zahtjeve
- **Fleksibilnost implementacije**: AI sposobnosti na uređaju uz poboljšanu privatnost kroz lokalnu obradu
- **Isplativost**: Smanjeni operativni troškovi u usporedbi s većim modelima uz održavanje kvalitete
- **Izolacija**: Čista odvojenost između instanci modela i sigurnih okruženja za izvršavanje
- **Skalabilnost**: Jednostavno horizontalno skaliranje za povećanje kapaciteta zaključivanja

## Ciljevi učenja

Na kraju ovog vodiča, moći ćete:

- Implementirati i optimizirati Phi-4-mini-instruct u različitim kontejnerskim okruženjima
- Primijeniti napredne strategije kvantizacije i kompresije za različite scenarije implementacije
- Konfigurirati kontejnersku orkestraciju spremnu za produkciju za radna opterećenja zaključivanja
- Procijeniti i odabrati odgovarajuće okvire implementacije na temelju specifičnih zahtjeva slučaja upotrebe
- Primijeniti sigurnosne, nadzorne i skalabilne najbolje prakse za kontejnerske SLM implementacije

## Razumijevanje Phi-4-mini klasifikacije

### Specifikacije modela

**Tehnički detalji:**
- **Parametri**: 3.8 milijardi (kategorija Mali SLM)
- **Arhitektura**: Gusto dekoder-samo Transformer s grupiranim upitima
- **Duljina konteksta**: 128K tokena (32K preporučeno za optimalne performanse)
- **Rječnik**: 200K tokena s podrškom za više jezika
- **Podaci za treniranje**: 5T tokena visokokvalitetnog sadržaja bogatog zaključivanjem

### Zahtjevi resursa

| Tip implementacije | Min RAM | Preporučeni RAM | VRAM (GPU) | Pohrana | Tipični slučajevi upotrebe |
|--------------------|---------|-----------------|------------|---------|---------------------------|
| **Razvoj**         | 6GB     | 8GB             | -          | 8GB     | Lokalno testiranje, prototipiranje |
| **Produkcija CPU** | 8GB     | 12GB            | -          | 10GB    | Rubni serveri, isplativa implementacija |
| **Produkcija GPU** | 6GB     | 8GB             | 4-6GB      | 8GB     | Servisi za zaključivanje visokog kapaciteta |
| **Optimizirano za rub** | 4GB | 6GB             | -          | 6GB     | Kvantizirana implementacija, IoT pristupnici |

### Sposobnosti Phi-4-mini

- **Matematička izvrsnost**: Napredno rješavanje problema iz aritmetike, algebre i kalkulusa
- **Inteligencija koda**: Generiranje koda u Pythonu, JavaScriptu i više jezika uz otklanjanje grešaka
- **Logičko zaključivanje**: Razlaganje problema korak po korak i konstrukcija rješenja
- **Obrazovna podrška**: Detaljna objašnjenja pogodna za učenje i podučavanje
- **Pozivanje funkcija**: Izvorna podrška za integraciju alata i API interakcije

## vLLM kontejnerska implementacija

vLLM pruža izvrsnu podršku za Phi-4-mini-instruct s optimiziranim performansama inferencije i OpenAI-kompatibilnim API-ima, što ga čini idealnim za produkcijske servise zaključivanja.

### Primjeri brzog početka

#### Osnovna CPU implementacija (Razvoj)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Produkcijska implementacija ubrzana GPU-om
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Produkcijska konfiguracija

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testiranje sposobnosti zaključivanja Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama kontejnerska implementacija

Ollama pruža izvrsnu podršku za Phi-4-mini-instruct uz pojednostavljenu implementaciju i upravljanje, što ga čini idealnim za razvoj i uravnotežene produkcijske implementacije.

### Brza postavka

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Produkcijska konfiguracija

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimizacija modela i varijante

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Primjeri korištenja API-ja

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine s ONNX Runtime

ONNX Runtime pruža optimalne performanse za implementaciju Phi-4-mini-instruct na rubnim uređajima uz naprednu optimizaciju i kompatibilnost između platformi.

### Osnovna postavka

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Pojednostavljena implementacija servera

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Skripta za konverziju modela

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Produkcijska konfiguracija

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testiranje ONNX implementacije

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Okvir za usporedbu

### Usporedba okvira za Phi-4-mini

| Značajka            | vLLM   | Ollama | ONNX Runtime |
|---------------------|--------|--------|--------------|
| **Kompleksnost postavke** | Umjerena | Jednostavna | Kompleksna |
| **Performanse (GPU)**      | Izvrsne (~25 tok/s) | Vrlo dobre (~20 tok/s) | Dobre (~15 tok/s) |
| **Performanse (CPU)**      | Dobre (~8 tok/s) | Vrlo dobre (~12 tok/s) | Izvrsne (~15 tok/s) |
| **Korištenje memorije**    | 8-12GB | 6-10GB | 4-8GB |
| **Kompatibilnost API-ja**  | OpenAI kompatibilan | Prilagođeni REST | Prilagođeni FastAPI |
| **Pozivanje funkcija**     | ✅ Izvorno | ✅ Podržano | ⚠️ Prilagođena implementacija |
| **Podrška za kvantizaciju**| AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX kvantizacija |
| **Spremnost za produkciju**| ✅ Izvrsna | ✅ Vrlo dobra | ✅ Dobra |
| **Implementacija na rubu** | Dobra | Izvrsna | Izvanredna |

## Dodatni resursi

### Službena dokumentacija
- **Microsoft Phi-4 Model Card**: Detaljne specifikacije i smjernice za korištenje
- **vLLM Dokumentacija**: Napredne opcije konfiguracije i optimizacije
- **Ollama Model Library**: Zajednički modeli i primjeri prilagodbe
- **ONNX Runtime Vodiči**: Strategije optimizacije performansi i implementacije

### Alati za razvoj
- **Hugging Face Transformers**: Za interakciju i prilagodbu modela
- **OpenAI API Specifikacija**: Za testiranje kompatibilnosti s vLLM
- **Docker Najbolje Prakse**: Sigurnost i optimizacija kontejnera
- **Kubernetes Implementacija**: Obrasci orkestracije za skaliranje u produkciji

### Resursi za učenje
- **SLM Benchmarking Performansi**: Metodologije za usporednu analizu
- **Implementacija Edge AI**: Najbolje prakse za okruženja s ograničenim resursima
- **Optimizacija zadataka zaključivanja**: Strategije za matematičke i logičke probleme
- **Sigurnost kontejnera**: Prakse za jačanje sigurnosti AI implementacija

## Ishodi učenja

Nakon završetka ovog modula, moći ćete:

1. Implementirati Phi-4-mini-instruct model u kontejnerskim okruženjima koristeći različite okvire
2. Konfigurirati i optimizirati SLM implementacije za različita hardverska okruženja
3. Primijeniti najbolje prakse sigurnosti za kontejnerske AI implementacije
4. Usporediti i odabrati odgovarajuće okvire implementacije na temelju specifičnih zahtjeva slučaja upotrebe
5. Primijeniti strategije nadzora i skaliranja za produkcijske SLM servise

## Što dalje

- Povratak na [Modul 1](../Module01/README.md)
- Povratak na [Modul 2](../Module02/README.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.