<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-19T01:09:54+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "hr"
}
-->
# Odjeljak 2: Destilacija modela - od teorije do prakse

## Sadržaj
1. [Uvod u destilaciju modela](../../../Module05)
2. [Zašto je destilacija važna](../../../Module05)
3. [Proces destilacije](../../../Module05)
4. [Praktična implementacija](../../../Module05)
5. [Primjer destilacije u Azure ML](../../../Module05)
6. [Najbolje prakse i optimizacija](../../../Module05)
7. [Primjene u stvarnom svijetu](../../../Module05)
8. [Zaključak](../../../Module05)

## Uvod u destilaciju modela {#introduction}

Destilacija modela je moćna tehnika koja nam omogućuje stvaranje manjih, učinkovitijih modela uz očuvanje većine performansi većih, složenijih modela. Ovaj proces uključuje treniranje kompaktnog "studentskog" modela da oponaša ponašanje većeg "učiteljskog" modela.

**Ključne prednosti:**
- **Smanjeni zahtjevi za računalnim resursima** tijekom inferencije
- **Niža potrošnja memorije** i potreba za pohranom
- **Brže vrijeme inferencije** uz održavanje zadovoljavajuće točnosti
- **Isplativa implementacija** u okruženjima s ograničenim resursima

## Zašto je destilacija važna {#why-distillation-matters}

Veliki jezični modeli (LLM) postaju sve moćniji, ali i sve zahtjevniji u pogledu resursa. Iako model s milijardama parametara može pružiti izvrsne rezultate, često nije praktičan za mnoge stvarne primjene zbog:

### Ograničenja resursa
- **Računalni troškovi**: Veliki modeli zahtijevaju značajnu GPU memoriju i procesorsku snagu
- **Kašnjenje inferencije**: Složeniji modeli trebaju više vremena za generiranje odgovora
- **Potrošnja energije**: Veći modeli troše više energije, povećavajući operativne troškove
- **Troškovi infrastrukture**: Hosting velikih modela zahtijeva skupu opremu

### Praktična ograničenja
- **Mobilna implementacija**: Veliki modeli ne mogu učinkovito raditi na mobilnim uređajima
- **Aplikacije u stvarnom vremenu**: Aplikacije koje zahtijevaju nisku latenciju ne mogu podržati sporu inferenciju
- **Edge računalstvo**: IoT i edge uređaji imaju ograničene računalne resurse
- **Troškovna ograničenja**: Mnoge organizacije ne mogu priuštiti infrastrukturu za implementaciju velikih modela

## Proces destilacije {#the-distillation-process}

Destilacija modela slijedi dvostupanjski proces koji prenosi znanje s učiteljskog modela na studentski model:

### Faza 1: Generiranje sintetičkih podataka

Učiteljski model generira odgovore za vaš skup podataka za treniranje, stvarajući visokokvalitetne sintetičke podatke koji odražavaju znanje i obrasce zaključivanja učitelja.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Ključni aspekti ove faze:**
- Učiteljski model obrađuje svaki primjer iz skupa podataka za treniranje
- Generirani odgovori postaju "referentna istina" za treniranje studenta
- Ovaj proces hvata obrasce odlučivanja učitelja
- Kvaliteta sintetičkih podataka izravno utječe na performanse studentskog modela

### Faza 2: Fino podešavanje studentskog modela

Studentski model se trenira na sintetičkom skupu podataka, učeći replicirati ponašanje i odgovore učitelja.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Ciljevi treniranja:**
- Minimizirati razliku između izlaza studenta i učitelja
- Očuvati znanje učitelja u manjem prostoru parametara
- Održati performanse uz smanjenje složenosti modela

## Praktična implementacija {#practical-implementation}

### Odabir učiteljskog i studentskog modela

**Odabir učiteljskog modela:**
- Odaberite velike LLM modele (100B+ parametara) s dokazanim performansama za vaš specifični zadatak
- Popularni učiteljski modeli uključuju:
  - **DeepSeek V3** (671B parametara) - izvrsno za zaključivanje i generiranje koda
  - **Meta Llama 3.1 405B Instruct** - sveobuhvatne opće sposobnosti
  - **GPT-4** - snažne performanse na raznovrsnim zadacima
  - **Claude 3.5 Sonnet** - izvrsno za složene zadatke zaključivanja
- Osigurajte da učiteljski model dobro radi na vašim podacima specifičnim za domenu

**Odabir studentskog modela:**
- Uravnotežite veličinu modela i zahtjeve za performansama
- Fokusirajte se na učinkovite, manje modele poput:
  - **Microsoft Phi-4-mini** - najnoviji učinkovit model s jakim sposobnostima zaključivanja
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K i 128K varijante)
  - Microsoft Phi-3.5 Mini Instruct

### Koraci implementacije

1. **Priprema podataka**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Postavljanje učiteljskog modela**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Generiranje sintetičkih podataka**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Treniranje studentskog modela**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Primjer destilacije u Azure ML {#azure-ml-example}

Azure Machine Learning pruža sveobuhvatnu platformu za implementaciju destilacije modela. Evo kako iskoristiti Azure ML za vaš destilacijski tijek rada:

### Preduvjeti

1. **Azure ML Workspace**: Postavite svoj radni prostor u odgovarajućoj regiji
   - Osigurajte pristup velikim učiteljskim modelima (DeepSeek V3, Llama 405B)
   - Konfigurirajte regije prema dostupnosti modela

2. **Računalni resursi**: Konfigurirajte odgovarajuće računalne instance za treniranje
   - Instance s velikom memorijom za inferenciju učiteljskog modela
   - GPU-omogućene instance za fino podešavanje studentskog modela

### Podržane vrste zadataka

Azure ML podržava destilaciju za razne zadatke:

- **Interpretacija prirodnog jezika (NLI)**
- **Konverzacijski AI**
- **Pitanja i odgovori (QA)**
- **Matematičko zaključivanje**
- **Sažimanje teksta**

### Primjer implementacije

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Praćenje i evaluacija

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Najbolje prakse i optimizacija {#best-practices}

### Kvaliteta podataka

**Visokokvalitetni podaci za treniranje su ključni:**
- Osigurajte raznolike i reprezentativne primjere za treniranje
- Koristite podatke specifične za domenu kad god je to moguće
- Validirajte izlaze učiteljskog modela prije nego ih koristite za treniranje studenta
- Uravnotežite skup podataka kako biste izbjegli pristranost u učenju studentskog modela

### Podešavanje hiperparametara

**Ključni parametri za optimizaciju:**
- **Stopa učenja**: Započnite s manjim stopama (1e-5 do 5e-5) za fino podešavanje
- **Veličina serije**: Uravnotežite memorijska ograničenja i stabilnost treniranja
- **Broj epoha**: Pratite prekomjerno prilagođavanje; obično su dovoljne 2-5 epohe
- **Skaliranje temperature**: Prilagodite mekoću izlaza učitelja za bolji prijenos znanja

### Razmatranja arhitekture modela

**Kompatibilnost učitelja i studenta:**
- Osigurajte arhitektonsku kompatibilnost između učiteljskog i studentskog modela
- Razmotrite podudaranje međuslojnih značajki za bolji prijenos znanja
- Koristite tehnike prijenosa pažnje kad je primjenjivo

### Strategije evaluacije

**Sveobuhvatan pristup evaluaciji:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Primjene u stvarnom svijetu {#real-world-applications}

### Mobilna i edge implementacija

Destilirani modeli omogućuju AI funkcionalnosti na uređajima s ograničenim resursima:
- **Aplikacije za pametne telefone** s obradom teksta u stvarnom vremenu
- **IoT uređaji** koji obavljaju lokalnu inferenciju
- **Ugrađeni sustavi** s ograničenim računalnim resursima

### Isplativi proizvodni sustavi

Organizacije koriste destilaciju za smanjenje operativnih troškova:
- **Chatbotovi za korisničku podršku** s bržim vremenom odgovora
- **Sustavi za moderaciju sadržaja** koji učinkovito obrađuju velike količine podataka
- **Usluge prijevoda u stvarnom vremenu** s nižim zahtjevima za latenciju

### Specifične primjene po domenama

Destilacija pomaže u stvaranju specijaliziranih modela:
- **Pomoć pri medicinskoj dijagnozi** s lokalnom inferencijom koja čuva privatnost
- **Analiza pravnih dokumenata** optimizirana za specifične pravne domene
- **Procjena financijskog rizika** s brzim donošenjem odluka

### Studija slučaja: Korisnička podrška s DeepSeek V3 → Phi-4-mini

Tehnološka tvrtka implementirala je destilaciju za svoj sustav korisničke podrške:

**Detalji implementacije:**
- **Učiteljski model**: DeepSeek V3 (671B parametara) - izvrsno zaključivanje za složene korisničke upite
- **Studentski model**: Phi-4-mini - optimiziran za brzu inferenciju i implementaciju
- **Podaci za treniranje**: 50,000 razgovora korisničke podrške
- **Zadatak**: Višekratna konverzacijska podrška s tehničkim rješavanjem problema

**Postignuti rezultati:**
- **85% smanjenje** vremena inferencije (s 3.2s na 0.48s po odgovoru)
- **95% smanjenje** zahtjeva za memorijom (s 1.2TB na 60GB)
- **92% zadržavanje** točnosti originalnog modela na zadacima podrške
- **60% smanjenje** operativnih troškova
- **Poboljšana skalabilnost** - sada može podržati 10x više istovremenih korisnika

**Razrada performansi:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Zaključak {#conclusion}

Destilacija modela predstavlja ključnu tehniku za demokratizaciju pristupa naprednim AI sposobnostima. Omogućujući stvaranje manjih, učinkovitijih modela koji zadržavaju većinu performansi svojih većih kolega, destilacija odgovara na rastuću potrebu za praktičnom implementacijom AI-a.

### Ključne točke

1. **Destilacija premošćuje jaz** između performansi modela i praktičnih ograničenja
2. **Dvostupanjski proces** osigurava učinkovit prijenos znanja s učitelja na studenta
3. **Azure ML pruža robusnu infrastrukturu** za implementaciju destilacijskih tijekova rada
4. **Pravilna evaluacija i optimizacija** ključni su za uspješnu destilaciju
5. **Primjene u stvarnom svijetu** pokazuju značajne prednosti u troškovima, brzini i dostupnosti

### Smjerovi za budućnost

Kako se područje nastavlja razvijati, možemo očekivati:
- **Napredne tehnike destilacije** s boljim metodama prijenosa znanja
- **Destilaciju s više učitelja** za poboljšane sposobnosti studentskog modela
- **Automatiziranu optimizaciju** procesa destilacije
- **Širu podršku za modele** kroz različite arhitekture i domene

Destilacija modela omogućuje organizacijama da iskoriste najnaprednije AI sposobnosti uz održavanje praktičnih ograničenja implementacije, čineći napredne jezične modele dostupnima u širokom rasponu aplikacija i okruženja.

## ➡️ Što slijedi

- [03: Fino podešavanje - prilagodba modela za specifične zadatke](./03.SLMOps-Finetuing.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogrešne interpretacije koje proizlaze iz korištenja ovog prijevoda.