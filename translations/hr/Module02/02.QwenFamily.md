<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T22:22:19+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "hr"
}
-->
# Odjeljak 2: Osnove Qwen obitelji

Modeli iz obitelji Qwen predstavljaju sveobuhvatan pristup Alibaba Clouda velikim jezičnim modelima i multimodalnoj umjetnoj inteligenciji, pokazujući da modeli otvorenog koda mogu postići izvanredne performanse dok su dostupni u raznim scenarijima implementacije. Važno je razumjeti kako obitelj Qwen omogućuje moćne AI sposobnosti s fleksibilnim opcijama implementacije, istovremeno održavajući konkurentne performanse u raznovrsnim zadacima.

## Resursi za programere

### Hugging Face Model Repository
Odabrani modeli iz obitelji Qwen dostupni su putem [Hugging Face](https://huggingface.co/models?search=qwen), pružajući pristup nekim varijantama ovih modela. Možete istražiti dostupne varijante, prilagoditi ih za svoje specifične potrebe i implementirati ih kroz različite okvire.

### Alati za lokalni razvoj
Za lokalni razvoj i testiranje možete koristiti [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) za pokretanje dostupnih Qwen modela na vašem razvojnom računalu s optimiziranim performansama.

### Dokumentacijski resursi
- [Qwen Model Documentation](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimizacija Qwen modela za implementaciju na rubnim uređajima](https://github.com/microsoft/olive)

## Uvod

U ovom vodiču istražit ćemo obitelj modela Qwen i njezine osnovne koncepte. Pokrit ćemo evoluciju obitelji Qwen, inovativne metodologije treniranja koje čine Qwen modele učinkovitim, ključne varijante u obitelji i praktične primjene u različitim scenarijima.

## Ciljevi učenja

Na kraju ovog vodiča moći ćete:

- Razumjeti filozofiju dizajna i evoluciju obitelji Qwen modela Alibaba Clouda
- Identificirati ključne inovacije koje omogućuju Qwen modelima postizanje visokih performansi u različitim veličinama parametara
- Prepoznati prednosti i ograničenja različitih varijanti Qwen modela
- Primijeniti znanje o Qwen modelima za odabir odgovarajućih varijanti za stvarne scenarije

## Razumijevanje suvremenog krajolika AI modela

Krajolik umjetne inteligencije značajno se razvio, s različitim organizacijama koje slijede različite pristupe razvoju jezičnih modela. Dok se neki fokusiraju na vlasničke modele zatvorenog koda, drugi naglašavaju dostupnost i transparentnost otvorenog koda. Tradicionalni pristup uključuje ili masivne vlasničke modele dostupne samo putem API-ja ili modele otvorenog koda koji mogu zaostajati u sposobnostima.

Ova paradigma stvara izazove za organizacije koje traže moćne AI sposobnosti dok održavaju kontrolu nad svojim podacima, troškovima i fleksibilnošću implementacije. Konvencionalni pristup često zahtijeva izbor između vrhunskih performansi i praktičnih razmatranja implementacije.

## Izazov dostupne AI izvrsnosti

Potreba za visokokvalitetnom, dostupnom umjetnom inteligencijom postaje sve važnija u raznim scenarijima. Razmotrite primjene koje zahtijevaju fleksibilne opcije implementacije za različite organizacijske potrebe, isplative implementacije gdje troškovi API-ja mogu postati značajni, višejezične sposobnosti za globalne aplikacije ili specijaliziranu stručnost u područjima poput programiranja i matematike.

### Ključni zahtjevi za implementaciju

Suvremene AI implementacije suočavaju se s nekoliko temeljnih zahtjeva koji ograničavaju praktičnu primjenjivost:

- **Dostupnost**: Dostupnost otvorenog koda za transparentnost i prilagodbu
- **Isplativost**: Razumni računalni zahtjevi za različite proračune
- **Fleksibilnost**: Više veličina modela za različite scenarije implementacije
- **Globalni doseg**: Snažne višejezične i međukulturalne sposobnosti
- **Specijalizacija**: Varijante specifične za domenu za određene slučajeve upotrebe

## Filozofija Qwen modela

Obitelj Qwen modela predstavlja sveobuhvatan pristup razvoju AI modela, prioritizirajući dostupnost otvorenog koda, višejezične sposobnosti i praktičnu implementaciju dok održava konkurentne karakteristike performansi. Qwen modeli to postižu kroz raznolike veličine modela, visokokvalitetne metodologije treniranja i specijalizirane varijante za različite domene.

Obitelj Qwen obuhvaća različite pristupe dizajnirane za pružanje opcija kroz spektar performansi i učinkovitosti, omogućujući implementaciju od mobilnih uređaja do korporativnih servera dok pruža značajne AI sposobnosti. Cilj je demokratizirati pristup visokokvalitetnoj umjetnoj inteligenciji dok se pruža fleksibilnost u izborima implementacije.

### Temeljna načela dizajna Qwen modela

Qwen modeli temelje se na nekoliko osnovnih načela koja ih razlikuju od drugih obitelji jezičnih modela:

- **Otvoreni kod na prvom mjestu**: Potpuna transparentnost i dostupnost za istraživanje i komercijalnu upotrebu
- **Sveobuhvatno treniranje**: Treniranje na masivnim, raznolikim skupovima podataka koji pokrivaju više jezika i domena
- **Skalabilna arhitektura**: Više veličina modela za usklađivanje s različitim računalnim zahtjevima
- **Specijalizirana izvrsnost**: Varijante specifične za domenu optimizirane za određene zadatke

## Ključne tehnologije koje omogućuju obitelj Qwen

### Masivno treniranje

Jedan od definirajućih aspekata obitelji Qwen je masivna skala podataka za treniranje i računalnih resursa uloženih u razvoj modela. Qwen modeli koriste pažljivo odabrane, višejezične skupove podataka koji obuhvaćaju trilijune tokena, dizajnirane za pružanje sveobuhvatnog znanja o svijetu i sposobnosti zaključivanja.

Ovaj pristup kombinira visokokvalitetni web sadržaj, akademsku literaturu, repozitorije koda i višejezične resurse. Metodologija treniranja naglašava i širinu znanja i dubinu razumijevanja u raznim domenama i jezicima.

### Napredno zaključivanje i razmišljanje

Nedavni Qwen modeli uključuju sofisticirane sposobnosti zaključivanja koje omogućuju složeno višekorakovno rješavanje problema:

**Način razmišljanja (Qwen3)**: Modeli se mogu upustiti u detaljno zaključivanje korak po korak prije nego što pruže konačne odgovore, slično ljudskim pristupima rješavanju problema.

**Dvo-modni rad**: Sposobnost prebacivanja između brzog načina odgovora za jednostavne upite i dubljeg načina razmišljanja za složene probleme.

**Integracija lanca misli**: Prirodna integracija koraka zaključivanja koji poboljšavaju transparentnost i točnost u složenim zadacima.

### Arhitektonske inovacije

Obitelj Qwen uključuje nekoliko arhitektonskih optimizacija dizajniranih za performanse i učinkovitost:

**Skalabilni dizajn**: Dosljedna arhitektura kroz veličine modela omogućuje jednostavno skaliranje i usporedbu.

**Multimodalna integracija**: Besprijekorna integracija obrade teksta, vizije i zvuka unutar jedinstvenih arhitektura.

**Optimizacija implementacije**: Više opcija kvantizacije i formata implementacije za različite hardverske konfiguracije.

## Veličina modela i opcije implementacije

Suvremena okruženja implementacije imaju koristi od fleksibilnosti Qwen modela u različitim računalnim zahtjevima:

### Mali modeli (0.5B-3B)

Qwen pruža učinkovite male modele prikladne za implementaciju na rubnim uređajima, mobilne aplikacije i okruženja s ograničenim resursima, dok održava impresivne sposobnosti.

### Srednji modeli (7B-32B)

Modeli srednjeg raspona nude poboljšane sposobnosti za profesionalne aplikacije, pružajući izvrsnu ravnotežu između performansi i računalnih zahtjeva.

### Veliki modeli (72B+)

Modeli pune skale pružaju vrhunske performanse za zahtjevne aplikacije, istraživanje i korporativne implementacije koje zahtijevaju maksimalne sposobnosti.

## Prednosti obitelji Qwen modela

### Dostupnost otvorenog koda

Qwen modeli pružaju potpunu transparentnost i mogućnosti prilagodbe, omogućujući organizacijama da razumiju, modificiraju i prilagode modele svojim specifičnim potrebama bez vezanja za dobavljača.

### Fleksibilnost implementacije

Raspon veličina modela omogućuje implementaciju u raznim hardverskim konfiguracijama, od mobilnih uređaja do vrhunskih servera, pružajući organizacijama fleksibilnost u izboru AI infrastrukture.

### Višejezična izvrsnost

Qwen modeli izvrsni su u višejezičnom razumijevanju i generiranju, podržavajući desetke jezika s posebnom snagom u engleskom i kineskom, što ih čini prikladnima za globalne aplikacije.

### Konkurentne performanse

Qwen modeli dosljedno postižu konkurentne rezultate na referentnim testovima dok pružaju dostupnost otvorenog koda, pokazujući da otvoreni modeli mogu parirati vlasničkim alternativama.

### Specijalizirane sposobnosti

Varijante specifične za domenu poput Qwen-Coder i Qwen-Math pružaju specijaliziranu stručnost dok održavaju opće sposobnosti razumijevanja jezika.

## Praktični primjeri i slučajevi upotrebe

Prije nego što se upustimo u tehničke detalje, istražimo neke konkretne primjere onoga što Qwen modeli mogu postići:

### Primjer matematičkog zaključivanja

Qwen-Math izvrsno rješava matematičke probleme korak po korak. Na primjer, kada se zatraži rješavanje složenog problema iz računa:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Primjer višejezične podrške

Qwen modeli pokazuju snažne višejezične sposobnosti u raznim jezicima:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Primjer multimodalnih sposobnosti

Qwen-VL može istovremeno obrađivati tekst i slike:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Primjer generiranja koda

Qwen-Coder izvrsno generira i objašnjava kod u više programskih jezika:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Izvrši binarno pretraživanje na sortiranoj listi kako bi pronašao ciljnu vrijednost.
    
    Args:
        arr (list): Sortirana lista usporedivih elemenata
        target: Vrijednost koju treba pretražiti
        
    Returns:
        int: Indeks cilja ako je pronađen, -1 ako nije pronađen
        
    Vremenska složenost: O(log n)
    Prostorna složenost: O(1)
    """
    # Inicijaliziraj lijeve i desne pokazivače
    left, right = 0, len(arr) - 1
    
    # Nastavi pretraživanje dok je prostor pretraživanja valjan
    while left <= right:
        # Izračunaj srednji indeks kako bi izbjegao prelijevanje cijelog broja
        mid = left + (right - left) // 2
        
        # Provjeri jesmo li pronašli cilj
        if arr[mid] == target:
            return mid
        
        # Ako je cilj manji, pretraži lijevu polovicu
        elif arr[mid] > target:
            right = mid - 1
        
        # Ako je cilj veći, pretraži desnu polovicu
        else:
            left = mid + 1
    
    # Cilj nije pronađen
    return -1

# Primjer upotrebe:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Indeks 7: {result}")  # Output: Indeks 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Primjer implementacije na rubnim uređajima

Qwen modeli mogu se implementirati na raznim rubnim uređajima s optimiziranim konfiguracijama:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Evolucija obitelji Qwen

### Qwen 1.0 i 1.5: Temeljni modeli

Rani Qwen modeli uspostavili su temeljna načela sveobuhvatnog treniranja i dostupnosti otvorenog koda:

- **Qwen-7B (7B parametara)**: Početno izdanje fokusirano na razumijevanje kineskog i engleskog jezika
- **Qwen-14B (14B parametara)**: Poboljšane sposobnosti s unaprijeđenim zaključivanjem i znanjem
- **Qwen-72B (72B parametara)**: Model velikih razmjera koji pruža vrhunske performanse
- **Serija Qwen1.5**: Proširena na više veličina (0.5B do 110B) s poboljšanim rukovanjem dugim kontekstom

### Obitelj Qwen2: Multimodalna ekspanzija

Serija Qwen2 označila je značajan napredak u jezičnim i multimodalnim sposobnostima:

- **Qwen2-0.5B do 72B**: Sveobuhvatan raspon jezičnih modela za različite potrebe implementacije
- **Qwen2-57B-A14B (MoE)**: Arhitektura mješavine stručnjaka za učinkovito korištenje parametara
- **Qwen2-VL**: Napredne sposobnosti vizije i jezika za razumijevanje slika
- **Qwen2-Audio**: Obrada i razumijevanje zvuka
- **Qwen2-Math**: Specijalizirano matematičko zaključivanje i rješavanje problema

### Obitelj Qwen2.5: Poboljšane performanse

Serija Qwen2.5 donijela je značajna poboljšanja u svim dimenzijama:

- **Prošireno treniranje**: 18 trilijuna tokena podataka za treniranje za poboljšane sposobnosti
- **Prošireni kontekst**: Do 128K tokena duljine konteksta, s Turbo varijantom koja podržava 1M tokena
- **Poboljšana specijalizacija**: Poboljšane varijante Qwen2.5-Coder i Qwen2.5-Math
- **Bolja višejezična podrška**: Poboljšane performanse u 27+ jezika

### Obitelj Qwen3: Napredno zaključivanje

Najnovija generacija pomiče granice sposobnosti zaključivanja i razmišljanja:

- **Qwen3-235B-A22B**: Vodeći model mješavine stručnjaka s ukupno 235B parametara
- **Qwen3-30B-A3B**: Učinkovit MoE model s jakim performansama po aktivnom parametru
- **Gusti modeli**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B za različite scenarije implementacije
- **Način razmišljanja**: Hibridni pristup zaključivanju koji podržava i brze odgovore i duboko razmišljanje
- **Višejezična izvrsnost**: Podrška za 119 jezika i dijalekata
- **Poboljšano treniranje**: 36 trilijuna tokena raznolikih, visokokvalitetnih podataka za treniranje

## Primjene Qwen modela

### Korporativne aplikacije

Organizacije koriste Qwen modele za analizu dokumenata, automatizaciju korisničke podrške, pomoć u generiranju koda i aplikacije poslovne inteligencije. Priroda otvorenog koda omogućuje prilagodbu za specifične poslovne potrebe uz održavanje privatnosti i kontrole podataka.

### Mobilno i rubno računalstvo

Mobilne aplikacije koriste Qwen modele za prijevod u stvarnom vremenu, inteligentne asistente, generiranje sadržaja i personalizirane preporuke. Raspon veličina modela omogućuje implementaciju od mobilnih uređaja do rubnih servera.

### Edukacijska tehnologija

Edukacijske platforme koriste Qwen modele za personalizirano podučavanje, automatizirano generiranje sadržaja, pomoć u učenju jezika i interaktivna edukacijska iskustva. Specijalizirani modeli poput Qwen-Math pružaju stručnost specifičnu za domenu.

### Globalne aplikacije

Međunarodne aplikacije imaju koristi od snažnih višejezičnih sposobnosti Qwen modela, omogućujući dosljedna AI iskustva u različitim jezicima i kulturnim kontekstima.

## Izazovi i ograničenja

### Računalni zahtjevi

Iako Qwen pruža modele u raznim veličinama, veće varijante i dalje zahtijevaju značajne računalne resurse za optimalne performanse
Evo kako započeti s Qwen modelima koristeći Hugging Face Transformers biblioteku:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Korištenje Qwen2.5 modela

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Specijalizirana upotreba modela

**Generiranje koda s Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Rješavanje matematičkih problema:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Zadaci vezani uz viziju i jezik:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Način razmišljanja (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 Mobilna i rubna implementacija

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Primjer API implementacije

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Benchmarkovi performansi i postignuća

Obitelj Qwen modela postigla je izvanredne rezultate na raznim benchmarkovima, uz održavanje otvorenog pristupa:

### Ključni naglasci performansi

**Izvrsnost u zaključivanju:**
- Qwen3-235B-A22B postiže konkurentne rezultate u evaluacijama kodiranja, matematike i općih sposobnosti u usporedbi s drugim vrhunskim modelima poput DeepSeek-R1, o1, o3-mini, Grok-3 i Gemini-2.5-Pro
- Qwen3-30B-A3B nadmašuje QwQ-32B s 10 puta više aktiviranih parametara
- Qwen3-4B može se usporediti s performansama Qwen2.5-72B-Instruct

**Postignuća u učinkovitosti:**
- Osnovni modeli Qwen3-MoE postižu slične performanse kao Qwen2.5 gusti osnovni modeli, koristeći samo 10% aktivnih parametara
- Značajne uštede u troškovima treninga i inferencije u usporedbi s gustim modelima

**Višejezične sposobnosti:**
- Qwen3 modeli podržavaju 119 jezika i dijalekata
- Snažne performanse u raznolikim jezičnim i kulturnim kontekstima

**Skala treninga:**
- Qwen3 koristi gotovo dvostruko više podataka, s približno 36 trilijuna tokena koji pokrivaju 119 jezika i dijalekata u usporedbi s Qwen2.5 koji koristi 18 trilijuna tokena

### Matrica usporedbe modela

| Serija modela | Raspon parametara | Duljina konteksta | Ključne snage | Najbolje primjene |
|---------------|-------------------|-------------------|---------------|-------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Uravnotežene performanse, višejezičnost | Opće aplikacije, produkcijska implementacija |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Generiranje koda, programiranje | Razvoj softvera, pomoć pri kodiranju |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Matematičko zaključivanje | Edukacijske platforme, STEM aplikacije |
| **Qwen2.5-VL** | Razni | Promjenjivo | Razumijevanje vizije i jezika | Multimodalne aplikacije, analiza slika |
| **Qwen3** | 0.6B-235B | Promjenjivo | Napredno zaključivanje, način razmišljanja | Kompleksno zaključivanje, istraživačke aplikacije |
| **Qwen3 MoE** | 30B-235B ukupno | Promjenjivo | Učinkovite performanse velikih razmjera | Poslovne aplikacije, potrebe visokih performansi |

## Vodič za odabir modela

### Za osnovne aplikacije
- **Qwen2.5-0.5B/1.5B**: Mobilne aplikacije, rubni uređaji, aplikacije u stvarnom vremenu
- **Qwen2.5-3B/7B**: Opći chatbotovi, generiranje sadržaja, sustavi pitanja i odgovora

### Za matematičke i zaključivačke zadatke
- **Qwen2.5-Math**: Rješavanje matematičkih problema i STEM edukacija
- **Qwen3 s načinom razmišljanja**: Kompleksno zaključivanje koje zahtijeva analizu korak po korak

### Za programiranje i razvoj
- **Qwen2.5-Coder**: Generiranje koda, otklanjanje grešaka, pomoć pri programiranju
- **Qwen3**: Napredni zadaci programiranja s sposobnostima zaključivanja

### Za multimodalne aplikacije
- **Qwen2.5-VL**: Razumijevanje slika, odgovaranje na vizualna pitanja
- **Qwen-Audio**: Obrada zvuka i razumijevanje govora

### Za poslovnu implementaciju
- **Qwen2.5-32B/72B**: Jezično razumijevanje visokih performansi
- **Qwen3-235B-A22B**: Maksimalne sposobnosti za zahtjevne aplikacije

## Platforme za implementaciju i dostupnost
### Cloud platforme
- **Hugging Face Hub**: Sveobuhvatan repozitorij modela s podrškom zajednice
- **ModelScope**: Alibaba platforma za modele s alatima za optimizaciju
- **Razni pružatelji usluga u oblaku**: Podrška putem standardnih ML platformi

### Okviri za lokalni razvoj
- **Transformers**: Standardna Hugging Face integracija za jednostavnu implementaciju
- **vLLM**: Poslužitelj visokih performansi za produkcijska okruženja
- **Ollama**: Pojednostavljena lokalna implementacija i upravljanje
- **ONNX Runtime**: Optimizacija za različite hardverske platforme
- **llama.cpp**: Učinkovita C++ implementacija za razne platforme

### Resursi za učenje
- **Qwen dokumentacija**: Službena dokumentacija i kartice modela
- **Hugging Face Model Hub**: Interaktivni demo primjeri i primjeri zajednice
- **Istraživački radovi**: Tehnički radovi na arxivu za detaljno razumijevanje
- **Forumi zajednice**: Aktivna podrška zajednice i rasprave

### Početak rada s Qwen modelima

#### Platforme za razvoj
1. **Hugging Face Transformers**: Započnite sa standardnom Python integracijom
2. **ModelScope**: Istražite Alibaba alate za optimiziranu implementaciju
3. **Lokalna implementacija**: Koristite Ollama ili direktne transformere za lokalno testiranje

#### Put učenja
1. **Razumijevanje osnovnih pojmova**: Proučite arhitekturu i sposobnosti obitelji Qwen
2. **Eksperimentiranje s varijantama**: Isprobajte različite veličine modela za razumijevanje kompromisa u performansama
3. **Praksa implementacije**: Implementirajte modele u razvojnim okruženjima
4. **Optimizacija implementacije**: Fino podešavanje za produkcijske slučajeve

#### Najbolje prakse
- **Započnite s manjim modelima**: Počnite s manjim modelima (1.5B-7B) za početni razvoj
- **Koristite predloške za chat**: Primijenite pravilno formatiranje za optimalne rezultate
- **Pratite resurse**: Pratite korištenje memorije i brzinu inferencije
- **Razmotrite specijalizaciju**: Odaberite varijante specifične za domenu kada je to prikladno

## Napredni obrasci korištenja

### Primjeri fino podešavanja

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specijalizirano inženjerstvo upita

**Za zadatke kompleksnog zaključivanja:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Za generiranje koda s kontekstom:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Višejezične aplikacije

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Obrasci produkcijske implementacije

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Strategije optimizacije performansi

### Optimizacija memorije

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimizacija inferencije

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Najbolje prakse i smjernice

### Sigurnost i privatnost

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Praćenje i evaluacija

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Zaključak

Obitelj Qwen modela predstavlja sveobuhvatan pristup demokratizaciji AI tehnologije uz održavanje konkurentnih performansi u raznolikim aplikacijama. Kroz svoju predanost otvorenom pristupu, višejezičnim sposobnostima i fleksibilnim opcijama implementacije, Qwen omogućuje organizacijama i developerima korištenje moćnih AI sposobnosti bez obzira na njihove resurse ili specifične zahtjeve.

### Ključni zaključci

**Izvrsnost otvorenog pristupa**: Qwen pokazuje da otvoreni modeli mogu postići performanse konkurentne vlasničkim alternativama, uz pružanje transparentnosti, prilagodljivosti i kontrole.

**Skalabilna arhitektura**: Raspon od 0.5B do 235B parametara omogućuje implementaciju u cijelom spektru računalnih okruženja, od mobilnih uređaja do poslovnih klastera.

**Specijalizirane sposobnosti**: Varijante specifične za domenu poput Qwen-Coder, Qwen-Math i Qwen-VL pružaju specijaliziranu ekspertizu uz održavanje općeg jezičnog razumijevanja.

**Globalna dostupnost**: Snažna podrška za više jezika (119+ jezika) čini Qwen prikladnim za međunarodne aplikacije i raznolike korisničke baze.

**Kontinuirana inovacija**: Evolucija od Qwen 1.0 do Qwen3 pokazuje dosljedno poboljšanje sposobnosti, učinkovitosti i opcija implementacije.

### Pogled u budućnost

Kako se obitelj Qwen nastavlja razvijati, možemo očekivati:

- **Poboljšanu učinkovitost**: Kontinuiranu optimizaciju za bolje omjere performansi po parametru
- **Proširene multimodalne sposobnosti**: Integraciju sofisticiranijih obrada vizije, zvuka i teksta
- **Naprednije zaključivanje**: Poboljšane mehanizme razmišljanja i sposobnosti rješavanja problema u više koraka
- **Bolje alate za implementaciju**: Poboljšane okvire i alate za optimizaciju za raznolike scenarije implementacije
- **Rast zajednice**: Prošireni ekosustav alata, aplikacija i doprinosa zajednice

### Sljedeći koraci

Bez obzira razvijate li chatbot, edukacijske alate, asistente za kodiranje ili radite na višejezičnim aplikacijama, obitelj Qwen pruža skalabilna rješenja uz snažnu podršku zajednice i sveobuhvatnu dokumentaciju.

Za najnovije informacije, izdanja modela i detaljnu tehničku dokumentaciju, posjetite službene Qwen repozitorije na Hugging Faceu i istražite aktivne rasprave i primjere zajednice.

Budućnost razvoja AI-a leži u dostupnim, transparentnim i moćnim alatima koji omogućuju inovaciju u svim sektorima i razmjerima. Obitelj Qwen utjelovljuje ovu viziju, pružajući organizacijama i developerima temelj za izgradnju sljedeće generacije AI aplikacija.

## Dodatni resursi

- **Službena dokumentacija**: [Qwen Dokumentacija](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Kolekcije](https://huggingface.co/collections/Qwen/)
- **Tehnički radovi**: [Qwen Istraživačke Publikacije](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Zajednica**: [GitHub Rasprave i Problemi](https://github.com/QwenLM/)
- **ModelScope Platforma**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Ishodi učenja

Nakon završetka ovog modula, moći ćete:

1. Objasniti arhitektonske prednosti obitelji Qwen modela i njihov pristup otvorenom kodu
2. Odabrati odgovarajuću Qwen varijantu na temelju specifičnih zahtjeva aplikacije i ograničenja resursa
3. Implementirati Qwen modele u raznim scenarijima implementacije s optimiziranim konfiguracijama
4. Primijeniti tehnike kvantizacije i optimizacije za poboljšanje performansi Qwen modela
5. Procijeniti kompromise između veličine modela, performansi i sposobnosti unutar obitelji Qwen

## Što slijedi

- [03: Osnove Gemma obitelji](03.GemmaFamily.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.