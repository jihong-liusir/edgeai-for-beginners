<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-17T18:04:22+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "ru"
}
-->
# Раздел 3: Тонкая настройка - адаптация моделей для конкретных задач

## Содержание
1. [Введение в тонкую настройку](../../../Module05)
2. [Почему тонкая настройка важна](../../../Module05)
3. [Типы тонкой настройки](../../../Module05)
4. [Тонкая настройка с Microsoft Olive](../../../Module05)
5. [Практические примеры](../../../Module05)
6. [Лучшие практики и рекомендации](../../../Module05)
7. [Продвинутые техники](../../../Module05)
8. [Оценка и мониторинг](../../../Module05)
9. [Распространенные проблемы и их решения](../../../Module05)
10. [Заключение](../../../Module05)

## Введение в тонкую настройку

**Тонкая настройка** — это мощная техника машинного обучения, которая позволяет адаптировать предварительно обученную модель для выполнения конкретных задач или работы с специализированными наборами данных. Вместо того чтобы обучать модель с нуля, тонкая настройка использует уже накопленные знания предварительно обученной модели и корректирует их для вашего конкретного случая.

### Что такое тонкая настройка?

Тонкая настройка — это форма **передачи обучения**, при которой:
- Вы начинаете с предварительно обученной модели, которая уже изучила общие шаблоны на больших наборах данных
- Корректируете внутренние параметры модели, используя ваш специфический набор данных
- Сохраняете ценные знания, одновременно адаптируя модель для вашей задачи

Представьте, что вы обучаете опытного шеф-повара готовить новую кухню — он уже понимает основы кулинарии, но ему нужно освоить специфические техники и вкусы для нового стиля.

### Основные преимущества

- **Экономия времени**: Значительно быстрее, чем обучение с нуля
- **Экономия данных**: Требует меньших наборов данных для достижения хороших результатов
- **Экономия ресурсов**: Снижает вычислительные затраты
- **Улучшенная производительность**: Часто превосходит результаты обучения с нуля
- **Оптимизация ресурсов**: Делает мощный ИИ доступным для небольших команд и организаций

## Почему тонкая настройка важна

### Применение в реальном мире

Тонкая настройка необходима во многих сценариях:

**1. Адаптация к домену**
- Медицинский ИИ: Адаптация общих языковых моделей для медицинской терминологии и клинических записей
- Юридические технологии: Специализация моделей для анализа юридических документов и проверки контрактов
- Финансовые услуги: Настройка моделей для анализа финансовых отчетов и оценки рисков

**2. Специализация задач**
- Генерация контента: Тонкая настройка для определенных стилей или тонов письма
- Генерация кода: Адаптация моделей для конкретных языков программирования или фреймворков
- Перевод: Улучшение производительности для определенных языковых пар или технических доменов

**3. Корпоративные приложения**
- Обслуживание клиентов: Создание чат-ботов, понимающих специфическую терминологию компании
- Внутренняя документация: Разработка ИИ-ассистентов, знакомых с процессами организации
- Отраслевые решения: Создание моделей, понимающих специфический жаргон и рабочие процессы отрасли

## Типы тонкой настройки

### 1. Полная тонкая настройка (Instruction Fine-Tuning)

При полной тонкой настройке обновляются все параметры модели. Этот подход:
- Обеспечивает максимальную гибкость и потенциал производительности
- Требует значительных вычислительных ресурсов
- Приводит к созданию полностью новой версии модели
- Идеален для сценариев с большим объемом данных и ресурсов

### 2. Эффективная настройка параметров (PEFT)

Методы PEFT обновляют только небольшую часть параметров, делая процесс более эффективным:

#### Low-Rank Adaptation (LoRA)
- Добавляет небольшие обучаемые матрицы разложения ранга к существующим весам
- Значительно сокращает количество обучаемых параметров
- Сохраняет производительность, близкую к полной тонкой настройке
- Позволяет легко переключаться между различными адаптациями

#### QLoRA (Quantized LoRA)
- Сочетает LoRA с техниками квантования
- Еще больше снижает требования к памяти
- Позволяет настраивать крупные модели на потребительском оборудовании
- Балансирует эффективность и производительность

#### Адаптеры
- Вставляют небольшие нейронные сети между существующими слоями
- Позволяют целенаправленную настройку, сохраняя базовую модель неизменной
- Обеспечивают модульный подход к настройке модели

### 3. Тонкая настройка для конкретных задач

Сосредоточена на адаптации моделей для конкретных задач:
- **Классификация**: Настройка моделей для задач категоризации
- **Генерация**: Оптимизация для создания контента и генерации текста
- **Извлечение**: Тонкая настройка для извлечения информации и распознавания именованных сущностей
- **Резюмирование**: Специализация моделей для создания кратких обзоров документов

## Тонкая настройка с Microsoft Olive

Microsoft Olive — это комплексный инструмент для оптимизации моделей, который упрощает процесс тонкой настройки и предоставляет функции корпоративного уровня.

### Что такое Microsoft Olive?

Microsoft Olive — это инструмент с открытым исходным кодом для оптимизации моделей, который:
- Упрощает рабочие процессы тонкой настройки для различных аппаратных платформ
- Поддерживает популярные архитектуры моделей (Llama, Phi, Qwen, Gemma)
- Предлагает варианты развертывания как в облаке, так и локально
- Интегрируется с Azure ML и другими сервисами Microsoft AI
- Поддерживает автоматическую оптимизацию и квантование

### Основные функции

- **Оптимизация с учетом оборудования**: Автоматически оптимизирует модели для конкретного оборудования (CPU, GPU, NPU)
- **Поддержка нескольких форматов**: Работает с моделями PyTorch, Hugging Face и ONNX
- **Автоматизированные рабочие процессы**: Снижает необходимость ручной настройки и экспериментов
- **Интеграция с корпоративными системами**: Встроенная поддержка Azure ML и облачных развертываний
- **Расширяемая архитектура**: Позволяет использовать пользовательские методы оптимизации

### Установка и настройка

#### Базовая установка

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Дополнительные зависимости

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Проверка установки

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Практические примеры

### Пример 1: Базовая тонкая настройка с Olive CLI

Этот пример демонстрирует тонкую настройку небольшой языковой модели для классификации фраз:

#### Шаг 1: Подготовьте окружение

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Шаг 2: Настройте модель

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Шаг 3: Оптимизируйте для развертывания

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Пример 2: Расширенная конфигурация с пользовательским набором данных

#### Шаг 1: Подготовьте пользовательский набор данных

Создайте JSON-файл с вашими обучающими данными:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Шаг 2: Создайте файл конфигурации

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Шаг 3: Выполните тонкую настройку

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Пример 3: Тонкая настройка QLoRA для экономии памяти

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Лучшие практики и рекомендации

### Подготовка данных

**1. Качество данных важнее количества**
- Отдавайте предпочтение качественным, разнообразным примерам, а не большим объемам низкокачественных данных
- Убедитесь, что данные представляют вашу целевую задачу
- Последовательно очищайте и предварительно обрабатывайте данные

**2. Формат данных и шаблоны**
- Используйте единообразное форматирование для всех обучающих примеров
- Создавайте четкие шаблоны ввода-вывода, соответствующие вашей задаче
- Включайте подходящее форматирование инструкций для моделей, настроенных на выполнение инструкций

**3. Разделение набора данных**
- Оставьте 10-20% данных для проверки
- Сохраняйте схожее распределение между обучающим и проверочным наборами
- Рассмотрите стратифицированную выборку для задач классификации

### Конфигурация обучения

**1. Выбор скорости обучения**
- Начинайте с меньших скоростей обучения (1e-5 до 1e-4) для тонкой настройки
- Используйте планирование скорости обучения для лучшей сходимости
- Следите за кривыми потерь, чтобы корректировать скорость

**2. Оптимизация размера пакета**
- Балансируйте размер пакета с доступной памятью
- Используйте накопление градиентов для увеличения эффективного размера пакета
- Учитывайте взаимосвязь между размером пакета и скоростью обучения

**3. Длительность обучения**
- Следите за метриками проверки, чтобы избежать переобучения
- Используйте раннюю остановку, если производительность проверки стабилизируется
- Регулярно сохраняйте контрольные точки для восстановления и анализа

### Выбор модели

**1. Выбор базовой модели**
- Выбирайте модели, предварительно обученные на схожих доменах, если возможно
- Учитывайте размер модели относительно ваших вычислительных ограничений
- Оценивайте лицензионные требования для коммерческого использования

**2. Выбор метода тонкой настройки**
- Используйте LoRA/QLoRA для ограниченных ресурсов
- Выбирайте полную тонкую настройку, если критична максимальная производительность
- Рассмотрите подходы на основе адаптеров для сценариев с несколькими задачами

### Управление ресурсами

**1. Оптимизация оборудования**
- Выбирайте подходящее оборудование для размера модели и метода
- Эффективно используйте память GPU с помощью контрольных точек градиента
- Рассмотрите облачные решения для крупных моделей

**2. Управление памятью**
- Используйте обучение с смешанной точностью, если доступно
- Реализуйте накопление градиентов для ограничений памяти
- Следите за использованием памяти GPU в процессе обучения

## Продвинутые техники

### Обучение с несколькими адаптерами

Обучайте несколько адаптеров для разных задач, используя одну базовую модель:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Оптимизация гиперпараметров

Реализуйте систематическую настройку гиперпараметров:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Пользовательские функции потерь

Реализуйте функции потерь, специфичные для домена:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Оценка и мониторинг

### Метрики и оценка

**1. Стандартные метрики**
- **Точность**: Общая правильность для задач классификации
- **Перплексия**: Мера качества языкового моделирования
- **BLEU/ROUGE**: Качество генерации текста и резюмирования
- **F1 Score**: Сбалансированная точность и полнота для классификации

**2. Метрики, специфичные для домена**
- **Эталонные тесты задач**: Используйте установленные эталонные тесты для вашего домена
- **Оценка человеком**: Включайте оценку человеком для субъективных задач
- **Бизнес-метрики**: Соотносите с реальными бизнес-целями

**3. Настройка оценки**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Мониторинг прогресса обучения

**1. Отслеживание потерь**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Мониторинг проверки**
- Следите за потерями проверки наряду с потерями обучения
- Следите за признаками переобучения (потери проверки увеличиваются, а потери обучения уменьшаются)
- Используйте раннюю остановку на основе метрик проверки

**3. Мониторинг ресурсов**
- Следите за использованием GPU/CPU
- Отслеживайте шаблоны использования памяти
- Следите за скоростью обучения и пропускной способностью

## Распространенные проблемы и их решения

### Проблема 1: Переобучение

**Симптомы:**
- Потери обучения продолжают уменьшаться, а потери проверки увеличиваются
- Большой разрыв между производительностью обучения и проверки
- Плохая генерализация на новых данных

**Решения:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Проблема 2: Ограничения памяти

**Решения:**
- Используйте контрольные точки градиента
- Реализуйте накопление градиентов
- Выбирайте методы с эффективным использованием параметров (LoRA, QLoRA)
- Используйте параллелизм модели для крупных моделей

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Проблема 3: Медленное обучение

**Решения:**
- Оптимизируйте конвейеры загрузки данных
- Используйте обучение с смешанной точностью
- Реализуйте эффективные стратегии пакетирования
- Рассмотрите распределенное обучение для больших наборов данных

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Проблема 4: Плохая производительность

**Шаги диагностики:**
1. Проверьте качество и формат данных
2. Проверьте скорость обучения и длительность обучения
3. Оцените выбор базовой модели
4. Проверьте предварительную обработку и токенизацию

**Решения:**
- Увеличьте разнообразие обучающих данных
- Настройте планирование скорости обучения
- Попробуйте разные базовые модели
- Реализуйте техники увеличения данных

## Заключение

Тонкая настройка — это мощная техника, которая делает передовые возможности ИИ доступными. Используя инструменты, такие как Microsoft Olive, организации могут эффективно адаптировать предварительно обученные модели к своим специфическим потребностям, оптимизируя производительность и ресурсы.

### Основные выводы

1. **Выбирайте правильный подход**: Выбирайте методы тонкой настройки в зависимости от ваших ресурсов и требований к производительности
2. **Качество данных имеет значение**: Инвестируйте в качественные, репрезентативные обучающие данные
3. **Мониторинг и итерации**: Постоянно оценивайте и улучшайте свои модели
4. **Используйте инструменты**: Упрощайте и оптимизируйте процесс с помощью таких фреймворков, как Olive
5. **Планируйте развертывание**: Заранее учитывайте оптимизацию и развертывание модели

## ➡️ Что дальше

- [04: Развертывание - внедрение готовой к производству модели](./04.SLMOps.Deployment.md)

---

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.