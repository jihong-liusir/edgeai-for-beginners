<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-09-17T17:55:03+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "ru"
}
-->
# Раздел 4: Углубленное изучение Apple MLX Framework

## Содержание
1. [Введение в Apple MLX](../../../Module04)
2. [Ключевые особенности для разработки LLM](../../../Module04)
3. [Руководство по установке](../../../Module04)
4. [Начало работы с MLX](../../../Module04)
5. [MLX-LM: языковые модели](../../../Module04)
6. [Работа с крупными языковыми моделями](../../../Module04)
7. [Интеграция с Hugging Face](../../../Module04)
8. [Конвертация и квантизация моделей](../../../Module04)
9. [Тонкая настройка языковых моделей](../../../Module04)
10. [Расширенные функции LLM](../../../Module04)
11. [Лучшие практики для LLM](../../../Module04)
12. [Устранение неполадок](../../../Module04)
13. [Дополнительные ресурсы](../../../Module04)

## Введение в Apple MLX

Apple MLX — это фреймворк массивов, специально разработанный для эффективного и гибкого машинного обучения на Apple Silicon, созданный Apple Machine Learning Research. Выпущенный в декабре 2023 года, MLX представляет собой ответ Apple на такие фреймворки, как PyTorch и TensorFlow, с особым акцентом на поддержку мощных возможностей крупных языковых моделей на компьютерах Mac.

### Чем MLX особенно хорош для LLM?

MLX полностью использует архитектуру объединенной памяти Apple Silicon, что делает его особенно подходящим для локального запуска и тонкой настройки крупных языковых моделей на компьютерах Mac. Фреймворк устраняет многие проблемы совместимости, с которыми традиционно сталкивались пользователи Mac при работе с LLM.

### Кто должен использовать MLX для LLM?

- **Пользователи Mac**, желающие запускать LLM локально без зависимости от облака
- **Исследователи**, экспериментирующие с тонкой настройкой и кастомизацией языковых моделей
- **Разработчики**, создающие AI-приложения с функциями языковых моделей
- **Любой**, кто хочет использовать Apple Silicon для генерации текста, чата и языковых задач

## Ключевые особенности для разработки LLM

### 1. Архитектура объединенной памяти
Объединенная память Apple Silicon позволяет MLX эффективно работать с крупными языковыми моделями без накладных расходов на копирование памяти, характерных для других фреймворков. Это означает, что вы можете работать с более крупными моделями на том же оборудовании.

### 2. Оптимизация для Apple Silicon
MLX разработан с нуля для чипов серии M от Apple, обеспечивая оптимальную производительность для архитектур трансформеров, часто используемых в языковых моделях.

### 3. Поддержка квантизации
Встроенная поддержка квантизации в 4 и 8 бит снижает требования к памяти, сохраняя качество модели, что позволяет запускать более крупные модели на потребительском оборудовании.

### 4. Интеграция с Hugging Face
Бесшовная интеграция с экосистемой Hugging Face предоставляет доступ к тысячам предварительно обученных языковых моделей с простыми инструментами конвертации.

### 5. Тонкая настройка с помощью LoRA
Поддержка Low-Rank Adaptation (LoRA) позволяет эффективно настраивать крупные модели с минимальными вычислительными ресурсами.

## Руководство по установке

### Системные требования
- **macOS 13.0+** (для оптимизации Apple Silicon)
- **Python 3.8+**
- **Apple Silicon** (серии M1, M2, M3, M4)
- **Нативная ARM-среда** (не под Rosetta)
- **8GB+ RAM** (рекомендуется 16GB+ для более крупных моделей)

### Быстрая установка для LLM

Самый простой способ начать работу с языковыми моделями — установить MLX-LM:

```bash
pip install mlx-lm
```

Эта команда устанавливает как основной фреймворк MLX, так и утилиты для языковых моделей.

### Настройка виртуальной среды (рекомендуется)

```bash
# Create and activate virtual environment
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# Install MLX-LM
pip install mlx-lm

# Verify installation
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### Дополнительные зависимости для аудиомоделей

Если вы планируете работать с речевыми моделями, такими как Whisper:

```bash
pip install mlx-lm[whisper]
# or
pip install mlx-lm ffmpeg-python
```

## Начало работы с MLX

### Ваша первая языковая модель

Начнем с простого примера генерации текста:

```bash
# Quick text generation from command line
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Пример использования Python API

```python
from mlx_lm import load, generate

# Load a quantized model (uses less memory)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Generate text
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### Понимание загрузки моделей

```python
from mlx_lm import load

# Different ways to load models
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # Full precision
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # Quantized

# Load with custom settings
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: языковые модели

### Поддерживаемые архитектуры моделей

MLX-LM поддерживает широкий спектр популярных архитектур языковых моделей:

- **LLaMA и LLaMA 2** — базовые модели от Meta
- **Mistral и Mixtral** — эффективные и мощные модели
- **Phi-3** — компактные языковые модели от Microsoft
- **Qwen** — многоязычные модели от Alibaba
- **Code Llama** — специализированные для генерации кода
- **Gemma** — открытые языковые модели от Google

### Интерфейс командной строки

Интерфейс командной строки MLX-LM предоставляет мощные инструменты для работы с языковыми моделями:

```bash
# Basic text generation
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# Generate with specific parameters
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# Interactive chat mode
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# Get help for all options
python -m mlx_lm.generate --help
```

### Python API для сложных случаев использования

```python
from mlx_lm import load, generate

# Load model once for multiple generations
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Single prompt generation
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# Batch generation
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## Работа с крупными языковыми моделями

### Шаблоны генерации текста

#### Генерация в один шаг
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### Следование инструкциям
```python
# Format prompts for instruction-following models
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### Творческое письмо
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # Higher temperature for more creativity
)
```

### Многошаговые диалоги

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Conversation history management
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # Format conversation for the model
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# Usage
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Интеграция с Hugging Face

### Поиск моделей, совместимых с MLX

MLX работает бесшовно с экосистемой Hugging Face:

- **Просмотр моделей MLX**: https://huggingface.co/models?library=mlx&sort=trending
- **Сообщество MLX**: https://huggingface.co/mlx-community (предварительно конвертированные модели)
- **Оригинальные модели**: большинство моделей LLaMA, Mistral, Phi и Qwen работают после конвертации

### Загрузка моделей из Hugging Face

```python
from mlx_lm import load

# Load pre-converted MLX models (recommended)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# Load original Hugging Face models (will be converted automatically)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### Скачивание моделей для офлайн-использования

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Download a model for offline use
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# Use the downloaded model
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## Конвертация и квантизация моделей

### Конвертация моделей Hugging Face в MLX

```bash
# Basic conversion
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# Convert with quantization (recommended for memory efficiency)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# Convert and upload to Hugging Face Hub
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### Понимание квантизации

Квантизация уменьшает размер модели и использование памяти с минимальной потерей качества:

```python
# Comparison of model sizes and memory usage

# Original model (float32): ~14GB for 7B parameters
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-bit quantized: ~4GB for 7B parameters
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-bit quantized: ~7GB for 7B parameters (better quality than 4-bit)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### Настраиваемая квантизация

```bash
# Different quantization options
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# Group size quantization (more precise)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## Тонкая настройка языковых моделей

### Тонкая настройка с помощью LoRA (Low-Rank Adaptation)

MLX поддерживает эффективную тонкую настройку с использованием LoRA, что позволяет адаптировать крупные модели с минимальными вычислительными ресурсами:

```python
# Basic LoRA fine-tuning setup
from mlx_lm import load
from mlx_lm.utils import load_dataset

# Load base model
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Prepare your dataset (JSON format)
# Each entry should have 'text' field with your training examples
dataset_path = "your_training_data.json"
```

### Подготовка данных для обучения

Создайте JSON-файл с примерами для обучения:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### Команда для тонкой настройки

```bash
# Fine-tune with LoRA
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### Использование настроенных моделей

```python
from mlx_lm import load

# Load base model with fine-tuned adapter
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# Generate with your fine-tuned model
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## Расширенные функции LLM

### Кэширование подсказок для повышения эффективности

Для повторного использования одного и того же контекста MLX поддерживает кэширование подсказок для улучшения производительности:

```bash
# Generate and cache a system prompt
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# Use cached prompt with new queries
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### Потоковая генерация текста

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# Stream tokens as they're generated
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### Работа с моделями генерации кода

```python
from mlx_lm import load, generate

# Load a code-specialized model
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# Code generation prompt
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # Lower temperature for more precise code
)

print(code_response)
```

### Работа с чат-моделями

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Proper chat formatting for Mistral models
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# Multi-turn conversation
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## Лучшие практики для LLM

### Управление памятью

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# Check memory before loading large models
check_memory_usage()

# Use quantized models for better memory efficiency
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# vs
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### Рекомендации по выбору моделей

**Для экспериментов и обучения:**
- Используйте модели с квантизацией в 4 бита (например, `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Начните с небольших моделей, таких как Phi-3-mini

**Для производственных приложений:**
- Учитывайте баланс между размером модели и качеством
- Тестируйте как квантизированные, так и модели с полной точностью
- Проводите тестирование на ваших конкретных задачах

**Для конкретных задач:**
- **Генерация кода**: CodeLlama, Code Llama Instruct
- **Общий чат**: Mistral-7B-Instruct, Phi-3
- **Многоязычные задачи**: модели Qwen
- **Творческое письмо**: более высокие настройки температуры с Mistral или LLaMA

### Лучшие практики для инженерии подсказок

```python
# Good prompt structure for instruction-following models
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# Example usage
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### Оптимизация производительности

```python
# Optimize generation parameters based on use case
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# Usage
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## Устранение неполадок

### Распространенные проблемы и их решения

#### Проблемы с установкой

**Проблема**: "No matching distribution found for mlx-lm"
```bash
# Check Python architecture
python -c "import platform; print(platform.processor())"
# Should output 'arm', not 'i386'

# If output is 'i386', you're using x86 Python under Rosetta
# Install native ARM Python or use Conda
```

**Решение**: Используйте нативный ARM Python или Miniconda:
```bash
# Install Miniconda for ARM64
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# Create new environment
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### Проблемы с памятью

**Проблема**: "RuntimeError: Out of memory"
```python
# Use smaller or quantized models
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# instead of
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# For macOS 15+, increase wired memory limit
# sudo sysctl iogpu.wired_limit_mb=8192  # Adjust based on your RAM
```

#### Проблемы с загрузкой модели

**Проблема**: Модель не загружается или генерирует плохой вывод
```python
# Verify model integrity
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# Test with a simple prompt
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### Проблемы с производительностью

**Проблема**: Медленная скорость генерации
- Закройте другие ресурсоемкие приложения
- Используйте квантизированные модели, если возможно
- Убедитесь, что вы не работаете под Rosetta
- Проверьте доступную память перед загрузкой моделей

### Советы по отладке

```python
# Enable verbose output for debugging
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # Shows generation progress
    max_tokens=50
)

# Monitor system resources
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## Дополнительные ресурсы

### Официальная документация и репозитории

- **Репозиторий MLX на GitHub**: https://github.com/ml-explore/mlx
- **Примеры MLX-LM**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **Документация MLX**: https://ml-explore.github.io/mlx/
- **Интеграция MLX с Hugging Face**: https://huggingface.co/docs/hub/en/mlx

### Коллекции моделей

- **Модели сообщества MLX**: https://huggingface.co/mlx-community
- **Популярные модели MLX**: https://huggingface.co/models?library=mlx&sort=trending

### Примерные приложения

1. **Персональный AI-ассистент**: Создайте локального чат-бота с памятью диалога
2. **Помощник по коду**: Создайте помощника для вашего рабочего процесса разработки
3. **Генератор контента**: Разработайте инструменты для написания, суммирования и создания контента
4. **Кастомизированные модели**: Адаптируйте модели для задач в конкретной области
5. **Мультимодальные приложения**: Объедините генерацию текста с другими возможностями MLX

### Сообщество и обучение

- **Обсуждения сообщества MLX**: GitHub Issues и Discussions
- **Форумы Hugging Face**: Поддержка сообщества и обмен моделями
- **Документация для разработчиков Apple**: Официальные ресурсы Apple ML

### Цитирование

Если вы используете MLX в своем исследовании, пожалуйста, укажите:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## Заключение

Apple MLX произвел революцию в области запуска крупных языковых моделей на компьютерах Mac. Благодаря нативной оптимизации для Apple Silicon, бесшовной интеграции с Hugging Face и мощным функциям, таким как квантизация и тонкая настройка LoRA, MLX позволяет запускать сложные языковые модели локально с отличной производительностью.

Будь то создание чат-ботов, помощников по коду, генераторов контента или кастомизированных моделей, MLX предоставляет инструменты и производительность, необходимые для полного использования потенциала вашего Mac с Apple Silicon для приложений языковых моделей. Фокус фреймворка на эффективности и простоте использования делает его отличным выбором как для исследований, так и для производственных приложений.

Начните с базовых примеров в этом руководстве, исследуйте богатую экосистему предварительно конвертированных моделей на Hugging Face и постепенно переходите к более сложным функциям, таким как тонкая настройка и разработка кастомизированных моделей. По мере роста экосистемы MLX он становится все более мощной платформой для разработки языковых моделей на оборудовании Apple.

---

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.