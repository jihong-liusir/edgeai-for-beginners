<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T17:53:43+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ru"
}
-->
# Раздел 3: Microsoft Olive Optimization Suite

## Содержание
1. [Введение](../../../Module04)
2. [Что такое Microsoft Olive?](../../../Module04)
3. [Установка](../../../Module04)
4. [Краткое руководство](../../../Module04)
5. [Пример: Конвертация Qwen3 в ONNX INT4](../../../Module04)
6. [Расширенное использование](../../../Module04)
7. [Лучшие практики](../../../Module04)
8. [Устранение неполадок](../../../Module04)
9. [Дополнительные ресурсы](../../../Module04)

## Введение

Microsoft Olive — это мощный и удобный инструмент для оптимизации моделей машинного обучения с учетом особенностей оборудования. Он упрощает процесс подготовки моделей для развертывания на различных аппаратных платформах. Независимо от того, нацелены ли вы на процессоры, графические процессоры или специализированные AI-ускорители, Olive помогает достичь оптимальной производительности, сохраняя точность модели.

## Что такое Microsoft Olive?

Olive — это инструмент для оптимизации моделей с учетом особенностей оборудования, который объединяет передовые методы сжатия, оптимизации и компиляции моделей. Он работает с ONNX Runtime как комплексное решение для оптимизации инференса.

### Основные возможности

- **Оптимизация с учетом оборудования**: Автоматически выбирает лучшие методы оптимизации для целевого оборудования
- **40+ встроенных компонентов оптимизации**: Включает сжатие моделей, квантование, оптимизацию графов и многое другое
- **Простой интерфейс CLI**: Удобные команды для выполнения стандартных задач оптимизации
- **Поддержка нескольких фреймворков**: Работает с PyTorch, моделями Hugging Face и ONNX
- **Поддержка популярных моделей**: Olive автоматически оптимизирует популярные архитектуры моделей, такие как Llama, Phi, Qwen, Gemma и другие

### Преимущества

- **Сокращение времени разработки**: Нет необходимости вручную экспериментировать с различными методами оптимизации
- **Увеличение производительности**: Значительное ускорение работы (до 6 раз в некоторых случаях)
- **Кроссплатформенное развертывание**: Оптимизированные модели работают на различных аппаратных платформах и операционных системах
- **Сохранение точности**: Оптимизация сохраняет качество модели, улучшая её производительность

## Установка

### Предварительные требования

- Python 3.8 или выше
- Менеджер пакетов pip
- Виртуальная среда (рекомендуется)

### Базовая установка

Создайте и активируйте виртуальную среду:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Установите Olive с функциями автооптимизации:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Дополнительные зависимости

Olive предлагает различные дополнительные зависимости для расширенных функций:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Проверка установки

```bash
olive --help
```

Если установка прошла успешно, вы увидите сообщение справки CLI Olive.

## Краткое руководство

### Первая оптимизация

Оптимизируем небольшую языковую модель с помощью функции автооптимизации Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Что делает эта команда

Процесс оптимизации включает: получение модели из локального кэша, захват графа ONNX и сохранение весов в файле данных ONNX, оптимизацию графа ONNX и квантование модели до int4 с использованием метода RTN.

### Пояснение параметров команды

- `--model_name_or_path`: Идентификатор модели Hugging Face или локальный путь
- `--output_path`: Каталог, в котором будет сохранена оптимизированная модель
- `--device`: Целевое устройство (cpu, gpu)
- `--provider`: Провайдер выполнения (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Использовать ONNX Runtime Generate AI для инференса
- `--precision`: Точность квантования (int4, int8, fp16)
- `--log_level`: Уровень детализации логов (0=минимальный, 1=подробный)

## Пример: Конвертация Qwen3 в ONNX INT4

На основе предоставленного примера Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) оптимизируем модель Qwen3:

### Шаг 1: Загрузка модели (опционально)

Чтобы сократить время загрузки, кэшируйте только необходимые файлы:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Шаг 2: Оптимизация модели Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Шаг 3: Тестирование оптимизированной модели

Создайте простой Python-скрипт для тестирования оптимизированной модели:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Структура вывода

После оптимизации ваш каталог вывода будет содержать:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Расширенное использование

### Конфигурационные файлы

Для более сложных рабочих процессов оптимизации можно использовать JSON-файлы конфигурации:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Запуск с конфигурацией:

```bash
olive run --config config.json
```

### Оптимизация для GPU

Для оптимизации CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Для DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Тонкая настройка с Olive

Olive также поддерживает тонкую настройку моделей:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Лучшие практики

### 1. Выбор модели
- Начните с небольших моделей для тестирования (например, 0.5B-7B параметров)
- Убедитесь, что архитектура вашей целевой модели поддерживается Olive

### 2. Учет особенностей оборудования
- Соотнесите цель оптимизации с вашим оборудованием для развертывания
- Используйте оптимизацию для GPU, если у вас есть оборудование, совместимое с CUDA
- Рассмотрите DirectML для Windows с интегрированной графикой

### 3. Выбор точности
- **INT4**: Максимальное сжатие, небольшая потеря точности
- **INT8**: Хороший баланс между размером и точностью
- **FP16**: Минимальная потеря точности, умеренное уменьшение размера

### 4. Тестирование и проверка
- Всегда тестируйте оптимизированные модели для ваших конкретных случаев использования
- Сравнивайте метрики производительности (задержка, пропускная способность, точность)
- Используйте репрезентативные входные данные для оценки

### 5. Итеративная оптимизация
- Начните с автооптимизации для быстрого результата
- Используйте конфигурационные файлы для тонкой настройки
- Экспериментируйте с различными этапами оптимизации

## Устранение неполадок

### Распространенные проблемы

#### 1. Проблемы с установкой
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Проблемы с CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Проблемы с памятью
- Используйте меньшие размеры пакетов во время оптимизации
- Попробуйте квантование с более высокой точностью (int8 вместо int4)
- Убедитесь, что на диске достаточно места для кэширования модели

#### 4. Ошибки загрузки модели
- Проверьте путь к модели и права доступа
- Убедитесь, что для модели требуется `trust_remote_code=True`
- Убедитесь, что все необходимые файлы модели загружены

### Получение помощи

- **Документация**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Примеры**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Дополнительные ресурсы

### Официальные ссылки
- **Репозиторий GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Документация ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Пример Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Примеры сообщества
- **Jupyter Notebooks**: Доступны в репозитории Olive на GitHub
- **Расширение VS Code**: Расширение AI Toolkit использует Olive для оптимизации моделей
- **Блог**: В блоге Microsoft Open Source есть подробные руководства по Olive

### Связанные инструменты
- **ONNX Runtime**: Высокопроизводительный движок инференса
- **Hugging Face Transformers**: Источник многих совместимых моделей
- **Azure Machine Learning**: Облачные рабочие процессы оптимизации

## ➡️ Что дальше

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.