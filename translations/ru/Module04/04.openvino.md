<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T17:48:53+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ru"
}
-->
# Раздел 4: Набор инструментов OpenVINO для оптимизации

## Содержание
1. [Введение](../../../Module04)
2. [Что такое OpenVINO?](../../../Module04)
3. [Установка](../../../Module04)
4. [Краткое руководство](../../../Module04)
5. [Пример: Конвертация и оптимизация моделей с OpenVINO](../../../Module04)
6. [Расширенное использование](../../../Module04)
7. [Лучшие практики](../../../Module04)
8. [Устранение неполадок](../../../Module04)
9. [Дополнительные ресурсы](../../../Module04)

## Введение

OpenVINO (Open Visual Inference and Neural Network Optimization) — это открытый набор инструментов Intel для развертывания высокопроизводительных решений на основе ИИ в облаке, локальных системах и на периферийных устройствах. Независимо от того, нацелены ли вы на процессоры, графические процессоры, VPU или специализированные ускорители ИИ, OpenVINO предоставляет комплексные возможности оптимизации, сохраняя точность модели и обеспечивая кроссплатформенное развертывание.

## Что такое OpenVINO?

OpenVINO — это открытый набор инструментов, который позволяет разработчикам эффективно оптимизировать, конвертировать и развертывать модели ИИ на различных аппаратных платформах. Он состоит из трех основных компонентов: OpenVINO Runtime для выполнения, Neural Network Compression Framework (NNCF) для оптимизации моделей и OpenVINO Model Server для масштабируемого развертывания.

### Основные возможности

- **Кроссплатформенное развертывание**: Поддержка Linux, Windows и macOS с API для Python, C++ и C
- **Аппаратное ускорение**: Автоматическое обнаружение устройств и оптимизация для CPU, GPU, VPU и ускорителей ИИ
- **Фреймворк для сжатия моделей**: Современные методы квантования, обрезки и оптимизации через NNCF
- **Совместимость с фреймворками**: Прямая поддержка моделей TensorFlow, ONNX, PaddlePaddle и PyTorch
- **Поддержка генеративного ИИ**: Специализированный OpenVINO GenAI для развертывания больших языковых моделей и приложений генеративного ИИ

### Преимущества

- **Оптимизация производительности**: Значительное увеличение скорости с минимальной потерей точности
- **Снижение объема развертывания**: Минимум внешних зависимостей упрощает установку и развертывание
- **Ускоренное время запуска**: Оптимизированная загрузка моделей и кэширование для быстрого запуска приложений
- **Масштабируемое развертывание**: От периферийных устройств до облачной инфраструктуры с единообразными API
- **Готовность к производству**: Надежность уровня предприятия с подробной документацией и поддержкой сообщества

## Установка

### Предварительные требования

- Python 3.8 или выше
- Менеджер пакетов pip
- Виртуальная среда (рекомендуется)
- Совместимое оборудование (рекомендуются процессоры Intel, но поддерживаются различные архитектуры)

### Базовая установка

Создайте и активируйте виртуальную среду:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Установите OpenVINO Runtime:

```bash
pip install openvino
```

Установите NNCF для оптимизации моделей:

```bash
pip install nncf
```

### Установка OpenVINO GenAI

Для приложений генеративного ИИ:

```bash
pip install openvino-genai
```

### Необязательные зависимости

Дополнительные пакеты для специфических случаев использования:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Проверка установки

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Если установка прошла успешно, вы увидите информацию о версии OpenVINO.

## Краткое руководство

### Ваша первая оптимизация модели

Давайте конвертируем и оптимизируем модель Hugging Face с помощью OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Что делает этот процесс

Рабочий процесс оптимизации включает: загрузку оригинальной модели из Hugging Face, конвертацию в формат промежуточного представления OpenVINO (IR), применение стандартных оптимизаций и компиляцию для целевого оборудования.

### Объяснение ключевых параметров

- `export=True`: Конвертирует модель в формат IR OpenVINO
- `compile=False`: Откладывает компиляцию до выполнения для гибкости
- `device`: Целевое оборудование ("CPU", "GPU", "AUTO" для автоматического выбора)
- `save_pretrained()`: Сохраняет оптимизированную модель для повторного использования

## Пример: Конвертация и оптимизация моделей с OpenVINO

### Шаг 1: Конвертация модели с квантованием NNCF

Пример применения квантования после обучения с использованием NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Шаг 2: Расширенная оптимизация с сжатием весов

Для моделей на основе трансформеров примените сжатие весов:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Шаг 3: Выполнение с оптимизированной моделью

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Структура вывода

После оптимизации ваша директория модели будет содержать:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Расширенное использование

### Конфигурация с использованием YAML NNCF

Для сложных рабочих процессов оптимизации используйте конфигурационные файлы NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Примените конфигурацию:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Оптимизация для GPU

Для ускорения на GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Оптимизация пакетной обработки

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Развертывание серверов моделей

Разверните оптимизированные модели с помощью OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Клиентский код для сервера моделей:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Лучшие практики

### 1. Выбор и подготовка модели
- Используйте модели из поддерживаемых фреймворков (PyTorch, TensorFlow, ONNX)
- Убедитесь, что входные данные модели имеют фиксированные или известные динамические размеры
- Тестируйте с репрезентативными наборами данных для калибровки

### 2. Выбор стратегии оптимизации
- **Квантование после обучения**: Начните с этого для быстрой оптимизации
- **Сжатие весов**: Идеально для больших языковых моделей и трансформеров
- **Обучение с учетом квантования**: Используйте, когда важна точность

### 3. Аппаратно-специфическая оптимизация
- **CPU**: Используйте квантование INT8 для сбалансированной производительности
- **GPU**: Используйте точность FP16 и пакетную обработку
- **VPU**: Сосредоточьтесь на упрощении модели и слиянии слоев

### 4. Настройка производительности
- **Режим пропускной способности**: Для обработки больших объемов данных
- **Режим задержки**: Для приложений с интерактивным режимом реального времени
- **AUTO Device**: Позвольте OpenVINO выбрать оптимальное оборудование

### 5. Управление памятью
- Используйте динамические размеры с осторожностью, чтобы избежать избыточного использования памяти
- Реализуйте кэширование моделей для ускорения последующих загрузок
- Следите за использованием памяти во время оптимизации

### 6. Проверка точности
- Всегда проверяйте оптимизированные модели на соответствие исходной производительности
- Используйте репрезентативные тестовые наборы данных для оценки
- Рассмотрите постепенную оптимизацию (начните с консервативных настроек)

## Устранение неполадок

### Распространенные проблемы

#### 1. Проблемы с установкой
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Ошибки конвертации модели
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Проблемы с производительностью
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Проблемы с памятью
- Уменьшите размер пакета модели во время оптимизации
- Используйте потоковую обработку для больших наборов данных
- Включите кэширование моделей: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Снижение точности
- Используйте более высокую точность (INT8 вместо INT4)
- Увеличьте размер калибровочного набора данных
- Примените оптимизацию с использованием смешанной точности

### Мониторинг производительности

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Получение помощи

- **Документация**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Форум сообщества**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Дополнительные ресурсы

### Официальные ссылки
- **Главная страница OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Репозиторий GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Репозиторий NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Модельный зоопарк**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Учебные материалы
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Краткое руководство**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Руководство по оптимизации**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Интеграционные инструменты
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Бенчмарки производительности
- **Официальные бенчмарки**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **Модельный зоопарк NNCF**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Примеры сообщества
- **Jupyter Notebooks**: [Репозиторий OpenVINO Notebooks](https://github.com/openvinotoolkit/openvino_notebooks) - Полные учебные материалы в репозитории OpenVINO Notebooks
- **Примерные приложения**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Примеры для различных областей (компьютерное зрение, NLP, аудио)
- **Блог-посты**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Блог Intel AI и посты сообщества с подробными кейсами

### Связанные инструменты
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Дополнительные методы оптимизации для оборудования Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Для сравнения развертывания на мобильных и периферийных устройствах
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Альтернативный кроссплатформенный движок выполнения

## ➡️ Что дальше

- [05: Подробное изучение Apple MLX Framework](./05.AppleMLX.md)

---

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.