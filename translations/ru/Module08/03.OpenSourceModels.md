<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T13:15:50+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ru"
}
-->
# Сессия 3: Обнаружение и управление моделями с открытым исходным кодом

## Обзор

Эта сессия посвящена практическому изучению моделей и их управлению с помощью Foundry Local. Вы узнаете, как просматривать доступные модели, тестировать различные варианты и понимать основные характеристики производительности. Подход ориентирован на практическое использование CLI Foundry, чтобы помочь вам выбрать подходящие модели для ваших задач.

## Цели обучения

- Освоить команды CLI Foundry для обнаружения и управления моделями
- Понять принципы кэширования моделей и локального хранения
- Научиться быстро тестировать и сравнивать различные модели
- Создать практические рабочие процессы для выбора и тестирования моделей
- Исследовать растущую экосистему моделей, доступных через Foundry Local

## Предварительные требования

- Завершение Сессии 1: Начало работы с Foundry Local
- Установленный и доступный Foundry Local CLI
- Достаточно места для хранения моделей (размер моделей может варьироваться от 1 ГБ до 20 ГБ+)
- Базовое понимание типов моделей и их применения

## Часть 6: Практическое упражнение

### Упражнение: Обнаружение и сравнение моделей

Создайте собственный скрипт для оценки моделей на основе Примера 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Ваше задание

1. **Запустите скрипт Пример 03**: `samples\03\list_and_bench.cmd`
2. **Попробуйте разные модели**: протестируйте как минимум 3 разные модели
3. **Сравните производительность**: отметьте различия в скорости и качестве ответов
4. **Документируйте результаты**: создайте простой сравнительный график

### Пример формата сравнения

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Часть 7: Устранение неполадок и лучшие практики

### Распространенные проблемы и их решения

**Модель не запускается:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Недостаточно памяти:**
- Начните с меньших моделей (`phi-4-mini`)
- Закройте другие приложения
- Обновите оперативную память, если часто сталкиваетесь с ограничениями

**Медленная производительность:**
- Убедитесь, что модель полностью загружена (проверьте подробный вывод)
- Закройте ненужные фоновые приложения
- Рассмотрите возможность использования более быстрого хранилища (SSD)

### Лучшие практики

1. **Начинайте с малого**: Используйте `phi-4-mini` для проверки настройки
2. **Одна модель за раз**: Останавливайте предыдущие модели перед запуском новых
3. **Следите за ресурсами**: Контролируйте использование памяти
4. **Тестируйте последовательно**: Используйте одинаковые запросы для честного сравнения
5. **Документируйте результаты**: Ведите записи о производительности моделей для ваших задач

## Часть 8: Следующие шаги и ресурсы

### Подготовка к Сессии 4

- **Тема Сессии 4**: Инструменты и методы оптимизации
- **Предварительные требования**: Уверенное переключение между моделями и базовое тестирование производительности
- **Рекомендуется**: Выберите 2-3 любимые модели из этой сессии

### Дополнительные ресурсы

- **[Документация Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Официальная документация
- **[Справочник CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Полный справочник команд
- **[Model Mondays](https://aka.ms/model-mondays)**: Еженедельные обзоры моделей
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Сообщество и вопросы
- **[Пример 03: Обнаружение моделей](samples/03/README.md)**: Практический пример скрипта

### Основные выводы

✅ **Обнаружение моделей**: Используйте `foundry model list` для изучения доступных моделей  
✅ **Быстрое тестирование**: Шаблон `list_and_bench.cmd` для оперативной оценки  
✅ **Мониторинг производительности**: Основные показатели использования ресурсов и времени отклика  
✅ **Выбор моделей**: Практические рекомендации по выбору моделей для различных задач  
✅ **Управление кэшем**: Понимание хранения и процедур очистки  

Теперь у вас есть практические навыки для обнаружения, тестирования и выбора подходящих моделей для ваших AI-приложений с использованием простого подхода CLI Foundry Local.

## Цели обучения

- Обнаруживать и оценивать модели с открытым исходным кодом для локального использования
- Компилировать и запускать выбранные модели Hugging Face в Foundry Local
- Применять стратегии выбора моделей с учетом точности, задержки и потребностей в ресурсах
- Управлять моделями локально с помощью кэша и версий

## Часть 1: Обнаружение моделей с Foundry CLI

### Основные команды управления моделями

Foundry CLI предоставляет простые команды для обнаружения и управления моделями:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Запуск первых моделей

Начните с популярных, хорошо протестированных моделей, чтобы понять их характеристики производительности:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Примечание:** Флаг `--verbose` предоставляет подробную информацию о запуске, включая:
- Прогресс загрузки модели (при первом запуске)
- Детали выделения памяти
- Информацию о привязке сервиса
- Метрики инициализации производительности

### Понимание категорий моделей

**Малые языковые модели (SLMs):**
- `phi-4-mini`: Быстрая, эффективная, отлично подходит для общего общения
- `phi-4`: Более мощная версия с улучшенными возможностями рассуждения

**Средние модели:**
- `qwen2.5-7b-instruct`: Отличное рассуждение и более длинный контекст
- `deepseek-r1-distill-qwen-7b`: Оптимизирована для генерации кода

**Большие модели:**
- `llama-3.2`: Последняя открытая модель от Meta
- `qwen2.5-14b-instruct`: Корпоративное качество рассуждения

## Часть 2: Быстрое тестирование и сравнение моделей

### Подход Примера 03: Простой список и тестирование

На основе шаблона Примера 03, вот минимальный рабочий процесс:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Тестирование производительности моделей

После запуска модели протестируйте её с использованием одинаковых запросов:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Альтернатива тестирования в PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Часть 3: Управление кэшем и хранилищем моделей

### Понимание кэша моделей

Foundry Local автоматически управляет загрузкой и кэшированием моделей:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Особенности хранения моделей

**Типичные размеры моделей:**
- `phi-4-mini`: ~2.5 ГБ
- `qwen2.5-7b-instruct`: ~4.1 ГБ  
- `deepseek-r1-distill-qwen-7b`: ~4.3 ГБ
- `llama-3.2`: ~4.9 ГБ
- `qwen2.5-14b-instruct`: ~8.2 ГБ

**Лучшие практики хранения:**
- Держите 2-3 модели в кэше для быстрого переключения
- Удаляйте неиспользуемые модели для освобождения места: `foundry cache clean`
- Следите за использованием диска, особенно на небольших SSD
- Учитывайте компромисс между размером модели и её возможностями

### Мониторинг производительности моделей

Во время работы моделей следите за системными ресурсами:

**Диспетчер задач Windows:**
- Контролируйте использование памяти (модели остаются загруженными в ОЗУ)
- Следите за загрузкой процессора во время выполнения запросов
- Проверяйте диск I/O при первоначальной загрузке модели

**Мониторинг через командную строку:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Часть 4: Практические рекомендации по выбору моделей

### Выбор моделей по задачам

**Для общего общения и вопросов-ответов:**
- Начните с: `phi-4-mini` (быстрая, эффективная)
- Перейдите на: `phi-4` (лучшее рассуждение)
- Продвинутый уровень: `qwen2.5-7b-instruct` (длинный контекст)

**Для генерации кода:**
- Рекомендуется: `deepseek-r1-distill-qwen-7b`
- Альтернатива: `qwen2.5-7b-instruct` (также подходит для кода)

**Для сложного рассуждения:**
- Лучший выбор: `qwen2.5-7b-instruct` или `qwen2.5-14b-instruct`
- Бюджетный вариант: `phi-4`

### Руководство по требованиям к оборудованию

**Минимальные системные требования:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Рекомендуется для лучшей производительности:**
- 32 ГБ+ ОЗУ для комфортного переключения между моделями
- SSD для более быстрой загрузки моделей
- Современный процессор с хорошей производительностью на одно ядро
- Поддержка NPU (ПК с Windows 11 Copilot+) для ускорения

### Рабочий процесс переключения моделей

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Часть 5: Простое тестирование производительности моделей

### Базовое тестирование производительности

Вот простой подход для сравнения производительности моделей:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Ручная оценка качества

Для каждой модели протестируйте её с одинаковыми запросами и оцените вручную:

**Тестовые запросы:**
1. "Объясните квантовые вычисления простыми словами."
2. "Напишите функцию на Python для сортировки списка."
3. "Каковы плюсы и минусы удалённой работы?"
4. "Суммируйте преимущества edge AI."

**Критерии оценки:**
- **Точность**: Насколько информация верна?
- **Ясность**: Насколько легко понять объяснение?
- **Полнота**: Охватывает ли ответ весь вопрос?
- **Скорость**: Как быстро модель отвечает?

### Мониторинг использования ресурсов

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Часть 6: Следующие шаги

- Подпишитесь на Model Mondays для новых моделей и советов: https://aka.ms/model-mondays
- Делитесь результатами с командой через `models.json`
- Подготовьтесь к Сессии 4: сравнение LLMs и SLMs, локальное и облачное использование, практические демонстрации

---

