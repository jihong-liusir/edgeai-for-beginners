<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T14:20:47+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ru"
}
-->
# Сессия 3: Модели с открытым исходным кодом в Foundry Local

## Обзор

В этой сессии мы изучим, как интегрировать модели с открытым исходным кодом в Foundry Local: выбор моделей сообщества, интеграция контента Hugging Face и использование стратегии «принеси свою модель» (BYOM). Вы также узнаете о серии Model Mondays для непрерывного обучения и поиска моделей.

Ссылки:
- Документация Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Компиляция моделей Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local на GitHub: https://github.com/microsoft/Foundry-Local

## Цели обучения
- Научиться находить и оценивать модели с открытым исходным кодом для локального использования
- Компилировать и запускать выбранные модели Hugging Face в Foundry Local
- Применять стратегии выбора моделей с учетом точности, задержки и потребления ресурсов
- Управлять моделями локально с использованием кэша и версий

## Часть 1: Поиск и выбор моделей (пошагово)

Шаг 1) Составьте список доступных моделей в локальном каталоге  
```cmd
foundry model list
```
  
Шаг 2) Быстро протестируйте двух кандидатов (автоматическая загрузка при первом запуске)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Шаг 3) Отметьте основные метрики  
- Обратите внимание на задержку (субъективно) и качество для фиксированного запроса  
- Следите за использованием памяти через Диспетчер задач во время работы каждой модели  

## Часть 2: Запуск моделей из каталога через CLI (пошагово)

Шаг 1) Запустите модель  
```cmd
foundry model run llama-3.2
```
  
Шаг 2) Отправьте тестовый запрос через совместимый с OpenAI endpoint  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Часть 3: BYOM – Компиляция моделей Hugging Face (пошагово)

Следуйте официальной инструкции по компиляции моделей. Общий процесс ниже — подробные команды и поддерживаемые конфигурации смотрите в статье Microsoft Learn.

Шаг 1) Подготовьте рабочую директорию  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Шаг 2) Скомпилируйте поддерживаемую модель HF  
- Используйте шаги из документации Learn для конвертации и размещения скомпилированной модели ONNX в директории `models`  
- Подтвердите с помощью:  
```cmd
foundry cache ls
```
  
Вы должны увидеть название вашей скомпилированной модели (например, `llama-3.2`).  

Шаг 3) Запустите скомпилированную модель  
```cmd
foundry model run llama-3.2 --verbose
```
  
Примечания:  
- Убедитесь, что у вас достаточно дискового пространства и оперативной памяти для компиляции и запуска  
- Начните с небольших моделей, чтобы проверить процесс, затем переходите к более крупным  

## Часть 4: Практическая работа с моделями (пошагово)

Шаг 1) Создайте реестр `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Шаг 2) Небольшой скрипт для выбора  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Часть 5: Практические тесты производительности (пошагово)

Шаг 1) Простой тест задержки  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Шаг 2) Проверка качества  
- Используйте фиксированный набор запросов, сохраните результаты в CSV/JSON  
- Оцените вручную беглость, релевантность и корректность (1–5)  

## Часть 6: Следующие шаги
- Подпишитесь на Model Mondays для получения новых моделей и советов: https://aka.ms/model-mondays  
- Делитесь результатами с командой через `models.json`  
- Подготовьтесь к Сессии 4: сравнение LLMs и SLMs, локальное и облачное использование, практические демонстрации  

---

