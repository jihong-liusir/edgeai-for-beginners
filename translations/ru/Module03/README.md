<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-17T18:07:16+00:00",
  "source_file": "Module03/README.md",
  "language_code": "ru"
}
-->
# Глава 03: Развертывание Малых Языковых Моделей (SLM)

Эта подробная глава охватывает полный жизненный цикл развертывания Малых Языковых Моделей (SLM), включая теоретические основы, практические стратегии реализации и готовые к производству контейнеризированные решения. Глава структурирована в три последовательных раздела, которые проводят читателей от базовых концепций к сложным сценариям развертывания.

## Структура главы и учебное путешествие

### **[Раздел 1: Продвинутое изучение SLM - Основы и оптимизация](./01.SLMAdvancedLearning.md)**
Начальный раздел закладывает теоретическую основу для понимания Малых Языковых Моделей и их стратегической важности в развертывании AI на периферии. В этом разделе рассматриваются:

- **Классификационная структура параметров**: Подробное изучение категорий SLM от микро-SLM (100M-1.4B параметров) до средних SLM (14B-30B параметров), с особым акцентом на модели, такие как Phi-4-mini-3.8B, серия Qwen3 и Google Gemma3, включая анализ требований к оборудованию и объема памяти для каждого уровня моделей
- **Продвинутые методы оптимизации**: Полное освещение методов квантования с использованием Llama.cpp, Microsoft Olive и Apple MLX, включая передовое квантование BitNET 1-бит с практическими примерами кода, демонстрирующими конвейеры квантования и результаты тестирования
- **Стратегии получения моделей**: Глубокий анализ экосистемы Hugging Face и каталога моделей Azure AI Foundry для развертывания SLM корпоративного уровня, с примерами кода для программного скачивания, проверки и конвертации форматов моделей
- **API для разработчиков**: Примеры кода на Python, C++ и C#, показывающие, как загружать модели, выполнять вывод и интегрироваться с популярными фреймворками, такими как PyTorch, TensorFlow и ONNX Runtime

Этот базовый раздел подчеркивает баланс между операционной эффективностью, гибкостью развертывания и экономической целесообразностью, который делает SLM идеальными для сценариев вычислений на периферии, с практическими примерами кода, которые разработчики могут напрямую использовать в своих проектах.

### **[Раздел 2: Развертывание в локальной среде - Решения с приоритетом конфиденциальности](./02.DeployingSLMinLocalEnv.md)**
Второй раздел переходит от теории к практической реализации, сосредотачиваясь на стратегиях локального развертывания, которые обеспечивают суверенитет данных и операционную независимость. Основные темы включают:

- **Универсальная платформа Ollama**: Полное исследование кроссплатформенного развертывания с акцентом на удобные для разработчиков рабочие процессы, управление жизненным циклом моделей и настройку через Modelfiles, включая примеры полной интеграции REST API и автоматизации CLI
- **Microsoft Foundry Local**: Решения корпоративного уровня для развертывания с оптимизацией на основе ONNX, интеграцией Windows ML и комплексными функциями безопасности, с примерами кода на C# и Python для интеграции в нативные приложения
- **Сравнительный анализ**: Подробное сравнение фреймворков, охватывающее техническую архитектуру, характеристики производительности и рекомендации по оптимизации использования, с кодом для оценки скорости вывода и использования памяти на различном оборудовании
- **Интеграция API**: Примеры приложений, показывающие, как создавать веб-сервисы, чат-приложения и конвейеры обработки данных с использованием локальных развертываний SLM, с примерами кода на Node.js, Python Flask/FastAPI и ASP.NET Core
- **Тестовые фреймворки**: Подходы к автоматизированному тестированию для обеспечения качества моделей, включая примеры модульных и интеграционных тестов для реализации SLM

Этот раздел предоставляет практическое руководство для организаций, стремящихся внедрить решения AI с сохранением конфиденциальности, при этом сохраняя полный контроль над своей средой развертывания, с готовыми к использованию примерами кода, которые разработчики могут адаптировать под свои конкретные требования.

### **[Раздел 3: Контейнеризированное облачное развертывание - Решения для масштабного производства](./03.DeployingSLMinCloud.md)**
Заключительный раздел посвящен продвинутым стратегиям контейнеризированного развертывания, с использованием Microsoft Phi-4-mini-instruct в качестве основного примера. В этом разделе рассматриваются:

- **Развертывание vLLM**: Оптимизация вывода высокой производительности с совместимыми API OpenAI, продвинутым ускорением GPU и конфигурацией для производства, включая полные Dockerfiles, манифесты Kubernetes и параметры настройки производительности
- **Оркестрация контейнеров Ollama**: Упрощенные рабочие процессы развертывания с Docker Compose, вариантами оптимизации моделей и интеграцией веб-интерфейса, с примерами CI/CD конвейеров для автоматизированного развертывания и тестирования
- **Реализация ONNX Runtime**: Оптимизированное для периферии развертывание с комплексной конвертацией моделей, стратегиями квантования и кроссплатформенной совместимостью, включая подробные примеры кода для оптимизации и развертывания моделей
- **Мониторинг и наблюдаемость**: Реализация панелей Prometheus/Grafana с пользовательскими метриками для мониторинга производительности SLM, включая конфигурации оповещений и агрегацию логов
- **Балансировка нагрузки и масштабирование**: Практические примеры стратегий горизонтального и вертикального масштабирования с конфигурациями автоматического масштабирования на основе использования CPU/GPU и шаблонов запросов
- **Укрепление безопасности**: Лучшие практики безопасности контейнеров, включая снижение привилегий, сетевые политики и управление секретами для ключей API и учетных данных доступа к моделям

Каждый подход к развертыванию представлен с полными примерами конфигурации, процедурами тестирования, контрольными списками готовности к производству и шаблонами инфраструктуры как кода, которые разработчики могут напрямую применять в своих рабочих процессах развертывания.

## Основные результаты обучения

После изучения этой главы читатели освоят:

1. **Стратегический выбор моделей**: Понимание границ параметров и выбор подходящих SLM на основе ограничений ресурсов и требований к производительности
2. **Мастерство оптимизации**: Реализация продвинутых методов квантования в различных фреймворках для достижения оптимального баланса между производительностью и эффективностью
3. **Гибкость развертывания**: Выбор между локальными решениями с приоритетом конфиденциальности и масштабируемыми контейнеризированными развертываниями в зависимости от потребностей организации
4. **Готовность к производству**: Настройка систем мониторинга, безопасности и масштабирования для развертываний SLM корпоративного уровня

## Практическая ориентация и реальные приложения

Глава сохраняет сильную практическую направленность, включая:

- **Практические примеры**: Полные файлы конфигурации, процедуры тестирования API и скрипты развертывания
- **Сравнение производительности**: Подробные сравнения скорости вывода, использования памяти и требований к ресурсам
- **Соображения безопасности**: Практики безопасности корпоративного уровня, рамки соответствия и стратегии защиты данных
- **Лучшие практики**: Проверенные в производстве рекомендации по мониторингу, масштабированию и обслуживанию

## Перспектива на будущее

Глава завершается перспективными инсайтами о новых тенденциях, включая:

- Продвинутые архитектуры моделей с улучшенными коэффициентами эффективности
- Более глубокую интеграцию с оборудованием, специализированным для AI
- Эволюцию экосистемы в сторону стандартизации и совместимости
- Модели корпоративного внедрения, ориентированные на конфиденциальность и требования соответствия

Этот комплексный подход гарантирует, что читатели будут хорошо подготовлены к решению как текущих задач развертывания SLM, так и будущих технологических разработок, принимая обоснованные решения, которые соответствуют их конкретным организационным требованиям и ограничениям.

Глава служит как практическим руководством для немедленной реализации, так и стратегическим ресурсом для долгосрочного планирования развертывания AI, подчеркивая критический баланс между возможностями, эффективностью и операционным совершенством, который определяет успешное развертывание SLM.

---

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.