<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T10:25:42+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "da"
}
-->
# Afsnit 2: Llama.cpp Implementeringsvejledning

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er Llama.cpp?](../../../Module04)
3. [Installation](../../../Module04)
4. [Bygning fra kildekode](../../../Module04)
5. [Modelkvantisering](../../../Module04)
6. [Grundlæggende brug](../../../Module04)
7. [Avancerede funktioner](../../../Module04)
8. [Python-integration](../../../Module04)
9. [Fejlfinding](../../../Module04)
10. [Bedste praksis](../../../Module04)

## Introduktion

Denne omfattende vejledning guider dig gennem alt, hvad du behøver at vide om Llama.cpp, fra grundlæggende installation til avancerede brugsscenarier. Llama.cpp er en kraftfuld C++-implementering, der muliggør effektiv inferens af store sprogmodeller (LLMs) med minimal opsætning og fremragende ydeevne på tværs af forskellige hardwarekonfigurationer.

## Hvad er Llama.cpp?

Llama.cpp er et LLM-inferensframework skrevet i C/C++, der gør det muligt at køre store sprogmodeller lokalt med minimal opsætning og førsteklasses ydeevne på en bred vifte af hardware. Nøglefunktioner inkluderer:

### Kernefunktioner
- **Ren C/C++-implementering** uden afhængigheder
- **Platformskompatibilitet** (Windows, macOS, Linux)
- **Hardwareoptimering** til forskellige arkitekturer
- **Kvantisering** (1,5-bit til 8-bit heltalskvantisering)
- **CPU- og GPU-acceleration**
- **Hukommelseseffektivitet** til begrænsede miljøer

### Fordele
- Kører effektivt på CPU uden behov for specialiseret hardware
- Understøtter flere GPU-backends (CUDA, Metal, OpenCL, Vulkan)
- Letvægts og bærbar
- Apple Silicon er en førsteklasses platform - optimeret via ARM NEON, Accelerate og Metal frameworks
- Understøtter forskellige kvantiseringsniveauer for reduceret hukommelsesforbrug

## Installation

### Metode 1: Forudbyggede binære filer (Anbefales til begyndere)

#### Download fra GitHub Releases
1. Besøg [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Download den passende binære fil til dit system:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` til Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` til macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` til Linux

3. Udpak arkivet, og tilføj mappen til dit systems PATH

#### Brug af pakkehåndteringssystemer

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Forskellige distributioner):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metode 2: Python-pakke (llama-cpp-python)

#### Grundlæggende installation
```bash
pip install llama-cpp-python
```

#### Med hardwareacceleration
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Bygning fra kildekode

### Forudsætninger

**Systemkrav:**
- C++-kompiler (GCC, Clang eller MSVC)
- CMake (version 3.14 eller højere)
- Git
- Byggeværktøjer til din platform

**Installation af forudsætninger:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Installer Visual Studio 2022 med C++-udviklingsværktøjer
- Installer CMake fra den officielle hjemmeside
- Installer Git

### Grundlæggende byggeproces

1. **Klon repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurer bygningen:**
```bash
cmake -B build
```

3. **Byg projektet:**
```bash
cmake --build build --config Release
```

For hurtigere kompilering, brug parallelle jobs:
```bash
cmake --build build --config Release -j 8
```

### Hardware-specifikke builds

#### CUDA-support (NVIDIA GPU'er)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal-support (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS-support (CPU-optimering)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan-support
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Avancerede byggeindstillinger

#### Debug-build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Med yderligere funktioner
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Modelkvantisering

### Forståelse af GGUF-formatet

GGUF (Generalized GGML Unified Format) er et optimeret filformat designet til effektivt at køre store sprogmodeller ved hjælp af Llama.cpp og andre frameworks. Det tilbyder:

- Standardiseret lagring af modelvægte
- Forbedret kompatibilitet på tværs af platforme
- Øget ydeevne
- Effektiv håndtering af metadata

### Kvantiseringstyper

Llama.cpp understøtter forskellige kvantiseringsniveauer:

| Type | Bits | Beskrivelse | Anvendelse |
|------|------|-------------|------------|
| F16 | 16 | Halv præcision | Høj kvalitet, stort hukommelsesforbrug |
| Q8_0 | 8 | 8-bit kvantisering | God balance |
| Q4_0 | 4 | 4-bit kvantisering | Moderat kvalitet, mindre størrelse |
| Q2_K | 2 | 2-bit kvantisering | Mindste størrelse, lavere kvalitet |

### Konvertering af modeller

#### Fra PyTorch til GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Direkte download fra Hugging Face
Mange modeller er tilgængelige i GGUF-format på Hugging Face:
- Søg efter modeller med "GGUF" i navnet
- Download det passende kvantiseringsniveau
- Brug direkte med llama.cpp

## Grundlæggende brug

### Kommandolinjegrænseflade

#### Enkel tekstgenerering
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Brug af modeller fra Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Servertilstand
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Almindelige parametre

| Parameter | Beskrivelse | Eksempel |
|-----------|-------------|----------|
| `-m` | Sti til modelfil | `-m model.gguf` |
| `-p` | Prompttekst | `-p "Hej verden"` |
| `-n` | Antal tokens, der skal genereres | `-n 100` |
| `-c` | Kontekststørrelse | `-c 4096` |
| `-t` | Antal tråde | `-t 8` |
| `-ngl` | GPU-lag | `-ngl 32` |
| `-temp` | Temperatur | `-temp 0.7` |

### Interaktiv tilstand

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Avancerede funktioner

### Server-API

#### Start af serveren
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API-brug
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Ydelsesoptimering

#### Hukommelsesstyring
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multitrådning
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU-acceleration
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python-integration

### Grundlæggende brug med llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chatgrænseflade

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streaming af svar

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integration med LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Fejlfinding

### Almindelige problemer og løsninger

#### Byggefejl

**Problem: CMake ikke fundet**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problem: Kompiler ikke fundet**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Kørselstidproblemer

**Problem: Modellæsning fejler**
- Bekræft sti til modelfil
- Tjek filrettigheder
- Sørg for tilstrækkelig RAM
- Prøv forskellige kvantiseringsniveauer

**Problem: Dårlig ydeevne**
- Aktivér hardwareacceleration
- Øg antallet af tråde
- Brug passende kvantisering
- Tjek GPU-hukommelsesforbrug

#### Hukommelsesproblemer

**Problem: Hukommelsen er opbrugt**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Platformspecifikke problemer

#### Windows
- Brug MinGW eller Visual Studio-kompiler
- Sørg for korrekt PATH-konfiguration
- Tjek for antivirusinterferens

#### macOS
- Aktivér Metal til Apple Silicon
- Brug Rosetta 2 for kompatibilitet, hvis nødvendigt
- Tjek Xcode-kommandolinjeværktøjer

#### Linux
- Installer udviklingspakker
- Tjek GPU-driverversioner
- Bekræft installation af CUDA-værktøjssæt

## Bedste praksis

### Modelvalg
1. **Vælg passende kvantisering** baseret på din hardware
2. **Overvej modelstørrelse** vs. kvalitet
3. **Test forskellige modeller** til din specifikke brugssag

### Ydelsesoptimering
1. **Brug GPU-acceleration**, når det er muligt
2. **Optimer antallet af tråde** til din CPU
3. **Indstil passende kontekststørrelse** til din brugssag
4. **Aktivér hukommelseskortlægning** til store modeller

### Produktionsudrulning
1. **Brug servertilstand** til API-adgang
2. **Implementer korrekt fejlhåndtering**
3. **Overvåg ressourceforbrug**
4. **Opsæt logning og overvågning**

### Udviklingsarbejdsgang
1. **Start med mindre modeller** til test
2. **Brug versionskontrol** til modelkonfigurationer
3. **Dokumentér dine konfigurationer**
4. **Test på tværs af forskellige platforme**

### Sikkerhedsovervejelser
1. **Valider inputprompter**
2. **Implementer hastighedsbegrænsning**
3. **Sikr API-endepunkter**
4. **Overvåg for misbrugsmønstre**

## Konklusion

Llama.cpp giver en kraftfuld og effektiv måde at køre store sprogmodeller lokalt på tværs af forskellige hardwarekonfigurationer. Uanset om du udvikler AI-applikationer, forsker eller blot eksperimenterer med LLM'er, tilbyder dette framework den fleksibilitet og ydeevne, der er nødvendig for en bred vifte af brugsscenarier.

Vigtige pointer:
- Vælg den installationsmetode, der passer bedst til dine behov
- Optimer til din specifikke hardwarekonfiguration
- Start med grundlæggende brug og udforsk gradvist avancerede funktioner
- Overvej at bruge Python-bindings for nemmere integration
- Følg bedste praksis for produktionsudrulninger

For mere information og opdateringer, besøg [den officielle Llama.cpp repository](https://github.com/ggml-org/llama.cpp) og henvis til den omfattende dokumentation og de tilgængelige ressourcer fra fællesskabet.

## ➡️ Hvad er det næste

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at opnå nøjagtighed, skal det bemærkes, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.