<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T10:15:34+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "da"
}
-->
# Afsnit 4: OpenVINO Toolkit Optimeringssuite

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er OpenVINO?](../../../Module04)
3. [Installation](../../../Module04)
4. [Hurtig startguide](../../../Module04)
5. [Eksempel: Konvertering og optimering af modeller med OpenVINO](../../../Module04)
6. [Avanceret brug](../../../Module04)
7. [Bedste praksis](../../../Module04)
8. [Fejlfinding](../../../Module04)
9. [Yderligere ressourcer](../../../Module04)

## Introduktion

OpenVINO (Open Visual Inference and Neural Network Optimization) er Intels open source-værktøj til at implementere effektive AI-løsninger på tværs af cloud, lokale og edge-miljøer. Uanset om du arbejder med CPU'er, GPU'er, VPU'er eller specialiserede AI-acceleratorer, tilbyder OpenVINO omfattende optimeringsmuligheder, samtidig med at modelnøjagtigheden bevares og muliggør tværplatformsimplementering.

## Hvad er OpenVINO?

OpenVINO er et open source-værktøj, der gør det muligt for udviklere at optimere, konvertere og implementere AI-modeller effektivt på tværs af forskellige hardwareplatforme. Det består af tre hovedkomponenter: OpenVINO Runtime til inferens, Neural Network Compression Framework (NNCF) til modeloptimering og OpenVINO Model Server til skalerbar implementering.

### Nøglefunktioner

- **Tværplatformsimplementering**: Understøtter Linux, Windows og macOS med Python-, C++- og C-API'er
- **Hardwareacceleration**: Automatisk enhedsopdagelse og optimering til CPU, GPU, VPU og AI-acceleratorer
- **Modelkomprimeringsframework**: Avancerede teknikker til kvantisering, beskæring og optimering via NNCF
- **Kompatibilitet med frameworks**: Direkte understøttelse af TensorFlow-, ONNX-, PaddlePaddle- og PyTorch-modeller
- **Generativ AI-understøttelse**: Specialiseret OpenVINO GenAI til implementering af store sprogmodeller og generative AI-applikationer

### Fordele

- **Optimering af ydeevne**: Betydelige hastighedsforbedringer med minimal nøjagtighedstab
- **Reduceret implementeringsfodaftryk**: Færre eksterne afhængigheder forenkler installation og implementering
- **Forbedret opstartstid**: Optimeret modellæsning og caching for hurtigere applikationsinitialisering
- **Skalerbar implementering**: Fra edge-enheder til cloud-infrastruktur med konsistente API'er
- **Produktionsklar**: Pålidelighed i virksomhedsklasse med omfattende dokumentation og fællesskabsstøtte

## Installation

### Forudsætninger

- Python 3.8 eller nyere
- pip-pakkestyring
- Virtuelt miljø (anbefales)
- Kompatibel hardware (Intel CPU'er anbefales, men understøtter forskellige arkitekturer)

### Grundlæggende installation

Opret og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installer OpenVINO Runtime:

```bash
pip install openvino
```

Installer NNCF til modeloptimering:

```bash
pip install nncf
```

### Installation af OpenVINO GenAI

Til generative AI-applikationer:

```bash
pip install openvino-genai
```

### Valgfrie afhængigheder

Yderligere pakker til specifikke anvendelser:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verificer installationen

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Hvis installationen lykkes, bør du se OpenVINO-versionens oplysninger.

## Hurtig startguide

### Din første modeloptimering

Lad os konvertere og optimere en Hugging Face-model ved hjælp af OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Hvad denne proces gør

Optimeringsarbejdsgangen indebærer: indlæsning af den originale model fra Hugging Face, konvertering til OpenVINO Intermediate Representation (IR)-format, anvendelse af standardoptimeringer og kompilering til målhardware.

### Forklaring af nøgleparametre

- `export=True`: Konverterer modellen til OpenVINO IR-format
- `compile=False`: Udsætter kompilering til runtime for fleksibilitet
- `device`: Målhardware ("CPU", "GPU", "AUTO" for automatisk valg)
- `save_pretrained()`: Gemmer den optimerede model til genbrug

## Eksempel: Konvertering og optimering af modeller med OpenVINO

### Trin 1: Modelkonvertering med NNCF-kvantisering

Sådan anvender du kvantisering efter træning med NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Trin 2: Avanceret optimering med vægtkomprimering

For transformerbaserede modeller kan du anvende vægtkomprimering:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Trin 3: Inferens med optimeret model

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Outputstruktur

Efter optimering vil din modelmappe indeholde:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Avanceret brug

### Konfiguration med NNCF YAML

Til komplekse optimeringsarbejdsgange kan du bruge NNCF-konfigurationsfiler:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Anvend konfigurationen:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-optimering

Til GPU-acceleration:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimering af batchbehandling

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Implementering af modelserver

Implementer optimerede modeller med OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Klientkode til modelserver:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Bedste praksis

### 1. Modelvalg og forberedelse
- Brug modeller fra understøttede frameworks (PyTorch, TensorFlow, ONNX)
- Sørg for, at modelinput har faste eller kendte dynamiske former
- Test med repræsentative datasæt til kalibrering

### 2. Valg af optimeringsstrategi
- **Kvantisering efter træning**: Start her for hurtig optimering
- **Vægtkomprimering**: Ideelt til store sprogmodeller og transformere
- **Kvantisering med træningsbevidsthed**: Bruges, når nøjagtighed er kritisk

### 3. Hardware-specifik optimering
- **CPU**: Brug INT8-kvantisering for balanceret ydeevne
- **GPU**: Udnyt FP16-præcision og batchbehandling
- **VPU**: Fokusér på modelsimplificering og lagfusion

### 4. Ydeevnetuning
- **Throughput Mode**: Til batchbehandling med høj volumen
- **Latency Mode**: Til interaktive applikationer i realtid
- **AUTO Device**: Lad OpenVINO vælge optimal hardware

### 5. Hukommelsesstyring
- Brug dynamiske former med omtanke for at undgå hukommelsesoverhead
- Implementer modelcaching for hurtigere efterfølgende indlæsninger
- Overvåg hukommelsesforbrug under optimering

### 6. Validering af nøjagtighed
- Valider altid optimerede modeller mod original ydeevne
- Brug repræsentative testdatasæt til evaluering
- Overvej gradvis optimering (start med konservative indstillinger)

## Fejlfinding

### Almindelige problemer

#### 1. Installationsproblemer
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Fejl ved modelkonvertering
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Ydeevneproblemer
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Hukommelsesproblemer
- Reducer modelbatchstørrelse under optimering
- Brug streaming til store datasæt
- Aktiver modelcaching: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Nøjagtighedsforringelse
- Brug højere præcision (INT8 i stedet for INT4)
- Øg kalibreringsdatasættets størrelse
- Anvend optimering med blandet præcision

### Overvågning af ydeevne

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Få hjælp

- **Dokumentation**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community Forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Yderligere ressourcer

### Officielle links
- **OpenVINO Hjemmeside**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Læringsressourcer
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Hurtig startguide**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimeringsguide**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integration Tools
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Ydeevnebenchmarks
- **Officielle benchmarks**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Eksempler fra fællesskabet
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Omfattende vejledninger tilgængelige i OpenVINO-notebooks repository
- **Eksempelapplikationer**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Virkelige eksempler til forskellige domæner (computer vision, NLP, lyd)
- **Blogindlæg**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI og fællesskabsblogindlæg med detaljerede anvendelsestilfælde

### Relaterede værktøjer
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Yderligere optimeringsteknikker til Intel-hardware
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Til sammenligninger af mobil- og edge-implementering
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativer til tværplatforms inferensmotorer

## ➡️ Hvad er næste skridt

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at opnå nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.