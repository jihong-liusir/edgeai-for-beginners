<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T20:00:32+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "da"
}
-->
# Afsnit 3: Microsoft Olive Optimization Suite

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Hurtig Start Guide](../../../Module04)
5. [Eksempel: Konvertering af Qwen3 til ONNX INT4](../../../Module04)
6. [Avanceret Brug](../../../Module04)
7. [Bedste Fremgangsmåder](../../../Module04)
8. [Fejlfinding](../../../Module04)
9. [Yderligere Ressourcer](../../../Module04)

## Introduktion

Microsoft Olive er et kraftfuldt og brugervenligt hardware-bevidst værktøj til modeloptimering, der gør det nemt at optimere maskinlæringsmodeller til implementering på forskellige hardwareplatforme. Uanset om du arbejder med CPU'er, GPU'er eller specialiserede AI-acceleratorer, hjælper Olive dig med at opnå optimal ydeevne, samtidig med at modelnøjagtigheden bevares.

## Hvad er Microsoft Olive?

Olive er et brugervenligt værktøj til hardware-bevidst modeloptimering, der samler førende teknikker inden for modelkomprimering, optimering og kompilering. Det fungerer med ONNX Runtime som en E2E-løsning til optimering af inferens.

### Nøglefunktioner

- **Hardware-bevidst optimering**: Vælger automatisk de bedste optimeringsteknikker til din målhardware
- **40+ indbyggede optimeringskomponenter**: Dækker modelkomprimering, kvantisering, grafoptimering og mere
- **Nem CLI-grænseflade**: Enkle kommandoer til almindelige optimeringsopgaver
- **Understøttelse af flere frameworks**: Fungerer med PyTorch, Hugging Face-modeller og ONNX
- **Understøttelse af populære modeller**: Olive kan automatisk optimere populære modelarkitekturer som Llama, Phi, Qwen, Gemma osv. direkte fra boksen

### Fordele

- **Reduceret udviklingstid**: Ingen behov for manuel eksperimentering med forskellige optimeringsteknikker
- **Ydeevneforbedringer**: Betydelige hastighedsforbedringer (op til 6x i nogle tilfælde)
- **Platformuafhængig implementering**: Optimerede modeller fungerer på tværs af forskellige hardware og operativsystemer
- **Bevaret nøjagtighed**: Optimeringer bevarer modelkvaliteten, mens ydeevnen forbedres

## Installation

### Forudsætninger

- Python 3.8 eller nyere
- pip-pakkemanager
- Virtuelt miljø (anbefales)

### Grundlæggende installation

Opret og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installer Olive med auto-optimeringsfunktioner:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valgfrie afhængigheder

Olive tilbyder forskellige valgfrie afhængigheder for ekstra funktioner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Bekræft installation

```bash
olive --help
```

Hvis installationen lykkes, bør du se Olive CLI-hjælpebeskeden.

## Hurtig Start Guide

### Din første optimering

Lad os optimere en lille sprogmodel ved hjælp af Olives auto-optimeringsfunktion:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hvad denne kommando gør

Optimeringsprocessen involverer: at hente modellen fra den lokale cache, fange ONNX-grafen og gemme vægtene i en ONNX-datafil, optimere ONNX-grafen og kvantisere modellen til int4 ved hjælp af RTN-metoden.

### Forklaring af kommandoparametre

- `--model_name_or_path`: Hugging Face-modelidentifikator eller lokal sti
- `--output_path`: Mappe, hvor den optimerede model gemmes
- `--device`: Målhardware (cpu, gpu)
- `--provider`: Eksekveringsudbyder (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Brug ONNX Runtime Generate AI til inferens
- `--precision`: Kvantiseringspræcision (int4, int8, fp16)
- `--log_level`: Logningsdetaljeringsgrad (0=minimal, 1=detaljeret)

## Eksempel: Konvertering af Qwen3 til ONNX INT4

Baseret på det angivne Hugging Face-eksempel på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), her er hvordan man optimerer en Qwen3-model:

### Trin 1: Download model (valgfrit)

For at minimere downloadtid, cache kun nødvendige filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Trin 2: Optimer Qwen3-model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Trin 3: Test den optimerede model

Opret et simpelt Python-script til at teste din optimerede model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Outputstruktur

Efter optimering vil din outputmappe indeholde:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avanceret Brug

### Konfigurationsfiler

For mere komplekse optimeringsarbejdsgange kan du bruge JSON-konfigurationsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kør med konfiguration:

```bash
olive run --config config.json
```

### GPU-optimering

For CUDA GPU-optimering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

For DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive understøtter også finjustering af modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bedste Fremgangsmåder

### 1. Modelvalg
- Start med mindre modeller til test (f.eks. 0.5B-7B parametre)
- Sørg for, at din målmodelarkitektur understøttes af Olive

### 2. Hardwareovervejelser
- Match din optimeringsmål med din implementeringshardware
- Brug GPU-optimering, hvis du har CUDA-kompatibel hardware
- Overvej DirectML til Windows-maskiner med integreret grafik

### 3. Præcisionsvalg
- **INT4**: Maksimal komprimering, let tab af nøjagtighed
- **INT8**: God balance mellem størrelse og nøjagtighed
- **FP16**: Minimal tab af nøjagtighed, moderat størrelsesreduktion

### 4. Test og validering
- Test altid optimerede modeller med dine specifikke anvendelsesscenarier
- Sammenlign ydeevnemålinger (latens, gennemløb, nøjagtighed)
- Brug repræsentative inputdata til evaluering

### 5. Iterativ optimering
- Start med auto-optimering for hurtige resultater
- Brug konfigurationsfiler til finjusteret kontrol
- Eksperimenter med forskellige optimeringspasser

## Fejlfinding

### Almindelige problemer

#### 1. Installationsproblemer
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problemer
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Hukommelsesproblemer
- Brug mindre batchstørrelser under optimering
- Prøv kvantisering med højere præcision først (int8 i stedet for int4)
- Sørg for tilstrækkelig diskplads til modelcaching

#### 4. Problemer med modelloading
- Bekræft modelsti og adgangstilladelser
- Tjek om modellen kræver `trust_remote_code=True`
- Sørg for, at alle nødvendige modelfiler er downloadet

### Få hjælp

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Eksempler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Yderligere Ressourcer

### Officielle links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Eksempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Eksempler
- **Jupyter Notebooks**: Tilgængelige i Olive GitHub-repositoriet — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit for VS Code oversigt — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogindlæg**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Relaterede værktøjer
- **ONNX Runtime**: Højtydende inferensmotor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Kilde til mange kompatible modeller — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-baserede optimeringsarbejdsgange — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Hvad er næste skridt

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

