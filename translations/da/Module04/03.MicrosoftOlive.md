<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T10:21:36+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "da"
}
-->
# Sektion 3: Microsoft Olive Optimeringssuite

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Hurtig startguide](../../../Module04)
5. [Eksempel: Konvertering af Qwen3 til ONNX INT4](../../../Module04)
6. [Avanceret brug](../../../Module04)
7. [Bedste praksis](../../../Module04)
8. [Fejlfinding](../../../Module04)
9. [Yderligere ressourcer](../../../Module04)

## Introduktion

Microsoft Olive er et kraftfuldt og brugervenligt hardwarebevidst værktøj til modeloptimering, der gør det nemt at optimere maskinlæringsmodeller til implementering på forskellige hardwareplatforme. Uanset om du arbejder med CPU'er, GPU'er eller specialiserede AI-acceleratorer, hjælper Olive dig med at opnå optimal ydeevne, samtidig med at modelnøjagtigheden bevares.

## Hvad er Microsoft Olive?

Olive er et brugervenligt værktøj til hardwarebevidst modeloptimering, der samler førende teknikker inden for modelkomprimering, optimering og kompilering. Det fungerer sammen med ONNX Runtime som en E2E-løsning til optimering af inferens.

### Nøglefunktioner

- **Hardwarebevidst optimering**: Vælger automatisk de bedste optimeringsteknikker til din målhardware
- **40+ indbyggede optimeringskomponenter**: Dækker modelkomprimering, kvantisering, grafoptimering og mere
- **Nem CLI-grænseflade**: Enkle kommandoer til almindelige optimeringsopgaver
- **Understøttelse af flere frameworks**: Fungerer med PyTorch, Hugging Face-modeller og ONNX
- **Understøttelse af populære modeller**: Olive kan automatisk optimere populære modelarkitekturer som Llama, Phi, Qwen, Gemma osv. direkte

### Fordele

- **Reduceret udviklingstid**: Ingen behov for manuel eksperimentering med forskellige optimeringsteknikker
- **Ydeevneforbedringer**: Betydelige hastighedsforbedringer (op til 6x i nogle tilfælde)
- **Platformuafhængig implementering**: Optimerede modeller fungerer på tværs af forskellige hardware og operativsystemer
- **Bevaret nøjagtighed**: Optimeringer bevarer modelkvaliteten, mens ydeevnen forbedres

## Installation

### Forudsætninger

- Python 3.8 eller nyere
- pip-pakkestyring
- Virtuelt miljø (anbefales)

### Grundlæggende installation

Opret og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installer Olive med funktioner til automatisk optimering:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valgfrie afhængigheder

Olive tilbyder forskellige valgfrie afhængigheder for ekstra funktioner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Bekræft installation

```bash
olive --help
```

Hvis installationen lykkes, bør du se Olive CLI-hjælpebeskeden.

## Hurtig startguide

### Din første optimering

Lad os optimere en lille sprogmodel ved hjælp af Olives funktion til automatisk optimering:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hvad denne kommando gør

Optimeringsprocessen omfatter: hentning af modellen fra den lokale cache, indfangning af ONNX-grafen og lagring af vægte i en ONNX-datafil, optimering af ONNX-grafen og kvantisering af modellen til int4 ved hjælp af RTN-metoden.

### Forklaring af kommandoparametre

- `--model_name_or_path`: Hugging Face-modelidentifikator eller lokal sti
- `--output_path`: Mappe, hvor den optimerede model gemmes
- `--device`: Målhardware (cpu, gpu)
- `--provider`: Eksekveringsudbyder (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Brug ONNX Runtime Generate AI til inferens
- `--precision`: Kvantiseringspræcision (int4, int8, fp16)
- `--log_level`: Logningsdetaljeringsgrad (0=minimal, 1=detaljeret)

## Eksempel: Konvertering af Qwen3 til ONNX INT4

Baseret på det angivne Hugging Face-eksempel på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), her er hvordan man optimerer en Qwen3-model:

### Trin 1: Download model (valgfrit)

For at minimere downloadtid, cache kun nødvendige filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Trin 2: Optimer Qwen3-model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Trin 3: Test den optimerede model

Opret et simpelt Python-script til at teste din optimerede model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Outputstruktur

Efter optimering vil din outputmappe indeholde:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avanceret brug

### Konfigurationsfiler

Til mere komplekse optimeringsarbejdsgange kan du bruge JSON-konfigurationsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kør med konfiguration:

```bash
olive run --config config.json
```

### GPU-optimering

Til CUDA GPU-optimering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Til DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive understøtter også finjustering af modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bedste praksis

### 1. Modelvalg
- Start med mindre modeller til test (f.eks. 0.5B-7B parametre)
- Sørg for, at din målmodelarkitektur understøttes af Olive

### 2. Hardwareovervejelser
- Match din optimeringsmål med din implementeringshardware
- Brug GPU-optimering, hvis du har CUDA-kompatibel hardware
- Overvej DirectML til Windows-maskiner med integreret grafik

### 3. Præcisionsvalg
- **INT4**: Maksimal komprimering, let tab af nøjagtighed
- **INT8**: God balance mellem størrelse og nøjagtighed
- **FP16**: Minimal tab af nøjagtighed, moderat størrelsesreduktion

### 4. Test og validering
- Test altid optimerede modeller med dine specifikke anvendelsesscenarier
- Sammenlign ydeevnemålinger (latens, gennemløb, nøjagtighed)
- Brug repræsentative inputdata til evaluering

### 5. Iterativ optimering
- Start med automatisk optimering for hurtige resultater
- Brug konfigurationsfiler til finjusteret kontrol
- Eksperimenter med forskellige optimeringspasser

## Fejlfinding

### Almindelige problemer

#### 1. Installationsproblemer
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problemer
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Hukommelsesproblemer
- Brug mindre batchstørrelser under optimering
- Prøv kvantisering med højere præcision først (int8 i stedet for int4)
- Sørg for tilstrækkelig diskplads til modelcaching

#### 4. Problemer med modelloading
- Bekræft modelsti og adgangstilladelser
- Tjek om modellen kræver `trust_remote_code=True`
- Sørg for, at alle nødvendige modelfiler er downloadet

### Få hjælp

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Eksempler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Yderligere ressourcer

### Officielle links
- **GitHub-repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime-dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-eksempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community-eksempler
- **Jupyter Notebooks**: Tilgængelige i Olive GitHub-repository
- **VS Code-udvidelse**: AI Toolkit-udvidelsen bruger Olive til modeloptimering
- **Blogindlæg**: Microsoft Open Source Blog har detaljerede Olive-tutorials

### Relaterede værktøjer
- **ONNX Runtime**: Højtydende inferensmotor
- **Hugging Face Transformers**: Kilde til mange kompatible modeller
- **Azure Machine Learning**: Cloud-baserede optimeringsarbejdsgange

## ➡️ Hvad er næste skridt

- [04: OpenVINO Toolkit Optimeringssuite](./04.openvino.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.