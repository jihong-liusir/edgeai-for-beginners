<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T20:21:56+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "da"
}
-->
# Session 3: Open-Source Modeller med Foundry Local

## Oversigt

Denne session handler om, hvordan man bringer open-source modeller til Foundry Local: valg af community-modeller, integration af Hugging Face-indhold og brug af "bring your own model" (BYOM)-strategier. Du vil også blive introduceret til Model Mondays-serien for løbende læring og opdagelse af modeller.

Referencer:
- Foundry Local-dokumentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilér Hugging Face-modeller: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Læringsmål
- Opdag og vurder open-source modeller til lokal inferens
- Kompilér og kør udvalgte Hugging Face-modeller i Foundry Local
- Anvend strategier for modelvalg baseret på nøjagtighed, latenstid og ressourcebehov
- Administrér modeller lokalt med cache og versionering

## Del 1: Modelopdagelse og -valg (Trin-for-trin)

Trin 1) List tilgængelige modeller i den lokale katalog
```cmd
foundry model list
```

Trin 2) Prøv hurtigt to kandidater (auto-download ved første kørsel)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

Trin 3) Notér grundlæggende metrics
- Observer latenstid (subjektivt) og kvalitet for en fast prompt
- Hold øje med hukommelsesforbrug via Task Manager, mens hver model kører

## Del 2: Kør katalogmodeller via CLI (Trin-for-trin)

Trin 1) Start en model
```cmd
foundry model run llama-3.2
```

Trin 2) Send en testprompt via OpenAI-kompatibel endpoint
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## Del 3: BYOM – Kompilér Hugging Face-modeller (Trin-for-trin)

Følg den officielle vejledning til kompilering af modeller. Overordnet flow nedenfor—se Microsoft Learn-artiklen for præcise kommandoer og understøttede konfigurationer.

Trin 1) Forbered en arbejdsmappe
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

Trin 2) Kompilér en understøttet HF-model
- Brug trinnene fra Learn-dokumentationen til at konvertere og placere den kompilerede ONNX-model i din `models`-mappe
- Bekræft med:
```cmd
foundry cache ls
```
Du bør se navnet på din kompilerede model (for eksempel `llama-3.2`).

Trin 3) Kør den kompilerede model
```cmd
foundry model run llama-3.2 --verbose
```

Noter:
- Sørg for tilstrækkelig diskplads og RAM til kompilering og kørsel
- Start med mindre modeller for at validere flowet, og skaler derefter op

## Del 4: Praktisk modelkuratering (Trin-for-trin)

Trin 1) Opret en `models.json`-registrering
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

Trin 2) Lille selektionsscript
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## Del 5: Praktiske benchmarks (Trin-for-trin)

Trin 1) Enkel latenstidsbenchmark
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

Trin 2) Kvalitetstjek
- Brug et fast prompt-sæt, og gem output i en CSV/JSON
- Bedøm manuelt flydende, relevans og korrekthed (1–5)

## Del 6: Næste skridt
- Abonner på Model Mondays for nye modeller og tips: https://aka.ms/model-mondays
- Bidrag med fund til dit teams `models.json`
- Forbered dig på Session 4: sammenligning af LLMs vs SLMs, lokal vs cloud-inferens og praktiske demoer

---

