<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:36:49+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "da"
}
-->
# Session 3: Opdagelse og Administration af Open-Source Modeller

## Oversigt

Denne session fokuserer på praktisk opdagelse og administration af modeller med Foundry Local. Du vil lære at liste tilgængelige modeller, teste forskellige muligheder og forstå grundlæggende præstationskarakteristika. Tilgangen lægger vægt på praktisk udforskning med foundry CLI for at hjælpe dig med at vælge de rette modeller til dine anvendelser.

## Læringsmål

- Mestre foundry CLI-kommandoer til opdagelse og administration af modeller
- Forstå modelcache og lokale lagringsmønstre
- Lære at teste og sammenligne forskellige modeller hurtigt
- Etablere praktiske arbejdsgange til modelvalg og benchmarking
- Udforske det voksende økosystem af modeller tilgængelige via Foundry Local

## Forudsætninger

- Gennemført Session 1: Kom godt i gang med Foundry Local
- Foundry Local CLI installeret og tilgængelig
- Tilstrækkelig lagerplads til modeldownloads (modeller kan variere fra 1GB til 20GB+)
- Grundlæggende forståelse af modeltyper og anvendelser

## Del 6: Praktisk Øvelse

### Øvelse: Opdagelse og Sammenligning af Modeller

Opret dit eget script til modelvurdering baseret på Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Din Opgave

1. **Kør Sample 03-scriptet**: `samples\03\list_and_bench.cmd`
2. **Prøv forskellige modeller**: Test mindst 3 forskellige modeller
3. **Sammenlign præstation**: Notér forskelle i hastighed og svarkvalitet
4. **Dokumentér resultater**: Opret et simpelt sammenligningsdiagram

### Eksempel på Sammenligningsformat

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Del 7: Fejlfinding og Bedste Fremgangsmåder

### Almindelige Problemer og Løsninger

**Model Starter Ikke:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Utilstrækkelig Hukommelse:**
- Start med mindre modeller (`phi-4-mini`)
- Luk andre applikationer
- Opgrader RAM, hvis du ofte rammer grænser

**Langsom Præstation:**
- Sørg for, at modellen er fuldt indlæst (tjek detaljeret output)
- Luk unødvendige baggrundsapplikationer
- Overvej hurtigere lager (SSD)

### Bedste Fremgangsmåder

1. **Start Småt**: Begynd med `phi-4-mini` for at validere opsætningen
2. **En Model ad Gangen**: Stop tidligere modeller, før du starter nye
3. **Overvåg Ressourcer**: Hold øje med hukommelsesforbrug
4. **Test Konsistent**: Brug de samme prompts for retfærdige sammenligninger
5. **Dokumentér Resultater**: Notér modelpræstation for dine anvendelser

## Del 8: Næste Skridt og Referencer

### Forberedelse til Session 4

- **Fokus for Session 4**: Optimeringsværktøjer og -teknikker
- **Forudsætninger**: Komfortabel med modelskift og grundlæggende præstationstest
- **Anbefalet**: Identificér 2-3 favoritmodeller fra denne session

### Yderligere Ressourcer

- **[Foundry Local Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Officiel dokumentation
- **[CLI Reference](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Komplet kommandooversigt
- **[Model Mondays](https://aka.ms/model-mondays)**: Ugentlige modelfremhævelser
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Fællesskab og problemer
- **[Sample 03: Model Discovery](samples/03/README.md)**: Praktisk eksempelscript

### Vigtige Pointer

✅ **Modelopdagelse**: Brug `foundry model list` til at udforske tilgængelige modeller  
✅ **Hurtig Testning**: Brug mønsteret `list_and_bench.cmd` til hurtig evaluering  
✅ **Præstationsovervågning**: Grundlæggende ressourceforbrug og responstid  
✅ **Modelvalg**: Praktiske retningslinjer for valg af modeller efter anvendelse  
✅ **Cacheadministration**: Forståelse af lagring og oprydningsprocedurer  

Du har nu de praktiske færdigheder til at opdage, teste og vælge passende modeller til dine AI-applikationer ved hjælp af Foundry Locals enkle CLI-tilgang.

## Læringsmål

- Opdag og vurder open-source modeller til lokal inferens
- Kompiler og kør udvalgte Hugging Face-modeller inden for Foundry Local
- Anvend strategier for modelvalg baseret på nøjagtighed, latenstid og ressourcebehov
- Administrér modeller lokalt med cache og versionering

## Del 1: Modelopdagelse med Foundry CLI

### Grundlæggende Kommandoer til Modeladministration

Foundry CLI tilbyder enkle kommandoer til opdagelse og administration af modeller:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Kørsel af Dine Første Modeller

Start med populære, veltestede modeller for at forstå præstationskarakteristika:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Bemærk:** Flaget `--verbose` giver detaljeret opstartsinformation, herunder:
- Modeldownloadstatus (ved første kørsel)
- Hukommelsesallokeringsdetaljer
- Servicebindingsinformation
- Præstationsinitialiseringsmålinger

### Forståelse af Modelkategorier

**Små Sproglige Modeller (SLMs):**
- `phi-4-mini`: Hurtig, effektiv, god til generel chat
- `phi-4`: Mere kapabel version med bedre ræsonnement

**Mellemstore Modeller:**
- `qwen2.5-7b`: Fremragende ræsonnement og længere kontekst
- `deepseek-r1-7b`: Optimeret til kodegenerering

**Større Modeller:**
- `llama-3.2`: Metas nyeste open-source model
- `qwen2.5-14b`: Enterprise-grade ræsonnement

## Del 2: Hurtig Testning og Sammenligning af Modeller

### Sample 03 Tilgang: Enkel Liste og Benchmark

Baseret på vores Sample 03-mønster er her den minimale arbejdsgang:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testning af Modelpræstation

Når en model kører, test den med konsistente prompts:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell Testningsalternativ

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Del 3: Cache- og Lagringsadministration af Modeller

### Forståelse af Modelcache

Foundry Local administrerer automatisk modeldownloads og caching:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Overvejelser om Modellagring

**Typiske Modelstørrelser:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Bedste Lagringspraksis:**
- Hold 2-3 modeller cachet for hurtig skift
- Fjern ubrugte modeller for at frigøre plads: `foundry cache clean`
- Overvåg diskforbrug, især på mindre SSD'er
- Overvej kompromis mellem modelstørrelse og kapabilitet

### Overvågning af Modelpræstation

Mens modeller kører, overvåg systemressourcer:

**Windows Task Manager:**
- Hold øje med hukommelsesforbrug (modeller forbliver indlæst i RAM)
- Overvåg CPU-udnyttelse under inferens
- Tjek disk I/O under initial modelindlæsning

**Kommandolinjeovervågning:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Del 4: Praktiske Retningslinjer for Modelvalg

### Valg af Modeller efter Anvendelse

**Til Generel Chat og Q&A:**
- Start med: `phi-4-mini` (hurtig, effektiv)
- Opgrader til: `phi-4` (bedre ræsonnement)
- Avanceret: `qwen2.5-7b` (længere kontekst)

**Til Kodegenerering:**
- Anbefalet: `deepseek-r1-7b`
- Alternativ: `qwen2.5-7b` (også god til kode)

**Til Kompleks Ræsonnement:**
- Bedst: `qwen2.5-7b` eller `qwen2.5-14b`
- Budgetmulighed: `phi-4`

### Hardwarekrav Guide

**Minimum Systemkrav:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Anbefalet for Bedste Præstation:**
- 32GB+ RAM for komfortabelt multi-model skift
- SSD-lagring for hurtigere modelindlæsning
- Moderne CPU med god single-thread præstation
- NPU-understøttelse (Windows 11 Copilot+ PC'er) for acceleration

### Arbejdsgang for Modelskift

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Del 5: Enkel Benchmarking af Modeller

### Grundlæggende Præstationstest

Her er en ligetil tilgang til at sammenligne modelpræstation:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuel Kvalitetsvurdering

For hver model, test med konsistente prompts og vurder manuelt:

**Testprompts:**
1. "Forklar kvantecomputing på en enkel måde."
2. "Skriv en Python-funktion til at sortere en liste."
3. "Hvad er fordele og ulemper ved fjernarbejde?"
4. "Opsummer fordelene ved edge AI."

**Vurderingskriterier:**
- **Nøjagtighed**: Er informationen korrekt?
- **Klarhed**: Er forklaringen let at forstå?
- **Fuldstændighed**: Besvarer den hele spørgsmålet?
- **Hastighed**: Hvor hurtigt svarer den?

### Overvågning af Ressourceforbrug

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Del 6: Næste Skridt
- Abonner på Model Mondays for nye modeller og tips: https://aka.ms/model-mondays
- Bidrag med resultater til dit teams `models.json`
- Forbered dig til Session 4: sammenligning af LLMs vs SLMs, lokal vs cloud inferens og praktiske demoer

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal det bemærkes, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.