<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:32:07+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "da"
}
-->
# Sektion 3: Praktisk Implementeringsguide

## Oversigt

Denne omfattende guide hjælper dig med at forberede dig til EdgeAI-kurset, som fokuserer på at bygge praktiske AI-løsninger, der kører effektivt på edge-enheder. Kurset lægger vægt på praktisk udvikling ved hjælp af moderne frameworks og avancerede modeller optimeret til edge-implementering.

## 1. Opsætning af udviklingsmiljø

### Programmeringssprog & Frameworks

**Python-miljø**
- **Version**: Python 3.10 eller nyere (anbefalet: Python 3.11)
- **Pakkehåndtering**: pip eller conda
- **Virtuelt miljø**: Brug venv eller conda-miljøer for isolation
- **Nøglebiblioteker**: Vi installerer specifikke EdgeAI-biblioteker under kurset

**Microsoft .NET-miljø**
- **Version**: .NET 8 eller nyere
- **IDE**: Visual Studio 2022, Visual Studio Code eller JetBrains Rider
- **SDK**: Sørg for, at .NET SDK er installeret til tværplatformsudvikling

### Udviklingsværktøjer

**Kodeeditorer & IDE'er**
- Visual Studio Code (anbefales til tværplatformsudvikling)
- PyCharm eller Visual Studio (til sprog-specifik udvikling)
- Jupyter Notebooks til interaktiv udvikling og prototyping

**Versionskontrol**
- Git (seneste version)
- GitHub-konto til adgang til repositories og samarbejde

## 2. Hardwarekrav & Anbefalinger

### Minimum systemkrav
- **CPU**: Multi-core processor (Intel i5/AMD Ryzen 5 eller tilsvarende)
- **RAM**: Minimum 8GB, anbefalet 16GB
- **Lagring**: 50GB ledig plads til modeller og udviklingsværktøjer
- **OS**: Windows 10/11, macOS 10.15+ eller Linux (Ubuntu 20.04+)

### Strategi for beregningsressourcer
Kurset er designet til at være tilgængeligt på forskellige hardwarekonfigurationer:

**Lokal udvikling (CPU/NPU-fokus)**
- Primær udvikling vil bruge CPU og NPU-acceleration
- Velegnet til de fleste moderne laptops og desktops
- Fokus på effektivitet og praktiske implementeringsscenarier

**Cloud GPU-ressourcer (valgfrit)**
- **Azure Machine Learning**: Til intensiv træning og eksperimentering
- **Google Colab**: Gratis niveau tilgængeligt til uddannelsesformål
- **Kaggle Notebooks**: Alternativ cloud computing-platform

### Overvejelser for edge-enheder
- Forståelse af ARM-baserede processorer
- Kendskab til hardwarebegrænsninger for mobile og IoT-enheder
- Erfaring med optimering af strømforbrug

## 3. Centrale modelfamilier & ressourcer

### Primære modelfamilier

**Microsoft Phi-4 Family**
- **Beskrivelse**: Kompakte, effektive modeller designet til edge-implementering
- **Styrker**: Fremragende forhold mellem ydeevne og størrelse, optimeret til ræsonnement
- **Ressource**: [Phi-4 Collection på Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Anvendelser**: Kodegenerering, matematisk ræsonnement, generel samtale

**Qwen-3 Family**
- **Beskrivelse**: Alibabas nyeste generation af flersprogede modeller
- **Styrker**: Stærke flersprogede kapaciteter, effektiv arkitektur
- **Ressource**: [Qwen-3 Collection på Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Anvendelser**: Flersprogede applikationer, tværkulturelle AI-løsninger

**Google Gemma-3n Family**
- **Beskrivelse**: Googles letvægtsmodeller optimeret til edge-implementering
- **Styrker**: Hurtig inferens, mobilvenlig arkitektur
- **Ressource**: [Gemma-3n Collection på Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Anvendelser**: Mobilapplikationer, realtidsbehandling

### Kriterier for modelvalg
- **Ydeevne vs. størrelse**: Forståelse af hvornår man skal vælge mindre vs. større modeller
- **Optimering til specifikke opgaver**: Match modeller til specifikke anvendelser
- **Implementeringsbegrænsninger**: Hukommelse, latenstid og strømforbrug

## 4. Kvantisering & optimeringsværktøjer

### Llama.cpp Framework
- **Repository**: [Llama.cpp på GitHub](https://github.com/ggml-org/llama.cpp)
- **Formål**: Højtydende inferensmotor til LLM'er
- **Nøglefunktioner**:
  - CPU-optimeret inferens
  - Flere kvantiseringsformater (Q4, Q5, Q8)
  - Tværplatformskompatibilitet
  - Hukommelseseffektiv eksekvering
- **Installation og grundlæggende brug**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repository**: [Microsoft Olive på GitHub](https://github.com/microsoft/olive)
- **Formål**: Modeloptimeringsværktøj til edge-implementering
- **Nøglefunktioner**:
  - Automatiserede modeloptimeringsarbejdsgange
  - Hardwarebevidst optimering
  - Integration med ONNX Runtime
  - Værktøjer til ydeevnebenchmarking
- **Installation og grundlæggende brug**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # Eksempel på Python-script til modeloptimering
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS-brugere)
- **Repository**: [Apple MLX på GitHub](https://github.com/ml-explore/mlx)
- **Formål**: Maskinlæringsframework til Apple Silicon
- **Nøglefunktioner**:
  - Native optimering til Apple Silicon
  - Hukommelseseffektive operationer
  - PyTorch-lignende API
  - Understøttelse af unified memory architecture
- **Installation og grundlæggende brug**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repository**: [ONNX Runtime på GitHub](https://github.com/microsoft/onnxruntime)
- **Formål**: Tværplatforms inferensacceleration for ONNX-modeller
- **Nøglefunktioner**:
  - Hardware-specifikke optimeringer (CPU, GPU, NPU)
  - Grafoptimeringer til inferens
  - Understøttelse af kvantisering
  - Tværsprogunderstøttelse (Python, C++, C#, JavaScript)
- **Installation og grundlæggende brug**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. Anbefalet læsning & ressourcer

### Væsentlig dokumentation
- **ONNX Runtime Dokumentation**: Forståelse af tværplatforms inferens
- **Hugging Face Transformers Guide**: Modelloading og inferens
- **Edge AI Design Patterns**: Best practices for edge-implementering

### Tekniske artikler
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Fællesskabsressourcer
- **EdgeAI Slack/Discord Communities**: Peer support og diskussion
- **GitHub Repositories**: Eksempelimplementeringer og tutorials
- **YouTube-kanaler**: Tekniske dybdegående videoer og tutorials

## 6. Vurdering & Verifikation

### Tjekliste før kursus
- [ ] Python 3.10+ installeret og verificeret
- [ ] .NET 8+ installeret og verificeret
- [ ] Udviklingsmiljø konfigureret
- [ ] Hugging Face-konto oprettet
- [ ] Grundlæggende kendskab til målmodelfamilier
- [ ] Kvantiseringsværktøjer installeret og testet
- [ ] Hardwarekrav opfyldt
- [ ] Cloud computing-konti oprettet (hvis nødvendigt)

## Centrale læringsmål

Ved afslutningen af denne guide vil du kunne:

1. Opsætte et komplet udviklingsmiljø til EdgeAI-applikationsudvikling
2. Installere og konfigurere de nødvendige værktøjer og frameworks til modeloptimering
3. Vælge passende hardware- og softwarekonfigurationer til dine EdgeAI-projekter
4. Forstå de vigtigste overvejelser for implementering af AI-modeller på edge-enheder
5. Forberede dit system til de praktiske øvelser i kurset

## Yderligere ressourcer

### Officiel dokumentation
- **Python Dokumentation**: Officiel dokumentation for Python-sproget
- **Microsoft .NET Dokumentation**: Officielle .NET-udviklingsressourcer
- **ONNX Runtime Dokumentation**: Omfattende guide til ONNX Runtime
- **TensorFlow Lite Dokumentation**: Officiel dokumentation for TensorFlow Lite

### Udviklingsværktøjer
- **Visual Studio Code**: Letvægtskodeeditor med AI-udviklingsudvidelser
- **Jupyter Notebooks**: Interaktivt computermiljø til ML-eksperimenter
- **Docker**: Containerplatform til konsistente udviklingsmiljøer
- **Git**: Versionskontrolsystem til kodestyring

### Læringsressourcer
- **EdgeAI Forskningsartikler**: Seneste akademisk forskning om effektive modeller
- **Onlinekurser**: Supplerende læringsmaterialer om AI-optimering
- **Fællesskabsfora**: Q&A-platforme til EdgeAI-udviklingsudfordringer
- **Benchmark-datasæt**: Standarddatasæt til evaluering af modelpræstation

## Læringsresultater

Efter at have gennemført denne forberedelsesguide vil du:

1. Have et fuldt konfigureret udviklingsmiljø klar til EdgeAI-udvikling
2. Forstå hardware- og softwarekravene for forskellige implementeringsscenarier
3. Være bekendt med de vigtigste frameworks og værktøjer, der bruges i kurset
4. Kunne vælge passende modeller baseret på enhedens begrænsninger og krav
5. Have grundlæggende viden om optimeringsteknikker til edge-implementering

## ➡️ Hvad er det næste

- [04: EdgeAI Hardware og Implementering](04.EdgeDeployment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at sikre nøjagtighed, skal det bemærkes, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.