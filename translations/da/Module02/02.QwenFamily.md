<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T09:19:28+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "da"
}
-->
# Afsnit 2: Qwen-familien Grundlæggende

Qwen-modelfamilien repræsenterer Alibaba Clouds omfattende tilgang til store sprogmodeller og multimodal AI, og viser, at open source-modeller kan opnå bemærkelsesværdig ydeevne, samtidig med at de er tilgængelige på tværs af forskellige implementeringsscenarier. Det er vigtigt at forstå, hvordan Qwen-familien muliggør kraftfulde AI-funktioner med fleksible implementeringsmuligheder, samtidig med at den opretholder konkurrencedygtig ydeevne på tværs af forskellige opgaver.

## Ressourcer til udviklere

### Hugging Face Model Repository
Udvalgte Qwen-familie modeller er tilgængelige via [Hugging Face](https://huggingface.co/models?search=qwen), hvilket giver adgang til nogle varianter af disse modeller. Du kan udforske de tilgængelige varianter, finjustere dem til dine specifikke anvendelser og implementere dem gennem forskellige frameworks.

### Lokale udviklingsværktøjer
Til lokal udvikling og test kan du bruge [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) til at køre tilgængelige Qwen-modeller på din udviklingsmaskine med optimeret ydeevne.

### Dokumentationsressourcer
- [Qwen Model Dokumentation](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Optimering af Qwen-modeller til Edge-implementering](https://github.com/microsoft/olive)

## Introduktion

I denne vejledning vil vi udforske Alibabas Qwen-modelfamilie og dens grundlæggende koncepter. Vi vil dække udviklingen af Qwen-familien, de innovative træningsmetoder, der gør Qwen-modeller effektive, nøglevarianter i familien og praktiske anvendelser på tværs af forskellige scenarier.

## Læringsmål

Ved afslutningen af denne vejledning vil du være i stand til at:

- Forstå designfilosofien og udviklingen af Alibabas Qwen-modelfamilie
- Identificere de vigtigste innovationer, der gør det muligt for Qwen-modeller at opnå høj ydeevne på tværs af forskellige parameterstørrelser
- Genkende fordelene og begrænsningerne ved forskellige Qwen-modelvarianter
- Anvende viden om Qwen-modeller til at vælge passende varianter til virkelige scenarier

## Forståelse af det moderne AI-model landskab

AI-landskabet har udviklet sig betydeligt, med forskellige organisationer, der forfølger forskellige tilgange til udvikling af sprogmodeller. Mens nogle fokuserer på proprietære lukkede modeller, lægger andre vægt på open source-tilgængelighed og gennemsigtighed. Den traditionelle tilgang involverer enten massive proprietære modeller, der kun er tilgængelige via API'er, eller open source-modeller, der kan halte bagefter i kapabiliteter.

Denne paradigme skaber udfordringer for organisationer, der søger kraftfulde AI-funktioner, samtidig med at de opretholder kontrol over deres data, omkostninger og implementeringsfleksibilitet. Den konventionelle tilgang kræver ofte et valg mellem banebrydende ydeevne og praktiske implementeringshensyn.

## Udfordringen med tilgængelig AI-ekspertise

Behovet for AI af høj kvalitet og tilgængelighed er blevet stadig vigtigere på tværs af forskellige scenarier. Overvej applikationer, der kræver fleksible implementeringsmuligheder for forskellige organisatoriske behov, omkostningseffektive løsninger, hvor API-omkostninger kan blive betydelige, flersprogede kapabiliteter til globale applikationer eller specialiseret domæneekspertise inden for områder som kodning og matematik.

### Nøglekrav til implementering

Moderne AI-implementeringer står over for flere grundlæggende krav, der begrænser praktisk anvendelighed:

- **Tilgængelighed**: Open source-tilgængelighed for gennemsigtighed og tilpasning
- **Omkostningseffektivitet**: Rimelige beregningskrav for forskellige budgetter
- **Fleksibilitet**: Flere modelstørrelser til forskellige implementeringsscenarier
- **Global rækkevidde**: Stærke flersprogede og tværkulturelle kapabiliteter
- **Specialisering**: Domænespecifikke varianter til særlige anvendelser

## Qwen-modellens filosofi

Qwen-modelfamilien repræsenterer en omfattende tilgang til AI-modeludvikling, der prioriterer open source-tilgængelighed, flersprogede kapabiliteter og praktisk implementering, samtidig med at den opretholder konkurrencedygtige ydeevneegenskaber. Qwen-modeller opnår dette gennem forskellige modelstørrelser, træningsmetoder af høj kvalitet og specialiserede varianter til forskellige domæner.

Qwen-familien omfatter forskellige tilgange designet til at give muligheder på tværs af ydeevne-effektivitet spektret, hvilket muliggør implementering fra mobile enheder til virksomhedens servere, samtidig med at der leveres meningsfulde AI-funktioner. Målet er at demokratisere adgangen til AI af høj kvalitet, samtidig med at der gives fleksibilitet i implementeringsvalg.

### Kerneprincipper for Qwen-design

Qwen-modeller er bygget på flere grundlæggende principper, der adskiller dem fra andre sprogmodelfamilier:

- **Open Source Først**: Fuld gennemsigtighed og tilgængelighed til forskning og kommerciel brug
- **Omfattende Træning**: Træning på massive, diverse datasæt, der dækker flere sprog og domæner
- **Skalerbar Arkitektur**: Flere modelstørrelser til at matche forskellige beregningskrav
- **Specialiseret Ekspertise**: Domænespecifikke varianter optimeret til særlige opgaver

## Nøgleteknologier, der muliggør Qwen-familien

### Træning i massiv skala

En af de definerende aspekter af Qwen-familien er den massive skala af træningsdata og beregningsressourcer, der investeres i modeludvikling. Qwen-modeller udnytter omhyggeligt kuraterede, flersprogede datasæt, der spænder over billioner af tokens, designet til at give omfattende verdensviden og ræsonnementsevner.

Denne tilgang kombinerer indhold af høj kvalitet fra internettet, akademisk litteratur, kode-repositorier og flersprogede ressourcer. Træningsmetoden lægger vægt på både bredde af viden og dybde af forståelse på tværs af forskellige domæner og sprog.

### Avanceret ræsonnement og tænkning

De nyeste Qwen-modeller inkorporerer sofistikerede ræsonnementsevner, der muliggør kompleks problemløsning i flere trin:

**Tænkemodus (Qwen3)**: Modeller kan engagere sig i detaljeret trin-for-trin ræsonnement, før de giver endelige svar, svarende til menneskelige problemløsningsmetoder.

**Dual-Mode Operation**: Evne til at skifte mellem hurtig svarmodus for enkle forespørgsler og dybere tænkning for komplekse problemer.

**Chain-of-Thought Integration**: Naturlig integration af ræsonnementstrin, der forbedrer gennemsigtighed og nøjagtighed i komplekse opgaver.

### Arkitektoniske innovationer

Qwen-familien inkorporerer flere arkitektoniske optimeringer designet til både ydeevne og effektivitet:

**Skalerbart Design**: Konsistent arkitektur på tværs af modelstørrelser, der muliggør nem skalering og sammenligning.

**Multimodal Integration**: Problemfri integration af tekst-, visions- og lydbehandlingsfunktioner inden for enhedlige arkitekturer.

**Implementeringsoptimering**: Flere kvantiseringsmuligheder og implementeringsformater til forskellige hardwarekonfigurationer.

## Modelstørrelse og implementeringsmuligheder

Moderne implementeringsmiljøer drager fordel af Qwen-modellers fleksibilitet på tværs af forskellige beregningskrav:

### Små modeller (0.5B-3B)

Qwen tilbyder effektive små modeller, der er velegnede til edge-implementering, mobile applikationer og miljøer med begrænsede ressourcer, samtidig med at de opretholder imponerende kapabiliteter.

### Mellemstore modeller (7B-32B)

Mellemstore modeller tilbyder forbedrede kapabiliteter til professionelle applikationer og giver en fremragende balance mellem ydeevne og beregningskrav.

### Store modeller (72B+)

Fuldskala modeller leverer state-of-the-art ydeevne til krævende applikationer, forskning og virksomhedens implementeringer, der kræver maksimal kapabilitet.

## Fordele ved Qwen-modelfamilien

### Open Source Tilgængelighed

Qwen-modeller giver fuld gennemsigtighed og tilpasningsmuligheder, hvilket gør det muligt for organisationer at forstå, modificere og tilpasse modeller til deres specifikke behov uden leverandørbinding.

### Implementeringsfleksibilitet

Rækken af modelstørrelser muliggør implementering på tværs af forskellige hardwarekonfigurationer, fra mobile enheder til avancerede servere, hvilket giver organisationer fleksibilitet i deres AI-infrastrukturvalg.

### Flersproget Ekspertise

Qwen-modeller udmærker sig i flersproget forståelse og generering, understøtter dusinvis af sprog med særlig styrke i engelsk og kinesisk, hvilket gør dem velegnede til globale applikationer.

### Konkurrencedygtig Ydeevne

Qwen-modeller opnår konsekvent konkurrencedygtige resultater på benchmarks, samtidig med at de giver open source-tilgængelighed, hvilket viser, at åbne modeller kan matche proprietære alternativer.

### Specialiserede Kapabiliteter

Domænespecifikke varianter som Qwen-Coder og Qwen-Math giver specialiseret ekspertise, samtidig med at de opretholder generel sprogforståelse.

## Praktiske eksempler og anvendelser

Før vi dykker ned i de tekniske detaljer, lad os udforske nogle konkrete eksempler på, hvad Qwen-modeller kan opnå:

### Eksempel på matematisk ræsonnement

Qwen-Math udmærker sig ved trin-for-trin matematisk problemløsning. For eksempel, når den bliver bedt om at løse et komplekst calculus-problem:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Eksempel på flersproget support

Qwen-modeller demonstrerer stærke flersprogede kapabiliteter på tværs af forskellige sprog:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Eksempel på multimodale kapabiliteter

Qwen-VL kan behandle både tekst og billeder samtidigt:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Eksempel på kodegenerering

Qwen-Coder udmærker sig ved at generere og forklare kode på tværs af flere programmeringssprog:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Udfør binær søgning på en sorteret liste for at finde målværdien.
    
    Args:
        arr (list): En sorteret liste af sammenlignelige elementer
        target: Værdien, der skal søges efter
        
    Returns:
        int: Indeks for mål, hvis fundet, -1 hvis ikke fundet
        
    Tidskompleksitet: O(log n)
    Pladskompleksitet: O(1)
    """
    # Initialiser venstre og højre pegepinde
    left, right = 0, len(arr) - 1
    
    # Fortsæt søgningen, mens søgeområdet er gyldigt
    while left <= right:
        # Beregn midterindeks for at undgå heltalsoverflow
        mid = left + (right - left) // 2
        
        # Tjek om vi fandt målet
        if arr[mid] == target:
            return mid
        
        # Hvis mål er mindre, søg venstre halvdel
        elif arr[mid] > target:
            right = mid - 1
        
        # Hvis mål er større, søg højre halvdel
        else:
            left = mid + 1
    
    # Mål ikke fundet
    return -1

# Eksempel på brug:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Indeks for 7: {result}")  # Output: Indeks for 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Eksempel på edge-implementering

Qwen-modeller kan implementeres på forskellige edge-enheder med optimerede konfigurationer:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Qwen-familiens udvikling

### Qwen 1.0 og 1.5: Grundlæggende modeller

De tidlige Qwen-modeller etablerede de grundlæggende principper for omfattende træning og open source-tilgængelighed:

- **Qwen-7B (7B parametre)**: Første udgivelse med fokus på kinesisk og engelsk sprogforståelse
- **Qwen-14B (14B parametre)**: Forbedrede kapabiliteter med forbedret ræsonnement og viden
- **Qwen-72B (72B parametre)**: Stor model, der leverer state-of-the-art ydeevne
- **Qwen1.5-serien**: Udvidet til flere størrelser (0.5B til 110B) med forbedret lang-kontekst håndtering

### Qwen2-familien: Multimodal udvidelse

Qwen2-serien markerede betydelige fremskridt inden for både sprog- og multimodale kapabiliteter:

- **Qwen2-0.5B til 72B**: Omfattende række af sprogmodeller til forskellige implementeringsbehov
- **Qwen2-57B-A14B (MoE)**: Mixture-of-experts arkitektur for effektiv parameterbrug
- **Qwen2-VL**: Avancerede vision-sprog kapabiliteter til billedforståelse
- **Qwen2-Audio**: Lydbehandling og forståelseskapabiliteter
- **Qwen2-Math**: Specialiseret matematisk ræsonnement og problemløsning

### Qwen2.5-familien: Forbedret ydeevne

Qwen2.5-serien bragte betydelige forbedringer på tværs af alle dimensioner:

- **Udvidet træning**: 18 billioner tokens træningsdata for forbedrede kapabiliteter
- **Udvidet kontekst**: Op til 128K tokens kontekstlængde, med Turbo-variant, der understøtter 1M tokens
- **Forbedret specialisering**: Forbedrede Qwen2.5-Coder og Qwen2.5-Math varianter
- **Bedre flersproget support**: Forbedret ydeevne på tværs af 27+ sprog

### Qwen3-familien: Avanceret ræsonnement

Den nyeste generation skubber grænserne for ræsonnement og tænkning:

- **Qwen3-235B-A22B**: Flagskib mixture-of-experts model med 235B samlede parametre
- **Qwen3-30B-A3B**: Effektiv MoE-model med stærk ydeevne pr. aktiv parameter
- **Dense-modeller**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B til forskellige implementeringsscenarier
- **Tænkemodus**: Hybrid ræsonnementstilgang, der understøtter både hurtige svar og dyb tænkning
- **Flersproget ekspertise**: Support for 119 sprog og dialekter
- **Forbedret træning**: 36 billioner tokens af diverse, høj-kvalitets træningsdata

## Anvendelser af Qwen-modeller

### Virksomhedsapplikationer

Organisationer bruger Qwen-modeller til dokumentanalyse, automatisering af kundeservice, kodegenereringsassistance og forretningsintelligensapplikationer. Open source-naturen muliggør tilpasning til specifikke forretningsbehov, samtidig med at datafortrolighed og kontrol opretholdes.

### Mobil og edge computing

Mobile applikationer udnytter Qwen-modeller til realtidsoversættelse, intelligente assistenter, indholdsgenerering og personlige anbefalinger. Rækken af modelstørrelser muliggør implementering fra mobile enheder til edge-servere.

### Uddannelsesteknologi

Uddannelsesplatforme bruger Qwen-modeller til personlig vejledning, automatiseret indholdsgenerering, sprogindlæringsassistance og interaktive uddannelsesoplevelser. Specialiserede modeller som Qwen-Math giver domænespecifik ekspertise.

### Globale applikationer

Internationale applikationer drager fordel af Qwen-modellers stærke flersprogede kapabiliteter, hvilket muliggør konsistente AI-oplevelser på tværs af forskellige sprog og kulturelle kontekster.

## Udfordringer og begrænsninger

### Beregningskrav

Selvom Qwen tilbyder modeller på tværs af forskellige størrelser, kræver større varianter stadig betydelige beregningsressourcer for optimal ydeevne, hvilket kan begrænse implementeringsmulighederne for nogle organisationer.

### Specialiseret domæneydelse

Selvom Qwen-modeller klarer sig godt på tværs af generelle domæner, kan stærkt specialiserede ap
Her er, hvordan du kommer i gang med Qwen-modeller ved hjælp af Hugging Face Transformers-biblioteket:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Brug af Qwen2.5-modeller

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Specialiseret brug af modeller

**Kodegenerering med Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Matematisk problemløsning:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Vision-sprog opgaver:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Tænkemodus (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 Mobil- og edge-implementering

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Eksempel på API-implementering

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Ydelsesbenchmark og resultater

Qwen-modelfamilien har opnået bemærkelsesværdig ydeevne på tværs af forskellige benchmarks, samtidig med at den opretholder open source-tilgængelighed:

### Vigtige præstationshøjdepunkter

**Fremragende ræsonnement:**
- Qwen3-235B-A22B opnår konkurrencedygtige resultater i benchmarkevalueringer af kodning, matematik og generelle evner sammenlignet med andre topmodeller som DeepSeek-R1, o1, o3-mini, Grok-3 og Gemini-2.5-Pro
- Qwen3-30B-A3B overgår QwQ-32B med 10 gange aktiverede parametre
- Qwen3-4B kan matche ydeevnen for Qwen2.5-72B-Instruct

**Effektivitet:**
- Qwen3-MoE basismodeller opnår lignende ydeevne som Qwen2.5 tætte basismodeller, mens de kun bruger 10% af de aktive parametre
- Betydelige omkostningsbesparelser både i træning og inferens sammenlignet med tætte modeller

**Multilinguale evner:**
- Qwen3-modeller understøtter 119 sprog og dialekter
- Stærk ydeevne på tværs af forskellige sproglige og kulturelle kontekster

**Træningsskala:**
- Qwen3 bruger næsten dobbelt så mange tokens, med cirka 36 billioner tokens, der dækker 119 sprog og dialekter, sammenlignet med Qwen2.5's 18 billioner tokens

### Model sammenligningsmatrix

| Modelserie      | Parameterområde | Kontekstlængde | Nøgleegenskaber         | Bedste anvendelsesområder         |
|------------------|-----------------|----------------|-------------------------|-----------------------------------|
| **Qwen2.5**     | 0.5B-72B        | 32K-128K       | Balanceret ydeevne, multilinguale | Generelle applikationer, produktionsimplementering |
| **Qwen2.5-Coder** | 1.5B-32B       | 128K           | Kodegenerering, programmering | Softwareudvikling, kodningsassistance |
| **Qwen2.5-Math** | 1.5B-72B       | 4K-128K        | Matematisk ræsonnement  | Uddannelsesplatforme, STEM-applikationer |
| **Qwen2.5-VL**   | Varierende      | Varierende     | Vision-sprog forståelse | Multimodale applikationer, billedanalyse |
| **Qwen3**        | 0.6B-235B      | Varierende     | Avanceret ræsonnement, tænkemodus | Kompleks ræsonnement, forskningsapplikationer |
| **Qwen3 MoE**    | 30B-235B total | Varierende     | Effektiv stor-skala ydeevne | Virksomhedsapplikationer, højtydende behov |

## Modelvalgsguide

### Til grundlæggende applikationer
- **Qwen2.5-0.5B/1.5B**: Mobilapps, edge-enheder, realtidsapplikationer
- **Qwen2.5-3B/7B**: Generelle chatbots, indholdsgenerering, Q&A-systemer

### Til matematiske og ræsonnementopgaver
- **Qwen2.5-Math**: Matematisk problemløsning og STEM-uddannelse
- **Qwen3 med tænkemodus**: Kompleks ræsonnement, der kræver trin-for-trin analyse

### Til programmering og udvikling
- **Qwen2.5-Coder**: Kodegenerering, debugging, programmeringsassistance
- **Qwen3**: Avancerede programmeringsopgaver med ræsonnementsevner

### Til multimodale applikationer
- **Qwen2.5-VL**: Billedforståelse, visuel spørgsmål-svar
- **Qwen-Audio**: Lydbehandling og taleforståelse

### Til virksomhedsimplementering
- **Qwen2.5-32B/72B**: Højtydende sprogforståelse
- **Qwen3-235B-A22B**: Maksimal kapacitet til krævende applikationer

## Implementeringsplatforme og tilgængelighed
### Cloud-platforme
- **Hugging Face Hub**: Omfattende modelrepository med fællesskabsstøtte
- **ModelScope**: Alibabas modelplatform med optimeringsværktøjer
- **Forskellige cloud-udbydere**: Support via standard ML-platforme

### Lokale udviklingsrammer
- **Transformers**: Standard Hugging Face-integration til nem implementering
- **vLLM**: Højtydende servering til produktionsmiljøer
- **Ollama**: Forenklet lokal implementering og styring
- **ONNX Runtime**: Tværplatformsoptimering til forskellige hardware
- **llama.cpp**: Effektiv C++-implementering til diverse platforme

### Læringsressourcer
- **Qwen-dokumentation**: Officiel dokumentation og modelkort
- **Hugging Face Model Hub**: Interaktive demoer og fællesskabseksempler
- **Forskningsartikler**: Tekniske artikler på arxiv for dybdegående forståelse
- **Fællesskabsfora**: Aktiv fællesskabsstøtte og diskussioner

### Kom godt i gang med Qwen-modeller

#### Udviklingsplatforme
1. **Hugging Face Transformers**: Start med standard Python-integration
2. **ModelScope**: Udforsk Alibabas optimerede implementeringsværktøjer
3. **Lokal implementering**: Brug Ollama eller direkte transformers til lokal test

#### Læringsvej
1. **Forstå kernekoncepter**: Studér Qwen-familiearkitekturen og dens evner
2. **Eksperimentér med varianter**: Prøv forskellige modelstørrelser for at forstå ydeevneafvejninger
3. **Praktiser implementering**: Implementér modeller i udviklingsmiljøer
4. **Optimer implementering**: Finjustér til produktionsbrug

#### Bedste praksis
- **Start småt**: Begynd med mindre modeller (1.5B-7B) til indledende udvikling
- **Brug chat-skabeloner**: Anvend korrekt formatering for optimale resultater
- **Overvåg ressourcer**: Spor hukommelsesbrug og inferenshastighed
- **Overvej specialisering**: Vælg domænespecifikke varianter, når det er relevant

## Avancerede brugsmønstre

### Eksempler på finjustering

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Specialiseret prompt engineering

**Til komplekse ræsonnementopgaver:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Til kodegenerering med kontekst:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Multilinguale applikationer

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Produktionsimplementeringsmønstre

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Ydelsesoptimeringsstrategier

### Hukommelsesoptimering

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Inferensoptimering

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Bedste praksis og retningslinjer

### Sikkerhed og privatliv

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Overvågning og evaluering

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Konklusion

Qwen-modelfamilien repræsenterer en omfattende tilgang til demokratisering af AI-teknologi, samtidig med at den opretholder konkurrencedygtig ydeevne på tværs af forskellige applikationer. Gennem sin forpligtelse til open source-tilgængelighed, multilinguale evner og fleksible implementeringsmuligheder gør Qwen det muligt for organisationer og udviklere at udnytte kraftfulde AI-evner uanset deres ressourcer eller specifikke krav.

### Vigtige pointer

**Open Source Excellence**: Qwen viser, at open source-modeller kan opnå ydeevne, der konkurrerer med proprietære alternativer, samtidig med at de tilbyder gennemsigtighed, tilpasning og kontrol.

**Skalerbar arkitektur**: Spændet fra 0.5B til 235B parametre muliggør implementering på tværs af hele spektret af computermiljøer, fra mobile enheder til virksomhedsclusters.

**Specialiserede evner**: Domænespecifikke varianter som Qwen-Coder, Qwen-Math og Qwen-VL tilbyder specialiseret ekspertise, samtidig med at de opretholder generel sprogforståelse.

**Global tilgængelighed**: Stærk multilinguale support på tværs af 119+ sprog gør Qwen velegnet til internationale applikationer og diverse brugergrupper.

**Kontinuerlig innovation**: Udviklingen fra Qwen 1.0 til Qwen3 viser konsekvent forbedring af evner, effektivitet og implementeringsmuligheder.

### Fremtidsperspektiver

Efterhånden som Qwen-familien fortsætter med at udvikle sig, kan vi forvente:

- **Forbedret effektivitet**: Fortsat optimering for bedre ydeevne pr. parameter
- **Udvidede multimodale evner**: Integration af mere sofistikeret vision-, lyd- og tekstbehandling
- **Forbedret ræsonnement**: Avancerede tænkemekanismer og multi-trins problemløsningskapaciteter
- **Bedre implementeringsværktøjer**: Forbedrede rammer og optimeringsværktøjer til diverse implementeringsscenarier
- **Fællesskabsvækst**: Udvidet økosystem af værktøjer, applikationer og fællesskabsbidrag

### Næste skridt

Uanset om du bygger en chatbot, udvikler uddannelsesværktøjer, skaber kodningsassistenter eller arbejder på multilinguale applikationer, tilbyder Qwen-familien skalerbare løsninger med stærk fællesskabsstøtte og omfattende dokumentation.

For de seneste opdateringer, modeludgivelser og detaljeret teknisk dokumentation, besøg de officielle Qwen-repositorier på Hugging Face og udforsk de aktive fællesskabsdiskussioner og eksempler.

Fremtiden for AI-udvikling ligger i tilgængelige, gennemsigtige og kraftfulde værktøjer, der muliggør innovation på tværs af alle sektorer og skalaer. Qwen-familien eksemplificerer denne vision og giver organisationer og udviklere fundamentet til at bygge næste generation af AI-drevne applikationer.

## Yderligere ressourcer

- **Officiel dokumentation**: [Qwen Dokumentation](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Tekniske artikler**: [Qwen Forskningspublikationer](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Fællesskab**: [GitHub Diskussioner og Issues](https://github.com/QwenLM/)
- **ModelScope Platform**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Læringsresultater

Efter at have gennemført dette modul vil du kunne:

1. Forklare de arkitektoniske fordele ved Qwen-modelfamilien og dens open source-tilgang
2. Vælge den passende Qwen-variant baseret på specifikke applikationskrav og ressourcebegrænsninger
3. Implementere Qwen-modeller i forskellige implementeringsscenarier med optimerede konfigurationer
4. Anvende kvantisering og optimeringsteknikker for at forbedre Qwen-modellens ydeevne
5. Evaluere afvejninger mellem modelstørrelse, ydeevne og kapaciteter på tværs af Qwen-familien

## Hvad er næste skridt

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.