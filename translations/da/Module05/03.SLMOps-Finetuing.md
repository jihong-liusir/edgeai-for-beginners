<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T10:34:36+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "da"
}
-->
# Sektion 3: Finjustering - Tilpasning af modeller til specifikke opgaver

## Indholdsfortegnelse
1. [Introduktion til finjustering](../../../Module05)
2. [Hvorfor finjustering er vigtig](../../../Module05)
3. [Typer af finjustering](../../../Module05)
4. [Finjustering med Microsoft Olive](../../../Module05)
5. [Praktiske eksempler](../../../Module05)
6. [Bedste praksis og retningslinjer](../../../Module05)
7. [Avancerede teknikker](../../../Module05)
8. [Evaluering og overvågning](../../../Module05)
9. [Almindelige udfordringer og løsninger](../../../Module05)
10. [Konklusion](../../../Module05)

## Introduktion til finjustering

**Finjustering** er en effektiv maskinlæringsteknik, der indebærer at tilpasse en forudtrænet model til at udføre specifikke opgaver eller arbejde med specialiserede datasæt. I stedet for at træne en model fra bunden, udnytter finjustering den viden, som en forudtrænet model allerede har opnået, og tilpasser den til din specifikke anvendelse.

### Hvad er finjustering?

Finjustering er en form for **transfer learning**, hvor du:
- Starter med en forudtrænet model, der har lært generelle mønstre fra store datasæt
- Justerer modellens interne parametre ved hjælp af dit specifikke datasæt
- Beholder den værdifulde viden, mens modellen specialiseres til din opgave

Tænk på det som at lære en dygtig kok at lave en ny type mad - de forstår allerede grundlæggende madlavning, men skal lære specifikke teknikker og smage til den nye stil.

### Centrale fordele

- **Tidsbesparende**: Meget hurtigere end at træne fra bunden
- **Dataeffektivitet**: Kræver mindre datasæt for at opnå gode resultater
- **Omkostningseffektiv**: Lavere krav til beregningsressourcer
- **Bedre ydeevne**: Opnår ofte bedre resultater sammenlignet med træning fra bunden
- **Ressourceoptimering**: Gør kraftfuld AI tilgængelig for mindre teams og organisationer

## Hvorfor finjustering er vigtig

### Anvendelser i den virkelige verden

Finjustering er afgørende i mange scenarier:

**1. Domænetilpasning**
- Medicinsk AI: Tilpasning af generelle sproglige modeller til medicinsk terminologi og kliniske noter
- Juridisk teknologi: Specialisering af modeller til analyse af juridiske dokumenter og kontraktgennemgang
- Finansielle tjenester: Tilpasning af modeller til analyse af finansielle rapporter og risikovurdering

**2. Opgavespecialisering**
- Indholdsgenerering: Finjustering til specifikke skrivestile eller toner
- Kodegenerering: Tilpasning af modeller til bestemte programmeringssprog eller rammer
- Oversættelse: Forbedring af ydeevne for specifikke sprogpar eller tekniske domæner

**3. Virksomhedsanvendelser**
- Kundeservice: Skabelse af chatbots, der forstår virksomhedsspecifik terminologi
- Intern dokumentation: Opbygning af AI-assistenter, der er bekendt med organisatoriske processer
- Branchespecifikke løsninger: Udvikling af modeller, der forstår sektorspecifik jargon og arbejdsgange

## Typer af finjustering

### 1. Fuld finjustering (Instruktionsfinjustering)

Ved fuld finjustering opdateres alle modellens parametre under træning. Denne tilgang:
- Giver maksimal fleksibilitet og ydeevnepotentiale
- Kræver betydelige beregningsressourcer
- Resulterer i en helt ny version af modellen
- Er bedst til scenarier, hvor du har omfattende træningsdata og beregningsressourcer

### 2. Parameter-effektiv finjustering (PEFT)

PEFT-metoder opdaterer kun en lille delmængde af parametrene, hvilket gør processen mere effektiv:

#### Low-Rank Adaptation (LoRA)
- Tilføjer små trænbare rank-dekompositionsmatricer til eksisterende vægte
- Reducerer dramatisk antallet af trænbare parametre
- Bevarer ydeevne tæt på fuld finjustering
- Muliggør nem skift mellem forskellige tilpasninger

#### QLoRA (Quantized LoRA)
- Kombinerer LoRA med kvantiseringsteknikker
- Reducerer yderligere hukommelseskrav
- Muliggør finjustering af større modeller på forbrugerhardware
- Balancerer effektivitet med ydeevne

#### Adapters
- Indsætter små neurale netværk mellem eksisterende lag
- Tillader målrettet finjustering, mens basismodellen forbliver uændret
- Muliggør en modulær tilgang til modeltilpasning

### 3. Opgavespecifik finjustering

Fokuserer på at tilpasse modeller til specifikke downstream-opgaver:
- **Klassifikation**: Justering af modeller til kategoriseringsopgaver
- **Generering**: Optimering til indholdsskabelse og tekstgenerering
- **Ekstraktion**: Finjustering til informationsudtrækning og navngiven enhedsgenkendelse
- **Opsummering**: Specialisering af modeller til dokumentopsummering

## Finjustering med Microsoft Olive

Microsoft Olive er et omfattende værktøj til modeloptimering, der forenkler finjusteringsprocessen og tilbyder funktioner i virksomhedsklasse.

### Hvad er Microsoft Olive?

Microsoft Olive er et open source-værktøj til modeloptimering, der:
- Strømliner finjusteringsarbejdsgange for forskellige hardwaremål
- Tilbyder indbygget support til populære modelarkitekturer (Llama, Phi, Qwen, Gemma)
- Giver både cloud- og lokale implementeringsmuligheder
- Integreres problemfrit med Azure ML og andre Microsoft AI-tjenester
- Understøtter automatisk optimering og kvantisering

### Centrale funktioner

- **Hardware-bevidst optimering**: Optimerer automatisk modeller til specifik hardware (CPU, GPU, NPU)
- **Multi-format support**: Arbejder med PyTorch, Hugging Face og ONNX-modeller
- **Automatiserede arbejdsgange**: Reducerer manuel konfiguration og forsøg-og-fejl
- **Virksomhedsintegration**: Indbygget support til Azure ML og cloud-implementeringer
- **Udvidelig arkitektur**: Muliggør brugerdefinerede optimeringsteknikker

### Installation og opsætning

#### Grundlæggende installation

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Valgfrie afhængigheder

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Verificer installation

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktiske eksempler

### Eksempel 1: Grundlæggende finjustering med Olive CLI

Dette eksempel demonstrerer finjustering af en lille sproglig model til fraseklassifikation:

#### Trin 1: Forbered dit miljø

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Trin 2: Finjuster modellen

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Trin 3: Optimer til implementering

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Eksempel 2: Avanceret konfiguration med brugerdefineret datasæt

#### Trin 1: Forbered brugerdefineret datasæt

Opret en JSON-fil med dine træningsdata:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Trin 2: Opret konfigurationsfil

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Trin 3: Udfør finjustering

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Eksempel 3: QLoRA finjustering for hukommelseseffektivitet

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Bedste praksis og retningslinjer

### Dataklargøring

**1. Datakvalitet frem for kvantitet**
- Prioriter eksempler af høj kvalitet frem for store mængder af dårlig data
- Sørg for, at data er repræsentativt for din målbrug
- Rens og forbehandl data konsekvent

**2. Dataformat og skabeloner**
- Brug ensartet formatering på tværs af alle træningseksempler
- Opret klare input-output skabeloner, der matcher din brugssag
- Inkluder passende instruktionsformatering til instruktionsfinjusterede modeller

**3. Datasæt-opdeling**
- Reserver 10-20% af data til validering
- Bevar lignende fordelinger på tværs af træning/valideringsopdelinger
- Overvej stratificeret sampling til klassifikationsopgaver

### Træningskonfiguration

**1. Valg af læringsrate**
- Start med mindre læringsrater (1e-5 til 1e-4) til finjustering
- Brug læringsrateplanlægning for bedre konvergens
- Overvåg tabskurver for at justere raterne

**2. Optimering af batchstørrelse**
- Balancer batchstørrelse med tilgængelig hukommelse
- Brug gradientakkumulering for større effektive batchstørrelser
- Overvej forholdet mellem batchstørrelse og læringsrate

**3. Træningsvarighed**
- Overvåg valideringsmålinger for at undgå overtilpasning
- Brug tidlig stopning, når valideringsydelsen flader ud
- Gem checkpoints regelmæssigt for genopretning og analyse

### Modelvalg

**1. Valg af basismodel**
- Vælg modeller, der er forudtrænet på lignende domæner, når det er muligt
- Overvej modellens størrelse i forhold til dine beregningsbegrænsninger
- Evaluer licenskrav for kommerciel brug

**2. Valg af finjusteringsmetode**
- Brug LoRA/QLoRA til ressourcebegrænsede miljøer
- Vælg fuld finjustering, når maksimal ydeevne er kritisk
- Overvej adapterbaserede tilgange til flere opgavescenarier

### Ressourcehåndtering

**1. Hardwareoptimering**
- Vælg passende hardware til din modelstørrelse og metode
- Udnyt GPU-hukommelse effektivt med gradientcheckpointing
- Overvej cloud-baserede løsninger til større modeller

**2. Hukommelseshåndtering**
- Brug blandet præcisionstræning, når det er muligt
- Implementer gradientakkumulering for hukommelsesbegrænsninger
- Overvåg GPU-hukommelsesbrug under hele træningen

## Avancerede teknikker

### Multi-adapter træning

Træn flere adapters til forskellige opgaver, mens basismodellen deles:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hyperparameteroptimering

Implementer systematisk hyperparameterjustering:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Brugerdefinerede tabfunktioner

Implementer domænespecifikke tabfunktioner:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Evaluering og overvågning

### Målinger og evaluering

**1. Standardmålinger**
- **Nøjagtighed**: Overordnet korrekthed for klassifikationsopgaver
- **Perpleksitet**: Kvalitetsmål for sproglig modellering
- **BLEU/ROUGE**: Kvalitet for tekstgenerering og opsummering
- **F1-score**: Balanceret præcision og recall for klassifikation

**2. Domænespecifikke målinger**
- **Opgavespecifikke benchmarks**: Brug etablerede benchmarks for dit domæne
- **Menneskelig evaluering**: Inkluder menneskelig vurdering for subjektive opgaver
- **Forretningsmålinger**: Tilpas til faktiske forretningsmål

**3. Evaluering opsætning**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Overvågning af træningsfremskridt

**1. Tabsovervågning**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Valideringsovervågning**
- Overvåg valideringstab sammen med træningstab
- Hold øje med tegn på overtilpasning (valideringstab stiger, mens træningstab falder)
- Brug tidlig stopning baseret på valideringsmålinger

**3. Ressourceovervågning**
- Overvåg GPU/CPU-udnyttelse
- Spor hukommelsesbrugsmønstre
- Overvåg træningshastighed og gennemløb

## Almindelige udfordringer og løsninger

### Udfordring 1: Overtilpasning

**Symptomer:**
- Træningstab fortsætter med at falde, mens valideringstab stiger
- Stor forskel mellem trænings- og valideringsydelse
- Dårlig generalisering til nye data

**Løsninger:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Udfordring 2: Hukommelsesbegrænsninger

**Løsninger:**
- Brug gradientcheckpointing
- Implementer gradientakkumulering
- Vælg parameter-effektive metoder (LoRA, QLoRA)
- Udnyt modelparallellisme til store modeller

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Udfordring 3: Langsom træning

**Løsninger:**
- Optimer dataindlæsningspipelines
- Brug blandet præcisionstræning
- Implementer effektive batchingstrategier
- Overvej distribueret træning til store datasæt

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Udfordring 4: Dårlig ydeevne

**Diagnosetrin:**
1. Verificer datakvalitet og formatering
2. Tjek læringsrate og træningsvarighed
3. Evaluer valg af basismodel
4. Gennemgå forbehandling og tokenisering

**Løsninger:**
- Øg diversiteten i træningsdata
- Juster læringsrateplanlægning
- Prøv forskellige basismodeller
- Implementer dataforøgelsesteknikker

## Konklusion

Finjustering er en kraftfuld teknik, der demokratiserer adgangen til avancerede AI-funktioner. Ved at udnytte værktøjer som Microsoft Olive kan organisationer effektivt tilpasse forudtrænede modeller til deres specifikke behov, samtidig med at de optimerer ydeevne og ressourcebegrænsninger.

### Centrale pointer

1. **Vælg den rigtige tilgang**: Vælg finjusteringsmetoder baseret på dine beregningsressourcer og ydeevnekrav
2. **Datakvalitet er afgørende**: Invester i høj kvalitet og repræsentative træningsdata
3. **Overvåg og iterer**: Evaluer og forbedr dine modeller løbende
4. **Udnyt værktøjer**: Brug rammer som Olive til at forenkle og optimere processen
5. **Overvej implementering**: Planlæg modeloptimering og implementering fra starten

## ➡️ Hvad er næste skridt

- [04: Implementering - Produktionsklar modelimplementering](./04.SLMOps.Deployment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.