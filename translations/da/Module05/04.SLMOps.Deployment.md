<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T10:37:02+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "da"
}
-->
# Afsnit 4: Udrulning - Implementering af produktionsklar model

## Oversigt

Denne omfattende vejledning vil guide dig gennem hele processen med at udrulle finjusterede kvantiserede modeller ved hj√¶lp af Foundry Local. Vi d√¶kker modelkonvertering, optimering af kvantisering og udrulningskonfiguration fra start til slut.

## Foruds√¶tninger

F√∏r du g√•r i gang, skal du sikre dig, at du har f√∏lgende:

- ‚úÖ En finjusteret onnx-model klar til udrulning
- ‚úÖ Windows- eller Mac-computer
- ‚úÖ Python 3.10 eller nyere
- ‚úÖ Mindst 8GB ledig RAM
- ‚úÖ Foundry Local installeret p√• dit system

## Del 1: Ops√¶tning af milj√∏

### Installation af n√∏dvendige v√¶rkt√∏jer

√Öbn din terminal (Command Prompt p√• Windows, Terminal p√• Mac) og k√∏r f√∏lgende kommandoer i r√¶kkef√∏lge:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Vigtig bem√¶rkning**: Du skal ogs√• bruge CMake version 3.31 eller nyere, som kan downloades fra [cmake.org](https://cmake.org/download/).

## Del 2: Modelkonvertering og kvantisering

### Valg af det rigtige format

For finjusterede sm√• sproglige modeller anbefaler vi at bruge **ONNX-format**, fordi det tilbyder:

- üöÄ Bedre optimering af ydeevne
- üîß Hardware-uafh√¶ngig udrulning
- üè≠ Produktionsklare egenskaber
- üì± Kompatibilitet p√• tv√¶rs af platforme

### Metode 1: Konvertering med √©n kommando (Anbefalet)

Brug f√∏lgende kommando til direkte at konvertere din finjusterede model:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Forklaring af parametre:**
- `--model_name_or_path`: Sti til din finjusterede model
- `--device cpu`: Brug CPU til optimering
- `--precision int4`: Brug INT4-kvantisering (ca. 75% reduktion i st√∏rrelse)
- `--output_path`: Output-sti for den konverterede model

### Metode 2: Konfigurationsfiltilgang (Avancerede brugere)

Opret en konfigurationsfil med navnet `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

K√∏r derefter:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Sammenligning af kvantiseringsmuligheder

| Pr√¶cision | Filst√∏rrelse | Inferenshastighed | Modelkvalitet | Anbefalet brug |
|-----------|--------------|-------------------|---------------|----------------|
| FP16      | Baseline √ó 0.5 | Hurtig | Bedst | High-end hardware |
| INT8      | Baseline √ó 0.25 | Meget hurtig | God | Balanceret valg |
| INT4      | Baseline √ó 0.125 | Hurtigst | Acceptabel | Ressourcebegr√¶nset |

üí° **Anbefaling**: Start med INT4-kvantisering til din f√∏rste udrulning. Hvis kvaliteten ikke er tilfredsstillende, pr√∏v INT8 eller FP16.

## Del 3: Foundry Local udrulningskonfiguration

### Oprettelse af modelkonfiguration

Naviger til Foundry Local models-mappen:

```bash
foundry cache cd ./models/
```

Opret din modelmappe-struktur:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Opret konfigurationsfilen `inference_model.json` i din modelmappe:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Model-specifikke skabelonkonfigurationer

#### For Qwen-seriens modeller:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Del 4: Modeltest og optimering

### Verificering af modelinstallation

Kontroller, om Foundry Local kan genkende din model:

```bash
foundry cache ls
```

Du b√∏r se `your-finetuned-model-int4` p√• listen.

### Start af modeltest

```bash
foundry model run your-finetuned-model-int4
```

### Ydelsesm√•ling

Overv√•g n√∏glemetrikker under testen:

1. **Responstid**: M√•l gennemsnitlig tid pr. svar
2. **Hukommelsesforbrug**: Overv√•g RAM-forbrug
3. **CPU-udnyttelse**: Tjek processorbelastning
4. **Outputkvalitet**: Evaluer svarenes relevans og sammenh√¶ng

### Tjekliste for kvalitetsvalidering

- ‚úÖ Modellen svarer korrekt p√• finjusterede dom√¶neforesp√∏rgsler
- ‚úÖ Svarformatet matcher den forventede outputstruktur
- ‚úÖ Ingen hukommelsesl√¶kager under l√¶ngerevarende brug
- ‚úÖ Konsistent ydeevne p√• tv√¶rs af forskellige inputl√¶ngder
- ‚úÖ Korrekt h√•ndtering af kanttilf√¶lde og ugyldige inputs

## Opsummering

Tillykke! Du har med succes gennemf√∏rt:

- ‚úÖ Konvertering af finjusteret modelformat
- ‚úÖ Optimering af modelkvantisering
- ‚úÖ Konfiguration af Foundry Local udrulning
- ‚úÖ Ydelsestuning og fejlfinding

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.