<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T10:01:37+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "da"
}
-->
# Sektion 03 - Integration af Model Context Protocol (MCP)

## Introduktion til MCP (Model Context Protocol)

Model Context Protocol (MCP) er en banebrydende ramme, der gør det muligt for sprogmodeller at interagere med eksterne værktøjer og systemer på en standardiseret måde. I modsætning til traditionelle metoder, hvor modeller er isolerede, skaber MCP en bro mellem AI-modeller og den virkelige verden gennem en veldefineret protokol.

### Hvad er MCP?

MCP fungerer som en kommunikationsprotokol, der gør det muligt for sprogmodeller at:
- Forbinde til eksterne datakilder
- Udføre værktøjer og funktioner
- Interagere med API'er og tjenester
- Få adgang til realtidsinformation
- Udføre komplekse operationer i flere trin

Denne protokol forvandler statiske sprogmodeller til dynamiske agenter, der kan udføre praktiske opgaver ud over tekstgenerering.

## Små sprogmodeller (SLMs) i MCP

Små sprogmodeller repræsenterer en effektiv tilgang til AI-implementering og tilbyder flere fordele:

### Fordele ved SLMs
- **Ressourceeffektivitet**: Lavere krav til beregningskraft
- **Hurtigere svartider**: Reduceret ventetid for realtidsapplikationer  
- **Omkostningseffektivitet**: Minimal infrastrukturbehov
- **Privatliv**: Kan køre lokalt uden dataoverførsel
- **Tilpasning**: Nem at finjustere til specifikke domæner

### Hvorfor SLMs fungerer godt med MCP

SLMs kombineret med MCP skaber en kraftfuld løsning, hvor modellens evne til at ræsonnere forstærkes af eksterne værktøjer, hvilket kompenserer for deres mindre parameterantal gennem forbedret funktionalitet.

## Oversigt over Python MCP SDK

Python MCP SDK giver fundamentet for at bygge MCP-aktiverede applikationer. SDK'en inkluderer:

- **Klientbiblioteker**: Til forbindelse med MCP-servere
- **Serverrammeværk**: Til oprettelse af brugerdefinerede MCP-servere
- **Protokolhåndterere**: Til styring af kommunikation
- **Værktøjsintegration**: Til udførelse af eksterne funktioner

## Praktisk implementering: Phi-4 MCP-klient

Lad os udforske en praktisk implementering ved hjælp af Microsofts Phi-4 mini-model integreret med MCP-funktioner.

### Systemarkitektur

Implementeringen følger en lagdelt arkitektur:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Centrale komponenter

#### 1. MCP-klientklasser

**BaseMCPClient**: Abstrakt fundament, der tilbyder fælles funktionalitet
- Asynkront kontekststyringsprotokol
- Standardgrænseflade-definition
- Ressourcehåndtering

**Phi4MiniMCPClient**: STDIO-baseret implementering
- Lokal proceskommunikation
- Håndtering af standard input/output
- Underprocesstyring

**Phi4MiniSSEMCPClient**: Server-Sent Events implementering
- HTTP-streamingkommunikation
- Realtidshåndtering af begivenheder
- Webbaseret serverforbindelse

#### 2. LLM-integration

**OllamaClient**: Lokal modelhosting  
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Højtydende servering  
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Værktøjsbehandlingspipeline

Værktøjsbehandlingspipen transformerer MCP-værktøjer til formater, der er kompatible med sprogmodeller:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Kom godt i gang: Trin-for-trin guide

### Trin 1: Opsætning af miljø

Installer nødvendige afhængigheder:  
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Trin 2: Grundlæggende konfiguration

Opsæt dine miljøvariabler:  
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Trin 3: Kør din første MCP-klient

**Grundlæggende Ollama-opsætning:**  
```bash
python ghmodel_mcp_demo.py
```

**Brug af vLLM-backend:**  
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events-forbindelse:**  
```bash
python ghmodel_mcp_demo.py --run sse
```

**Brugerdefineret MCP-server:**  
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Trin 4: Programmerbar brug

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Avancerede funktioner

### Multi-backend support

Implementeringen understøtter både Ollama og vLLM-backends, så du kan vælge baseret på dine behov:

- **Ollama**: Bedre til lokal udvikling og test
- **vLLM**: Optimeret til produktion og høj gennemstrømning

### Fleksible forbindelsesprotokoller

To forbindelsestilstande understøttes:

**STDIO-tilstand**: Direkte proceskommunikation
- Lavere ventetid
- Velegnet til lokale værktøjer
- Enkel opsætning

**SSE-tilstand**: HTTP-baseret streaming
- Netværkskapabel
- Bedre til distribuerede systemer
- Realtidsopdateringer

### Værktøjsintegrationsmuligheder

Systemet kan integreres med forskellige værktøjer:
- Webautomatisering (Playwright)
- Filoperationer
- API-interaktioner
- Systemkommandoer
- Brugerdefinerede funktioner

## Fejlhåndtering og bedste praksis

### Omfattende fejlhåndtering

Implementeringen inkluderer robust fejlhåndtering for:

**Forbindelsesfejl:**
- MCP-servernedbrud
- Netværkstidsudløb
- Forbindelsesproblemer

**Værktøjsudførelsesfejl:**
- Manglende værktøjer
- Parametervalidering
- Udførelsesfejl

**Svarbehandlingsfejl:**
- JSON-parsning
- Formatinkonsistenser
- Anomalier i LLM-svar

### Bedste praksis

1. **Ressourcehåndtering**: Brug asynkrone kontekststyrere
2. **Fejlhåndtering**: Implementer omfattende try-catch blokke
3. **Logning**: Aktivér passende logningsniveauer
4. **Sikkerhed**: Valider input og rens output
5. **Ydeevne**: Brug forbindelsespooling og caching

## Virkelige anvendelser

### Webautomatisering  
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Databehandling  
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API-integration  
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Ydelsesoptimering

### Hukommelseshåndtering
- Effektiv håndtering af beskedhistorik
- Korrekt oprydning af ressourcer
- Forbindelsespooling

### Netværksoptimering
- Asynkrone HTTP-operationer
- Konfigurerbare tidsgrænser
- Elegant fejlgenopretning

### Samtidig behandling
- Ikke-blokerende I/O
- Parallel værktøjsudførelse
- Effektive asynkrone mønstre

## Sikkerhedsovervejelser

### Databeskyttelse
- Sikker håndtering af API-nøgler
- Inputvalidering
- Outputsanitering

### Netværkssikkerhed
- HTTPS-understøttelse
- Lokale endepunktsstandarder
- Sikker tokenhåndtering

### Udførelsessikkerhed
- Filtrering af værktøjer
- Sandkassemiljøer
- Audit-logning

## Konklusion

SLMs integreret med MCP repræsenterer et paradigmeskift inden for udvikling af AI-applikationer. Ved at kombinere effektiviteten af små modeller med kraften fra eksterne værktøjer kan udviklere skabe intelligente systemer, der både er ressourceeffektive og meget kapable.

Phi-4 MCP-klientimplementeringen demonstrerer, hvordan denne integration kan opnås i praksis og giver et solidt fundament for at bygge sofistikerede AI-drevne applikationer.

Vigtige pointer:
- MCP bygger bro mellem sprogmodeller og eksterne systemer
- SLMs tilbyder effektivitet uden at gå på kompromis med funktionalitet, når de suppleres med værktøjer
- Den modulære arkitektur muliggør nem udvidelse og tilpasning
- Korrekt fejlhåndtering og sikkerhedsforanstaltninger er afgørende for produktionsbrug

Denne vejledning giver fundamentet for at bygge dine egne SLM-drevne MCP-applikationer og åbner op for muligheder inden for automatisering, databehandling og intelligent systemintegration.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på at sikre nøjagtighed, skal det bemærkes, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvar for eventuelle misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.