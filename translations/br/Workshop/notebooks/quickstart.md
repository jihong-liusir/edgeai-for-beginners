<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T11:15:54+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "br"
}
-->
# Guias R√°pidos dos Notebooks do Workshop

## √çndice

- [Pr√©-requisitos](../../../../Workshop/notebooks)
- [Configura√ß√£o Inicial](../../../../Workshop/notebooks)
- [Sess√£o 04: Compara√ß√£o de Modelos](../../../../Workshop/notebooks)
- [Sess√£o 05: Orquestrador Multi-Agente](../../../../Workshop/notebooks)
- [Sess√£o 06: Roteamento Baseado em Inten√ß√£o](../../../../Workshop/notebooks)
- [Vari√°veis de Ambiente](../../../../Workshop/notebooks)
- [Comandos Comuns](../../../../Workshop/notebooks)

---

## Pr√©-requisitos

### 1. Instalar Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verificar Instala√ß√£o:**
```bash
foundry --version
```

### 2. Instalar Depend√™ncias do Python

```bash
cd Workshop
pip install -r requirements.txt
```

Ou instale individualmente:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Configura√ß√£o Inicial

### Iniciar o Servi√ßo Foundry Local

**Necess√°rio antes de executar qualquer notebook:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

Sa√≠da esperada:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### Baixar e Carregar Modelos

Os notebooks utilizam estes modelos por padr√£o:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Verificar Configura√ß√£o

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Sess√£o 04: Compara√ß√£o de Modelos

### Prop√≥sito
Comparar o desempenho entre Modelos de Linguagem Pequenos (SLM) e Modelos de Linguagem Grandes (LLM).

### Configura√ß√£o R√°pida

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Executando o Notebook

1. **Abra** `session04_model_compare.ipynb` no VS Code ou Jupyter
2. **Reinicie o kernel** (Kernel ‚Üí Reiniciar Kernel)
3. **Execute todas as c√©lulas** na ordem

### Configura√ß√£o Principal

**Modelos Padr√£o:**
- **SLM:** `phi-4-mini` (~4GB RAM, mais r√°pido)
- **LLM:** `qwen2.5-3b` (~3GB RAM, otimizado para mem√≥ria)

**Vari√°veis de Ambiente (opcional):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### Sa√≠da Esperada

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### Personaliza√ß√£o

**Usar modelos diferentes:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Prompt personalizado:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Lista de Verifica√ß√£o de Valida√ß√£o

- [ ] A c√©lula 12 mostra os modelos corretos (phi-4-mini, qwen2.5-3b)
- [ ] A c√©lula 12 mostra o endpoint correto (porta 59959)
- [ ] Diagn√≥stico da c√©lula 16 passa (‚úÖ Servi√ßo est√° em execu√ß√£o)
- [ ] Verifica√ß√£o pr√©via da c√©lula 20 passa (ambos os modelos ok)
- [ ] Compara√ß√£o da c√©lula 22 completa com valores de lat√™ncia
- [ ] Valida√ß√£o da c√©lula 24 mostra üéâ TODOS OS TESTES PASSARAM!

### Estimativa de Tempo
- **Primeira execu√ß√£o:** 5-10 minutos (incluindo downloads de modelos)
- **Execu√ß√µes subsequentes:** 1-2 minutos

---

## Sess√£o 05: Orquestrador Multi-Agente

### Prop√≥sito
Demonstrar colabora√ß√£o multi-agente usando o SDK Foundry Local - agentes trabalham juntos para produzir resultados refinados.

### Configura√ß√£o R√°pida

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Executando o Notebook

1. **Abra** `session05_agents_orchestrator.ipynb`
2. **Reinicie o kernel**
3. **Execute todas as c√©lulas** na ordem

### Configura√ß√£o Principal

**Configura√ß√£o Padr√£o (Mesmo Modelo para Ambos os Agentes):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Configura√ß√£o Avan√ßada (Modelos Diferentes):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Arquitetura

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### Sa√≠da Esperada

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Extens√µes

**Adicionar mais agentes:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Testes em lote:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Estimativa de Tempo
- **Primeira execu√ß√£o:** 3-5 minutos
- **Execu√ß√µes subsequentes:** 1-2 minutos por pergunta

---

## Sess√£o 06: Roteamento Baseado em Inten√ß√£o

### Prop√≥sito
Roteamento inteligente de prompts para modelos especializados com base na inten√ß√£o detectada.

### Configura√ß√£o R√°pida

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Nota:** A Sess√£o 06 utiliza modelos de CPU por padr√£o para m√°xima compatibilidade.

### Executando o Notebook

1. **Abra** `session06_models_router.ipynb`
2. **Reinicie o kernel**
3. **Execute todas as c√©lulas** na ordem

### Configura√ß√£o Principal

**Cat√°logo Padr√£o (Modelos de CPU):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternativa (Modelos de GPU):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Detec√ß√£o de Inten√ß√£o

O roteador utiliza padr√µes regex para detectar inten√ß√µes:

| Inten√ß√£o         | Exemplos de Padr√µes         | Roteado Para          |
|------------------|-----------------------------|-----------------------|
| `code`          | "refatorar", "implementar fun√ß√£o" | phi-3.5-mini-cpu     |
| `classification`| "categorizar", "classificar isso" | qwen2.5-0.5b-cpu     |
| `summarize`     | "resumir", "tl;dr"          | phi-4-mini-cpu        |
| `general`       | Todo o resto                | phi-4-mini-cpu        |

### Sa√≠da Esperada

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### Personaliza√ß√£o

**Adicionar inten√ß√£o personalizada:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Habilitar rastreamento de tokens:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### Alternando para Modelos de GPU

Se voc√™ tiver 8GB+ de VRAM:

1. Na **C√©lula #6**, comente o cat√°logo de CPU
2. Descomente o cat√°logo de GPU
3. Carregue os modelos de GPU:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Reinicie o kernel e reexecute o notebook

### Estimativa de Tempo
- **Primeira execu√ß√£o:** 5-10 minutos (carregamento de modelos)
- **Execu√ß√µes subsequentes:** 30-60 segundos por teste

---

## Vari√°veis de Ambiente

### Configura√ß√£o Global

Defina antes de iniciar o Jupyter/VS Code:

**Windows (Prompt de Comando):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Configura√ß√£o no Notebook

Defina no in√≠cio de qualquer notebook:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Comandos Comuns

### Gerenciamento de Servi√ßo

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Gerenciamento de Modelos

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Testando Endpoints

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Comandos de Diagn√≥stico

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Melhores Pr√°ticas

### Antes de Iniciar Qualquer Notebook

1. **Verifique se o servi√ßo est√° em execu√ß√£o:**
   ```bash
   foundry service status
   ```

2. **Confirme se os modelos est√£o carregados:**
   ```bash
   foundry model ls
   ```

3. **Reinicie o kernel do notebook** se estiver reexecutando

4. **Limpe todos os outputs** para uma execu√ß√£o limpa

### Gerenciamento de Recursos

1. **Use modelos de CPU por padr√£o** para compatibilidade
2. **Troque para modelos de GPU** apenas se tiver 8GB+ de VRAM
3. **Feche outros aplicativos de GPU** antes de executar
4. **Mantenha o servi√ßo em execu√ß√£o** entre sess√µes de notebook
5. **Monitore o uso de recursos** com Gerenciador de Tarefas / nvidia-smi

### Solu√ß√£o de Problemas

1. **Sempre verifique o servi√ßo primeiro** antes de depurar o c√≥digo
2. **Reinicie o kernel** se voc√™ vir configura√ß√µes antigas
3. **Reexecute c√©lulas de diagn√≥stico** ap√≥s qualquer altera√ß√£o
4. **Verifique se os nomes dos modelos** correspondem aos carregados
5. **Confirme se a porta do endpoint** corresponde ao status do servi√ßo

---

## Refer√™ncia R√°pida: Apelidos de Modelos

### Modelos Comuns

| Apelido         | Tamanho | Melhor Para               | RAM/VRAM | Variantes              |
|-----------------|---------|--------------------------|----------|-----------------------|
| `phi-4-mini`   | ~4B     | Chat geral, sumariza√ß√£o  | 4-6GB    | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B   | Gera√ß√£o de c√≥digo        | 3-5GB    | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b`   | ~3B     | Tarefas gerais, eficiente| 3-4GB    | `-cpu`, `-cuda-gpu`         |
| `qwen2.5-1.5b` | ~1.5B   | R√°pido, poucos recursos  | 2-3GB    | `-cpu`, `-cuda-gpu`         |
| `qwen2.5-0.5b` | ~0.5B   | Classifica√ß√£o, m√≠nimo recurso | 1-2GB | `-cpu`, `-cuda-gpu`         |

### Nomea√ß√£o de Variantes

- **Nome base** (ex.: `phi-4-mini`): Seleciona automaticamente a melhor variante para seu hardware
- **`-cpu`**: Otimizado para CPU, funciona em qualquer lugar
- **`-cuda-gpu`**: Otimizado para GPU NVIDIA, requer 8GB+ de VRAM
- **`-npu`**: Otimizado para NPU Qualcomm, requer drivers NPU

**Recomenda√ß√£o:** Use nomes base (sem sufixo) e deixe o Foundry Local selecionar automaticamente a melhor variante.

---

## Indicadores de Sucesso

Voc√™ est√° pronto quando:

‚úÖ `foundry service status` mostra "running"  
‚úÖ `foundry model ls` mostra os modelos necess√°rios  
‚úÖ Servi√ßo acess√≠vel no endpoint correto  
‚úÖ Verifica√ß√£o de sa√∫de retorna 200 OK  
‚úÖ C√©lulas de diagn√≥stico do notebook passam  
‚úÖ Sem erros de conex√£o na sa√≠da  

---

## Obtendo Ajuda

### Documenta√ß√£o
- **Reposit√≥rio Principal**: https://github.com/microsoft/Foundry-Local
- **SDK Python**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **Refer√™ncia CLI**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **Solu√ß√£o de Problemas**: Veja `troubleshooting.md` neste diret√≥rio

### Problemas no GitHub
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**√öltima Atualiza√ß√£o:** 8 de outubro de 2025  
**Vers√£o:** Workshop Notebooks 2.0

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.