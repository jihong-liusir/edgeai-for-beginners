<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T21:17:35+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "br"
}
-->
# Sessão 3: Descoberta e Gerenciamento de Modelos Open-Source

## Visão Geral

Esta sessão foca na descoberta prática de modelos e no gerenciamento com o Foundry Local. Você aprenderá a listar modelos disponíveis, testar diferentes opções e entender características básicas de desempenho. A abordagem enfatiza a exploração prática com o CLI do Foundry para ajudá-lo a selecionar os modelos certos para seus casos de uso.

## Objetivos de Aprendizagem

- Dominar os comandos do CLI do Foundry para descoberta e gerenciamento de modelos
- Compreender padrões de cache de modelos e armazenamento local
- Aprender a testar e comparar rapidamente diferentes modelos
- Estabelecer fluxos de trabalho práticos para seleção e benchmarking de modelos
- Explorar o crescente ecossistema de modelos disponíveis no Foundry Local

## Pré-requisitos

- Conclusão da Sessão 1: Introdução ao Foundry Local
- CLI do Foundry Local instalado e acessível
- Espaço de armazenamento suficiente para download de modelos (os modelos podem variar de 1GB a mais de 20GB)
- Compreensão básica de tipos de modelos e casos de uso

## Parte 6: Exercício Prático

### Exercício: Descoberta e Comparação de Modelos

Crie seu próprio script de avaliação de modelos com base no Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Sua Tarefa

1. **Execute o script Sample 03**: `samples\03\list_and_bench.cmd`
2. **Teste diferentes modelos**: Experimente pelo menos 3 modelos diferentes
3. **Compare o desempenho**: Observe diferenças de velocidade e qualidade de resposta
4. **Documente os resultados**: Crie um gráfico simples de comparação

### Exemplo de Formato de Comparação

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Parte 7: Solução de Problemas e Melhores Práticas

### Problemas Comuns e Soluções

**O modelo não inicia:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Memória insuficiente:**
- Comece com modelos menores (`phi-4-mini`)
- Feche outros aplicativos
- Atualize a RAM se frequentemente atingir limites

**Desempenho lento:**
- Certifique-se de que o modelo está totalmente carregado (verifique a saída detalhada)
- Feche aplicativos em segundo plano desnecessários
- Considere armazenamento mais rápido (SSD)

### Melhores Práticas

1. **Comece pequeno**: Inicie com `phi-4-mini` para validar a configuração
2. **Um modelo por vez**: Pare modelos anteriores antes de iniciar novos
3. **Monitore recursos**: Fique atento ao uso de memória
4. **Teste de forma consistente**: Use os mesmos prompts para comparações justas
5. **Documente os resultados**: Mantenha notas sobre o desempenho dos modelos para seus casos de uso

## Parte 8: Próximos Passos e Referências

### Preparação para a Sessão 4

- **Foco da Sessão 4**: Ferramentas e técnicas de otimização
- **Pré-requisitos**: Familiaridade com troca de modelos e testes básicos de desempenho
- **Recomendado**: Identificar 2-3 modelos favoritos desta sessão

### Recursos Adicionais

- **[Documentação do Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Documentação oficial
- **[Referência CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Referência completa de comandos
- **[Model Mondays](https://aka.ms/model-mondays)**: Destaques semanais de modelos
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Comunidade e problemas
- **[Sample 03: Model Discovery](samples/03/README.md)**: Exemplo prático de script

### Principais Conclusões

✅ **Descoberta de Modelos**: Use `foundry model list` para explorar modelos disponíveis  
✅ **Testes Rápidos**: O padrão `list_and_bench.cmd` para avaliação rápida  
✅ **Monitoramento de Desempenho**: Uso básico de recursos e medição de tempo de resposta  
✅ **Seleção de Modelos**: Diretrizes práticas para escolher modelos por caso de uso  
✅ **Gerenciamento de Cache**: Compreensão de armazenamento e procedimentos de limpeza  

Agora você tem as habilidades práticas para descobrir, testar e selecionar modelos apropriados para suas aplicações de IA usando a abordagem simples do CLI do Foundry Local.

## Objetivos de Aprendizagem

- Descobrir e avaliar modelos open-source para inferência local
- Compilar e executar modelos selecionados do Hugging Face no Foundry Local
- Aplicar estratégias de seleção de modelos considerando precisão, latência e necessidades de recursos
- Gerenciar modelos localmente com cache e versionamento

## Parte 1: Descoberta de Modelos com o Foundry CLI

### Comandos Básicos de Gerenciamento de Modelos

O CLI do Foundry oferece comandos simples para descoberta e gerenciamento de modelos:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Executando Seus Primeiros Modelos

Comece com modelos populares e bem testados para entender características de desempenho:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Nota:** A flag `--verbose` fornece informações detalhadas de inicialização, incluindo:
- Progresso do download do modelo (na primeira execução)
- Detalhes de alocação de memória
- Informações de vinculação de serviço
- Métricas de inicialização de desempenho

### Compreendendo Categorias de Modelos

**Modelos de Linguagem Pequenos (SLMs):**
- `phi-4-mini`: Rápido, eficiente, ótimo para conversas gerais
- `phi-4`: Versão mais capaz com melhor raciocínio

**Modelos Médios:**
- `qwen2.5-7b-instruct`: Excelente raciocínio e contexto mais longo
- `deepseek-r1-distill-qwen-7b`: Otimizado para geração de código

**Modelos Maiores:**
- `llama-3.2`: Último modelo open-source da Meta
- `qwen2.5-14b-instruct`: Raciocínio de nível empresarial

## Parte 2: Teste Rápido e Comparação de Modelos

### Abordagem do Sample 03: Lista e Benchmark Simples

Com base no padrão do Sample 03, aqui está o fluxo de trabalho mínimo:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testando o Desempenho do Modelo

Depois que um modelo estiver em execução, teste-o com prompts consistentes:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternativa de Teste com PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Parte 3: Gerenciamento de Cache e Armazenamento de Modelos

### Compreendendo o Cache de Modelos

O Foundry Local gerencia automaticamente downloads e cache de modelos:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Considerações sobre Armazenamento de Modelos

**Tamanhos Típicos de Modelos:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Melhores Práticas de Armazenamento:**
- Mantenha 2-3 modelos em cache para troca rápida
- Remova modelos não utilizados para liberar espaço: `foundry cache clean`
- Monitore o uso do disco, especialmente em SSDs menores
- Considere o trade-off entre tamanho e capacidade do modelo

### Monitoramento de Desempenho de Modelos

Enquanto os modelos estão em execução, monitore os recursos do sistema:

**Gerenciador de Tarefas do Windows:**
- Observe o uso de memória (os modelos permanecem carregados na RAM)
- Monitore a utilização da CPU durante a inferência
- Verifique o I/O do disco durante o carregamento inicial do modelo

**Monitoramento via Linha de Comando:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Parte 4: Diretrizes Práticas para Seleção de Modelos

### Escolhendo Modelos por Caso de Uso

**Para Conversas Gerais e Q&A:**
- Comece com: `phi-4-mini` (rápido, eficiente)
- Atualize para: `phi-4` (melhor raciocínio)
- Avançado: `qwen2.5-7b-instruct` (contexto mais longo)

**Para Geração de Código:**
- Recomendado: `deepseek-r1-distill-qwen-7b`
- Alternativa: `qwen2.5-7b-instruct` (também bom para código)

**Para Raciocínio Complexo:**
- Melhor: `qwen2.5-7b-instruct` ou `qwen2.5-14b-instruct`
- Opção econômica: `phi-4`

### Guia de Requisitos de Hardware

**Requisitos Mínimos do Sistema:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Recomendado para Melhor Desempenho:**
- 32GB+ de RAM para troca confortável entre modelos
- Armazenamento SSD para carregamento mais rápido de modelos
- CPU moderna com bom desempenho de thread único
- Suporte a NPU (PCs com Windows 11 Copilot+) para aceleração

### Fluxo de Trabalho para Troca de Modelos

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Parte 5: Benchmarking Simples de Modelos

### Teste Básico de Desempenho

Aqui está uma abordagem direta para comparar o desempenho de modelos:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Avaliação Manual de Qualidade

Para cada modelo, teste com prompts consistentes e avalie manualmente:

**Prompts de Teste:**
1. "Explique computação quântica de forma simples."
2. "Escreva uma função Python para ordenar uma lista."
3. "Quais são os prós e contras do trabalho remoto?"
4. "Resuma os benefícios da IA de borda."

**Critérios de Avaliação:**
- **Precisão**: As informações estão corretas?
- **Clareza**: A explicação é fácil de entender?
- **Completude**: Responde à pergunta por completo?
- **Velocidade**: Quão rápido responde?

### Monitoramento de Uso de Recursos

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Parte 6: Próximos Passos

- Inscreva-se no Model Mondays para novos modelos e dicas: https://aka.ms/model-mondays
- Contribua com descobertas para o `models.json` da sua equipe
- Prepare-se para a Sessão 4: comparando LLMs vs SLMs, inferência local vs na nuvem e demonstrações práticas

---

