<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T23:56:03+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "br"
}
-->
# Se√ß√£o 4: Implanta√ß√£o - Implementa√ß√£o de Modelo Pronto para Produ√ß√£o

## Vis√£o Geral

Este tutorial abrangente ir√° gui√°-lo por todo o processo de implanta√ß√£o de modelos quantizados ajustados usando o Foundry Local. Vamos abordar a convers√£o de modelos, otimiza√ß√£o de quantiza√ß√£o e configura√ß√£o de implanta√ß√£o do in√≠cio ao fim.

## Pr√©-requisitos

Antes de come√ßar, certifique-se de ter o seguinte:

- ‚úÖ Um modelo onnx ajustado pronto para implanta√ß√£o
- ‚úÖ Computador com Windows ou Mac
- ‚úÖ Python 3.10 ou superior
- ‚úÖ Pelo menos 8GB de RAM dispon√≠vel
- ‚úÖ Foundry Local instalado no seu sistema

## Parte 1: Configura√ß√£o do Ambiente

### Instalando Ferramentas Necess√°rias

Abra seu terminal (Prompt de Comando no Windows, Terminal no Mac) e execute os seguintes comandos em sequ√™ncia:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Nota Importante**: Voc√™ tamb√©m precisar√° da vers√£o 3.31 ou mais recente do CMake, que pode ser baixada em [cmake.org](https://cmake.org/download/).

## Parte 2: Convers√£o e Quantiza√ß√£o do Modelo

### Escolhendo o Formato Certo

Para modelos de linguagem pequenos ajustados, recomendamos usar o formato **ONNX**, pois ele oferece:

- üöÄ Melhor otimiza√ß√£o de desempenho
- üîß Implanta√ß√£o independente de hardware
- üè≠ Capacidades prontas para produ√ß√£o
- üì± Compatibilidade entre plataformas

### M√©todo 1: Convers√£o com um Comando (Recomendado)

Use o seguinte comando para converter diretamente seu modelo ajustado:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Explica√ß√£o dos Par√¢metros:**
- `--model_name_or_path`: Caminho para o seu modelo ajustado
- `--device cpu`: Usa CPU para otimiza√ß√£o
- `--precision int4`: Usa quantiza√ß√£o INT4 (redu√ß√£o de tamanho de aproximadamente 75%)
- `--output_path`: Caminho de sa√≠da para o modelo convertido

### M√©todo 2: Abordagem com Arquivo de Configura√ß√£o (Usu√°rios Avan√ßados)

Crie um arquivo de configura√ß√£o chamado `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Em seguida, execute:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Compara√ß√£o de Op√ß√µes de Quantiza√ß√£o

| Precis√£o | Tamanho do Arquivo | Velocidade de Infer√™ncia | Qualidade do Modelo | Uso Recomendado |
|----------|--------------------|--------------------------|---------------------|-----------------|
| FP16     | Base √ó 0.5        | R√°pida                  | Melhor             | Hardware de alto desempenho |
| INT8     | Base √ó 0.25       | Muito R√°pida            | Boa                | Escolha equilibrada |
| INT4     | Base √ó 0.125      | Mais R√°pida             | Aceit√°vel          | Recursos limitados |

üí° **Recomenda√ß√£o**: Comece com a quantiza√ß√£o INT4 para sua primeira implanta√ß√£o. Se a qualidade n√£o for satisfat√≥ria, experimente INT8 ou FP16.

## Parte 3: Configura√ß√£o de Implanta√ß√£o no Foundry Local

### Criando a Configura√ß√£o do Modelo

Navegue at√© o diret√≥rio de modelos do Foundry Local:

```bash
foundry cache cd ./models/
```

Crie a estrutura de diret√≥rios do seu modelo:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Crie o arquivo de configura√ß√£o `inference_model.json` no diret√≥rio do seu modelo:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configura√ß√µes de Template Espec√≠ficas para Modelos

#### Para Modelos da S√©rie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Parte 4: Teste e Otimiza√ß√£o do Modelo

### Verificando a Instala√ß√£o do Modelo

Verifique se o Foundry Local consegue reconhecer seu modelo:

```bash
foundry cache ls
```

Voc√™ deve ver `your-finetuned-model-int4` na lista.

### Iniciando o Teste do Modelo

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking de Desempenho

Monitore m√©tricas-chave durante o teste:

1. **Tempo de Resposta**: Me√ßa o tempo m√©dio por resposta
2. **Uso de Mem√≥ria**: Monitore o consumo de RAM
3. **Utiliza√ß√£o da CPU**: Verifique a carga do processador
4. **Qualidade de Sa√≠da**: Avalie a relev√¢ncia e a coer√™ncia das respostas

### Lista de Verifica√ß√£o de Valida√ß√£o de Qualidade

- ‚úÖ O modelo responde adequadamente a consultas do dom√≠nio ajustado
- ‚úÖ O formato da resposta corresponde √† estrutura de sa√≠da esperada
- ‚úÖ Sem vazamentos de mem√≥ria durante uso prolongado
- ‚úÖ Desempenho consistente em diferentes comprimentos de entrada
- ‚úÖ Manipula√ß√£o adequada de casos extremos e entradas inv√°lidas

## Resumo

Parab√©ns! Voc√™ concluiu com sucesso:

- ‚úÖ Convers√£o de formato de modelo ajustado
- ‚úÖ Otimiza√ß√£o de quantiza√ß√£o do modelo
- ‚úÖ Configura√ß√£o de implanta√ß√£o no Foundry Local
- ‚úÖ Ajuste de desempenho e solu√ß√£o de problemas

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.