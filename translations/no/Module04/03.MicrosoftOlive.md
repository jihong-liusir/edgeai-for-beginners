<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T10:22:04+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "no"
}
-->
# Seksjon 3: Microsoft Olive Optimaliseringsverktøy

## Innholdsfortegnelse
1. [Introduksjon](../../../Module04)
2. [Hva er Microsoft Olive?](../../../Module04)
3. [Installasjon](../../../Module04)
4. [Hurtigstartguide](../../../Module04)
5. [Eksempel: Konvertering av Qwen3 til ONNX INT4](../../../Module04)
6. [Avansert bruk](../../../Module04)
7. [Beste praksis](../../../Module04)
8. [Feilsøking](../../../Module04)
9. [Ekstra ressurser](../../../Module04)

## Introduksjon

Microsoft Olive er et kraftig og brukervennlig verktøy for maskinvarebevisst modelloptimalisering som forenkler prosessen med å optimalisere maskinlæringsmodeller for distribusjon på ulike maskinvareplattformer. Enten du sikter mot CPU-er, GPU-er eller spesialiserte AI-akseleratorer, hjelper Olive deg med å oppnå optimal ytelse samtidig som modellens nøyaktighet opprettholdes.

## Hva er Microsoft Olive?

Olive er et brukervennlig verktøy for maskinvarebevisst modelloptimalisering som kombinerer ledende teknikker innen modellkompresjon, optimalisering og kompilering. Det fungerer med ONNX Runtime som en ende-til-ende løsning for inferensoptimalisering.

### Nøkkelfunksjoner

- **Maskinvarebevisst optimalisering**: Velger automatisk de beste optimaliseringsteknikkene for din målmaskinvare
- **40+ innebygde optimaliseringskomponenter**: Dekker modellkompresjon, kvantisering, grafoptimalisering og mer
- **Enkel CLI-grensesnitt**: Enkle kommandoer for vanlige optimaliseringsoppgaver
- **Støtte for flere rammeverk**: Fungerer med PyTorch, Hugging Face-modeller og ONNX
- **Støtte for populære modeller**: Olive kan automatisk optimalisere populære modellarkitekturer som Llama, Phi, Qwen, Gemma, osv. rett ut av boksen

### Fordeler

- **Redusert utviklingstid**: Ingen behov for manuell eksperimentering med ulike optimaliseringsteknikker
- **Ytelsesforbedringer**: Betydelige hastighetsforbedringer (opptil 6x i noen tilfeller)
- **Plattformuavhengig distribusjon**: Optimaliserte modeller fungerer på tvers av ulike maskinvare og operativsystemer
- **Bevart nøyaktighet**: Optimaliseringer opprettholder modellkvalitet samtidig som ytelsen forbedres

## Installasjon

### Forutsetninger

- Python 3.8 eller nyere
- pip-pakkebehandler
- Virtuelt miljø (anbefalt)

### Grunnleggende installasjon

Opprett og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installer Olive med funksjoner for automatisk optimalisering:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valgfrie avhengigheter

Olive tilbyr ulike valgfrie avhengigheter for ekstra funksjoner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifiser installasjonen

```bash
olive --help
```

Hvis installasjonen er vellykket, skal du se Olive CLI-hjelpemeldingen.

## Hurtigstartguide

### Din første optimalisering

La oss optimalisere en liten språkmodell ved hjelp av Olives funksjon for automatisk optimalisering:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hva denne kommandoen gjør

Optimaliseringsprosessen innebærer: å hente modellen fra den lokale cachen, fange ONNX-grafen og lagre vektene i en ONNX-datafil, optimalisere ONNX-grafen og kvantisere modellen til int4 ved hjelp av RTN-metoden.

### Forklaring av kommandoens parametere

- `--model_name_or_path`: Hugging Face-modellidentifikator eller lokal sti
- `--output_path`: Katalog der den optimaliserte modellen lagres
- `--device`: Målmaskinvare (cpu, gpu)
- `--provider`: Utførelsesleverandør (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Bruk ONNX Runtime Generate AI for inferens
- `--precision`: Kvantiseringspresisjon (int4, int8, fp16)
- `--log_level`: Loggnivå (0=minimalt, 1=detaljert)

## Eksempel: Konvertering av Qwen3 til ONNX INT4

Basert på det oppgitte Hugging Face-eksempelet på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), slik kan du optimalisere en Qwen3-modell:

### Steg 1: Last ned modellen (valgfritt)

For å minimere nedlastingstid, cache kun nødvendige filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Steg 2: Optimaliser Qwen3-modellen

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Steg 3: Test den optimaliserte modellen

Opprett et enkelt Python-skript for å teste den optimaliserte modellen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output-struktur

Etter optimalisering vil utgangskatalogen inneholde:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avansert bruk

### Konfigurasjonsfiler

For mer komplekse optimaliseringsarbeidsflyter kan du bruke JSON-konfigurasjonsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kjør med konfigurasjon:

```bash
olive run --config config.json
```

### GPU-optimalisering

For CUDA GPU-optimalisering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

For DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive støtter også finjustering av modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Beste praksis

### 1. Modellvalg
- Start med mindre modeller for testing (f.eks. 0.5B-7B parametere)
- Sørg for at målmodellens arkitektur støttes av Olive

### 2. Maskinvarehensyn
- Match optimaliseringsmålet med distribusjonsmaskinvaren din
- Bruk GPU-optimalisering hvis du har CUDA-kompatibel maskinvare
- Vurder DirectML for Windows-maskiner med integrert grafikk

### 3. Presisjonsvalg
- **INT4**: Maksimal kompresjon, liten nøyaktighetstap
- **INT8**: God balanse mellom størrelse og nøyaktighet
- **FP16**: Minimalt nøyaktighetstap, moderat størrelsesreduksjon

### 4. Testing og validering
- Test alltid optimaliserte modeller med dine spesifikke bruksområder
- Sammenlign ytelsesmålinger (latens, gjennomstrømning, nøyaktighet)
- Bruk representativ inputdata for evaluering

### 5. Iterativ optimalisering
- Start med automatisk optimalisering for raske resultater
- Bruk konfigurasjonsfiler for detaljert kontroll
- Eksperimenter med ulike optimaliseringspass

## Feilsøking

### Vanlige problemer

#### 1. Installasjonsproblemer
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problemer
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Minneproblemer
- Bruk mindre batch-størrelser under optimalisering
- Prøv kvantisering med høyere presisjon først (int8 i stedet for int4)
- Sørg for tilstrekkelig diskplass for modellcaching

#### 4. Modellinnlastingsfeil
- Verifiser modellsti og tilgangstillatelser
- Sjekk om modellen krever `trust_remote_code=True`
- Sørg for at alle nødvendige modelfiler er lastet ned

### Få hjelp

- **Dokumentasjon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Eksempler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Ekstra ressurser

### Offisielle lenker
- **GitHub-repositorium**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime-dokumentasjon**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-eksempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Fellesskapseksempler
- **Jupyter Notebooks**: Tilgjengelig i Olive GitHub-repositoriet
- **VS Code-utvidelse**: AI Toolkit-utvidelsen bruker Olive for modelloptimalisering
- **Blogginnlegg**: Microsoft Open Source Blog har detaljerte Olive-veiledninger

### Relaterte verktøy
- **ONNX Runtime**: Høyytelses inferensmotor
- **Hugging Face Transformers**: Kilde til mange kompatible modeller
- **Azure Machine Learning**: Skybaserte optimaliseringsarbeidsflyter

## ➡️ Hva er neste

- [04: OpenVINO Toolkit Optimaliseringsverktøy](./04.openvino.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.