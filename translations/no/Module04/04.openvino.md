<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T10:16:15+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "no"
}
-->
# Seksjon 4: OpenVINO Toolkit Optimaliseringssuite

## Innholdsfortegnelse
1. [Introduksjon](../../../Module04)
2. [Hva er OpenVINO?](../../../Module04)
3. [Installasjon](../../../Module04)
4. [Hurtigstartguide](../../../Module04)
5. [Eksempel: Konvertering og optimalisering av modeller med OpenVINO](../../../Module04)
6. [Avansert bruk](../../../Module04)
7. [Beste praksis](../../../Module04)
8. [Feilsøking](../../../Module04)
9. [Tilleggsressurser](../../../Module04)

## Introduksjon

OpenVINO (Open Visual Inference and Neural Network Optimization) er Intels åpen kildekode-verktøysett for å distribuere høytytende AI-løsninger på tvers av sky, lokale miljøer og kant-enheter. Enten du jobber med CPU-er, GPU-er, VPU-er eller spesialiserte AI-akseleratorer, gir OpenVINO omfattende optimaliseringsmuligheter samtidig som modellens nøyaktighet opprettholdes og plattformuavhengig distribusjon muliggjøres.

## Hva er OpenVINO?

OpenVINO er et åpen kildekode-verktøysett som gjør det mulig for utviklere å optimalisere, konvertere og distribuere AI-modeller effektivt på tvers av ulike maskinvareplattformer. Det består av tre hovedkomponenter: OpenVINO Runtime for inferens, Neural Network Compression Framework (NNCF) for modelloptimalisering, og OpenVINO Model Server for skalerbar distribusjon.

### Nøkkelfunksjoner

- **Plattformuavhengig distribusjon**: Støtter Linux, Windows og macOS med Python-, C++- og C-API-er
- **Maskinvareakselerasjon**: Automatisk enhetsoppdagelse og optimalisering for CPU, GPU, VPU og AI-akseleratorer
- **Modellkompresjonsrammeverk**: Avanserte teknikker for kvantisering, beskjæring og optimalisering via NNCF
- **Rammeverkskompatibilitet**: Direkte støtte for TensorFlow-, ONNX-, PaddlePaddle- og PyTorch-modeller
- **Generativ AI-støtte**: Spesialisert OpenVINO GenAI for distribusjon av store språkmodeller og generative AI-applikasjoner

### Fordeler

- **Ytelsesoptimalisering**: Betydelige hastighetsforbedringer med minimal nøyaktighetstap
- **Redusert distribusjonsfotavtrykk**: Minimale eksterne avhengigheter forenkler installasjon og distribusjon
- **Forbedret oppstartstid**: Optimalisert modellinnlasting og caching for raskere applikasjonsstart
- **Skalerbar distribusjon**: Fra kant-enheter til skyinfrastruktur med konsistente API-er
- **Klar for produksjon**: Pålitelighet på bedriftsnivå med omfattende dokumentasjon og fellesskapsstøtte

## Installasjon

### Forutsetninger

- Python 3.8 eller nyere
- pip-pakkebehandler
- Virtuelt miljø (anbefalt)
- Kompatibel maskinvare (Intel CPU-er anbefales, men støtter ulike arkitekturer)

### Grunnleggende installasjon

Opprett og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installer OpenVINO Runtime:

```bash
pip install openvino
```

Installer NNCF for modelloptimalisering:

```bash
pip install nncf
```

### OpenVINO GenAI-installasjon

For generative AI-applikasjoner:

```bash
pip install openvino-genai
```

### Valgfrie avhengigheter

Ekstra pakker for spesifikke bruksområder:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verifiser installasjonen

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Hvis installasjonen er vellykket, skal du se OpenVINO-versjonsinformasjonen.

## Hurtigstartguide

### Din første modelloptimalisering

La oss konvertere og optimalisere en Hugging Face-modell ved hjelp av OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Hva denne prosessen gjør

Optimaliseringsarbeidsflyten innebærer: å laste inn den originale modellen fra Hugging Face, konvertere til OpenVINO Intermediate Representation (IR)-format, bruke standardoptimaliseringer og kompilere for målmaskinvare.

### Forklaring av nøkkelparametere

- `export=True`: Konverterer modellen til OpenVINO IR-format
- `compile=False`: Utsetter kompilering til kjøretid for fleksibilitet
- `device`: Målmaskinvare ("CPU", "GPU", "AUTO" for automatisk valg)
- `save_pretrained()`: Lagrer den optimaliserte modellen for gjenbruk

## Eksempel: Konvertering og optimalisering av modeller med OpenVINO

### Trinn 1: Modellkonvertering med NNCF-kvantisering

Slik bruker du ettertreningskvantisering med NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Trinn 2: Avansert optimalisering med vektkompresjon

For transformatorbaserte modeller, bruk vektkompresjon:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Trinn 3: Inferens med optimalisert modell

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Utgangsstruktur

Etter optimalisering vil modellmappen din inneholde:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Avansert bruk

### Konfigurasjon med NNCF YAML

For komplekse optimaliseringsarbeidsflyter, bruk NNCF-konfigurasjonsfiler:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Bruk konfigurasjonen:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-optimalisering

For GPU-akselerasjon:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimalisering av batch-prosessering

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modellserverdistribusjon

Distribuer optimaliserte modeller med OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Klientkode for modellserver:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Beste praksis

### 1. Modellvalg og forberedelse
- Bruk modeller fra støttede rammeverk (PyTorch, TensorFlow, ONNX)
- Sørg for at modellens inndata har faste eller kjente dynamiske former
- Test med representative datasett for kalibrering

### 2. Valg av optimaliseringsstrategi
- **Ettertreningskvantisering**: Start her for rask optimalisering
- **Vektkompresjon**: Ideelt for store språkmodeller og transformatorer
- **Kvantisering med bevissthet om nøyaktighet**: Bruk når nøyaktighet er kritisk

### 3. Maskinvare-spesifikk optimalisering
- **CPU**: Bruk INT8-kvantisering for balansert ytelse
- **GPU**: Utnytt FP16-presisjon og batch-prosessering
- **VPU**: Fokuser på modellsimplifisering og lagfusjon

### 4. Ytelsestilpasning
- **Gjennomstrømmingsmodus**: For høyvolum batch-prosessering
- **Latensmodus**: For sanntids interaktive applikasjoner
- **AUTO-enhet**: La OpenVINO velge optimal maskinvare

### 5. Minnehåndtering
- Bruk dynamiske former med omhu for å unngå minneoverhead
- Implementer modellcaching for raskere påfølgende innlastinger
- Overvåk minnebruk under optimalisering

### 6. Validering av nøyaktighet
- Valider alltid optimaliserte modeller mot original ytelse
- Bruk representative testdatasett for evaluering
- Vurder gradvis optimalisering (start med konservative innstillinger)

## Feilsøking

### Vanlige problemer

#### 1. Installasjonsproblemer
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modellkonverteringsfeil
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Ytelsesproblemer
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Minneproblemer
- Reduser modellens batch-størrelse under optimalisering
- Bruk streaming for store datasett
- Aktiver modellcaching: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Nøyaktighetstap
- Bruk høyere presisjon (INT8 i stedet for INT4)
- Øk kalibreringsdatasettets størrelse
- Bruk blandet presisjonsoptimalisering

### Ytelsesovervåking

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Få hjelp

- **Dokumentasjon**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Fellesskapsforum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Tilleggsressurser

### Offisielle lenker
- **OpenVINO Hjemmeside**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Læringsressurser
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Hurtigstartguide**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimaliseringsguide**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integrasjonsverktøy
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Ytelsesbenchmarker
- **Offisielle benchmarker**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Fellesskapseksempler
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Omfattende opplæringer tilgjengelig i OpenVINO-notebooks-repositoriet
- **Eksempelapplikasjoner**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Virkelige eksempler for ulike domener (datamaskinsyn, NLP, lyd)
- **Blogginnlegg**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI og fellesskapsblogginnlegg med detaljerte bruksområder

### Relaterte verktøy
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Ytterligere optimaliseringsteknikker for Intel-maskinvare
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - For sammenligning av distribusjon på mobil og kant
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Tverrplattform inferensmotoralternativer

## ➡️ Hva er neste

- [05: Apple MLX Framework Dypdykk](./05.AppleMLX.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.