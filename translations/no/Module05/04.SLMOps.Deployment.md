<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T10:37:19+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "no"
}
-->
# Seksjon 4: Implementering av produksjonsklar modell

## Oversikt

Denne omfattende veiledningen vil lede deg gjennom hele prosessen med √• distribuere finjusterte kvantiserte modeller ved hjelp av Foundry Local. Vi dekker modellkonvertering, kvantiseringsoptimalisering og distribusjonskonfigurasjon fra start til slutt.

## Forutsetninger

F√∏r du begynner, s√∏rg for at du har f√∏lgende:

- ‚úÖ En finjustert onnx-modell klar for distribusjon
- ‚úÖ Windows- eller Mac-datamaskin
- ‚úÖ Python 3.10 eller nyere
- ‚úÖ Minst 8GB tilgjengelig RAM
- ‚úÖ Foundry Local installert p√• systemet ditt

## Del 1: Oppsett av milj√∏

### Installere n√∏dvendige verkt√∏y

√Öpne terminalen din (Command Prompt p√• Windows, Terminal p√• Mac) og kj√∏r f√∏lgende kommandoer i rekkef√∏lge:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Viktig merknad**: Du trenger ogs√• CMake versjon 3.31 eller nyere, som kan lastes ned fra [cmake.org](https://cmake.org/download/).

## Del 2: Modellkonvertering og kvantisering

### Velge riktig format

For finjusterte sm√• spr√•kmodeller anbefaler vi √• bruke **ONNX-format** fordi det tilbyr:

- üöÄ Bedre ytelsesoptimalisering
- üîß Maskinvareuavhengig distribusjon
- üè≠ Produksjonsklare egenskaper
- üì± Plattformuavhengig kompatibilitet

### Metode 1: Konvertering med √©n kommando (Anbefalt)

Bruk f√∏lgende kommando for √• direkte konvertere din finjusterte modell:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Forklaring av parametere:**
- `--model_name_or_path`: Sti til din finjusterte modell
- `--device cpu`: Bruk CPU for optimalisering
- `--precision int4`: Bruk INT4-kvantisering (omtrent 75% reduksjon i st√∏rrelse)
- `--output_path`: Utgangssti for den konverterte modellen

### Metode 2: Konfigurasjonsfiltiln√¶rming (For avanserte brukere)

Opprett en konfigurasjonsfil kalt `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Kj√∏r deretter:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Sammenligning av kvantiseringsalternativer

| Presisjon | Filst√∏rrelse | Inference-hastighet | Modellkvalitet | Anbefalt bruk |
|-----------|--------------|---------------------|----------------|---------------|
| FP16      | Baseline √ó 0.5 | Rask | Best | H√∏y-end maskinvare |
| INT8      | Baseline √ó 0.25 | Veldig rask | God | Balansert valg |
| INT4      | Baseline √ó 0.125 | Raskest | Akseptabel | Ressursbegrenset |

üí° **Anbefaling**: Start med INT4-kvantisering for din f√∏rste distribusjon. Hvis kvaliteten ikke er tilfredsstillende, pr√∏v INT8 eller FP16.

## Del 3: Konfigurasjon for Foundry Local-distribusjon

### Opprette modellkonfigurasjon

Naviger til Foundry Local-modellkatalogen:

```bash
foundry cache cd ./models/
```

Opprett katalogstrukturen for modellen din:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Opprett konfigurasjonsfilen `inference_model.json` i modellkatalogen din:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Modellspecifikke malkonfigurasjoner

#### For Qwen-seriemodeller:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Del 4: Testing og optimalisering av modell

### Verifisere modellinstallasjon

Sjekk om Foundry Local kan gjenkjenne modellen din:

```bash
foundry cache ls
```

Du b√∏r se `your-finetuned-model-int4` i listen.

### Starte modelltesting

```bash
foundry model run your-finetuned-model-int4
```

### Ytelsesbenchmarking

Overv√•k n√∏kkelmetrikker under testing:

1. **Responstid**: M√•l gjennomsnittlig tid per respons
2. **Minnebruk**: Overv√•k RAM-forbruk
3. **CPU-utnyttelse**: Sjekk prosessorbelastning
4. **Utgangskvalitet**: Evaluer responsens relevans og sammenheng

### Kvalitetsvalideringssjekkliste

- ‚úÖ Modellen svarer riktig p√• finjusterte domeneforesp√∏rsler
- ‚úÖ Responsformatet samsvarer med forventet utgangsstruktur
- ‚úÖ Ingen minnelekkasjer under langvarig bruk
- ‚úÖ Konsistent ytelse p√• tvers av ulike inputlengder
- ‚úÖ Riktig h√•ndtering av kanttilfeller og ugyldige input

## Oppsummering

Gratulerer! Du har fullf√∏rt:

- ‚úÖ Konvertering av finjustert modellformat
- ‚úÖ Optimalisering av modellkvantisering
- ‚úÖ Konfigurasjon for Foundry Local-distribusjon
- ‚úÖ Ytelsestilpasning og feils√∏king

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.