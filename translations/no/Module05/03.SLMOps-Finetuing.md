<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T10:35:22+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "no"
}
-->
# Seksjon 3: Finjustering - Tilpasning av modeller for spesifikke oppgaver

## Innholdsfortegnelse
1. [Introduksjon til finjustering](../../../Module05)
2. [Hvorfor finjustering er viktig](../../../Module05)
3. [Typer av finjustering](../../../Module05)
4. [Finjustering med Microsoft Olive](../../../Module05)
5. [Praktiske eksempler](../../../Module05)
6. [Beste praksis og retningslinjer](../../../Module05)
7. [Avanserte teknikker](../../../Module05)
8. [Evaluering og overvåking](../../../Module05)
9. [Vanlige utfordringer og løsninger](../../../Module05)
10. [Konklusjon](../../../Module05)

## Introduksjon til finjustering

**Finjustering** er en kraftig maskinlæringsteknikk som innebærer å tilpasse en forhåndstrent modell til å utføre spesifikke oppgaver eller arbeide med spesialiserte datasett. I stedet for å trene en modell fra bunnen av, utnytter finjustering kunnskapen som allerede er lært av en forhåndstrent modell og justerer den for din spesifikke bruk.

### Hva er finjustering?

Finjustering er en form for **overføringslæring** der du:
- Starter med en forhåndstrent modell som har lært generelle mønstre fra store datasett
- Justerer modellens interne parametere ved hjelp av ditt spesifikke datasett
- Beholder verdifull kunnskap mens du spesialiserer modellen for din oppgave

Tenk på det som å lære en dyktig kokk å lage en ny type mat – de forstår allerede grunnleggende matlagingsprinsipper, men må lære spesifikke teknikker og smaker for den nye stilen.

### Viktige fordeler

- **Tidsbesparende**: Betydelig raskere enn å trene fra bunnen av
- **Dataeffektiv**: Krever mindre datasett for å oppnå god ytelse
- **Kostnadseffektiv**: Lavere krav til beregningsressurser
- **Bedre ytelse**: Oppnår ofte bedre resultater sammenlignet med trening fra bunnen av
- **Ressursoptimalisering**: Gjør kraftig AI tilgjengelig for mindre team og organisasjoner

## Hvorfor finjustering er viktig

### Virkelige applikasjoner

Finjustering er avgjørende i mange situasjoner:

**1. Tilpasning til domene**
- Medisinsk AI: Tilpasse generelle språkmodeller for medisinsk terminologi og kliniske notater
- Juridisk teknologi: Spesialisere modeller for analyse av juridiske dokumenter og kontrakter
- Finansielle tjenester: Tilpasse modeller for analyse av finansielle rapporter og risikovurdering

**2. Oppgavespesialisering**
- Innholdsgenerering: Finjustering for spesifikke skrivestiler eller toner
- Kodegenerering: Tilpasse modeller for spesifikke programmeringsspråk eller rammeverk
- Oversettelse: Forbedre ytelsen for spesifikke språkpar eller tekniske domener

**3. Bedriftsapplikasjoner**
- Kundeservice: Lage chatboter som forstår bedriftsspesifikk terminologi
- Intern dokumentasjon: Bygge AI-assistenter som er kjent med organisasjonens prosesser
- Bransjespesifikke løsninger: Utvikle modeller som forstår sektor-spesifikk sjargong og arbeidsflyt

## Typer av finjustering

### 1. Full finjustering (Instruksjonsfinjustering)

Ved full finjustering oppdateres alle modellens parametere under trening. Denne tilnærmingen:
- Gir maksimal fleksibilitet og ytelsespotensial
- Krever betydelige beregningsressurser
- Resulterer i en helt ny versjon av modellen
- Best egnet for situasjoner der du har omfattende treningsdata og beregningsressurser

### 2. Parameter-effektiv finjustering (PEFT)

PEFT-metoder oppdaterer kun en liten del av parametrene, noe som gjør prosessen mer effektiv:

#### Low-Rank Adaptation (LoRA)
- Legger til små trenbare matriser til eksisterende vekter
- Reduserer antall trenbare parametere dramatisk
- Opprettholder ytelse nær full finjustering
- Gjør det enkelt å bytte mellom ulike tilpasninger

#### QLoRA (Quantized LoRA)
- Kombinerer LoRA med kvantiseringsteknikker
- Reduserer minnekrav ytterligere
- Muliggjør finjustering av større modeller på forbrukerhardware
- Balanserer effektivitet med ytelse

#### Adapters
- Setter inn små nevrale nettverk mellom eksisterende lag
- Tillater målrettet finjustering mens basismodellen forblir uendret
- Muliggjør en modulær tilnærming til modelltilpasning

### 3. Oppgavespesifikk finjustering

Fokuserer på å tilpasse modeller for spesifikke oppgaver:
- **Klassifisering**: Justere modeller for kategoriseringsoppgaver
- **Generering**: Optimalisere for innholdsskaping og tekstgenerering
- **Ekstraksjon**: Finjustering for informasjonsuttrekk og navngitt enhetsgjenkjenning
- **Oppsummering**: Spesialisere modeller for dokumentoppsummering

## Finjustering med Microsoft Olive

Microsoft Olive er et omfattende verktøy for modelloptimalisering som forenkler finjusteringsprosessen samtidig som det tilbyr funksjoner på bedriftsnivå.

### Hva er Microsoft Olive?

Microsoft Olive er et åpen kildekode-verktøy for modelloptimalisering som:
- Strømlinjeformer finjusteringsarbeidsflyter for ulike hardware-mål
- Gir innebygd støtte for populære modellarkitekturer (Llama, Phi, Qwen, Gemma)
- Tilbyr både sky- og lokal distribusjonsalternativer
- Integreres sømløst med Azure ML og andre Microsoft AI-tjenester
- Støtter automatisk optimalisering og kvantisering

### Viktige funksjoner

- **Hardware-bevisst optimalisering**: Optimaliserer modeller automatisk for spesifikke hardware (CPU, GPU, NPU)
- **Multi-format støtte**: Fungerer med PyTorch, Hugging Face og ONNX-modeller
- **Automatiserte arbeidsflyter**: Reduserer manuell konfigurasjon og prøving og feiling
- **Bedriftsintegrasjon**: Innebygd støtte for Azure ML og skybaserte distribusjoner
- **Utvidbar arkitektur**: Tillater tilpassede optimaliseringsteknikker

### Installasjon og oppsett

#### Grunnleggende installasjon

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Valgfrie avhengigheter

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Verifiser installasjon

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktiske eksempler

### Eksempel 1: Grunnleggende finjustering med Olive CLI

Dette eksemplet demonstrerer finjustering av en liten språkmodell for fraseklassifisering:

#### Steg 1: Forbered miljøet ditt

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Steg 2: Finjuster modellen

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Steg 3: Optimaliser for distribusjon

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Eksempel 2: Avansert konfigurasjon med tilpasset datasett

#### Steg 1: Forbered tilpasset datasett

Lag en JSON-fil med treningsdataene dine:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Steg 2: Lag konfigurasjonsfil

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Steg 3: Utfør finjustering

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Eksempel 3: QLoRA finjustering for minneeffektivitet

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Beste praksis og retningslinjer

### Datapreparering

**1. Datakvalitet fremfor kvantitet**
- Prioriter høykvalitets, mangfoldige eksempler fremfor store mengder dårlig data
- Sørg for at dataene er representative for din målbruk
- Rens og forhåndsprosesser dataene konsekvent

**2. Dataformat og maler**
- Bruk konsekvent formatering på alle treningsdata
- Lag klare input-output maler som passer til din bruk
- Inkluder passende instruksjonsformatering for instruksjonsfinjusterte modeller

**3. Datasett-deling**
- Reserver 10-20 % av dataene til validering
- Oppretthold lignende distribusjoner mellom trenings- og valideringsdeler
- Vurder stratifisert sampling for klassifiseringsoppgaver

### Treningskonfigurasjon

**1. Valg av læringsrate**
- Start med mindre læringsrater (1e-5 til 1e-4) for finjustering
- Bruk læringsrateplanlegging for bedre konvergens
- Overvåk tapkurver for å justere rater deretter

**2. Optimalisering av batch-størrelse**
- Balanser batch-størrelse med tilgjengelig minne
- Bruk gradientakkumulering for større effektiv batch-størrelse
- Vurder forholdet mellom batch-størrelse og læringsrate

**3. Treningsvarighet**
- Overvåk valideringsmetrikker for å unngå overtilpasning
- Bruk tidlig stopp når valideringsytelsen flater ut
- Lagre sjekkpunkter regelmessig for gjenoppretting og analyse

### Modellvalg

**1. Valg av basismodell**
- Velg modeller forhåndstrent på lignende domener når mulig
- Vurder modellstørrelse i forhold til dine beregningsbegrensninger
- Evaluer lisenskrav for kommersiell bruk

**2. Valg av finjusteringsmetode**
- Bruk LoRA/QLoRA for ressursbegrensede miljøer
- Velg full finjustering når maksimal ytelse er kritisk
- Vurder adapter-baserte tilnærminger for flere oppgavescenarier

### Ressursstyring

**1. Hardware-optimalisering**
- Velg passende hardware for modellstørrelse og metode
- Utnytt GPU-minne effektivt med gradient-sjekkpunkt
- Vurder skybaserte løsninger for større modeller

**2. Minnehåndtering**
- Bruk trening med blandet presisjon når tilgjengelig
- Implementer gradientakkumulering for minnebegrensninger
- Overvåk GPU-minnebruk gjennom hele treningen

## Avanserte teknikker

### Multi-adapter trening

Tren flere adaptere for ulike oppgaver mens du deler basismodellen:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Hyperparameteroptimalisering

Implementer systematisk hyperparameterjustering:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Tilpassede tapfunksjoner

Implementer domene-spesifikke tapfunksjoner:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Evaluering og overvåking

### Metrikker og evaluering

**1. Standardmetrikker**
- **Nøyaktighet**: Total korrekthet for klassifiseringsoppgaver
- **Perpleksitet**: Kvalitetsmål for språkmodellering
- **BLEU/ROUGE**: Kvalitet for tekstgenerering og oppsummering
- **F1-score**: Balansert presisjon og tilbakekalling for klassifisering

**2. Domene-spesifikke metrikker**
- **Oppgavespesifikke benchmarks**: Bruk etablerte benchmarks for ditt domene
- **Menneskelig evaluering**: Inkluder menneskelig vurdering for subjektive oppgaver
- **Forretningsmetrikker**: Juster med faktiske forretningsmål

**3. Evaluering oppsett**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Overvåking av treningsprogresjon

**1. Tapsporing**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Valideringsovervåking**
- Spor valideringstap sammen med treningstap
- Overvåk tegn på overtilpasning (valideringstap øker mens treningstap synker)
- Bruk tidlig stopp basert på valideringsmetrikker

**3. Ressursovervåking**
- Overvåk GPU/CPU-bruk
- Spor minnebruksmønstre
- Overvåk treningshastighet og gjennomstrømning

## Vanlige utfordringer og løsninger

### Utfordring 1: Overtilpasning

**Symptomer:**
- Treningstap fortsetter å synke mens valideringstap øker
- Stor forskjell mellom trening og valideringsytelse
- Dårlig generalisering til nye data

**Løsninger:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Utfordring 2: Minnebegrensninger

**Løsninger:**
- Bruk gradient-sjekkpunkt
- Implementer gradientakkumulering
- Velg parameter-effektive metoder (LoRA, QLoRA)
- Utnytt modellparallellisme for store modeller

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Utfordring 3: Langsom trening

**Løsninger:**
- Optimaliser datalastingspipelines
- Bruk trening med blandet presisjon
- Implementer effektive batch-strategier
- Vurder distribuert trening for store datasett

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Utfordring 4: Dårlig ytelse

**Diagnosetrinn:**
1. Verifiser datakvalitet og formatering
2. Sjekk læringsrate og treningsvarighet
3. Evaluer basismodellvalg
4. Gjennomgå forhåndsprosessering og tokenisering

**Løsninger:**
- Øk mangfoldet i treningsdata
- Juster læringsrateplan
- Prøv ulike basismodeller
- Implementer dataforbedringsteknikker

## Konklusjon

Finjustering er en kraftig teknikk som demokratiserer tilgang til toppmoderne AI-funksjoner. Ved å bruke verktøy som Microsoft Olive kan organisasjoner effektivt tilpasse forhåndstrente modeller til sine spesifikke behov samtidig som de optimaliserer for ytelse og ressursbegrensninger.

### Viktige punkter

1. **Velg riktig tilnærming**: Velg finjusteringsmetoder basert på dine beregningsressurser og ytelseskrav
2. **Datakvalitet er viktig**: Invester i høykvalitets, representative treningsdata
3. **Overvåk og iterer**: Evaluer og forbedre modellene dine kontinuerlig
4. **Utnytt verktøy**: Bruk rammeverk som Olive for å forenkle og optimalisere prosessen
5. **Tenk på distribusjon**: Planlegg for modelloptimalisering og distribusjon fra starten av

## ➡️ Hva er neste steg

- [04: Distribusjon - Produksjonsklare modellimplementeringer](./04.SLMOps.Deployment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.