<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T10:45:21+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "no"
}
-->
# Containerisert skyutplassering - Løsninger i produksjonsskala

Denne omfattende veiledningen dekker tre hovedtilnærminger for å utplassere Microsofts Phi-4-mini-instruct-modell i containeriserte miljøer: vLLM, Ollama og SLM Engine med ONNX Runtime. Denne modellen med 3,8 milliarder parametere representerer et optimalt valg for resonneringsoppgaver samtidig som den opprettholder effektivitet for edge-utplassering.

## Innholdsfortegnelse

1. [Introduksjon til Phi-4-mini containerutplassering](../../../Module03)
2. [Læringsmål](../../../Module03)
3. [Forstå Phi-4-mini klassifisering](../../../Module03)
4. [vLLM containerutplassering](../../../Module03)
5. [Ollama containerutplassering](../../../Module03)
6. [SLM Engine med ONNX Runtime](../../../Module03)
7. [Sammenligningsrammeverk](../../../Module03)
8. [Beste praksis](../../../Module03)

## Introduksjon til Phi-4-mini containerutplassering

Small Language Models (SLMs) representerer et viktig fremskritt innen EdgeAI, og muliggjør avanserte naturlige språkbehandlingsfunksjoner på enheter med begrensede ressurser. Denne veiledningen fokuserer på strategier for containerisert utplassering av Microsofts Phi-4-mini-instruct, en banebrytende resonneringsmodell som balanserer kapasitet med effektivitet.

### Utvalgt modell: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 milliarder parametere)**: Microsofts nyeste lette instruksjonsjusterte modell designet for minne-/beregningsbegrensede miljøer med eksepsjonelle evner innen:
- **Matematisk resonnering og komplekse beregninger**
- **Kodegenerering, feilsøking og analyse**
- **Logisk problemløsning og steg-for-steg resonnering**
- **Utdanningsapplikasjoner som krever detaljerte forklaringer**
- **Funksjonskall og verktøyintegrasjon**

Som en del av kategorien "Small SLMs" (1,5B - 13,9B parametere) oppnår Phi-4-mini en optimal balanse mellom resonneringskapasitet og ressursbruk.

### Fordeler med containerisert Phi-4-mini utplassering

- **Operasjonell effektivitet**: Rask inferens for resonneringsoppgaver med lavere beregningskrav
- **Fleksibilitet i utplassering**: AI-funksjoner på enheten med forbedret personvern gjennom lokal behandling
- **Kostnadseffektivitet**: Reduserte driftskostnader sammenlignet med større modeller, samtidig som kvaliteten opprettholdes
- **Isolasjon**: Ren separasjon mellom modellinstanser og sikre utførelsesmiljøer
- **Skalerbarhet**: Enkel horisontal skalering for økt resonneringskapasitet

## Læringsmål

Ved slutten av denne veiledningen vil du kunne:

- Utplassere og optimalisere Phi-4-mini-instruct i ulike containeriserte miljøer
- Implementere avanserte kvantisering- og komprimeringsstrategier for ulike utplasseringsscenarier
- Konfigurere produksjonsklare containerorkestreringer for resonneringsarbeidsbelastninger
- Evaluere og velge passende utplasseringsrammeverk basert på spesifikke bruksbehov
- Anvende sikkerhets-, overvåkings- og skaleringspraksis for containeriserte SLM-utplasseringer

## Forstå Phi-4-mini klassifisering

### Modellspesifikasjoner

**Tekniske detaljer:**
- **Parametere**: 3,8 milliarder (Small SLM-kategori)
- **Arkitektur**: Tett dekoder-basert Transformer med gruppert spørringsoppmerksomhet
- **Kontekstlengde**: 128K tokens (32K anbefalt for optimal ytelse)
- **Vokabular**: 200K tokens med flerspråklig støtte
- **Treningsdata**: 5T tokens med innhold som er rikt på resonnering

### Ressurskrav

| Utplasseringsstype | Min RAM | Anbefalt RAM | VRAM (GPU) | Lagring | Typiske bruksområder |
|--------------------|---------|--------------|------------|---------|----------------------|
| **Utvikling** | 6GB | 8GB | - | 8GB | Lokal testing, prototyping |
| **Produksjon CPU** | 8GB | 12GB | - | 10GB | Edge-servere, kostnadsoptimalisert utplassering |
| **Produksjon GPU** | 6GB | 8GB | 4-6GB | 8GB | Høy gjennomstrømming for resonneringstjenester |
| **Edge-optimalisert** | 4GB | 6GB | - | 6GB | Kvantisert utplassering, IoT-gatewayer |

### Phi-4-mini kapasiteter

- **Matematisk dyktighet**: Avansert aritmetikk, algebra og kalkulus problemløsning
- **Kodeintelligens**: Python, JavaScript og flerspråklig kodegenerering med feilsøking
- **Logisk resonnering**: Steg-for-steg problemdekomponering og løsningskonstruksjon
- **Utdanningsstøtte**: Detaljerte forklaringer egnet for lærings- og undervisningsscenarier
- **Funksjonskall**: Naturlig støtte for verktøyintegrasjon og API-interaksjoner

## vLLM containerutplassering

vLLM gir utmerket støtte for Phi-4-mini-instruct med optimalisert inferensytelse og OpenAI-kompatible API-er, noe som gjør det ideelt for produksjonstjenester innen resonnering.

### Eksempler på rask oppstart

#### Grunnleggende CPU-utplassering (Utvikling)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-akselerert produksjonsutplassering
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Produksjonskonfigurasjon

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testing av Phi-4-mini resonneringsevner

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama containerutplassering

Ollama gir utmerket støtte for Phi-4-mini-instruct med forenklet utplassering og administrasjon, noe som gjør det ideelt for utvikling og balanserte produksjonsutplasseringer.

### Rask oppsett

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Produksjonskonfigurasjon

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Modelloptimalisering og varianter

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Eksempler på API-bruk

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine med ONNX Runtime

ONNX Runtime gir optimal ytelse for edge-utplassering av Phi-4-mini-instruct med avansert optimalisering og plattformuavhengig kompatibilitet.

### Grunnleggende oppsett

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Forenklet serverimplementering

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Modellkonverteringsskript

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Produksjonskonfigurasjon

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testing av ONNX-utplassering

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Sammenligningsrammeverk

### Sammenligning av rammeverk for Phi-4-mini

| Funksjon | vLLM | Ollama | ONNX Runtime |
|----------|------|--------|--------------|
| **Oppsettskompleksitet** | Moderat | Enkel | Kompleks |
| **Ytelse (GPU)** | Utmerket (~25 tok/s) | Veldig bra (~20 tok/s) | Bra (~15 tok/s) |
| **Ytelse (CPU)** | Bra (~8 tok/s) | Veldig bra (~12 tok/s) | Utmerket (~15 tok/s) |
| **Minnebruk** | 8-12GB | 6-10GB | 4-8GB |
| **API-kompatibilitet** | OpenAI-kompatibel | Egendefinert REST | Egendefinert FastAPI |
| **Funksjonskall** | ✅ Naturlig | ✅ Støttet | ⚠️ Egendefinert implementering |
| **Kvantiseringstøtte** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Kvantisering |
| **Produksjonsklar** | ✅ Utmerket | ✅ Veldig bra | ✅ Bra |
| **Edge-utplassering** | Bra | Utmerket | Fremragende |

## Tilleggsressurser

### Offisiell dokumentasjon
- **Microsoft Phi-4 Model Card**: Detaljerte spesifikasjoner og bruksretningslinjer
- **vLLM Dokumentasjon**: Avanserte konfigurasjons- og optimaliseringsalternativer
- **Ollama Modellbibliotek**: Fellesskapsmodeller og tilpasningseksempler
- **ONNX Runtime Veiledninger**: Ytelsesoptimalisering og utplasseringsstrategier

### Utviklingsverktøy
- **Hugging Face Transformers**: For modellinteraksjon og tilpasning
- **OpenAI API-spesifikasjon**: For vLLM-kompatibilitetstesting
- **Docker Beste Praksis**: Container-sikkerhet og optimaliseringsretningslinjer
- **Kubernetes Utplassering**: Orkestreringsmønstre for produksjonsskala

### Læringsressurser
- **SLM Ytelsesbenchmarking**: Metodologier for komparativ analyse
- **Edge AI Utplassering**: Beste praksis for ressursbegrensede miljøer
- **Optimalisering av resonneringsoppgaver**: Strategier for matematisk og logisk problemløsning
- **Container-sikkerhet**: Sikringspraksis for AI-modellutplasseringer

## Læringsutbytte

Etter å ha fullført dette modulen, vil du kunne:

1. Utplassere Phi-4-mini-instruct-modellen i containeriserte miljøer ved bruk av flere rammeverk
2. Konfigurere og optimalisere SLM-utplasseringer for ulike maskinvaremiljøer
3. Implementere sikkerhetspraksis for containeriserte AI-utplasseringer
4. Sammenligne og velge passende utplasseringsrammeverk basert på spesifikke bruksbehov
5. Anvende overvåkings- og skaleringsstrategier for produksjonsklare SLM-tjenester

## Hva skjer videre

- Gå tilbake til [Modul 1](../Module01/README.md)
- Gå tilbake til [Modul 2](../Module02/README.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.