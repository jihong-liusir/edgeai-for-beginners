<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T20:22:19+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "no"
}
-->
# Sesjon 3: Åpne kildekode-modeller med Foundry Local

## Oversikt

Denne sesjonen utforsker hvordan man kan ta i bruk åpne kildekode-modeller i Foundry Local: valg av modeller fra fellesskapet, integrering av innhold fra Hugging Face, og strategier for "ta med din egen modell" (BYOM). Du vil også bli kjent med serien Model Mondays for kontinuerlig læring og oppdagelse av modeller.

Referanser:
- Foundry Local-dokumentasjon: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Kompilere Hugging Face-modeller: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Læringsmål
- Oppdage og evaluere åpne kildekode-modeller for lokal inferens
- Kompilere og kjøre utvalgte Hugging Face-modeller i Foundry Local
- Bruke strategier for modellvalg basert på nøyaktighet, ventetid og ressursbehov
- Administrere modeller lokalt med cache og versjonshåndtering

## Del 1: Modelloppdagelse og valg (Steg-for-steg)

Steg 1) List opp tilgjengelige modeller i den lokale katalogen  
```cmd
foundry model list
```
  
Steg 2) Prøv raskt to kandidater (automatisk nedlasting ved første kjøring)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Steg 3) Noter grunnleggende målinger  
- Observer ventetid (subjektivt) og kvalitet for en fast prompt  
- Følg med på minnebruk via Oppgavebehandling mens hver modell kjører  

## Del 2: Kjøre katalogmodeller via CLI (Steg-for-steg)

Steg 1) Start en modell  
```cmd
foundry model run llama-3.2
```
  
Steg 2) Send en testprompt via OpenAI-kompatibel endepunkt  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Del 3: BYOM – Kompilere Hugging Face-modeller (Steg-for-steg)

Følg den offisielle veiledningen for å kompilere modeller. Hovedflyt nedenfor—se Microsoft Learn-artikkelen for nøyaktige kommandoer og støttede konfigurasjoner.

Steg 1) Forbered en arbeidsmappe  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Steg 2) Kompiler en støttet HF-modell  
- Bruk stegene fra Learn-dokumentasjonen for å konvertere og plassere den kompilerte ONNX-modellen i `models`-mappen din  
- Bekreft med:  
```cmd
foundry cache ls
```
  
Du bør se navnet på den kompilerte modellen (for eksempel, `llama-3.2`).  

Steg 3) Kjøre den kompilerte modellen  
```cmd
foundry model run llama-3.2 --verbose
```
  
Notater:  
- Sørg for tilstrekkelig diskplass og RAM for kompilering og kjøring  
- Start med mindre modeller for å validere flyten, og skaler deretter opp  

## Del 4: Praktisk modellkuratering (Steg-for-steg)

Steg 1) Opprett en `models.json`-register  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Steg 2) Lite utvalgsskript  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Del 5: Praktiske benchmarks (Steg-for-steg)

Steg 1) Enkel ventetidsbenchmark  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Steg 2) Kvalitetssjekk  
- Bruk et fast sett med prompts, lagre utdataene til en CSV/JSON  
- Vurder manuelt flyt, relevans og korrekthet (1–5)  

## Del 6: Neste steg
- Abonner på Model Mondays for nye modeller og tips: https://aka.ms/model-mondays  
- Bidra med funn til teamets `models.json`  
- Forbered deg til Sesjon 4: sammenligning av LLMs vs SLMs, lokal vs sky-inferens, og praktiske demoer  

---

