<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:39:45+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "no"
}
-->
# Sesjon 3: Oppdagelse og administrasjon av åpen kildekode-modeller

## Oversikt

Denne sesjonen fokuserer på praktisk oppdagelse og administrasjon av modeller med Foundry Local. Du vil lære hvordan du lister tilgjengelige modeller, tester ulike alternativer og forstår grunnleggende ytelsesegenskaper. Tilnærmingen legger vekt på praktisk utforskning med foundry CLI for å hjelpe deg med å velge de riktige modellene for dine bruksområder.

## Læringsmål

- Mestre foundry CLI-kommandoer for oppdagelse og administrasjon av modeller
- Forstå mønstre for modellbuffer og lokal lagring
- Lær å raskt teste og sammenligne ulike modeller
- Etablere praktiske arbeidsflyter for modellvalg og benchmarking
- Utforske det voksende økosystemet av modeller tilgjengelig gjennom Foundry Local

## Forutsetninger

- Fullført Sesjon 1: Komme i gang med Foundry Local
- Foundry Local CLI installert og tilgjengelig
- Tilstrekkelig lagringsplass for modellnedlastinger (modeller kan variere fra 1GB til 20GB+)
- Grunnleggende forståelse av modelltyper og bruksområder

## Del 6: Praktisk øvelse

### Øvelse: Oppdagelse og sammenligning av modeller

Lag ditt eget skript for evaluering av modeller basert på Eksempel 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Din oppgave

1. **Kjør Eksempel 03-skriptet**: `samples\03\list_and_bench.cmd`
2. **Prøv ulike modeller**: Test minst 3 forskjellige modeller
3. **Sammenlign ytelse**: Noter forskjeller i hastighet og responskvalitet
4. **Dokumenter funn**: Lag et enkelt sammenligningsdiagram

### Eksempel på sammenligningsformat

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Del 7: Feilsøking og beste praksis

### Vanlige problemer og løsninger

**Modellen starter ikke:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Utilstrekkelig minne:**
- Start med mindre modeller (`phi-4-mini`)
- Lukk andre applikasjoner
- Oppgrader RAM hvis du ofte når grensen

**Treg ytelse:**
- Sørg for at modellen er fullstendig lastet (sjekk detaljert output)
- Lukk unødvendige bakgrunnsprogrammer
- Vurder raskere lagring (SSD)

### Beste praksis

1. **Start smått**: Begynn med `phi-4-mini` for å validere oppsettet
2. **Én modell om gangen**: Stopp tidligere modeller før du starter nye
3. **Overvåk ressurser**: Hold øye med minnebruk
4. **Test konsekvent**: Bruk de samme promptene for rettferdige sammenligninger
5. **Dokumenter resultater**: Ta notater om modellens ytelse for dine bruksområder

## Del 8: Neste steg og referanser

### Forberedelse til Sesjon 4

- **Fokus for Sesjon 4**: Optimaliseringsverktøy og teknikker
- **Forutsetninger**: Komfortabel med modellbytte og grunnleggende ytelsestesting
- **Anbefalt**: Identifiser 2-3 favorittmodeller fra denne sesjonen

### Ekstra ressurser

- **[Foundry Local Dokumentasjon](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Offisiell dokumentasjon
- **[CLI Referanse](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Komplett kommandooversikt
- **[Model Mondays](https://aka.ms/model-mondays)**: Ukentlige modellfokus
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Fellesskap og problemer
- **[Eksempel 03: Modelloppdagelse](samples/03/README.md)**: Praktisk eksempelskript

### Viktige punkter

✅ **Modelloppdagelse**: Bruk `foundry model list` for å utforske tilgjengelige modeller  
✅ **Rask testing**: Mønsteret `list_and_bench.cmd` for rask evaluering  
✅ **Ytelsesovervåking**: Grunnleggende ressursbruk og responstidsmåling  
✅ **Modellvalg**: Praktiske retningslinjer for å velge modeller basert på bruksområde  
✅ **Bufferadministrasjon**: Forstå lagring og oppryddingsprosedyrer  

Du har nå de praktiske ferdighetene til å oppdage, teste og velge passende modeller for dine AI-applikasjoner ved hjelp av Foundry Locals enkle CLI-tilnærming.

## Læringsmål

- Oppdag og evaluer åpen kildekode-modeller for lokal inferens
- Kompiler og kjør utvalgte Hugging Face-modeller innen Foundry Local
- Bruk strategier for modellvalg basert på nøyaktighet, latenstid og ressursbehov
- Administrer modeller lokalt med buffer og versjonering

## Del 1: Modelloppdagelse med Foundry CLI

### Grunnleggende kommandoer for modelladministrasjon

Foundry CLI tilbyr enkle kommandoer for oppdagelse og administrasjon av modeller:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Kjøre dine første modeller

Start med populære, velprøvde modeller for å forstå ytelsesegenskaper:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Merk:** Flagget `--verbose` gir detaljert oppstartsinformasjon, inkludert:
- Fremdrift for modellnedlasting (ved første kjøring)
- Detaljer om minnetildeling
- Informasjon om tjenestebinding
- Ytelsesinitieringsmålinger

### Forstå modellkategorier

**Små språkmodeller (SLMs):**
- `phi-4-mini`: Rask, effektiv, flott for generell chat
- `phi-4`: Mer kapabel versjon med bedre resonnement

**Middels modeller:**
- `qwen2.5-7b`: Utmerket resonnement og lengre kontekst
- `deepseek-r1-7b`: Optimalisert for kodegenerering

**Større modeller:**
- `llama-3.2`: Metas nyeste åpen kildekode-modell
- `qwen2.5-14b`: Enterprise-grade resonnement

## Del 2: Rask testing og sammenligning av modeller

### Eksempel 03-tilnærming: Enkel liste og benchmark

Basert på vårt Eksempel 03-mønster, her er den minimale arbeidsflyten:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testing av modellens ytelse

Når en modell kjører, test den med konsistente prompt:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell-testing alternativ

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Del 3: Administrasjon av modellbuffer og lagring

### Forstå modellbufferen

Foundry Local administrerer automatisk modellnedlastinger og buffer:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Vurderinger for modelllagring

**Typiske modellstørrelser:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Beste praksis for lagring:**
- Hold 2-3 modeller bufret for raskt bytte
- Fjern ubrukte modeller for å frigjøre plass: `foundry cache clean`
- Overvåk diskbruk, spesielt på mindre SSD-er
- Vurder kompromisset mellom modellstørrelse og kapasitet

### Overvåking av modellens ytelse

Mens modeller kjører, overvåk systemressurser:

**Windows Oppgavebehandling:**
- Følg med på minnebruk (modeller forblir lastet i RAM)
- Overvåk CPU-bruk under inferens
- Sjekk disk I/O under initial modelllasting

**Kommandolinjeovervåking:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Del 4: Praktiske retningslinjer for modellvalg

### Velge modeller basert på bruksområde

**For generell chat og spørsmål/svar:**
- Start med: `phi-4-mini` (rask, effektiv)
- Oppgrader til: `phi-4` (bedre resonnement)
- Avansert: `qwen2.5-7b` (lengre kontekst)

**For kodegenerering:**
- Anbefalt: `deepseek-r1-7b`
- Alternativ: `qwen2.5-7b` (også bra for kode)

**For komplekst resonnement:**
- Best: `qwen2.5-7b` eller `qwen2.5-14b`
- Budsjettalternativ: `phi-4`

### Veiledning for maskinvarekrav

**Minimum systemkrav:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Anbefalt for best ytelse:**
- 32GB+ RAM for komfortabelt bytte mellom modeller
- SSD-lagring for raskere modelllasting
- Moderne CPU med god enkelttrådsytelse
- NPU-støtte (Windows 11 Copilot+ PC-er) for akselerasjon

### Arbeidsflyt for modellbytte

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Del 5: Enkel benchmarking av modeller

### Grunnleggende ytelsestesting

Her er en enkel tilnærming for å sammenligne modellens ytelse:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuell kvalitetsvurdering

For hver modell, test med konsistente prompt og vurder manuelt:

**Testprompt:**
1. "Forklar kvantedatabehandling på en enkel måte."
2. "Skriv en Python-funksjon for å sortere en liste."
3. "Hva er fordeler og ulemper med fjernarbeid?"
4. "Oppsummer fordelene med edge AI."

**Vurderingskriterier:**
- **Nøyaktighet**: Er informasjonen korrekt?
- **Klarhet**: Er forklaringen lett å forstå?
- **Fullstendighet**: Dekker den hele spørsmålet?
- **Hastighet**: Hvor raskt svarer den?

### Overvåking av ressursbruk

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Del 6: Neste steg

- Abonner på Model Mondays for nye modeller og tips: https://aka.ms/model-mondays
- Bidra med funn til teamets `models.json`
- Forbered deg til Sesjon 4: sammenligning av LLMs vs SLMs, lokal vs sky-inferens, og praktiske demoer

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.