<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T00:40:11+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "no"
}
-->
# Sesjon 4: Bygg produksjonsklare chat-applikasjoner med Chainlit

## Oversikt

Denne sesjonen fokuserer på å bygge produksjonsklare chat-applikasjoner ved hjelp av Chainlit og Microsoft Foundry Local. Du vil lære å lage moderne webgrensesnitt for AI-samtaler, implementere strømmende svar og distribuere robuste chat-applikasjoner med riktig feilhåndtering og brukeropplevelsesdesign.

**Hva du vil bygge:**
- **Chainlit Chat App**: Moderne webgrensesnitt med strømmende svar
- **WebGPU Demo**: Nettleserbasert inferens for personvernfokuserte applikasjoner  
- **Open WebUI-integrasjon**: Profesjonelt chat-grensesnitt med Foundry Local
- **Produksjonsmønstre**: Feilhåndtering, overvåking og distribusjonsstrategier

## Læringsmål

- Bygg produksjonsklare chat-applikasjoner med Chainlit
- Implementer strømmende svar for forbedret brukeropplevelse
- Mestre integrasjonsmønstre for Foundry Local SDK
- Anvend riktig feilhåndtering og elegant degradering
- Distribuer og konfigurer chat-applikasjoner for ulike miljøer
- Forstå moderne webgrensesnittmønstre for samtale-AI

## Forutsetninger

- **Foundry Local**: Installert og kjører ([Installasjonsveiledning](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 eller nyere med støtte for virtuelle miljøer
- **Modell**: Minst én modell lastet (`foundry model run phi-4-mini`)
- **Nettleser**: Moderne nettleser med WebGPU-støtte (Chrome/Edge)
- **Docker**: For Open WebUI-integrasjon (valgfritt)

## Del 1: Forstå moderne chat-applikasjoner

### Arkitekturoversikt

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Nøkkelteknologier

**Foundry Local SDK-mønstre:**
- `FoundryLocalManager(alias)`: Automatisk tjenestestyring
- `manager.endpoint` og `manager.api_key`: Tilkoblingsdetaljer
- `manager.get_model_info(alias).id`: Modellidentifikasjon

**Chainlit-rammeverk:**
- `@cl.on_chat_start`: Initialiser chat-økter
- `@cl.on_message`: Håndter innkommende brukermeldinger  
- `cl.Message().stream_token()`: Strømming i sanntid
- Automatisk UI-generering og WebSocket-håndtering

## Del 2: Lokal vs sky beslutningsmatrise

### Ytelsesegenskaper

| Aspekt | Lokal (Foundry) | Sky (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Forsinkelse** | 🚀 50-200ms (ingen nettverk) | ⏱️ 200-2000ms (nettverksavhengig) |
| **Personvern** | 🔒 Data forlater aldri enheten | ⚠️ Data sendes til skyen |
| **Kostnad** | 💰 Gratis etter maskinvare | 💸 Betal per token |
| **Offline** | ✅ Fungerer uten internett | ❌ Krever internett |
| **Modellstørrelse** | ⚠️ Begrenset av maskinvare | ✅ Tilgang til de største modellene |
| **Skalering** | ⚠️ Maskinvareavhengig | ✅ Ubegrenset skalering |

### Hybridstrategimønstre

**Lokal først med fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Oppgavebasert ruting:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Del 3: Eksempel 04 - Chainlit Chat-applikasjon

### Hurtigstart

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Applikasjonen åpnes automatisk på `http://localhost:8080` med et moderne chat-grensesnitt.

### Kjerneimplementering

Eksempel 04-applikasjonen demonstrerer produksjonsklare mønstre:

**Automatisk tjenesteoppdagelse:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Strømmende chat-håndtering:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Konfigurasjonsalternativer

**Miljøvariabler:**

| Variabel | Beskrivelse | Standard | Eksempel |
|----------|-------------|---------|----------|
| `MODEL` | Modellalias som skal brukes | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local-endepunkt | Automatisk oppdaget | `http://localhost:51211` |
| `API_KEY` | API-nøkkel (valgfritt for lokal) | `""` | `your-api-key` |

**Avansert bruk:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Del 4: Lage og bruke Jupyter-notatbøker

### Oversikt over notatbokstøtte

Eksempel 04 inkluderer en omfattende Jupyter-notatbok (`chainlit_app.ipynb`) som gir:

- **📚 Pedagogisk innhold**: Trinn-for-trinn læringsmateriale
- **🔬 Interaktiv utforskning**: Kjør og eksperimenter med kodeceller
- **📊 Visuelle demonstrasjoner**: Diagrammer, grafer og visualisering av utdata
- **🛠️ Utviklingsverktøy**: Testing og feilsøkingsmuligheter

### Lage egne notatbøker

#### Trinn 1: Sett opp Jupyter-miljø

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Trinn 2: Lag en ny notatbok

**Bruke VS Code:**
1. Åpne VS Code i Module08-katalogen
2. Opprett en ny fil med `.ipynb`-utvidelse
3. Velg "Foundry Local"-kjernen når du blir bedt om det
4. Begynn å legge til celler med innholdet ditt

**Bruke Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Beste praksis for notatbokstruktur

#### Celleorganisering

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Interaktive eksempler og øvelser

#### Øvelse 1: Testing av klientkonfigurasjon

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Øvelse 2: Simulering av strømmende svar

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Del 5: WebGPU nettleser-inferensdemo

### Oversikt

WebGPU gjør det mulig å kjøre AI-modeller direkte i nettleseren for maksimal personvern og opplevelser uten installasjon. Dette eksemplet demonstrerer ONNX Runtime Web med WebGPU-eksekvering.

### Trinn 1: Sjekk WebGPU-støtte

**Nettleserkrav:**
- Chrome/Edge 113+ med WebGPU aktivert
- Sjekk: `chrome://gpu` → bekreft "WebGPU"-status
- Programmatisk sjekk: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Trinn 2: Lag WebGPU-demo

Opprett katalog: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Trinn 3: Kjør demoen

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Del 6: Open WebUI-integrasjon

### Oversikt

Open WebUI gir et profesjonelt ChatGPT-lignende grensesnitt som kobles til Foundry Locals OpenAI-kompatible API.

### Trinn 1: Forutsetninger

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Trinn 2: Docker-oppsett (anbefalt)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Merk:** `host.docker.internal` lar Docker-containere få tilgang til vertsmaskinen på Windows.

### Trinn 3: Konfigurasjon

1. **Åpne nettleser:** Naviger til `http://localhost:3000`
2. **Initial oppsett:** Opprett administratorkonto
3. **Modellkonfigurasjon:**
   - Innstillinger → Modeller → OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API-nøkkel: `foundry-local-key` (hvilken som helst verdi fungerer)
4. **Test tilkobling:** Modeller skal vises i nedtrekksmenyen

### Feilsøking

**Vanlige problemer:**

1. **Tilkobling nektet:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modeller vises ikke:**
   - Bekreft at modellen er lastet: `foundry model list`
   - Sjekk API-respons: `curl http://localhost:51211/v1/models`
   - Start Open WebUI-containeren på nytt

## Del 7: Betraktninger for produksjonsdistribusjon

### Miljøkonfigurasjon

**Utviklingsoppsett:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Produksjonsdistribusjon:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Vanlige portproblemer og løsninger

**Forebygging av portkonflikt på 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Ytelsesovervåking

**Implementering av helsesjekk:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Oppsummering

Sesjon 4 dekket bygging av produksjonsklare Chainlit-applikasjoner for samtale-AI. Du lærte om:

- ✅ **Chainlit-rammeverk**: Moderne UI og støtte for strømming i chat-applikasjoner
- ✅ **Foundry Local-integrasjon**: Bruk av SDK og konfigurasjonsmønstre  
- ✅ **WebGPU-inferens**: Nettleserbasert AI for maksimal personvern
- ✅ **Open WebUI-oppsett**: Distribusjon av profesjonelt chat-grensesnitt
- ✅ **Produksjonsmønstre**: Feilhåndtering, overvåking og skalering

Eksempel 04-applikasjonen demonstrerer beste praksis for å bygge robuste chat-grensesnitt som utnytter lokale AI-modeller via Microsoft Foundry Local, samtidig som den gir en utmerket brukeropplevelse.

## Referanser

- **[Eksempel 04: Chainlit-applikasjon](samples/04/README.md)**: Komplett applikasjon med dokumentasjon
- **[Chainlit pedagogisk notatbok](samples/04/chainlit_app.ipynb)**: Interaktive læringsmaterialer
- **[Foundry Local-dokumentasjon](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Komplett plattformdokumentasjon
- **[Chainlit-dokumentasjon](https://docs.chainlit.io/)**: Offisiell rammeverksdokumentasjon
- **[Open WebUI-integrasjonsveiledning](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Offisiell veiledning

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.