<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-15T17:06:44+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ja"
}
-->
# セクション 4 : OpenVINO Toolkit 最適化スイート

## 目次
1. [はじめに](../../../Module04)
2. [OpenVINOとは？](../../../Module04)
3. [インストール](../../../Module04)
4. [クイックスタートガイド](../../../Module04)
5. [例: OpenVINOを使ったモデルの変換と最適化](../../../Module04)
6. [高度な使用法](../../../Module04)
7. [ベストプラクティス](../../../Module04)
8. [トラブルシューティング](../../../Module04)
9. [追加リソース](../../../Module04)

## はじめに

OpenVINO (Open Visual Inference and Neural Network Optimization) は、クラウド、オンプレミス、エッジ環境で高性能なAIソリューションを展開するためのIntelのオープンソースツールキットです。CPU、GPU、VPU、または専用AIアクセラレータを対象とする場合でも、OpenVINOはモデルの精度を維持しながら包括的な最適化機能を提供し、クロスプラットフォーム展開を可能にします。

## OpenVINOとは？

OpenVINOは、開発者が多様なハードウェアプラットフォームでAIモデルを効率的に最適化、変換、展開できるようにするオープンソースツールキットです。主に以下の3つのコンポーネントで構成されています：推論用のOpenVINO Runtime、モデル最適化用のNeural Network Compression Framework (NNCF)、そしてスケーラブルな展開を可能にするOpenVINO Model Server。

### 主な特徴

- **クロスプラットフォーム展開**: Linux、Windows、macOSをサポートし、Python、C++、C APIを提供
- **ハードウェアアクセラレーション**: CPU、GPU、VPU、AIアクセラレータ向けの自動デバイス検出と最適化
- **モデル圧縮フレームワーク**: NNCFを通じた高度な量子化、プルーニング、最適化技術
- **フレームワーク互換性**: TensorFlow、ONNX、PaddlePaddle、PyTorchモデルを直接サポート
- **生成AI対応**: 大規模言語モデルや生成AIアプリケーション向けのOpenVINO GenAIを提供

### 利点

- **性能最適化**: 精度をほとんど損なうことなく大幅な速度向上
- **展開フットプリントの削減**: 外部依存関係を最小化し、インストールと展開を簡素化
- **起動時間の向上**: モデルのロードとキャッシュを最適化し、アプリケーションの初期化を高速化
- **スケーラブルな展開**: エッジデバイスからクラウドインフラまで一貫したAPIを提供
- **プロダクション対応**: エンタープライズグレードの信頼性と包括的なドキュメント、コミュニティサポート

## インストール

### 必要条件

- Python 3.8以上
- pipパッケージマネージャ
- 仮想環境（推奨）
- 対応ハードウェア（Intel CPU推奨、その他のアーキテクチャもサポート）

### 基本インストール

仮想環境を作成して有効化します：

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

OpenVINO Runtimeをインストールします：

```bash
pip install openvino
```

モデル最適化用にNNCFをインストールします：

```bash
pip install nncf
```

### OpenVINO GenAIのインストール

生成AIアプリケーション向け：

```bash
pip install openvino-genai
```

### オプションの依存関係

特定のユースケース向けの追加パッケージ：

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### インストールの確認

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

成功すると、OpenVINOのバージョン情報が表示されます。

## クイックスタートガイド

### 初めてのモデル最適化

Hugging FaceモデルをOpenVINOで変換・最適化してみましょう：

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### このプロセスの内容

最適化ワークフローには以下が含まれます：Hugging Faceから元のモデルをロードし、OpenVINO Intermediate Representation (IR)形式に変換、デフォルトの最適化を適用し、ターゲットハードウェア向けにコンパイル。

### 主なパラメータの説明

- `export=True`: モデルをOpenVINO IR形式に変換
- `compile=False`: 柔軟性のためにコンパイルを実行時まで遅延
- `device`: ターゲットハードウェア（"CPU"、"GPU"、"AUTO"で自動選択）
- `save_pretrained()`: 最適化されたモデルを再利用のために保存

## 例: OpenVINOを使ったモデルの変換と最適化

### ステップ1: NNCF量子化によるモデル変換

ポストトレーニング量子化を適用する方法：

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### ステップ2: 重み圧縮による高度な最適化

Transformerベースのモデルに重み圧縮を適用：

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### ステップ3: 最適化されたモデルでの推論

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### 出力構造

最適化後、モデルディレクトリには以下が含まれます：

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## 高度な使用法

### NNCF YAMLによる設定

複雑な最適化ワークフローにはNNCF設定ファイルを使用：

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

設定を適用：

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU最適化

GPUアクセラレーションを利用：

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### バッチ処理の最適化

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### モデルサーバー展開

OpenVINO Model Serverを使用して最適化されたモデルを展開：

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

モデルサーバー用のクライアントコード：

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## ベストプラクティス

### 1. モデルの選択と準備
- サポートされているフレームワーク（PyTorch、TensorFlow、ONNX）のモデルを使用
- モデル入力が固定または動的な形状であることを確認
- キャリブレーション用に代表的なデータセットでテスト

### 2. 最適化戦略の選択
- **ポストトレーニング量子化**: 簡単な最適化から開始
- **重み圧縮**: 大規模言語モデルやTransformerに最適
- **量子化対応トレーニング**: 精度が重要な場合に使用

### 3. ハードウェア固有の最適化
- **CPU**: INT8量子化でバランスの取れた性能を実現
- **GPU**: FP16精度とバッチ処理を活用
- **VPU**: モデルの簡素化とレイヤー融合に注力

### 4. 性能調整
- **スループットモード**: 大量バッチ処理向け
- **レイテンシモード**: リアルタイム対話型アプリケーション向け
- **AUTOデバイス**: OpenVINOに最適なハードウェアを選択させる

### 5. メモリ管理
- メモリオーバーヘッドを避けるために動的形状を慎重に使用
- モデルキャッシュを実装して後続のロードを高速化
- 最適化中のメモリ使用量を監視

### 6. 精度検証
- 最適化されたモデルを元の性能と比較して常に検証
- 評価には代表的なテストデータセットを使用
- 段階的な最適化を検討（保守的な設定から開始）

## トラブルシューティング

### よくある問題

#### 1. インストールの問題
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. モデル変換エラー
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. 性能の問題
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. メモリの問題
- 最適化中にモデルのバッチサイズを減らす
- 大規模データセットにはストリーミングを使用
- モデルキャッシュを有効化：`core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. 精度の低下
- より高い精度を使用（INT4ではなくINT8）
- キャリブレーションデータセットのサイズを増加
- 混合精度最適化を適用

### 性能モニタリング

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### ヘルプを得る

- **ドキュメント**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **コミュニティフォーラム**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## 追加リソース

### 公式リンク
- **OpenVINO ホームページ**: [openvino.ai](https://openvino.ai/)
- **GitHub リポジトリ**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF リポジトリ**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **モデルズー**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### 学習リソース
- **OpenVINO ノートブック**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **クイックスタートガイド**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **最適化ガイド**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### 統合ツール
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### 性能ベンチマーク
- **公式ベンチマーク**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF モデルズー**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### コミュニティ例
- **Jupyter ノートブック**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINOノートブックリポジトリで利用可能な包括的なチュートリアル
- **サンプルアプリケーション**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - コンピュータビジョン、NLP、音声などのさまざまな分野の実例
- **ブログ投稿**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AIおよびコミュニティブログ投稿で詳細なユースケースを紹介

### 関連ツール
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intelハードウェア向けの追加最適化技術
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - モバイルおよびエッジ展開の比較用
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - クロスプラットフォーム推論エンジンの代替

## ➡️ 次に進む

- [05: Apple MLX Framework 詳細解説](./05.AppleMLX.md)

---

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。