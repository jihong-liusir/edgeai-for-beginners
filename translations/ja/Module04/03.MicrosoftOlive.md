<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "631e56c1941b89d2251e10893987aedc",
  "translation_date": "2025-07-22T05:13:19+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ja"
}
-->
# セクション 3 : Microsoft Olive 最適化スイート

## 目次
1. [はじめに](../../../Module04)
2. [Microsoft Oliveとは？](../../../Module04)
3. [インストール](../../../Module04)
4. [クイックスタートガイド](../../../Module04)
5. [例: Qwen3をONNX INT4に変換する](../../../Module04)
6. [高度な使い方](../../../Module04)
7. [ベストプラクティス](../../../Module04)
8. [トラブルシューティング](../../../Module04)
9. [追加リソース](../../../Module04)

## はじめに

Microsoft Oliveは、ハードウェアに最適化されたモデルを簡単にデプロイできるようにする強力で使いやすいモデル最適化ツールキットです。CPU、GPU、またはAIアクセラレータなど、さまざまなハードウェアプラットフォームを対象に、モデルの精度を維持しながら最適なパフォーマンスを実現します。

## Microsoft Oliveとは？

Oliveは、モデル圧縮、最適化、コンパイルといった業界最先端の技術を組み合わせた、ハードウェアに最適化されたモデル最適化ツールです。ONNX Runtimeと連携し、エンドツーエンドの推論最適化ソリューションを提供します。

### 主な特徴

- **ハードウェアに最適化**: 対象ハードウェアに最適な最適化技術を自動的に選択
- **40以上の組み込み最適化コンポーネント**: モデル圧縮、量子化、グラフ最適化などを網羅
- **簡単なCLIインターフェース**: 一般的な最適化タスクを簡単なコマンドで実行
- **マルチフレームワーク対応**: PyTorch、Hugging Faceモデル、ONNXに対応
- **人気モデル対応**: Llama、Phi、Qwen、Gemmaなどのモデルを自動的に最適化可能

### 利点

- **開発時間の短縮**: 最適化技術を手動で試す必要がない
- **パフォーマンス向上**: 最大6倍の速度向上を実現する場合も
- **クロスプラットフォーム展開**: 最適化されたモデルは異なるハードウェアやOSで動作
- **精度の維持**: パフォーマンスを向上させながらモデル品質を保持

## インストール

### 必要条件

- Python 3.8以上
- pipパッケージマネージャー
- 仮想環境（推奨）

### 基本的なインストール

仮想環境を作成して有効化します:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

自動最適化機能付きでOliveをインストールします:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### オプションの依存関係

Oliveは追加機能のためのオプション依存関係を提供しています:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### インストールの確認

```bash
olive --help
```

成功すると、Olive CLIのヘルプメッセージが表示されます。

## クイックスタートガイド

### 最初の最適化

Oliveの自動最適化機能を使って小型の言語モデルを最適化してみましょう:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### このコマンドの内容

最適化プロセスには以下が含まれます: ローカルキャッシュからモデルを取得、ONNXグラフをキャプチャして重みをONNXデータファイルに保存、ONNXグラフの最適化、RTN法を使用したモデルのint4量子化。

### コマンドパラメータの説明

- `--model_name_or_path`: Hugging Faceモデルの識別子またはローカルパス
- `--output_path`: 最適化されたモデルを保存するディレクトリ
- `--device`: 対象デバイス（cpu, gpu）
- `--provider`: 実行プロバイダー（CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider）
- `--use_ort_genai`: 推論にONNX Runtime Generate AIを使用
- `--precision`: 量子化精度（int4, int8, fp16）
- `--log_level`: ログの詳細度（0=最小, 1=詳細）

## 例: Qwen3をONNX INT4に変換する

Hugging Faceの例 [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) に基づき、Qwen3モデルを最適化する方法を紹介します。

### ステップ1: モデルのダウンロード（オプション）

ダウンロード時間を短縮するため、必要なファイルのみをキャッシュします:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ステップ2: Qwen3モデルの最適化

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ステップ3: 最適化されたモデルのテスト

最適化されたモデルをテストする簡単なPythonスクリプトを作成します:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### 出力構造

最適化後、出力ディレクトリには以下が含まれます:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## 高度な使い方

### 設定ファイル

より複雑な最適化ワークフローにはJSON設定ファイルを使用できます:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

設定ファイルを使用して実行:

```bash
olive run --config config.json
```

### GPU最適化

CUDA GPU最適化の場合:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML（Windows）の場合:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Oliveを使ったファインチューニング

Oliveはモデルのファインチューニングもサポートしています:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## ベストプラクティス

### 1. モデル選択
- テストには小型モデル（例: 0.5B-7Bパラメータ）から始める
- 対象モデルアーキテクチャがOliveでサポートされていることを確認

### 2. ハードウェアの考慮
- 最適化対象をデプロイ先のハードウェアに合わせる
- CUDA対応ハードウェアがある場合はGPU最適化を使用
- WindowsマシンではDirectMLを検討

### 3. 精度の選択
- **INT4**: 最大圧縮、わずかな精度低下
- **INT8**: サイズと精度のバランスが良い
- **FP16**: 精度低下が最小、サイズ削減は中程度

### 4. テストと検証
- 最適化されたモデルを特定のユースケースで必ずテスト
- パフォーマンス指標（レイテンシ、スループット、精度）を比較
- 評価には代表的な入力データを使用

### 5. 繰り返し最適化
- クイックな結果には自動最適化を使用
- 詳細な制御には設定ファイルを使用
- 異なる最適化パスを試してみる

## トラブルシューティング

### よくある問題

#### 1. インストールの問題
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPUの問題
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. メモリの問題
- 最適化中にバッチサイズを小さくする
- まず高精度の量子化（int8など）を試す
- モデルキャッシュ用に十分なディスク容量を確保

#### 4. モデル読み込みエラー
- モデルパスとアクセス権を確認
- モデルが`trust_remote_code=True`を必要とするか確認
- 必要なモデルファイルがすべてダウンロードされていることを確認

### ヘルプを得る

- **ドキュメント**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **例**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## 追加リソース

### 公式リンク
- **GitHubリポジトリ**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtimeドキュメント**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face例**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### コミュニティ例
- **Jupyterノートブック**: Olive GitHubリポジトリで利用可能
- **VS Code拡張機能**: AI Toolkit拡張機能がOliveを使用してモデルを最適化
- **ブログ記事**: Microsoft Open Source Blogに詳細なOliveチュートリアルあり

### 関連ツール
- **ONNX Runtime**: 高性能推論エンジン
- **Hugging Face Transformers**: 多くの互換性のあるモデルのソース
- **Azure Machine Learning**: クラウドベースの最適化ワークフロー

## ➡️ 次に進む

- [04: Apple MLX Framework Deep Dive](./04.AppleMLX.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知おきください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。