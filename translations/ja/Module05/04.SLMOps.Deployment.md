<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-07-22T04:13:41+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ja"
}
-->
# セクション4: デプロイメント - 本番環境向けモデルの実装

## 概要

この包括的なチュートリアルでは、Foundry Localを使用して微調整済みの量子化モデルをデプロイするプロセスを最初から最後まで案内します。モデルの変換、量子化の最適化、デプロイメント設定について詳しく説明します。

## 前提条件

始める前に、以下を準備してください:

- ✅ デプロイ可能な微調整済みONNXモデル
- ✅ WindowsまたはMacのコンピュータ
- ✅ Python 3.10以上
- ✅ 少なくとも8GBの空きRAM
- ✅ システムにインストールされたFoundry Local

## パート1: 環境のセットアップ

### 必要なツールのインストール

ターミナル（Windowsではコマンドプロンプト、Macではターミナル）を開き、以下のコマンドを順に実行してください:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **重要な注意**: CMakeバージョン3.31以上も必要です。[cmake.org](https://cmake.org/download/)からダウンロードできます。

## パート2: モデルの変換と量子化

### 適切なフォーマットの選択

微調整済みの小型言語モデルには、**ONNXフォーマット**を推奨します。その理由は以下の通りです:

- 🚀 パフォーマンス最適化の向上
- 🔧 ハードウェアに依存しないデプロイ
- 🏭 本番環境向けの機能
- 📱 クロスプラットフォーム互換性

### 方法1: ワンコマンド変換（推奨）

以下のコマンドを使用して、微調整済みモデルを直接変換します:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**パラメータの説明:**
- `--model_name_or_path`: 微調整済みモデルのパス
- `--device cpu`: 最適化にCPUを使用
- `--precision int4`: INT4量子化を使用（約75%のサイズ削減）
- `--output_path`: 変換後のモデルの出力パス

### 方法2: 設定ファイルを使用したアプローチ（上級者向け）

`finetuned_conversion_config.json`という名前の設定ファイルを作成します:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

その後、以下を実行します:

```bash
olive run --config ./finetuned_conversion_config.json
```

### 量子化オプションの比較

| 精度       | ファイルサイズ       | 推論速度       | モデル品質       | 推奨用途           |
|------------|----------------------|----------------|------------------|--------------------|
| FP16       | ベースライン × 0.5  | 高速           | 最良            | 高性能ハードウェア |
| INT8       | ベースライン × 0.25 | 非常に高速     | 良好            | バランスの取れた選択肢 |
| INT4       | ベースライン × 0.125| 最速           | 許容範囲        | リソース制限環境   |

💡 **推奨**: 最初のデプロイではINT4量子化を試してください。品質に満足できない場合は、INT8またはFP16を試してください。

## パート3: Foundry Localのデプロイ設定

### モデル設定の作成

Foundry Localのモデルディレクトリに移動します:

```bash
foundry cache cd ./models/
```

モデルのディレクトリ構造を作成します:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

モデルディレクトリ内に`inference_model.json`設定ファイルを作成します:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### モデル固有のテンプレート設定

#### Qwenシリーズモデルの場合:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## パート4: モデルのテストと最適化

### モデルインストールの確認

Foundry Localがモデルを認識できるか確認します:

```bash
foundry cache ls
```

`your-finetuned-model-int4`がリストに表示されるはずです。

### モデルテストの開始

```bash
foundry model run your-finetuned-model-int4
```

### パフォーマンスベンチマーク

テスト中に以下の主要な指標を監視します:

1. **応答時間**: 平均応答時間を測定
2. **メモリ使用量**: RAM消費量を監視
3. **CPU使用率**: プロセッサの負荷を確認
4. **出力品質**: 応答の関連性と一貫性を評価

### 品質検証チェックリスト

- ✅ 微調整されたドメインクエリに適切に応答する
- ✅ 応答形式が期待される出力構造に一致する
- ✅ 長時間使用中にメモリリークが発生しない
- ✅ 異なる入力長にわたって一貫したパフォーマンスを維持
- ✅ エッジケースや無効な入力を適切に処理

## まとめ

おめでとうございます！以下を無事に完了しました:

- ✅ 微調整済みモデルのフォーマット変換
- ✅ モデルの量子化最適化
- ✅ Foundry Localのデプロイ設定
- ✅ パフォーマンス調整とトラブルシューティング

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解について、当社は責任を負いません。