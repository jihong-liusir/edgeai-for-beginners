<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-07-22T04:24:11+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "ja"
}
-->
# セクション3: ファインチューニング - 特定のタスクに合わせたモデルのカスタマイズ

## 目次
1. [ファインチューニングの概要](../../../Module05)
2. [ファインチューニングが重要な理由](../../../Module05)
3. [ファインチューニングの種類](../../../Module05)
4. [Microsoft Oliveを使ったファインチューニング](../../../Module05)
5. [実践例](../../../Module05)
6. [ベストプラクティスとガイドライン](../../../Module05)
7. [高度な技術](../../../Module05)
8. [評価とモニタリング](../../../Module05)
9. [よくある課題と解決策](../../../Module05)
10. [結論](../../../Module05)

## ファインチューニングの概要

**ファインチューニング**は、事前学習済みモデルを特定のタスクや専門的なデータセットに適応させる強力な機械学習技術です。モデルをゼロから学習させるのではなく、事前学習済みモデルがすでに学習した知識を活用し、特定のユースケースに合わせて調整します。

### ファインチューニングとは？

ファインチューニングは**転移学習**の一形態であり、以下の手順を含みます：
- 大規模データセットから一般的なパターンを学習した事前学習済みモデルを使用
- 特定のデータセットを用いてモデルの内部パラメータを調整
- 有用な知識を保持しつつ、タスクに特化したモデルを作成

熟練したシェフに新しい料理を教えるようなものです。彼らはすでに料理の基本を理解していますが、新しいスタイルの技術や味を学ぶ必要があります。

### 主な利点

- **時間効率**: ゼロから学習するよりもはるかに速い
- **データ効率**: 少量のデータセットで良好な性能を達成可能
- **コスト効率**: 計算リソースの必要量が少ない
- **優れた性能**: ゼロから学習するよりも優れた結果を得られることが多い
- **リソース最適化**: 小規模なチームや組織でも強力なAIを利用可能

## ファインチューニングが重要な理由

### 実際の応用例

ファインチューニングは多くの場面で不可欠です：

**1. ドメイン適応**
- 医療AI: 一般的な言語モデルを医療用語や臨床ノートに適応
- 法律技術: 法的文書分析や契約レビューに特化したモデル
- 金融サービス: 財務報告分析やリスク評価に特化したモデル

**2. タスク特化**
- コンテンツ生成: 特定の文体やトーンに合わせたファインチューニング
- コード生成: 特定のプログラミング言語やフレームワークに適応
- 翻訳: 特定の言語ペアや技術分野での性能向上

**3. 企業向け応用**
- カスタマーサービス: 企業特有の用語を理解するチャットボットの作成
- 内部文書: 組織のプロセスに精通したAIアシスタントの構築
- 業界特化型ソリューション: 業界特有の専門用語やワークフローを理解するモデルの開発

## ファインチューニングの種類

### 1. フルファインチューニング（指示型ファインチューニング）

フルファインチューニングでは、トレーニング中にすべてのモデルパラメータを更新します。この方法は：
- 最大の柔軟性と性能向上の可能性を提供
- 大量の計算リソースを必要とする
- 完全に新しいバージョンのモデルを生成
- 十分なトレーニングデータと計算リソースがある場合に最適

### 2. パラメータ効率型ファインチューニング（PEFT）

PEFTメソッドでは、パラメータの一部のみを更新するため、効率的です：

#### Low-Rank Adaptation (LoRA)
- 既存の重み行列に小さなランク分解行列を追加
- トレーニングするパラメータ数を大幅に削減
- フルファインチューニングに近い性能を維持
- 異なる適応間の簡単な切り替えを可能に

#### QLoRA (Quantized LoRA)
- LoRAと量子化技術を組み合わせる
- メモリ要件をさらに削減
- コンシューマーハードウェアでの大規模モデルのファインチューニングを可能に
- 効率性と性能のバランスを実現

#### アダプター
- 既存の層間に小さなニューラルネットワークを挿入
- ベースモデルを固定したままターゲット型ファインチューニングを実現
- モジュール型アプローチでモデルをカスタマイズ可能

### 3. タスク特化型ファインチューニング

特定の下流タスクにモデルを適応：
- **分類**: カテゴリ分けタスク向けのモデル調整
- **生成**: コンテンツ作成やテキスト生成の最適化
- **抽出**: 情報抽出や固有表現認識向けのファインチューニング
- **要約**: 文書要約に特化したモデルの作成

## Microsoft Oliveを使ったファインチューニング

Microsoft Oliveは、ファインチューニングプロセスを簡素化し、エンタープライズ向けの機能を提供する包括的なモデル最適化ツールキットです。

### Microsoft Oliveとは？

Microsoft Oliveはオープンソースのモデル最適化ツールであり：
- 様々なハードウェアターゲット向けのファインチューニングワークフローを簡素化
- 人気のあるモデルアーキテクチャ（Llama、Phi、Qwen、Gemma）をサポート
- クラウドおよびローカルでのデプロイオプションを提供
- Azure MLやその他のMicrosoft AIサービスとシームレスに統合
- 自動最適化と量子化をサポート

### 主な特徴

- **ハードウェア対応型最適化**: 特定のハードウェア（CPU、GPU、NPU）向けにモデルを自動的に最適化
- **マルチフォーマット対応**: PyTorch、Hugging Face、ONNXモデルに対応
- **自動化ワークフロー**: 手動設定や試行錯誤を削減
- **エンタープライズ統合**: Azure MLやクラウドデプロイメントをサポート
- **拡張可能なアーキテクチャ**: カスタム最適化技術を導入可能

### インストールとセットアップ

#### 基本インストール

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### オプション依存関係

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### インストール確認

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## 実践例

### 例1: Olive CLIを使った基本的なファインチューニング

この例では、小型言語モデルをフレーズ分類用にファインチューニングします：

#### ステップ1: 環境の準備

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### ステップ2: モデルのファインチューニング

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### ステップ3: デプロイメント向けの最適化

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### 例2: カスタムデータセットを使った高度な設定

#### ステップ1: カスタムデータセットの準備

トレーニングデータを含むJSONファイルを作成：

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### ステップ2: 設定ファイルの作成

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### ステップ3: ファインチューニングの実行

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### 例3: メモリ効率を重視したQLoRAファインチューニング

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## ベストプラクティスとガイドライン

### データ準備

**1. データの質を量より重視**
- 大量の低品質データよりも高品質で多様な例を優先
- ターゲットユースケースを代表するデータを確保
- 一貫したデータのクリーニングと前処理を実施

**2. データ形式とテンプレート**
- トレーニング例全体で一貫したフォーマットを使用
- ユースケースに合った明確な入力-出力テンプレートを作成
- 指示型モデルには適切な指示フォーマットを含める

**3. データセットの分割**
- データの10-20%を検証用に確保
- トレーニング/検証分割間で類似した分布を維持
- 分類タスクには層化サンプリングを検討

### トレーニング設定

**1. 学習率の選択**
- ファインチューニングには小さめの学習率（1e-5～1e-4）を使用
- 学習率スケジューリングを活用して収束を改善
- 損失曲線を監視して適宜調整

**2. バッチサイズの最適化**
- 利用可能なメモリとバッチサイズのバランスを取る
- 勾配累積を使用して効果的なバッチサイズを拡大
- バッチサイズと学習率の関係を考慮

**3. トレーニング期間**
- 検証指標を監視して過学習を回避
- 検証性能が停滞した場合に早期停止を使用
- 復旧と分析のために定期的にチェックポイントを保存

### モデル選択

**1. ベースモデルの選択**
- 可能であれば類似ドメインで事前学習されたモデルを選択
- 計算制約に応じたモデルサイズを検討
- 商業利用のライセンス要件を評価

**2. ファインチューニング方法の選択**
- リソースが限られている環境ではLoRA/QLoRAを使用
- 最大性能が重要な場合はフルファインチューニングを選択
- 複数タスクシナリオにはアダプター型アプローチを検討

### リソース管理

**1. ハードウェア最適化**
- モデルサイズと方法に適したハードウェアを選択
- 勾配チェックポイントを使用してGPUメモリを効率的に利用
- 大規模モデルにはクラウドベースのソリューションを検討

**2. メモリ管理**
- 利用可能な場合は混合精度トレーニングを使用
- メモリ制約には勾配累積を実装
- トレーニング中のGPUメモリ使用量を監視

## 高度な技術

### マルチアダプタートレーニング

複数のタスク向けにアダプターをトレーニングし、ベースモデルを共有：

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### ハイパーパラメータ最適化

体系的なハイパーパラメータチューニングを実施：

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### カスタム損失関数

ドメイン特化型損失関数を実装：

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## 評価とモニタリング

### 指標と評価

**1. 標準指標**
- **正確性**: 分類タスクの全体的な正確性
- **困惑度**: 言語モデルの品質指標
- **BLEU/ROUGE**: テキスト生成や要約の品質指標
- **F1スコア**: 分類タスクの精度と再現率のバランス

**2. ドメイン特化型指標**
- **タスク特化型ベンチマーク**: ドメインにおける確立されたベンチマークを使用
- **人間による評価**: 主観的なタスクには人間の評価を含める
- **ビジネス指標**: 実際のビジネス目標に合わせる

**3. 評価セットアップ**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### トレーニング進捗のモニタリング

**1. 損失の追跡**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. 検証モニタリング**
- トレーニング損失と並行して検証損失を追跡
- 過学習の兆候を監視（トレーニング損失が減少し続ける一方で検証損失が増加）
- 検証指標に基づいて早期停止を使用

**3. リソースモニタリング**
- GPU/CPUの利用状況を監視
- メモリ使用パターンを追跡
- トレーニング速度とスループットを監視

## よくある課題と解決策

### 課題1: 過学習

**症状:**
- トレーニング損失が減少し続ける一方で検証損失が増加
- トレーニングと検証の性能に大きなギャップ
- 新しいデータへの一般化が不十分

**解決策:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### 課題2: メモリ制限

**解決策:**
- 勾配チェックポイントを使用
- 勾配累積を実装
- パラメータ効率型メソッド（LoRA、QLoRA）を選択
- 大規模モデルにはモデル並列化を利用

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### 課題3: トレーニングの遅さ

**解決策:**
- データ読み込みパイプラインを最適化
- 混合精度トレーニングを使用
- 効率的なバッチ戦略を実装
- 大規模データセットには分散トレーニングを検討

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### 課題4: 性能が低い

**診断ステップ:**
1. データの品質とフォーマットを確認
2. 学習率とトレーニング期間をチェック
3. ベースモデルの選択を評価
4. 前処理とトークン化を見直し

**解決策:**
- トレーニングデータの多様性を増加
- 学習率スケジュールを調整
- 異なるベースモデルを試す
- データ拡張技術を実装

## 結論

ファインチューニングは、最先端のAI機能へのアクセスを民主化する強力な技術です。Microsoft Oliveのようなツールを活用することで、事前学習済みモデルを効率的に特定のニーズに適応させ、性能とリソース制約を最適化できます。

### 重要なポイント

1. **適切なアプローチを選択**: 計算リソースと性能要件に基づいてファインチューニング方法を選択
2. **データの質が重要**: 高品質で代表的なトレーニングデータに投資
3. **モニタリングと改善**: モデルを継続的に評価し改善
4. **ツールを活用**: Oliveのようなフレームワークを使用してプロセスを簡素化・最適化
5. **デプロイメントを考慮**: 初期段階からモデルの最適化とデプロイメントを計画

## ➡️ 次に進む

- [04: デプロイメント - 実運用モデルの実装](./04.SLMOps.Deployment.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。