<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-07-22T04:48:54+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ja"
}
-->
# セクション 03 - モデルコンテキストプロトコル (MCP) の統合

## MCP (モデルコンテキストプロトコル) の概要

モデルコンテキストプロトコル (MCP) は、言語モデルが外部ツールやシステムと標準化された方法でやり取りできるようにする革新的なフレームワークです。従来のアプローチではモデルが孤立していましたが、MCP は明確に定義されたプロトコルを通じて AI モデルと現実世界をつなぐ橋を構築します。

### MCP とは？

MCP は、言語モデルが以下を可能にする通信プロトコルです：
- 外部データソースへの接続
- ツールや関数の実行
- APIやサービスとのやり取り
- リアルタイム情報へのアクセス
- 複雑なマルチステップ操作の実行

このプロトコルにより、静的な言語モデルがテキスト生成を超えた実用的なタスクを実行できる動的なエージェントへと変貌します。

## MCP における小型言語モデル (SLM)

小型言語モデルは効率的な AI 展開を可能にし、以下のような利点を提供します：

### SLM の利点
- **リソース効率**: 計算要件が低い
- **高速な応答時間**: リアルタイムアプリケーションでの遅延が少ない  
- **コスト効率**: 最小限のインフラ要件
- **プライバシー**: データ送信なしでローカル実行可能
- **カスタマイズ性**: 特定の分野に合わせた微調整が容易

### MCP と SLM の相性が良い理由

SLM と MCP を組み合わせることで、モデルの推論能力が外部ツールによって強化され、パラメータ数が少ないモデルでも機能性を向上させる強力な組み合わせが実現します。

## Python MCP SDK の概要

Python MCP SDK は、MCP 対応アプリケーションを構築するための基盤を提供します。この SDK には以下が含まれます：

- **クライアントライブラリ**: MCP サーバーへの接続用
- **サーバーフレームワーク**: カスタム MCP サーバーの作成用
- **プロトコルハンドラー**: 通信管理用
- **ツール統合**: 外部関数の実行用

## 実践例: Phi-4 MCP クライアント

Microsoft の Phi-4 ミニモデルを MCP 機能と統合した実際の実装を見てみましょう。

### システムアーキテクチャ

この実装は階層型アーキテクチャに従います：

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### コアコンポーネント

#### 1. MCP クライアントクラス

**BaseMCPClient**: 共通機能を提供する抽象的な基盤
- 非同期コンテキストマネージャープロトコル
- 標準インターフェース定義
- リソース管理

**Phi4MiniMCPClient**: STDIO ベースの実装
- ローカルプロセス通信
- 標準入力/出力の処理
- サブプロセス管理

**Phi4MiniSSEMCPClient**: サーバー送信イベント (SSE) 実装
- HTTP ストリーミング通信
- リアルタイムイベント処理
- Web ベースのサーバー接続

#### 2. LLM 統合

**OllamaClient**: ローカルモデルホスティング
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: 高性能サービング
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. ツール処理パイプライン

ツール処理パイプラインは、MCP ツールを言語モデルと互換性のある形式に変換します：

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## 始め方: ステップバイステップガイド

### ステップ 1: 環境設定

必要な依存関係をインストールします：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### ステップ 2: 基本設定

環境変数を設定します：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### ステップ 3: 初めての MCP クライアントの実行

**基本的な Ollama 設定:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM バックエンドの使用:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**サーバー送信イベント接続:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**カスタム MCP サーバー:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### ステップ 4: プログラムによる使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高度な機能

### マルチバックエンド対応

この実装は Ollama と vLLM の両方のバックエンドをサポートしており、要件に応じて選択できます：

- **Ollama**: ローカル開発やテストに最適
- **vLLM**: 本番環境や高スループットシナリオに最適化

### 柔軟な接続プロトコル

2 つの接続モードがサポートされています：

**STDIO モード**: プロセス間の直接通信
- 低遅延
- ローカルツールに適している
- シンプルなセットアップ

**SSE モード**: HTTP ベースのストリーミング
- ネットワーク対応
- 分散システムに適している
- リアルタイム更新

### ツール統合機能

このシステムはさまざまなツールと統合可能です：
- Web 自動化 (Playwright)
- ファイル操作
- API とのやり取り
- システムコマンド
- カスタム関数

## エラー処理とベストプラクティス

### 包括的なエラー管理

この実装には以下のエラー処理が含まれています：

**接続エラー:**
- MCP サーバーの障害
- ネットワークタイムアウト
- 接続問題

**ツール実行エラー:**
- ツールの欠如
- パラメータ検証
- 実行失敗

**レスポンス処理エラー:**
- JSON パース問題
- フォーマットの不整合
- LLM レスポンスの異常

### ベストプラクティス

1. **リソース管理**: 非同期コンテキストマネージャーを使用
2. **エラー処理**: 包括的な try-catch ブロックを実装
3. **ログ記録**: 適切なログレベルを有効化
4. **セキュリティ**: 入力を検証し、出力をサニタイズ
5. **パフォーマンス**: 接続プールとキャッシュを使用

## 実際のアプリケーション

### Web 自動化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### データ処理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 統合
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## パフォーマンス最適化

### メモリ管理
- 効率的なメッセージ履歴管理
- 適切なリソースクリーンアップ
- 接続プール

### ネットワーク最適化
- 非同期 HTTP 操作
- 設定可能なタイムアウト
- 優雅なエラー回復

### 同時処理
- 非ブロッキング I/O
- ツールの並列実行
- 効率的な非同期パターン

## セキュリティに関する考慮事項

### データ保護
- 安全な API キー管理
- 入力検証
- 出力サニタイズ

### ネットワークセキュリティ
- HTTPS サポート
- ローカルエンドポイントのデフォルト設定
- 安全なトークン管理

### 実行の安全性
- ツールフィルタリング
- サンドボックス環境
- 監査ログ

## 結論

MCP と統合された SLM は、AI アプリケーション開発におけるパラダイムシフトを表しています。小型モデルの効率性と外部ツールの力を組み合わせることで、リソース効率が高く、非常に有能なインテリジェントシステムを構築できます。

Phi-4 MCP クライアントの実装は、この統合を実現する方法を示しており、高度な AI 対応アプリケーションを構築するための堅固な基盤を提供します。

重要なポイント：
- MCP は言語モデルと外部システムのギャップを埋める
- SLM はツールによる拡張で効率性を維持しつつ能力を向上
- モジュラーアーキテクチャにより拡張とカスタマイズが容易
- 本番環境での使用には適切なエラー処理とセキュリティ対策が不可欠

このチュートリアルは、SLM を活用した MCP アプリケーションを構築するための基盤を提供し、自動化、データ処理、インテリジェントシステム統合の可能性を広げます。

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。