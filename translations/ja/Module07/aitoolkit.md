<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "65a22ed38b95f334dd8a893bf2c55806",
  "translation_date": "2025-10-02T11:41:28+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "ja"
}
-->
# Visual Studio Code用AIツールキット - エッジAI開発ガイド

## はじめに

Visual Studio Code用AIツールキットを使用したエッジAI開発の包括的なガイドへようこそ。人工知能が集中型クラウドコンピューティングから分散型エッジデバイスへと移行する中で、開発者はリソース制約やオフライン動作要件など、エッジ展開の特有の課題に対応できる強力で統合されたツールを必要としています。

Visual Studio Code用AIツールキットは、エッジデバイス上で効率的に動作するAIアプリケーションを構築、テスト、最適化するための完全な開発環境を提供することで、このギャップを埋めます。IoTセンサー、モバイルデバイス、組み込みシステム、エッジサーバー向けの開発を行う場合でも、このツールキットはおなじみのVS Code環境内で開発ワークフロー全体を効率化します。

このガイドでは、モデル選択から本番展開まで、AIツールキットを活用したエッジAIプロジェクトのための基本的な概念、ツール、ベストプラクティスを紹介します。

## 概要

Visual Studio Code用AIツールキットは、エージェント開発とAIアプリケーション作成を効率化する強力な拡張機能です。このツールキットは、Anthropic、OpenAI、GitHub、Googleなどの幅広いプロバイダーからAIモデルを探索、評価、展開するための包括的な機能を提供し、ONNXやOllamaを使用したローカルモデル実行をサポートします。

AIツールキットの特徴は、AI開発ライフサイクル全体を網羅する包括的なアプローチです。従来のAI開発ツールが単一の側面に焦点を当てるのに対し、AIツールキットはモデルの発見、実験、エージェント開発、評価、展開を統合された環境で提供します。

このプラットフォームは、プロトタイピングから本番展開まで迅速に対応できるよう設計されており、プロンプト生成、クイックスターター、シームレスなMCP（Model Context Protocol）ツール統合、広範な評価機能などの特徴を備えています。エッジAI開発においては、VS Code内で完全な開発ワークフローを維持しながら、効率的にAIアプリケーションを開発、テスト、最適化できます。

## 学習目標

このガイドを終える頃には、以下のことができるようになります：

### 基本スキル
- Visual Studio Code用AIツールキットをエッジAI開発ワークフローに**インストールおよび設定**する
- AIツールキットのインターフェース（Model Catalog、Playground、Agent Builderなど）を**操作および活用**する
- パフォーマンスやリソース制約に基づいてエッジ展開に適したAIモデルを**選択および評価**する
- ONNX形式や量子化技術を使用してモデルを**変換および最適化**する

### エッジAI開発スキル
- 統合開発環境を使用してエッジAIアプリケーションを**設計および実装**する
- ローカル推論やリソースモニタリングを使用してエッジに近い条件でモデルを**テスト**する
- エッジ展開シナリオに最適化されたAIエージェントを**作成およびカスタマイズ**する
- エッジコンピューティングに関連する指標（レイテンシ、メモリ使用量、精度）を使用してモデルの**パフォーマンスを評価**する

### 最適化と展開
- モデルサイズを削減しながら許容可能なパフォーマンスを維持するために**量子化とプルーニング**技術を適用する
- CPU、GPU、NPUアクセラレーションを含む特定のエッジハードウェアプラットフォーム向けにモデルを**最適化**する
- リソース管理やフォールバック戦略を含むエッジAI開発の**ベストプラクティスを実装**する
- エッジデバイス上での本番展開に向けてモデルとアプリケーションを**準備**する

### 高度なエッジAI概念
- ONNX Runtime、Windows ML、TensorFlow LiteなどのエッジAIフレームワークと**統合**する
- エッジ環境向けのマルチモデルアーキテクチャやフェデレーテッドラーニングシナリオを**実装**する
- メモリ制約、推論速度、ハードウェア互換性などの一般的なエッジAI問題を**トラブルシューティング**する
- 本番環境でのエッジAIアプリケーション向けのモニタリングおよびログ戦略を**設計**する

### 実践的な応用
- モデル選択から展開までの**エンドツーエンドのエッジAIソリューションを構築**する
- エッジ特有の開発ワークフローや最適化技術における**熟練度を示す**
- 学んだ概念をIoT、モバイル、組み込みアプリケーションなどの実際のエッジAIユースケースに**適用**する
- 異なるエッジAI展開戦略とそのトレードオフを**評価および比較**する

## エッジAI開発向けの主要機能

### 1. モデルカタログと探索
- **マルチプロバイダーサポート**: Anthropic、OpenAI、GitHub、GoogleなどのプロバイダーからAIモデルを閲覧およびアクセス
- **ローカルモデル統合**: ONNXやOllamaモデルの簡易探索でエッジ展開をサポート
- **GitHubモデル**: GitHubのモデルホスティングとの直接統合で効率的なアクセス
- **モデル比較**: エッジデバイスの制約に最適なバランスを見つけるためのモデルの並列比較

### 2. インタラクティブプレイグラウンド
- **インタラクティブなテスト環境**: 制御された環境でモデルの機能を迅速に実験
- **マルチモーダルサポート**: エッジシナリオで一般的な画像、テキストなどの入力をテスト
- **リアルタイム実験**: モデルの応答とパフォーマンスに関する即時フィードバック
- **パラメータ最適化**: エッジ展開要件に合わせたモデルパラメータの微調整

### 3. プロンプト（エージェント）ビルダー
- **自然言語生成**: 自然言語記述を使用してスタータープロンプトを生成
- **反復的な改良**: モデルの応答とパフォーマンスに基づいてプロンプトを改善
- **タスク分解**: プロンプトチェーンや構造化出力を使用して複雑なタスクを分解
- **変数サポート**: プロンプトに変数を使用して動的なエージェント動作を実現
- **本番コード生成**: 本番環境向けのコードを迅速に生成してアプリ開発を効率化

### 4. 一括実行と評価
- **マルチモデルテスト**: 選択したモデルで複数のプロンプトを同時に実行
- **効率的なスケールテスト**: 様々な入力と構成を効率的にテスト
- **カスタムテストケース**: エージェントをテストケースで実行して機能を検証
- **パフォーマンス比較**: 異なるモデルや構成間で結果を比較

### 5. データセットを使用したモデル評価
- **標準指標**: 組み込み評価者（F1スコア、関連性、類似性、一貫性）を使用してAIモデルをテスト
- **カスタム評価者**: 特定のユースケース向けに独自の評価指標を作成
- **データセット統合**: 包括的なデータセットでモデルをテスト
- **パフォーマンス測定**: エッジ展開の意思決定のためにモデルのパフォーマンスを定量化

### 6. ファインチューニング機能
- **モデルカスタマイズ**: 特定のユースケースやドメイン向けにモデルをカスタマイズ
- **専門的な適応**: 特定のドメインや要件にモデルを適応
- **エッジ最適化**: エッジ展開の制約に特化したモデルをファインチューニング
- **ドメイン特化型トレーニング**: 特定のエッジユースケースに合わせたモデルを作成

### 7. MCPツール統合
- **外部ツール接続**: Model Context Protocolサーバーを通じてエージェントを外部ツールに接続
- **実世界のアクション**: エージェントがデータベースをクエリしたり、APIにアクセスしたり、カスタムロジックを実行したりすることを可能に
- **既存のMCPサーバー**: コマンド（stdio）またはHTTP（サーバー送信イベント）プロトコルのツールを使用
- **カスタムMCP開発**: Agent Builderでテスト可能な新しいMCPサーバーを構築およびスキャフォールド

### 8. エージェント開発とテスト
- **関数呼び出しサポート**: エージェントが外部関数を動的に呼び出すことを可能に
- **リアルタイム統合テスト**: 実行中の統合とツール使用をリアルタイムでテスト
- **エージェントのバージョン管理**: 評価結果の比較機能を備えたエージェントのバージョン管理
- **デバッグとトレース**: エージェント開発のためのローカルトレースとデバッグ機能

## エッジAI開発ワークフロー

### フェーズ1: モデル探索と選択
1. **モデルカタログを探索**: モデルカタログを使用してエッジ展開に適したモデルを見つける
2. **パフォーマンスを比較**: サイズ、精度、推論速度に基づいてモデルを評価
3. **ローカルでテスト**: OllamaまたはONNXモデルを使用してエッジ展開前にローカルでテスト
4. **リソース要件を評価**: 対象エッジデバイスのメモリと計算ニーズを判断

### フェーズ2: モデル最適化
1. **ONNXに変換**: 選択したモデルをエッジ互換性のためにONNX形式に変換
2. **量子化を適用**: INT8またはINT4量子化でモデルサイズを削減
3. **ハードウェア最適化**: 対象エッジハードウェア（ARM、x86、専用アクセラレータ）向けに最適化
4. **パフォーマンス検証**: 最適化されたモデルが許容可能な精度を維持していることを検証

### フェーズ3: アプリケーション開発
1. **エージェント設計**: Agent Builderを使用してエッジ最適化されたAIエージェントを作成
2. **プロンプトエンジニアリング**: 小型エッジモデルで効果的に動作するプロンプトを開発
3. **統合テスト**: シミュレートされたエッジ条件でエージェントをテスト
4. **コード生成**: エッジ展開向けに最適化された本番コードを生成

### フェーズ4: 評価とテスト
1. **バッチ評価**: 複数の構成をテストして最適なエッジ設定を見つける
2. **パフォーマンスプロファイリング**: 推論速度、メモリ使用量、精度を分析
3. **エッジシミュレーション**: 対象エッジ展開環境に類似した条件でテスト
4. **ストレステスト**: 様々な負荷条件下でのパフォーマンスを評価

### フェーズ5: 展開準備
1. **最終的な最適化**: テスト結果に基づいて最終的な最適化を適用
2. **展開パッケージ化**: モデルとコードをエッジ展開向けにパッケージ化
3. **ドキュメント化**: 展開要件と構成を文書化
4. **モニタリング設定**: エッジ展開向けのモニタリングとログを準備

## エッジAI開発の対象者

### エッジAI開発者
- AI搭載エッジデバイスやIoTソリューションを構築するアプリケーション開発者
- リソース制約のあるデバイスにAI機能を統合する組み込みシステム開発者
- スマートフォンやタブレット向けのオンデバイスAIアプリケーションを作成するモバイル開発者

### エッジAIエンジニア
- エッジ展開向けにモデルを最適化し、推論パイプラインを管理するAIエンジニア
- 分散型エッジインフラストラクチャ全体でAIモデルを展開および管理するDevOpsエンジニア
- エッジハードウェアの制約に合わせてAIワークロードを最適化するパフォーマンスエンジニア

### 研究者と教育者
- エッジコンピューティング向けの効率的なモデルやアルゴリズムを開発するAI研究者
- エッジAIの概念を教え、最適化技術を実演する教育者
- エッジAI展開の課題と解決策を学ぶ学生

## エッジAIユースケース

### スマートIoTデバイス
- **リアルタイム画像認識**: IoTカメラやセンサーでコンピュータビジョンモデルを展開
- **音声処理**: スマートスピーカーで音声認識や自然言語処理を実装
- **予知保全**: 工業用エッジデバイスで異常検知モデルを実行
- **環境モニタリング**: 環境アプリケーション向けにセンサーデータ分析モデルを展開

### モバイルおよび組み込みアプリケーション
- **オンデバイス翻訳**: オフラインで動作する言語翻訳モデルを実装
- **拡張現実**: ARアプリケーション向けにリアルタイムの物体認識と追跡を展開
- **健康モニタリング**: ウェアラブルデバイスや医療機器で健康分析モデルを実行
- **自律システム**: ドローン、ロボット、車両向けの意思決定モデルを実装

### エッジコンピューティングインフラストラクチャ
- **エッジデータセンター**: 低レイテンシアプリケーション向けにエッジデータセンターでAIモデルを展開
- **CDN統合**: コンテンツ配信ネットワークにAI処理機能を統合
- **5Gエッジ**: 5Gエッジコンピューティングを活用したAI対応アプリケーション
- **フォグコンピューティング**: フォグコンピューティング環境でAI処理を実装

## インストールとセットアップ

### 拡張機能のインストール
Visual Studio Code MarketplaceからAIツールキット拡張機能を直接インストールします：

**拡張機能ID**: `ms-windows-ai-studio.windows-ai-studio`

**インストール方法**:
1. **VS Code Marketplace**: Extensionsビューで「AI Toolkit」を検索
2. **コマンドライン**: `code --install-extension ms-windows-ai-studio.windows-ai-st
2. 自然言語の説明を使用してスタータープロンプトを生成する  
3. モデルの応答に基づいてプロンプトを反復し、改善する  
4. MCPツールを統合してエージェントの機能を強化する  

#### ステップ3: テストと評価  
1. **Bulk Run**を使用して、選択したモデルで複数のプロンプトをテストする  
2. テストケースを使用してエージェントを実行し、機能を検証する  
3. 組み込みまたはカスタムメトリクスを使用して精度とパフォーマンスを評価する  
4. 異なるモデルと構成を比較する  

#### ステップ4: 微調整と最適化  
1. 特定のエッジユースケースに合わせてモデルをカスタマイズする  
2. ドメイン固有の微調整を適用する  
3. エッジ展開の制約に合わせて最適化する  
4. 異なるエージェント構成をバージョン管理し、比較する  

#### ステップ5: 展開準備  
1. Agent Builderを使用して、プロダクション対応のコードを生成する  
2. プロダクション用途のためにMCPサーバー接続を設定する  
3. エッジデバイス向けの展開パッケージを準備する  
4. 監視と評価メトリクスを設定する  

## エッジAI開発のベストプラクティス  

### モデル選択  
- **サイズ制約**: ターゲットデバイスのメモリ制限内に収まるモデルを選択する  
- **推論速度**: リアルタイムアプリケーション向けに高速な推論を優先する  
- **精度のトレードオフ**: モデルの精度とリソース制約のバランスを取る  
- **形式の互換性**: エッジ展開にはONNXやハードウェア最適化形式を優先する  

### 最適化技術  
- **量子化**: INT8またはINT4量子化を使用してモデルサイズを縮小し、速度を向上させる  
- **プルーニング**: 不要なモデルパラメータを削除して計算要件を削減する  
- **知識蒸留**: 大きなモデルの性能を維持しつつ、小さなモデルを作成する  
- **ハードウェアアクセラレーション**: NPU、GPU、または専用アクセラレータを活用する  

### 開発ワークフロー  
- **反復テスト**: 開発中にエッジに近い条件で頻繁にテストする  
- **パフォーマンス監視**: リソース使用量と推論速度を継続的に監視する  
- **バージョン管理**: モデルのバージョンと最適化設定を追跡する  
- **ドキュメント化**: すべての最適化決定とパフォーマンスのトレードオフを記録する  

### 展開の考慮事項  
- **リソース監視**: プロダクションでのメモリ、CPU、電力使用量を監視する  
- **フォールバック戦略**: モデルの失敗に備えたフォールバックメカニズムを実装する  
- **更新メカニズム**: モデルの更新とバージョン管理を計画する  
- **セキュリティ**: エッジAIアプリケーションに適切なセキュリティ対策を実施する  

## エッジAIフレームワークとの統合  

### ONNX Runtime  
- **クロスプラットフォーム展開**: ONNXモデルを異なるエッジプラットフォームで展開する  
- **ハードウェア最適化**: ONNX Runtimeのハードウェア固有の最適化を活用する  
- **モバイル対応**: スマートフォンやタブレットアプリケーション向けにONNX Runtime Mobileを使用する  
- **IoT統合**: ONNX Runtimeの軽量ディストリビューションを使用してIoTデバイスに展開する  

### Windows ML  
- **Windowsデバイス**: WindowsベースのエッジデバイスやPC向けに最適化する  
- **NPUアクセラレーション**: WindowsデバイスのNeural Processing Unitを活用する  
- **DirectML**: WindowsプラットフォームでGPUアクセラレーションにDirectMLを使用する  
- **UWP統合**: Universal Windows Platformアプリケーションと統合する  

### TensorFlow Lite  
- **モバイル最適化**: TensorFlow Liteモデルをモバイルや組み込みデバイスに展開する  
- **ハードウェアデリゲート**: 専用ハードウェアデリゲートを使用してアクセラレーションする  
- **マイクロコントローラー**: TensorFlow Lite Microを使用してマイクロコントローラーに展開する  
- **クロスプラットフォーム対応**: Android、iOS、組み込みLinuxシステムで展開する  

### Azure IoT Edge  
- **クラウド-エッジハイブリッド**: クラウドでのトレーニングとエッジでの推論を組み合わせる  
- **モジュール展開**: AIモデルをIoT Edgeモジュールとして展開する  
- **デバイス管理**: エッジデバイスとモデル更新をリモートで管理する  
- **テレメトリ**: エッジ展開からパフォーマンスデータとモデルメトリクスを収集する  

## 高度なエッジAIシナリオ  

### マルチモデル展開  
- **モデルアンサンブル**: 精度向上や冗長性のために複数のモデルを展開する  
- **A/Bテスト**: エッジデバイスで異なるモデルを同時にテストする  
- **動的選択**: 現在のデバイス状況に基づいてモデルを選択する  
- **リソース共有**: 複数の展開モデル間でリソース使用を最適化する  

### フェデレーテッドラーニング  
- **分散トレーニング**: 複数のエッジデバイスでモデルをトレーニングする  
- **プライバシー保護**: トレーニングデータをローカルに保持しながらモデル改善を共有する  
- **協調学習**: デバイスが集団的な経験から学習できるようにする  
- **エッジ-クラウド連携**: エッジデバイスとクラウドインフラ間で学習を調整する  

### リアルタイム処理  
- **ストリーム処理**: エッジデバイスで継続的なデータストリームを処理する  
- **低遅延推論**: 推論遅延を最小限に抑えるよう最適化する  
- **バッチ処理**: エッジデバイスで効率的にデータバッチを処理する  
- **適応処理**: 現在のデバイス能力に基づいて処理を調整する  

## エッジAI開発のトラブルシューティング  

### よくある問題  
- **メモリ制約**: モデルがターゲットデバイスのメモリに収まらない  
- **推論速度**: モデルの推論がリアルタイム要件に対して遅すぎる  
- **精度の低下**: 最適化によってモデルの精度が許容範囲を超えて低下する  
- **ハードウェア互換性**: モデルがターゲットハードウェアと互換性がない  

### デバッグ戦略  
- **パフォーマンスプロファイリング**: AI Toolkitのトレース機能を使用してボトルネックを特定する  
- **リソース監視**: 開発中にメモリとCPU使用量を監視する  
- **段階的テスト**: 最適化を段階的にテストして問題を特定する  
- **ハードウェアシミュレーション**: 開発ツールを使用してターゲットハードウェアをシミュレートする  

### 最適化ソリューション  
- **さらなる量子化**: より積極的な量子化技術を適用する  
- **モデルアーキテクチャ**: エッジ向けに最適化された異なるモデルアーキテクチャを検討する  
- **前処理の最適化**: エッジ制約に合わせてデータ前処理を最適化する  
- **推論の最適化**: ハードウェア固有の推論最適化を使用する  

## リソースと次のステップ  

### 公式ドキュメント  
- [AI Toolkit Developer Documentation](https://aka.ms/AIToolkit/doc)  
- [Installation and Setup Guide](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)  
- [VS Code Intelligent Apps Documentation](https://code.visualstudio.com/docs/intelligentapps)  
- [Model Context Protocol (MCP) Documentation](https://modelcontextprotocol.io/)  

### コミュニティとサポート  
- [AI Toolkit GitHub Repository](https://github.com/microsoft/vscode-ai-toolkit)  
- [GitHub Issues and Feature Requests](https://aka.ms/AIToolkit/feedback)  
- [Azure AI Foundry Discord Community](https://aka.ms/azureaifoundry/discord)  
- [VS Code Extension Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)  

### 技術リソース  
- [ONNX Runtime Documentation](https://onnxruntime.ai/)  
- [Ollama Documentation](https://ollama.ai/)  
- [Windows ML Documentation](https://docs.microsoft.com/en-us/windows/ai/)  
- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)  

### 学習パス  
- [エッジAI基礎コース](../Module01/README.md)  
- [小型言語モデルガイド](../Module02/README.md)  
- [エッジ展開戦略](../Module03/README.md)  
- [WindowsエッジAI開発](./windowdeveloper.md)  

### 追加リソース  
- **リポジトリ統計**: 1.8k+スター、150+フォーク、18+コントリビューター  
- **ライセンス**: MITライセンス  
- **セキュリティ**: Microsoftのセキュリティポリシーが適用される  
- **テレメトリ**: VS Codeのテレメトリ設定を尊重する  

## 結論  

Visual Studio Code用AI Toolkitは、現代のAI開発における包括的なプラットフォームを提供し、特にエッジAIアプリケーションにおいて価値のある効率的なエージェント開発機能を備えています。Anthropic、OpenAI、GitHub、Googleなどのプロバイダーをサポートする広範なモデルカタログと、ONNXやOllamaを使用したローカル実行を組み合わせることで、多様なエッジ展開シナリオに必要な柔軟性を提供します。

このツールキットの強みは統合されたアプローチにあります。Playgroundでのモデル探索と実験から、Prompt Builderを使用した高度なエージェント開発、包括的な評価機能、そしてシームレスなMCPツール統合まで、エッジAI開発者にとって、エージェントのプロトタイプ作成とテストを迅速に行い、リソース制約のある環境向けに最適化する能力を提供します。

エッジAI開発における主な利点は以下の通りです:  
- **迅速な実験**: エッジ展開にコミットする前にモデルとエージェントを迅速にテスト  
- **マルチプロバイダの柔軟性**: 最適なエッジソリューションを見つけるためにさまざまなソースからモデルにアクセス  
- **ローカル開発**: オフラインおよびプライバシー保護の開発のためにONNXとOllamaを使用  
- **プロダクション対応**: プロダクション対応のコードを生成し、MCPを介して外部ツールと統合  
- **包括的な評価**: 組み込みおよびカスタムメトリクスを使用してエッジAIのパフォーマンスを検証  

AIがエッジ展開シナリオに向かう中で、VS Code用AI Toolkitは、リソース制約のある環境向けにインテリジェントなアプリケーションを構築、テスト、最適化するために必要な開発環境とワークフローを提供します。IoTソリューション、モバイルAIアプリケーション、組み込みインテリジェンスシステムを開発する際、このツールキットの包括的な機能セットと統合されたワークフローは、エッジAI開発ライフサイクル全体をサポートします。

継続的な開発と活発なコミュニティ（1.8k+ GitHubスター）により、AI Toolkitはエッジ展開シナリオを構築する現代のAI開発者のニーズを満たすために進化し続けています。

[Next Foundry Local](./foundrylocal.md)  

---

**免責事項**:  
この文書は、AI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。