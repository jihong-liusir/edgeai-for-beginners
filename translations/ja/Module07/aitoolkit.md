<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ab6b3d55f53ea3d498b3c067b17f8816",
  "translation_date": "2025-09-15T17:27:41+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "ja"
}
-->
# AI Toolkit for Visual Studio Code - Edge AI 開発ガイド

## はじめに

AI Toolkit for Visual Studio Code を使用した Edge AI 開発の包括的なガイドへようこそ。人工知能が集中型のクラウドコンピューティングから分散型のエッジデバイスへと移行する中で、開発者はリソース制約やオフライン動作要件など、エッジ展開の特有の課題に対応できる強力で統合されたツールを必要としています。

AI Toolkit for Visual Studio Code は、エッジデバイス上で効率的に動作する AI アプリケーションを構築、テスト、最適化するための完全な開発環境を提供することで、このギャップを埋めます。IoT センサー、モバイルデバイス、組み込みシステム、エッジサーバー向けに開発する場合でも、このツールキットは VS Code の使い慣れた環境内で開発ワークフロー全体を効率化します。

このガイドでは、AI Toolkit を活用した Edge AI プロジェクトの初期モデル選択から本番展開までの重要な概念、ツール、ベストプラクティスについて説明します。

## 概要

AI Toolkit は、VS Code 内で Edge AI アプリケーションのライフサイクル全体をサポートする統合開発環境を提供します。OpenAI、Anthropic、Google、GitHub などの人気のある AI モデルとのシームレスな統合を提供し、ONNX や Ollama を通じたローカルモデル展開をサポートします。これらは、オンデバイス推論を必要とする Edge AI アプリケーションにとって重要な機能です。

AI Toolkit が Edge AI 開発において際立っているのは、エッジ展開パイプライン全体に焦点を当てている点です。従来の AI 開発ツールが主にクラウド展開を対象としているのに対し、AI Toolkit はモデル最適化、リソース制約下でのテスト、エッジ特有のパフォーマンス評価など、エッジ展開に特化した機能を備えています。エッジ AI 開発には、より小型のモデルサイズ、迅速な推論時間、オフライン機能、ハードウェア特有の最適化など、異なる考慮事項が必要であることを理解しています。

このプラットフォームは、単純なオンデバイス推論から複雑なマルチモデルエッジアーキテクチャまで、さまざまな展開シナリオをサポートします。モデル変換、量子化、最適化のためのツールを提供し、エッジ展開を成功させるために必要な機能を備えています。また、VS Code が提供する開発者の生産性を維持します。

## 学習目標

このガイドを終える頃には、以下のことができるようになります：

### 基本スキル
- **インストールと設定**: Edge AI 開発ワークフローのための AI Toolkit for Visual Studio Code のインストールと設定
- **インターフェースの操作**: Model Catalog、Playground、Agent Builder を含む AI Toolkit インターフェースの操作
- **モデルの選択と評価**: パフォーマンスとリソース制約に基づいてエッジ展開に適した AI モデルを選択・評価
- **モデルの変換と最適化**: ONNX フォーマットや量子化技術を使用してエッジデバイス向けにモデルを変換・最適化

### Edge AI 開発スキル
- **設計と実装**: 統合開発環境を使用した Edge AI アプリケーションの設計と実装
- **モデルテスト**: ローカル推論とリソースモニタリングを使用したエッジに近い条件でのモデルテスト
- **AI エージェントの作成とカスタマイズ**: エッジ展開シナリオに最適化された AI エージェントの作成とカスタマイズ
- **モデルパフォーマンスの評価**: エッジコンピューティングに関連する指標（レイテンシ、メモリ使用量、精度）を使用したモデルパフォーマンスの評価

### 最適化と展開
- **量子化とプルーニング**: モデルサイズを削減しつつ許容可能なパフォーマンスを維持する技術の適用
- **モデルの最適化**: CPU、GPU、NPU アクセラレーションを含む特定のエッジハードウェアプラットフォーム向けのモデル最適化
- **ベストプラクティスの実装**: リソース管理やフォールバック戦略を含む Edge AI 開発のベストプラクティスの実装
- **モデルとアプリケーションの準備**: エッジデバイスでの本番展開に向けたモデルとアプリケーションの準備

### 高度な Edge AI 概念
- **エッジ AI フレームワークとの統合**: ONNX Runtime、Windows ML、TensorFlow Lite などのフレームワークとの統合
- **マルチモデルアーキテクチャの実装**: エッジ環境向けのマルチモデルアーキテクチャやフェデレーテッドラーニングシナリオの実装
- **一般的なエッジ AI 問題のトラブルシューティング**: メモリ制約、推論速度、ハードウェア互換性などの問題の解決
- **モニタリングとログ設計**: 本番環境での Edge AI アプリケーションのモニタリングとログ設計

### 実践的な応用
- **エンドツーエンドの Edge AI ソリューション構築**: モデル選択から展開までのプロセスを実践
- **エッジ特有の開発ワークフローと最適化技術の習得**: 実践的なスキルの習得
- **学んだ概念の応用**: IoT、モバイル、組み込みアプリケーションを含む実際の Edge AI ユースケースへの応用
- **異なるエッジ AI 展開戦略の評価と比較**: トレードオフの分析

## Edge AI 開発のための主要機能

### 1. モデルカタログと発見
- **ローカルモデルサポート**: エッジ展開に最適化された AI モデルの発見とアクセス
- **ONNX 統合**: エッジ推論に効率的な ONNX フォーマットのモデルへのアクセス
- **Ollama サポート**: プライバシーとオフライン動作のためのローカル実行モデルの活用
- **モデル比較**: パフォーマンスとリソース消費のバランスを最適化するためのモデルの並列比較

### 2. インタラクティブプレイグラウンド
- **ローカルテスト環境**: エッジ展開前にモデルをローカルでテスト
- **マルチモーダル実験**: エッジシナリオで一般的な画像、テキストなどの入力を使用したテスト
- **パラメータ調整**: エッジ制約に最適化するための異なるモデルパラメータの実験
- **リアルタイムパフォーマンスモニタリング**: 開発中の推論速度とリソース使用量の観察

### 3. エージェントビルダーによるエッジアプリケーション
- **プロンプトエンジニアリング**: 小型エッジモデルで効率的に動作する最適化されたプロンプトの作成
- **MCP ツール統合**: エッジエージェント機能を強化する Model Context Protocol ツールの統合
- **コード生成**: エッジ展開シナリオに最適化された本番コードの生成
- **構造化出力**: エッジアプリケーションに適した一貫性のある構造化応答の設計

### 4. モデル評価とテスト
- **パフォーマンス指標**: エッジ展開に関連する指標（レイテンシ、メモリ使用量、精度）を使用したモデル評価
- **バッチテスト**: 複数のモデル構成を同時にテストして最適なエッジ設定を見つける
- **カスタム評価**: エッジ AI ユースケースに特化した評価基準の作成
- **リソースプロファイリング**: エッジ展開計画のためのメモリと計算要件の分析

### 5. モデル変換と最適化
- **ONNX 変換**: さまざまなフォーマットから ONNX へのモデル変換によるエッジ互換性の向上
- **量子化**: モデルサイズの削減と推論速度の向上を実現する量子化技術
- **ハードウェア最適化**: 特定のエッジハードウェア（CPU、GPU、NPU）向けのモデル最適化
- **フォーマット変換**: Hugging Face などのソースからエッジ展開向けにモデルを変換

### 6. エッジシナリオ向けの微調整
- **ドメイン適応**: 特定のエッジユースケースや環境向けにモデルをカスタマイズ
- **ローカルトレーニング**: エッジ特有の要件に対応する GPU サポートを活用したローカルトレーニング
- **Azure 統合**: エッジ展開前のクラウドベースの微調整に Azure Container Apps を活用
- **転移学習**: エッジ特有のタスクや制約に対応するための事前学習モデルの適応

### 7. パフォーマンスモニタリングとトレーシング
- **エッジパフォーマンス分析**: エッジに近い条件でのモデルパフォーマンスのモニタリング
- **トレース収集**: 最適化のための詳細なパフォーマンスデータの収集
- **ボトルネックの特定**: エッジデバイスへの展開前にパフォーマンス問題を特定
- **リソース使用量の追跡**: エッジ最適化のためのメモリ、CPU、推論時間のモニタリング

## Edge AI 開発ワークフロー

### フェーズ 1: モデルの発見と選択
1. **モデルカタログの探索**: エッジ展開に適したモデルをモデルカタログで探索
2. **パフォーマンスの比較**: サイズ、精度、推論速度に基づいてモデルを評価
3. **ローカルテスト**: Ollama や ONNX モデルを使用して展開前にローカルでテスト
4. **リソース要件の評価**: 対象エッジデバイスのメモリと計算ニーズを判断

### フェーズ 2: モデルの最適化
1. **ONNX への変換**: 選択したモデルを ONNX フォーマットに変換してエッジ互換性を向上
2. **量子化の適用**: INT8 や INT4 量子化を使用してモデルサイズを削減
3. **ハードウェア最適化**: 対象エッジハードウェア（ARM、x86、専用アクセラレータ）向けに最適化
4. **パフォーマンス検証**: 最適化されたモデルが許容可能な精度を維持していることを確認

### フェーズ 3: アプリケーション開発
1. **エージェント設計**: Agent Builder を使用してエッジ最適化された AI エージェントを作成
2. **プロンプトエンジニアリング**: 小型モデルで効果的に動作するプロンプトを開発
3. **統合テスト**: シミュレーションされたエッジ条件でエージェントをテスト
4. **コード生成**: エッジ展開に最適化された本番コードを生成

### フェーズ 4: 評価とテスト
1. **バッチ評価**: 複数の構成をテストして最適なエッジ設定を見つける
2. **パフォーマンスプロファイリング**: 推論速度、メモリ使用量、精度を分析
3. **エッジシミュレーション**: 対象エッジ展開環境に近い条件でテスト
4. **ストレステスト**: さまざまな負荷条件下でのパフォーマンスを評価

### フェーズ 5: 展開準備
1. **最終的な最適化**: テスト結果に基づいて最終的な最適化を適用
2. **展開パッケージの作成**: エッジ展開向けにモデルとコードをパッケージ化
3. **ドキュメント作成**: 展開要件と構成を文書化
4. **モニタリング設定**: 本番展開向けのモニタリングとログを準備

## Edge AI 開発の対象者

### Edge AI 開発者
- AI 搭載エッジデバイスや IoT ソリューションを構築するアプリケーション開発者
- リソース制約のあるデバイスに AI 機能を統合する組み込みシステム開発者
- スマートフォンやタブレット向けのオンデバイス AI アプリケーションを作成するモバイル開発者

### Edge AI エンジニア
- エッジ展開向けにモデルを最適化し、推論パイプラインを管理する AI エンジニア
- 分散型エッジインフラストラクチャ全体で AI モデルを展開・管理する DevOps エンジニア
- エッジハードウェアの制約に対応する AI ワークロードを最適化するパフォーマンスエンジニア

### 研究者と教育者
- エッジコンピューティング向けの効率的なモデルやアルゴリズムを開発する AI 研究者
- Edge AI の概念を教え、最適化技術を実演する教育者
- Edge AI 展開の課題と解決策を学ぶ学生

## Edge AI ユースケース

### スマート IoT デバイス
- **リアルタイム画像認識**: IoT カメラやセンサーでコンピュータビジョンモデルを展開
- **音声処理**: スマートスピーカーで音声認識や自然言語処理を実装
- **予測保守**: 産業用エッジデバイスで異常検知モデルを実行
- **環境モニタリング**: 環境アプリケーション向けにセンサーデータ分析モデルを展開

### モバイルおよび組み込みアプリケーション
- **オンデバイス翻訳**: オフラインで動作する言語翻訳モデルを実装
- **拡張現実**: AR アプリケーション向けにリアルタイムの物体認識と追跡を展開
- **健康モニタリング**: ウェアラブルデバイスや医療機器で健康分析モデルを実行
- **自律システム**: ドローン、ロボット、車両向けの意思決定モデルを実装

### エッジコンピューティングインフラストラクチャ
- **エッジデータセンター**: 低レイテンシアプリケーション向けにエッジデータセンターで AI モデルを展開
- **CDN 統合**: コンテンツ配信ネットワークに AI 処理機能を統合
- **5G エッジ**: 5G エッジコンピューティングを活用した AI 搭載アプリケーション
- **フォグコンピューティング**: フォグコンピューティング環境で AI 処理を実装

## インストールとセットアップ

### クイックインストール
Visual Studio Code Marketplace から AI Toolkit 拡張機能を直接インストールします：

```
Install: AI Toolkit for Visual Studio Code (ms-windows-ai-studio.windows-ai-studio)
```

### Edge AI 開発の前提条件
- **ONNX Runtime**: モデル推論のために ONNX Runtime をインストール
- **Ollama**（オプション）: ローカルモデル
- **セキュリティ**: エッジAIアプリケーションに適切なセキュリティ対策を実施する

## エッジAIフレームワークとの統合

### ONNX Runtime
- **クロスプラットフォーム展開**: ONNXモデルを異なるエッジプラットフォームに展開する
- **ハードウェア最適化**: ONNX Runtimeのハードウェア固有の最適化を活用する
- **モバイル対応**: ONNX Runtime Mobileを使用してスマートフォンやタブレット向けアプリケーションを展開する
- **IoT統合**: ONNX Runtimeの軽量ディストリビューションを使用してIoTデバイスに展開する

### Windows ML
- **Windowsデバイス**: WindowsベースのエッジデバイスやPC向けに最適化する
- **NPUアクセラレーション**: WindowsデバイスのNeural Processing Unitを活用する
- **DirectML**: WindowsプラットフォームでGPUアクセラレーションを利用する
- **UWP統合**: Universal Windows Platformアプリケーションと統合する

### TensorFlow Lite
- **モバイル最適化**: TensorFlow Liteモデルをモバイルや組み込みデバイスに展開する
- **ハードウェアデリゲート**: 専用ハードウェアデリゲートを使用してアクセラレーションを実現する
- **マイクロコントローラー**: TensorFlow Lite Microを使用してマイクロコントローラーに展開する
- **クロスプラットフォーム対応**: Android、iOS、組み込みLinuxシステムに展開する

### Azure IoT Edge
- **クラウドとエッジのハイブリッド**: クラウドでのトレーニングとエッジでの推論を組み合わせる
- **モジュール展開**: AIモデルをIoT Edgeモジュールとして展開する
- **デバイス管理**: エッジデバイスやモデルの更新をリモートで管理する
- **テレメトリ**: エッジ展開からパフォーマンスデータやモデルメトリクスを収集する

## 高度なエッジAIシナリオ

### マルチモデル展開
- **モデルアンサンブル**: 複数のモデルを展開して精度向上や冗長性を確保する
- **A/Bテスト**: エッジデバイス上で異なるモデルを同時にテストする
- **動的選択**: 現在のデバイス状況に基づいてモデルを選択する
- **リソース共有**: 複数の展開モデル間でリソース使用を最適化する

### フェデレーションラーニング
- **分散トレーニング**: 複数のエッジデバイスでモデルをトレーニングする
- **プライバシー保護**: トレーニングデータをローカルに保持しながらモデル改善を共有する
- **協調学習**: デバイスが集団的な経験から学習できるようにする
- **エッジとクラウドの連携**: エッジデバイスとクラウドインフラ間で学習を調整する

### リアルタイム処理
- **ストリーム処理**: エッジデバイスで継続的なデータストリームを処理する
- **低遅延推論**: 推論の遅延を最小限に抑えるよう最適化する
- **バッチ処理**: エッジデバイスで効率的にデータバッチを処理する
- **適応処理**: 現在のデバイス能力に基づいて処理を調整する

## エッジAI開発のトラブルシューティング

### よくある問題
- **メモリ制約**: モデルがターゲットデバイスのメモリに収まらない
- **推論速度**: モデル推論がリアルタイム要件に対して遅すぎる
- **精度低下**: 最適化によってモデル精度が許容範囲を超えて低下する
- **ハードウェア互換性**: モデルがターゲットハードウェアと互換性がない

### デバッグ戦略
- **パフォーマンスプロファイリング**: AI Toolkitのトレース機能を使用してボトルネックを特定する
- **リソースモニタリング**: 開発中にメモリやCPU使用率を監視する
- **段階的テスト**: 最適化を段階的にテストして問題を特定する
- **ハードウェアシミュレーション**: 開発ツールを使用してターゲットハードウェアをシミュレートする

### 最適化ソリューション
- **さらなる量子化**: より積極的な量子化技術を適用する
- **モデルアーキテクチャ**: エッジ向けに最適化された異なるモデルアーキテクチャを検討する
- **前処理の最適化**: エッジ制約に合わせてデータ前処理を最適化する
- **推論の最適化**: ハードウェア固有の推論最適化を使用する

## リソースと次のステップ

### ドキュメント
- [AI Toolkit Models Guide](https://code.visualstudio.com/docs/intelligentapps/models)
- [Model Playground Documentation](https://code.visualstudio.com/docs/intelligentapps/playground)
- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Windows ML Documentation](https://docs.microsoft.com/en-us/windows/ai/)

### コミュニティとサポート
- [VS Code AI Toolkit GitHub](https://github.com/microsoft/vscode-ai-toolkit)
- [ONNX Community](https://github.com/onnx/onnx)
- [Edge AI Developer Community](https://docs.microsoft.com/en-us/azure/iot-edge/community)
- [VS Code Extension Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 学習リソース
- [Edge AI Fundamentals Course](./Module01/README.md)
- [Small Language Models Guide](./Module02/README.md)
- [Edge Deployment Strategies](./Module03/README.md)
- [Windows Edge AI Development](./windowdeveloper.md)

## 結論

Visual Studio CodeのAI Toolkitは、モデルの発見や最適化から展開とモニタリングまで、エッジAI開発のための包括的なプラットフォームを提供します。このツールキットの統合されたツールとワークフローを活用することで、開発者はリソース制約のあるエッジデバイスで効果的に動作するAIアプリケーションを効率的に作成、テスト、展開することができます。

ONNX、Ollama、さまざまなクラウドプロバイダーのサポートに加え、最適化と評価機能を備えたこのツールキットは、エッジAI開発に最適な選択肢です。IoTアプリケーション、モバイルAI機能、組み込みインテリジェンスシステムを構築する際に、AI Toolkitは成功するエッジAI展開に必要なツールとワークフローを提供します。

エッジAIが進化し続ける中、VS CodeのAI Toolkitは最前線に立ち、次世代のインテリジェントエッジアプリケーションを構築するための最先端のツールと機能を開発者に提供し続けます。

---

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。