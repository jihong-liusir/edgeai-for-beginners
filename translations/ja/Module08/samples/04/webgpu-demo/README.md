<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7a474b8e201d5316c0095cdbc3bf0555",
  "translation_date": "2025-09-24T10:45:39+00:00",
  "source_file": "Module08/samples/04/webgpu-demo/README.md",
  "language_code": "ja"
}
-->
# WebGPU + ONNX Runtime デモ

このデモは、WebGPUを利用したハードウェアアクセラレーションとONNX Runtime Webを使用して、AIモデルをブラウザ内で直接実行する方法を示します。

## このデモで示す内容

- **ブラウザベースのAI**: モデルを完全にブラウザ内で実行
- **WebGPUアクセラレーション**: 利用可能な場合のハードウェア加速推論
- **プライバシー重視**: データがデバイス外に出ることはありません
- **インストール不要**: 対応ブラウザで動作
- **スムーズなフォールバック**: WebGPUが利用できない場合はCPUに切り替え

## 必要条件

**ブラウザの互換性:**
- WebGPUが有効化されたChrome/Edge 113以上
- WebGPUのステータス確認: `chrome://gpu`
- WebGPUを有効化: `chrome://flags/#enable-unsafe-webgpu`

## デモの実行方法

### オプション1: ローカルサーバー (推奨)

```cmd
# Navigate to the demo directory
cd Module08\samples\04\webgpu-demo

# Start a local server
python -m http.server 5173

# Open browser to http://localhost:5173
```

### オプション2: VS Code Live Server

1. VS Codeで「Live Server」拡張機能をインストール
2. `index.html`を右クリック → 「Open with Live Server」を選択
3. ブラウザでデモが自動的に開きます

## 画面で確認できる内容

1. **WebGPU検出**: ブラウザの互換性をチェック
2. **モデル読み込み**: MNIST分類器をダウンロードして初期化
3. **推論実行**: サンプルデータで予測を実行
4. **パフォーマンス指標**: 読み込み時間と推論速度を表示
5. **結果表示**: 予測の信頼度と生データ出力

## 期待されるパフォーマンス

| 実行プロバイダー | モデル読み込み | 推論 | 備考 |
|-------------------|------------|-----------|-------|
| **WebGPU** | 約2-5秒 | 約10-50ms | ハードウェア加速 |
| **CPU (WASM)** | 約2-5秒 | 約50-200ms | ソフトウェアフォールバック |

## トラブルシューティング

**WebGPUが利用できない場合:**
- Chrome/Edge 113以上に更新
- `chrome://flags`でWebGPUを有効化
- GPUドライバーが最新であることを確認
- デモは自動的にCPUにフォールバックします

**読み込みエラー:**
- HTTP経由で提供されていることを確認 (file://ではなく)
- モデルダウンロードのためのネットワーク接続を確認
- ONNXモデルがCORSによってブロックされていないことを確認

**パフォーマンスの問題:**
- WebGPUはCPUに比べて大幅な速度向上を提供
- 初回実行はモデルダウンロードのため遅くなる可能性あり
- 2回目以降はブラウザキャッシュを使用

## Foundry Localとの統合

このWebGPUデモは、Foundry Localと以下の点で補完し合います:

- **クライアントサイド推論**による究極のプライバシー
- **オフライン機能**でインターネットが利用できない場合に対応  
- **エッジ展開**でリソースが限られた環境に対応
- **ハイブリッドアーキテクチャ**でローカルとサーバー推論を組み合わせ

本番環境での利用を検討する場合:
- サーバーサイド推論にはFoundry Localを使用
- クライアントサイドの前処理/後処理にはWebGPUを使用
- ローカル/リモート推論間のインテリジェントなルーティングを実装

## 技術的詳細

**使用モデル:**
- MNIST数字分類器 (ONNX形式)
- 入力: 28x28のグレースケール画像
- 出力: 10クラスの確率分布
- サイズ: 約500KB (高速ダウンロード)

**ONNX Runtime Web:**
- GPUアクセラレーションのためのWebGPU実行プロバイダー
- CPUフォールバックのためのWASM実行プロバイダー
- 自動最適化とグラフ最適化

**ブラウザAPI:**
- ハードウェアアクセスのためのWebGPU
- バックグラウンド処理のためのWeb Workers (将来的な拡張)
- 効率的な計算のためのWebAssembly

## 次のステップ

- カスタムONNXモデルで試してみる
- 実際の画像アップロードと分類を実装
- 大規模モデル向けのストリーミング推論を追加
- カメラ/マイク入力との統合を検討

---

