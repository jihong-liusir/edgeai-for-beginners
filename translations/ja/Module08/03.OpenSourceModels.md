<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:33:01+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ja"
}
-->
# セッション3: オープンソースモデルの発見と管理

## 概要

このセッションでは、Foundry Localを使用した実践的なモデルの発見と管理に焦点を当てます。利用可能なモデルをリスト化し、さまざまなオプションをテストし、基本的な性能特性を理解する方法を学びます。Foundry CLIを使った実践的な探索を通じて、ユースケースに適したモデルを選択するスキルを身につけます。

## 学習目標

- モデルの発見と管理に関するFoundry CLIコマンドを習得する
- モデルキャッシュとローカルストレージのパターンを理解する
- さまざまなモデルを迅速にテストし比較する方法を学ぶ
- モデル選択とベンチマークのための実践的なワークフローを確立する
- Foundry Localを通じて利用可能なモデルの拡大するエコシステムを探る

## 前提条件

- セッション1「Foundry Localの始め方」を完了していること
- Foundry Local CLIがインストールされ、アクセス可能であること
- モデルダウンロードのための十分なストレージスペース（モデルサイズは1GBから20GB以上）
- モデルタイプとユースケースに関する基本的な理解

## 概要

このセッションでは、オープンソースモデルをFoundry Localに導入する方法を探ります。

## パート6: 実践演習

### 演習: モデルの発見と比較

Sample 03を基にした独自のモデル評価スクリプトを作成してください:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### タスク

1. **Sample 03スクリプトを実行**: `samples\03\list_and_bench.cmd`
2. **異なるモデルを試す**: 少なくとも3つの異なるモデルをテストする
3. **性能を比較**: スピードや応答品質の違いを記録する
4. **結果を文書化**: 簡単な比較チャートを作成する

### 比較フォーマット例

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## パート7: トラブルシューティングとベストプラクティス

### よくある問題と解決策

**モデルが起動しない場合:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**メモリ不足:**
- 小型モデル（`phi-4-mini`）から始める
- 他のアプリケーションを閉じる
- 頻繁に制限に達する場合はRAMをアップグレードする

**性能が遅い場合:**
- モデルが完全にロードされていることを確認する（詳細出力をチェック）
- 不要なバックグラウンドアプリケーションを閉じる
- 高速ストレージ（SSD）を検討する

### ベストプラクティス

1. **小型モデルから始める**: セットアップを検証するために`phi-4-mini`を使用
2. **1モデルずつ実行**: 新しいモデルを開始する前に以前のモデルを停止する
3. **リソースを監視**: メモリ使用量を確認する
4. **一貫したテスト**: 公平な比較のために同じプロンプトを使用する
5. **結果を文書化**: ユースケースに基づいたモデル性能の記録を保持する

## パート8: 次のステップと参考資料

### セッション4の準備

- **セッション4の焦点**: 最適化ツールと技術
- **前提条件**: モデル切り替えと基本的な性能テストに慣れていること
- **推奨**: このセッションでお気に入りのモデルを2～3つ特定しておくこと

### 追加リソース

- **[Foundry Local ドキュメント](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: 公式ドキュメント
- **[CLIリファレンス](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: コマンドリファレンス
- **[Model Mondays](https://aka.ms/model-mondays)**: 毎週のモデルスポットライト
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: コミュニティと問題報告
- **[Sample 03: Model Discovery](samples/03/README.md)**: 実践的なサンプルスクリプト

### 重要なポイント

✅ **モデルの発見**: `foundry model list`を使用して利用可能なモデルを探索  
✅ **迅速なテスト**: `list_and_bench.cmd`パターンでの迅速な評価  
✅ **性能モニタリング**: 基本的なリソース使用量と応答時間の測定  
✅ **モデル選択**: ユースケースに基づいたモデル選択の実践的なガイドライン  
✅ **キャッシュ管理**: ストレージとクリーンアップ手順の理解  

これで、Foundry Localの簡単なCLIアプローチを使用して、AIアプリケーションに適したモデルを発見、テスト、選択するための実践的なスキルを習得しました。

参考資料:
- Foundry Local ドキュメント: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Faceモデルのコンパイル: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## 学習目標
- ローカル推論のためのオープンソースモデルを発見し評価する
- Foundry Local内で選択したHugging Faceモデルをコンパイルして実行する
- 精度、遅延、リソースニーズに基づいたモデル選択戦略を適用する
- キャッシュとバージョン管理を使用してモデルをローカルで管理する

## パート1: Foundry CLIを使用したモデル発見

### 基本的なモデル管理コマンド

Foundry CLIは、モデルの発見と管理に関する簡単なコマンドを提供します:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### 初めてのモデルを実行する

性能特性を理解するために人気のある信頼性の高いモデルから始めましょう:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**注意:** `--verbose`フラグは以下の詳細な起動情報を提供します:
- 初回実行時のモデルダウンロード進捗
- メモリ割り当ての詳細
- サービスバインディング情報
- 性能初期化メトリクス

### モデルカテゴリの理解

**小型言語モデル (SLMs):**
- `phi-4-mini`: 高速で効率的、一般的なチャットに最適
- `phi-4`: より優れた推論能力を持つバージョン

**中型モデル:**
- `qwen2.5-7b`: 優れた推論能力と長いコンテキスト
- `deepseek-r1-7b`: コード生成に最適化

**大型モデル:**
- `llama-3.2`: Metaの最新オープンソースモデル
- `qwen2.5-14b`: エンタープライズ向けの推論能力

## パート2: モデルの迅速なテストと比較

### Sample 03アプローチ: シンプルなリストとベンチ

Sample 03パターンに基づく最小限のワークフローはこちらです:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### モデル性能のテスト

モデルが実行中の場合、一貫したプロンプトでテストしてください:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShellによるテストの代替方法

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## パート3: モデルキャッシュとストレージ管理

### モデルキャッシュの理解

Foundry Localはモデルのダウンロードとキャッシュを自動的に管理します:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### モデルストレージの考慮事項

**典型的なモデルサイズ:**
- `phi-4-mini`: 約2.5 GB
- `qwen2.5-7b`: 約4.1 GB  
- `deepseek-r1-7b`: 約4.3 GB
- `llama-3.2`: 約4.9 GB
- `qwen2.5-14b`: 約8.2 GB

**ストレージのベストプラクティス:**
- 迅速な切り替えのために2～3モデルをキャッシュしておく
- 未使用のモデルを削除してスペースを確保する: `foundry cache clean`
- 特に小型SSDでディスク使用量を監視する
- モデルサイズと能力のトレードオフを検討する

### モデル性能モニタリング

モデルが実行中の場合、システムリソースを監視してください:

**Windowsタスクマネージャー:**
- メモリ使用量を確認（モデルはRAMに常駐）
- 推論中のCPU使用率を監視
- 初期モデルロード中のディスクI/Oを確認

**コマンドラインモニタリング:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## パート4: 実践的なモデル選択ガイドライン

### ユースケース別のモデル選択

**一般的なチャットとQ&Aの場合:**
- 初めに: `phi-4-mini`（高速で効率的）
- アップグレード: `phi-4`（より優れた推論能力）
- 高度な選択肢: `qwen2.5-7b`（長いコンテキスト）

**コード生成の場合:**
- 推奨: `deepseek-r1-7b`
- 代替: `qwen2.5-7b`（コードにも適している）

**複雑な推論の場合:**
- 最適: `qwen2.5-7b`または`qwen2.5-14b`
- 予算オプション: `phi-4`

### ハードウェア要件ガイド

**最低システム要件:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**最適な性能のための推奨構成:**
- 快適なマルチモデル切り替えのために32GB以上のRAM
- 高速モデルロードのためのSSDストレージ
- 優れたシングルスレッド性能を持つ最新CPU
- 加速のためのNPUサポート（Windows 11 Copilot+ PC）

### モデル切り替えワークフロー

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## パート5: シンプルなモデルベンチマーク

### 基本的な性能テスト

モデル性能を比較するための簡単なアプローチはこちらです:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### 手動品質評価

各モデルについて、一貫したプロンプトでテストし、手動で評価してください:

**テストプロンプト:**
1. 「量子コンピューティングを簡単に説明してください。」
2. 「リストをソートするPython関数を書いてください。」
3. 「リモートワークの利点と欠点は何ですか？」
4. 「エッジAIの利点を要約してください。」

**評価基準:**
- **正確性**: 情報は正しいか？
- **明確さ**: 説明は分かりやすいか？
- **完全性**: 質問全体に対応しているか？
- **速度**: 応答はどれくらい速いか？

### リソース使用量のモニタリング

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## パート6: 次のステップ
- 新しいモデルやヒントのためにModel Mondaysに登録: https://aka.ms/model-mondays
- チームの`models.json`に結果を共有
- セッション4の準備: LLMとSLMの比較、ローカルとクラウド推論、実践デモ

---

**免責事項**:  
この文書は、AI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてお考えください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。