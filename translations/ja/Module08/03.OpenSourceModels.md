<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T12:23:57+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ja"
}
-->
# セッション3: Foundry Localでのオープンソースモデル

## 概要

このセッションでは、Foundry Localにオープンソースモデルを導入する方法を探ります。コミュニティモデルの選定、Hugging Faceコンテンツの統合、「独自モデル持ち込み」（BYOM）戦略の採用について学びます。また、継続的な学習とモデル発見のための「Model Mondays」シリーズについても紹介します。

参考資料:
- Foundry Local ドキュメント: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Faceモデルのコンパイル: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## 学習目標
- ローカル推論用のオープンソースモデルを発見し評価する
- Foundry Local内で選択したHugging Faceモデルをコンパイルして実行する
- 精度、遅延、リソース要件に基づいたモデル選択戦略を適用する
- キャッシュとバージョン管理を使用してモデルをローカルで管理する

## パート1: モデルの発見と選定 (ステップバイステップ)

ステップ1) ローカルカタログ内の利用可能なモデルをリストアップ
```cmd
foundry model list
```

ステップ2) 候補モデルを2つ試す（初回実行時に自動ダウンロード）
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

ステップ3) 基本的な指標を記録
- 固定プロンプトで遅延（主観的）と品質を観察
- 各モデル実行中にタスクマネージャーでメモリ使用量を確認

## パート2: CLIを使用したカタログモデルの実行 (ステップバイステップ)

ステップ1) モデルを起動
```cmd
foundry model run llama-3.2
```

ステップ2) OpenAI互換エンドポイントを介してテストプロンプトを送信
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## パート3: BYOM – Hugging Faceモデルのコンパイル (ステップバイステップ)

公式の手順に従ってモデルをコンパイルします。以下は概要です。正確なコマンドや対応する設定についてはMicrosoft Learnの記事を参照してください。

ステップ1) 作業ディレクトリを準備
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

ステップ2) 対応するHFモデルをコンパイル
- Learnドキュメントの手順を使用して、コンパイルされたONNXモデルを`models`ディレクトリに配置
- 確認:
```cmd
foundry cache ls
```
コンパイルされたモデル名（例: `llama-3.2`）が表示されるはずです。

ステップ3) コンパイル済みモデルを実行
```cmd
foundry model run llama-3.2 --verbose
```

注意事項:
- コンパイルと実行に十分なディスク容量とRAMを確保する
- 小型モデルから始めてフローを検証し、徐々にスケールアップする

## パート4: 実践的なモデルキュレーション (ステップバイステップ)

ステップ1) `models.json`レジストリを作成
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

ステップ2) 小型セレクタースクリプト
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## パート5: ハンズオンベンチマーク (ステップバイステップ)

ステップ1) シンプルな遅延ベンチマーク
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

ステップ2) 品質のスポットチェック
- 固定プロンプトセットを使用し、出力をCSV/JSONにキャプチャ
- 流暢さ、関連性、正確性を手動で評価（1–5）

## パート6: 次のステップ
- 新しいモデルやヒントを得るためにModel Mondaysに登録: https://aka.ms/model-mondays
- チームの`models.json`に発見内容を共有
- セッション4の準備: LLMとSLMの比較、ローカル推論とクラウド推論の比較、ハンズオンデモ

---

