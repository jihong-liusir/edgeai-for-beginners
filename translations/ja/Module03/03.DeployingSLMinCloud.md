<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-07-22T04:59:49+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "ja"
}
-->
# コンテナ化されたクラウド展開 - 本番規模のソリューション

この包括的なチュートリアルでは、MicrosoftのPhi-4-mini-instructモデルをコンテナ化された環境で展開するための3つの主要なアプローチ（vLLM、Ollama、SLM Engine with ONNX Runtime）を取り上げます。この3.8Bパラメータモデルは、効率を維持しながらエッジ展開に適した推論タスクの最適な選択肢を提供します。

## 目次

1. [Phi-4-miniコンテナ展開の概要](../../../Module03)
2. [学習目標](../../../Module03)
3. [Phi-4-mini分類の理解](../../../Module03)
4. [vLLMコンテナ展開](../../../Module03)
5. [Ollamaコンテナ展開](../../../Module03)
6. [SLM Engine with ONNX Runtime](../../../Module03)
7. [比較フレームワーク](../../../Module03)
8. [ベストプラクティス](../../../Module03)

## Phi-4-miniコンテナ展開の概要

Small Language Models (SLMs)は、リソースが限られたデバイスで高度な自然言語処理機能を可能にするEdgeAIの重要な進歩を表しています。このチュートリアルでは、MicrosoftのPhi-4-mini-instructのコンテナ化された展開戦略に焦点を当てています。このモデルは、能力と効率のバランスを取った最先端の推論モデルです。

### 注目のモデル: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8Bパラメータ)**: Microsoftが開発した最新の軽量な指示調整モデルで、メモリや計算リソースが限られた環境でも以下のような優れた能力を発揮します：
- **数学的推論と複雑な計算**
- **コード生成、デバッグ、分析**
- **論理的問題解決とステップバイステップの推論**
- **詳細な説明を必要とする教育用途**
- **関数呼び出しとツール統合**

「Small SLMs」カテゴリ（1.5B～13.9Bパラメータ）の一部であるPhi-4-miniは、推論能力とリソース効率の最適なバランスを実現しています。

### Phi-4-miniコンテナ展開の利点

- **運用効率**: 推論タスクにおける高速な推論と低い計算要件
- **展開の柔軟性**: ローカル処理によるプライバシー強化を伴うオンデバイスAI機能
- **コスト効率**: 大規模モデルと比較して運用コストを削減しつつ品質を維持
- **分離性**: モデルインスタンス間の明確な分離と安全な実行環境
- **スケーラビリティ**: 推論スループットを増加させるための容易な水平スケーリング

## 学習目標

このチュートリアルを終えると、以下ができるようになります：

- Phi-4-mini-instructをさまざまなコンテナ化された環境で展開および最適化する
- 異なる展開シナリオに応じた高度な量子化および圧縮戦略を実装する
- 推論ワークロードのための本番対応のコンテナオーケストレーションを構成する
- 特定のユースケース要件に基づいて適切な展開フレームワークを評価および選択する
- コンテナ化されたSLM展開のためのセキュリティ、監視、およびスケーリングのベストプラクティスを適用する

## Phi-4-mini分類の理解

### モデル仕様

**技術的詳細:**
- **パラメータ数**: 38億（Small SLMカテゴリ）
- **アーキテクチャ**: Dense decoder-only Transformer with grouped-query attention
- **コンテキスト長**: 128Kトークン（最適なパフォーマンスには32K推奨）
- **ボキャブラリ**: 20万トークン（多言語対応）
- **トレーニングデータ**: 高品質な推論密度コンテンツ5兆トークン

### リソース要件

| 展開タイプ | 最小RAM | 推奨RAM | VRAM (GPU) | ストレージ | 主なユースケース |
|------------|---------|---------|------------|------------|-------------------|
| **開発** | 6GB | 8GB | - | 8GB | ローカルテスト、プロトタイピング |
| **本番CPU** | 8GB | 12GB | - | 10GB | エッジサーバー、コスト最適化展開 |
| **本番GPU** | 6GB | 8GB | 4-6GB | 8GB | 高スループット推論サービス |
| **エッジ最適化** | 4GB | 6GB | - | 6GB | 量子化展開、IoTゲートウェイ |

### Phi-4-miniの能力

- **数学的卓越性**: 高度な算術、代数、微積分の問題解決
- **コードインテリジェンス**: Python、JavaScript、多言語コード生成とデバッグ
- **論理的推論**: 問題のステップバイステップ分解と解決策の構築
- **教育支援**: 学習や教育シナリオに適した詳細な説明
- **関数呼び出し**: ツール統合とAPI相互作用のネイティブサポート

## vLLMコンテナ展開

vLLMは、Phi-4-mini-instructの最適化された推論性能とOpenAI互換APIを提供し、本番推論サービスに最適です。

### クイックスタート例

#### 基本的なCPU展開（開発用）
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU加速本番展開
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### 本番構成

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Phi-4-mini推論能力のテスト

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollamaコンテナ展開

Ollamaは、Phi-4-mini-instructの簡素化された展開と管理を提供し、開発およびバランスの取れた本番展開に最適です。

### クイックセットアップ

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### 本番構成

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### モデル最適化とバリアント

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API使用例

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine with ONNX Runtime

ONNX Runtimeは、Phi-4-mini-instructのエッジ展開に最適な性能を提供し、高度な最適化とクロスプラットフォーム互換性を備えています。

### 基本セットアップ

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### 簡易サーバー実装

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### モデル変換スクリプト

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### 本番構成

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ONNX展開のテスト

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## 比較フレームワーク

### Phi-4-miniのフレームワーク比較

| 特徴 | vLLM | Ollama | ONNX Runtime |
|------|------|--------|--------------|
| **セットアップの複雑さ** | 中程度 | 簡単 | 複雑 |
| **性能 (GPU)** | 優秀 (~25 tok/s) | 非常に良い (~20 tok/s) | 良い (~15 tok/s) |
| **性能 (CPU)** | 良い (~8 tok/s) | 非常に良い (~12 tok/s) | 優秀 (~15 tok/s) |
| **メモリ使用量** | 8-12GB | 6-10GB | 4-8GB |
| **API互換性** | OpenAI互換 | カスタムREST | カスタムFastAPI |
| **関数呼び出し** | ✅ ネイティブ | ✅ サポート | ⚠️ カスタム実装 |
| **量子化サポート** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX量子化 |
| **本番対応** | ✅ 優秀 | ✅ 非常に良い | ✅ 良い |
| **エッジ展開** | 良い | 優秀 | 卓越 |

## 追加リソース

### 公式ドキュメント
- **Microsoft Phi-4モデルカード**: 詳細な仕様と使用ガイドライン
- **vLLMドキュメント**: 高度な構成と最適化オプション
- **Ollamaモデルライブラリ**: コミュニティモデルとカスタマイズ例
- **ONNX Runtimeガイド**: 性能最適化と展開戦略

### 開発ツール
- **Hugging Face Transformers**: モデルの操作とカスタマイズ
- **OpenAI API仕様**: vLLM互換性テスト用
- **Dockerベストプラクティス**: コンテナのセキュリティと最適化ガイドライン
- **Kubernetes展開**: 本番スケーリングのためのオーケストレーションパターン

### 学習リソース
- **SLM性能ベンチマーク**: 比較分析の方法論
- **エッジAI展開**: リソース制約環境向けのベストプラクティス
- **推論タスク最適化**: 数学的および論理的問題のプロンプト戦略
- **コンテナセキュリティ**: AIモデル展開のための強化策

## 学習成果

このモジュールを完了すると、以下ができるようになります：

1. 複数のフレームワークを使用してPhi-4-mini-instructモデルをコンテナ化された環境で展開する
2. 異なるハードウェア環境に合わせてSLM展開を構成および最適化する
3. コンテナ化されたAI展開のセキュリティベストプラクティスを実装する
4. 特定のユースケース要件に基づいて適切な展開フレームワークを比較および選択する
5. 本番グレードのSLMサービスの監視およびスケーリング戦略を適用する

## 次のステップ

- [モジュール1](../Module01/README.md)に戻る
- [モジュール2](../Module02/README.md)に戻る

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてお考えください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。