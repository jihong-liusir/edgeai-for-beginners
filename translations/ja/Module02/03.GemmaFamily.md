<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-07-22T04:08:10+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "ja"
}
-->
# セクション3: Gemmaファミリーの基本

Gemmaモデルファミリーは、Googleが提供するオープンソースの大規模言語モデルとマルチモーダルAIに対する包括的なアプローチを表しています。これにより、アクセス可能なモデルが優れた性能を発揮し、モバイルデバイスから企業のワークステーションまで、さまざまなシナリオで展開可能であることを示しています。Gemmaファミリーが柔軟な展開オプションを提供しつつ、競争力のある性能と責任あるAIの実践を維持しながら、強力なAI機能を可能にする方法を理解することが重要です。

## はじめに

このチュートリアルでは、GoogleのGemmaモデルファミリーとその基本概念を探ります。Gemmaファミリーの進化、効果的なGemmaモデルを実現する革新的なトレーニング手法、ファミリー内の主要なバリアント、そしてさまざまな展開シナリオにおける実用的な応用について説明します。

## 学習目標

このチュートリアルを終える頃には、以下のことができるようになります：

- GoogleのGemmaモデルファミリーの設計哲学と進化を理解する
- Gemmaモデルがさまざまなパラメータサイズで高性能を達成するための主要な革新を特定する
- Gemmaモデルの異なるバリアントの利点と限界を認識する
- 実際のシナリオに適したバリアントを選択するためにGemmaモデルの知識を応用する

## 現代のAIモデルの状況を理解する

AIの状況は大きく進化しており、さまざまな組織が言語モデル開発に対して異なるアプローチを追求しています。一部はAPIを通じてのみアクセス可能な独自のクローズドソースモデルに焦点を当てる一方で、他はオープンソースのアクセス性と透明性を重視しています。従来のアプローチでは、継続的なコストがかかる巨大な独自モデルか、展開に多大な技術的専門知識を必要とするオープンソースモデルのいずれかを選ぶ必要がありました。

このパラダイムは、データ、コスト、展開の柔軟性を維持しながら強力なAI機能を求める組織にとって課題を生み出します。従来のアプローチでは、最先端の性能と実用的な展開の考慮事項の間で選択を迫られることが多いです。

## アクセス可能なAIの卓越性の課題

高品質でアクセス可能なAIの必要性は、さまざまなシナリオでますます重要になっています。たとえば、異なる組織のニーズに対応する柔軟な展開オプション、APIコストが大きくなる可能性がある費用対効果の高い実装、包括的な理解のためのマルチモーダル機能、またはモバイルやエッジデバイスでの専門的な展開が求められる場合などです。

### 展開における重要な要件

現代のAI展開は、実用性を制限するいくつかの基本的な要件に直面しています：

- **アクセス性**: 透明性とカスタマイズ性のためのオープンソースの利用可能性
- **費用対効果**: さまざまな予算に対応する合理的な計算要件
- **柔軟性**: 異なる展開シナリオに対応する複数のモデルサイズ
- **マルチモーダル理解**: ビジョン、テキスト、音声処理機能
- **エッジ展開**: モバイルやリソース制約のあるデバイスでの最適化された性能

## Gemmaモデルの哲学

Gemmaモデルファミリーは、オープンソースのアクセス性、マルチモーダル機能、実用的な展開を優先しながら、競争力のある性能特性を維持するAIモデル開発に対するGoogleの包括的なアプローチを表しています。Gemmaモデルは、Gemini研究から派生した高品質なトレーニング手法、多様なモデルサイズ、そして異なるドメインや展開シナリオに特化したバリアントを通じてこれを実現しています。

Gemmaファミリーは、性能と効率のスペクトラム全体にわたる選択肢を提供するさまざまなアプローチを包含しており、モバイルデバイスから企業サーバーまでの展開を可能にしながら、意味のあるAI機能を提供します。目標は、高品質なAI技術へのアクセスを民主化し、展開の選択肢に柔軟性を提供することです。

### Gemmaの設計の基本原則

Gemmaモデルは、他の言語モデルファミリーと区別されるいくつかの基本原則に基づいて構築されています：

- **オープンソース優先**: 研究および商業利用のための完全な透明性とアクセス性
- **研究主導の開発**: Geminiモデルを支える研究と技術を活用して構築
- **スケーラブルなアーキテクチャ**: 異なる計算要件に対応する複数のモデルサイズ
- **責任あるAI**: 安全対策と責任ある開発実践の統合

## Gemmaファミリーを可能にする主要技術

### 高度なトレーニング手法

Gemmaファミリーの特徴の一つは、GoogleのGemini研究から派生した洗練されたトレーニングアプローチです。Gemmaモデルは、より大きなモデルからの蒸留、人間のフィードバックによる強化学習（RLHF）、およびモデル統合技術を活用して、数学、コーディング、指示追従において優れた性能を達成しています。

トレーニングプロセスには、より大きな指示モデルからの蒸留、人間の好みに合わせるための人間フィードバックによる強化学習（RLHF）、数学的推論のための機械フィードバックによる強化学習（RLMF）、およびコーディング能力のための実行フィードバックによる強化学習（RLEF）が含まれます。

### マルチモーダル統合と理解

最新のGemmaモデルは、異なる入力タイプにわたる包括的な理解を可能にする洗練されたマルチモーダル機能を組み込んでいます：

**ビジョンと言語の統合（Gemma 3）**: Gemma 3はテキストと画像を同時に処理でき、画像を分析し、視覚的な内容に関する質問に答え、画像からテキストを抽出し、複雑な視覚データを理解することができます。

**音声処理（Gemma 3n）**: Gemma 3nは、高度な音声機能を備えており、自動音声認識（ASR）や自動音声翻訳（AST）を含み、特に英語とスペイン語、フランス語、イタリア語、ポルトガル語間の翻訳で優れた性能を発揮します。

**交互入力処理**: Gemmaモデルはモダリティ間で交互に入力を処理することをサポートしており、テキスト、画像、音声を一緒に処理する複雑なマルチモーダル相互作用を理解することができます。

### アーキテクチャの革新

Gemmaファミリーは、性能と効率の両方を目的としたいくつかのアーキテクチャ最適化を組み込んでいます：

**コンテキストウィンドウの拡張**: Gemma 3モデルは128Kトークンのコンテキストウィンドウを備えており、以前のGemmaモデルの16倍のサイズで、複数の文書や数百の画像を含む膨大な情報を処理することができます。

**モバイル優先アーキテクチャ（Gemma 3n）**: Gemma 3nは、Per-Layer Embeddings（PLE）技術とMatFormerアーキテクチャを活用しており、従来の小型モデルと同等のメモリフットプリントでより大きなモデルを実行することができます。

**関数呼び出し機能**: Gemma 3は関数呼び出しをサポートしており、開発者がプログラミングインターフェースの自然言語インターフェースを構築し、インテリジェントな自動化システムを作成することを可能にします。

## モデルサイズと展開オプション

現代の展開環境は、さまざまな計算要件に対応するGemmaモデルの柔軟性から恩恵を受けています：

### 小型モデル（0.6B-4B）

Gemmaは、エッジ展開、モバイルアプリケーション、リソース制約のある環境に適した効率的な小型モデルを提供しており、印象的な機能を維持しています。1Bモデルは小規模なアプリケーションに理想的であり、4Bモデルはマルチモーダルサポートを備えた性能と柔軟性のバランスを提供します。

### 中型モデル（8B-14B）

中型モデルは、プロフェッショナルなアプリケーション向けに強化された機能を提供し、ワークステーションやサーバー展開において性能と計算要件の優れたバランスを提供します。

### 大型モデル（27B以上）

フルスケールモデルは、最大の能力を必要とする要求の厳しいアプリケーション、研究、企業展開向けに最先端の性能を提供します。27Bモデルは、単一GPUで実行可能な最も能力の高いオプションを表しています。

### モバイル最適化モデル（Gemma 3n）

Gemma 3n E2BおよびE4Bモデルは、モバイルおよびエッジ展開向けに特別に設計されており、2Bおよび4Bの有効パラメータ数を持ちながら、メモリフットプリントをE2Bで2GB、E4Bで3GBにまで削減する革新的なアーキテクチャを使用しています。

## Gemmaモデルファミリーの利点

### オープンソースのアクセス性

Gemmaモデルは、責任ある商業利用を可能にするオープンウェイトを備えた完全な透明性とカスタマイズ機能を提供しており、組織が独自のプロジェクトやアプリケーションにモデルを調整して展開することを可能にします。

### 展開の柔軟性

さまざまなモデルサイズの範囲により、モバイルデバイスから高性能サーバーまでの多様なハードウェア構成にわたる展開が可能であり、Google Cloud TPU、NVIDIA GPU、ROCmを介したAMD GPU、Gemma.cppを介したCPU実行など、さまざまなプラットフォームに最適化されています。

### 多言語の卓越性

Gemmaモデルは、140以上の言語をサポートする多言語理解と生成において卓越しており、グローバルなアプリケーションに適しています。

### 競争力のある性能

Gemmaモデルは、ユーザーの好み評価において、人気のある独自モデルおよびオープンモデルの両方の中で高い評価を得ており、ベンチマークで一貫して競争力のある結果を達成しています。

### 特化した機能

ドメイン固有のアプリケーションは、Gemmaのマルチモーダル理解、関数呼び出し機能、およびさまざまなハードウェアプラットフォームにわたる最適化された性能から恩恵を受けます。

## 実用例とユースケース

技術的な詳細に入る前に、Gemmaモデルが達成できる具体的な例をいくつか見てみましょう：

### マルチモーダル理解の例

Gemma 3は、テキストと画像を同時に処理して包括的な分析を行うことができます：

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### 多言語サポートの例

Gemmaモデルは、140以上の言語にわたる強力な多言語機能を示しています：

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### 関数呼び出しの例

Gemma 3は、自然言語インターフェースを構築するための関数呼び出しをサポートしています：

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### モバイル展開の例（Gemma 3n）

Gemma 3nは、効率的なメモリ使用量でモバイルおよびエッジ展開に最適化されています：

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### 音声処理の例（Gemma 3n）

Gemma 3nは、音声認識と翻訳のための高度な音声機能を備えています：

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Gemmaファミリーの進化

### Gemma 1.0と2.0: 基盤モデル

初期のGemmaモデルは、オープンソースのアクセス性と実用的な展開の基本原則を確立しました：

- **Gemma-2Bと7B**: 効率的な言語理解に焦点を当てた初期リリース
- **Gemma 1.5シリーズ**: 拡張されたコンテキスト処理と性能向上
- **Gemma 2ファミリー**: マルチモーダル機能と拡張されたモデルサイズの導入

### Gemma 3: マルチモーダルの卓越性

Gemma 3シリーズは、マルチモーダル機能と性能において大きな進歩を遂げました。Gemini 2.0モデルを支える研究と技術から構築されたGemma 3は、ビジョンと言語の理解、128Kトークンのコンテキストウィンドウ、関数呼び出し、140以上の言語のサポートを導入しました。

Gemma 3の主な特徴：
- **Gemma 3-1Bから27B**: さまざまな展開ニーズに対応する包括的な範囲
- **マルチモーダル理解**: 高度なテキストと視覚的推論能力
- **拡張コンテキスト**: 128Kトークンの処理能力
- **関数呼び出し**: 自然言語インターフェースの構築
- **強化されたトレーニング**: 蒸留と強化学習を使用して最適化

### Gemma 3n: モバイル優先の革新

Gemma 3nは、モバイル優先のAIアーキテクチャにおける画期的な進歩を表しており、革新的なPer-Layer Embeddings（PLE）技術、計算の柔軟性を提供するMatFormerアーキテクチャ、音声処理を含む包括的なマルチモーダル機能を備えています。

Gemma 3nの革新：
- **E2BおよびE4Bモデル**: メモリフットプリントを削減しながら、2Bおよび4Bの有効パラメータ性能
- **音声機能**: 高品質なASRと音声翻訳
- **ビデオ理解**: 大幅に強化されたビデオ処理能力
- **モバイル最適化**: スマートフォンやタブレットでのリアルタイムAI向けに設計

## Gemmaモデルの応用

### 企業アプリケーション

組織は、視覚的なコンテンツを含む文書分析、マルチモーダルサポートを備えた顧客サービスの自動化、インテリジェントなコーディング支援、ビジネスインテリジェンスアプリケーションにGemmaモデルを使用しています。オープンソースの性質により、特定のビジネスニーズに合わせたカスタマイズが可能であり、データのプライバシーとコントロールを維持できます。

### モバイルお
- Gemma 3は、開発者向けに高度なテキストおよび視覚的推論能力を提供し、マルチモーダル理解のために画像とテキスト入力をサポートします  
- Gemma 3nは、Chatbot ArenaのEloスコアで人気のあるプロプライエタリモデルやオープンモデルの中で高い評価を受けており、ユーザーの強い支持を示しています  

**効率性の達成:**  
- Gemma 3モデルは最大128Kトークンのプロンプト入力を処理可能で、以前のGemmaモデルの16倍のコンテキストウィンドウを提供します  
- Gemma 3nはPer-Layer Embeddings (PLE)を活用し、大規模モデルの能力を維持しながらRAM使用量を大幅に削減します  

**モバイル最適化:**  
- Gemma 3n E2Bはわずか2GBのメモリで動作し、E4Bは8Bのパラメータ数にもかかわらず3GBのみを必要とします  
- プライバシーを重視したオフライン対応のリアルタイムAI機能をモバイルデバイス上で実現  

**トレーニング規模:**  
- Gemma 3は、Google TPUとJAXフレームワークを使用して、1Bモデルで2Tトークン、4Bモデルで4Tトークン、12Bモデルで12Tトークン、27Bモデルで14Tトークンをトレーニングしました  

### モデル比較マトリックス  

| モデルシリーズ | パラメータ範囲 | コンテキスト長 | 主な強み | 最適なユースケース |  
|----------------|----------------|----------------|----------|--------------------|  
| **Gemma 3**    | 1B-27B         | 128K           | マルチモーダル理解、関数呼び出し | 一般的なアプリケーション、ビジョンと言語のタスク |  
| **Gemma 3n**   | E2B (5B), E4B (8B) | 可変         | モバイル最適化、音声処理 | モバイルアプリ、エッジコンピューティング、リアルタイムAI |  
| **Gemma 2.5**  | 0.5B-72B       | 32K-128K       | バランスの取れた性能、多言語対応 | 本番環境での展開、既存のワークフロー |  
| **Gemma-VL**   | 各種           | 可変           | ビジョンと言語の特化 | 画像解析、視覚的質問応答 |  

## モデル選択ガイド  

### 基本的なアプリケーション向け  
- **Gemma 3-1B**: 軽量なテキストタスク、シンプルなモバイルアプリケーション  
- **Gemma 3-4B**: 一般的な用途向けのマルチモーダルサポートを備えたバランスの取れた性能  

### マルチモーダルアプリケーション向け  
- **Gemma 3-4B/12B**: 画像理解、視覚的質問応答  
- **Gemma 3n**: 音声処理機能を備えたモバイルマルチモーダルアプリ  

### モバイルおよびエッジ展開向け  
- **Gemma 3n E2B**: リソース制約のあるデバイス、リアルタイムモバイルAI  
- **Gemma 3n E4B**: 音声機能を備えたモバイル性能の向上  

### エンタープライズ展開向け  
- **Gemma 3-12B/27B**: 高性能な言語および視覚理解  
- **関数呼び出し機能**: インテリジェントな自動化システムの構築  

### グローバルアプリケーション向け  
- **Gemma 3の任意のバリアント**: 140以上の言語サポートと文化的理解  
- **Gemma 3n**: 音声翻訳を備えたモバイルファーストのグローバルアプリケーション  

## 展開プラットフォームとアクセス性  

### クラウドプラットフォーム  
- **Vertex AI**: サーバーレス体験を備えたエンドツーエンドのMLOps機能  
- **Google Kubernetes Engine (GKE)**: 複雑なワークロードのためのスケーラブルなコンテナ展開  
- **Google GenAI API**: 高速プロトタイピングのための直接APIアクセス  
- **NVIDIA APIカタログ**: NVIDIA GPU上での最適化された性能  

### ローカル開発フレームワーク  
- **Hugging Face Transformers**: 開発のための標準的な統合  
- **Ollama**: 簡素化されたローカル展開と管理  
- **vLLM**: 本番環境向けの高性能サービング  
- **Gemma.cpp**: CPU最適化実行  
- **Google AI Edge**: モバイルおよびエッジ展開の最適化  

### 学習リソース  
- **Google AI Studio**: 数クリックでGemmaモデルを試す  
- **KaggleおよびHugging Face**: モデルウェイトとコミュニティ例をダウンロード  
- **技術レポート**: 包括的なドキュメントと研究論文  
- **コミュニティフォーラム**: 活発なコミュニティサポートと議論  

### Gemmaモデルの始め方  

#### 開発プラットフォーム  
1. **Google AI Studio**: ウェブベースの実験から始める  
2. **Hugging Face Hub**: モデルとコミュニティ実装を探索  
3. **ローカル展開**: OllamaまたはTransformersを使用して開発  

#### 学習パス  
1. **コアコンセプトを理解**: マルチモーダル機能と展開オプションを学ぶ  
2. **バリアントを試す**: 異なるモデルサイズと特化バージョンを試す  
3. **実装を練習**: 開発環境でモデルを展開  
4. **本番環境向けに最適化**: 特定のユースケースとプラットフォームに合わせて微調整  

#### ベストプラクティス  
- **小規模から始める**: 初期開発とテストにはGemma 3-4Bを使用  
- **公式テンプレートを使用**: 最適な結果を得るために適切なチャットテンプレートを適用  
- **リソースを監視**: メモリ使用量と推論性能を追跡  
- **特化を検討**: マルチモーダルやモバイルのニーズに適したバリアントを選択  

## 高度な使用パターン  

### ファインチューニングの例  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### 特化したプロンプトエンジニアリング  

**マルチモーダルタスク向け:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**コンテキストを伴う関数呼び出し向け:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### 文化的文脈を考慮した多言語アプリケーション  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### 本番環境展開パターン  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## 性能最適化戦略  

### メモリ最適化  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### 推論最適化  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## ベストプラクティスとガイドライン  

### セキュリティとプライバシー  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### モニタリングと評価  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## 結論  

Gemmaモデルファミリーは、GoogleがAI技術を民主化し、多様なアプリケーションや展開シナリオで競争力のある性能を維持するための包括的なアプローチを表しています。オープンソースのアクセス性、マルチモーダル機能、革新的なアーキテクチャ設計への取り組みを通じて、Gemmaは組織や開発者がリソースや特定の要件に関係なく強力なAI機能を活用できるようにします。  

### 主なポイント  

**オープンソースの卓越性**: Gemmaは、オープンソースモデルがプロプライエタリな代替品と競争できる性能を達成しながら、透明性、カスタマイズ性、AI展開の制御を提供できることを示しています。  

**マルチモーダルの革新**: Gemma 3およびGemma 3nにおけるテキスト、ビジョン、音声機能の統合は、アクセス可能なマルチモーダルAIにおける重要な進歩を示し、異なる入力タイプ間での包括的な理解を可能にします。  

**モバイルファーストアーキテクチャ**: Gemma 3nの画期的なPer-Layer Embeddings (PLE)技術とモバイル最適化は、リソース制約のあるデバイス上で効率的に動作する強力なAIを、能力を犠牲にすることなく実現します。  

**スケーラブルな展開**: 1Bから27Bパラメータまでの範囲と、特化したモバイルバリアントにより、一貫した品質と性能を維持しながら、計算環境全体での展開が可能です。  

**責任あるAI統合**: ShieldGemma 2を通じた組み込みの安全対策と責任ある開発プラクティスにより、強力なAI機能を安全かつ倫理的に展開できます。  

### 将来の展望  

Gemmaファミリーが進化を続ける中で、以下が期待されます:  

**モバイル機能の強化**: Gemma 3nアーキテクチャのAndroidやChromeなどの主要プラットフォームへの統合によるモバイルおよびエッジ展開のさらなる最適化。  

**マルチモーダル理解の拡大**: ビジョン、言語、音声の統合のさらなる進歩による、より包括的なAI体験の実現。  

**効率性の向上**: パラメータごとの性能比率の向上と計算要件の削減を実現する継続的なアーキテクチャ革新。  

**エコシステム統合の拡大**: 既存のワークフローへのシームレスな統合を可能にする開発フレームワーク、クラウドプラットフォーム、展開ツール全体でのサポートの強化。  

**コミュニティの成長**: コミュニティが作成したモデル、ツール、アプリケーションの拡大により、Gemmaのコア機能を拡張。  

### 次のステップ  

リアルタイムAI機能を備えたモバイルアプリケーションの構築、マルチモーダル教育ツールの開発、インテリジェントな自動化システムの作成、または多言語対応を必要とするグローバルアプリケーションの作業を行う場合、Gemmaファミリーはスケーラブルなソリューションを提供し、強力なコミュニティサポートと包括的なドキュメントを備えています。  

**始めるための推奨事項:**  
1. **Google AI Studioで実験**: 即座にハンズオン体験を開始  
2. **Hugging Faceからモデルをダウンロード**: ローカル開発とカスタマイズ  
3. **特化したバリアントを探索**: モバイルアプリケーション向けのGemma 3nを試す  
4. **マルチモーダル機能を実装**: 包括的なAI体験を実現  
5. **セキュリティのベストプラクティスに従う**: 本番環境展開のために  

**モバイル開発向け**: 音声および視覚機能を備えたリソース効率の高い展開にはGemma 3n E2Bを使用。  

**エンタープライズアプリケーション向け**: 最大の能力を備えたGemma 3-12Bまたは27Bモデルを検討。  

**グローバルアプリケーション向け**: 140以上の言語サポートと文化的配慮を備えたGemmaを活用。  

**特化したユースケース向け**: ファインチューニングアプローチとドメイン特化の最適化技術を探索。  

### 🔮 AIの民主化  

Gemmaファミリーは、強力で能力のあるモデルが個人開発者から大企業まで誰でも利用可能である未来のAI開発を体現しています。最先端の研究とオープンソースのアクセス性を組み合わせることで、Googleはすべてのセクターと規模でのイノベーションを可能にする基盤を作り上げました。  

100万回以上のダウンロードと60,000以上のコミュニティバリアントを超える成功を収めたGemmaは、オープンなコラボレーションがAI技術の進歩を推進する力を示しています。今後もGemmaファミリーはAIイノベーションの触媒として機能し、以前はプロプライエタリで高価なモデルでしか実現できなかったアプリケーションの開発を可能にします。  

AIの未来はオープンでアクセス可能、そして強力です。Gemmaファミリーは、このビジョンを現実のものにするためのリーダーです。

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は責任を負いません。