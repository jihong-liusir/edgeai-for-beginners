<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20892f7994d9e5d2473ad19dfa93cf6d",
  "translation_date": "2025-07-22T03:53:06+00:00",
  "source_file": "Module02/01.PhiFamily.md",
  "language_code": "ja"
}
-->
# セクション 1: Microsoft Phi モデルファミリーの基礎

Microsoft Phi モデルファミリーは、人工知能におけるパラダイムシフトを示しており、コンパクトで効率的なモデルが、従来の大規模言語モデルよりもはるかにリソース効率が高い状態で驚異的なパフォーマンスを達成できることを証明しています。Phi ファミリーがどのようにして計算要件を削減しながら、さまざまなタスクで高いパフォーマンスを維持しつつ強力な AI 機能を可能にしているかを理解することが重要です。

## 開発者向けリソース

### Azure AI Foundry モデルカタログ
Phi モデルファミリー（Phi-silica を除く）は [Azure AI Foundry Model Catalog](https://ai.azure.com/explore/models?q=phi) を通じて利用可能で、開発者がこれらのモデルを簡単にアクセス、微調整、そしてアプリケーションに展開することができます。このカタログは、さまざまな Phi バリアントを試し、プロジェクトに統合するための効率的な方法を提供します。

### Azure AI Foundry
[Azure AI Foundry](https://ai.azure.com) を使用して Phi モデルを展開し、実験することができます。これにより、最小限のセットアップで AI ソリューションを構築、テスト、展開するための包括的な環境が提供されます。

### Foundry Local
ローカルでの開発と展開には、[Microsoft Foundry Local](https://github.com/microsoft/foundry-local) をチェックしてください。これにより、開発マシン上で最適化された構成で Phi モデルを実行できます。

### ドキュメントリソース
- [Microsoft Research: Phi Model Technical Reports](https://ai.azure.com/labs/projects/phi-4)
- [Phi Cookbook](https://aka.ms/phicookbook)

## はじめに

このレッスンでは、Microsoft の Phi モデルファミリーとその基本概念を探ります。Phi ファミリーの進化、Phi モデルを効率的にする革新的なトレーニング手法、ファミリー内の主要なバリアント、そしてさまざまなシナリオでの実用的な応用について説明します。

## 学習目標

このレッスンの終了時には、以下のことができるようになります：

- Microsoft の Phi モデルファミリーの設計哲学と進化を理解する。
- Phi モデルが少ないパラメータで高いパフォーマンスを達成するための主要な革新を特定する。
- 異なる Phi モデルバリアントの利点と制限を認識する。
- 実際のシナリオに適したバリアントを選択するために Phi モデルの知識を応用する。

## 従来の AI モデルパラダイムの理解

従来、自然言語処理で高いパフォーマンスを達成するには、数十億または数百億のパラメータを持つ巨大な言語モデルが必要でした。組織は通常、これらのモデルを強力な GPU クラスター上で展開し、API インターフェースや専用ハードウェアインフラを通じてその機能にアクセスします。

このアプローチは多くのアプリケーションでうまく機能しますが、実際の展開シナリオでは固有の制限があります。従来の方法では、膨大な計算リソース、大量のメモリ、そして多大なエネルギー消費を必要とするモデルを使用します。この方法は最先端の機能へのアクセスを提供しますが、高価なハードウェアへの依存を生み出し、高い運用コストを招き、展開の柔軟性を制限します。

## 効率的な AI 展開の課題

効率的な AI の必要性は、さまざまなシナリオでますます重要になっています。プライバシーの理由でローカル展開が必要なアプリケーション、クラウド API コストが高額になるコストセンシティブな実装、ハードウェアリソースが限られたエッジコンピューティングシナリオ、またはリアルタイムアプリケーションでの低遅延が求められる場合を考えてみてください。

### 主な展開制約

従来の大規模モデル展開には、実用性を制限するいくつかの基本的な制約があります：

- **コスト制限**: 高い計算コストにより、多くの組織にとって継続的な展開が高額になります。
- **リソース制約**: 高性能 GPU インフラへのアクセスが限られているため、展開オプションが制限されます。
- **プライバシー要件**: 敏感なアプリケーションでは、データプライバシーを維持するためにローカル処理が必要です。
- **遅延感度**: リアルタイムアプリケーションでは、クラウド往復の遅延なしで即時応答が必要です。

## Microsoft Phi モデルの哲学

Microsoft Phi モデルファミリーは、効率性と実用的な展開を優先しながら強力なパフォーマンス特性を維持するという、AI モデル設計哲学の根本的な変化を表しています。Phi モデルは、革新的なアーキテクチャ、高品質なトレーニング手法、そして専門的な最適化技術を通じてこれを実現しています。

Phi ファミリーは、標準的なハードウェアでの展開を可能にしながら、意味のある AI 機能を提供するために、パラメータごとのパフォーマンスを最大化するさまざまなアプローチを包含しています。目標は、計算要件、メモリ使用量、運用コストを劇的に削減しながら競争力のあるパフォーマンスを維持することです。

### Phi の設計の基本原則

Phi モデルは、従来の大規模言語モデルとは異なるいくつかの基本原則に基づいて構築されています：

- **効率性重視**: 絶対的な規模ではなく、パラメータごとの最大パフォーマンスを最適化。
- **高品質なトレーニング**: 大量のデータセットではなく、高品質で厳選されたトレーニングデータに焦点を当てる。
- **展開の柔軟性**: さまざまなハードウェア構成で効果的に動作するよう設計。
- **専門的な能力**: 特定のタスクやドメインに最適化されることが多く、効果を最大化。

## Phi ファミリーを可能にする主要技術

### 「教科書」トレーニングアプローチ

Phi ファミリーの最も革新的な側面の一つは、「教科書品質」のトレーニング手法です。膨大な量の未フィルタリングのインターネットデータでトレーニングする代わりに、Phi モデルは、推論、数学、コーディング、一般知識を効果的に教えるよう設計された厳選された高品質な教育コンテンツを使用します。

このアプローチは、高品質な教科書や学術資料を模倣した合成教育コンテンツを作成することで機能します。トレーニングデータは教育的に健全であるように特別に設計されており、明確な説明、段階的な推論、構造化された知識の提示に焦点を当てています。

### 高度な推論トレーニング

最近の Phi モデルは、複雑な多段階問題解決を可能にする洗練された推論トレーニング手法を取り入れています。これらの技術には以下が含まれます：

**Chain-of-Thought トレーニング**: モデルが複雑な問題を中間的な推論ステップに分解することを学び、問題解決プロセスをより透明で信頼性の高いものにします。

**推論時スケーリング**: モデルが応答生成中に追加の計算リソースを活用して詳細な推論チェーンを生成し、精度を向上させます。

**能力の限界トレーニング**: トレーニングデータは、モデルの現在の能力の限界に挑戦するように特別に選ばれ、複雑な推論パターンの学習を促進します。

### アーキテクチャの革新

Phi ファミリーは、効率性を特に重視したいくつかのアーキテクチャ最適化を取り入れています：

**パラメータ効率**: モデル内の各パラメータの影響を最大化する慎重なアーキテクチャ選択。

**マルチモーダル統合**: コンパクトなアーキテクチャ内でのテキスト、ビジョン、音声処理能力の効率的な統合。

**ハードウェア最適化**: 特定のハードウェアプラットフォームや展開シナリオに最適化された専門的なバリアント。

## Phi モデルのハードウェア最適化

現代の展開環境は、さまざまなハードウェア構成にわたる Phi モデルの効率性の恩恵を受けています：

### CPU 最適化展開

Phi モデルは CPU のみのハードウェアで効果的に動作するよう設計されており、特別な AI アクセラレータを必要とせずに標準的なコンピューティングインフラでの展開を可能にします。

### GPU 加速

強力な GPU を必要としない一方で、Phi モデルは利用可能な GPU リソースを活用してパフォーマンスを向上させることができ、展開構成に柔軟性を提供します。

### エッジデバイス統合

Phi-3-Silica のような専門的なバリアントは、特定のエッジコンピューティングプラットフォームに最適化されており、1.5W の消費電力で 650 トークン/秒という驚異的な効率指標を達成しています。

## Phi モデルファミリーの利点

### コスト効率

Phi モデルは、競争力のあるパフォーマンスを維持しながら、必要な計算インフラを大幅に削減することで運用コストを劇的に削減します。これにより、予算が限られている組織や推論あたりのコストが重要な高ボリュームアプリケーションに AI を提供します。

### 展開の柔軟性

Phi モデルの効率性により、個人のノートパソコンから企業のサーバーまで、幅広いハードウェア構成での展開が可能になり、組織に AI インフラ選択の柔軟性を提供します。

### プライバシーとセキュリティ

Phi モデルの効率性により、プライバシーに敏感なアプリケーションでのローカル展開が可能になり、機密データが組織の管理を離れることなく強力な AI 機能を提供します。

### リアルタイムパフォーマンス

計算要件の削減は推論時間の短縮につながり、リアルタイムアプリケーションで即時応答を可能にし、AI 機能の品質を損なうことなく実現します。

### アクセシブルな AI

展開の障壁を減らすことで、Phi モデルは限られたリソースを持つ組織に高度な AI 機能を提供し、小規模なチームや個々の開発者が洗練された言語理解を活用できるようにします。

## 実用例とユースケース

技術的な進化に入る前に、Phi モデルが達成できる具体的な例をいくつか見てみましょう：

### 数学的推論の例

Phi-4-mini-reasoning は段階的な数学的問題解決に優れています。例えば、「3*x^2+4*x+5=1 を解く方法は？」と尋ねると、モデルは詳細な推論ステップを提供します：

```
<|system|>Your name is Phi, an AI math expert developed by Microsoft.<|end|>
<|user|>How to solve 3*x^2+4*x+5=1?<|end|>
<|assistant|>
I need to solve the quadratic equation 3x² + 4x + 5 = 1.

First, let me rearrange this to standard form:
3x² + 4x + 5 = 1
3x² + 4x + 5 - 1 = 0  
3x² + 4x + 4 = 0

Now I can use the quadratic formula: x = (-b ± √(b² - 4ac)) / (2a)
Where a = 3, b = 4, c = 4

Let me calculate the discriminant:
b² - 4ac = 4² - 4(3)(4) = 16 - 48 = -32

Since the discriminant is negative (-32), this equation has no real solutions.
The solutions are complex: x = (-4 ± √(-32)) / 6 = (-4 ± 4i√2) / 6 = (-2 ± 2i√2) / 3
```

### 多言語サポートの例

Phi-3.5-mini-instruct はさまざまな言語で強力な多言語能力を示します。以下はプロンプト形式の例です：

```
<|system|>You are a helpful assistant.<|end|>
<|user|>How to explain Internet for a medieval knight?<|end|>
<|assistant|>
Imagine, good sir knight, a vast network of invisible roads that span the entire realm and beyond. The Internet is like a magical system of messenger birds that can carry any message - be it text, images, or even moving pictures - instantly to any castle, village, or hamlet in the world.

Think of it as if every scribe's work could be copied and shared with thousands of people simultaneously, as if you could visit any library in any kingdom without leaving your chambers, or as if you could hold council with knights from distant lands as if they were sitting at your very table.

This "Internet" allows people to:
- Send messages faster than the swiftest horse
- Access knowledge from scholars across all kingdoms
- Trade goods without traveling to distant markets
- Share stories and news from lands you've never seen

'Tis like having a thousand scribes, merchants, and storytellers all working for you at once, good knight!
```

### マルチモーダル能力の例

Phi-4-multimodal はテキスト、画像、音声を同時に処理できます。以下は実用的な応用例です：

**音声入力を使った旅行計画**:
Phi-4 Multimodal が話し言葉を分析してシアトルへの旅行計画を支援する様子を見てください。高度な音声処理と推奨機能を示しています。

**画像からの数学的問題解決**:
Phi-4 Multimodal が画像に提示された方程式を処理して解決する様子を見てください。視覚入力を処理して問題を解決する能力を示しています。

**関数呼び出しの例**:
Phi-4-mini や Phi-4-multimodal は、検索エンジンの統合やさまざまなツールとの接続を通じてテキスト処理能力を拡張できます。例として、Phi-4-mini がプレミアリーグの試合情報を取得する様子を示し、外部データソースとのシームレスな連携能力を示しています。

### コード生成の例

Phi-4-multimodal は画像コンテンツと提供されたプロンプトに基づいて構造化されたプロジェクトコードを生成できます。この実用的なワークフローを以下に示します：

1. ワイヤーフレームやデザインの画像をアップロード
2. プロジェクト要件に関するコンテキストを提供
3. モデルが完全な機能的コード構造を生成
4. 特定のフレームワークや言語に基づいてコードをカスタマイズ可能

### エッジ展開の例

量子化されたモデルをエッジデバイスに展開できます。Microsoft Olive と ONNX GenAI Runtime を組み合わせることで、Phi-4-mini を Windows、iPhone、Android などのデバイスに展開できます。これは iPhone 12 Pro で実行されている例です。

展開プロセスには以下が含まれます：
- モバイル最適化のためのモデル量子化
- クロスプラットフォーム互換性のための ONNX ランタイム統合
- インターネット接続なしでのローカル推論
- 最小限の電力消費でのリアルタイムパフォーマンス

## Phi ファミリーの進化

### Phi-1 と Phi-2: 基盤モデル

初期の Phi モデルは、高品質なトレーニングデータと効率的なアーキテクチャの基本原則を確立しました：

- **Phi-1 (1.3B パラメータ)**: 基本的な言語理解とコード生成のための厳選されたトレーニングデータの概念を導入。
- **Phi-2 (2.7B パラメータ)**: 合成 NLP データと慎重にフィルタリングされたウェブコンテンツを通じて推論能力を強化。

### Phi-3 ファミリー: 主流採用

Phi-3 シリーズは、複数の専門的なバリアントを備えた SLM 能力のブレークスルーを示しました：

- **Phi-3-mini (3.8B パラメータ)**: 優れた効率性を持つ一般的な言語タスクで、2倍のサイズのモデルを上回る性能。
- **Phi-3-small (7B パラメータ)**: GPT-3.5 Turbo をさまざまなベンチマークで上回る高度な性能。
- **Phi-3-medium (14B パラメータ)**: Gemini 1.0 Pro を上回るエンタープライズグレードの性能。
- **Phi-3-vision (4.2B パラメータ)**: 画像とテキスト処理のためのマルチモーダル能力。
- **Phi-3-Silica (3.3B パラメータ)**: Windows 11 組み込み
Phiファミリーは、AIの展開において、単に大規模なモデルを構築するだけでなく、よりスマートで効率的なモデルを構築することが重要であることを示しています。これにより、多様なハードウェア環境で高いパフォーマンス基準を維持しながら効果的に動作することが可能になります。

## 開発と統合の例

### Transformersを使ったクイックスタート

Hugging Face Transformersライブラリを使用してPhiモデルを始める方法は以下の通りです：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Phi-4-mini-instruct
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation="flash_attention_2"  # For optimized performance
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")

# Format your prompt using the chat template
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

# Apply chat template
input_text = tokenizer.apply_chat_template(messages, tokenize=False)
inputs = tokenizer(input_text, return_tensors="pt")

# Generate response
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)
    
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### ファインチューニングの例

以下の例は、Phi-4-mini-instructを特定のタスクにファインチューニングする方法を示しています：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig
from trl import SFTTrainer
from datasets import load_dataset

# Configuration for LoRA fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules="all-linear"
)

# Training configuration optimized for efficiency
training_config = TrainingArguments(
    output_dir="./phi-4-mini-finetuned",
    learning_rate=5.0e-06,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    num_train_epochs=1,
    bf16=True,
    gradient_checkpointing=True,
    warmup_ratio=0.2,
    save_steps=100
)

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = 'right'

# Load and process dataset
train_dataset = load_dataset("HuggingFaceH4/ultrachat_200k", split="train_sft")

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_config,
    peft_config=peft_config,
    train_dataset=train_dataset,
    max_seq_length=2048,
    tokenizer=tokenizer,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### 専用プロンプト形式

**推論タスク向け (Phi-4-reasoning-plus):**  
```
<|im_start|>system<|im_sep|>
You are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions.

Please structure your response into two main sections:
<think>
{Thought section}
</think>
{Solution section}
<|im_end|>
<|im_start|>user<|im_sep|>
What is the derivative of x^2?
<|im_end|>
<|im_start|>assistant<|im_sep|>
```

**数学タスク向け (Phi-4-mini-reasoning):**  
```
<|system|>
Your name is Phi, an AI math expert developed by Microsoft.
<|end|>
<|user|>
Solve this calculus problem: Find the integral of 2x + 3
<|end|>
<|assistant|>
```

### ONNXを使ったモバイル展開

```python
import onnxruntime as ort
import numpy as np

# Load ONNX model for mobile deployment
session = ort.InferenceSession("phi-4-mini-quantized.onnx")

# Prepare input
input_ids = tokenizer.encode("Hello, how are you?", return_tensors="np")

# Run inference
outputs = session.run(None, {"input_ids": input_ids})
predicted_ids = outputs[0]

# Decode response
response = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)
```

## パフォーマンスベンチマークと成果

Phiモデルファミリーは、さまざまなベンチマークで顕著なパフォーマンスを達成しており、しばしばはるかに大きなモデルを凌駕しています。

### 主なパフォーマンスハイライト

**数学的推論の卓越性：**
- Phi-4はAIME 2025（数学オリンピアド予選）で82.5%の精度を達成
- Phi-4-reasoning (14B)は、推論ベンチマークでDeepSeek-R1-Distill-70B（5倍のサイズ）を上回る
- Phi-4-mini-reasoning (3.8B)は、数学的推論タスクで2倍のサイズのモデルに匹敵

**効率性の成果：**
- Phi-3-Silicaは、わずか1.5Wの消費電力で650トークン/秒を達成
- Phi-4-mini (3.8B)は、はるかに大きなモデルと同等のパフォーマンスを実現

**ベンチマークパフォーマンス：**
- **MMLU (Massive Multitask Language Understanding)**: 57の学術分野で競争力のあるパフォーマンス
- **HumanEval**: 特にPythonでの強力なコード生成能力
- **MGSM**: 多言語の小学校レベルの数学問題解決
- **DROP**: 複雑な理解と推論タスク
- **SimpleQA**: 事実に基づく応答の正確性

### 📊 モデル比較マトリックス

| モデル | パラメータ | コンテキスト長 | 主な強み | 最適なユースケース |
|-------|------------|----------------|----------|--------------------|
| **Phi-3-mini** | 3.8B | 4K/128K | 一般的な効率性 | モバイルアプリ、基本的なチャットボット |
| **Phi-3.5-mini** | 3.8B | 128K | 多言語対応 | 国際的なアプリケーション |
| **Phi-4-mini** | 3.8B | 128K | 強化された推論、関数呼び出し | ビジネス自動化 |
| **Phi-4-mini-reasoning** | 3.8B | 128K | 数学的推論 | 教育プラットフォーム |
| **Phi-4** | 14B | 32K | 複雑な推論 | 研究、高度な分析 |
| **Phi-4-reasoning** | 14B | 32K/64K | マルチステップ推論 | 科学計算 |
| **Phi-4-reasoning-plus** | 14B | 32K | 最大精度の推論 | 重要な意思決定 |
| **Phi-4-multimodal** | 5.6B | 可変 | 音声、ビジョン、テキスト | マルチメディアアプリケーション |

## モデル選択ガイド

### 基本的なアプリケーション向け
- **Phi-3-mini**: シンプルなテキスト生成、基本的なQ&A、迅速な応答
- **Phi-4-mini**: 関数呼び出し機能を備えた強化された推論

### 数学および推論タスク向け
- **Phi-4**: 複雑な数学問題解決と推論
- **Phi-4-reasoning**: 詳細な説明を伴うマルチステップ推論
- **Phi-4-reasoning-plus**: 重要な推論アプリケーション向けの最大精度
- **Phi-4-mini-reasoning**: リソース制約のある環境向けの効率的な数学的推論

### マルチモーダルアプリケーション向け
- **Phi-3-vision**: 画像とテキストの処理の組み合わせ
- **Phi-4-multimodal**: 音声、ビジョン、テキストの包括的な機能

### エンタープライズ展開向け
- **Phi-3-medium**: ビジネスアプリケーション向けの高度な言語理解
- **Phi-3-Silica**: 特定のハードウェアプラットフォーム向けに最適化

## 展開プラットフォームとアクセス性

### クラウドプラットフォーム
- **Azure AI Foundry**: エンタープライズツールを備えたフル機能の展開
- **Hugging Face**: オープンソースのモデルリポジトリとコミュニティリソース
- **NVIDIA API Catalog**: マイクロサービス展開オプション

### ローカル開発フレームワーク
- **Ollama**: ローカルモデル展開の軽量フレームワーク
- **ONNX Runtime**: さまざまなハードウェア構成に最適化  
- **DirectML**: Windows向けに最適化されたパフォーマンス
- **llama.cpp**: クロスプラットフォーム推論エンジン

### 学習リソース
- **Phi Portal**: Microsoft Phi公式ドキュメントハブ
- **Phi Cookbook**: 包括的な例とチュートリアル
- **Technical Reports**: arxiv上の詳細な研究論文
- **Community Spaces**: Hugging Faceのインタラクティブデモ

### Phiモデルの始め方

#### 開発プラットフォーム
1. **Azure AI Foundry**: シンプルなローカルCLIとモデル管理
2. **Hugging Face Transformers**: ローカルでの迅速な実験
3. **Ollama**: テスト用のシンプルなローカル展開

#### 学習パス
1. **コアコンセプトを理解する**: 基本的な設計原則を学ぶ
2. **バリアントを試す**: 異なるPhiモデルを試して能力を理解する
3. **実装を練習する**: テスト環境でモデルを展開する
4. **展開を拡大する**: 成功したパイロットに基づいて使用を徐々に拡大する

#### ベストプラクティス
- **小規模から始める**: 初期開発にはPhi-miniモデルを使用
- **プロンプトを最適化する**: 最良の結果を得るために適切なチャット形式を使用
- **パフォーマンスを監視する**: 推論速度と精度の指標を追跡
- **ハードウェアを考慮する**: 利用可能な計算リソースにモデルサイズを合わせる

## 結論

Microsoft Phiモデルファミリーは、AIモデル設計における革新的なアプローチを示しており、小型で効率的なモデルがさまざまなタスクで顕著なパフォーマンスを達成できることを証明しています。高品質なトレーニングデータとアーキテクチャの最適化に焦点を当てることで、Phiファミリーは従来の大規模言語モデルと比較して、計算要件を大幅に削減しながら卓越した能力を提供します。

## 主な学習目標

1. MicrosoftのPhiモデルファミリーの設計哲学と進化（Phi-1からPhi-4まで）を理解する
2. 「教科書品質」のトレーニングやアーキテクチャの最適化など、主要なイノベーションを特定する
3. 異なるPhiバリアントの利点と制限を、さまざまな展開シナリオで認識する
4. 特定のユースケースやハードウェア制約に適したPhiモデルを選択する知識を適用する
5. リソース制約のあるデバイスでPhiモデルを展開するための最適化技術を実装する
6. 従来の大規模言語モデルに対するPhiモデルファミリーのアーキテクチャ上の利点を説明する
7. 特定のアプリケーション要件やハードウェア制約に基づいて適切なPhiバリアントを選択する
8. クラウドおよびエッジ展開シナリオでPhiモデルを最適化された構成で実装する
9. ターゲットデバイスでPhiモデルのパフォーマンスを向上させるための量子化および最適化技術を適用する
10. Phiファミリー全体でモデルサイズ、パフォーマンス、能力のトレードオフを評価する

## 次のステップ

- [02: Qwenファミリーの基礎](02.QwenFamily.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。