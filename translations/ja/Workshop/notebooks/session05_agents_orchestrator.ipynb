{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# セッション 5 – マルチエージェントオーケストレーター\n",
    "\n",
    "Foundry Local を使用して、シンプルな2エージェントのパイプライン（リサーチャー -> エディター）をデモンストレーションします。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### 説明: 依存関係のインストール\n",
    "ローカルモデルへのアクセスとチャット完了に必要な`foundry-local-sdk`と`openai`をインストールします。冪等性があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# シナリオ\n",
    "最小限の2エージェントオーケストレーターパターンを実装します：\n",
    "- **研究者エージェント**は簡潔な事実を箇条書きで収集します\n",
    "- **編集者エージェント**は経営層向けの明確な表現に書き換えます\n",
    "\n",
    "各エージェントごとの共有メモリ、中間出力の順次受け渡し、そしてシンプルなパイプライン関数を示します。さらに役割（例：批評家、検証者）や並列分岐を追加して拡張可能です。\n",
    "\n",
    "**環境変数:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - 使用するデフォルトモデル（デフォルト: phi-4-mini）\n",
    "- `AGENT_MODEL_PRIMARY` - 主エージェントモデル（ALIASを上書き）\n",
    "- `AGENT_MODEL_EDITOR` - 編集者エージェントモデル（デフォルトは主エージェントモデル）\n",
    "\n",
    "**SDKリファレンス:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**動作の仕組み:**\n",
    "1. **FoundryLocalManager**がFoundry Localサービスを自動的に起動\n",
    "2. 指定されたモデルをダウンロードしてロード（またはキャッシュ版を使用）\n",
    "3. OpenAI互換のエンドポイントを提供して対話を可能に\n",
    "4. 各エージェントが専門的なタスクに異なるモデルを使用可能\n",
    "5. 内蔵のリトライロジックが一時的な障害を優雅に処理\n",
    "\n",
    "**主な特徴:**\n",
    "- ✅ 自動サービス検出と初期化\n",
    "- ✅ モデルライフサイクル管理（ダウンロード、キャッシュ、ロード）\n",
    "- ✅ OpenAI SDK互換性で馴染みのあるAPIを提供\n",
    "- ✅ エージェント専門化のためのマルチモデル対応\n",
    "- ✅ リトライロジックによる堅牢なエラー処理\n",
    "- ✅ ローカル推論（クラウドAPI不要）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### 説明: コアインポートと型付け\n",
    "エージェントメッセージの保存に使用するdataclassを導入し、型ヒントでコードの明確性を向上させます。後続のエージェントアクションのためにFoundry LocalマネージャーとOpenAIクライアントをインポートします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### 説明: モデル初期化 (SDKパターン)\n",
    "Foundry Local Python SDKを使用して、堅牢なモデル管理を実現します:\n",
    "- **FoundryLocalManager(alias)** - サービスを自動的に開始し、エイリアスでモデルをロード\n",
    "- **get_model_info(alias)** - エイリアスを具体的なモデルIDに解決\n",
    "- **manager.endpoint** - OpenAIクライアント用のサービスエンドポイントを提供\n",
    "- **manager.api_key** - APIキーを提供 (ローカル使用の場合はオプション)\n",
    "- 異なるエージェント用に別々のモデルをサポート (プライマリ vs エディター)\n",
    "- 指数バックオフを伴う組み込みのリトライロジックで耐障害性を向上\n",
    "- サービスが準備完了していることを確認する接続検証\n",
    "\n",
    "**主要なSDKパターン:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**ライフサイクル管理:**\n",
    "- マネージャーはグローバルに保存され、適切にクリーンアップされる\n",
    "- 各エージェントは専門化のために異なるモデルを使用可能\n",
    "- 自動サービス検出と接続処理\n",
    "- 障害時の指数バックオフによる優雅なリトライ\n",
    "\n",
    "これにより、エージェントのオーケストレーションが始まる前に適切な初期化が保証されます。\n",
    "\n",
    "**参考:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### 説明: Agent & Memory クラス\n",
    "軽量なメモリエントリ用の `AgentMsg` と、以下をカプセル化する `Agent` を定義します:\n",
    "- **システムロール** - エージェントの人格と指示\n",
    "- **メッセージ履歴** - 会話のコンテキストを保持\n",
    "- **act() メソッド** - 適切なエラーハンドリングでアクションを実行\n",
    "\n",
    "エージェントは異なるモデル（プライマリ vs エディター）を使用でき、エージェントごとに独立したコンテキストを維持します。このパターンにより以下が可能になります:\n",
    "- アクション間でのメモリの永続性\n",
    "- エージェントごとの柔軟なモデル割り当て\n",
    "- エラーの分離と回復\n",
    "- 簡単なチェーン化とオーケストレーション\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### 説明: オーケストレーションされたパイプライン\n",
    "2つの専門的なエージェントを作成します:\n",
    "- **リサーチャー**: 主なモデルを使用し、事実情報を収集\n",
    "- **エディター**: 別のモデルを使用可能（設定されている場合）、情報を洗練し書き直し\n",
    "\n",
    "`pipeline` 関数の流れ:\n",
    "1. リサーチャーが生の情報を収集\n",
    "2. エディターが経営層向けの完成度の高い出力に仕上げる\n",
    "3. 中間結果と最終結果の両方を返す\n",
    "\n",
    "このパターンの利点:\n",
    "- モデルの専門化（役割ごとに異なるモデルを使用可能）\n",
    "- 多段階処理による品質向上\n",
    "- 情報変換の追跡可能性\n",
    "- エージェントの追加や並列処理への簡単な拡張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### 説明: パイプラインの実行と結果\n",
    "コンプライアンスと遅延に関するテーマの質問に対して、マルチエージェントパイプラインを実行し、以下を示します:\n",
    "- 情報の多段階変換\n",
    "- エージェントの専門性と協力\n",
    "- 洗練による出力品質の向上\n",
    "- トレーサビリティ（中間出力と最終出力の両方を保持）\n",
    "\n",
    "**結果構造:**\n",
    "- `question` - ユーザーの元の質問\n",
    "- `research` - 生の調査結果（事実の箇条書き）\n",
    "- `final` - 洗練されたエグゼクティブサマリー\n",
    "- `models` - 各段階で使用されたモデル\n",
    "\n",
    "**拡張アイデア:**\n",
    "1. 品質レビューのためのCriticエージェントを追加\n",
    "2. 異なる側面に対する並列調査エージェントを実装\n",
    "3. 事実確認のためのVerifierエージェントを追加\n",
    "4. 異なる複雑さのレベルに応じて異なるモデルを使用\n",
    "5. 反復的な改善のためのフィードバックループを実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### 上級編: カスタムエージェント設定\n",
    "\n",
    "初期化セルを実行する前に環境変数を変更して、エージェントの動作をカスタマイズしてみましょう:\n",
    "\n",
    "**利用可能なモデル:**\n",
    "- ターミナルで `foundry model ls` を使用して、利用可能なすべてのモデルを確認できます\n",
    "- 例: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b など\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責事項**:  \nこの文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-08T19:43:17+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "ja"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}