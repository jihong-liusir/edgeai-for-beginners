<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T20:39:23+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "sw"
}
-->
# Sehemu ya 1: Kujifunza kwa Kina kuhusu SLM - Misingi na Uboreshaji

Small Language Models (SLMs) ni maendeleo muhimu katika EdgeAI, yanayowezesha uwezo wa hali ya juu wa usindikaji wa lugha asilia kwenye vifaa vyenye rasilimali ndogo. Kuelewa jinsi ya kupeleka, kuboresha, na kutumia SLMs kwa ufanisi ni muhimu kwa kujenga suluhisho za AI zinazotegemea makali.

## Utangulizi

Katika somo hili, tutachunguza Small Language Models (SLMs) na mikakati yao ya utekelezaji wa hali ya juu. Tutajadili dhana za msingi za SLMs, mipaka ya vigezo na uainishaji wake, mbinu za uboreshaji, na mikakati ya utekelezaji wa vitendo kwa mazingira ya kompyuta za makali.

## Malengo ya Kujifunza

Mwisho wa somo hili, utaweza:

- üî¢ Kuelewa mipaka ya vigezo na uainishaji wa Small Language Models.
- üõ†Ô∏è Kutambua mbinu muhimu za uboreshaji kwa utekelezaji wa SLM kwenye vifaa vya makali.
- üöÄ Kujifunza kutekeleza mikakati ya hali ya juu ya upunguzaji na ukandamizaji kwa SLMs.

## Kuelewa Mipaka ya Vigezo na Uainishaji wa SLM

Small Language Models (SLMs) ni mifano ya AI iliyoundwa kusindika, kuelewa, na kuzalisha maudhui ya lugha asilia kwa kutumia vigezo vichache sana ikilinganishwa na mifano mikubwa. Wakati Large Language Models (LLMs) zina vigezo vya mabilioni hadi trilioni, SLMs zimeundwa mahsusi kwa ufanisi na utekelezaji wa makali.

Mfumo wa uainishaji wa vigezo hutusaidia kuelewa makundi tofauti ya SLMs na matumizi yao yanayofaa. Uainishaji huu ni muhimu kwa kuchagua mfano sahihi kwa hali maalum za kompyuta za makali.

### Mfumo wa Uainishaji wa Vigezo

Kuelewa mipaka ya vigezo husaidia kuchagua mifano inayofaa kwa hali tofauti za kompyuta za makali:

- **üî¨ Micro SLMs**: Vigezo milioni 100 - bilioni 1.4 (nyepesi sana kwa vifaa vya rununu)
- **üì± Small SLMs**: Vigezo bilioni 1.5 - bilioni 13.9 (utendaji na ufanisi wa uwiano)
- **‚öñÔ∏è Medium SLMs**: Vigezo bilioni 14 - bilioni 30 (karibu na uwezo wa LLM huku ikihifadhi ufanisi)

Mipaka halisi bado ni ya kubadilika katika jamii ya utafiti, lakini wataalamu wengi huzingatia mifano yenye vigezo chini ya bilioni 30 kama "ndogo," huku vyanzo vingine vikibainisha kizingiti hata chini zaidi cha bilioni 10.

### Faida Muhimu za SLMs

SLMs zinatoa faida kadhaa za msingi zinazowafanya kuwa bora kwa matumizi ya kompyuta za makali:

**Ufanisi wa Uendeshaji**: SLMs hutoa nyakati za utambuzi wa haraka kutokana na vigezo vichache vya kusindika, na kuwafanya kuwa bora kwa matumizi ya wakati halisi. Zinahitaji rasilimali za chini za kompyuta, kuwezesha utekelezaji kwenye vifaa vyenye rasilimali ndogo huku zikihifadhi matumizi ya nishati na kupunguza alama ya kaboni.

**Urahisi wa Utekelezaji**: Mifano hii inawezesha uwezo wa AI kwenye kifaa bila mahitaji ya muunganisho wa mtandao, huongeza faragha na usalama kupitia usindikaji wa ndani, inaweza kubinafsishwa kwa matumizi maalum ya kikoa, na inafaa kwa mazingira mbalimbali ya kompyuta za makali.

**Ufanisi wa Gharama**: SLMs hutoa mafunzo na utekelezaji wa gharama nafuu ikilinganishwa na LLMs, na kupunguza gharama za uendeshaji na mahitaji ya bandwidth kwa matumizi ya makali.

## Mikakati ya Kupata Mifano ya Hali ya Juu

### Mfumo wa Hugging Face

Hugging Face ni kitovu kikuu cha kugundua na kufikia SLMs za hali ya juu. Jukwaa linatoa rasilimali kamili kwa ugunduzi wa mifano na utekelezaji:

**Vipengele vya Ugunduzi wa Mifano**: Jukwaa linatoa uchujaji wa hali ya juu kwa idadi ya vigezo, aina ya leseni, na vipimo vya utendaji. Watumiaji wanaweza kufikia zana za kulinganisha mifano kwa upande, viwango vya utendaji wa wakati halisi na matokeo ya tathmini, na maonyesho ya WebGPU kwa majaribio ya haraka.

**Makusanyo ya SLM Zilizochaguliwa**: Mifano maarufu ni pamoja na Phi-4-mini-3.8B kwa kazi za uamuzi wa hali ya juu, mfululizo wa Qwen3 (0.6B/1.7B/4B) kwa matumizi ya lugha nyingi, Google Gemma3 kwa kazi za jumla zenye ufanisi, na mifano ya majaribio kama BitNET kwa utekelezaji wa usahihi wa chini sana. Jukwaa pia lina makusanyo yanayoendeshwa na jamii yenye mifano maalum kwa kikoa fulani na aina zilizofunzwa awali na zilizotunzwa kwa maagizo kwa matumizi tofauti.

### Katalogi ya Mifano ya Azure AI Foundry

Katalogi ya Mifano ya Azure AI Foundry inatoa ufikiaji wa daraja la biashara kwa SLMs na uwezo wa ujumuishaji ulioboreshwa:

**Ujumuishaji wa Biashara**: Katalogi inajumuisha mifano inayouzwa moja kwa moja na Azure na msaada wa daraja la biashara na SLAs, ikiwa na Phi-4-mini-3.8B kwa uwezo wa uamuzi wa hali ya juu na Llama 3-8B kwa utekelezaji wa uzalishaji. Pia ina mifano ikiwa ni pamoja na Qwen3 8B kutoka kwa mfano wa chanzo wazi wa wahusika wa tatu wanaoaminika.

**Faida za Biashara**: Zana zilizojengwa ndani kwa ajili ya kurekebisha, ufuatiliaji, na AI inayowajibika zimeunganishwa na Provisioned Throughput inayoweza kubadilika katika familia za mifano. Msaada wa moja kwa moja wa Microsoft na SLAs za biashara, vipengele vya usalama na ufuasi vilivyopachikwa, na mifumo kamili ya utekelezaji huimarisha uzoefu wa biashara.

## Mbinu za Hali ya Juu za Upunguzaji na Uboreshaji

### Mfumo wa Uboreshaji wa Llama.cpp

Llama.cpp inatoa mbinu za upunguzaji za hali ya juu kwa ufanisi wa juu katika utekelezaji wa makali:

**Mbinu za Upunguzaji**: Mfumo huu unasaidia viwango mbalimbali vya upunguzaji ikiwa ni pamoja na Q4_0 (upunguzaji wa biti 4 na kupunguzwa kwa ukubwa bora - bora kwa utekelezaji wa rununu wa Qwen3-0.6B), Q5_1 (upunguzaji wa biti 5 unaosawazisha ubora na ukandamizaji - unaofaa kwa utambuzi wa makali wa Phi-4-mini-3.8B), na Q8_0 (upunguzaji wa biti 8 kwa ubora karibu na wa asili - unapendekezwa kwa matumizi ya uzalishaji ya Google Gemma3). BitNET inawakilisha ukingo wa hali ya juu na upunguzaji wa biti 1 kwa hali za ukandamizaji wa hali ya juu.

**Faida za Utekelezaji**: Utambuzi ulioboreshwa wa CPU na kasi ya SIMD hutoa upakiaji wa mifano kwa ufanisi wa kumbukumbu na utekelezaji. Ulinganifu wa majukwaa mbalimbali kwenye usanifu wa x86, ARM, na Apple Silicon huwezesha uwezo wa utekelezaji usioegemea vifaa.

**Mfano wa Utekelezaji wa Vitendo**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Ulinganisho wa Kumbukumbu**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Suite ya Uboreshaji ya Microsoft Olive

Microsoft Olive inatoa mifumo kamili ya uboreshaji wa mifano iliyoundwa kwa mazingira ya uzalishaji:

**Mbinu za Uboreshaji**: Suite inajumuisha upunguzaji wa nguvu kwa uteuzi wa usahihi wa kiotomatiki (hasa bora na mifano ya mfululizo wa Qwen3), uboreshaji wa grafu na fusion ya opereta (imeboreshwa kwa usanifu wa Google Gemma3), uboreshaji maalum wa vifaa kwa CPU, GPU, na NPU (na msaada maalum kwa Phi-4-mini-3.8B kwenye vifaa vya ARM), na mifumo ya uboreshaji ya hatua nyingi. Mifano ya BitNET inahitaji mifumo maalum ya upunguzaji wa biti 1 ndani ya mfumo wa Olive.

**Uendeshaji wa Mfumo**: Ulinganisho wa kiotomatiki katika anuwai za uboreshaji huhakikisha uhifadhi wa vipimo vya ubora wakati wa uboreshaji. Ujumuishaji na mifumo maarufu ya ML kama PyTorch na ONNX hutoa uwezo wa uboreshaji wa wingu na makali.

**Mfano wa Utekelezaji wa Vitendo**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Mfumo wa Apple MLX

Apple MLX inatoa uboreshaji wa asili ulioundwa mahsusi kwa vifaa vya Apple Silicon:

**Uboreshaji wa Apple Silicon**: Mfumo hutumia usanifu wa kumbukumbu uliofungamana na ujumuishaji wa Metal Performance Shaders, utambuzi wa usahihi mchanganyiko wa kiotomatiki (hasa bora na Google Gemma3), na matumizi bora ya bandwidth ya kumbukumbu. Phi-4-mini-3.8B inaonyesha utendaji wa kipekee kwenye chips za M-series, wakati Qwen3-1.7B hutoa usawa bora kwa utekelezaji wa MacBook Air.

**Vipengele vya Maendeleo**: Msaada wa API za Python na Swift na operesheni zinazolingana na safu za NumPy, uwezo wa utofautishaji wa kiotomatiki, na ujumuishaji usio na mshono na zana za maendeleo za Apple hutoa mazingira kamili ya maendeleo.

**Mfano wa Utekelezaji wa Vitendo**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Mikakati ya Utekelezaji wa Uzalishaji na Utambuzi

### Ollama: Utekelezaji Rahisi wa Ndani

Ollama hurahisisha utekelezaji wa SLM na vipengele vilivyotayarishwa kwa biashara kwa mazingira ya ndani na makali:

**Uwezo wa Utekelezaji**: Usakinishaji wa mfano kwa amri moja na utekelezaji na upakuaji wa mfano wa kiotomatiki na uhifadhi. Msaada kwa Phi-4-mini-3.8B, mfululizo mzima wa Qwen3 (0.6B/1.7B/4B), na Google Gemma3 na REST API kwa ujumuishaji wa programu na usimamizi wa mifano mingi na uwezo wa kubadilisha. Mifano ya BitNET inahitaji usanidi wa majaribio kwa msaada wa upunguzaji wa biti 1.

**Vipengele vya Hali ya Juu**: Msaada wa kurekebisha mifano maalum, kizazi cha faili za Docker kwa utekelezaji wa kontena, kasi ya GPU na ugunduzi wa kiotomatiki, na chaguo za upunguzaji wa mifano na uboreshaji hutoa urahisi kamili wa utekelezaji.

### VLLM: Utambuzi wa Utendaji wa Juu

VLLM hutoa uboreshaji wa utambuzi wa daraja la uzalishaji kwa hali za upitishaji wa juu:

**Uboreshaji wa Utendaji**: PagedAttention kwa hesabu ya umakini yenye ufanisi wa kumbukumbu (hasa yenye manufaa kwa usanifu wa transformer wa Phi-4-mini-3.8B), upakiaji wa nguvu kwa uboreshaji wa upitishaji (imeboreshwa kwa usindikaji sambamba wa mfululizo wa Qwen3), usawa wa tensor kwa upanuzi wa GPU nyingi (msaada wa Google Gemma3), na usimbaji wa majaribio kwa kupunguza ucheleweshaji. Mifano ya BitNET inahitaji kernel maalum za utambuzi kwa operesheni za biti 1.

**Ujumuishaji wa Biashara**: Vituo vya API vinavyolingana na OpenAI, msaada wa utekelezaji wa Kubernetes, ujumuishaji wa ufuatiliaji na ufuatiliaji, na uwezo wa kupanua kiotomatiki hutoa suluhisho za utekelezaji wa daraja la biashara.

### Foundry Local: Suluhisho la Makali la Microsoft

Foundry Local inatoa uwezo kamili wa utekelezaji wa makali kwa mazingira ya biashara:

**Vipengele vya Kompyuta za Makali**: Usanifu wa kwanza wa nje ya mtandao na uboreshaji wa vikwazo vya rasilimali, usimamizi wa rejista ya mifano ya ndani, na uwezo wa usawazishaji wa makali hadi wingu huhakikisha utekelezaji wa makali unaotegemewa.

**Usalama na Ufuasi**: Usindikaji wa data wa ndani kwa uhifadhi wa faragha, udhibiti wa usalama wa biashara, ukaguzi wa kumbukumbu na ripoti za ufuasi, na usimamizi wa ufikiaji unaotegemea majukumu hutoa usalama kamili kwa utekelezaji wa makali.

## Mazoea Bora kwa Utekelezaji wa SLM

### Miongozo ya Uchaguzi wa Mfano

Unapochagua SLMs kwa utekelezaji wa makali, zingatia mambo yafuatayo:

**Mahitaji ya Idadi ya Vigezo**: Chagua Micro SLMs kama Qwen3-0.6B kwa matumizi ya rununu nyepesi sana, Small SLMs kama Qwen3-1.7B au Google Gemma3 kwa hali za utendaji wa uwiano, na Medium SLMs kama Phi-4-mini-3.8B au Qwen3-4B unapokaribia uwezo wa LLM huku ukihifadhi ufanisi. Mifano ya BitNET hutoa ukandamizaji wa hali ya juu wa majaribio kwa matumizi maalum ya utafiti.

**Ulinganifu wa Matumizi**: Linganisha uwezo wa mfano na mahitaji maalum ya programu, ukizingatia mambo kama ubora wa majibu, kasi ya utambuzi, vikwazo vya kumbukumbu, na mahitaji ya operesheni ya nje ya mtandao.

### Uchaguzi wa Mkakati wa Uboreshaji

**Mbinu za Upunguzaji**: Chagua viwango vya upunguzaji vinavyofaa kulingana na mahitaji ya ubora na vikwazo vya vifaa. Zingatia Q4_0 kwa ukandamizaji wa juu (bora kwa utekelezaji wa rununu wa Qwen3-0.6B), Q5_1 kwa usawa wa ubora na ukandamizaji (unaofaa kwa Phi-4-mini-3.8B na Google Gemma3), na Q8_0 kwa uhifadhi wa ubora karibu na wa asili (unapendekezwa kwa mazingira ya uzalishaji ya Qwen3-4B). Upunguzaji wa biti 1 wa BitNET unawakilisha ukingo wa hali ya juu wa ukandamizaji kwa matumizi maalum.

**Uchaguzi wa Mfumo**: Chagua mifumo ya uboreshaji kulingana na vifaa lengwa na mahitaji ya utekelezaji. Tumia Llama.cpp kwa utekelezaji ulioboreshwa wa CPU, Microsoft Olive kwa mifumo kamili ya uboreshaji, na Apple MLX kwa vifaa vya Apple Silicon.

## Mifano ya Vitendo ya Mifano na Matumizi

### Hali za Utekelezaji wa Ulimwengu Halisi

**Programu za Rununu**: Qwen3-0.6B inang'aa katika programu za chatbot za simu za mkononi zenye kumbukumbu ndogo, wakati Google Gemma3 inatoa utendaji wa uwiano kwa zana za elimu za tablet. Phi-4-mini-3.8B hutoa uwezo wa hali ya juu wa uamuzi kwa programu za uzalishaji wa rununu.

**Kompyuta za Mezani na Makali**: Qwen3-1.7B hutoa utendaji bora kwa programu za msaidizi wa kompyuta ya mezani, Phi-4-mini-3.8B hutoa uwezo wa hali ya juu wa kizazi cha msimbo kwa zana za watengenezaji, na Qwen3-4B inawezesha uchambuzi wa hati za hali ya juu kwenye mazingira ya workstation.

**Utafiti na Majaribio**: Mifano ya BitNET inawezesha uchunguzi wa utambuzi wa usahihi wa chini sana kwa utafiti wa kitaaluma na programu za uthibitisho wa dhana zinazohitaji vikwazo vya rasilimali vya hali ya juu.

### Viwango vya Utendaji na Ulinganisho

**Kasi ya Utambuzi**: Qwen3-0.6B inapata nyakati za utambuzi wa haraka zaidi kwenye CPU za rununu, Google Gemma3 inatoa uwiano wa kasi-ubora kwa programu za jumla, Phi-4-mini-3.8B hutoa kasi ya uamuzi bora kwa kazi ngumu, na BitNET hutoa upitishaji wa juu wa kinadharia na vifaa maalum.

**Mahitaji ya Kumbukumbu**: Kumbukumbu za mifano zinatofautiana kutoka Qwen3-0.6B (chini ya 1GB ikiwa imepunguzwa) hadi Phi-4-mini-3.8B (takriban 3-4GB ikiwa imepunguzwa), huku BitNET ikifikia kumbukumbu chini ya 500MB

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.