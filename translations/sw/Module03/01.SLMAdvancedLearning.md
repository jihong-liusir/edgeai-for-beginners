<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-18T17:21:25+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "sw"
}
-->
# Sehemu ya 1: Kujifunza kwa Kina kuhusu SLM - Misingi na Uboreshaji

Small Language Models (SLMs) ni hatua muhimu katika EdgeAI, zikitoa uwezo wa hali ya juu wa kuchakata lugha asilia kwenye vifaa vyenye rasilimali ndogo. Kuelewa jinsi ya kupeleka, kuboresha, na kutumia SLMs kwa ufanisi ni muhimu kwa kujenga suluhisho za AI zinazotegemea vifaa vya pembezoni.

## Utangulizi

Katika somo hili, tutachunguza Small Language Models (SLMs) na mikakati yao ya utekelezaji wa hali ya juu. Tutajadili dhana za msingi za SLMs, mipaka ya vigezo na uainishaji wake, mbinu za uboreshaji, na mikakati ya utekelezaji wa vitendo kwa mazingira ya kompyuta ya pembezoni.

## Malengo ya Kujifunza

Mwisho wa somo hili, utaweza:

- üî¢ Kuelewa mipaka ya vigezo na uainishaji wa Small Language Models.
- üõ†Ô∏è Kutambua mbinu muhimu za uboreshaji wa SLM kwa vifaa vya pembezoni.
- üöÄ Kujifunza kutekeleza mikakati ya hali ya juu ya kupunguza ukubwa na kubana data kwa SLMs.

## Kuelewa Mipaka ya Vigezo na Uainishaji wa SLM

Small Language Models (SLMs) ni mifano ya AI iliyoundwa kuchakata, kuelewa, na kuzalisha maudhui ya lugha asilia kwa kutumia vigezo vichache sana ikilinganishwa na mifano mikubwa. Wakati Large Language Models (LLMs) zina mabilioni hadi matrilioni ya vigezo, SLMs zimeundwa mahsusi kwa ufanisi na utekelezaji wa pembezoni.

Mfumo wa uainishaji wa vigezo husaidia kuelewa makundi tofauti ya SLMs na matumizi yake yanayofaa. Uainishaji huu ni muhimu kwa kuchagua mfano sahihi kwa hali maalum za kompyuta ya pembezoni.

### Mfumo wa Uainishaji wa Vigezo

Kuelewa mipaka ya vigezo husaidia kuchagua mifano inayofaa kwa hali tofauti za kompyuta ya pembezoni:

- **üî¨ Micro SLMs**: Vigezo milioni 100 - bilioni 1.4 (nyepesi sana kwa vifaa vya rununu)
- **üì± Small SLMs**: Vigezo bilioni 1.5 - bilioni 13.9 (utendaji na ufanisi vilivyowiana)
- **‚öñÔ∏è Medium SLMs**: Vigezo bilioni 14 - bilioni 30 (karibu na uwezo wa LLM huku ikihifadhi ufanisi)

Mipaka halisi bado ni ya kubadilika katika jamii ya utafiti, lakini wataalamu wengi huchukulia mifano yenye vigezo chini ya bilioni 30 kama "ndogo," huku vyanzo vingine vikishusha kizingiti hadi bilioni 10.

### Faida Muhimu za SLMs

SLMs zina faida kadhaa za msingi zinazowafanya kuwa bora kwa matumizi ya kompyuta ya pembezoni:

**Ufanisi wa Uendeshaji**: SLMs hutoa nyakati za uchakataji haraka kutokana na vigezo vichache vya kuchakata, na kuzifanya kuwa bora kwa matumizi ya wakati halisi. Zinahitaji rasilimali ndogo za kompyuta, hivyo kuwezesha utekelezaji kwenye vifaa vyenye rasilimali ndogo huku zikihifadhi matumizi ya nishati na kupunguza alama ya kaboni.

**Urahisi wa Utekelezaji**: Mifano hii inawezesha uwezo wa AI kwenye kifaa bila hitaji la muunganisho wa mtandao, huimarisha faragha na usalama kupitia uchakataji wa ndani, inaweza kubinafsishwa kwa matumizi maalum ya kikoa, na inafaa kwa mazingira mbalimbali ya kompyuta ya pembezoni.

**Gharama Nafuu**: SLMs hutoa mafunzo na utekelezaji wa gharama nafuu ikilinganishwa na LLMs, na kupunguza gharama za uendeshaji na mahitaji ya bandwidth kwa matumizi ya pembezoni.

## Mikakati ya Kupata Mifano ya Hali ya Juu

### Mfumo wa Hugging Face

Hugging Face ni kitovu kikuu cha kugundua na kufikia SLMs za hali ya juu. Jukwaa hili linatoa rasilimali kamili kwa ugunduzi wa mifano na utekelezaji:

**Vipengele vya Ugunduzi wa Mifano**: Jukwaa linatoa uchujaji wa hali ya juu kwa idadi ya vigezo, aina ya leseni, na vipimo vya utendaji. Watumiaji wanaweza kufikia zana za kulinganisha mifano kwa upande, viwango vya utendaji wa wakati halisi na matokeo ya tathmini, na maonyesho ya WebGPU kwa majaribio ya haraka.

**Makusanyo ya SLM Zilizochaguliwa**: Mifano maarufu ni pamoja na Phi-4-mini-3.8B kwa kazi za kufikiri za hali ya juu, mfululizo wa Qwen3 (0.6B/1.7B/4B) kwa matumizi ya lugha nyingi, Google Gemma3 kwa kazi za jumla zenye ufanisi, na mifano ya majaribio kama BitNET kwa utekelezaji wa usahihi wa chini sana. Jukwaa pia lina makusanyo yanayoendeshwa na jamii yenye mifano maalum kwa kikoa fulani na aina zilizofunzwa awali na zilizotunzwa kwa maelekezo kwa matumizi tofauti.

### Katalogi ya Mifano ya Azure AI Foundry

Katalogi ya Mifano ya Azure AI Foundry inatoa ufikiaji wa daraja la biashara kwa SLMs na uwezo wa ujumuishaji ulioboreshwa:

**Ujumuishaji wa Biashara**: Katalogi inajumuisha mifano inayouzwa moja kwa moja na Azure yenye msaada wa daraja la biashara na SLAs, ikiwa ni pamoja na Phi-4-mini-3.8B kwa uwezo wa kufikiri wa hali ya juu na Llama 3-8B kwa utekelezaji wa uzalishaji. Pia inajumuisha mifano kama Qwen3 8B kutoka kwa mifano ya chanzo wazi ya wahusika wa tatu wanaoaminika.

**Faida za Biashara**: Zana zilizojengwa ndani kwa ajili ya kurekebisha, ufuatiliaji, na AI inayowajibika zimeunganishwa na Provisioned Throughput inayoweza kubadilika kati ya familia za mifano. Msaada wa moja kwa moja wa Microsoft na SLAs za biashara, vipengele vya usalama na uzingatiaji vilivyopachikwa, na mifumo kamili ya utekelezaji huimarisha uzoefu wa biashara.

## Mbinu za Hali ya Juu za Kupunguza Ukubwa na Uboreshaji

### Mfumo wa Uboreshaji wa Llama.cpp

Llama.cpp inatoa mbinu za hali ya juu za kupunguza ukubwa kwa ufanisi wa juu katika utekelezaji wa pembezoni:

**Mbinu za Kupunguza Ukubwa**: Mfumo huu unasaidia viwango mbalimbali vya kupunguza ukubwa ikiwa ni pamoja na Q4_0 (kupunguza ukubwa kwa biti 4 - bora kwa utekelezaji wa Qwen3-0.6B kwenye rununu), Q5_1 (kupunguza ukubwa kwa biti 5 ikilinganishwa na ubora - inafaa kwa Phi-4-mini-3.8B), na Q8_0 (kupunguza ukubwa kwa biti 8 kwa ubora karibu na asili - inapendekezwa kwa Google Gemma3). BitNET inawakilisha kiwango cha juu zaidi na kupunguza ukubwa kwa biti 1 kwa hali za ukandamizaji wa hali ya juu.

**Faida za Utekelezaji**: Uchakataji ulioboreshwa wa CPU na kasi ya SIMD hutoa upakiaji wa mifano kwa ufanisi wa kumbukumbu. Ulinganifu wa majukwaa mbalimbali kwenye usanifu wa x86, ARM, na Apple Silicon huwezesha uwezo wa utekelezaji bila kujali vifaa.

**Mfano wa Utekelezaji wa Vitendo**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Ulinganisho wa Kumbukumbu**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Suite ya Uboreshaji ya Microsoft Olive

Microsoft Olive inatoa mifumo kamili ya uboreshaji wa mifano iliyoundwa kwa mazingira ya uzalishaji:

**Mbinu za Uboreshaji**: Suite hii inajumuisha kupunguza ukubwa kwa njia ya nguvu kwa uteuzi wa usahihi wa kiotomatiki (hasa bora kwa mifano ya mfululizo wa Qwen3), uboreshaji wa grafu na muunganisho wa waendeshaji (imeboreshwa kwa usanifu wa Google Gemma3), uboreshaji maalum wa vifaa kwa CPU, GPU, na NPU (na msaada maalum kwa Phi-4-mini-3.8B kwenye vifaa vya ARM), na mifumo ya uboreshaji ya hatua nyingi. Mifano ya BitNET inahitaji mifumo maalum ya kupunguza ukubwa kwa biti 1 ndani ya mfumo wa Olive.

**Uendeshaji wa Kiotomatiki**: Ulinganisho wa kiotomatiki wa viwango vya uboreshaji unahakikisha uhifadhi wa vipimo vya ubora wakati wa uboreshaji. Ujumuishaji na mifumo maarufu ya ML kama PyTorch na ONNX hutoa uwezo wa uboreshaji wa utekelezaji wa wingu na pembezoni.

**Mfano wa Utekelezaji wa Vitendo**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Mfumo wa Apple MLX

Apple MLX inatoa uboreshaji wa asili ulioundwa mahsusi kwa vifaa vya Apple Silicon:

**Uboreshaji wa Apple Silicon**: Mfumo huu hutumia usanifu wa kumbukumbu uliofungamanishwa na ujumuishaji wa Metal Performance Shaders, uchakataji wa usahihi mchanganyiko wa kiotomatiki (hasa bora kwa Google Gemma3), na matumizi bora ya bandwidth ya kumbukumbu. Phi-4-mini-3.8B inaonyesha utendaji wa kipekee kwenye chips za M-series, huku Qwen3-1.7B ikitoa usawa bora kwa utekelezaji wa MacBook Air.

**Vipengele vya Maendeleo**: Msaada wa API za Python na Swift na operesheni zinazolingana na NumPy, uwezo wa utofautishaji wa kiotomatiki, na ujumuishaji wa moja kwa moja na zana za maendeleo za Apple hutoa mazingira kamili ya maendeleo.

**Mfano wa Utekelezaji wa Vitendo**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Mikakati ya Utekelezaji wa Uzalishaji na Uchakataji

### Ollama: Utekelezaji Rahisi wa Kwenye Kifaa

Ollama hurahisisha utekelezaji wa SLMs na vipengele vya daraja la biashara kwa mazingira ya ndani na pembezoni:

**Uwezo wa Utekelezaji**: Usakinishaji wa mifano kwa amri moja na uchakataji wa kiotomatiki wa mifano. Msaada kwa Phi-4-mini-3.8B, mfululizo mzima wa Qwen3 (0.6B/1.7B/4B), na Google Gemma3 na REST API kwa ujumuishaji wa programu na usimamizi wa mifano mingi. Mifano ya BitNET inahitaji usanidi wa majaribio kwa msaada wa kupunguza ukubwa kwa biti 1.

**Vipengele vya Hali ya Juu**: Msaada wa kurekebisha mifano maalum, kizazi cha faili za Docker kwa utekelezaji wa kontena, kasi ya GPU na ugunduzi wa kiotomatiki, na chaguzi za kupunguza ukubwa na uboreshaji wa mifano hutoa urahisi wa utekelezaji.

### VLLM: Uchakataji wa Utendaji wa Juu

VLLM hutoa uboreshaji wa uchakataji wa daraja la uzalishaji kwa hali za uchakataji wa kiwango cha juu:

**Uboreshaji wa Utendaji**: PagedAttention kwa uchakataji wa kumbukumbu kwa ufanisi (hasa muhimu kwa usanifu wa transformer wa Phi-4-mini-3.8B), upakiaji wa nguvu kwa uboreshaji wa kiwango cha uchakataji (imeboreshwa kwa mfululizo wa Qwen3), ushirikiano wa tensor kwa upanuzi wa GPU nyingi (msaada wa Google Gemma3), na uchakataji wa majaribio kwa kupunguza muda wa kusubiri. Mifano ya BitNET inahitaji kernel maalum za uchakataji kwa operesheni za biti 1.

**Ujumuishaji wa Biashara**: Vituo vya API vinavyolingana na OpenAI, msaada wa utekelezaji wa Kubernetes, ujumuishaji wa ufuatiliaji na ufuatiliaji, na uwezo wa upanuzi wa kiotomatiki hutoa suluhisho za utekelezaji wa daraja la biashara.

### Foundry Local: Suluhisho la Microsoft kwa Pembezoni

Foundry Local inatoa uwezo kamili wa utekelezaji wa pembezoni kwa mazingira ya biashara:

**Vipengele vya Kompyuta ya Pembezoni**: Muundo wa usanifu wa kwanza wa nje ya mtandao na uboreshaji wa rasilimali ndogo, usimamizi wa rejista ya mifano ya ndani, na uwezo wa usawazishaji wa pembezoni hadi wingu huhakikisha utekelezaji wa pembezoni unaotegemewa.

**Usalama na Uzingatiaji**: Uchakataji wa data wa ndani kwa uhifadhi wa faragha, udhibiti wa usalama wa biashara, ukaguzi wa kumbukumbu na ripoti za uzingatiaji, na usimamizi wa ufikiaji kulingana na majukumu hutoa usalama kamili kwa utekelezaji wa pembezoni.

## Mazoea Bora kwa Utekelezaji wa SLM

### Miongozo ya Uchaguzi wa Mifano

Wakati wa kuchagua SLMs kwa utekelezaji wa pembezoni, zingatia mambo yafuatayo:

**Mambo ya Idadi ya Vigezo**: Chagua Micro SLMs kama Qwen3-0.6B kwa matumizi ya rununu yenye uzito mdogo, Small SLMs kama Qwen3-1.7B au Google Gemma3 kwa hali za utendaji wa uwiano, na Medium SLMs kama Phi-4-mini-3.8B au Qwen3-4B unapokaribia uwezo wa LLM huku ukihifadhi ufanisi. Mifano ya BitNET hutoa ukandamizaji wa hali ya juu kwa majaribio maalum ya utafiti.

**Ulinganifu wa Matumizi**: Linganisha uwezo wa mifano na mahitaji maalum ya programu, ukizingatia mambo kama ubora wa majibu, kasi ya uchakataji, vikwazo vya kumbukumbu, na mahitaji ya operesheni ya nje ya mtandao.

### Uchaguzi wa Mikakati ya Uboreshaji

**Mbinu za Kupunguza Ukubwa**: Chagua viwango sahihi vya kupunguza ukubwa kulingana na mahitaji ya ubora na vikwazo vya vifaa. Zingatia Q4_0 kwa ukandamizaji wa juu (bora kwa utekelezaji wa Qwen3-0.6B kwenye rununu), Q5_1 kwa uwiano wa ubora na ukandamizaji (inayofaa kwa Phi-4-mini-3.8B na Google Gemma3), na Q8_0 kwa uhifadhi wa ubora karibu na asili (inapendekezwa kwa mazingira ya uzalishaji ya Qwen3-4B). Kupunguza ukubwa kwa biti 1 kwa BitNET kunawakilisha kiwango cha juu cha ukandamizaji kwa matumizi maalum.

**Uchaguzi wa Mfumo**: Chagua mifumo ya uboreshaji kulingana na vifaa lengwa na mahitaji ya utekelezaji. Tumia Llama.cpp kwa utekelezaji ulioboreshwa wa CPU, Microsoft Olive kwa mifumo kamili ya uboreshaji, na Apple MLX kwa vifaa vya Apple Silicon.

## Mifano ya Vitendo ya Mifano na Matumizi

### Hali Halisi za Utekelezaji

**Programu za Rununu**: Qwen3-0.6B inang'ara katika programu za mazungumzo kwenye simu za mkononi zenye kumbukumbu ndogo, huku Google Gemma3 ikitoa utendaji wa uwiano kwa zana za elimu kwenye vidonge. Phi-4-mini-3.8B inatoa uwezo wa kufikiri wa hali ya juu kwa programu za uzalishaji kwenye rununu.

**Kompyuta ya Mezani na Pembezoni**: Qwen3-1.7B hutoa utendaji bora kwa programu za msaidizi wa kompyuta ya mezani, Phi-4-mini-3.8B inatoa uwezo wa hali ya juu wa kizazi cha msimbo kwa zana za watengenezaji, na Qwen3-4B inawezesha uchambuzi wa hati za hali ya juu kwenye mazingira ya kazi.

**Utafiti na Majaribio**: Mifano ya BitNET inawezesha uchunguzi wa uchakataji wa usahihi wa chini sana kwa utafiti wa kitaaluma na majaribio ya dhana yanayohitaji vikwazo vya rasilimali vya hali ya juu.

### Viwango vya Utendaji na Ulinganisho

**Kasi ya Uchakataji**: Qwen3-0.6B inapata nyakati za uchakataji haraka zaidi kwenye CPU za rununu, Google Gemma3 inatoa uwiano wa kasi na ubora kwa matumizi ya jumla, Phi-4-mini-3.8B inatoa kasi ya kufikiri ya hali ya juu kwa kazi ngumu, na BitNET hutoa kiwango cha juu cha uchakataji kinadharia na vifaa maalum.

**Mahitaji ya Kumbukumbu**: Kumbukumbu ya mifano inatofautiana kutoka Qwen3-0.6B (chini ya 1GB ikiwa imebanwa) hadi Phi-4-mini-3.8B (takriban 3-4GB ikiwa imebanwa), huku BitNET ikifikia kumbukumbu chini ya 500MB katika usanidi wa majaribio.

## Changamoto na Masuala ya Kuzingatia

###

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.