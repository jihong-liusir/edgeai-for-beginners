<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T17:24:30+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "sw"
}
-->
# Utekelezaji wa Wingu kwa Njia ya Kontena - Suluhisho za Kiwango cha Uzalishaji

Mafunzo haya ya kina yanashughulikia mbinu kuu tatu za kupeleka modeli ya Phi-4-mini-instruct ya Microsoft katika mazingira ya kontena: vLLM, Ollama, na SLM Engine na ONNX Runtime. Modeli hii yenye vigezo bilioni 3.8 inawakilisha chaguo bora kwa kazi za kufikiria huku ikidumisha ufanisi kwa utekelezaji wa ukingoni.

## Jedwali la Maudhui

1. [Utangulizi wa Utekelezaji wa Kontena la Phi-4-mini](../../../Module03)
2. [Malengo ya Kujifunza](../../../Module03)
3. [Kuelewa Uainishaji wa Phi-4-mini](../../../Module03)
4. [Utekelezaji wa Kontena la vLLM](../../../Module03)
5. [Utekelezaji wa Kontena la Ollama](../../../Module03)
6. [SLM Engine na ONNX Runtime](../../../Module03)
7. [Mfumo wa Ulinganisho](../../../Module03)
8. [Mbinu Bora](../../../Module03)

## Utangulizi wa Utekelezaji wa Kontena la Phi-4-mini

Small Language Models (SLMs) ni maendeleo muhimu katika EdgeAI, yanayowezesha uwezo wa hali ya juu wa usindikaji wa lugha asilia kwenye vifaa vyenye rasilimali ndogo. Mafunzo haya yanazingatia mikakati ya utekelezaji wa kontena kwa Phi-4-mini-instruct ya Microsoft, modeli ya hali ya juu ya kufikiria inayosawazisha uwezo na ufanisi.

### Modeli Iliyoangaziwa: Phi-4-mini-instruct

**Phi-4-mini-instruct (vigezo bilioni 3.8)**: Modeli nyepesi ya Microsoft iliyotunzwa kwa maelekezo, iliyoundwa kwa mazingira yenye kumbukumbu/hesabu ndogo, yenye uwezo wa kipekee katika:
- **Kufikiria kihisabati na hesabu ngumu**
- **Uzalishaji wa msimbo, urekebishaji, na uchambuzi**
- **Kutatua matatizo ya kimantiki na kufikiria hatua kwa hatua**
- **Matumizi ya kielimu yanayohitaji maelezo ya kina**
- **Kuita kazi na kuunganisha zana**

Sehemu ya kitengo cha "Small SLMs" (vigezo bilioni 1.5 - 13.9), Phi-4-mini inasawazisha kwa njia bora kati ya uwezo wa kufikiria na ufanisi wa rasilimali.

### Faida za Utekelezaji wa Kontena la Phi-4-mini

- **Ufanisi wa Kazi**: Utoaji wa haraka wa majibu kwa kazi za kufikiria kwa mahitaji ya chini ya hesabu
- **Uwezo wa Kubadilika**: Uwezo wa AI kwenye kifaa na faragha iliyoboreshwa kupitia usindikaji wa ndani
- **Ufanisi wa Gharama**: Kupunguza gharama za uendeshaji ikilinganishwa na modeli kubwa huku ukidumisha ubora
- **Kutengwa**: Utengano safi kati ya matukio ya modeli na mazingira salama ya utekelezaji
- **Uwezo wa Kupanuka**: Upanuzi wa usawa kwa urahisi ili kuongeza uwezo wa kufikiria

## Malengo ya Kujifunza

Mwisho wa mafunzo haya, utaweza:

- Kuweka na kuboresha Phi-4-mini-instruct katika mazingira mbalimbali ya kontena
- Kutekeleza mikakati ya hali ya juu ya upunguzaji na ukandamizaji kwa hali tofauti za utekelezaji
- Kuseti uratibu wa kontena ulio tayari kwa uzalishaji kwa kazi za kufikiria
- Kutathmini na kuchagua mifumo sahihi ya utekelezaji kulingana na mahitaji maalum ya matumizi
- Kutumia mbinu bora za usalama, ufuatiliaji, na upanuzi kwa utekelezaji wa kontena la SLM

## Kuelewa Uainishaji wa Phi-4-mini

### Maelezo ya Modeli

**Maelezo ya Kiufundi:**
- **Vigezo**: Bilioni 3.8 (Kitengo cha Small SLM)
- **Muundo**: Transformer ya msimbuaji pekee yenye umakini wa maswali yaliyopangwa
- **Urefu wa Muktadha**: Tokeni 128K (Tokeni 32K zinapendekezwa kwa utendaji bora)
- **Msamiati**: Tokeni 200K na msaada wa lugha nyingi
- **Data ya Mafunzo**: Tokeni trilioni 5 za maudhui yenye msisitizo wa kufikiria

### Mahitaji ya Rasilimali

| Aina ya Utekelezaji | RAM ya Chini | RAM Inayopendekezwa | VRAM (GPU) | Hifadhi | Matumizi ya Kawaida |
|---------------------|--------------|---------------------|------------|---------|---------------------|
| **Maendeleo** | 6GB | 8GB | - | 8GB | Upimaji wa ndani, utengenezaji wa mfano |
| **Uzalishaji CPU** | 8GB | 12GB | - | 10GB | Seva za ukingoni, utekelezaji wa gharama nafuu |
| **Uzalishaji GPU** | 6GB | 8GB | 4-6GB | 8GB | Huduma za kufikiria zenye kasi |
| **Ukingo Ulioboreshwa** | 4GB | 6GB | - | 6GB | Utekelezaji uliopunguzwa, milango ya IoT |

### Uwezo wa Phi-4-mini

- **Ubora wa Hisabati**: Kutatua matatizo ya hesabu, algebra, na calculus
- **Ujuzi wa Msimbo**: Uzalishaji wa msimbo wa Python, JavaScript, na lugha nyingi na urekebishaji
- **Kufikiria Kimantiki**: Kugawanya matatizo hatua kwa hatua na kujenga suluhisho
- **Msaada wa Kielimu**: Maelezo ya kina yanayofaa kwa kujifunza na kufundisha
- **Kuita Kazi**: Msaada wa asili kwa kuunganisha zana na mwingiliano wa API

## Utekelezaji wa Kontena la vLLM

vLLM hutoa msaada bora kwa Phi-4-mini-instruct na utendaji wa utoaji wa majibu ulioboreshwa na API zinazolingana na OpenAI, na kuifanya kuwa bora kwa huduma za kufikiria za uzalishaji.

### Mifano ya Kuanza Haraka

#### Utekelezaji wa CPU wa Kawaida (Maendeleo)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Utekelezaji wa GPU wa Uzalishaji
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Usanidi wa Uzalishaji

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Kupima Uwezo wa Kufikiria wa Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Utekelezaji wa Kontena la Ollama

Ollama hutoa msaada bora kwa Phi-4-mini-instruct na utekelezaji rahisi na usimamizi, na kuifanya kuwa bora kwa maendeleo na utekelezaji wa uzalishaji uliosawazishwa.

### Usanidi wa Haraka

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Usanidi wa Uzalishaji

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Uboreshaji wa Modeli na Tofauti

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Mifano ya Matumizi ya API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine na ONNX Runtime

ONNX Runtime hutoa utendaji bora kwa utekelezaji wa ukingoni wa Phi-4-mini-instruct na uboreshaji wa hali ya juu na utangamano wa majukwaa mbalimbali.

### Usanidi wa Msingi

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Utekelezaji Rahisi wa Seva

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script ya Kubadilisha Modeli

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Usanidi wa Uzalishaji

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Kupima Utekelezaji wa ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Mfumo wa Ulinganisho

### Ulinganisho wa Mfumo kwa Phi-4-mini

| Kipengele | vLLM | Ollama | ONNX Runtime |
|-----------|------|--------|--------------|
| **Ugumu wa Usanidi** | Wastani | Rahisi | Mgumu |
| **Utendaji (GPU)** | Bora (~25 tok/s) | Nzuri Sana (~20 tok/s) | Nzuri (~15 tok/s) |
| **Utendaji (CPU)** | Nzuri (~8 tok/s) | Nzuri Sana (~12 tok/s) | Bora (~15 tok/s) |
| **Matumizi ya Kumbukumbu** | 8-12GB | 6-10GB | 4-8GB |
| **Utangamano wa API** | Inayolingana na OpenAI | REST ya Kawaida | FastAPI ya Kawaida |
| **Kuita Kazi** | ✅ Asili | ✅ Inasaidiwa | ⚠️ Utekelezaji wa Kawaida |
| **Msaada wa Upunguzaji** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Upunguzaji wa ONNX |
| **Uzalishaji Tayari** | ✅ Bora | ✅ Nzuri Sana | ✅ Nzuri |
| **Utekelezaji wa Ukingoni** | Nzuri | Bora Sana | Bora Zaidi |

## Rasilimali za Ziada

### Nyaraka Rasmi
- **Kadi ya Modeli ya Phi-4 ya Microsoft**: Maelezo ya kina na miongozo ya matumizi
- **Nyaraka za vLLM**: Chaguo za usanidi wa hali ya juu na uboreshaji
- **Maktaba ya Modeli ya Ollama**: Modeli za jamii na mifano ya ubinafsishaji
- **Miongozo ya ONNX Runtime**: Mikakati ya uboreshaji wa utendaji na utekelezaji

### Zana za Maendeleo
- **Hugging Face Transformers**: Kwa mwingiliano wa modeli na ubinafsishaji
- **Maelezo ya API ya OpenAI**: Kwa upimaji wa utangamano wa vLLM
- **Mbinu Bora za Docker**: Usalama wa kontena na miongozo ya uboreshaji
- **Utekelezaji wa Kubernetes**: Mifumo ya uratibu kwa upanuzi wa uzalishaji

### Rasilimali za Kujifunza
- **Ulinganisho wa Utendaji wa SLM**: Mbinu za uchambuzi wa kulinganisha
- **Utekelezaji wa Edge AI**: Mbinu bora kwa mazingira yenye rasilimali ndogo
- **Uboreshaji wa Kazi za Kufikiria**: Mikakati ya kuunda maswali kwa matatizo ya hisabati na kimantiki
- **Usalama wa Kontena**: Mbinu za kuimarisha utekelezaji wa modeli za AI

## Matokeo ya Kujifunza

Baada ya kukamilisha moduli hii, utaweza:

1. Kuweka modeli ya Phi-4-mini-instruct katika mazingira ya kontena kwa kutumia mifumo mbalimbali
2. Kuseti na kuboresha utekelezaji wa SLM kwa mazingira tofauti ya vifaa
3. Kutekeleza mbinu bora za usalama kwa utekelezaji wa AI wa kontena
4. Kulinganisha na kuchagua mifumo sahihi ya utekelezaji kulingana na mahitaji maalum ya matumizi
5. Kutumia mikakati ya ufuatiliaji na upanuzi kwa huduma za SLM za kiwango cha uzalishaji

## Nini Kinachofuata

- Rudi kwa [Moduli ya 1](../Module01/README.md)
- Rudi kwa [Moduli ya 2](../Module02/README.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.