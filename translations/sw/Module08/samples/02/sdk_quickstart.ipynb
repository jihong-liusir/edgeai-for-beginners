{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9261c3ed",
   "metadata": {},
   "source": [
    "# Mfano 02: Ujumuishaji wa OpenAI SDK\n",
    "\n",
    "Notebook hii inaonyesha ujumuishaji wa hali ya juu na OpenAI Python SDK, ikihusisha Microsoft Foundry Local na Azure OpenAI kwa majibu ya mtiririko na usimamizi sahihi wa makosa.\n",
    "\n",
    "## Muhtasari\n",
    "\n",
    "Mfano huu unaonyesha:\n",
    "- Kubadilisha kwa urahisi kati ya Foundry Local na Azure OpenAI\n",
    "- Majibu ya mazungumzo ya mtiririko kwa uzoefu bora wa mtumiaji\n",
    "- Matumizi sahihi ya FoundryLocalManager SDK\n",
    "- Usimamizi thabiti wa makosa na mifumo ya kurudi nyuma\n",
    "- Mifumo ya msimbo inayofaa kwa uzalishaji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5c157",
   "metadata": {},
   "source": [
    "## Mahitaji ya Awali\n",
    "\n",
    "- **Foundry Local**: Imewekwa na inafanya kazi (kwa utabiri wa ndani)\n",
    "- **Python**: Toleo la 3.8 au jipya zaidi lenye OpenAI SDK\n",
    "- **Azure OpenAI**: Endpoint halali na API key (kwa utabiri wa wingu)\n",
    "\n",
    "### Sakinisha Vitegemezi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efed1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai foundry-local-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945a8a6",
   "metadata": {},
   "source": [
    "## Ingiza Maktaba na Usanidi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "try:\n",
    "    from foundry_local import FoundryLocalManager\n",
    "    FOUNDRY_SDK_AVAILABLE = True\n",
    "    print(\"✅ Foundry Local SDK is available\")\n",
    "except ImportError:\n",
    "    FOUNDRY_SDK_AVAILABLE = False\n",
    "    print(\"⚠️ Foundry Local SDK not available, manual configuration will be used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bcd9f",
   "metadata": {},
   "source": [
    "## Chaguo za Usanidi\n",
    "\n",
    "Chagua kati ya Azure OpenAI (wingu) au Foundry Local (kwenye kifaa) kwa kuweka vigezo sahihi vya mazingira.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ebbb",
   "metadata": {},
   "source": [
    "### Chaguo la 1: Usanidi wa Azure OpenAI\n",
    "\n",
    "Ondoa maoni na weka maelezo yako ya Azure OpenAI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e76294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "# Uncomment and set your actual values\n",
    "\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource.openai.azure.com\"\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "# os.environ[\"MODEL\"] = \"your-deployment-name\"  # e.g., \"gpt-4\"\n",
    "\n",
    "print(\"Azure OpenAI configuration ready (if credentials are set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89431f",
   "metadata": {},
   "source": [
    "### Chaguo la 2: Usanidi wa Ndani wa Foundry\n",
    "\n",
    "Mipangilio ya chaguo-msingi kwa uchambuzi wa ndani:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fe391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundry Local Configuration (default)\n",
    "FOUNDRY_MODEL = \"phi-4-mini\"  # Change to your preferred model\n",
    "FOUNDRY_BASE_URL = \"http://localhost:51211\"\n",
    "FOUNDRY_API_KEY = \"\"  # Usually empty for local\n",
    "\n",
    "print(f\"Foundry Local configuration ready with model: {FOUNDRY_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269662a5",
   "metadata": {},
   "source": [
    "## Kazi za Kiwanda cha Wateja\n",
    "\n",
    "Kazi hizi huunda mteja sahihi wa OpenAI kulingana na usanidi wako:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a65349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_client() -> Tuple[OpenAI, str]:\n",
    "    \"\"\"Create Azure OpenAI client.\"\"\"\n",
    "    azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    azure_api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
    "    \n",
    "    if not azure_endpoint or not azure_api_key:\n",
    "        raise ValueError(\"Azure OpenAI endpoint and API key are required\")\n",
    "    \n",
    "    model = os.environ.get(\"MODEL\", \"your-deployment-name\")\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{azure_endpoint}/openai\",\n",
    "        api_key=azure_api_key,\n",
    "        default_query={\"api-version\": azure_api_version},\n",
    "    )\n",
    "    \n",
    "    print(f\"🌐 Azure OpenAI client created with model: {model}\")\n",
    "    return client, model\n",
    "\n",
    "\n",
    "def create_foundry_client() -> Tuple[OpenAI, str]:\n",
    "    \"\"\"Create Foundry Local client with SDK management.\"\"\"\n",
    "    alias = FOUNDRY_MODEL\n",
    "    \n",
    "    if FOUNDRY_SDK_AVAILABLE:\n",
    "        try:\n",
    "            # Use FoundryLocalManager for proper service management\n",
    "            print(f\"🔄 Initializing Foundry Local with model: {alias}...\")\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            \n",
    "            # Configure OpenAI client to use local Foundry service\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key  # API key is not required for local usage\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Foundry Local SDK initialized\")\n",
    "            print(f\"   Endpoint: {manager.endpoint}\")\n",
    "            print(f\"   Model: {model_info.id}\")\n",
    "            return client, model_info.id\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not use Foundry SDK ({e}), falling back to manual configuration\")\n",
    "    \n",
    "    # Fallback to manual configuration\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{FOUNDRY_BASE_URL}/v1\",\n",
    "        api_key=FOUNDRY_API_KEY\n",
    "    )\n",
    "    \n",
    "    print(f\"🔧 Manual Foundry Local configuration\")\n",
    "    print(f\"   Endpoint: {FOUNDRY_BASE_URL}/v1\")\n",
    "    print(f\"   Model: {alias}\")\n",
    "    return client, alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a884e4b",
   "metadata": {},
   "source": [
    "## Anzisha Mteja\n",
    "\n",
    "Hii inatambua kiotomatiki ikiwa utumie Azure OpenAI au Foundry Local:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_client() -> Tuple[OpenAI, str, str]:\n",
    "    \"\"\"Initialize the appropriate OpenAI client.\"\"\"\n",
    "    \n",
    "    # Check for Azure OpenAI configuration\n",
    "    azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    \n",
    "    if azure_endpoint and azure_api_key:\n",
    "        print(\"🌐 Azure OpenAI configuration detected\")\n",
    "        try:\n",
    "            client, model = create_azure_client()\n",
    "            return client, model, \"azure\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Azure OpenAI initialization failed: {e}\")\n",
    "            print(\"🔄 Falling back to Foundry Local...\")\n",
    "    \n",
    "    # Use Foundry Local\n",
    "    print(\"🏠 Using Foundry Local configuration\")\n",
    "    try:\n",
    "        client, model = create_foundry_client()\n",
    "        return client, model, \"foundry\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Foundry Local initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize the client\n",
    "print(\"Initializing OpenAI client...\")\n",
    "print(\"=\" * 50)\n",
    "client, model, provider = initialize_client()\n",
    "print(\"=\" * 50)\n",
    "print(f\"✅ Initialization complete! Using {provider} with model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85447b",
   "metadata": {},
   "source": [
    "## Kukamilisha Mazungumzo ya Msingi\n",
    "\n",
    "Jaribu kukamilisha mazungumzo rahisi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(prompt: str, max_tokens: int = 150) -> str:\n",
    "    \"\"\"Send a simple chat message and get response.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test basic chat\n",
    "test_prompt = \"Say hello from the SDK quickstart and explain what you are in one sentence.\"\n",
    "\n",
    "print(f\"👤 User: {test_prompt}\")\n",
    "print(\"\\n🤖 Assistant:\")\n",
    "response = simple_chat(test_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75fe31",
   "metadata": {},
   "source": [
    "## Kutiririsha Majibu ya Gumzo\n",
    "\n",
    "Onyesha jinsi ya kutiririsha majibu kwa uzoefu bora wa mtumiaji:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_chat(prompt: str, max_tokens: int = 300) -> str:\n",
    "    \"\"\"Send a chat message with streaming response.\"\"\"\n",
    "    try:\n",
    "        print(\"🤖 Assistant (streaming):\")\n",
    "        \n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "        \n",
    "        print(\"\\n\")  # New line after streaming\n",
    "        return full_response\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test streaming chat\n",
    "streaming_prompt = \"Explain the key benefits of using Microsoft Foundry Local for AI development. Include aspects like privacy, performance, and cost.\"\n",
    "\n",
    "print(f\"👤 User: {streaming_prompt}\\n\")\n",
    "streaming_response = streaming_chat(streaming_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c517e2",
   "metadata": {},
   "source": [
    "## Mazungumzo ya Zamu Nyingi\n",
    "\n",
    "Onyesha jinsi ya kudumisha muktadha wa mazungumzo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a738a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    \"\"\"Manages multi-turn conversations with context.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None):\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def send_message(self, user_message: str, max_tokens: int = 200) -> str:\n",
    "        \"\"\"Send a message and get response while maintaining context.\"\"\"\n",
    "        # Add user message to conversation\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=self.messages,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            assistant_message = response.choices[0].message.content\n",
    "            \n",
    "            # Add assistant response to conversation\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "            \n",
    "            return assistant_message\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def get_conversation_length(self) -> int:\n",
    "        \"\"\"Get the number of messages in the conversation.\"\"\"\n",
    "        return len(self.messages)\n",
    "\n",
    "# Create conversation manager with system prompt\n",
    "system_prompt = \"You are a helpful AI assistant specialized in explaining AI and machine learning concepts. Be concise but informative.\"\n",
    "conversation = ConversationManager(system_prompt)\n",
    "\n",
    "# Multi-turn conversation example\n",
    "conversation_turns = [\n",
    "    \"What is the difference between AI inference on-device vs in the cloud?\",\n",
    "    \"Which approach is better for privacy?\",\n",
    "    \"What about performance and latency considerations?\"\n",
    "]\n",
    "\n",
    "for i, turn in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"👤 User: {turn}\")\n",
    "    \n",
    "    response = conversation.send_message(turn)\n",
    "    print(f\"\\n🤖 Assistant: {response}\")\n",
    "\n",
    "print(f\"\\n📊 Conversation summary: {conversation.get_conversation_length()} messages total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576e583",
   "metadata": {},
   "source": [
    "## Ulinganisho wa Utendaji\n",
    "\n",
    "Linganisha muda wa majibu kwa hali tofauti:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_response_time(prompt: str, iterations: int = 3) -> dict:\n",
    "    \"\"\"Benchmark response time for a given prompt.\"\"\"\n",
    "    times = []\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=50  # Keep responses short for timing\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            times.append(response_time)\n",
    "            responses.append(response.choices[0].message.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {i+1}: {e}\")\n",
    "    \n",
    "    if times:\n",
    "        avg_time = sum(times) / len(times)\n",
    "        min_time = min(times)\n",
    "        max_time = max(times)\n",
    "        \n",
    "        return {\n",
    "            \"average_time\": avg_time,\n",
    "            \"min_time\": min_time,\n",
    "            \"max_time\": max_time,\n",
    "            \"all_times\": times,\n",
    "            \"sample_response\": responses[0] if responses else None\n",
    "        }\n",
    "    \n",
    "    return {\"error\": \"No successful responses\"}\n",
    "\n",
    "# Benchmark different types of prompts\n",
    "benchmark_prompts = [\n",
    "    \"What is AI?\",\n",
    "    \"Explain machine learning in simple terms.\",\n",
    "    \"List 3 benefits of edge computing.\"\n",
    "]\n",
    "\n",
    "print(f\"⏱️  Performance Benchmark ({provider} - {model})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in benchmark_prompts:\n",
    "    print(f\"\\n📝 Prompt: '{prompt}'\")\n",
    "    results = benchmark_response_time(prompt)\n",
    "    \n",
    "    if \"error\" not in results:\n",
    "        print(f\"   ⏰ Average time: {results['average_time']:.2f}s\")\n",
    "        print(f\"   ⚡ Fastest: {results['min_time']:.2f}s\")\n",
    "        print(f\"   🐌 Slowest: {results['max_time']:.2f}s\")\n",
    "        print(f\"   📄 Sample response: {results['sample_response'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"   ❌ {results['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461b399",
   "metadata": {},
   "source": [
    "## Usanidi wa Juu na Ushughulikiaji wa Makosa\n",
    "\n",
    "Jaribu vigezo tofauti na hali za makosa:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_different_parameters():\n",
    "    \"\"\"Test chat completions with different parameters.\"\"\"\n",
    "    prompt = \"Write a creative short story about AI.\"\n",
    "    \n",
    "    # Test different temperature values\n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n🌡️ Temperature: {temp}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            print(f\"Response: {response.choices[0].message.content[:150]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with temperature {temp}: {e}\")\n",
    "\n",
    "test_different_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9954a02",
   "metadata": {},
   "source": [
    "## Ukaguzi wa Afya ya Huduma\n",
    "\n",
    "Ukaguzi wa kina wa afya na uwezo wa huduma:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_health_check():\n",
    "    \"\"\"Perform comprehensive health check of the service.\"\"\"\n",
    "    print(\"🏥 Comprehensive Health Check\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Check model listing\n",
    "    try:\n",
    "        models_response = client.models.list()\n",
    "        available_models = [m.id for m in models_response.data]\n",
    "        print(f\"✅ Model listing: SUCCESS\")\n",
    "        print(f\"   📋 Available models: {available_models}\")\n",
    "        \n",
    "        if model in available_models:\n",
    "            print(f\"   ✅ Current model '{model}' is available\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Current model '{model}' not found in available models\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model listing: FAILED - {e}\")\n",
    "    \n",
    "    # 2. Test basic completion\n",
    "    try:\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'Health check successful'\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f\"✅ Basic completion: SUCCESS\")\n",
    "        print(f\"   💬 Response: {test_response.choices[0].message.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Basic completion: FAILED - {e}\")\n",
    "    \n",
    "    # 3. Test streaming\n",
    "    try:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Count to 3\"}],\n",
    "            max_tokens=20,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        stream_content = \"\"\n",
    "        chunk_count = 0\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                stream_content += chunk.choices[0].delta.content\n",
    "                chunk_count += 1\n",
    "        \n",
    "        print(f\"✅ Streaming: SUCCESS\")\n",
    "        print(f\"   📦 Chunks received: {chunk_count}\")\n",
    "        print(f\"   💬 Streamed content: {stream_content.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Streaming: FAILED - {e}\")\n",
    "    \n",
    "    # 4. Provider-specific information\n",
    "    print(f\"\\n📊 Configuration Summary:\")\n",
    "    print(f\"   🏢 Provider: {provider}\")\n",
    "    print(f\"   🤖 Model: {model}\")\n",
    "    if provider == \"foundry\":\n",
    "        print(f\"   🏠 Foundry SDK Available: {FOUNDRY_SDK_AVAILABLE}\")\n",
    "        print(f\"   🔗 Base URL: {FOUNDRY_BASE_URL}\")\n",
    "    elif provider == \"azure\":\n",
    "        print(f\"   🌐 Azure Endpoint: {os.environ.get('AZURE_OPENAI_ENDPOINT', 'Not set')}\")\n",
    "        print(f\"   🔑 API Version: {os.environ.get('AZURE_OPENAI_API_VERSION', 'Not set')}\")\n",
    "\n",
    "comprehensive_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3d47b",
   "metadata": {},
   "source": [
    "## Upimaji wa Kimaingiliano\n",
    "\n",
    "Tumia seli hii kujaribu maelekezo yako mwenyewe kwa njia ya kimaingiliano:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing - modify the prompt below\n",
    "custom_prompt = \"Explain the concept of 'edge AI' and why it's becoming important.\"\n",
    "use_streaming = True  # Set to False for regular completion\n",
    "\n",
    "print(f\"👤 Custom Prompt: {custom_prompt}\\n\")\n",
    "\n",
    "if use_streaming:\n",
    "    custom_response = streaming_chat(custom_prompt, max_tokens=250)\n",
    "else:\n",
    "    custom_response = simple_chat(custom_prompt, max_tokens=250)\n",
    "    print(f\"🤖 Assistant: {custom_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae41525",
   "metadata": {},
   "source": [
    "## Muhtasari na Hatua Zifuatazo\n",
    "\n",
    "Notebook hii ilionyesha ujumuishaji wa hali ya juu wa OpenAI SDK na:\n",
    "\n",
    "### ✅ Vipengele Muhimu Vilivyofunikwa\n",
    "\n",
    "1. **Msaada wa Watoa Huduma Wengi**: Kubadilisha kwa urahisi kati ya Azure OpenAI na Foundry Local\n",
    "2. **Majibu ya Kutiririka**: Uzalishaji wa tokeni kwa wakati halisi kwa UX bora\n",
    "3. **Usimamizi wa Mazungumzo**: Mazungumzo ya mizunguko mingi yenye muktadha\n",
    "4. **Upimaji wa Utendaji**: Kipimo na uchambuzi wa muda wa majibu\n",
    "5. **Ukaguzi wa Afya wa Kina**: Uthibitishaji wa huduma na uchunguzi\n",
    "6. **Ushughulikiaji wa Makosa**: Ushughulikiaji wa makosa wenye nguvu na mifumo ya mbadala\n",
    "\n",
    "### 🏆 Foundry Local vs Azure OpenAI\n",
    "\n",
    "| Kipengele | Foundry Local | Azure OpenAI |\n",
    "|-----------|---------------|--------------|\n",
    "| **Faragha** | ✅ Data inabaki ndani | ⚠️ Data inatumwa kwa wingu |\n",
    "| **Muda wa Kusubiri** | ✅ Chini (utambuzi wa ndani) | ⚠️ Juu (kutegemea mtandao) |\n",
    "| **Gharama** | ✅ Bila malipo (baada ya vifaa) | 💰 Kulipia kwa tokeni |\n",
    "| **Nje ya Mtandao** | ✅ Inafanya kazi bila mtandao | ❌ Inahitaji mtandao |\n",
    "| **Aina ya Modeli** | ⚠️ Uchaguzi mdogo | ✅ Ufikiaji kamili wa modeli |\n",
    "| **Upanuzi** | ⚠️ Kutegemea vifaa | ✅ Upanuzi usio na kikomo |\n",
    "\n",
    "### 🚀 Hatua Zifuatazo\n",
    "\n",
    "- **Mfano 04**: Kujenga programu ya mazungumzo ya Chainlit\n",
    "- **Mfano 05**: Mifumo ya uratibu wa mawakala wengi\n",
    "- **Mfano 06**: Uelekezaji wa modeli wenye akili\n",
    "- **Utekelezaji wa Uzalishaji**: Masuala ya upanuzi na ufuatiliaji\n",
    "\n",
    "### 💡 Mazoea Bora\n",
    "\n",
    "1. **Daima tekeleza mifumo ya mbadala** kati ya watoa huduma\n",
    "2. **Tumia kutiririka kwa majibu marefu** ili kuboresha utendaji unaoonekana\n",
    "3. **Tekeleza ushughulikiaji sahihi wa makosa** kwa programu za uzalishaji\n",
    "4. **Fuatilia muda wa majibu na gharama** kwa watoa huduma tofauti\n",
    "5. **Chagua mtoa huduma sahihi** kulingana na mahitaji yako maalum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "08b8e1c177cf88f6a4f3025003c50830",
   "translation_date": "2025-09-25T03:51:54+00:00",
   "source_file": "Module08/samples/02/sdk_quickstart.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}