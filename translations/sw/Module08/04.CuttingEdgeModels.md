<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T01:09:04+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "sw"
}
-->
# Kipindi cha 4: Kujenga Programu za Gumzo za Uzalishaji kwa Chainlit

## Muhtasari

Kipindi hiki kinazingatia kujenga programu za gumzo zinazofaa kwa uzalishaji kwa kutumia Chainlit na Microsoft Foundry Local. Utajifunza kuunda miingiliano ya kisasa ya wavuti kwa mazungumzo ya AI, kutekeleza majibu ya mtiririko, na kupeleka programu za gumzo zenye nguvu na utunzaji sahihi wa makosa na muundo wa uzoefu wa mtumiaji.

**Unachojenga:**
- **Programu ya Gumzo ya Chainlit**: UI ya kisasa ya wavuti na majibu ya mtiririko
- **Demo ya WebGPU**: Utoaji wa maelezo kwenye kivinjari kwa programu zinazojali faragha  
- **Ujumuishaji wa Open WebUI**: Kiolesura cha kitaalamu cha gumzo na Foundry Local
- **Mifumo ya Uzalishaji**: Utunzaji wa makosa, ufuatiliaji, na mikakati ya kupeleka

## Malengo ya Kujifunza

- Kujenga programu za gumzo zinazofaa kwa uzalishaji kwa Chainlit
- Kutekeleza majibu ya mtiririko kwa uzoefu bora wa mtumiaji
- Kumiliki mifumo ya ujumuishaji ya Foundry Local SDK
- Kutumia utunzaji sahihi wa makosa na kupunguza athari kwa neema
- Kupeleka na kusanidi programu za gumzo kwa mazingira tofauti
- Kuelewa mifumo ya kisasa ya UI ya wavuti kwa AI ya mazungumzo

## Mahitaji ya Awali

- **Foundry Local**: Imewekwa na inafanya kazi ([Mwongozo wa Usakinishaji](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 au zaidi na uwezo wa mazingira ya kawaida
- **Modeli**: Angalau modeli moja imepakiwa (`foundry model run phi-4-mini`)
- **Kivinjari**: Kivinjari cha kisasa cha wavuti chenye msaada wa WebGPU (Chrome/Edge)
- **Docker**: Kwa ujumuishaji wa Open WebUI (hiari)

## Sehemu ya 1: Kuelewa Programu za Kisasa za Gumzo

### Muhtasari wa Muundo

```
User Browser ‚Üê‚Üí Chainlit UI ‚Üê‚Üí Python Backend ‚Üê‚Üí Foundry Local ‚Üê‚Üí AI Model
      ‚Üì              ‚Üì              ‚Üì              ‚Üì            ‚Üì
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Teknolojia Muhimu

**Mifumo ya Foundry Local SDK:**
- `FoundryLocalManager(alias)`: Usimamizi wa huduma otomatiki
- `manager.endpoint` na `manager.api_key`: Maelezo ya muunganisho
- `manager.get_model_info(alias).id`: Utambulisho wa modeli

**Mfumo wa Chainlit:**
- `@cl.on_chat_start`: Kuanzisha vikao vya gumzo
- `@cl.on_message`: Kushughulikia ujumbe wa mtumiaji unaoingia  
- `cl.Message().stream_token()`: Mtiririko wa wakati halisi
- Uundaji wa UI otomatiki na usimamizi wa WebSocket

## Sehemu ya 2: Uamuzi wa Kati ya Local na Cloud

### Tabia za Utendaji

| Kipengele | Local (Foundry) | Cloud (Azure OpenAI) |
|-----------|-----------------|---------------------|
| **Latency** | üöÄ 50-200ms (hakuna mtandao) | ‚è±Ô∏è 200-2000ms (inategemea mtandao) |
| **Faragha** | üîí Data haiondoki kwenye kifaa | ‚ö†Ô∏è Data inatumwa kwa wingu |
| **Gharama** | üí∞ Bila malipo baada ya vifaa | üí∏ Lipa kwa tokeni |
| **Offline** | ‚úÖ Inafanya kazi bila mtandao | ‚ùå Inahitaji mtandao |
| **Ukubwa wa Modeli** | ‚ö†Ô∏è Inategemea vifaa | ‚úÖ Ufikiaji wa modeli kubwa zaidi |
| **Scaling** | ‚ö†Ô∏è Inategemea vifaa | ‚úÖ Scaling isiyo na kikomo |

### Mifumo ya Mkakati wa Mseto

**Local-First na Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Uelekezaji wa Kazi:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Sehemu ya 3: Sampuli 04 - Programu ya Gumzo ya Chainlit

### Mwanzo wa Haraka

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Programu inafunguka kiotomatiki kwenye `http://localhost:8080` na kiolesura cha kisasa cha gumzo.

### Utekelezaji wa Msingi

Programu ya Sampuli 04 inaonyesha mifumo inayofaa kwa uzalishaji:

**Ugunduzi wa Huduma Otomatiki:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Kishughulikia Gumzo cha Mtiririko:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Chaguo za Usanidi

**Vigezo vya Mazingira:**

| Kigezo | Maelezo | Chaguo-msingi | Mfano |
|--------|---------|--------------|-------|
| `MODEL` | Alias ya modeli ya kutumia | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Endpoint ya Foundry Local | Inatambuliwa kiotomatiki | `http://localhost:51211` |
| `API_KEY` | API key (hiari kwa local) | `""` | `your-api-key` |

**Matumizi ya Juu:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Sehemu ya 4: Kuunda na Kutumia Jupyter Notebooks

### Muhtasari wa Msaada wa Notebook

Sampuli 04 inajumuisha notebook ya Jupyter (`chainlit_app.ipynb`) yenye:

- **üìö Maudhui ya Elimu**: Vifaa vya kujifunza hatua kwa hatua
- **üî¨ Uchunguzi wa Kuingiliana**: Kuendesha na kujaribu seli za msimbo
- **üìä Maonyesho ya Visual**: Chati, michoro, na taswira ya matokeo
- **üõ†Ô∏è Zana za Maendeleo**: Kujaribu na kutatua matatizo

### Kuunda Notebook Zako

#### Hatua ya 1: Sanidi Mazingira ya Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Hatua ya 2: Unda Notebook Mpya

**Kutumia VS Code:**
1. Fungua VS Code kwenye saraka ya Module08
2. Unda faili mpya yenye kiendelezi `.ipynb`
3. Chagua kernel ya "Foundry Local" unapoulizwa
4. Anza kuongeza seli na maudhui yako

**Kutumia Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Mifumo Bora ya Muundo wa Notebook

#### Mpangilio wa Seli

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("‚úÖ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Mifano ya Kuingiliana na Mazoezi

#### Zoezi 1: Kupima Usanidi wa Mteja

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nüß™ Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'‚úÖ Success' if result['status'] == 'ok' else '‚ùå Failed'}")
```

#### Zoezi 2: Simulizi ya Majibu ya Mtiririko

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("üåä Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n‚úÖ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Sehemu ya 5: Demo ya Utoaji wa Maelezo ya WebGPU Kwenye Kivinjari

### Muhtasari

WebGPU inawezesha kuendesha modeli za AI moja kwa moja kwenye kivinjari kwa faragha ya juu na uzoefu wa bila usakinishaji. Sampuli hii inaonyesha ONNX Runtime Web na utekelezaji wa WebGPU.

### Hatua ya 1: Angalia Msaada wa WebGPU

**Mahitaji ya Kivinjari:**
- Chrome/Edge 113+ yenye WebGPU imewezeshwa
- Angalia: `chrome://gpu` ‚Üí thibitisha hali ya "WebGPU"
- Angalia kwa programu: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Hatua ya 2: Unda Demo ya WebGPU

Unda saraka: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>üöÄ WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '‚ùå WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'üîç WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('‚úÖ ONNX Runtime session created with WebGPU');
        log(`üìä Input names: ${session.inputNames.join(', ')}`);
        log(`üìä Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '‚úÖ WebGPU inference complete!';
        log(`üéØ Predicted class: ${maxIdx}`);
        log(`üìà Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `‚ùå Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Hatua ya 3: Endesha Demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Sehemu ya 6: Ujumuishaji wa Open WebUI

### Muhtasari

Open WebUI hutoa kiolesura cha kitaalamu kama ChatGPT kinachounganisha na API inayolingana ya OpenAI ya Foundry Local.

### Hatua ya 1: Mahitaji ya Awali

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Hatua ya 2: Usanidi wa Docker (Inapendekezwa)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Kumbuka:** `host.docker.internal` inaruhusu kontena za Docker kufikia mashine ya mwenyeji kwenye Windows.

### Hatua ya 3: Usanidi

1. **Fungua Kivinjari:** Tembelea `http://localhost:3000`
2. **Usanidi wa Awali:** Unda akaunti ya msimamizi
3. **Usanidi wa Modeli:**
   - Mipangilio ‚Üí Modeli ‚Üí OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (thamani yoyote inafanya kazi)
4. **Jaribu Muunganisho:** Modeli zinapaswa kuonekana kwenye dropdown

### Kutatua Matatizo

**Masuala ya Kawaida:**

1. **Muunganisho Umekataliwa:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modeli Hazionekani:**
   - Thibitisha modeli imepakiwa: `foundry model list`
   - Angalia majibu ya API: `curl http://localhost:51211/v1/models`
   - Washa tena kontena la Open WebUI

## Sehemu ya 7: Mazingatio ya Utekelezaji wa Uzalishaji

### Usanidi wa Mazingira

**Usanidi wa Maendeleo:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Utekelezaji wa Uzalishaji:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Masuala ya Kawaida ya Bandari na Suluhisho

**Kuzuia Migogoro ya Bandari 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Ufuatiliaji wa Utendaji

**Utekelezaji wa Ukaguzi wa Afya:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Muhtasari

Kipindi cha 4 kilifunika kujenga programu za Chainlit zinazofaa kwa uzalishaji kwa AI ya mazungumzo. Umejifunza kuhusu:

- ‚úÖ **Mfumo wa Chainlit**: UI ya kisasa na msaada wa mtiririko kwa programu za gumzo
- ‚úÖ **Ujumuishaji wa Foundry Local**: Matumizi ya SDK na mifumo ya usanidi  
- ‚úÖ **Utoaji wa Maelezo ya WebGPU**: AI ya kivinjari kwa faragha ya juu
- ‚úÖ **Usanidi wa Open WebUI**: Utekelezaji wa kiolesura cha kitaalamu cha gumzo
- ‚úÖ **Mifumo ya Uzalishaji**: Utunzaji wa makosa, ufuatiliaji, na scaling

Programu ya Sampuli 04 inaonyesha mifumo bora kwa kujenga miingiliano ya gumzo yenye nguvu inayotumia modeli za AI za ndani kupitia Microsoft Foundry Local huku ikitoa uzoefu bora wa mtumiaji.

## Marejeleo

- **[Sampuli 04: Programu ya Chainlit](samples/04/README.md)**: Programu kamili na nyaraka
- **[Notebook ya Elimu ya Chainlit](samples/04/chainlit_app.ipynb)**: Vifaa vya kujifunza vya kuingiliana
- **[Nyaraka za Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Nyaraka kamili za jukwaa
- **[Nyaraka za Chainlit](https://docs.chainlit.io/)**: Nyaraka rasmi za mfumo
- **[Mwongozo wa Ujumuishaji wa Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Mafunzo rasmi

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.