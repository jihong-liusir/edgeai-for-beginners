<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-23T01:11:27+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "sw"
}
-->
# Kipindi cha 4: Miundo ya Kisasa – LLMs, SLMs, na Utoaji wa Uamuzi Kifaa

## Muhtasari

Linganisheni LLMs na SLMs, tathmini faida na hasara za utoaji wa uamuzi wa ndani dhidi ya wingu, na tekeleza maonyesho yanayoonyesha hali za EdgeAI kwa kutumia Phi na ONNX Runtime. Pia tutazungumzia Chainlit RAG, chaguo za utoaji wa uamuzi kwa WebGPU, na ujumuishaji wa Open WebUI.

Marejeleo:
- Foundry Local docs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI how-to (chat app with Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Malengo ya Kujifunza
- Elewa faida na hasara za LLM dhidi ya SLM kwa gharama, muda wa majibu, na usahihi
- Chagua kati ya utoaji wa uamuzi wa ndani na wingu kulingana na mahitaji maalum ya biashara
- Tekeleza maonyesho madogo ya RAG kwa kutumia Chainlit
- Chunguza WebGPU kwa kuongeza kasi upande wa kivinjari
- Unganisha Open WebUI na Foundry Local

## Sehemu ya 1: LLM vs SLM – Matriz ya Uamuzi

Fikiria:
- Muda wa majibu: SLMs kwenye kifaa mara nyingi hutoa majibu ya haraka chini ya sekunde moja
- Gharama: utoaji wa uamuzi wa ndani hupunguza gharama za wingu
- Faragha: data nyeti inabaki kwenye kifaa
- Uwezo: LLMs zinaweza kuzidi SLMs katika kazi ngumu
- Uaminifu: mikakati mseto hupunguza hatari ya muda wa kutopatikana

## Sehemu ya 2: Utoaji wa Uamuzi wa Ndani dhidi ya Wingu – Mifumo Mseto

- Kwanza ndani na wingu kama mbadala kwa maelezo makubwa/mazito
- Kwanza wingu na ndani kwa hali nyeti za faragha au zisizo na mtandao
- Elekeza kulingana na aina ya kazi (code-gen kwa DeepSeek, mazungumzo ya jumla kwa Phi/Qwen)

## Sehemu ya 3: RAG Chat App na Chainlit (Rahisi)

Sakinisha utegemezi:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Endesha:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Panua: ongeza mtafutaji rahisi (mafaili ya ndani) na ongeza muktadha uliopatikana kwenye maelezo ya mtumiaji.

## Sehemu ya 4: Utoaji wa Uamuzi kwa WebGPU (Tahadhari)

Endesha miundo midogo moja kwa moja kwenye kivinjari kwa kutumia WebGPU. Hii ni bora kwa maonyesho yanayozingatia faragha na uzoefu usiohitaji usakinishaji. Hapa chini kuna mfano rahisi, hatua kwa hatua kwa kutumia ONNX Runtime Web na mtoa huduma wa utekelezaji wa WebGPU.

1) Angalia msaada wa WebGPU
- Kivinjari cha Chromium: chrome://gpu → thibitisha “WebGPU” imewezeshwa
- Angalia kwa mpangilio (tutathibitisha pia kwenye msimbo): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) Unda mradi rahisi
Unda folda na faili mbili: `index.html` na `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Hudumia ndani (Windows cmd.exe)
Tumia seva rahisi ya faili tuli ili kivinjari kiweze kupata mfano.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Fungua http://localhost:5173 kwenye kivinjari chako. Unapaswa kuona magogo ya kuanzisha, uundaji wa kikao na WebGPU, na utabiri wa argmax.

4) Utatuzi wa matatizo
- Ikiwa WebGPU haipatikani: sasisha Chrome/Edge na hakikisha madereva ya GPU yako yamesasishwa, kisha angalia chrome://flags kwa “Enable WebGPU”.
- Ikiwa kuna makosa ya CORS au fetch: hakikisha unahudumia faili kupitia http:// (sio file://) na URL ya mfano inaruhusu maombi ya msalaba.
- Rudisha kwa CPU: badilisha `executionProviders: ['wasm']` ili kuthibitisha tabia ya msingi.

5) Hatua zinazofuata
- Badilisha na mfano maalum wa ONNX (mfano, uainishaji wa picha au mfano mdogo wa maandishi).
- Ongeza mantiki ya usindikaji wa awali/baada ya usindikaji kwa pembejeo halisi.
- Kwa miundo mikubwa au muda wa uzalishaji, pendelea Foundry Local au ONNX Runtime Server.

## Sehemu ya 5: Open WebUI + Foundry Local (Hatua kwa hatua)

Hii inaunganisha Open WebUI na mwisho wa Foundry Local unaoendana na OpenAI kwa UI ya mazungumzo ya ndani.

1) Mahitaji ya awali
- Foundry Local imewekwa na inafanya kazi (`foundry --version`)
- Mfano mmoja uko tayari kuendeshwa ndani (mfano, `phi-4-mini`)
- Docker Desktop imewekwa (inapendekezwa kwa Open WebUI)

2) Anzisha mfano na Foundry Local
```powershell
foundry model run phi-4-mini
```
Hii inatoa API inayofanana na OpenAI kwenye `http://localhost:8000`.

3) Anzisha Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Maelezo:
- Kwenye Windows, `host.docker.internal` inaruhusu kontena kufikia mwenyeji wako kwenye `localhost`.
- Tumeweka `OPENAI_API_BASE_URL` kwa mwisho wa Foundry Local na `OPENAI_API_KEY` ya mfano.

4) Sanidi kutoka kwa UI ya Open WebUI (mbadala)
- Tembelea http://localhost:3000
- Kamilisha usanidi wa awali (mtumiaji mkuu)
- Nenda kwa Settings → Models/Providers
- Weka Base URL: `http://host.docker.internal:8000/v1`
- Weka API Key: `local-key` (mfano)
- Hifadhi

5) Endesha maelezo ya majaribio
- Katika mazungumzo ya Open WebUI, chagua au ingiza jina la mfano `phi-4-mini`
- Maelezo: “Orodhesha faida tano za utoaji wa uamuzi wa AI kwenye kifaa.”
- Unapaswa kuona majibu yanayotiririka kutoka kwa mfano wako wa ndani

6) Utatuzi wa matatizo
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Hiari: Hifadhi data ya Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Orodha ya Vitendo
- [ ] Linganisha majibu/muda wa majibu kati ya SLM na LLM ndani
- [ ] Endesha maonyesho ya Chainlit dhidi ya angalau miundo miwili
- [ ] Unganisha Open WebUI na mwisho wako wa ndani na ujaribu

## Hatua Zifuatazo
- Jiandae kwa mtiririko wa wakala katika Kipindi cha 5
- Tambua hali ambapo mseto wa ndani/wingu unaboreshwa ROI

---

