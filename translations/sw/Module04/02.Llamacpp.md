<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T17:01:53+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "sw"
}
-->
# Sehemu ya 2: Mwongozo wa Utekelezaji wa Llama.cpp

## Jedwali la Maudhui
1. [Utangulizi](../../../Module04)
2. [Llama.cpp ni nini?](../../../Module04)
3. [Usakinishaji](../../../Module04)
4. [Kujenga kutoka Chanzo](../../../Module04)
5. [Ubadilishaji wa Mfano](../../../Module04)
6. [Matumizi ya Msingi](../../../Module04)
7. [Vipengele vya Juu](../../../Module04)
8. [Ujumuishaji wa Python](../../../Module04)
9. [Utatuzi wa Matatizo](../../../Module04)
10. [Mbinu Bora](../../../Module04)

## Utangulizi

Mwongozo huu wa kina utakuelekeza kila kitu unachohitaji kujua kuhusu Llama.cpp, kuanzia usakinishaji wa msingi hadi hali za matumizi ya juu. Llama.cpp ni utekelezaji wenye nguvu wa C++ unaowezesha utabiri wa mifano mikubwa ya lugha (LLMs) kwa ufanisi, kwa usanidi mdogo na utendaji bora kwenye usanifu mbalimbali wa vifaa.

## Llama.cpp ni nini?

Llama.cpp ni mfumo wa utabiri wa LLM ulioandikwa kwa C/C++ unaowezesha kuendesha mifano mikubwa ya lugha kwa ndani kwa usanidi mdogo na utendaji wa hali ya juu kwenye vifaa mbalimbali. Vipengele muhimu ni pamoja na:

### Vipengele Muhimu
- **Utekelezaji wa C/C++ wa kawaida** bila utegemezi
- **Ulinganifu wa majukwaa mbalimbali** (Windows, macOS, Linux)
- **Uboreshaji wa vifaa** kwa usanifu tofauti
- **Msaada wa ubadilishaji** (1.5-bit hadi 8-bit integer quantization)
- **Uharakishaji wa CPU na GPU**
- **Ufanisi wa kumbukumbu** kwa mazingira yenye vikwazo

### Faida
- Inaendesha kwa ufanisi kwenye CPU bila kuhitaji vifaa maalum
- Inasaidia nyuma za GPU nyingi (CUDA, Metal, OpenCL, Vulkan)
- Nyepesi na inayoweza kubebeka
- Apple silicon ni kipaumbele - imeboreshwa kupitia ARM NEON, Accelerate na Metal frameworks
- Inasaidia viwango mbalimbali vya ubadilishaji kwa matumizi ya kumbukumbu yaliyopunguzwa

## Usakinishaji

### Njia ya 1: Faili Zilizojengwa Tayari (Inapendekezwa kwa Wanaoanza)

#### Pakua kutoka GitHub Releases
1. Tembelea [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Pakua faili inayofaa kwa mfumo wako:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` kwa Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` kwa macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` kwa Linux

3. Fungua jalada na ongeza saraka kwenye PATH ya mfumo wako

#### Kutumia Wasimamizi wa Pakiti

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Usambazaji mbalimbali):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Njia ya 2: Pakiti ya Python (llama-cpp-python)

#### Usakinishaji wa Msingi
```bash
pip install llama-cpp-python
```

#### Na Uharakishaji wa Vifaa
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Kujenga kutoka Chanzo

### Mahitaji

**Mahitaji ya Mfumo:**
- Kisakinishi cha C++ (GCC, Clang, au MSVC)
- CMake (toleo la 3.14 au zaidi)
- Git
- Zana za ujenzi kwa jukwaa lako

**Usakinishaji wa Mahitaji:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Sakinisha Visual Studio 2022 na zana za maendeleo ya C++
- Sakinisha CMake kutoka tovuti rasmi
- Sakinisha Git

### Mchakato wa Msingi wa Ujenzi

1. **Kloni hifadhi:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Sanidi ujenzi:**
```bash
cmake -B build
```

3. **Jenga mradi:**
```bash
cmake --build build --config Release
```

Kwa kasi ya juu ya ujenzi, tumia kazi sambamba:
```bash
cmake --build build --config Release -j 8
```

### Ujenzi Maalum wa Vifaa

#### Msaada wa CUDA (GPU za NVIDIA)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Msaada wa Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Msaada wa OpenBLAS (Uboreshaji wa CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Msaada wa Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Chaguo za Juu za Ujenzi

#### Ujenzi wa Debug
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Na Vipengele vya Ziada
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Ubadilishaji wa Mfano

### Kuelewa Muundo wa GGUF

GGUF (Generalized GGML Unified Format) ni muundo wa faili ulioboreshwa ulioundwa kwa ajili ya kuendesha mifano mikubwa ya lugha kwa ufanisi kwa kutumia Llama.cpp na mifumo mingine. Inatoa:

- Uhifadhi wa uzito wa mfano uliosawazishwa
- Ulinganifu ulioboreshwa kwenye majukwaa
- Utendaji ulioboreshwa
- Ushughulikiaji wa metadata kwa ufanisi

### Aina za Ubadilishaji

Llama.cpp inasaidia viwango mbalimbali vya ubadilishaji:

| Aina | Bits | Maelezo | Matumizi |
|------|------|-------------|----------|
| F16 | 16 | Usahihi wa nusu | Ubora wa juu, kumbukumbu kubwa |
| Q8_0 | 8 | Ubadilishaji wa 8-bit | Mizani nzuri |
| Q4_0 | 4 | Ubadilishaji wa 4-bit | Ubora wa wastani, saizi ndogo |
| Q2_K | 2 | Ubadilishaji wa 2-bit | Saizi ndogo zaidi, ubora wa chini |

### Kubadilisha Mifano

#### Kutoka PyTorch hadi GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Pakua Moja kwa Moja kutoka Hugging Face
Mifano mingi inapatikana katika muundo wa GGUF kwenye Hugging Face:
- Tafuta mifano yenye "GGUF" kwenye jina
- Pakua kiwango cha ubadilishaji kinachofaa
- Tumia moja kwa moja na llama.cpp

## Matumizi ya Msingi

### Kiolesura cha Amri

#### Uzalishaji Rahisi wa Maandishi
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Kutumia Mifano kutoka Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Hali ya Seva
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Vigezo vya Kawaida

| Kigezo | Maelezo | Mfano |
|-----------|-------------|---------|
| `-m` | Njia ya faili ya mfano | `-m model.gguf` |
| `-p` | Maandishi ya mwongozo | `-p "Hello world"` |
| `-n` | Idadi ya tokeni za kuzalisha | `-n 100` |
| `-c` | Ukubwa wa muktadha | `-c 4096` |
| `-t` | Idadi ya nyuzi | `-t 8` |
| `-ngl` | Tabaka za GPU | `-ngl 32` |
| `-temp` | Joto | `-temp 0.7` |

### Hali ya Maingiliano

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Vipengele vya Juu

### API ya Seva

#### Kuanza Seva
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Matumizi ya API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Uboreshaji wa Utendaji

#### Usimamizi wa Kumbukumbu
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Kazi Sambamba
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Uharakishaji wa GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Ujumuishaji wa Python

### Matumizi ya Msingi na llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Kiolesura cha Gumzo

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Majibu ya Kutiririka

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Ujumuishaji na LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Utatuzi wa Matatizo

### Masuala ya Kawaida na Suluhisho

#### Makosa ya Ujenzi

**Tatizo: CMake haijapatikana**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Tatizo: Kisakinishi hakijapatikana**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Masuala ya Wakati wa Uendeshaji

**Tatizo: Upakiaji wa mfano unashindwa**
- Hakikisha njia ya faili ya mfano
- Angalia ruhusa za faili
- Hakikisha RAM ya kutosha
- Jaribu viwango tofauti vya ubadilishaji

**Tatizo: Utendaji duni**
- Washa uharakishaji wa vifaa
- Ongeza idadi ya nyuzi
- Tumia ubadilishaji unaofaa
- Angalia matumizi ya kumbukumbu ya GPU

#### Masuala ya Kumbukumbu

**Tatizo: Kumbukumbu imejaa**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Masuala Maalum ya Jukwaa

#### Windows
- Tumia kisakinishi cha MinGW au Visual Studio
- Hakikisha usanidi sahihi wa PATH
- Angalia usumbufu wa antivirus

#### macOS
- Washa Metal kwa Apple Silicon
- Tumia Rosetta 2 kwa ulinganifu ikiwa inahitajika
- Angalia zana za mstari wa amri za Xcode

#### Linux
- Sakinisha pakiti za maendeleo
- Angalia matoleo ya dereva za GPU
- Hakikisha usakinishaji wa toolkit ya CUDA

## Mbinu Bora

### Uchaguzi wa Mfano
1. **Chagua ubadilishaji unaofaa** kulingana na vifaa vyako
2. **Fikiria saizi ya mfano** dhidi ya ubora
3. **Jaribu mifano tofauti** kwa matumizi yako maalum

### Uboreshaji wa Utendaji
1. **Tumia uharakishaji wa GPU** inapopatikana
2. **Boresha idadi ya nyuzi** kwa CPU yako
3. **Weka ukubwa wa muktadha unaofaa** kwa matumizi yako
4. **Washa ramani ya kumbukumbu** kwa mifano mikubwa

### Utekelezaji wa Uzalishaji
1. **Tumia hali ya seva** kwa ufikiaji wa API
2. **Tekeleza usimamizi sahihi wa makosa**
3. **Fuatilia matumizi ya rasilimali**
4. **Sanidi ufuatiliaji na ukaguzi**

### Mtiririko wa Kazi wa Maendeleo
1. **Anza na mifano midogo** kwa majaribio
2. **Tumia udhibiti wa toleo** kwa usanidi wa mifano
3. **Andika nyaraka za usanidi wako**
4. **Jaribu kwenye majukwaa tofauti**

### Masuala ya Usalama
1. **Thibitisha mwongozo wa pembejeo**
2. **Tekeleza mipaka ya kiwango**
3. **Linda sehemu za mwisho za API**
4. **Fuatilia mifumo ya matumizi mabaya**

## Hitimisho

Llama.cpp inatoa njia yenye nguvu na bora ya kuendesha mifano mikubwa ya lugha kwa ndani kwenye usanifu mbalimbali wa vifaa. Ikiwa unajenga programu za AI, unafanya utafiti, au unajaribu tu LLMs, mfumo huu unatoa kubadilika na utendaji unaohitajika kwa matumizi mbalimbali.

Mambo muhimu:
- Chagua njia ya usakinishaji inayokufaa
- Boresha kwa usanifu maalum wa vifaa vyako
- Anza na matumizi ya msingi na uchunguze vipengele vya juu hatua kwa hatua
- Fikiria kutumia viunganishi vya Python kwa ujumuishaji rahisi
- Fuata mbinu bora kwa utekelezaji wa uzalishaji

Kwa maelezo zaidi na masasisho, tembelea [hifadhi rasmi ya Llama.cpp](https://github.com/ggml-org/llama.cpp) na rejelea nyaraka za kina na rasilimali za jamii zinazopatikana.

## ➡️ Nini kinachofuata

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya kutafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati asilia katika lugha yake ya awali inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.