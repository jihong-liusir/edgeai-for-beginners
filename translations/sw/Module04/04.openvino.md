<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T16:52:27+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "sw"
}
-->
# Sehemu ya 4: OpenVINO Toolkit Optimization Suite

## Jedwali la Yaliyomo
1. [Utangulizi](../../../Module04)
2. [OpenVINO ni nini?](../../../Module04)
3. [Usakinishaji](../../../Module04)
4. [Mwongozo wa Kuanza Haraka](../../../Module04)
5. [Mfano: Kubadilisha na Kuboresha Miundo kwa OpenVINO](../../../Module04)
6. [Matumizi ya Juu](../../../Module04)
7. [Mbinu Bora](../../../Module04)
8. [Kutatua Changamoto](../../../Module04)
9. [Rasilimali za Ziada](../../../Module04)

## Utangulizi

OpenVINO (Open Visual Inference and Neural Network Optimization) ni zana ya wazi ya Intel kwa kupeleka suluhisho za AI zenye utendaji mzuri katika mazingira ya wingu, ndani ya kampuni, na ukingo. Iwe unalenga CPU, GPU, VPU, au viharakishi maalum vya AI, OpenVINO hutoa uwezo wa kina wa kuboresha huku ikihifadhi usahihi wa modeli na kuwezesha kupelekwa kwa majukwaa mbalimbali.

## OpenVINO ni nini?

OpenVINO ni zana ya wazi inayowezesha watengenezaji kuboresha, kubadilisha, na kupeleka miundo ya AI kwa ufanisi katika majukwaa mbalimbali ya vifaa. Inajumuisha vipengele vikuu vitatu: OpenVINO Runtime kwa inference, Neural Network Compression Framework (NNCF) kwa uboreshaji wa modeli, na OpenVINO Model Server kwa kupelekwa kwa kiwango kikubwa.

### Vipengele Muhimu

- **Kupelekwa kwa Majukwaa Mbalimbali**: Inasaidia Linux, Windows, na macOS na API za Python, C++, na C
- **Uharakishaji wa Vifaa**: Ugunduzi wa kifaa kiotomatiki na uboreshaji kwa CPU, GPU, VPU, na viharakishi vya AI
- **Mfumo wa Kubana Miundo**: Mbinu za hali ya juu za quantization, pruning, na uboreshaji kupitia NNCF
- **Ulinganifu wa Mfumo**: Msaada wa moja kwa moja kwa miundo ya TensorFlow, ONNX, PaddlePaddle, na PyTorch
- **Msaada wa AI ya Kizazi**: OpenVINO GenAI maalum kwa kupeleka miundo mikubwa ya lugha na programu za AI ya kizazi

### Faida

- **Uboreshaji wa Utendaji**: Maboresho makubwa ya kasi na upotevu mdogo wa usahihi
- **Kupunguza Alama ya Kupelekwa**: Mahitaji madogo ya nje hurahisisha usakinishaji na kupelekwa
- **Kuimarisha Muda wa Kuanza**: Upakiaji wa modeli ulioboreshwa na caching kwa uanzishaji wa haraka wa programu
- **Kupelekwa kwa Kiwango Kikubwa**: Kutoka vifaa vya ukingo hadi miundombinu ya wingu na API thabiti
- **Tayari kwa Uzalishaji**: Uaminifu wa daraja la biashara na nyaraka za kina na msaada wa jamii

## Usakinishaji

### Mahitaji ya Awali

- Python 3.8 au zaidi
- Meneja wa kifurushi cha pip
- Mazingira ya kawaida (inapendekezwa)
- Vifaa vinavyolingana (CPU za Intel zinapendekezwa, lakini inasaidia usanifu mbalimbali)

### Usakinishaji wa Msingi

Unda na uwashe mazingira ya kawaida:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Sakinisha OpenVINO Runtime:

```bash
pip install openvino
```

Sakinisha NNCF kwa uboreshaji wa modeli:

```bash
pip install nncf
```

### Usakinishaji wa OpenVINO GenAI

Kwa programu za AI ya kizazi:

```bash
pip install openvino-genai
```

### Mahitaji ya Hiari

Vifurushi vya ziada kwa matumizi maalum:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Thibitisha Usakinishaji

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Ikiwa imefanikiwa, unapaswa kuona maelezo ya toleo la OpenVINO.

## Mwongozo wa Kuanza Haraka

### Uboreshaji wa Modeli Yako ya Kwanza

Hebu tubadilishe na kuboresha modeli ya Hugging Face kwa kutumia OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Kile Mchakato Huu Unafanya

Mchakato wa uboreshaji unahusisha: kupakia modeli ya awali kutoka Hugging Face, kubadilisha kuwa muundo wa OpenVINO Intermediate Representation (IR), kutumia uboreshaji wa msingi, na kuandaa kwa vifaa lengwa.

### Vigezo Muhimu Vilivyoelezwa

- `export=True`: Hubadilisha modeli kuwa muundo wa IR wa OpenVINO
- `compile=False`: Huchelewesha uandaji hadi wakati wa kukimbia kwa kubadilika
- `device`: Vifaa lengwa ("CPU", "GPU", "AUTO" kwa uteuzi wa kiotomatiki)
- `save_pretrained()`: Huokoa modeli iliyoboreshwa kwa matumizi ya baadaye

## Mfano: Kubadilisha na Kuboresha Miundo kwa OpenVINO

### Hatua ya 1: Kubadilisha Modeli kwa Quantization ya NNCF

Hivi ndivyo unavyotumia quantization baada ya mafunzo kwa NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Hatua ya 2: Uboreshaji wa Juu kwa Kubana Uzito

Kwa miundo inayotegemea transformer, tumia kubana uzito:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Hatua ya 3: Inference na Modeli Iliyoboreshwa

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Muundo wa Matokeo

Baada ya uboreshaji, saraka ya modeli yako itakuwa na:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Matumizi ya Juu

### Usanidi na NNCF YAML

Kwa mchakato wa uboreshaji wa hali ya juu, tumia faili za usanidi za NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Tumia usanidi:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Uboreshaji wa GPU

Kwa uharakishaji wa GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Uboreshaji wa Usindikaji wa Kundi

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Kupelekwa kwa Model Server

Peleka miundo iliyoboreshwa kwa OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Msimbo wa mteja kwa model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Mbinu Bora

### 1. Uchaguzi na Maandalizi ya Modeli
- Tumia miundo kutoka mifumo inayoungwa mkono (PyTorch, TensorFlow, ONNX)
- Hakikisha pembejeo za modeli zina maumbo yaliyowekwa au yanayojulikana
- Jaribu na seti za data zinazowakilisha kwa kalibrasi

### 2. Uchaguzi wa Mkakati wa Uboreshaji
- **Quantization Baada ya Mafunzo**: Anza hapa kwa uboreshaji wa haraka
- **Kubana Uzito**: Inafaa kwa miundo mikubwa ya lugha na transformers
- **Mafunzo Yenye Uelewa wa Quantization**: Tumia pale ambapo usahihi ni muhimu

### 3. Uboreshaji Maalum kwa Vifaa
- **CPU**: Tumia quantization ya INT8 kwa utendaji wa usawa
- **GPU**: Tumia usahihi wa FP16 na usindikaji wa kundi
- **VPU**: Zingatia urahisishaji wa modeli na fusion ya tabaka

### 4. Urekebishaji wa Utendaji
- **Njia ya Kupitisha**: Kwa usindikaji wa kundi la kiwango cha juu
- **Njia ya Ucheleweshaji**: Kwa programu za mwingiliano wa wakati halisi
- **AUTO Device**: Ruhusu OpenVINO kuchagua vifaa bora

### 5. Usimamizi wa Kumbukumbu
- Tumia maumbo ya nguvu kwa busara ili kuepuka mzigo wa kumbukumbu
- Tekeleza caching ya modeli kwa upakiaji wa haraka wa baadaye
- Fuatilia matumizi ya kumbukumbu wakati wa uboreshaji

### 6. Uthibitishaji wa Usahihi
- Kila wakati thibitisha miundo iliyoboreshwa dhidi ya utendaji wa awali
- Tumia seti za data za majaribio zinazowakilisha kwa tathmini
- Fikiria uboreshaji wa hatua kwa hatua (anza na mipangilio ya kihafidhina)

## Kutatua Changamoto

### Changamoto za Kawaida

#### 1. Changamoto za Usakinishaji
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Makosa ya Kubadilisha Modeli
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Changamoto za Utendaji
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Changamoto za Kumbukumbu
- Punguza ukubwa wa kundi la modeli wakati wa uboreshaji
- Tumia utiririshaji kwa seti kubwa za data
- Washa caching ya modeli: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Kupungua kwa Usahihi
- Tumia usahihi wa juu (INT8 badala ya INT4)
- Ongeza ukubwa wa seti ya data ya kalibrasi
- Tumia uboreshaji wa usahihi mchanganyiko

### Ufuatiliaji wa Utendaji

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Kupata Msaada

- **Nyaraka**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Masuala ya GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Jukwaa la Jamii**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Rasilimali za Ziada

### Viungo Rasmi
- **Ukurasa wa Nyumbani wa OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Hifadhi ya GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Hifadhi ya NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Rasilimali za Kujifunza
- **Notebooks za OpenVINO**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Mwongozo wa Kuanza Haraka**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Mwongozo wa Uboreshaji**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Zana za Ujumuishaji
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Viwango vya Utendaji
- **Viwango Rasmi**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Mifano ya Jamii
- **Notebooks za Jupyter**: [Hifadhi ya Notebooks za OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks) - Mafunzo ya kina yanapatikana katika hifadhi ya notebooks za OpenVINO
- **Programu za Mfano**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Mifano halisi kwa nyanja mbalimbali (maono ya kompyuta, NLP, sauti)
- **Machapisho ya Blogu**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Machapisho ya blogu ya Intel AI na jamii yenye mifano ya kina ya matumizi

### Zana Zinazohusiana
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Mbinu za ziada za uboreshaji kwa vifaa vya Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Kwa kulinganisha kupelekwa kwa simu na ukingo
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Mbadala wa injini ya inference ya majukwaa mbalimbali

## ➡️ Nini kinachofuata

- [05: Uchambuzi wa Kina wa Apple MLX Framework](./05.AppleMLX.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya kutafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.