<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T23:16:06+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sw"
}
-->
# Sehemu ya 3: Microsoft Olive Optimization Suite

## Jedwali la Maudhui
1. [Utangulizi](../../../Module04)
2. [Microsoft Olive ni nini?](../../../Module04)
3. [Usakinishaji](../../../Module04)
4. [Mwongozo wa Kuanza Haraka](../../../Module04)
5. [Mfano: Kubadilisha Qwen3 kuwa ONNX INT4](../../../Module04)
6. [Matumizi ya Juu](../../../Module04)
7. [Mbinu Bora](../../../Module04)
8. [Kutatua Tatizo](../../../Module04)
9. [Rasilimali za Ziada](../../../Module04)

## Utangulizi

Microsoft Olive ni zana yenye nguvu na rahisi kutumia kwa uboreshaji wa mifano ya kujifunza kwa mashine inayozingatia vifaa. Inarahisisha mchakato wa kuboresha mifano kwa ajili ya matumizi kwenye majukwaa tofauti ya vifaa. Iwe unalenga CPU, GPU, au viharakishi maalum vya AI, Olive hukusaidia kufanikisha utendaji bora huku ikihifadhi usahihi wa mfano.

## Microsoft Olive ni nini?

Olive ni zana rahisi kutumia inayozingatia vifaa kwa uboreshaji wa mifano, ikijumuisha mbinu za kisasa za uboreshaji, ukandamizaji wa mifano, na uundaji. Inafanya kazi na ONNX Runtime kama suluhisho la mwisho kwa uboreshaji wa inferensi.

### Vipengele Muhimu

- **Uboreshaji Unaotegemea Vifaa**: Huchagua kiotomatiki mbinu bora za uboreshaji kwa vifaa unavyolenga.
- **Vipengele 40+ Vilivyojengwa Ndani**: Inashughulikia ukandamizaji wa mifano, upunguzaji, uboreshaji wa grafu, na zaidi.
- **Kiolesura Rahisi cha CLI**: Amri rahisi kwa kazi za kawaida za uboreshaji.
- **Msaada wa Mfumo Mbalimbali**: Inafanya kazi na PyTorch, mifano ya Hugging Face, na ONNX.
- **Msaada wa Mifano Maarufu**: Olive inaweza kuboresha kiotomatiki miundo maarufu kama Llama, Phi, Qwen, Gemma, n.k. bila mabadiliko makubwa.

### Faida

- **Kupunguza Muda wa Maendeleo**: Hakuna haja ya kujaribu mbinu tofauti za uboreshaji kwa mikono.
- **Kuongezeka kwa Utendaji**: Maboresho makubwa ya kasi (hadi mara 6 katika baadhi ya matukio).
- **Utumiaji wa Majukwaa Mbalimbali**: Mifano iliyoboreshwa inafanya kazi kwenye vifaa na mifumo ya uendeshaji tofauti.
- **Usahihi Uliodumishwa**: Uboreshaji unahifadhi ubora wa mfano huku ukiboresha utendaji.

## Usakinishaji

### Mahitaji ya Awali

- Python 3.8 au zaidi
- Meneja wa pakiti ya pip
- Mazingira ya kawaida (inapendekezwa)

### Usakinishaji wa Msingi

Unda na uwashe mazingira ya kawaida:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Sakinisha Olive na vipengele vya uboreshaji kiotomatiki:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Mahitaji ya Hiari

Olive inatoa mahitaji ya hiari kwa vipengele vya ziada:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Thibitisha Usakinishaji

```bash
olive --help
```

Ikiwa imefanikiwa, unapaswa kuona ujumbe wa msaada wa Olive CLI.

## Mwongozo wa Kuanza Haraka

### Uboreshaji Wako wa Kwanza

Hebu tuboreshe mfano mdogo wa lugha kwa kutumia kipengele cha uboreshaji kiotomatiki cha Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kile Amri Hii Inafanya

Mchakato wa uboreshaji unahusisha: kupata mfano kutoka kwenye hifadhi ya ndani, kunasa Grafu ya ONNX na kuhifadhi uzito katika faili ya data ya ONNX, kuboresha Grafu ya ONNX, na kupunguza mfano hadi int4 kwa kutumia mbinu ya RTN.

### Maelezo ya Vigezo vya Amri

- `--model_name_or_path`: Kitambulisho cha mfano wa Hugging Face au njia ya ndani.
- `--output_path`: Saraka ambapo mfano ulioboreshwa utaokolewa.
- `--device`: Kifaa kinacholengwa (cpu, gpu).
- `--provider`: Mtoa huduma wa utekelezaji (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider).
- `--use_ort_genai`: Tumia ONNX Runtime Generate AI kwa inferensi.
- `--precision`: Usahihi wa upunguzaji (int4, int8, fp16).
- `--log_level`: Kiwango cha maelezo ya kumbukumbu (0=minimal, 1=verbose).

## Mfano: Kubadilisha Qwen3 kuwa ONNX INT4

Kulingana na mfano uliotolewa wa Hugging Face katika [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), hapa kuna jinsi ya kuboresha mfano wa Qwen3:

### Hatua ya 1: Pakua Mfano (Hiari)

Ili kupunguza muda wa kupakua, hifadhi faili muhimu tu:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Hatua ya 2: Boresha Mfano wa Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hatua ya 3: Jaribu Mfano Ulioboreshwa

Unda script rahisi ya Python kujaribu mfano wako ulioboreshwa:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Muundo wa Matokeo

Baada ya uboreshaji, saraka yako ya matokeo itakuwa na:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Matumizi ya Juu

### Faili za Usanidi

Kwa mchakato wa uboreshaji wa hali ya juu, unaweza kutumia faili za usanidi za JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Endesha na usanidi:

```bash
olive run --config config.json
```

### Uboreshaji wa GPU

Kwa uboreshaji wa GPU ya CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Kwa DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kuboresha Zaidi na Olive

Olive pia inasaidia kuboresha zaidi mifano:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mbinu Bora

### 1. Uchaguzi wa Mfano
- Anza na mifano midogo kwa majaribio (mfano, vigezo 0.5B-7B).
- Hakikisha usanifu wa mfano unalengwa na Olive.

### 2. Mahitaji ya Vifaa
- Linganisha lengo lako la uboreshaji na vifaa vya matumizi.
- Tumia uboreshaji wa GPU ikiwa una vifaa vinavyooana na CUDA.
- Fikiria DirectML kwa mashine za Windows zilizo na grafiki zilizounganishwa.

### 3. Uchaguzi wa Usahihi
- **INT4**: Ukandamizaji wa juu zaidi, upotevu mdogo wa usahihi.
- **INT8**: Uwiano mzuri wa ukubwa na usahihi.
- **FP16**: Upotevu mdogo wa usahihi, kupunguzwa kwa ukubwa wa wastani.

### 4. Upimaji na Uthibitishaji
- Daima jaribu mifano iliyoboreshwa na matumizi yako maalum.
- Linganisha vipimo vya utendaji (latency, throughput, usahihi).
- Tumia data ya pembejeo inayowakilisha kwa tathmini.

### 5. Uboreshaji wa Mara kwa Mara
- Anza na uboreshaji kiotomatiki kwa matokeo ya haraka.
- Tumia faili za usanidi kwa udhibiti wa kina.
- Jaribu njia tofauti za uboreshaji.

## Kutatua Tatizo

### Masuala ya Kawaida

#### 1. Matatizo ya Usakinishaji
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masuala ya CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masuala ya Kumbukumbu
- Tumia ukubwa mdogo wa kundi wakati wa uboreshaji.
- Jaribu upunguzaji wa usahihi wa juu kwanza (int8 badala ya int4).
- Hakikisha nafasi ya diski ya kutosha kwa hifadhi ya mfano.

#### 4. Hitilafu za Kupakia Mfano
- Thibitisha njia ya mfano na ruhusa za ufikiaji.
- Angalia ikiwa mfano unahitaji `trust_remote_code=True`.
- Hakikisha faili zote muhimu za mfano zimepakuliwa.

### Kupata Msaada

- **Nyaraka**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Masuala ya GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Mifano**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Rasilimali za Ziada

### Viungo Rasmi
- **Hifadhi ya GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Nyaraka za ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Mfano wa Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Mifano ya Jamii
- **Noti za Jupyter**: Zinapatikana katika hifadhi ya GitHub ya Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Kiendelezi cha VS Code**: Muhtasari wa AI Toolkit kwa VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Machapisho ya Blogu**: Blogu ya Microsoft Open Source — https://opensource.microsoft.com/blog/

### Zana Zinazohusiana
- **ONNX Runtime**: Injini ya inferensi yenye utendaji wa juu — https://onnxruntime.ai/
- **Hugging Face Transformers**: Chanzo cha mifano mingi inayooana — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Michakato ya uboreshaji inayotegemea wingu — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Nini kinachofuata

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

