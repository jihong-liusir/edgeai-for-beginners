<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T17:11:25+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "sw"
}
-->
# Sehemu ya 3: Kuboresha - Kubinafsisha Miundo kwa Kazi Maalum

## Jedwali la Yaliyomo
1. [Utangulizi wa Kuboresha](../../../Module05)
2. [Kwa Nini Kuboresha ni Muhimu](../../../Module05)
3. [Aina za Kuboresha](../../../Module05)
4. [Kuboresha na Microsoft Olive](../../../Module05)
5. [Mifano ya Vitendo](../../../Module05)
6. [Mbinu Bora na Miongozo](../../../Module05)
7. [Mbinu za Juu](../../../Module05)
8. [Tathmini na Ufuatiliaji](../../../Module05)
9. [Changamoto za Kawaida na Suluhisho](../../../Module05)
10. [Hitimisho](../../../Module05)

## Utangulizi wa Kuboresha

**Kuboresha** ni mbinu yenye nguvu ya kujifunza kwa mashine inayohusisha kurekebisha modeli iliyokwisha kufundishwa ili kutekeleza kazi maalum au kufanya kazi na seti za data maalum. Badala ya kufundisha modeli kutoka mwanzo, kuboresha kunatumia maarifa ambayo tayari yamejifunzwa na modeli iliyokwisha kufundishwa na kuirekebisha kwa matumizi yako maalum.

### Kuboresha ni Nini?

Kuboresha ni aina ya **ujifunzaji wa uhamisho** ambapo:
- Unaanzia na modeli iliyokwisha kufundishwa ambayo imejifunza mifumo ya jumla kutoka kwa seti kubwa za data
- Unarekebisha vigezo vya ndani vya modeli kwa kutumia seti yako maalum ya data
- Unahifadhi maarifa muhimu huku ukibinafsisha modeli kwa kazi yako

Fikiria kama kumfundisha mpishi mwenye ujuzi kupika vyakula vipya - tayari anaelewa misingi ya kupika, lakini anahitaji kujifunza mbinu na ladha maalum za mtindo mpya.

### Faida Muhimu

- **Ufanisi wa Muda**: Haraka sana kuliko kufundisha kutoka mwanzo
- **Ufanisi wa Data**: Inahitaji seti ndogo za data kufikia utendaji mzuri
- **Gharama Nafuu**: Mahitaji ya chini ya kompyuta
- **Utendaji Bora**: Mara nyingi hufikia matokeo bora kuliko kufundisha kutoka mwanzo
- **Uboreshaji wa Rasilimali**: Hufanya AI yenye nguvu kupatikana kwa timu ndogo na mashirika

## Kwa Nini Kuboresha ni Muhimu

### Matumizi ya Kwenye Ulimwengu Halisi

Kuboresha ni muhimu katika hali nyingi:

**1. Urekebishaji wa Uwanja**
- AI ya Matibabu: Kubinafsisha modeli za lugha kwa istilahi za matibabu na maelezo ya kliniki
- Teknolojia ya Kisheria: Kubinafsisha modeli kwa uchambuzi wa nyaraka za kisheria na ukaguzi wa mikataba
- Huduma za Kifedha: Kubinafsisha modeli kwa uchambuzi wa ripoti za kifedha na tathmini ya hatari

**2. Utaalamu wa Kazi**
- Uundaji wa Maudhui: Kuboresha kwa mitindo maalum ya uandishi au sauti
- Uundaji wa Nambari: Kubinafsisha modeli kwa lugha za programu au mifumo maalum
- Tafsiri: Kuboresha utendaji kwa jozi maalum za lugha au nyanja za kiufundi

**3. Matumizi ya Kampuni**
- Huduma kwa Wateja: Kuunda chatbots zinazofahamu istilahi maalum za kampuni
- Nyaraka za Ndani: Kujenga wasaidizi wa AI wanaofahamu michakato ya shirika
- Suluhisho za Sekta Maalum: Kuendeleza modeli zinazofahamu istilahi na mtiririko wa kazi wa sekta maalum

## Aina za Kuboresha

### 1. Kuboresha Kamili (Kuboresha Maelekezo)

Katika kuboresha kamili, vigezo vyote vya modeli vinarekebishwa wakati wa mafunzo. Mbinu hii:
- Inatoa kubadilika kwa kiwango cha juu na uwezo wa utendaji
- Inahitaji rasilimali kubwa za kompyuta
- Inasababisha toleo jipya kabisa la modeli
- Bora kwa hali ambapo una data nyingi ya mafunzo na rasilimali za kompyuta

### 2. Kuboresha kwa Ufanisi wa Vigezo (PEFT)

Mbinu za PEFT zinarekebisha sehemu ndogo tu ya vigezo, na kufanya mchakato kuwa wa ufanisi zaidi:

#### Low-Rank Adaptation (LoRA)
- Huongeza matriki ndogo za daraja la mafunzo kwenye uzito uliopo
- Inapunguza sana idadi ya vigezo vya mafunzo
- Inahifadhi utendaji karibu na kuboresha kamili
- Inaruhusu kubadilisha kwa urahisi kati ya marekebisho tofauti

#### QLoRA (Quantized LoRA)
- Inachanganya LoRA na mbinu za upunguzaji wa kumbukumbu
- Inapunguza zaidi mahitaji ya kumbukumbu
- Inaruhusu kuboresha modeli kubwa kwenye vifaa vya kawaida
- Inasawazisha ufanisi na utendaji

#### Adapters
- Huongeza mitandao midogo ya neva kati ya tabaka zilizopo
- Inaruhusu kuboresha kwa kulenga huku ikihifadhi modeli ya msingi
- Inatoa mbinu ya modular kwa ubinafsishaji wa modeli

### 3. Kuboresha Maalum kwa Kazi

Inalenga kubinafsisha modeli kwa kazi maalum za chini:
- **Uainishaji**: Kurekebisha modeli kwa kazi za upangaji
- **Uundaji**: Kuboresha kwa uundaji wa maudhui na kizazi cha maandishi
- **Uchimbaji**: Kuboresha kwa uchimbaji wa taarifa na utambuzi wa vyombo vilivyotajwa
- **Muhtasari**: Kubinafsisha modeli kwa muhtasari wa nyaraka

## Kuboresha na Microsoft Olive

Microsoft Olive ni zana kamili ya kuboresha modeli inayorahisisha mchakato wa kuboresha huku ikitoa vipengele vya kiwango cha biashara.

### Microsoft Olive ni Nini?

Microsoft Olive ni zana ya kuboresha modeli ya chanzo huria ambayo:
- Inarahisisha mtiririko wa kazi wa kuboresha kwa malengo mbalimbali ya vifaa
- Inatoa msaada wa ndani kwa usanifu maarufu wa modeli (Llama, Phi, Qwen, Gemma)
- Inatoa chaguo za kupelekwa kwa wingu na ndani ya nchi
- Inaunganishwa bila shida na Azure ML na huduma nyingine za AI za Microsoft
- Inasaidia kuboresha kiotomatiki na upunguzaji wa kumbukumbu

### Vipengele Muhimu

- **Kuboresha kwa Kujua Vifaa**: Inaboresha modeli kiotomatiki kwa vifaa maalum (CPU, GPU, NPU)
- **Msaada wa Muundo Mbalimbali**: Inafanya kazi na modeli za PyTorch, Hugging Face, na ONNX
- **Mtiririko wa Kazi Kiotomatiki**: Inapunguza usanidi wa mwongozo na majaribio
- **Uunganishaji wa Biashara**: Msaada wa ndani kwa Azure ML na kupelekwa kwa wingu
- **Usanifu Unaoweza Kupanuka**: Inaruhusu mbinu za kuboresha maalum

### Usakinishaji na Usanidi

#### Usakinishaji wa Msingi

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Vitegemezi vya Hiari

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Thibitisha Usakinishaji

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Mifano ya Vitendo

### Mfano 1: Kuboresha Msingi kwa Olive CLI

Mfano huu unaonyesha kuboresha modeli ndogo ya lugha kwa uainishaji wa misemo:

#### Hatua ya 1: Andaa Mazingira Yako

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Hatua ya 2: Kuboresha Modeli

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Hatua ya 3: Boresha kwa Kupelekwa

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Mfano 2: Usanidi wa Juu na Seti ya Data Maalum

#### Hatua ya 1: Andaa Seti ya Data Maalum

Unda faili ya JSON yenye data yako ya mafunzo:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Hatua ya 2: Unda Faili ya Usanidi

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Hatua ya 3: Tekeleza Kuboresha

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Mfano 3: Kuboresha QLoRA kwa Ufanisi wa Kumbukumbu

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Mbinu Bora na Miongozo

### Maandalizi ya Data

**1. Ubora wa Data Badala ya Wingi**
- Peana mifano ya hali ya juu, tofauti badala ya idadi kubwa ya data duni
- Hakikisha data inawakilisha matumizi yako lengwa
- Safisha na uandae data kwa usawa

**2. Muundo wa Data na Violezo**
- Tumia muundo thabiti katika mifano yote ya mafunzo
- Unda violezo vya wazi vya ingizo-tokeo vinavyolingana na matumizi yako
- Jumuisha muundo sahihi wa maelekezo kwa modeli zilizofundishwa kwa maelekezo

**3. Kugawanya Seti ya Data**
- Hifadhi 10-20% ya data kwa uthibitishaji
- Dumisha usambazaji sawa katika mgawanyo wa mafunzo/uthibitishaji
- Fikiria sampuli iliyopangwa kwa kazi za uainishaji

### Usanidi wa Mafunzo

**1. Uchaguzi wa Kiwango cha Kujifunza**
- Anza na viwango vidogo vya kujifunza (1e-5 hadi 1e-4) kwa kuboresha
- Tumia upangaji wa kiwango cha kujifunza kwa muunganisho bora
- Fuatilia mikondo ya hasara ili kurekebisha viwango ipasavyo

**2. Uboreshaji wa Ukubwa wa Kundi**
- Linganisha ukubwa wa kundi na kumbukumbu inayopatikana
- Tumia mkusanyiko wa gradient kwa ukubwa mkubwa wa kundi
- Fikiria uhusiano kati ya ukubwa wa kundi na kiwango cha kujifunza

**3. Muda wa Mafunzo**
- Fuatilia vipimo vya uthibitishaji ili kuepuka kujifunza kupita kiasi
- Tumia kusimamisha mapema wakati utendaji wa uthibitishaji unapotulia
- Hifadhi alama za ukaguzi mara kwa mara kwa urejeshaji na uchambuzi

### Uchaguzi wa Modeli

**1. Uchaguzi wa Modeli ya Msingi**
- Chagua modeli zilizofundishwa awali kwenye nyanja zinazofanana inapowezekana
- Fikiria ukubwa wa modeli kulingana na vikwazo vyako vya kompyuta
- Tathmini mahitaji ya leseni kwa matumizi ya kibiashara

**2. Uchaguzi wa Mbinu ya Kuboresha**
- Tumia LoRA/QLoRA kwa mazingira yenye vikwazo vya rasilimali
- Chagua kuboresha kamili wakati utendaji wa juu ni muhimu
- Fikiria mbinu zinazotegemea adapta kwa hali za kazi nyingi

### Usimamizi wa Rasilimali

**1. Uboreshaji wa Vifaa**
- Chagua vifaa vinavyofaa kwa ukubwa wa modeli yako na mbinu
- Tumia kumbukumbu ya GPU kwa ufanisi na ukaguzi wa gradient
- Fikiria suluhisho za wingu kwa modeli kubwa

**2. Usimamizi wa Kumbukumbu**
- Tumia mafunzo ya usahihi mchanganyiko inapopatikana
- Tekeleza mkusanyiko wa gradient kwa vikwazo vya kumbukumbu
- Fuatilia matumizi ya kumbukumbu ya GPU wakati wa mafunzo

## Mbinu za Juu

### Mafunzo ya Adapta Nyingi

Fundisha adapta nyingi kwa kazi tofauti huku ukishiriki modeli ya msingi:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Uboreshaji wa Vigezo vya Hyper

Tekeleza uboreshaji wa vigezo vya hyper kwa utaratibu:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Kazi za Hasara Maalum

Tekeleza kazi za hasara maalum za nyanja:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Tathmini na Ufuatiliaji

### Vipimo na Tathmini

**1. Vipimo vya Kawaida**
- **Usahihi**: Usahihi wa jumla kwa kazi za uainishaji
- **Perplexity**: Kipimo cha ubora wa modeli ya lugha
- **BLEU/ROUGE**: Ubora wa kizazi cha maandishi na muhtasari
- **F1 Score**: Uwiano wa usahihi na urejeshaji kwa uainishaji

**2. Vipimo vya Nyanja Maalum**
- **Viwango vya Kazi Maalum**: Tumia viwango vilivyowekwa kwa nyanja yako
- **Tathmini ya Binadamu**: Jumuisha tathmini ya binadamu kwa kazi za kiakili
- **Vipimo vya Biashara**: Linganisha na malengo halisi ya biashara

**3. Usanidi wa Tathmini**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Ufuatiliaji wa Maendeleo ya Mafunzo

**1. Ufuatiliaji wa Hasara**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Ufuatiliaji wa Uthibitishaji**
- Fuatilia hasara ya uthibitishaji sambamba na hasara ya mafunzo
- Fuatilia dalili za kujifunza kupita kiasi (hasara ya uthibitishaji ikiongezeka huku hasara ya mafunzo ikipungua)
- Tumia kusimamisha mapema kulingana na vipimo vya uthibitishaji

**3. Ufuatiliaji wa Rasilimali**
- Fuatilia matumizi ya GPU/CPU
- Fuatilia mifumo ya matumizi ya kumbukumbu
- Fuatilia kasi ya mafunzo na ufanisi

## Changamoto za Kawaida na Suluhisho

### Changamoto 1: Kujifunza Kupita Kiasi

**Dalili:**
- Hasara ya mafunzo inaendelea kupungua huku hasara ya uthibitishaji ikiongezeka
- Pengo kubwa kati ya utendaji wa mafunzo na uthibitishaji
- Utendaji duni kwa data mpya

**Suluhisho:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Changamoto 2: Vikwazo vya Kumbukumbu

**Suluhisho:**
- Tumia ukaguzi wa gradient
- Tekeleza mkusanyiko wa gradient
- Chagua mbinu za ufanisi wa vigezo (LoRA, QLoRA)
- Tumia ushirikiano wa modeli kwa modeli kubwa

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Changamoto 3: Mafunzo Polepole

**Suluhisho:**
- Boresha njia za kupakia data
- Tumia mafunzo ya usahihi mchanganyiko
- Tekeleza mikakati bora ya batching
- Fikiria mafunzo ya usambazaji kwa seti kubwa za data

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Changamoto 4: Utendaji Duni

**Hatua za Uchunguzi:**
1. Thibitisha ubora wa data na muundo
2. Angalia kiwango cha kujifunza na muda wa mafunzo
3. Tathmini uchaguzi wa modeli ya msingi
4. Kagua maandalizi na urekebishaji wa tokeni

**Suluhisho:**
- Ongeza utofauti wa data ya mafunzo
- Rekebisha ratiba ya kiwango cha kujifunza
- Jaribu modeli tofauti za msingi
- Tekeleza mbinu za kuongeza data

## Hitimisho

Kuboresha ni mbinu yenye nguvu inayowezesha upatikanaji wa uwezo wa hali ya juu wa AI. Kwa kutumia zana kama Microsoft Olive, mashirika yanaweza kubinafsisha modeli zilizokwisha kufundishwa kwa mahitaji yao maalum kwa ufanisi huku yakiboresha utendaji na vikwazo vya rasilimali.

### Mambo Muhimu ya Kujifunza

1. **Chagua Mbinu Sahihi**: Chagua mbinu za kuboresha kulingana na rasilimali zako za kompyuta na mahitaji ya utendaji
2. **Ubora wa Data ni Muhimu**: Wekeza katika data ya mafunzo yenye ubora wa juu na inayowakilisha
3. **Fuatilia na Rekebisha**: Tathmini na boresha modeli zako mara kwa mara
4. **Tumia Zana**: Tumia mifumo kama Olive kurahisisha na kuboresha mchakato
5. **Fikiria Kupelekwa**: Panga kuboresha modeli na kupelekwa tangu mwanzo

## ➡️ Nini Kinachofuata

- [04: Kupelekwa - Utekelezaji wa Modeli Tayari kwa Uzalishaji](./04.SLMOps.Deployment.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.