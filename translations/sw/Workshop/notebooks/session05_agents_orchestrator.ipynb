{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# Kipindi cha 5 – Mpangaji wa Wakala Wengi\n",
    "\n",
    "Inaonyesha mchakato rahisi wa mawakala wawili (Mtafiti -> Mhariri) kwa kutumia Foundry Local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### Maelezo: Usakinishaji wa Vitegemezi\n",
    "Inasakinisha `foundry-local-sdk` na `openai` zinazohitajika kwa ufikiaji wa modeli za ndani na kukamilisha mazungumzo. Inaweza kurudiwa bila athari mbaya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# Hali\n",
    "Inatekeleza muundo wa msingi wa wakala wawili:\n",
    "- **Wakala Mtafiti** hukusanya vidokezo vya ukweli kwa ufupi\n",
    "- **Wakala Mhariri** huandika upya kwa uwazi wa kiutendaji\n",
    "\n",
    "Inaonyesha kumbukumbu ya pamoja kwa kila wakala, kupitisha kwa mfululizo matokeo ya kati, na kazi rahisi ya bomba. Inaweza kupanuliwa kwa majukumu zaidi (mfano, Mkosoaji, Mhakiki) au matawi sambamba.\n",
    "\n",
    "**Vigezo vya Mazingira:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - Mfano wa chaguo-msingi wa kutumia (chaguo-msingi: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - Mfano wa wakala wa msingi (unazidi ALIAS)\n",
    "- `AGENT_MODEL_EDITOR` - Mfano wa wakala mhariri (chaguo-msingi ni wa msingi)\n",
    "\n",
    "**Marejeleo ya SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**Jinsi inavyofanya kazi:**\n",
    "1. **FoundryLocalManager** huanzisha huduma ya Foundry Local kiotomatiki\n",
    "2. Inapakua na kupakia mfano uliotajwa (au hutumia toleo lililohifadhiwa)\n",
    "3. Inatoa sehemu ya mwisho inayolingana na OpenAI kwa mwingiliano\n",
    "4. Kila wakala anaweza kutumia mfano tofauti kwa kazi maalum\n",
    "5. Mantiki ya kujirudia iliyojengwa ndani hushughulikia kushindwa kwa muda kwa urahisi\n",
    "\n",
    "**Vipengele Muhimu:**\n",
    "- ✅ Ugunduzi wa huduma na uanzishaji kiotomatiki\n",
    "- ✅ Usimamizi wa mzunguko wa maisha wa mfano (kupakua, kuhifadhi, kupakia)\n",
    "- ✅ Utangamano wa SDK ya OpenAI kwa API inayofahamika\n",
    "- ✅ Msaada wa mifano mingi kwa utaalamu wa wakala\n",
    "- ✅ Ushughulikiaji wa makosa thabiti na mantiki ya kujirudia\n",
    "- ✅ Utoaji wa maelezo wa ndani (hakuna API ya wingu inayohitajika)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### Maelezo: Uingizaji Muhimu & Uandishi\n",
    "Inatambulisha dataclasses kwa ajili ya kuhifadhi ujumbe wa wakala na vidokezo vya uandishi kwa uwazi. Inaingiza Foundry Local manager + mteja wa OpenAI kwa hatua za wakala zinazofuata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### Maelezo: Uanzishaji wa Modeli (Mfumo wa SDK)\n",
    "Inatumia Foundry Local Python SDK kwa usimamizi thabiti wa modeli:\n",
    "- **FoundryLocalManager(alias)** - Huanzisha huduma kiotomatiki na kupakia modeli kwa kutumia alias\n",
    "- **get_model_info(alias)** - Hutafsiri alias kuwa ID halisi ya modeli\n",
    "- **manager.endpoint** - Hutoa endpoint ya huduma kwa mteja wa OpenAI\n",
    "- **manager.api_key** - Hutoa API key (hiari kwa matumizi ya ndani)\n",
    "- Inasaidia modeli tofauti kwa mawakala tofauti (kama wakuu dhidi ya wahariri)\n",
    "- Ina mantiki ya kujirudia iliyojengwa ndani na backoff ya kielelezo kwa uimara\n",
    "- Uhakikisho wa muunganisho ili kuhakikisha huduma iko tayari\n",
    "\n",
    "**Mfumo Muhimu wa SDK:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**Usimamizi wa Mzunguko wa Maisha:**\n",
    "- Wasimamizi huhifadhiwa kimataifa kwa usafi sahihi\n",
    "- Kila wakala anaweza kutumia modeli tofauti kwa utaalamu\n",
    "- Ugunduzi wa huduma kiotomatiki na usimamizi wa muunganisho\n",
    "- Kujirudia kwa neema na backoff ya kielelezo wakati wa kushindwa\n",
    "\n",
    "Hii inahakikisha uanzishaji sahihi kabla ya upangaji wa mawakala kuanza.\n",
    "\n",
    "**Rejea:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### Maelezo: Madarasa ya Agent & Memory\n",
    "Hufafanua `AgentMsg` nyepesi kwa ajili ya maingizo ya kumbukumbu na `Agent` inayojumuisha:\n",
    "- **Jukumu la Mfumo** - Nafasi ya wakala na maelekezo yake\n",
    "- **Historia ya Ujumbe** - Hudumisha muktadha wa mazungumzo\n",
    "- **Njia ya act()** - Hufanya vitendo kwa kushughulikia makosa ipasavyo\n",
    "\n",
    "Wakala anaweza kutumia mifano tofauti (ya msingi dhidi ya mhariri) na huhifadhi muktadha wa pekee kwa kila wakala. Muundo huu huwezesha:\n",
    "- Uendelevu wa kumbukumbu kati ya vitendo\n",
    "- Uteuzi rahisi wa mfano kwa kila wakala\n",
    "- Kutengwa kwa makosa na urejeshaji\n",
    "- Urahisi wa kuunganisha na kupanga shughuli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### Maelezo: Njia ya Mchakato Iliyoratibiwa\n",
    "Inaunda mawakala wawili maalum:\n",
    "- **Mtafiti**: Hutumia mfano wa msingi, hukusanya taarifa za ukweli\n",
    "- **Mhariri**: Anaweza kutumia mfano tofauti (ikiwa umewekwa), husahihisha na kuandika upya\n",
    "\n",
    "Kazi ya `pipeline`:\n",
    "1. Mtafiti hukusanya taarifa ghafi\n",
    "2. Mhariri husahihisha na kuibadilisha kuwa matokeo yaliyo tayari kwa utekelezaji\n",
    "3. Inarudisha matokeo ya kati na ya mwisho\n",
    "\n",
    "Mfumo huu unaruhusu:\n",
    "- Utaalamu wa mfano (miundo tofauti kwa majukumu tofauti)\n",
    "- Kuboresha ubora kupitia usindikaji wa hatua nyingi\n",
    "- Ufuatiliaji wa mabadiliko ya taarifa\n",
    "- Urahisi wa kuongeza mawakala zaidi au usindikaji sambamba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### Maelezo: Utekelezaji wa Njia ya Uendeshaji & Matokeo\n",
    "Inatekeleza njia ya mawakala wengi kwenye swali lenye mada ya uzingatiaji + ucheleweshaji ili kuonyesha:\n",
    "- Mabadiliko ya taarifa kwa hatua nyingi\n",
    "- Utaalamu wa mawakala na ushirikiano\n",
    "- Uboreshaji wa ubora wa matokeo kupitia marekebisho\n",
    "- Ufuatiliaji (matokeo ya kati na ya mwisho yanahifadhiwa)\n",
    "\n",
    "**Muundo wa Matokeo:**\n",
    "- `question` - Swali la awali la mtumiaji\n",
    "- `research` - Matokeo ya utafiti wa awali (pointi za ukweli)\n",
    "- `final` - Muhtasari wa mwisho ulioboreshwa\n",
    "- `models` - Ni mifano gani ilitumika kwa kila hatua\n",
    "\n",
    "**Mawazo ya Kupanua:**\n",
    "1. Ongeza wakala Mkosoaji kwa ukaguzi wa ubora\n",
    "2. Tekeleza mawakala wa utafiti sambamba kwa vipengele tofauti\n",
    "3. Ongeza wakala Mhakiki kwa uhakiki wa ukweli\n",
    "4. Tumia mifano tofauti kwa viwango tofauti vya ugumu\n",
    "5. Tekeleza mizunguko ya maoni kwa uboreshaji wa hatua kwa hatua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### Kiwango cha Juu: Usanidi wa Wakala Maalum\n",
    "\n",
    "Jaribu kubadilisha tabia ya wakala kwa kurekebisha vigezo vya mazingira kabla ya kuendesha seli ya uanzishaji:\n",
    "\n",
    "**Mifano Inayopatikana:**\n",
    "- Tumia `foundry model ls` kwenye terminal kuona mifano yote inayopatikana\n",
    "- Mifano: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b, nk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Kanusho**:  \nHati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-10-09T22:05:53+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}