<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-18T16:11:21+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "sw"
}
-->
# Sehemu ya 3: Misingi ya Familia ya Gemma

Familia ya modeli ya Gemma inawakilisha mbinu ya kina ya Google kwa modeli kubwa za lugha za chanzo huria na AI ya multimodal, ikionyesha kwamba modeli zinazopatikana zinaweza kufanikisha utendaji wa ajabu huku zikifaa kutumika katika hali mbalimbali kutoka kwa vifaa vya mkononi hadi vituo vya kazi vya biashara. Ni muhimu kuelewa jinsi familia ya Gemma inavyowezesha uwezo wa AI wenye nguvu kwa chaguo za utekelezaji zinazobadilika huku ikidumisha utendaji wa ushindani na mazoea ya AI yenye uwajibikaji.

## Utangulizi

Katika mafunzo haya, tutachunguza familia ya modeli ya Gemma ya Google na dhana zake za msingi. Tutajadili mabadiliko ya familia ya Gemma, mbinu za mafunzo za ubunifu zinazofanya modeli za Gemma kuwa bora, aina kuu katika familia, na matumizi ya vitendo katika hali tofauti za utekelezaji.

## Malengo ya Kujifunza

Mwisho wa mafunzo haya, utaweza:

- Kuelewa falsafa ya muundo na mabadiliko ya familia ya modeli ya Gemma ya Google
- Kutambua ubunifu muhimu unaowezesha modeli za Gemma kufanikisha utendaji wa juu katika ukubwa mbalimbali wa vigezo
- Kutambua faida na mapungufu ya aina tofauti za modeli za Gemma
- Kutumia maarifa ya modeli za Gemma kuchagua aina zinazofaa kwa hali halisi

## Kuelewa Mandhari ya Kisasa ya Modeli za AI

Mandhari ya AI imebadilika sana, huku mashirika tofauti yakifuata mbinu mbalimbali za ukuzaji wa modeli za lugha. Wakati wengine wanazingatia modeli za chanzo kilichofungwa zinazopatikana tu kupitia API, wengine wanasisitiza upatikanaji wa chanzo huria na uwazi. Mbinu ya jadi inahusisha ama modeli kubwa za chanzo kilichofungwa zenye gharama zinazoendelea au modeli za chanzo huria zinazoweza kuhitaji utaalamu mkubwa wa kiufundi kwa utekelezaji.

Paradigmu hii inaleta changamoto kwa mashirika yanayotafuta uwezo wa AI wenye nguvu huku yakidumisha udhibiti wa data yao, gharama, na kubadilika kwa utekelezaji. Mbinu ya kawaida mara nyingi inahitaji kuchagua kati ya utendaji wa hali ya juu na kuzingatia utekelezaji wa vitendo.

## Changamoto ya Ubora wa AI Inayopatikana

Hitaji la AI ya ubora wa juu inayopatikana limekuwa muhimu zaidi katika hali mbalimbali. Fikiria matumizi yanayohitaji chaguo za utekelezaji zinazobadilika kwa mahitaji tofauti ya shirika, utekelezaji wa gharama nafuu ambapo gharama za API zinaweza kuwa kubwa, uwezo wa multimodal kwa uelewa wa kina, au utekelezaji maalum kwenye vifaa vya mkononi na vya ukingoni.

### Mahitaji Muhimu ya Utekelezaji

Utekelezaji wa kisasa wa AI unakabiliwa na mahitaji kadhaa ya msingi yanayopunguza matumizi ya vitendo:

- **Upatikanaji**: Upatikanaji wa chanzo huria kwa uwazi na ubinafsishaji
- **Ufanisi wa Gharama**: Mahitaji ya kompyuta yanayofaa kwa bajeti mbalimbali
- **Kubadilika**: Ukubwa tofauti wa modeli kwa hali tofauti za utekelezaji
- **Uelewa wa Multimodal**: Uwezo wa kuchakata maono, maandishi, na sauti
- **Utekelezaji wa Ukingoni**: Utendaji ulioboreshwa kwenye vifaa vya mkononi na vilivyo na rasilimali chache

## Falsafa ya Modeli ya Gemma

Familia ya modeli ya Gemma inawakilisha mbinu ya kina ya Google kwa ukuzaji wa modeli za AI, ikipa kipaumbele upatikanaji wa chanzo huria, uwezo wa multimodal, na utekelezaji wa vitendo huku ikidumisha sifa za utendaji wa ushindani. Modeli za Gemma zinafanikisha hili kupitia ukubwa tofauti wa modeli, mbinu za mafunzo za ubora wa juu zinazotokana na utafiti wa Gemini, na aina maalum kwa nyanja tofauti na hali za utekelezaji.

Familia ya Gemma inajumuisha mbinu mbalimbali zilizoundwa kutoa chaguo katika wigo wa utendaji na ufanisi, ikiruhusu utekelezaji kutoka kwa vifaa vya mkononi hadi seva za biashara huku ikitoa uwezo wa AI wa maana. Lengo ni kuleta teknolojia ya AI ya ubora wa juu kwa kila mtu huku ikitoa kubadilika kwa chaguo za utekelezaji.

### Kanuni za Msingi za Muundo wa Gemma

Modeli za Gemma zimejengwa juu ya kanuni kadhaa za msingi zinazozitofautisha na familia nyingine za modeli za lugha:

- **Chanzo Huria Kwanza**: Uwazi kamili na upatikanaji kwa utafiti na matumizi ya kibiashara
- **Ukuzaji Unaotokana na Utafiti**: Zimejengwa kwa kutumia utafiti na teknolojia sawa inayowezesha modeli za Gemini
- **Usanifu Unaoweza Kupimika**: Ukubwa tofauti wa modeli ili kuendana na mahitaji tofauti ya kompyuta
- **AI yenye Uwajibikaji**: Hatua za usalama zilizojumuishwa na mazoea ya ukuzaji yenye uwajibikaji

## Teknolojia Muhimu Zinazowezesha Familia ya Gemma

### Mbinu za Mafunzo za Juu

Moja ya vipengele vinavyotofautisha familia ya Gemma ni mbinu ya mafunzo ya kisasa inayotokana na utafiti wa Gemini wa Google. Modeli za Gemma zinatumia distillation kutoka kwa modeli kubwa, kujifunza kwa kuimarisha kutoka maoni ya binadamu (RLHF), na mbinu za kuunganisha modeli ili kufanikisha utendaji ulioboreshwa katika hesabu, usimbaji, na kufuata maagizo.

Mchakato wa mafunzo unahusisha distillation kutoka kwa modeli kubwa za maagizo, kujifunza kwa kuimarisha kutoka maoni ya binadamu (RLHF) ili kuendana na mapendeleo ya binadamu, kujifunza kwa kuimarisha kutoka maoni ya mashine (RLMF) kwa uelewa wa hesabu, na kujifunza kwa kuimarisha kutoka maoni ya utekelezaji (RLEF) kwa uwezo wa usimbaji.

### Ujumuishaji na Uelewa wa Multimodal

Modeli za Gemma za hivi karibuni zinajumuisha uwezo wa multimodal wa kisasa unaowezesha uelewa wa kina katika aina tofauti za pembejeo:

**Ujumuishaji wa Maono na Lugha (Gemma 3)**: Gemma 3 inaweza kuchakata maandishi na picha kwa wakati mmoja, ikiruhusu kuchanganua picha, kujibu maswali kuhusu maudhui ya kuona, kutoa maandishi kutoka kwa picha, na kuelewa data ya kuona yenye changamoto.

**Usindikaji wa Sauti (Gemma 3n)**: Gemma 3n ina uwezo wa sauti wa hali ya juu ikiwa ni pamoja na utambuzi wa hotuba kiotomatiki (ASR) na tafsiri ya hotuba kiotomatiki (AST), ikiwa na utendaji bora hasa kwa tafsiri kati ya Kiingereza na Kihispania, Kifaransa, Kiitaliano, na Kireno.

**Usindikaji wa Pembejeo Zilizochanganyika**: Modeli za Gemma zinaunga mkono pembejeo zilizochanganyika katika njia mbalimbali, ikiruhusu uelewa wa mwingiliano wa multimodal ambapo maandishi, picha, na sauti vinaweza kuchakatwa pamoja.

### Ubunifu wa Kimuundo

Familia ya Gemma inajumuisha uboreshaji kadhaa wa usanifu ulioundwa kwa utendaji na ufanisi:

**Upanuzi wa Dirisha la Muktadha**: Modeli za Gemma 3 zina dirisha la muktadha la tokeni 128K, mara 16 kubwa zaidi kuliko modeli za Gemma za awali, ikiruhusu kuchakata kiasi kikubwa cha taarifa ikiwa ni pamoja na nyaraka nyingi au mamia ya picha.

**Usanifu wa Kwanza wa Simu (Gemma 3n)**: Gemma 3n inatumia teknolojia ya Per-Layer Embeddings (PLE) na usanifu wa MatFormer, ikiruhusu modeli kubwa kuendeshwa na alama za kumbukumbu zinazolingana na modeli ndogo za jadi.

**Uwezo wa Kuita Kazi**: Gemma 3 inaunga mkono kuita kazi, ikiruhusu watengenezaji kujenga miingiliano ya lugha asilia kwa miingiliano ya programu na kuunda mifumo ya kiotomatiki yenye akili.

## Ukubwa wa Modeli na Chaguo za Utekelezaji

Mazingira ya kisasa ya utekelezaji yanapata faida kutoka kwa kubadilika kwa modeli za Gemma katika mahitaji tofauti ya kompyuta:

### Modeli Ndogo (0.6B-4B)

Gemma inatoa modeli ndogo zenye ufanisi zinazofaa kwa utekelezaji wa ukingoni, programu za mkononi, na mazingira yenye rasilimali chache huku ikidumisha uwezo wa kuvutia. Modeli ya 1B ni bora kwa programu ndogo, huku modeli ya 4B ikitoa utendaji wa usawa na kubadilika na msaada wa multimodal.

### Modeli za Kati (8B-14B)

Modeli za kiwango cha kati zinatoa uwezo ulioboreshwa kwa programu za kitaalamu, zikitoa usawa bora kati ya utendaji na mahitaji ya kompyuta kwa utekelezaji wa vituo vya kazi na seva.

### Modeli Kubwa (27B+)

Modeli za kiwango cha juu hutoa utendaji wa hali ya juu kwa programu zinazohitaji sana, utafiti, na utekelezaji wa biashara unaohitaji uwezo wa juu zaidi. Modeli ya 27B inawakilisha chaguo lenye uwezo zaidi ambalo bado linaweza kuendeshwa kwenye GPU moja.

### Modeli Zilizoboreshwa kwa Simu (Gemma 3n)

Modeli za Gemma 3n E2B na E4B zimeundwa mahsusi kwa utekelezaji wa simu na ukingoni, zikiwa na hesabu za vigezo vya 2B na 4B mtawalia, huku zikitumia usanifu wa ubunifu kupunguza alama ya kumbukumbu hadi GB 2 kwa E2B na GB 3 kwa E4B.

## Faida za Familia ya Modeli ya Gemma

### Upatikanaji wa Chanzo Huria

Modeli za Gemma zinatoa uwazi kamili na uwezo wa ubinafsishaji na uzito wa chanzo huria unaoruhusu matumizi ya kibiashara yenye uwajibikaji, ikiruhusu mashirika kuziboresha na kuzitekeleza katika miradi na programu zao.

### Kubadilika kwa Utekelezaji

Wigo wa ukubwa wa modeli unaruhusu utekelezaji katika usanidi tofauti wa vifaa, kutoka kwa vifaa vya mkononi hadi seva za hali ya juu, huku zikiwa na uboreshaji kwa majukwaa mbalimbali ikiwa ni pamoja na Google Cloud TPUs, NVIDIA GPUs, AMD GPUs kupitia ROCm, na utekelezaji wa CPU kupitia Gemma.cpp.

### Ubora wa Lugha Nyingi

Modeli za Gemma zinang'ara katika uelewa wa lugha nyingi na kizazi, zikiunga mkono zaidi ya lugha 140 na uwezo wa lugha nyingi usio na kifani, na kuzifanya kufaa kwa programu za kimataifa.

### Utendaji wa Ushindani

Modeli za Gemma mara kwa mara hufanikisha matokeo ya ushindani kwenye viwango vya majaribio, huku Gemma 3 ikiorodheshwa juu kati ya modeli maarufu za chanzo kilichofungwa na huria katika tathmini za mapendeleo ya watumiaji.

### Uwezo Maalum

Matumizi maalum ya nyanja yanapata faida kutoka kwa uelewa wa multimodal wa Gemma, uwezo wa kuita kazi, na utendaji ulioboreshwa katika majukwaa mbalimbali ya vifaa.

## Mifano ya Vitendo na Matumizi

Kabla ya kuingia katika maelezo ya kiufundi, hebu tuchunguze mifano halisi ya kile modeli za Gemma zinaweza kufanikisha:

### Mfano wa Uelewa wa Multimodal

Gemma 3 inaweza kuchakata maandishi na picha kwa wakati mmoja kwa uchambuzi wa kina:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### Mfano wa Msaada wa Lugha Nyingi

Modeli za Gemma zinaonyesha uwezo mkubwa wa lugha nyingi katika zaidi ya lugha 140:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### Mfano wa Kuita Kazi

Gemma 3 inaunga mkono kuita kazi kwa kujenga miingiliano ya lugha asilia:

```python
# Example function definitions
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### Mfano wa Utekelezaji wa Simu (Gemma 3n)

Gemma 3n imeboreshwa kwa utekelezaji wa simu na ukingoni kwa matumizi bora ya kumbukumbu:

```python
# Mobile-optimized inference with Gemma 3n
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Gemma 3n for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Further optimize for mobile
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile usage
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### Mfano wa Usindikaji wa Sauti (Gemma 3n)

Gemma 3n inajumuisha uwezo wa sauti wa hali ya juu kwa utambuzi wa hotuba na tafsiri:

```python
# Audio processing with Gemma 3n
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # Gemma 3n audio processing pipeline
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # Process with Gemma 3n
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# Example usage
# Transcribe Spanish audio to text
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# Translate Spanish speech to English text
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## Mabadiliko ya Familia ya Gemma

### Gemma 1.0 na 2.0: Modeli za Msingi

Modeli za awali za Gemma zilianzisha kanuni za msingi za upatikanaji wa chanzo huria na utekelezaji wa vitendo:

- **Gemma-2B na 7B**: Toleo la awali likilenga uelewa wa lugha wenye ufanisi
- **Mfululizo wa Gemma 1.5**: Uboreshaji wa kushughulikia muktadha na utendaji ulioboreshwa
- **Familia ya Gemma 2**: Utangulizi wa uwezo wa multimodal na ukubwa wa modeli ulioongezwa

### Gemma 3: Ubora wa Multimodal

Mfululizo wa Gemma 3 ulileta maendeleo makubwa katika uwezo wa multimodal na utendaji. Imejengwa kutoka kwa utafiti na teknolojia sawa inayowezesha modeli za Gemini 2.0, Gemma 3 ilianzisha uelewa wa maono na lugha, dirisha la muktadha la tokeni 128K, kuita kazi, na msaada kwa zaidi ya lugha 140.

Vipengele muhimu vya Gemma 3 ni pamoja na:
- **Gemma 3-1B hadi 27B**: Wigo kamili kwa mahitaji tofauti ya utekelezaji
- **Uelewa wa Multimodal**: Uwezo wa hali ya juu wa maandishi na maono
- **Muktadha Ulioongezwa**: Uwezo wa kuchakata tokeni 128K
- **Kuita Kazi**: Ujenzi wa miingiliano ya lugha asilia
- **Mafunzo Yaliyoimarishwa**: Yameboreshwa kwa kutumia distillation na kujifunza kwa kuimarisha

### Gemma 3n: Ubunifu wa Kwanza wa Simu

Gemma 3n inawakilisha mafanikio katika usanifu wa AI wa kwanza wa simu, ikijumuisha teknolojia ya Per-Layer Embeddings (PLE), usanifu wa MatFormer kwa kubadilika kwa kompyuta, na uwezo wa multimodal wa kina ikiwa ni pamoja na usindikaji wa sauti.

Ubunifu wa Gemma 3n ni pamoja na:
- **Modeli za E2B na E4B**: Utendaji wa vigezo vya 2B na 4B wenye alama ya kumbukumbu iliyopunguzwa
- **Uwezo wa Sauti**: ASR ya ubora wa juu na tafsiri ya hotuba
- **Uelewa wa Video**: Uwezo wa usindikaji wa video ulioboreshwa sana
- **Uboreshaji wa Simu**: Imeundwa kwa AI ya wakati halisi kwenye simu na vidonge

## Matumizi ya Modeli za Gemma

### Matumizi ya Biashara

Mashirika hutumia modeli za Gemma kwa uchambuzi wa nyaraka zenye maudhui ya kuona, otomatiki ya huduma kwa wateja yenye msaada wa multimodal, usaidizi wa usimbaji wenye akili, na programu za ujasusi wa biashara. Asili ya chanzo huria inaruhusu ubinafsishaji kwa mahitaji maalum ya biashara huku ikidumisha faragha ya data na udhibiti.

### Kompyuta ya Simu na Ukingoni

Programu za mkononi zinatumia Gemma 3n kwa AI ya wakati halisi inayofanya kazi moja kwa moja kwenye vifaa, ikiruhusu uzoefu wa kibinafsi na wa faragha wenye uwezo wa AI wa multimodal wa haraka. Matumizi ni pamoja na tafsiri ya wakati halisi, wasaidizi wenye akili, kizazi cha maudhui, na mapendekezo ya kibinafsi.

### Teknolojia ya Elimu

Majukwaa ya elimu hutumia modeli za Gemma kwa uzoefu wa kufundisha wa multimodal, kizazi cha maudhui kiotomatiki chenye vipengele vya kuona, usaidizi wa kujifunza lugha kwa usindikaji wa sauti, na uzoefu wa elimu wa maingiliano unaochanganya maandishi, picha, na hotuba.

### Matumizi ya Kimataifa

Matumizi ya kimataifa yanapata faida kutoka kwa uwezo wa lugha nyingi na wa kuvuka tamaduni wa modeli za Gemma, ikiruhusu uzoefu wa AI thabiti katika lugha na muktadha wa kitamaduni tofauti na uelewa wa kuona na sauti.

## Changamoto na Mapungufu

### Mahitaji ya Kompyuta

Ingawa Gemma inatoa modeli katika ukubwa tofauti, aina kubwa bado zinahitaji rasilimali kubwa za kompyuta kwa utendaji bora. Mahitaji ya kumbukumbu yanatofautiana kutoka takriban GB 2 kwa modeli ndogo zilizopunguzwa hadi GB 54 kwa modeli kubwa zaidi ya 27B.

### Utendaji wa Nyanja Maalum

Ingawa modeli za Gemma zinafanya vizuri katika nyanja za jumla na kazi za multimodal,
- Gemma 3 inatoa uwezo mkubwa kwa watengenezaji kwa uwezo wa hali ya juu wa kufikiri kwa maandishi na picha, ikiruhusu pembejeo za picha na maandishi kwa uelewa wa multimodal.  
- Gemma 3n inashika nafasi ya juu kati ya mifano maarufu ya wamiliki na wazi katika alama za Chatbot Arena Elo, ikionyesha upendeleo mkubwa wa watumiaji.  

**Mafanikio ya Ufanisi:**  
- Mifano ya Gemma 3 inaweza kushughulikia pembejeo za maelezo hadi tokeni 128K, dirisha la muktadha lenye ukubwa wa mara 16 zaidi kuliko mifano ya awali ya Gemma.  
- Gemma 3n inatumia Per-Layer Embeddings (PLE) ambayo inapunguza matumizi ya RAM kwa kiasi kikubwa huku ikihifadhi uwezo wa mifano mikubwa.  

**Uboreshaji wa Simu:**  
- Gemma 3n E2B inafanya kazi na kumbukumbu ya chini ya 2GB huku E4B ikihitaji tu 3GB, licha ya kuwa na hesabu ya vigezo ghafi ya 5B na 8B mtawalia.  
- Uwezo wa AI wa wakati halisi moja kwa moja kwenye vifaa vya simu na operesheni inayozingatia faragha, tayari kwa nje ya mtandao.  

**Kiwango cha Mafunzo:**  
- Gemma 3 ilifundishwa kwa tokeni 2T kwa 1B, 4T kwa 4B, 12T kwa 12B, na tokeni 14T kwa mifano ya 27B kwa kutumia Google TPUs na Mfumo wa JAX.  

### Jedwali la Ulinganisho wa Mifano  

| Mfululizo wa Mifano | Kiwango cha Vigezo | Urefu wa Muktadha | Nguvu Muhimu | Matumizi Bora |  
|----------------------|--------------------|-------------------|--------------|---------------|  
| **Gemma 3**         | 1B-27B            | 128K             | Uelewa wa multimodal, kupiga simu za kazi | Matumizi ya jumla, kazi za lugha-picha |  
| **Gemma 3n**        | E2B (5B), E4B (8B)| Inabadilika       | Uboreshaji wa simu, usindikaji wa sauti | Programu za simu, kompyuta ya ukingoni, AI ya wakati halisi |  
| **Gemma 2.5**       | 0.5B-72B          | 32K-128K         | Utendaji wa usawa, lugha nyingi | Utekelezaji wa uzalishaji, mifumo iliyopo |  
| **Gemma-VL**        | Mbalimbali         | Inabadilika       | Utaalamu wa lugha-picha | Uchambuzi wa picha, kujibu maswali ya kuona |  

## Mwongozo wa Uchaguzi wa Mfano  

### Kwa Matumizi ya Msingi  
- **Gemma 3-1B**: Kazi nyepesi za maandishi, programu rahisi za simu.  
- **Gemma 3-4B**: Utendaji wa usawa na msaada wa multimodal kwa matumizi ya jumla.  

### Kwa Matumizi ya Multimodal  
- **Gemma 3-4B/12B**: Uelewa wa picha, kujibu maswali ya kuona.  
- **Gemma 3n**: Programu za simu za multimodal na uwezo wa usindikaji wa sauti.  

### Kwa Utekelezaji wa Simu na Ukingoni  
- **Gemma 3n E2B**: Vifaa vilivyo na rasilimali chache, AI ya simu ya wakati halisi.  
- **Gemma 3n E4B**: Utendaji wa simu ulioboreshwa na uwezo wa sauti.  

### Kwa Utekelezaji wa Biashara  
- **Gemma 3-12B/27B**: Uelewa wa lugha na picha wa utendaji wa juu.  
- **Uwezo wa kupiga simu za kazi**: Kujenga mifumo ya kiotomatiki yenye akili.  

### Kwa Matumizi ya Ulimwenguni  
- **Aina yoyote ya Gemma 3**: Msaada wa lugha 140+ na uelewa wa kitamaduni.  
- **Gemma 3n**: Programu za simu za kwanza za kimataifa na tafsiri ya sauti.  

## Majukwaa ya Utekelezaji na Upatikanaji  

### Majukwaa ya Wingu  
- **Vertex AI**: Uwezo wa MLOps wa mwisho hadi mwisho na uzoefu wa bila seva.  
- **Google Kubernetes Engine (GKE)**: Utekelezaji wa kontena unaoweza kupanuka kwa mizigo tata.  
- **Google GenAI API**: Ufikiaji wa moja kwa moja wa API kwa uundaji wa haraka.  
- **Katalogi ya API ya NVIDIA**: Utendaji ulioboreshwa kwenye GPU za NVIDIA.  

### Mifumo ya Maendeleo ya Ndani  
- **Hugging Face Transformers**: Ujumuishaji wa kawaida kwa maendeleo.  
- **Ollama**: Utekelezaji rahisi wa ndani na usimamizi.  
- **vLLM**: Huduma ya utendaji wa juu kwa uzalishaji.  
- **Gemma.cpp**: Utekelezaji ulioboreshwa wa CPU.  
- **Google AI Edge**: Uboreshaji wa utekelezaji wa simu na ukingoni.  

### Rasilimali za Kujifunza  
- **Google AI Studio**: Jaribu mifano ya Gemma kwa mibofyo michache tu.  
- **Kaggle na Hugging Face**: Pakua uzito wa mifano na mifano ya jamii.  
- **Ripoti za Kiufundi**: Nyaraka za kina na karatasi za utafiti.  
- **Majukwaa ya Jamii**: Msaada wa jamii hai na mijadala.  

### Kuanza na Mifano ya Gemma  

#### Majukwaa ya Maendeleo  
1. **Google AI Studio**: Anza na majaribio ya mtandao.  
2. **Hugging Face Hub**: Chunguza mifano na utekelezaji wa jamii.  
3. **Utekelezaji wa Ndani**: Tumia Ollama au Transformers kwa maendeleo.  

#### Njia ya Kujifunza  
1. **Elewa Dhana za Msingi**: Soma uwezo wa multimodal na chaguo za utekelezaji.  
2. **Jaribu Aina Tofauti**: Jaribu saizi tofauti za mifano na matoleo maalum.  
3. **Fanya Mazoezi ya Utekelezaji**: Tekeleza mifano katika mazingira ya maendeleo.  
4. **Boresha kwa Uzalishaji**: Rekebisha kwa matumizi maalum na majukwaa.  

#### Mazoea Bora  
- **Anza Kidogo**: Anza na Gemma 3-4B kwa maendeleo ya awali na majaribio.  
- **Tumia Violezo Rasmi**: Tumia violezo sahihi vya mazungumzo kwa matokeo bora.  
- **Fuatilia Rasilimali**: Fuata matumizi ya kumbukumbu na utendaji wa uelekezi.  
- **Fikiria Utaalamu**: Chagua matoleo yanayofaa kwa mahitaji ya multimodal au simu.  

## Mifumo ya Matumizi ya Juu  

### Mifano ya Kurekebisha  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### Uhandisi Maalum wa Maelezo  

**Kwa Kazi za Multimodal:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**Kwa Kupiga Simu za Kazi na Muktadha:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### Matumizi ya Lugha Nyingi na Muktadha wa Kitamaduni  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### Mifumo ya Utekelezaji wa Uzalishaji  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## Mikakati ya Uboreshaji wa Utendaji  

### Uboreshaji wa Kumbukumbu  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### Uboreshaji wa Uelekezi  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## Mazoea Bora na Miongozo  

### Usalama na Faragha  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### Ufuatiliaji na Tathmini  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## Hitimisho  

Familia ya mifano ya Gemma inawakilisha mbinu ya kina ya Google ya kuleta teknolojia ya AI kwa kila mtu huku ikihifadhi utendaji wa ushindani katika matumizi mbalimbali na hali za utekelezaji. Kupitia kujitolea kwake kwa upatikanaji wa chanzo wazi, uwezo wa multimodal, na miundo ya ubunifu, Gemma inawawezesha mashirika na watengenezaji kutumia uwezo wa AI wenye nguvu bila kujali rasilimali zao au mahitaji maalum.  

### Mambo Muhimu  

**Ubora wa Chanzo Wazi**: Gemma inaonyesha kwamba mifano ya chanzo wazi inaweza kufikia utendaji unaoshindana na mbadala za wamiliki huku ikitoa uwazi, ubinafsishaji, na udhibiti wa utekelezaji wa AI.  

**Ubunifu wa Multimodal**: Ujumuishaji wa uwezo wa maandishi, picha, na sauti katika Gemma 3 na Gemma 3n unawakilisha maendeleo makubwa katika AI ya multimodal inayopatikana, ikiruhusu uelewa wa kina wa aina tofauti za pembejeo.  

**Miundo ya Kwanza ya Simu**: Teknolojia ya Per-Layer Embeddings (PLE) ya Gemma 3n na uboreshaji wa simu inaonyesha kwamba AI yenye nguvu inaweza kufanya kazi kwa ufanisi kwenye vifaa vilivyo na rasilimali chache bila kupoteza uwezo.  

**Utekelezaji Unaoweza Kupanuka**: Kiwango kutoka vigezo 1B hadi 27B, na matoleo maalum ya simu, huruhusu utekelezaji katika wigo mzima wa mazingira ya kompyuta huku ikihifadhi ubora na utendaji thabiti.  

**Ujumuishaji wa AI Wenye Uwajibikaji**: Hatua za usalama zilizojengwa kupitia ShieldGemma 2 na mazoea ya maendeleo yenye uwajibikaji zinahakikisha kwamba uwezo wa AI wenye nguvu unaweza kutekelezwa kwa usalama na kimaadili.  

### Mtazamo wa Baadaye  

Kadri familia ya Gemma inavyoendelea kubadilika, tunaweza kutarajia:  

**Uwezo wa Simu Ulioboreshwa**: Uboreshaji zaidi wa utekelezaji wa simu na ukingoni kwa ujumuishaji wa usanifu wa Gemma 3n katika majukwaa makubwa kama Android na Chrome.  

**Uelewa wa Multimodal Ulioongezeka**: Maendeleo endelevu katika ujumuishaji wa lugha-picha-sauti kwa uzoefu wa AI wa kina zaidi.  

**Ufanisi Ulioboreshwa**: Ubunifu wa usanifu unaoendelea ili kutoa uwiano bora wa utendaji kwa vigezo na mahitaji ya kompyuta yaliyopunguzwa.  

**Ujumuishaji Mpana wa Mfumo**: Msaada ulioboreshwa katika mifumo ya maendeleo, majukwaa ya wingu, na zana za utekelezaji kwa ujumuishaji rahisi katika mifumo iliyopo.  

**Ukuaji wa Jamii**: Upanuzi unaoendelea wa Gemmaverse na mifano, zana, na programu zilizoundwa na jamii zinazopanua uwezo wa msingi.  

### Hatua Zifuatazo  

Ikiwa unajenga programu za simu zenye uwezo wa AI wa wakati halisi, unaunda zana za elimu za multimodal, unaunda mifumo ya kiotomatiki yenye akili, au unafanya kazi kwenye programu za kimataifa zinazohitaji msaada wa lugha nyingi, familia ya Gemma inatoa suluhisho zinazoweza kupanuka na msaada mkubwa wa jamii na nyaraka za kina.  

**Mapendekezo ya Kuanza:**  
1. **Jaribu Google AI Studio** kwa uzoefu wa mikono mara moja.  
2. **Pakua mifano kutoka Hugging Face** kwa maendeleo ya ndani na ubinafsishaji.  
3. **Chunguza matoleo maalum** kama Gemma 3n kwa programu za simu.  
4. **Tekeleza uwezo wa multimodal** kwa uzoefu wa AI wa kina.  
5. **Fuata mazoea bora ya usalama** kwa utekelezaji wa uzalishaji.  

**Kwa Maendeleo ya Simu**: Anza na Gemma 3n E2B kwa utekelezaji wa rasilimali-ufanisi na uwezo wa sauti na picha.  

**Kwa Programu za Biashara**: Fikiria mifano ya Gemma 3-12B au 27B kwa uwezo wa juu na kupiga simu za kazi.  

**Kwa Matumizi ya Ulimwenguni**: Tumia msaada wa lugha 140+ wa Gemma na uhandisi wa maelezo unaozingatia kitamaduni.  

**Kwa Matumizi Maalum**: Chunguza mbinu za kurekebisha na mbinu za uboreshaji maalum wa kikoa.  

### 🔮 Demokrasia ya AI  

Familia ya Gemma inaonyesha mustakabali wa maendeleo ya AI ambapo mifano yenye nguvu na uwezo inapatikana kwa kila mtu kutoka kwa watengenezaji binafsi hadi mashirika makubwa. Kwa kuchanganya utafiti wa hali ya juu na upatikanaji wa chanzo wazi, Google imeunda msingi unaowezesha uvumbuzi katika sekta zote na mizani yote.  

Mafanikio ya Gemma na zaidi ya upakuaji milioni 100 na matoleo ya jamii 60,000+ yanaonyesha nguvu ya ushirikiano wa wazi katika kuendeleza teknolojia ya AI. Kadri tunavyoendelea, familia ya Gemma itaendelea kuwa kichocheo cha uvumbuzi wa AI, ikiruhusu maendeleo ya programu ambazo hapo awali zilikuwa zinawezekana tu kwa mifano ya wamiliki, ghali.  

Mustakabali wa AI ni wazi, unapatikana, na wenye nguvu – na familia ya Gemma inaongoza njia ya kufanya maono haya kuwa halisi.  

## Rasilimali za Ziada  

**Nyaraka Rasmi na Mifano:**  
- **Google AI Studio**: [Jaribu mifano ya Gemma moja kwa moja](https://aistudio.google.com)  
- **Makusanyo ya Hugging Face**:  
  - [Toleo la Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [Mapitio ya Gemma 3n](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **Nyaraka za Google AI Developer**: [Mwongozo wa kina wa Gemma](https://ai.google.dev/gemma)  
- **Nyaraka za Vertex AI**: [Miongozo ya utekelezaji wa biashara](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**Rasilimali za Kiufundi:**  
- **Karatasi za Utafiti na Ripoti za Kiufundi**: [Machapisho ya Google DeepMind](https://deepmind.google/models/gemma/)  
- **Machapisho ya Blogu ya Watengenezaji**: [Matangazo na mafunzo ya hivi karibuni](https://developers.googleblog.com)  
- **Kadi za Mfano**: Maelezo ya kiufundi na alama za utendaji.  

**Jamii na Msaada:**  
- **Jamii ya Hugging Face**: Mijadala hai na mifano ya jamii.  
- **Hifadhi za GitHub**: Utekelezaji wa chanzo wazi na zana.  
- **Majukwaa ya Watengenezaji**: Msaada wa jamii ya Google AI Developer.  
- **Stack Overflow**: Maswali yaliyotambulishwa na suluhisho za jamii.  

**Zana za Maendeleo:**  
- **Ollama**: [Utekelezaji rahisi wa ndani](https://ollama.ai)  
- **vLLM**: [Huduma ya utendaji wa juu](https://github.com/vllm-project/vllm)  
- **Maktaba ya Transformers**: [Ujumuishaji wa Hugging Face](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: Uboreshaji wa utekelezaji wa simu na ukingoni.  

**Njia za Kujifunza:**  
- **Mwanzo**: Anza na Google AI Studio → Mifano ya Hugging Face → Utekelezaji wa ndani.  
- **Mtengenezaji**: Ujumuishaji wa Transformers → Programu maalum → Utekelezaji wa uzalishaji.  
- **Mtafiti**: Karatasi za kiufundi → Kurekebisha → Programu mpya.  
- **Biashara**: Utekelezaji wa Vertex AI → Utekelezaji wa usalama → Uboreshaji wa mizani.  

Familia ya mifano ya Gemma inawakilisha si tu mkusanyiko wa mifano ya AI, bali mfumo mzima wa kujenga mustakabali wa programu za AI zinazopatikana, zenye nguvu, na zenye uwajibikaji. Anza kuchunguza leo na jiunge na jamii inayokua ya watengenezaji na watafiti wanaosukuma mipaka ya kile kinachowezekana na AI ya chanzo wazi.  

## Rasilimali za Ziada  

### Nyaraka Rasmi  
- Nyaraka za Kiufundi za Google Gemma  
- Kadi za Mfano na Miongozo ya Matumizi  
- Mwongozo wa Utekelezaji wa AI Wenye Uwajibikaji  
- Mwongozo wa Ujumuishaji wa Vertex AI wa Google  

### Zana za Maendeleo  
- Google AI Studio kwa utekelezaji wa wingu.  
- Hugging Face Transformers kwa ujumuishaji wa mifano.  
- vLLM kwa huduma ya utendaji wa juu.  
- Gemma.cpp kwa uelekezi ulioboreshwa wa CPU.  

### Rasilimali za Kujifunza  
- Karatasi za Kiufundi za

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya kutafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuhakikisha usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.