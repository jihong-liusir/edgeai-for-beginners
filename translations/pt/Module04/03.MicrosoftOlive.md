<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T13:28:31+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "pt"
}
-->
# Secção 3: Microsoft Olive Optimization Suite

## Índice
1. [Introdução](../../../Module04)
2. [O que é o Microsoft Olive?](../../../Module04)
3. [Instalação](../../../Module04)
4. [Guia Rápido](../../../Module04)
5. [Exemplo: Converter Qwen3 para ONNX INT4](../../../Module04)
6. [Uso Avançado](../../../Module04)
7. [Melhores Práticas](../../../Module04)
8. [Resolução de Problemas](../../../Module04)
9. [Recursos Adicionais](../../../Module04)

## Introdução

Microsoft Olive é uma ferramenta poderosa e fácil de usar para otimização de modelos, consciente do hardware, que simplifica o processo de otimização de modelos de aprendizagem automática para implementação em diferentes plataformas de hardware. Quer esteja a trabalhar com CPUs, GPUs ou aceleradores de IA especializados, Olive ajuda a alcançar o desempenho ideal enquanto mantém a precisão do modelo.

## O que é o Microsoft Olive?

Olive é uma ferramenta de otimização de modelos, consciente do hardware, que combina técnicas líderes da indústria em compressão, otimização e compilação de modelos. Funciona com ONNX Runtime como uma solução de otimização de inferência de ponta a ponta.

### Principais Funcionalidades

- **Otimização Consciente do Hardware**: Seleciona automaticamente as melhores técnicas de otimização para o hardware alvo
- **Mais de 40 Componentes de Otimização Integrados**: Inclui compressão de modelos, quantização, otimização de grafos e mais
- **Interface CLI Simples**: Comandos fáceis para tarefas comuns de otimização
- **Suporte Multi-Framework**: Compatível com PyTorch, modelos Hugging Face e ONNX
- **Suporte a Modelos Populares**: Olive pode otimizar automaticamente arquiteturas de modelos populares como Llama, Phi, Qwen, Gemma, entre outros

### Benefícios

- **Redução do Tempo de Desenvolvimento**: Não é necessário experimentar manualmente diferentes técnicas de otimização
- **Melhorias de Desempenho**: Aumentos significativos de velocidade (até 6x em alguns casos)
- **Implementação Multiplataforma**: Modelos otimizados funcionam em diferentes hardware e sistemas operativos
- **Manutenção da Precisão**: As otimizações preservam a qualidade do modelo enquanto melhoram o desempenho

## Instalação

### Pré-requisitos

- Python 3.8 ou superior
- Gestor de pacotes pip
- Ambiente virtual (recomendado)

### Instalação Básica

Crie e ative um ambiente virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instale o Olive com funcionalidades de auto-otimização:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependências Opcionais

Olive oferece várias dependências opcionais para funcionalidades adicionais:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificar Instalação

```bash
olive --help
```

Se for bem-sucedido, deverá ver a mensagem de ajuda do CLI do Olive.

## Guia Rápido

### Sua Primeira Otimização

Vamos otimizar um pequeno modelo de linguagem usando a funcionalidade de auto-otimização do Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### O que Este Comando Faz

O processo de otimização inclui: adquirir o modelo do cache local, capturar o grafo ONNX e armazenar os pesos num ficheiro de dados ONNX, otimizar o grafo ONNX e quantizar o modelo para int4 usando o método RTN.

### Explicação dos Parâmetros do Comando

- `--model_name_or_path`: Identificador do modelo Hugging Face ou caminho local
- `--output_path`: Diretório onde o modelo otimizado será guardado
- `--device`: Dispositivo alvo (cpu, gpu)
- `--provider`: Provedor de execução (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Usar ONNX Runtime Generate AI para inferência
- `--precision`: Precisão de quantização (int4, int8, fp16)
- `--log_level`: Verbosidade do registo (0=minimal, 1=detalhado)

## Exemplo: Converter Qwen3 para ONNX INT4

Com base no exemplo fornecido pela Hugging Face em [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), veja como otimizar um modelo Qwen3:

### Passo 1: Transferir Modelo (Opcional)

Para minimizar o tempo de transferência, faça cache apenas dos ficheiros essenciais:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Passo 2: Otimizar Modelo Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Passo 3: Testar o Modelo Otimizado

Crie um script Python simples para testar o modelo otimizado:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Estrutura de Saída

Após a otimização, o diretório de saída conterá:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Uso Avançado

### Ficheiros de Configuração

Para fluxos de trabalho de otimização mais complexos, pode usar ficheiros de configuração JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Executar com configuração:

```bash
olive run --config config.json
```

### Otimização para GPU

Para otimização com CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ajuste Fino com Olive

Olive também suporta ajuste fino de modelos:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Melhores Práticas

### 1. Seleção de Modelos
- Comece com modelos menores para testes (ex.: 0.5B-7B parâmetros)
- Certifique-se de que a arquitetura do modelo alvo é suportada pelo Olive

### 2. Considerações de Hardware
- Combine o alvo de otimização com o hardware de implementação
- Use otimização para GPU se tiver hardware compatível com CUDA
- Considere DirectML para máquinas Windows com gráficos integrados

### 3. Seleção de Precisão
- **INT4**: Máxima compressão, ligeira perda de precisão
- **INT8**: Bom equilíbrio entre tamanho e precisão
- **FP16**: Perda mínima de precisão, redução moderada de tamanho

### 4. Testes e Validação
- Teste sempre os modelos otimizados com os seus casos de uso específicos
- Compare métricas de desempenho (latência, throughput, precisão)
- Use dados de entrada representativos para avaliação

### 5. Otimização Iterativa
- Comece com auto-otimização para resultados rápidos
- Use ficheiros de configuração para controlo detalhado
- Experimente diferentes passes de otimização

## Resolução de Problemas

### Problemas Comuns

#### 1. Problemas de Instalação
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemas com CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemas de Memória
- Use tamanhos de lote menores durante a otimização
- Experimente quantização com maior precisão primeiro (int8 em vez de int4)
- Certifique-se de que há espaço suficiente em disco para cache de modelos

#### 4. Erros ao Carregar Modelos
- Verifique o caminho do modelo e as permissões de acesso
- Confirme se o modelo requer `trust_remote_code=True`
- Certifique-se de que todos os ficheiros necessários do modelo foram transferidos

### Obter Ajuda

- **Documentação**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemas no GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemplos**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Recursos Adicionais

### Links Oficiais
- **Repositório GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Documentação ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemplo Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemplos da Comunidade
- **Jupyter Notebooks**: Disponíveis no repositório GitHub do Olive
- **Extensão VS Code**: A extensão AI Toolkit usa Olive para otimização de modelos
- **Posts em Blog**: O blog Microsoft Open Source tem tutoriais detalhados sobre Olive

### Ferramentas Relacionadas
- **ONNX Runtime**: Motor de inferência de alto desempenho
- **Hugging Face Transformers**: Fonte de muitos modelos compatíveis
- **Azure Machine Learning**: Fluxos de trabalho de otimização baseados na nuvem

## ➡️ Próximos Passos

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, é importante ter em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autoritária. Para informações críticas, recomenda-se a tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes da utilização desta tradução.