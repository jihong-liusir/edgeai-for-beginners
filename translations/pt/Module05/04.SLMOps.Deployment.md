<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T13:42:29+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "pt"
}
-->
# Sec√ß√£o 4: Implementa√ß√£o de Modelos Prontos para Produ√ß√£o

## Vis√£o Geral

Este tutorial abrangente ir√° gui√°-lo atrav√©s do processo completo de implementa√ß√£o de modelos quantizados ajustados utilizando o Foundry Local. Vamos abordar a convers√£o do modelo, a otimiza√ß√£o por quantiza√ß√£o e a configura√ß√£o de implementa√ß√£o do in√≠cio ao fim.

## Pr√©-requisitos

Antes de come√ßar, certifique-se de ter o seguinte:

- ‚úÖ Um modelo onnx ajustado, pronto para implementa√ß√£o
- ‚úÖ Computador com Windows ou Mac
- ‚úÖ Python 3.10 ou superior
- ‚úÖ Pelo menos 8GB de RAM dispon√≠vel
- ‚úÖ Foundry Local instalado no seu sistema

## Parte 1: Configura√ß√£o do Ambiente

### Instala√ß√£o das Ferramentas Necess√°rias

Abra o seu terminal (Prompt de Comando no Windows, Terminal no Mac) e execute os seguintes comandos em sequ√™ncia:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Nota Importante**: Tamb√©m ser√° necess√°rio o CMake vers√£o 3.31 ou mais recente, que pode ser descarregado em [cmake.org](https://cmake.org/download/).

## Parte 2: Convers√£o e Quantiza√ß√£o do Modelo

### Escolher o Formato Adequado

Para modelos de linguagem pequenos ajustados, recomendamos o uso do formato **ONNX**, pois oferece:

- üöÄ Melhor otimiza√ß√£o de desempenho
- üîß Implementa√ß√£o independente de hardware
- üè≠ Capacidades prontas para produ√ß√£o
- üì± Compatibilidade entre plataformas

### M√©todo 1: Convers√£o com um √önico Comando (Recomendado)

Utilize o seguinte comando para converter diretamente o seu modelo ajustado:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Explica√ß√£o dos Par√¢metros:**
- `--model_name_or_path`: Caminho para o seu modelo ajustado
- `--device cpu`: Utilizar CPU para otimiza√ß√£o
- `--precision int4`: Usar quantiza√ß√£o INT4 (redu√ß√£o de tamanho de aproximadamente 75%)
- `--output_path`: Caminho de sa√≠da para o modelo convertido

### M√©todo 2: Abordagem com Ficheiro de Configura√ß√£o (Utilizadores Avan√ßados)

Crie um ficheiro de configura√ß√£o chamado `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Depois, execute:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Compara√ß√£o de Op√ß√µes de Quantiza√ß√£o

| Precis√£o | Tamanho do Ficheiro | Velocidade de Infer√™ncia | Qualidade do Modelo | Uso Recomendado |
|----------|----------------------|--------------------------|---------------------|-----------------|
| FP16     | Baseline √ó 0.5      | R√°pido                  | Melhor              | Hardware de alto desempenho |
| INT8     | Baseline √ó 0.25     | Muito R√°pido            | Bom                 | Escolha equilibrada |
| INT4     | Baseline √ó 0.125    | Mais R√°pido             | Aceit√°vel           | Recursos limitados |

üí° **Recomenda√ß√£o**: Comece com a quantiza√ß√£o INT4 para a sua primeira implementa√ß√£o. Se a qualidade n√£o for satisfat√≥ria, experimente INT8 ou FP16.

## Parte 3: Configura√ß√£o de Implementa√ß√£o no Foundry Local

### Criar a Configura√ß√£o do Modelo

Navegue at√© ao diret√≥rio de modelos do Foundry Local:

```bash
foundry cache cd ./models/
```

Crie a estrutura de diret√≥rios do seu modelo:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Crie o ficheiro de configura√ß√£o `inference_model.json` no diret√≥rio do seu modelo:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configura√ß√µes de Template Espec√≠ficas ao Modelo

#### Para Modelos da S√©rie Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Parte 4: Teste e Otimiza√ß√£o do Modelo

### Verificar a Instala√ß√£o do Modelo

Confirme se o Foundry Local consegue reconhecer o seu modelo:

```bash
foundry cache ls
```

Dever√° ver `your-finetuned-model-int4` na lista.

### Iniciar o Teste do Modelo

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking de Desempenho

Monitore m√©tricas-chave durante os testes:

1. **Tempo de Resposta**: Me√ßa o tempo m√©dio por resposta
2. **Uso de Mem√≥ria**: Monitore o consumo de RAM
3. **Utiliza√ß√£o da CPU**: Verifique a carga do processador
4. **Qualidade da Sa√≠da**: Avalie a relev√¢ncia e a coer√™ncia das respostas

### Lista de Verifica√ß√£o para Valida√ß√£o da Qualidade

- ‚úÖ O modelo responde adequadamente a consultas do dom√≠nio ajustado
- ‚úÖ O formato da resposta corresponde √† estrutura de sa√≠da esperada
- ‚úÖ N√£o h√° fugas de mem√≥ria durante o uso prolongado
- ‚úÖ Desempenho consistente com diferentes comprimentos de entrada
- ‚úÖ Tratamento adequado de casos extremos e entradas inv√°lidas

## Resumo

Parab√©ns! Concluiu com sucesso:

- ‚úÖ Convers√£o do formato do modelo ajustado
- ‚úÖ Otimiza√ß√£o por quantiza√ß√£o do modelo
- ‚úÖ Configura√ß√£o de implementa√ß√£o no Foundry Local
- ‚úÖ Ajuste de desempenho e resolu√ß√£o de problemas

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.