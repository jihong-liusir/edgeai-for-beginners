<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "48d0fb38be925084a6ebd957d4b045e5",
  "translation_date": "2025-10-08T20:48:36+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "pt"
}
-->
# EdgeAI para Iniciantes - Workshop

> **Percurso Pr√°tico para Constru√ß√£o de Aplica√ß√µes Edge AI Prontas para Produ√ß√£o**
>
> Domine a implementa√ß√£o de IA local com Microsoft Foundry Local, desde a primeira conclus√£o de chat at√© a orquestra√ß√£o de m√∫ltiplos agentes em 6 sess√µes progressivas.

---

## üéØ Introdu√ß√£o

Bem-vindo ao **Workshop EdgeAI para Iniciantes** - o seu guia pr√°tico e pr√°tico para construir aplica√ß√µes inteligentes que funcionam inteiramente em hardware local. Este workshop transforma conceitos te√≥ricos de Edge AI em compet√™ncias reais atrav√©s de exerc√≠cios progressivamente desafiantes utilizando Microsoft Foundry Local e Modelos de Linguagem Pequenos (SLMs).

### Porqu√™ Este Workshop?

**A Revolu√ß√£o Edge AI J√° Chegou**

Organiza√ß√µes em todo o mundo est√£o a mudar de IA dependente da cloud para computa√ß√£o na edge por tr√™s raz√µes cr√≠ticas:

1. **Privacidade e Conformidade** - Processar dados sens√≠veis localmente sem transmiss√£o para a cloud (HIPAA, GDPR, regulamentos financeiros)
2. **Desempenho** - Eliminar lat√™ncia de rede (50-500ms local vs 500-2000ms ida e volta na cloud)
3. **Controlo de Custos** - Remover custos por token de API e escalar sem despesas na cloud

**Mas Edge AI √© Diferente**

Executar IA localmente requer novas compet√™ncias:
- Sele√ß√£o e otimiza√ß√£o de modelos para restri√ß√µes de recursos
- Gest√£o de servi√ßos locais e acelera√ß√£o de hardware
- Engenharia de prompts para modelos menores
- Padr√µes de implementa√ß√£o em produ√ß√£o para dispositivos edge

**Este Workshop Ensina Essas Compet√™ncias**

Em 6 sess√µes focadas (~3 horas no total), ir√° progredir de "Hello World" at√© √† implementa√ß√£o de sistemas multi-agentes prontos para produ√ß√£o - tudo a funcionar localmente na sua m√°quina.

---

## üìö Objetivos de Aprendizagem

Ao completar este workshop, ser√° capaz de:

### Compet√™ncias Principais
1. **Implementar e Gerir Servi√ßos de IA Locais**
   - Instalar e configurar o Microsoft Foundry Local
   - Selecionar modelos apropriados para implementa√ß√£o na edge
   - Gerir o ciclo de vida dos modelos (download, carregamento, cache)
   - Monitorizar o uso de recursos e otimizar o desempenho

2. **Construir Aplica√ß√µes com IA**
   - Implementar conclus√µes de chat compat√≠veis com OpenAI localmente
   - Projetar prompts eficazes para Modelos de Linguagem Pequenos
   - Lidar com respostas em streaming para uma melhor experi√™ncia de utilizador
   - Integrar modelos locais em aplica√ß√µes existentes

3. **Criar Sistemas RAG (Gera√ß√£o Aumentada por Recupera√ß√£o)**
   - Construir pesquisa sem√¢ntica com embeddings
   - Basear respostas de LLM em conhecimento espec√≠fico do dom√≠nio
   - Avaliar a qualidade de RAG com m√©tricas padr√£o da ind√∫stria
   - Escalar de prot√≥tipo para produ√ß√£o

4. **Otimizar o Desempenho de Modelos**
   - Testar m√∫ltiplos modelos para o seu caso de uso
   - Medir lat√™ncia, throughput e tempo do primeiro token
   - Selecionar modelos √≥timos com base em trade-offs de velocidade/qualidade
   - Comparar trade-offs entre SLM e LLM em cen√°rios reais

5. **Orquestrar Sistemas Multi-Agentes**
   - Projetar agentes especializados para diferentes tarefas
   - Implementar mem√≥ria de agentes e gest√£o de contexto
   - Coordenar agentes em fluxos de trabalho complexos
   - Roteamento inteligente de pedidos entre m√∫ltiplos modelos

6. **Implementar Solu√ß√µes Prontas para Produ√ß√£o**
   - Implementar l√≥gica de tratamento de erros e tentativas
   - Monitorizar o uso de tokens e recursos do sistema
   - Construir arquiteturas escal√°veis com padr√µes de modelo como ferramentas
   - Planear caminhos de migra√ß√£o de edge para h√≠brido (edge + cloud)

---

## üéì Resultados de Aprendizagem

### O Que Ir√° Construir

No final deste workshop, ter√° criado:

| Sess√£o | Entreg√°vel | Compet√™ncias Demonstradas |
|--------|------------|---------------------------|
| **1** | Aplica√ß√£o de chat com streaming | Configura√ß√£o de servi√ßo, conclus√µes b√°sicas, UX de streaming |
| **2** | Sistema RAG com avalia√ß√£o | Embeddings, pesquisa sem√¢ntica, m√©tricas de qualidade |
| **3** | Suite de benchmarking multi-modelo | Medi√ß√£o de desempenho, compara√ß√£o de modelos |
| **4** | Comparador SLM vs LLM | An√°lise de trade-offs, estrat√©gias de otimiza√ß√£o |
| **5** | Orquestrador multi-agente | Design de agentes, gest√£o de mem√≥ria, coordena√ß√£o |
| **6** | Sistema de roteamento inteligente | Dete√ß√£o de inten√ß√£o, sele√ß√£o de modelos, escalabilidade |

### Matriz de Compet√™ncias

| N√≠vel de Compet√™ncia | Sess√£o 1-2 | Sess√£o 3-4 | Sess√£o 5-6 |
|-----------------------|------------|------------|------------|
| **Iniciante**         | ‚úÖ Configura√ß√£o & b√°sicos | ‚ö†Ô∏è Desafiador | ‚ùå Demasiado avan√ßado |
| **Interm√©dio**        | ‚úÖ Revis√£o r√°pida | ‚úÖ Aprendizagem principal | ‚ö†Ô∏è Objetivos ambiciosos |
| **Avan√ßado**          | ‚úÖ F√°cil de completar | ‚úÖ Refinamento | ‚úÖ Padr√µes de produ√ß√£o |

### Compet√™ncias Preparadas para a Carreira

**Ap√≥s este workshop, estar√° preparado para:**

‚úÖ **Construir Aplica√ß√µes com Foco na Privacidade**
- Aplica√ß√µes de sa√∫de que lidam com PHI/PII localmente
- Servi√ßos financeiros com requisitos de conformidade
- Sistemas governamentais com necessidades de soberania de dados

‚úÖ **Otimizar para Ambientes Edge**
- Dispositivos IoT com recursos limitados
- Aplica√ß√µes m√≥veis offline-first
- Sistemas em tempo real de baixa lat√™ncia

‚úÖ **Projetar Arquiteturas Inteligentes**
- Sistemas multi-agentes para fluxos de trabalho complexos
- Implementa√ß√µes h√≠bridas edge-cloud
- Infraestrutura de IA otimizada para custos

‚úÖ **Liderar Iniciativas de Edge AI**
- Avaliar a viabilidade de Edge AI para projetos
- Selecionar modelos e frameworks apropriados
- Arquitetar solu√ß√µes de IA locais escal√°veis

---

## üó∫Ô∏è Estrutura do Workshop

### Vis√£o Geral das Sess√µes (6 Sess√µes √ó 30 Minutos = 3 Horas)

| Sess√£o | T√≥pico | Foco | Dura√ß√£o |
|--------|--------|------|---------|
| **1** | Introdu√ß√£o ao Foundry Local | Instalar, validar, primeiras conclus√µes | 30 min |
| **2** | Constru√ß√£o de Solu√ß√µes de IA com RAG | Engenharia de prompts, embeddings, avalia√ß√£o | 30 min |
| **3** | Modelos Open Source | Descoberta de modelos, benchmarking, sele√ß√£o | 30 min |
| **4** | Modelos de √öltima Gera√ß√£o | SLM vs LLM, otimiza√ß√£o, frameworks | 30 min |
| **5** | Agentes com IA | Design de agentes, orquestra√ß√£o, mem√≥ria | 30 min |
| **6** | Modelos como Ferramentas | Roteamento, encadeamento, estrat√©gias de escalabilidade | 30 min |

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

**Requisitos de Sistema:**
- **OS**: Windows 10/11, macOS 11+, ou Linux (Ubuntu 20.04+)
- **RAM**: M√≠nimo de 8GB, recomendado 16GB+
- **Armazenamento**: 10GB+ de espa√ßo livre para modelos
- **CPU**: Processador moderno com suporte AVX2
- **GPU** (opcional): Compat√≠vel com CUDA ou Qualcomm NPU para acelera√ß√£o

**Requisitos de Software:**
- **Python 3.8+** ([Download](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Guia de Instala√ß√£o](../../../Workshop))
- **Git** ([Download](https://git-scm.com/downloads))
- **Visual Studio Code** (recomendado) ([Download](https://code.visualstudio.com/))

### Configura√ß√£o em 3 Passos

#### 1. Instalar Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verificar Instala√ß√£o:**
```bash
foundry --version
foundry service status
```

#### 2. Clonar Reposit√≥rio e Instalar Depend√™ncias

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Executar o Primeiro Exemplo

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Sucesso!** Deve ver uma resposta em streaming sobre Edge AI.

---

## üì¶ Recursos do Workshop

### Exemplos em Python

Exemplos pr√°ticos progressivos demonstrando cada conceito:

| Sess√£o | Exemplo | Descri√ß√£o | Tempo de Execu√ß√£o |
|--------|---------|-----------|-------------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat b√°sico & streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG com embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | Avalia√ß√£o de qualidade RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Benchmarking multi-modelo | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Compara√ß√£o SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Sistema multi-agente | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Roteamento baseado em inten√ß√£o | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Pipeline multi-etapas | ~60s |

### Notebooks Jupyter

Explora√ß√£o interativa com explica√ß√µes e visualiza√ß√µes:

| Sess√£o | Notebook | Descri√ß√£o | Dificuldade |
|--------|----------|-----------|-------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | B√°sicos de chat & streaming | ‚≠ê Iniciante |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Construir sistema RAG | ‚≠ê‚≠ê Interm√©dio |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Avaliar qualidade RAG | ‚≠ê‚≠ê Interm√©dio |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Benchmarking de modelos | ‚≠ê‚≠ê Interm√©dio |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Compara√ß√£o de modelos | ‚≠ê‚≠ê Interm√©dio |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orquestra√ß√£o de agentes | ‚≠ê‚≠ê‚≠ê Avan√ßado |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Roteamento por inten√ß√£o | ‚≠ê‚≠ê‚≠ê Avan√ßado |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orquestra√ß√£o de pipeline | ‚≠ê‚≠ê‚≠ê Avan√ßado |

### Documenta√ß√£o

Guias e refer√™ncias abrangentes:

| Documento | Descri√ß√£o | Quando Usar |
|-----------|-----------|-------------|
| [QUICK_START.md](./QUICK_START.md) | Guia de configura√ß√£o r√°pida | Come√ßando do zero |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Resumo de comandos & APIs | Precisa de respostas r√°pidas |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Padr√µes & exemplos de SDK | Escrevendo c√≥digo |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Guia de vari√°veis de ambiente | Configurando exemplos |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Melhorias recentes nos exemplos | Compreendendo mudan√ßas |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Guia de migra√ß√£o | Atualizando c√≥digo |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Problemas comuns & solu√ß√µes | Depurando problemas |

---

## üéì Recomenda√ß√µes de Percurso de Aprendizagem

### Para Iniciantes (3-4 horas)
1. ‚úÖ Sess√£o 1: Introdu√ß√£o (foco na configura√ß√£o e chat b√°sico)
2. ‚úÖ Sess√£o 2: B√°sicos de RAG (ignorar avalia√ß√£o inicialmente)
3. ‚úÖ Sess√£o 3: Benchmarking simples (apenas 2 modelos)
4. ‚è≠Ô∏è Ignorar Sess√µes 4-6 por agora
5. üîÑ Retornar √†s Sess√µes 4-6 ap√≥s construir a primeira aplica√ß√£o

### Para Desenvolvedores Interm√©dios (3 horas)
1. ‚ö° Sess√£o 1: Valida√ß√£o r√°pida da configura√ß√£o
2. ‚úÖ Sess√£o 2: Pipeline completo de RAG com avalia√ß√£o
3. ‚úÖ Sess√£o 3: Suite completa de benchmarking
4. ‚úÖ Sess√£o 4: Otimiza√ß√£o de modelos
5. ‚úÖ Sess√µes 5-6: Foco em padr√µes de arquitetura

### Para Praticantes Avan√ßados (2-3 horas)
1. ‚ö° Sess√µes 1-3: Revis√£o r√°pida e valida√ß√£o
2. ‚úÖ Sess√£o 4: Mergulho profundo na otimiza√ß√£o
3. ‚úÖ Sess√£o 5: Arquitetura multi-agente
4. ‚úÖ Sess√£o 6: Padr√µes de produ√ß√£o e escalabilidade
5. üöÄ Extens√£o: Construir l√≥gica de roteamento personalizada e implementa√ß√µes h√≠bridas

---

## Pacote de Sess√µes do Workshop (Laborat√≥rios Focados de 30 Minutos)

Se estiver a seguir o formato condensado de 6 sess√µes do workshop, utilize estes guias dedicados (cada um complementa os m√≥dulos mais amplos acima):

| Sess√£o do Workshop | Guia | Foco Principal |
|--------------------|------|----------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Instalar, validar, executar phi & GPT-OSS-20B, acelera√ß√£o |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Engenharia de prompts, padr√µes RAG, grounding em CSV & documentos, migra√ß√£o |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Integra√ß√£o com Hugging Face, benchmarking, sele√ß√£o de modelos |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, acelera√ß√£o ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Fun√ß√µes de agentes, mem√≥ria, ferramentas, orquestra√ß√£o |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Roteamento, encadeamento, caminho de escalabilidade para Azure |
Cada ficheiro de sess√£o inclui: resumo, objetivos de aprendizagem, fluxo de demonstra√ß√£o de 30 minutos, projeto inicial, lista de verifica√ß√£o de valida√ß√£o, resolu√ß√£o de problemas e refer√™ncias ao SDK oficial Foundry Local Python.

### Scripts de Exemplo

Instalar depend√™ncias do workshop (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Se estiver a executar o servi√ßo Foundry Local numa m√°quina ou VM diferente (Windows) a partir de macOS, exporte o endpoint:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sess√£o | Script(s) | Descri√ß√£o |
|--------|-----------|-----------|
| 1 | `samples/session01/chat_bootstrap.py` | Servi√ßo inicial e chat em streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG m√≠nimo (embeddings em mem√≥ria) |
|   | `samples/session02/rag_eval_ragas.py` | Avalia√ß√£o RAG com m√©tricas ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmarking de lat√™ncia e throughput multi-modelo |
| 4 | `samples/session04/model_compare.py` | Compara√ß√£o SLM vs LLM (lat√™ncia e sa√≠da de exemplo) |
| 5 | `samples/session05/agents_orchestrator.py` | Pipeline de investiga√ß√£o ‚Üí editorial com dois agentes |
| 6 | `samples/session06/models_router.py` | Demonstra√ß√£o de routing baseado em inten√ß√µes |
|   | `samples/session06/models_pipeline.py` | Cadeia de planeamento/execu√ß√£o/refinamento em m√∫ltiplos passos |

### Vari√°veis de Ambiente (Comuns a Todos os Exemplos)

| Vari√°vel | Finalidade | Exemplo |
|----------|------------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Alias padr√£o de modelo √∫nico para exemplos b√°sicos | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Modelo SLM expl√≠cito vs modelo maior para compara√ß√£o | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Lista de aliases para benchmarking | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Repeti√ß√µes de benchmark por modelo | `3` |
| `BENCH_PROMPT` | Prompt usado no benchmarking | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Modelo de embeddings de sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Substituir consulta de teste para pipeline RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Substituir consulta para pipeline de agentes | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias de modelo para agente de investiga√ß√£o | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias de modelo para agente editor (pode ser diferente) | `gpt-oss-20b` |
| `SHOW_USAGE` | Quando `1`, imprime uso de tokens por conclus√£o | `1` |
| `RETRY_ON_FAIL` | Quando `1`, tenta novamente em caso de erros transit√≥rios no chat | `1` |
| `RETRY_BACKOFF` | Segundos de espera antes de tentar novamente | `1.0` |

Se uma vari√°vel n√£o estiver definida, os scripts utilizam valores padr√£o sensatos. Para demonstra√ß√µes de modelo √∫nico, normalmente s√≥ precisa de `FOUNDRY_LOCAL_ALIAS`.

### M√≥dulo de Utilidade

Todos os exemplos agora partilham um helper `samples/workshop_utils.py` que fornece:

* Cria√ß√£o de cliente OpenAI + `FoundryLocalManager` com cache
* Helper `chat_once()` com op√ß√£o de retry + impress√£o de uso
* Relat√≥rios simples de uso de tokens (ativar via `SHOW_USAGE=1`)

Isto reduz duplica√ß√£o e destaca boas pr√°ticas para orquestra√ß√£o eficiente de modelos locais.

## Melhorias Opcionais (Entre Sess√µes)

| Tema | Melhoria | Sess√µes | Env / Altern√¢ncia |
|------|----------|---------|-------------------|
| Determinismo | Temperatura fixa + conjuntos de prompts est√°veis | 1‚Äì6 | Definir `temperature=0`, `top_p=1` |
| Visibilidade de Uso de Tokens | Ensino consistente de custo/efici√™ncia | 1‚Äì6 | `SHOW_USAGE=1` |
| Primeiro Token em Streaming | M√©trica de lat√™ncia percebida | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| Resili√™ncia a Retries | Lida com cold-start transit√≥rio | Todas | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Agentes Multi-Modelo | Especializa√ß√£o de pap√©is heterog√©neos | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Routing Adaptativo | Heur√≠sticas de inten√ß√£o + custo | 6 | Expandir router com l√≥gica de escalonamento |
| Mem√≥ria Vetorial | Recall sem√¢ntico de longo prazo | 2,5,6 | Integrar √≠ndice de embeddings FAISS/Chroma |
| Exporta√ß√£o de Tra√ßos | Auditoria e avalia√ß√£o | 2,5,6 | Adicionar linhas JSON por passo |
| Rubricas de Qualidade | Acompanhamento qualitativo | 3‚Äì6 | Prompts de pontua√ß√£o secund√°ria |
| Testes de Fumo | Valida√ß√£o r√°pida pr√©-workshop | Todas | `python Workshop/tests/smoke.py` |

### In√≠cio R√°pido Determin√≠stico

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Espere contagens de tokens est√°veis em entradas id√™nticas repetidas.

### Avalia√ß√£o RAG (Sess√£o 2)

Utilize `rag_eval_ragas.py` para calcular relev√¢ncia da resposta, fidelidade e precis√£o de contexto num pequeno conjunto de dados sint√©tico:

```powershell
python samples/session02/rag_eval_ragas.py
```

Expanda fornecendo um JSONL maior de perguntas, contextos e verdades base, convertendo depois para um `Dataset` do Hugging Face.

## Ap√™ndice de Precis√£o de Comandos CLI

O workshop utiliza deliberadamente apenas comandos CLI Foundry Local atualmente documentados/est√°veis.

### Comandos Est√°veis Referenciados

| Categoria | Comando | Finalidade |
|-----------|---------|------------|
| Core | `foundry --version` | Mostrar vers√£o instalada |
| Core | `foundry init` | Inicializar configura√ß√£o |
| Servi√ßo | `foundry service start` | Iniciar servi√ßo local (se n√£o autom√°tico) |
| Servi√ßo | `foundry status` | Mostrar estado do servi√ßo |
| Modelos | `foundry model list` | Listar cat√°logo / modelos dispon√≠veis |
| Modelos | `foundry model download <alias>` | Transferir pesos do modelo para cache |
| Modelos | `foundry model run <alias>` | Lan√ßar (carregar) um modelo localmente; combinar com `--prompt` para one-shot |
| Modelos | `foundry model unload <alias>` / `foundry model stop <alias>` | Descarregar um modelo da mem√≥ria (se suportado) |
| Cache | `foundry cache list` | Listar modelos em cache (transferidos) |
| Sistema | `foundry system info` | Snapshot de capacidades de hardware e acelera√ß√£o |
| Sistema | `foundry system gpu-info` | Informa√ß√µes de diagn√≥stico de GPU |
| Configura√ß√£o | `foundry config list` | Mostrar valores de configura√ß√£o atuais |
| Configura√ß√£o | `foundry config set <key> <value>` | Atualizar configura√ß√£o |

### Padr√£o de Prompt One-Shot

Em vez de um subcomando `model chat` descontinuado, utilize:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Isto executa um ciclo √∫nico de prompt/resposta e depois sai.

### Padr√µes Removidos / Evitados

| Descontinuado / N√£o Documentado | Substitui√ß√£o / Orienta√ß√£o |
|---------------------------------|---------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Utilizar `foundry model list` simples + atividade recente / logs |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Utilizar script de benchmark Python + ferramentas do SO (Task Manager / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking e Telemetria

- Lat√™ncia, p95, tokens/segundo: `samples/session03/benchmark_oss_models.py`
- Lat√™ncia do primeiro token (streaming): definir `BENCH_STREAM=1`
- Uso de recursos: monitores do SO (Task Manager, Activity Monitor, `nvidia-smi`) + `foundry system info`.

√Ä medida que novos comandos de telemetria CLI estabilizam, podem ser incorporados com edi√ß√µes m√≠nimas nos markdowns das sess√µes.

### Linter Autom√°tico de CLI

Um linter autom√°tico impede a reintrodu√ß√£o de padr√µes CLI descontinuados dentro de blocos de c√≥digo delimitados em ficheiros markdown:

Script: `Workshop/scripts/lint_markdown_cli.py`

Padr√µes descontinuados s√£o bloqueados dentro de blocos de c√≥digo.

Substitui√ß√µes recomendadas:
| Descontinuado | Substitui√ß√£o |
|---------------|-------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Script de benchmark + ferramentas do sistema |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Executar localmente:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` executa em cada push e PR.

Hook opcional de pr√©-commit:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Tabela R√°pida de Migra√ß√£o CLI ‚Üí SDK

| Tarefa | Comando CLI | Equivalente SDK (Python) | Notas |
|-------|-------------|--------------------------|-------|
| Executar um modelo uma vez (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | O SDK inicializa automaticamente o servi√ßo e o cache |
| Transferir (cache) modelo | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | O Manager escolhe a melhor variante se o alias mapear para m√∫ltiplas builds |
| Listar cat√°logo | `foundry model list` | `# use manager for each alias or maintain known list` | CLI agrega; SDK atualmente por inst√¢ncia de alias |
| Listar modelos em cache | `foundry cache list` | `manager.list_cached_models()` | Ap√≥s inicializa√ß√£o do manager (qualquer alias) |
| Ativar acelera√ß√£o GPU | `foundry config set compute.onnx.enable_gpu true` | `# CLI action; SDK assumes config already applied` | Configura√ß√£o √© efeito externo |
| Obter URL do endpoint | (impl√≠cito) | `manager.endpoint` | Usado para criar cliente compat√≠vel com OpenAI |
| Aquecer um modelo | `foundry model run <alias>` e depois primeiro prompt | `chat_once(alias, messages=[...])` (utilit√°rio) | Utilit√°rios lidam com aquecimento inicial de lat√™ncia fria |
| Medir lat√™ncia | `python benchmark_oss_models.py` | `import benchmark_oss_models` (ou novo script de exporta√ß√£o) | Preferir script para m√©tricas consistentes |
| Parar / descarregar modelo | `foundry model unload <alias>` | (N√£o exposto ‚Äì reiniciar servi√ßo/processo) | Normalmente n√£o necess√°rio para fluxo de workshop |
| Recuperar uso de tokens | (ver sa√≠da) | `resp.usage.total_tokens` | Fornecido se o backend retornar objeto de uso |

## Exporta√ß√£o Markdown de Benchmark

Utilize o script `Workshop/scripts/export_benchmark_markdown.py` para executar um benchmark fresco (mesma l√≥gica que `samples/session03/benchmark_oss_models.py`) e emitir uma tabela Markdown amig√°vel para GitHub mais JSON bruto.

### Exemplo

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Ficheiros gerados:
| Ficheiro | Conte√∫do |
|----------|----------|
| `benchmark_report.md` | Tabela Markdown + dicas de interpreta√ß√£o |
| `benchmark_report.json` | Array de m√©tricas bruto (para compara√ß√£o/detec√ß√£o de tend√™ncias) |

Defina `BENCH_STREAM=1` no ambiente para incluir lat√™ncia do primeiro token, se suportado.

---

**Aviso**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.