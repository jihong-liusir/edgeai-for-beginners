<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T12:53:40+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "pt"
}
-->
# Sessão 3: Modelos Open-Source com Foundry Local

## Visão Geral

Nesta sessão, exploramos como trazer modelos open-source para o Foundry Local: selecionar modelos da comunidade, integrar conteúdos do Hugging Face e adotar estratégias de “traga o seu próprio modelo” (BYOM). Também descobrirá a série Model Mondays para aprendizagem contínua e descoberta de novos modelos.

Referências:
- Documentação do Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compilar modelos do Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local no GitHub: https://github.com/microsoft/Foundry-Local

## Objetivos de Aprendizagem
- Descobrir e avaliar modelos open-source para inferência local
- Compilar e executar modelos selecionados do Hugging Face no Foundry Local
- Aplicar estratégias de seleção de modelos considerando precisão, latência e necessidades de recursos
- Gerir modelos localmente com cache e controlo de versões

## Parte 1: Descoberta e Seleção de Modelos (Passo a passo)

Passo 1) Liste os modelos disponíveis no catálogo local  
```cmd
foundry model list
```
  
Passo 2) Teste rápido de dois candidatos (download automático na primeira execução)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Passo 3) Registe métricas básicas  
- Observe a latência (subjetiva) e a qualidade para um prompt fixo  
- Monitore o uso de memória através do Gestor de Tarefas enquanto cada modelo é executado  

## Parte 2: Executar Modelos do Catálogo via CLI (Passo a passo)

Passo 1) Inicie um modelo  
```cmd
foundry model run llama-3.2
```
  
Passo 2) Envie um prompt de teste através do endpoint compatível com OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Parte 3: BYOM – Compilar Modelos do Hugging Face (Passo a passo)

Siga o guia oficial para compilar modelos. Fluxo geral abaixo — consulte o artigo da Microsoft Learn para comandos exatos e configurações suportadas.

Passo 1) Prepare um diretório de trabalho  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Passo 2) Compile um modelo HF suportado  
- Utilize os passos do guia para converter e colocar o modelo ONNX compilado no diretório `models`  
- Confirme com:  
```cmd
foundry cache ls
```
  
Deverá ver o nome do modelo compilado (por exemplo, `llama-3.2`).  

Passo 3) Execute o modelo compilado  
```cmd
foundry model run llama-3.2 --verbose
```
  
Notas:  
- Certifique-se de que há espaço suficiente em disco e RAM para compilar e executar  
- Comece com modelos menores para validar o fluxo, depois escale gradualmente  

## Parte 4: Curadoria Prática de Modelos (Passo a passo)

Passo 1) Crie um registo `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Passo 2) Script simples de seleção  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Parte 5: Benchmarks Práticos (Passo a passo)

Passo 1) Benchmark simples de latência  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Passo 2) Verificação de qualidade  
- Utilize um conjunto fixo de prompts, capture os resultados num ficheiro CSV/JSON  
- Avalie manualmente fluência, relevância e correção (1–5)  

## Parte 6: Próximos Passos
- Subscreva o Model Mondays para novos modelos e dicas: https://aka.ms/model-mondays  
- Contribua com descobertas para o `models.json` da sua equipa  
- Prepare-se para a Sessão 4: comparação entre LLMs e SLMs, inferência local vs na cloud, e demonstrações práticas  

---

