<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-10-01T00:05:18+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "pt"
}
-->
# Sessão 3: Descoberta e Gestão de Modelos Open-Source

## Visão Geral

Esta sessão foca na descoberta prática de modelos e na gestão com o Foundry Local. Vais aprender a listar modelos disponíveis, testar diferentes opções e compreender características básicas de desempenho. A abordagem enfatiza a exploração prática com o CLI do Foundry para te ajudar a selecionar os modelos certos para os teus casos de uso.

## Objetivos de Aprendizagem

- Dominar os comandos do CLI do Foundry para descoberta e gestão de modelos
- Compreender padrões de cache de modelos e armazenamento local
- Aprender a testar e comparar rapidamente diferentes modelos
- Estabelecer fluxos de trabalho práticos para seleção e benchmarking de modelos
- Explorar o ecossistema crescente de modelos disponíveis através do Foundry Local

## Pré-requisitos

- Sessão 1 concluída: Introdução ao Foundry Local
- CLI do Foundry Local instalado e acessível
- Espaço de armazenamento suficiente para downloads de modelos (os modelos podem variar entre 1GB e mais de 20GB)
- Compreensão básica dos tipos de modelos e casos de uso

## Parte 6: Exercício Prático

### Exercício: Descoberta e Comparação de Modelos

Cria o teu próprio script de avaliação de modelos com base no Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### A Tua Tarefa

1. **Executa o script Sample 03**: `samples\03\list_and_bench.cmd`
2. **Experimenta diferentes modelos**: Testa pelo menos 3 modelos diferentes
3. **Compara o desempenho**: Nota as diferenças na velocidade e qualidade de resposta
4. **Documenta os resultados**: Cria um gráfico simples de comparação

### Exemplo de Formato de Comparação

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Parte 7: Resolução de Problemas e Melhores Práticas

### Problemas Comuns e Soluções

**O Modelo Não Inicia:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Memória Insuficiente:**
- Começa com modelos menores (`phi-4-mini`)
- Fecha outras aplicações
- Faz upgrade à RAM se frequentemente atingires limites

**Desempenho Lento:**
- Certifica-te de que o modelo está totalmente carregado (verifica a saída detalhada)
- Fecha aplicações desnecessárias em segundo plano
- Considera armazenamento mais rápido (SSD)

### Melhores Práticas

1. **Começa Pequeno**: Inicia com `phi-4-mini` para validar a configuração
2. **Um Modelo de Cada Vez**: Para modelos anteriores antes de iniciar novos
3. **Monitoriza Recursos**: Mantém atenção ao uso de memória
4. **Testa Consistentemente**: Usa os mesmos prompts para comparações justas
5. **Documenta Resultados**: Mantém notas sobre o desempenho dos modelos para os teus casos de uso

## Parte 8: Próximos Passos e Referências

### Preparação para a Sessão 4

- **Foco da Sessão 4**: Ferramentas e técnicas de otimização
- **Pré-requisitos**: Conforto com troca de modelos e testes básicos de desempenho
- **Recomendado**: Identificar 2-3 modelos favoritos desta sessão

### Recursos Adicionais

- **[Documentação do Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Documentação oficial
- **[Referência CLI](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Referência completa de comandos
- **[Model Mondays](https://aka.ms/model-mondays)**: Destaques semanais de modelos
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Comunidade e problemas
- **[Sample 03: Descoberta de Modelos](samples/03/README.md)**: Exemplo prático de script

### Principais Conclusões

✅ **Descoberta de Modelos**: Usa `foundry model list` para explorar modelos disponíveis  
✅ **Testes Rápidos**: O padrão `list_and_bench.cmd` para avaliação rápida  
✅ **Monitorização de Desempenho**: Medição básica de uso de recursos e tempo de resposta  
✅ **Seleção de Modelos**: Diretrizes práticas para escolher modelos por caso de uso  
✅ **Gestão de Cache**: Compreensão de armazenamento e procedimentos de limpeza  

Agora tens as competências práticas para descobrir, testar e selecionar modelos apropriados para as tuas aplicações de IA usando a abordagem simples do CLI do Foundry Local.

## Objetivos de Aprendizagem

- Descobrir e avaliar modelos open-source para inferência local
- Compilar e executar modelos selecionados do Hugging Face no Foundry Local
- Aplicar estratégias de seleção de modelos para precisão, latência e necessidades de recursos
- Gerir modelos localmente com cache e versionamento

## Parte 1: Descoberta de Modelos com o Foundry CLI

### Comandos Básicos de Gestão de Modelos

O CLI do Foundry fornece comandos simples para descoberta e gestão de modelos:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Executar os Teus Primeiros Modelos

Começa com modelos populares e bem testados para compreender características de desempenho:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Nota:** A flag `--verbose` fornece informações detalhadas de inicialização, incluindo:
- Progresso do download do modelo (na primeira execução)
- Detalhes de alocação de memória
- Informações de ligação de serviço
- Métricas de inicialização de desempenho

### Compreender Categorias de Modelos

**Modelos de Linguagem Pequenos (SLMs):**
- `phi-4-mini`: Rápido, eficiente, ótimo para conversas gerais
- `phi-4`: Versão mais capaz com melhor raciocínio

**Modelos Médios:**
- `qwen2.5-7b`: Excelente raciocínio e contexto mais longo
- `deepseek-r1-7b`: Otimizado para geração de código

**Modelos Maiores:**
- `llama-3.2`: O mais recente modelo open-source da Meta
- `qwen2.5-14b`: Raciocínio de nível empresarial

## Parte 2: Testes Rápidos e Comparação de Modelos

### Abordagem do Sample 03: Lista e Benchmark Simples

Com base no padrão do Sample 03, aqui está o fluxo de trabalho mínimo:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testar Desempenho de Modelos

Depois de um modelo estar em execução, testa-o com prompts consistentes:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternativa de Teste com PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Parte 3: Gestão de Cache e Armazenamento de Modelos

### Compreender o Cache de Modelos

O Foundry Local gere automaticamente downloads e cache de modelos:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Considerações de Armazenamento de Modelos

**Tamanhos Típicos de Modelos:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b`: ~4.1 GB  
- `deepseek-r1-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b`: ~8.2 GB

**Melhores Práticas de Armazenamento:**
- Mantém 2-3 modelos em cache para troca rápida
- Remove modelos não utilizados para libertar espaço: `foundry cache clean`
- Monitoriza o uso do disco, especialmente em SSDs menores
- Considera o equilíbrio entre tamanho do modelo e capacidade

### Monitorização de Desempenho de Modelos

Enquanto os modelos estão em execução, monitoriza os recursos do sistema:

**Gestor de Tarefas do Windows:**
- Observa o uso de memória (os modelos permanecem carregados na RAM)
- Monitoriza a utilização da CPU durante a inferência
- Verifica o I/O do disco durante o carregamento inicial do modelo

**Monitorização na Linha de Comando:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Parte 4: Diretrizes Práticas para Seleção de Modelos

### Escolher Modelos por Caso de Uso

**Para Conversas Gerais e Q&A:**
- Começa com: `phi-4-mini` (rápido, eficiente)
- Faz upgrade para: `phi-4` (melhor raciocínio)
- Avançado: `qwen2.5-7b` (contexto mais longo)

**Para Geração de Código:**
- Recomendado: `deepseek-r1-7b`
- Alternativa: `qwen2.5-7b` (também bom para código)

**Para Raciocínio Complexo:**
- Melhor: `qwen2.5-7b` ou `qwen2.5-14b`
- Opção económica: `phi-4`

### Guia de Requisitos de Hardware

**Requisitos Mínimos do Sistema:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Recomendado para Melhor Desempenho:**
- 32GB+ de RAM para troca confortável entre modelos
- Armazenamento SSD para carregamento mais rápido de modelos
- CPU moderna com bom desempenho de single-thread
- Suporte NPU (PCs com Windows 11 Copilot+) para aceleração

### Fluxo de Trabalho de Troca de Modelos

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Parte 5: Benchmarking Simples de Modelos

### Testes Básicos de Desempenho

Aqui está uma abordagem simples para comparar o desempenho de modelos:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Avaliação Manual de Qualidade

Para cada modelo, testa com prompts consistentes e avalia manualmente:

**Prompts de Teste:**
1. "Explica computação quântica em termos simples."
2. "Escreve uma função Python para ordenar uma lista."
3. "Quais são os prós e contras do trabalho remoto?"
4. "Resume os benefícios da IA de edge."

**Critérios de Avaliação:**
- **Precisão**: A informação está correta?
- **Clareza**: A explicação é fácil de entender?
- **Completude**: Responde à questão por completo?
- **Velocidade**: Quão rápido responde?

### Monitorização de Uso de Recursos

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Parte 6: Próximos Passos

- Subscreve o Model Mondays para novos modelos e dicas: https://aka.ms/model-mondays
- Contribui com descobertas para o `models.json` da tua equipa
- Prepara-te para a Sessão 4: comparação entre LLMs e SLMs, inferência local vs cloud, e demonstrações práticas

---

**Aviso**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, é importante notar que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autoritária. Para informações críticas, recomenda-se uma tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes da utilização desta tradução.