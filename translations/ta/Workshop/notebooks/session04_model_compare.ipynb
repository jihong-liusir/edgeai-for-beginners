{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# роЕрооро░рпНро╡рпБ 4 тАУ SLM рооро▒рпНро▒рпБроорпН LLM роТрокрпНрокрпАроЯрпБ\n",
    "\n",
    "роЪро┐ро▒ро┐роп роорпКро┤ро┐ рооро╛роЯро▓рпН рооро▒рпНро▒рпБроорпН Foundry Local роорпВро▓роорпН роЗропроЩрпНроХрпБроорпН рокрпЖро░ро┐роп рооро╛роЯро▓ро┐ройрпН роЗроЯрпИропро┐ро▓ро╛рой родро╛роородроорпН рооро▒рпНро▒рпБроорпН рооро╛родро┐ро░ро┐ рокродро┐ро▓рпН родро░родрпНродрпИ роТрокрпНрокро┐роЯрпБроЩрпНроХро│рпН.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## тЪб ро╡ро┐ро░рпИро╡ро╛рой родрпКроЯроХрпНроХроорпН\n",
    "\n",
    "**роорпЖрооро░ро┐-роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓ро╛рой роЕроорпИрокрпНрокрпБ (рокрпБродрпБрокрпНрокро┐роХрпНроХрокрпНрокроЯрпНроЯродрпБ):**\n",
    "1. рооро╛роЯро▓рпНроХро│рпН родро╛ройро╛роХ CPU рооро╛ро▒рпБрокро╛роЯрпБроХро│рпИ родрпЗро░рпНро╡рпБ роЪрпЖропрпНроХро┐ройрпНро▒рой (роОроирпНрод ро╣ро╛ро░рпНроЯрпНро╡рпЗро░ро┐ро▓рпБроорпН ро╡рпЗро▓рпИ роЪрпЖропрпНроХро┐ро▒родрпБ)\n",
    "2. `qwen2.5-3b` роР 7B роЗро▒рпНроХрпБрокрпН рокродро┐ро▓ро╛роХ рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ (роЪрпБрооро╛ро░рпН 4GB RAM роЪрпЗрооро┐роХрпНроХро┐ро▒родрпБ)\n",
    "3. рокрпЛро░рпНроЯрпН родро╛ройро╛роХ роХрогрпНроЯро▒ро┐родро▓рпН (роХрпИропрпЗроЯрпБ роЕроорпИрокрпНрокрпБ родрпЗро╡рпИропро┐ро▓рпНро▓рпИ)\n",
    "4. роорпКродрпНрод RAM родрпЗро╡рпИ: ~8GB рокро░ро┐роирпНродрпБро░рпИроХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ (рооро╛роЯро▓рпНроХро│рпН + OS)\n",
    "\n",
    "**роЯрпЖро░рпНрооро┐ройро▓рпН роЕроорпИрокрпНрокрпБ (30 ро╡ро┐роиро╛роЯро┐роХро│рпН):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "рокро┐ройрпНройро░рпН роЗроирпНрод роирпЛроЯрпНрокрпБроХрпН роЗропроХрпНроХро╡рпБроорпН! ЁЯЪА\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: роЪро╛ро░рпНрокрпБ роиро┐ро▒рпБро╡ро▓рпН\n",
    "роирпЗро░роорпН рооро▒рпНро▒рпБроорпН роЙро░рпИропро╛роЯро▓рпН роХрпЛро░ро┐роХрпНроХрпИроХро│рпБроХрпНроХрпБродрпН родрпЗро╡рпИропро╛рой роХрпБро▒рпИроирпНродрокроЯрпНроЪ родрпКроХрпБрокрпНрокрпБроХро│рпИ (`foundry-local-sdk`, `openai`, `numpy`) роиро┐ро▒рпБро╡рпБроХро┐ро▒родрпБ. роорпАрогрпНроЯрпБроорпН роорпАрогрпНроЯрпБроорпН роЗропроХрпНроХрпБро╡родро▒рпНроХрпБ рокро╛родрпБроХро╛рокрпНрокро╛ройродрпБ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# роиро┐ро▓рпИроорпИ\n",
    "роТро░рпБ роЪро┐ро▒ро┐роп роорпКро┤ро┐ рооро╛родро┐ро░ро┐ (SLM) рооро▒рпНро▒рпБроорпН роТро░рпБ рокрпЖро░ро┐роп рооро╛родро┐ро░ро┐ропрпИ роТро░рпЗ роХрпЗро│рпНро╡ро┐ропро┐ро▓рпН роТрокрпНрокро┐роЯрпНроЯрпБ, рокро┐ройрпНро╡ро░рпБроорпН роЕроорпНроЪроЩрпНроХро│ро┐ро▓рпН ро╡ро┐родрпНродро┐ропро╛роЪроЩрпНроХро│рпИ ро╡ро┐ро│роХрпНроХро╡рпБроорпН:\n",
    "- **родро╛роород ро╡ро┐родрпНродро┐ропро╛роЪроорпН** (роирпЗро░ роирпЗро░роорпН ро╡ро┐роиро╛роЯро┐роХро│ро┐ро▓рпН)\n",
    "- **роЯрпЛроХрпНроХройрпН рокропройрпНрокро╛роЯрпБ** (роЗро░рпБрокрпНрокро┐ройрпН рокрпЛродрпБ) родро│ро╡ро╛роЯ родро┐ро▒ройрпБроХрпНроХро╛рой роТро░рпБ роХрпБро▒ро┐ропрпАроЯро╛роХ\n",
    "- **рооро╛родро┐ро░ро┐ропро╛роХрпНроХрокрпНрокроЯрпНроЯ родро░рооро╛рой ро╡рпЖро│ро┐ропрпАроЯрпБ** роЙроЯройроЯро┐ рокро╛ро░рпНро╡рпИроХрпНроХро╛роХ\n",
    "- **ро╡рпЗроХродрпНродрпИроХрпН роХрогроХрпНроХро┐роЯрпБродро▓рпН** роЪрпЖропро▓рпНродро┐ро▒ройрпН роорпБройрпНройрпЗро▒рпНро▒родрпНродрпИ роЕро│ро╡ро┐роЯ\n",
    "\n",
    "**роЪрпБро▒рпНро▒рпБроЪрпНроЪрпВро┤ро▓рпН рооро╛ро▒ро┐роХро│рпН:**\n",
    "- `SLM_ALIAS` - роЪро┐ро▒ро┐роп роорпКро┤ро┐ рооро╛родро┐ро░ро┐ (роЗропро▓рпНрокрпБроиро┐ро▓рпИ: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - рокрпЖро░ро┐роп роорпКро┤ро┐ рооро╛родро┐ро░ро┐ (роЗропро▓рпНрокрпБроиро┐ро▓рпИ: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - роТрокрпНрокрпАроЯрпНроЯрпБроХрпНроХро╛рой роЪрпЛродройрпИ роХрпЗро│рпНро╡ро┐\n",
    "- `COMPARE_RETRIES` - роорпАрогрпНроЯрпБроорпН роорпБропро▒рпНроЪро┐роХрпНроХ ро╡рпЗрогрпНроЯро┐роп роОрогрпНрогро┐роХрпНроХрпИ (роЗропро▓рпНрокрпБроиро┐ро▓рпИ: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - роЪрпЗро╡рпИ роорпБроЯро┐ро╡рпБроХрпНроХро╛рой рооро╛ро▒рпНро▒рпБ (роЕроорпИроХрпНроХрокрпНрокроЯро╛ро╡ро┐роЯрпНроЯро╛ро▓рпН родро╛ройро╛роХ роХрогрпНроЯро▒ро┐ропрокрпНрокроЯрпБроорпН)\n",
    "\n",
    "**роЗродрпБ роОрокрпНрокроЯро┐ роЪрпЖропро▓рпНрокроЯрпБроХро┐ро▒родрпБ (роЕродро┐роХро╛ро░рокрпНрокрпВро░рпНро╡ SDK роорпБро▒рпИ):**\n",
    "1. **FoundryLocalManager** Foundry Local роЪрпЗро╡рпИропрпИ родрпБро╡роХрпНроХро┐ роиро┐ро░рпНро╡роХро┐роХрпНроХро┐ро▒родрпБ\n",
    "2. роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒ро╛ро▓рпН родро╛ройро╛роХ родрпБро╡роЩрпНроХрпБроорпН (роХрпИроорпБро▒рпИропро╛роХ роЕроорпИроХрпНроХ родрпЗро╡рпИропро┐ро▓рпНро▓рпИ)\n",
    "3. рооро╛родро┐ро░ро┐роХро│рпН роХрпБро▒ро┐роЪрпНроЪрпКро▒рпНроХро│ро┐ро▓ро┐ро░рпБроирпНродрпБ родро╛ройро╛роХ роЕроЯрпИропро╛ро│роЩрпНроХро│рпН роорпВро▓роорпН родрпАро░рпНрооро╛ройро┐роХрпНроХрокрпНрокроЯрпБроорпН\n",
    "4. ро╡ройрпНрокрпКро░рпБро│рпН-роорпЗроорпНрокроЯрпБродрпНродрокрпНрокроЯрпНроЯ рооро╛ро▒рпБрокро╛роЯрпБроХро│рпН родрпЗро░рпНроирпНродрпЖроЯрпБроХрпНроХрокрпНрокроЯрпБроорпН (CUDA, NPU, роЕро▓рпНро▓родрпБ CPU)\n",
    "5. OpenAI-роХрпНроХрпБ роЗрогроХрпНроХрооро╛рой роХро┐ро│рпИропрогрпНроЯрпН роЙро░рпИропро╛роЯро▓рпН роорпБроЯро┐ро╡рпБроХро│рпИ роЪрпЖропро▓рпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ\n",
    "6. роЕро│ро╡рпАроЯрпБроХро│рпН рокродро┐ро╡рпБ роЪрпЖропрпНропрокрпНрокроЯрпБроорпН: родро╛роородроорпН, роЯрпЛроХрпНроХройрпНроХро│рпН, ро╡рпЖро│ро┐ропрпАроЯрпНроЯрпБ родро░роорпН\n",
    "7. роорпБроЯро┐ро╡рпБроХро│рпН роТрокрпНрокро┐роЯрокрпНрокроЯрпНроЯрпБ ро╡рпЗроХродрпНродрпИроХрпН роХрогроХрпНроХро┐роЯрпБроорпН ро╡ро┐роХро┐родроорпН роХрогроХрпНроХро┐роЯрокрпНрокроЯрпБроорпН\n",
    "\n",
    "роЗроирпНрод роЪро┐ро▒ро┐роп роТрокрпНрокрпАроЯрпБ роЙроЩрпНроХро│рпН рокропройрпНрокро╛роЯрпНроЯро┐ро▒рпНроХро╛роХ рокрпЖро░ро┐роп рооро╛родро┐ро░ро┐ропрпИродрпН родрпЗро░рпНроирпНродрпЖроЯрпБрокрпНрокродрпБ роОрокрпНрокрпЛродрпБ роиро┐ропро╛ропрооро╛ройродрпБ роОройрпНрокродрпИ родрпАро░рпНрооро╛ройро┐роХрпНроХ роЙродро╡рпБроХро┐ро▒родрпБ.\n",
    "\n",
    "**SDK роХрпБро▒ро┐рокрпНрокрпБ:** \n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- рокрогро┐рооройрпИ роЙродро╡ро┐роХро│рпН: ../samples/workshop_utils.py роЗро▓рпН роЗро░рпБроирпНродрпБ роЕродро┐роХро╛ро░рокрпНрокрпВро░рпНро╡ роорпБро▒рпИропрпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ\n",
    "\n",
    "**роорпБроХрпНроХро┐роп роиройрпНроорпИроХро│рпН:**\n",
    "- тЬЕ родро╛ройро┐ропроЩрпНроХро┐ роЪрпЗро╡рпИ роХрогрпНроЯро▒ро┐родро▓рпН рооро▒рпНро▒рпБроорпН родрпБро╡роХрпНроХроорпН\n",
    "- тЬЕ роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒ро╛ро▓рпН родро╛ройро╛роХ родрпБро╡роЩрпНроХрпБроорпН\n",
    "- тЬЕ роЙро│рпНро│роорпИроХрпНроХрокрпНрокроЯрпНроЯ рооро╛родро┐ро░ро┐ родрпАро░рпНрооро╛ройроорпН рооро▒рпНро▒рпБроорпН роЪрпЗрооро┐рокрпНрокрпБ\n",
    "- тЬЕ ро╡ройрпНрокрпКро░рпБро│рпН роорпЗроорпНрокро╛роЯрпБ (CUDA/NPU/CPU)\n",
    "- тЬЕ OpenAI SDK роЗрогроХрпНроХродрпНродройрпНроорпИ\n",
    "- тЬЕ роорпАрогрпНроЯрпБроорпН роорпБропро▒рпНроЪро┐роХро│рпБроЯройрпН ро╡ро▓рпБро╡ро╛рой рокро┐ро┤рпИ роХрпИропро╛ро│рпБродро▓рпН\n",
    "- тЬЕ роЙро│рпНро│рпВро░рпН родрпАро░рпНро╡рпБ (роорпЗроХ API родрпЗро╡рпИропро┐ро▓рпНро▓рпИ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ЁЯЪи роорпБройрпНройрпЛроЯрпНроЯроорпН: Foundry Local роЗропроЩрпНроХ ро╡рпЗрогрпНроЯрпБроорпН!\n",
    "\n",
    "**роЗроирпНрод роирпЛроЯрпНрокрпБроХрпН роЗропроХрпНроХрпБро╡родро▒рпНроХрпБ роорпБройрпН**, Foundry Local роЪрпЗро╡рпИ роЕроорпИроХрпНроХрокрпНрокроЯрпНроЯро┐ро░рпБроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН:\n",
    "\n",
    "### ро╡ро┐ро░рпИро╡ро╛рой родрпКроЯроХрпНроХроХрпН роХроЯрпНроЯро│рпИроХро│рпН (Terminal-ро▓рпН роЗропроХрпНроХро╡рпБроорпН):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### рооро╛ро▒рпНро▒рпБ рооро╛роЯро▓рпНроХро│рпН (роЗропро▓рпНрокрпБроиро┐ро▓рпИ роХро┐роЯрпИроХрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒ро╛ро▓рпН):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "тЪая╕П **роЗроирпНрод рокроЯро┐роХро│рпИ родро╡ро┐ро░рпНродрпНродро╛ро▓рпН**, роХрпАро┤рпЗ роЙро│рпНро│ роирпЛроЯрпНрокрпБроХрпН роЪрпЖро▓рпНроХро│рпИ роЗропроХрпНроХрпБроорпНрокрпЛродрпБ `APIConnectionError` родрпЛройрпНро▒рпБроорпН.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: роорпБроХрпНроХро┐роп роЗро▒роХрпНроХрпБроорпИроХро│рпН  \n",
    "роирпЗро░ роорпЗро▓ро╛рогрпНроорпИ роЙродро╡ро┐роХро│рпН рооро▒рпНро▒рпБроорпН Foundry Local / OpenAI роХро┐ро│рпИропрогрпНроЯрпБроХро│рпИ роХрпКрогрпНроЯрпБ ро╡ро░рпБроХро┐ро▒родрпБ, рооро╛роЯро▓рпН родроХро╡ро▓рпИ рокрпЖро▒ро╡рпБроорпН рооро▒рпНро▒рпБроорпН роЙро░рпИропро╛роЯро▓рпН роиро┐ро▒рпИро╡рпБ роЪрпЖропрпНропро╡рпБроорпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпБроХро┐ро▒родрпБ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: рооро╛ро▒рпНро▒рпБрокрпНрокрпЖропро░рпНроХро│рпН & роЙроирпНродрпБродро▓ро┐ройрпН роЕроорпИрокрпНрокрпБ  \n",
    "роЪро┐ро▒ро┐роп рооро▒рпНро▒рпБроорпН рокрпЖро░ро┐роп рооро╛родро┐ро░ро┐роХро│рпБроХрпНроХро╛рой роЪрпВро┤ро▓рпН-роХроЯрпНроЯроорпИроХрпНроХроХрпНроХрпВроЯро┐роп рооро╛ро▒рпНро▒рпБрокрпНрокрпЖропро░рпНроХро│рпИ рооро▒рпНро▒рпБроорпН роТрокрпНрокрпАроЯрпНроЯрпБ роЙроирпНродрпБродро▓рпИ ро╡ро░рпИропро▒рпБроХрпНроХро┐ро▒родрпБ. рооро╛роЯро▓рпН роХрпБроЯрпБроорпНрокроЩрпНроХро│рпН роЕро▓рпНро▓родрпБ рокрогро┐роХро│ро┐ро▓рпН ро╡рпЗро▒рпБрокро╛роЯрпБроХро│рпИ роЖро░ро╛роп роЪрпВро┤ро▓рпН рооро╛ро▒ро┐роХро│рпИ роЪро░ро┐роЪрпЖропрпНропро╡рпБроорпН.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ЁЯТб роиро┐ройрпИро╡роХродрпНродрпИ роорпИропрооро╛роХроХрпН роХрпКрогрпНроЯ роЕроорпИрокрпНрокрпБ\n",
    "\n",
    "**роЗроирпНрод роирпЛроЯрпНрокрпБроХрпН роиро┐ройрпИро╡роХродрпНродрпИ роорпИропрооро╛роХроХрпН роХрпКрогрпНроЯ рооро╛роЯро▓рпНроХро│рпИ роЗропро▓рпНрокро╛роХ рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ:**\n",
    "- `phi-4-mini` тЖТ ~4GB RAM (Foundry Local родро╛ройро╛роХ CPU рооро╛ро▒рпБрокро╛роЯрпНроЯрпИ родрпЗро░рпНроирпНродрпЖроЯрпБроХрпНроХро┐ро▒родрпБ)\n",
    "- `qwen2.5-3b` тЖТ ~3GB RAM (7B рооро╛ро▒рпБрокро╛роЯрпНроЯрпБроХрпНроХрпБ ~7GB+ родрпЗро╡рпИрокрпНрокроЯрпБроорпН)\n",
    "\n",
    "**рокрпЛро░рпНроЯрпН родро╛ройро╛роХроХрпН роХрогрпНроЯро▒ро┐родро▓рпН:**\n",
    "- Foundry Local рокро▓рпНро╡рпЗро▒рпБ рокрпЛро░рпНроЯрпНроХро│рпИ рокропройрпНрокроЯрпБродрпНродро▓ро╛роорпН (рокрпКродрпБро╡ро╛роХ 55769 роЕро▓рпНро▓родрпБ 59959)\n",
    "- роХрпАро┤рпЗ роЙро│рпНро│ роЯропроХрпНройрпЛро╕рпНроЯро┐роХрпН роЪрпЖро▓рпН роЪро░ро┐ропро╛рой рокрпЛро░рпНроЯрпНроЯрпИ родро╛ройро╛роХроХрпН роХрогрпНроЯро▒ро┐роХро┐ро▒родрпБ\n",
    "- роХрпИропрпЗроЯрпБ роЕроорпИрокрпНрокрпБроХро│рпН родрпЗро╡рпИропро┐ро▓рпНро▓рпИ!\n",
    "\n",
    "**роЙроЩрпНроХро│ро┐роЯроорпН RAM роХрпБро▒рпИро╡ро╛роХ (<8GB) роЗро░рпБроирпНродро╛ро▓рпН, роорпЗро▓рпБроорпН роЪро┐ро▒ро┐роп рооро╛роЯро▓рпНроХро│рпИ рокропройрпНрокроЯрпБродрпНродро╡рпБроорпН:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ЁЯТб Using official Foundry SDK pattern from workshop_utils\n",
      "   тЖТ FoundryLocalManager handles service lifecycle\n",
      "   тЖТ Automatic model resolution and hardware optimization\n",
      "   тЖТ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nЁЯТб Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   тЖТ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   тЖТ Automatic model resolution and hardware optimization\")\n",
    "print(\"   тЖТ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: роЪрпЖропро▓рпНрокро╛роЯрпНроЯрпБ роЙродро╡ро┐роХро│рпН (Foundry SDK роорпБро▒рпИ)\n",
    "Workshop рооро╛родро┐ро░ро┐роХро│ро┐ро▓рпН роЖро╡рогрооро╛роХрпНроХрокрпНрокроЯрпНроЯ Foundry Local SDK роорпБро▒рпИропрпИ рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ:\n",
    "\n",
    "**роорпБро▒рпИ:**\n",
    "- **FoundryLocalManager** - Foundry Local роЪрпЗро╡рпИропрпИ родрпКроЯроЩрпНроХро┐ роиро┐ро░рпНро╡роХро┐роХрпНроХро┐ро▒родрпБ\n",
    "- **родро╛ройро┐ропроЩрпНроХ роХрогрпНроЯро▒ро┐родро▓рпН** - роЗро▒рпБродро┐рокрпНрокрпБро│рпНро│ро┐ропрпИ родро╛ройро╛роХро╡рпЗ роХрогрпНроЯро▒ро┐роирпНродрпБ роЪрпЗро╡рпИ ро╡ро╛ро┤рпНроХрпНроХрпИроЪрпНроЪрпБро┤ро▒рпНроЪро┐ропрпИ роиро┐ро░рпНро╡роХро┐роХрпНроХро┐ро▒родрпБ\n",
    "- **рооро╛родро┐ро░ро┐ родрпАро░рпНро╡рпБ** - рокрпВро░рог рооро╛родро┐ро░ро┐ роРроЯро┐роХро│рпИ (роЙродро╛ро░рогрооро╛роХ, phi-4-mini тЖТ phi-4-mini-instruct-cpu) родрпАро░рпНрооро╛ройро┐роХрпНроХро┐ро▒родрпБ\n",
    "- **ро╣ро╛ро░рпНроЯрпНро╡рпЗро░ро┐ройрпН роорпЗроорпНрокро╛роЯрпБ** - роХро┐роЯрпИроХрпНроХроХрпНроХрпВроЯро┐роп ро╣ро╛ро░рпНроЯрпНро╡рпЗро░рпБроХрпНроХрпБ роЪро┐ро▒роирпНрод рооро╛ро▒рпБрокро╛роЯрпНроЯрпИ родрпЗро░рпНроирпНродрпЖроЯрпБроХрпНроХро┐ро▒родрпБ (CUDA, NPU, роЕро▓рпНро▓родрпБ CPU)\n",
    "- **OpenAI роХро┐ро│рпИропрогрпНроЯрпН** - OpenAI-роХрпНроХрпБ роЗрогроХрпНроХрооро╛рой API роЕрогрпБроХро▓рпБроХрпНроХро╛роХ роорпЗро▓ро╛ро│ро░ро┐ройрпН роЗро▒рпБродро┐рокрпНрокрпБро│рпНро│ро┐ропрпБроЯройрпН роЕроорпИроХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ\n",
    "\n",
    "**роиро┐ро▓рпИродрпНродройрпНроорпИ роЕроорпНроЪроЩрпНроХро│рпН:**\n",
    "- рокро░ро╡ро▓ро╛рой рокро┐ройрпНро╡ро╛роЩрпНроХ retry родро▒рпНроХро╛ро▓ро┐роХроорпН (роЪрпВро┤ро▓рпН роорпВро▓роорпН роЕроорпИроХрпНроХроХрпНроХрпВроЯро┐ропродрпБ)\n",
    "- роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒ро╛ро▓рпН родро╛ройро╛роХро╡рпЗ родрпКроЯроЩрпНроХрпБроХро┐ро▒родрпБ\n",
    "- родрпКроЯроХрпНроХродрпНродро┐ро▒рпНроХрпБрокрпН рокро┐ро▒роХрпБ роЗрогрпИрокрпНрокрпБ роЪро░ро┐рокро╛ро░рпНрокрпНрокрпБ\n",
    "- ро╡ро┐ро░ро┐ро╡ро╛рой рокро┐ро┤рпИ роЕро▒ро┐роХрпНроХрпИропрпБроЯройрпН роорпЖро▓рпНро▓ро┐роп рокро┐ро┤рпИ роХрпИропро╛ро│рпБродро▓рпН\n",
    "- роорпАрогрпНроЯрпБроорпН родрпКроЯроЩрпНроХ avoided роЪрпЖропрпНроп рооро╛родро┐ро░ро┐ роХро╛роЯрпНроЪро┐рокрпНрокроЯрпБродрпНродро▓рпН\n",
    "\n",
    "**роорпБроЯро┐ро╡ро┐ройрпН роЕроорпИрокрпНрокрпБ:**\n",
    "- родро╛роород роЕро│ро╡рпАроЯрпБ (роЪрпБро╡ро░рпН роХроЯро┐роХро╛ро░ роирпЗро░роорпН)\n",
    "- роЯрпЛроХрпНроХройрпН рокропройрпНрокро╛роЯрпНроЯро┐ройрпН роХрогрпНроХро╛рогро┐рокрпНрокрпБ (роХро┐роЯрпИроХрпНроХроХрпНроХрпВроЯро┐ропродрпБ роОройрпНро▒ро╛ро▓рпН)\n",
    "- рооро╛родро┐ро░ро┐ ро╡рпЖро│ро┐ропрпАроЯрпБ (ро╡ро╛роЪро┐роХрпНроХ роОро│ро┐родро╛роХ роЪрпБро░рпБроХрпНроХрокрпНрокроЯрпНроЯродрпБ)\n",
    "- родрпЛро▓рпНро╡ро┐ропроЯрпИроирпНрод роХрпЛро░ро┐роХрпНроХрпИроХро│рпБроХрпНроХро╛рой рокро┐ро┤рпИ ро╡ро┐ро╡ро░роЩрпНроХро│рпН\n",
    "\n",
    "роЗроирпНрод роорпБро▒рпИ workshop_utils рооро╛роЯрпБро▓рпИ рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ, роЗродрпБ роЕродро┐роХро╛ро░рокрпНрокрпВро░рпНро╡ SDK роорпБро▒рпИропрпИ рокро┐ройрпНрокро▒рпНро▒рпБроХро┐ро▒родрпБ.\n",
    "\n",
    "**SDK роХрпБро▒ро┐рокрпНрокрпБ:**\n",
    "- роорпБроХрпНроХро┐роп Repo: https://github.com/microsoft/Foundry-Local\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- Workshop Utils: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Execution helpers defined: setup(), run()\n",
      "   тЖТ Uses workshop_utils for proper SDK integration\n",
      "   тЖТ setup() initializes with FoundryLocalManager\n",
      "   тЖТ run() executes inference via OpenAI-compatible API\n",
      "   тЖТ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        тЖТ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        тЖТ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        тЖТ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        тЖТ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nтЭМ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nЁЯТб Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      тЖТ foundry service status\")\n",
    "    print(f\"      тЖТ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      тЖТ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      тЖТ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"тЬЕ Execution helpers defined: setup(), run()\")\n",
    "print(\"   тЖТ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   тЖТ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   тЖТ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   тЖТ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: рокрпНро░рпА-роГрокрпНро│рпИроЯрпН роЪрпЖро▓рпНроГрокрпН-роЯрпЖро╕рпНроЯрпН\n",
    "FoundryLocalManager-роР рокропройрпНрокроЯрпБродрпНродро┐ роЗро░рпБ рооро╛роЯро▓рпНроХро│рпБроХрпНроХрпБроорпН роТро░рпБ роОро│ро┐роп роЗрогрпИрокрпНрокрпБ роЪрпЛродройрпИропрпИ роЗропроХрпНроХрпБроХро┐ро▒родрпБ. роЗродрпБ рокро┐ройрпНро╡ро░рпБро╡ройро╡ро▒рпНро▒рпИ роЙро▒рпБродро┐рокрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ:\n",
    "- роЪрпЗро╡рпИ роЕрогрпБроХроХрпНроХрпВроЯро┐ропродрпБ\n",
    "- рооро╛роЯро▓рпНроХро│рпН родрпКроЯроЩрпНроХрокрпНрокроЯроХрпНроХрпВроЯро┐ропро╡рпИ\n",
    "- Alias-роХро│рпН роЙрогрпНроорпИропро╛рой рооро╛роЯро▓рпН ID-роХро│рпБроХрпНроХрпБ родрпАро░рпНрооро╛ройро┐роХрпНроХро┐ройрпНро▒рой\n",
    "- роТрокрпНрокрпАроЯрпБ роЪрпЖропрпНропрпБроорпН роорпБройрпН роЗрогрпИрокрпНрокрпБ роиро┐ро▓рпИропро╛ройродрпБ\n",
    "\n",
    "setup() роЪрпЖропро▓рпНрокро╛роЯрпБ workshop_utils-роЗройрпН роЕродро┐роХро╛ро░рокрпНрокрпВро░рпНро╡ SDK роорпБро▒рпИроорпИропрпИ рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "тЭМ Foundry Local service not found!\n",
      "\n",
      "ЁЯТб To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "тЪая╕П  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"тЬЕ Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"тЬЕ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"тЪая╕П  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"тЪая╕П  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nтЭМ Foundry Local service not found!\")\n",
    "    print(\"\\nЁЯТб To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nтЬЕ Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nтЪая╕П  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪая╕П  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ЁЯТб Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"тЪая╕П  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ЁЯТб Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ЁЯЫая╕П ро╡ро┐ро░рпИро╡ро╛рой роЪро░ро┐роЪрпЖропрпНродро▓рпН: роирпЛроЯрпНрокрпБроХрпН роорпВро▓роорпН Foundry Local роР родрпКроЯроЩрпНроХрпБродро▓рпН (ро╡ро┐ро░рпБрокрпНрокроорпН)\n",
    "\n",
    "роорпЗро▓рпЗ роЙро│рпНро│ роЯропроХрпНройрпЛро╕рпНроЯро┐роХрпН роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒рпБ роХро╛роЯрпНроЯро┐ройро╛ро▓рпН, роирпАроЩрпНроХро│рпН роЗродро┐ро▓ро┐ро░рпБроирпНродрпБ родрпКроЯроЩрпНроХ роорпБропро▒рпНроЪро┐роХрпНроХро▓ро╛роорпН:\n",
    "\n",
    "**роХрпБро▒ро┐рокрпНрокрпБ:** роЗродрпБ Windows-ро▓рпН роЪро┐ро▒рокрпНрокро╛роХ роЪрпЖропро▓рпНрокроЯрпБроорпН. рооро▒рпНро▒ рокро┐ро│ро╛роЯрпНроГрокро╛ро░роЩрпНроХро│ро┐ро▓рпН, роЯрпЖро░рпНрооро┐ройро▓рпН роХроЯрпНроЯро│рпИроХро│рпИ рокропройрпНрокроЯрпБродрпНродро╡рпБроорпН.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### тЪая╕П роЗрогрпИрокрпНрокрпБ рокро┐ро┤рпИроХро│рпИ роЪро░ро┐роЪрпЖропрпНродро▓рпН\n",
    "\n",
    "роирпАроЩрпНроХро│рпН `APIConnectionError` роОройрпНрокродрпИрокрпН рокро╛ро░рпНроХрпНроХро┐ро▒рпАро░рпНроХро│рпН роОройрпНро▒ро╛ро▓рпН, Foundry Local роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роЕро▓рпНро▓родрпБ рооро╛родро┐ро░ро┐роХро│рпН роПро▒рпНро▒рокрпНрокроЯро╡ро┐ро▓рпНро▓рпИ. роЗроирпНрод роироЯро╡роЯро┐роХрпНроХрпИроХро│рпИ роорпБропро▒рпНроЪро┐роХрпНроХро╡рпБроорпН:\n",
    "\n",
    "**1. роЪрпЗро╡рпИ роиро┐ро▓рпИропрпИ роЪро░ро┐рокро╛ро░рпНроХрпНроХро╡рпБроорпН:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**2. роЪрпЗро╡рпИропрпИ родрпКроЯроЩрпНроХро╡рпБроорпН (роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ роОройрпНро▒ро╛ро▓рпН):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**3. родрпЗро╡рпИропро╛рой рооро╛родро┐ро░ро┐роХро│рпИ роПро▒рпНро▒ро╡рпБроорпН:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**4. рооро╛родро┐ро░ро┐роХро│рпН роХро┐роЯрпИроХрпНроХро┐ройрпНро▒ройро╡ро╛ роОройрпНрокродрпИ роЙро▒рпБродро┐рокрпНрокроЯрпБродрпНродро╡рпБроорпН:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**роЪро╛родро╛ро░рог рокро┐ро░роЪрпНроЪро┐ройрпИроХро│рпН:**\n",
    "- тЭМ роЪрпЗро╡рпИ роЗропроЩрпНроХро╡ро┐ро▓рпНро▓рпИ тЖТ `foundry service start` роР роЗропроХрпНроХро╡рпБроорпН\n",
    "- тЭМ рооро╛родро┐ро░ро┐роХро│рпН роПро▒рпНро▒рокрпНрокроЯро╡ро┐ро▓рпНро▓рпИ тЖТ `foundry model run <model-name>` роР роЗропроХрпНроХро╡рпБроорпН\n",
    "- тЭМ рокрпЛро░рпНроЯрпН роорпЛродро▓рпНроХро│рпН тЖТ рооро▒рпНро▒рпКро░рпБ роЪрпЗро╡рпИ рокрпЛро░рпНроЯрпНроЯрпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родро╛ роОройрпНрокродрпИроЪрпН роЪро░ро┐рокро╛ро░рпНроХрпНроХро╡рпБроорпН\n",
    "- тЭМ роГрокропро░рпНро╡ро╛ро▓рпН родроЯрпБрокрпНрокродрпБ тЖТ роЙро│рпНро│рпВро░рпН роЗрогрпИрокрпНрокрпБроХро│рпН роЕройрпБроородро┐роХрпНроХрокрпНрокроЯрпБроХро┐ро▒родро╛ роОройрпНрокродрпИ роЙро▒рпБродро┐рокрпНрокроЯрпБродрпНродро╡рпБроорпН\n",
    "\n",
    "**ро╡ро┐ро░рпИро╡ро╛рой родрпАро░рпНро╡рпБ:** рокрпНро░рпА-роГрокрпНро│рпИроЯрпН роЪро░ро┐рокро╛ро░рпНрокрпНрокрпБроХрпНроХрпБ роорпБройрпН роХрпАро┤рпЗ роЙро│рпНро│ роЯропроХрпНройрпЛро╕рпНроЯро┐роХрпН роЪрпЖро▓рпНроХро│рпИ роЗропроХрпНроХро╡рпБроорпН.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  тЬЕ phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  тЬЕ qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'тЬЕ' if details['status'] == 'success' else 'тЭМ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### тЬЕ роорпБройрпНрокродро┐ро╡рпЗро▒рпНрокрпБ роЪрпЛродройрпИ: рооро╛роЯро▓рпН роХро┐роЯрпИрокрпНрокродрпБ\n",
    "\n",
    "роЗроирпНрод роЪрпЖро▓рпН, роТрокрпНрокрпАроЯрпНроЯрпИ роЗропроХрпНроХрпБро╡родро▒рпНроХрпБ роорпБройрпН, роЗро░рпБ рооро╛роЯро▓рпНроХро│рпБроорпН роЕроорпИроХрпНроХрокрпНрокроЯрпНроЯ роорпБроЯрпБроХрпНроХродрпНродро┐ро▓рпН роЕрогрпБроХроХрпНроХрпВроЯро┐ропродро╛ роОройрпНрокродрпИ роЪро░ро┐рокро╛ро░рпНроХрпНроХро┐ро▒родрпБ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### ро╡ро┐ро│роХрпНроХроорпН: роТрокрпНрокрпАроЯрпБ роЗропроХрпНроХроорпН & роорпБроЯро┐ро╡рпБроХро│рпИ роЪрпЗроХро░ро┐родрпНродро▓рпН\n",
    "Foundry SDK роорпБро▒рпИропрпИрокрпН рокропройрпНрокроЯрпБродрпНродро┐ роЗро░рпБ alias-роХро│ро┐ро▓рпБроорпН роорпАрогрпНроЯрпБроорпН роЪрпЖропро▓рпНрокроЯрпБроХро┐ро▒родрпБ:\n",
    "1. роТро╡рпНро╡рпКро░рпБ рооро╛роЯро▓рпИропрпБроорпН setup() роорпВро▓роорпН родрпКроЯроЩрпНроХро╡рпБроорпН (FoundryLocalManager рокропройрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ)\n",
    "2. OpenAI-роЗройрпН роЗрогроХрпНроХрооро╛рой API роорпВро▓роорпН inference роЗропроХрпНроХро╡рпБроорпН\n",
    "3. latency, tokens, рооро▒рпНро▒рпБроорпН рооро╛родро┐ро░ро┐ output-роРрокрпН рокродро┐ро╡рпБ роЪрпЖропрпНропро╡рпБроорпН\n",
    "4. роТрокрпНрокрпАроЯрпНроЯрпБ рокроХрпБрокрпНрокро╛ропрпНро╡рпБроЯройрпН JSON роЪрпБро░рпБроХрпНроХродрпНродрпИ роЙро░рпБро╡ро╛роХрпНроХро╡рпБроорпН\n",
    "\n",
    "роЗродрпБ session04/model_compare.py-роЗро▓рпН Workshop рооро╛родро┐ро░ро┐роХро│ро┐ройрпН роЕродрпЗ роорпБро▒рпИропрпИрокрпН рокро┐ройрпНрокро▒рпНро▒рпБроХро┐ро▒родрпБ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "тЬЕ phi-4-mini         success         51.975          ~247 (est.)    \n",
      "тЬЕ qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ЁЯТб SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'тЬЕ' if status == 'success' else 'тЭМ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nЁЯТб SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nтЪая╕П  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### роорпБроЯро┐ро╡рпБроХро│рпИ рокрпБро░ро┐роирпНродрпБроХрпКро│рпНро╡родрпБ\n",
    "\n",
    "**роорпБроХрпНроХро┐роп роЕро│ро╡рпБроХрпЛро▓рпНроХро│рпН:**\n",
    "- **Latency (родро╛роородроорпН)**: роХрпБро▒рпИро╡ро╛роХ роЗро░рпБроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН - роЗродрпБ ро╡ро┐ро░рпИро╡ро╛рой рокродро┐ро▓ро│ро┐роХрпНроХрпБроорпН роирпЗро░родрпНродрпИ роХрпБро▒ро┐роХрпНроХро┐ро▒родрпБ\n",
    "- **Tokens (роЯрпЛроХрпНроХройрпНроХро│рпН)**: роЕродро┐роХрооро╛рой роЪрпЖропро▓рпНродро┐ро▒ройрпН = роЕродро┐роХ роЯрпЛроХрпНроХройрпНроХро│рпН роЪрпЖропро▓ро╛роХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой\n",
    "- **Route (рокро╛родрпИ)**: роОроирпНрод API роорпБроЯрпБроХрпНроХроорпН рокропройрпНрокроЯрпБродрпНродрокрпНрокроЯрпНроЯродрпБ роОройрпНрокродрпИ роЙро▒рпБродро┐рокрпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ\n",
    "\n",
    "**SLM рооро▒рпНро▒рпБроорпН LLM роОрокрпНрокрпЛродрпБ рокропройрпНрокроЯрпБродрпНрод ро╡рпЗрогрпНроЯрпБроорпН:**\n",
    "- **SLM (роЪро┐ро▒ро┐роп роорпКро┤ро┐ рооро╛родро┐ро░ро┐)**: ро╡ро┐ро░рпИро╡ро╛рой рокродро┐ро▓рпНроХро│рпН, роХрпБро▒рпИроирпНрод ро╡ро│роЩрпНроХро│ро┐ройрпН рокропройрпНрокро╛роЯрпБ, роОро│ро┐роп рокрогро┐роХро│рпБроХрпНроХрпБ роЪро┐ро▒роирпНродродрпБ\n",
    "- **LLM (рокрпЖро░ро┐роп роорпКро┤ро┐ рооро╛родро┐ро░ро┐)**: роЙропро░рпНроирпНрод родро░роорпН, роЪро┐ро▒роирпНрод роХро╛ро░рогрооро▒ро┐родро▓рпН, родро░роорпН рооро┐роХ роорпБроХрпНроХро┐ропрооро╛рой рокрпЛродрпБ рокропройрпНрокроЯрпБродрпНродро╡рпБроорпН\n",
    "\n",
    "**роЕроЯрпБродрпНрод рокроЯро┐роХро│рпН:**\n",
    "1. роЪро┐роХрпНроХро▓ро┐ройрпН родро╛роХрпНроХродрпНродрпИ роТрокрпНрокрпАроЯрпНроЯро┐ро▓рпН роОрокрпНрокроЯро┐ рокро╛родро┐роХрпНроХро┐ро▒родрпБ роОройрпНрокродрпИ роХро╛рог ро╡рпЖро╡рпНро╡рпЗро▒рпБ роХрпЗро│рпНро╡ро┐роХро│рпИ роорпБропро▒рпНроЪро┐роХрпНроХро╡рпБроорпН\n",
    "2. рооро▒рпНро▒ рооро╛родро┐ро░ро┐ роЬрпЛроЯро┐роХро│рпИ рокро░ро┐роЪрпЛродро┐роХрпНроХро╡рпБроорпН\n",
    "3. рокрогро┐ропро┐ройрпН роЪро┐роХрпНроХро▓ро┐ройрпН роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓рпН рокрпБродрпНродро┐роЪро╛ро▓ро┐ропро╛роХ ро╡ро┤ро┐рооро╛ро▒рпНро▒рпБро╡родро▒рпНроХрпБ Workshop router рооро╛родро┐ро░ро┐роХро│рпИ (Session 06) рокропройрпНрокроЯрпБродрпНродро╡рпБроорпН\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "тЬЕ SLM Model: phi-4-mini\n",
      "тЬЕ LLM Model: qwen2.5-7b\n",
      "тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "тЬЕ Pre-flight passed: True\n",
      "тЬЕ Comparison completed: True\n",
      "тЬЕ Both models responded: True\n",
      "======================================================================\n",
      "ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"тЬЕ SLM Model: {SLM}\")\n",
    "print(f\"тЬЕ LLM Model: {LLM}\")\n",
    "print(f\"тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"тЬЕ Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"тЪая╕П  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"тЪая╕П  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"тЪая╕П  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nтЪая╕П  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nЁЯТб Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nЁЯТб Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**роХрпБро▒ро┐рокрпНрокрпБ**:  \nроЗроирпНрод роЖро╡рогроорпН [Co-op Translator](https://github.com/Azure/co-op-translator) роОройрпНро▒ AI роорпКро┤ро┐рокрпЖропро░рпНрокрпНрокрпБ роЪрпЗро╡рпИропрпИрокрпН рокропройрпНрокроЯрпБродрпНродро┐ роорпКро┤ро┐рокрпЖропро░рпНроХрпНроХрокрпНрокроЯрпНроЯрпБро│рпНро│родрпБ. роиро╛роЩрпНроХро│рпН родрпБро▓рпНро▓ро┐ропродрпНродро┐ро▒рпНроХро╛роХ роорпБропро▒рпНроЪро┐роХрпНроХро┐ройрпНро▒рпЛроорпН, роЖройро╛ро▓рпН родро╛ройро┐ропроЩрпНроХро┐ роорпКро┤ро┐рокрпЖропро░рпНрокрпНрокрпБроХро│ро┐ро▓рпН рокро┐ро┤рпИроХро│рпН роЕро▓рпНро▓родрпБ родро╡ро▒ро╛рой родроХро╡ро▓рпНроХро│рпН роЗро░рпБроХрпНроХроХрпНроХрпВроЯрпБроорпН роОройрпНрокродрпИ родропро╡рпБроЪрпЖропрпНродрпБ роХро╡ройродрпНродро┐ро▓рпН роХрпКро│рпНро│рпБроЩрпНроХро│рпН. роЕродройрпН родро╛ропрпНроорпКро┤ро┐ропро┐ро▓рпН роЙро│рпНро│ роорпВро▓ роЖро╡рогроорпН роЕродро┐роХро╛ро░рокрпНрокрпВро░рпНро╡ роЖродро╛ро░рооро╛роХ роХро░рпБродрокрпНрокроЯ ро╡рпЗрогрпНроЯрпБроорпН. роорпБроХрпНроХро┐ропрооро╛рой родроХро╡ро▓рпНроХро│рпБроХрпНроХрпБ, родрпКро┤ро┐ро▓рпНроорпБро▒рпИ рооройро┐род роорпКро┤ро┐рокрпЖропро░рпНрокрпНрокрпБ рокро░ро┐роирпНродрпБро░рпИроХрпНроХрокрпНрокроЯрпБроХро┐ро▒родрпБ. роЗроирпНрод роорпКро┤ро┐рокрпЖропро░рпНрокрпНрокрпИрокрпН рокропройрпНрокроЯрпБродрпНродрпБро╡родро╛ро▓рпН роПро▒рпНрокроЯрпБроорпН роОроирпНрод родро╡ро▒ро╛рой рокрпБро░ро┐родро▓рпНроХро│рпН роЕро▓рпНро▓родрпБ родро╡ро▒ро╛рой ро╡ро┐ро│роХрпНроХроЩрпНроХро│рпБроХрпНроХрпБ роиро╛роЩрпНроХро│рпН рокрпКро▒рпБрокрпНрокро▓рпНро▓.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-11T13:06:32+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "ta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}