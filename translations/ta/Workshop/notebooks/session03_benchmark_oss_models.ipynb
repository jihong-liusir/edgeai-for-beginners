{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91644746",
   "metadata": {},
   "source": [
    "# ‡ÆÖ‡ÆÆ‡Æ∞‡Øç‡Æµ‡ØÅ 3 ‚Äì ‡Æ§‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÇ‡Æ≤ ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡Æï‡Æ≥‡Øà ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æ§‡Æ≤‡Øç\n",
    "\n",
    "Foundry Local ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡Æ™‡Æ≤ ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æ§‡Ææ‡ÆÆ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æö‡Æ∞‡Ææ‡Æö‡Æ∞‡Æø ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Øç/‡Æµ‡Æø‡Æ©‡Ææ‡Æü‡Æø ‡ÆÖ‡Æ≥‡Æµ‡ØÄ‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æµ‡ØÅ‡ÆÆ‡Øç.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef532433",
   "metadata": {},
   "source": [
    "## üíæ ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡ÆÆ‡Øà‡ÆØ‡ÆÆ‡Ææ‡Æï‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü ‡ÆÖ‡ÆÆ‡Øà‡Æ™‡Øç‡Æ™‡ØÅ\n",
    "\n",
    "**‡Æá‡Æ®‡Øç‡Æ§ ‡Æ®‡Øã‡Æü‡Øç‡Æ™‡ØÅ‡Æï‡Øç ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï ‡Æ§‡Æø‡Æ±‡Æ©‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æï CPU ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà CUDA ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡Øá‡Æ≤‡Ææ‡Æï ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡ØÅ‡Æ∞‡Æø‡ÆÆ‡Øà ‡ÆÖ‡Æ≥‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.**\n",
    "\n",
    "### ‡Æè‡Æ©‡Øç CPU ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç?\n",
    "- **30-50% ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï** ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Ææ‡Æü‡ØÅ CUDA ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æï‡Øà‡ÆØ‡Æø‡Æ≤‡Øç\n",
    "- **‡Æé‡Æ®‡Øç‡Æ§‡Æµ‡Øä‡Æ∞‡ØÅ ‡Æπ‡Ææ‡Æ∞‡Øç‡Æü‡Øç‡Æµ‡Øá‡Æ∞‡Æø‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç** (GPU ‡Æ§‡Øá‡Æµ‡Øà‡ÆØ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà)\n",
    "- **‡Æö‡Æ∞‡Ææ‡Æö‡Æ∞‡Æø ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Øç** ‡Æ§‡Æ∞‡Æµ‡ØÅ‡Æï‡Æ≥‡Øà ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÄ‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æâ‡Æï‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ\n",
    "- **‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï ‡Æ™‡Æø‡Æ∞‡Æö‡Øç‡Æö‡Æø‡Æ©‡Øà‡Æï‡Æ≥‡Øà‡Æ§‡Øç ‡Æ§‡Æµ‡Æø‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ** ‡Æ™‡Æ≤ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡Æö‡Øã‡Æ§‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Øã‡Æ§‡ØÅ\n",
    "\n",
    "### ‡Æ§‡Ææ‡Æ©‡Æø‡ÆØ‡Æô‡Øç‡Æï‡Æø ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ§‡Øá‡Æ∞‡Øç‡Æµ‡ØÅ\n",
    "‡Æá‡Æ®‡Øç‡Æ§ ‡Æ®‡Øã‡Æü‡Øç‡Æ™‡ØÅ‡Æï‡Øç ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï‡Æµ‡Øá ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡Æ™‡Æø‡Æ©‡Øç‡Æµ‡Æ∞‡ØÅ‡ÆÆ‡Ææ‡Æ±‡ØÅ ‡Æµ‡Æü‡Æø‡Æï‡Æü‡Øç‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ:\n",
    "1. ‚úÖ **CPU-‡Æï‡Øç‡Æï‡ØÅ ‡Æâ‡Æï‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç** (‡Æé.‡Æï‡Ææ., `phi-4-mini-cpu`, `qwen2.5-0.5b-cpu-int4`)\n",
    "2. ‚úÖ **‡Æï‡ØÅ‡Æµ‡Ææ‡Æ£‡Øç‡Æü‡Øà‡Æ∏‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç** (‡Æé.‡Æï‡Ææ., `*-int4`, `*-q4`)\n",
    "3. ‚ö†Ô∏è **‡ÆÆ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Øç** (CPU ‡Æï‡Æø‡Æü‡Øà‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç CUDA ‡Æ§‡Æµ‡Æø‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç)\n",
    "4. ‚ùå **CUDA ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç** (CPU ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡Æ§‡Æ™‡Øã‡Æ§‡ØÅ ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç)\n",
    "\n",
    "### ‡Æï‡Øà‡ÆØ‡Øá‡Æü‡ØÅ ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ÆÆ‡Øç\n",
    "‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÄ‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ, `BENCH_MODELS` ‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡Æø‡ÆØ‡Øà ‡ÆÖ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç:\n",
    "```python\n",
    "import os\n",
    "os.environ['BENCH_MODELS'] = 'phi-4-mini,qwen2.5-0.5b'  # Will auto-select CPU variants\n",
    "```\n",
    "\n",
    "### ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æ© ‡Æ™‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æ∞‡Øà‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç\n",
    "- `phi-3.5-mini` (~2GB RAM)\n",
    "- `qwen2.5-0.5b` (~500MB RAM)\n",
    "- `phi-4-mini` (~4GB RAM)\n",
    "- `qwen2.5-3b` (~3GB RAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe2c2d",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡ØÅ ‡Æ®‡Æø‡Æ±‡ØÅ‡Æµ‡Æ≤‡Øç\n",
    "‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æ© ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§‡Æ™‡Æü‡Øç‡Æö ‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øà ‡Æ®‡Æø‡Æ±‡ØÅ‡Æµ‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ:\n",
    "- ‡Æâ‡Æ≥‡Øç‡Æ≥‡ØÇ‡Æ∞‡Øç ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡Æï‡Æ≥‡Øà ‡Æ®‡Æø‡Æ∞‡Øç‡Æµ‡Æï‡Æø‡Æï‡Øç‡Æï/‡Æá‡Æ£‡Øà‡Æï‡Øç‡Æï `foundry-local-sdk`.\n",
    "- ‡Æé‡Æ≥‡Æø‡ÆØ ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ®‡Æø‡Æ±‡Øà‡Æµ‡ØÅ ‡Æï‡Æø‡Æ≥‡Øà‡ÆØ‡Æ£‡Øç‡Æü‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æï `openai`.\n",
    "- `numpy` (‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æï‡Ææ‡Æ≤ ‡Æµ‡Æø‡Æ∞‡Æø‡Æµ‡Ææ‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æµ‡ØÜ‡Æï‡Øç‡Æü‡Æ∞‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Øç ‡Æ§‡Øá‡Æµ‡Øà‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç).\n",
    "Idempotent; ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æá‡ÆØ‡Æï‡Øç‡Æï‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡Æ™‡Ææ‡Æ§‡ØÅ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡Ææ‡Æ©‡Æ§‡ØÅ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5751c",
   "metadata": {},
   "source": [
    "# ‡Æ®‡Æø‡Æ≤‡Øà‡ÆÆ‡Øà\n",
    "‡Æá‡Æ®‡Øç‡Æ§ ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æ®‡Øã‡Æü‡Øç‡Æ™‡ØÅ‡Æï‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡ØÇ‡Æ∞‡Øç Foundry Local ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡Æí‡Æ∞‡Øá ‡Æ®‡Øá‡Æ∞‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æ™‡Æ≤ ‡Æ§‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÇ‡Æ≤ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÖ‡Æ≤‡Æø‡ÆØ‡Ææ‡Æ∏‡Øç-‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æ§‡Ææ‡ÆÆ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ§‡ØÅ‡Æ∞‡Æø‡Æ§‡ÆÆ‡Øç (tokens/sec) ‡ÆÖ‡Æ≥‡Æµ‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æá‡Æ§‡ØÅ:\n",
    "- ‡Æï‡Æø‡Æü‡Øà‡Æï‡Øç‡Æï‡Æï‡Øç‡Æï‡ØÇ‡Æü‡Æø‡ÆØ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æê‡Æü‡Æø‡Æï‡Æ≥‡Øà ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æï‡Æø‡Æ±‡Æ§‡ØÅ (‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ BENCH_MODELS ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡Æø‡ÆØ‡Øà ‡ÆÆ‡Æ§‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ).\n",
    "- ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç ‡Æï‡ØÅ‡Æ≥‡Æø‡Æ∞‡Øç ‡Æ§‡Øä‡Æü‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æï‡ØÅ‡Æ±‡Øà‡Æï‡Øç‡Æï ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æí‡Æ∞‡ØÅ‡ÆÆ‡ØÅ‡Æ±‡Øà ‡Æµ‡ØÜ‡Æ™‡Øç‡Æ™‡ÆÆ‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "- ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ≤ ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ®‡Æø‡Æ±‡Øà‡Æµ‡ØÅ ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø, ‡Æ§‡Ææ‡ÆÆ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡Øà ‡Æ§‡Øä‡Æï‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "- JSON ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç Markdown-‡Æï‡Øç‡Æï‡ØÅ ‡Æè‡Æ±‡Øç‡Æ± ‡Æö‡ØÅ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÖ‡Æü‡Øç‡Æü‡Æµ‡Æ£‡Øà‡ÆØ‡Øà ‡Æµ‡ØÜ‡Æ≥‡Æø‡ÆØ‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "\n",
    "‡Æö‡Æø‡Æ±‡Æø‡ÆØ ‡ÆÆ‡Øä‡Æ¥‡Æø ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æµ‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡Æï-offs (‡Æµ‡Øá‡Æï‡ÆÆ‡Øç vs. ‡Æ§‡Æø‡Æ±‡Æ©‡Øç) ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ, ‡Æµ‡Æ¥‡Æø‡ÆÆ‡ØÅ‡Æ±‡Øà ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æö‡ØÜ‡Æ≤‡Æµ‡ØÅ‡Æï‡Øç ‡Æï‡Æ£‡Æï‡Øç‡Æï‡ØÄ‡Æü‡ØÅ‡Æï‡Æ≥‡Øà ‡Æí‡Æ∞‡ØÅ‡Æô‡Øç‡Æï‡Æø‡Æ£‡Øà‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æá‡Æ§‡Øà‡Æ™‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æµ‡ØÅ‡ÆÆ‡Øç.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880bb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38238d54",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡Æö‡Øá‡Æµ‡Øà ‡Æü‡ÆØ‡Æï‡Øç‡Æ©‡Øã‡Æ∏‡Øç‡Æü‡Æø‡Æï‡Øç & ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øç\n",
    "‡Æ™‡Æ≤ ‡Æâ‡Æ§‡Øç‡Æ§‡Æø‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ ‡Æö‡Øá‡Æµ‡Øà ‡ÆÜ‡Æ∞‡Øã‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡Æö‡Øã‡Æ§‡Æ©‡Øà ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øà ‡ÆÆ‡Øá‡Æ±‡Øç‡Æï‡Øä‡Æ≥‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ:\n",
    "\n",
    "1. ‡Æ™‡Øä‡Æ§‡ØÅ‡Æµ‡Ææ‡Æ© ‡Æ™‡Øã‡Æ∞‡Øç‡Æü‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æ®‡Øá‡Æ∞‡Æü‡Æø ‡ÆÜ‡Æ∞‡Øã‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡Æé‡Æ£‡Øç‡Æü‡Øç‡Æ™‡Ææ‡ÆØ‡Æø‡Æ£‡Øç‡Æü‡Øç ‡Æö‡Øã‡Æ§‡Æ©‡Øà‡Æï‡Æ≥‡Øç  \n",
    "   ‡Æá‡Æ§‡ØÅ ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æø‡Æô‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æö‡Øá‡Æµ‡Øà ‡ÆÖ‡Æ£‡ØÅ‡Æï‡Æï‡Øç‡Æï‡ØÇ‡Æü‡Æø‡ÆØ‡Æ§‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡Ææ ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡Øà ‡Æâ‡Æ±‡ØÅ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "\n",
    "2. REST API ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ™‡Æü‡Øç‡Æü‡Æø‡ÆØ‡Æ≤‡Øç  \n",
    "3. ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡Æï‡Øç‡Æï‡ØÇ‡Æü‡Æø‡ÆØ ‡Æö‡Æø‡Æï‡Øç‡Æï‡Æ≤‡Øç ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æµ‡ØÅ ‡Æµ‡Æ¥‡Æø‡Æï‡Ææ‡Æü‡Øç‡Æü‡ØÅ‡Æ§‡Æ≤‡Øà ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4481aa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "‚úÖ Service auto-detected via SDK at http://127.0.0.1:59959/v1\n",
      "\n",
      "‚úÖ Service detected - ready for benchmarking\n"
     ]
    }
   ],
   "source": [
    "import os, time, statistics, json\n",
    "import requests\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running and detect the endpoint automatically.\"\"\"\n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    # Strategy 1: Use SDK to detect service automatically\n",
    "    try:\n",
    "        # Try to connect to any available model to detect the service\n",
    "        # This will auto-discover the endpoint\n",
    "        temp_manager = FoundryLocalManager()\n",
    "        detected_endpoint = temp_manager.endpoint\n",
    "        \n",
    "        if detected_endpoint:\n",
    "            print(f\"‚úÖ Service auto-detected via SDK at {detected_endpoint}\")\n",
    "            \n",
    "            # Verify by listing models\n",
    "            try:\n",
    "                models_response = requests.get(f\"{detected_endpoint}/v1/models\", timeout=2)\n",
    "                if models_response.status_code == 200:\n",
    "                    models_data = models_response.json()\n",
    "                    model_count = len(models_data.get('data', []))\n",
    "                    print(f\"‚úÖ Found {model_count} models available\")\n",
    "                    if model_count > 0:\n",
    "                        model_ids = [m.get('id', 'unknown') for m in models_data.get('data', [])[:10]]\n",
    "                        print(f\"   Models: {model_ids}\")\n",
    "                return detected_endpoint\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not list models: {e}\")\n",
    "                return detected_endpoint\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  SDK auto-detection failed: {e}\")\n",
    "    \n",
    "    # Strategy 2: Fallback to manual port scanning\n",
    "    print(\"[Diagnostic] Trying manual port detection...\")\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "        \"http://localhost:57127\",\n",
    "        \"http://127.0.0.1:57127\",\n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"‚úÖ Service found at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"‚úÖ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            model_ids = [m.get('id', 'unknown') for m in models_data.get('data', [])[:10]]\n",
    "                            print(f\"   Models: {model_ids}\")\n",
    "                        return endpoint\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Could not list models: {e}\")\n",
    "                    return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\n‚ùå Foundry Local service not found!\")\n",
    "    print(\"\\nüí° To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-0.5b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\n‚úÖ Service detected - ready for benchmarking\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No service detected - benchmarking will likely fail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7df483",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æï‡Æü‡Øç‡Æü‡ÆÆ‡Øà‡Æ™‡Øç‡Æ™‡ØÅ & ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æµ‡Æü‡Æø‡Æï‡Æü‡Øç‡Æü‡Æ≤‡Øç (‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø-‡ÆÖ‡Æ™‡Øç‡Æü‡Æø‡ÆÆ‡Øà‡Æ∏‡Øç‡Æü‡Øç‡Æü‡ØÅ)\n",
    "‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æö‡Ææ‡Æ∞‡Øç‡Æ®‡Øç‡Æ§ ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡ÆÖ‡Æ≥‡Æµ‡ØÅ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øà (‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æ≥‡Øç, ‡Æ™‡Øç‡Æ∞‡Ææ‡ÆÆ‡Øç‡Æ™‡Øç‡Æü‡Øç, ‡Æú‡ØÜ‡Æ©‡Æ∞‡Øá‡Æ∑‡Æ©‡Øç ‡ÆÖ‡ÆÆ‡Øà‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç) ‡ÆÖ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æé‡Æ£‡Øç‡Æü‡Øç‡Æ™‡Ææ‡ÆØ‡Æø‡Æ£‡Øç‡Æü‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡Æì‡Æµ‡Æ∞‡Øç‡Æ∞‡Øà‡Æü‡ØÅ ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "\n",
    "**‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø ‡ÆÖ‡Æ™‡Øç‡Æü‡Æø‡ÆÆ‡Øà‡Æ∏‡Øá‡Æ∑‡Æ©‡Øç ‡Æâ‡Æ§‡Øç‡Æ§‡Æø:**\n",
    "- ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï ‡Æµ‡Æü‡Æø‡Æï‡Æü‡Øç‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, CPU ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Øà CUDA-‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡Øá‡Æ≤‡Ææ‡Æï ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡ØÅ‡Æ∞‡Æø‡ÆÆ‡Øà ‡ÆÖ‡Æ≥‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "- CPU ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç 30-50% ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø‡ÆØ‡Øà ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Øà ‡Æ™‡Æ∞‡Ææ‡ÆÆ‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©\n",
    "- ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡ØÅ‡Æ∞‡Æø‡ÆÆ‡Øà: CPU-‡ÆÖ‡Æ™‡Øç‡Æü‡Æø‡ÆÆ‡Øà‡Æ∏‡Øç‡Æü‡Øç‡Æü‡ØÅ > ‡Æï‡ØÅ‡Æµ‡Ææ‡Æ£‡Øç‡Æü‡Øà‡Æ∏‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç > ‡Æ™‡Æø‡Æ± ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Øç > CUDA (‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡Æ§‡Æ™‡Øã‡Æ§‡ØÅ ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá)\n",
    "- BENCH_MODELS ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æö‡Øç‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡Æï‡Øà‡ÆØ‡Øá‡Æü‡ØÅ ‡Æì‡Æµ‡Æ∞‡Øç‡Æ∞‡Øà‡Æü‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æ≤‡Ææ‡ÆÆ‡Øç\n",
    "\n",
    "‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æø‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø-‡Æ§‡Æø‡Æ±‡Æ©‡Øç ‡Æµ‡Ææ‡ÆØ‡Øç‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æµ‡Æü‡Æø‡Æï‡Æü‡Øç‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©, ‡Æé‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Øá‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡ØÜ‡Æü‡ØÅ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ© ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡Øà ‡Æï‡Ææ‡Æü‡Øç‡Æü ‡Æâ‡Æ§‡Æµ‡Æø‡Æï‡Æ∞‡ÆÆ‡Ææ‡Æ© ‡Æ≤‡Ææ‡Æï‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c1a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model discovery failed: Connection error.\n",
      "Warning: No models discovered at BASE_URL. Ensure Foundry Local is running and models are loaded.\n",
      "Notice: The following requested models were not discovered and may fail during benchmarking: ['phi-4-mini', 'gpt-oss-20b']\n",
      "Benchmarking models: ['phi-4-mini', 'gpt-oss-20b']\n",
      "Rounds: 3  Max Tokens: 120  Temp: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Benchmark configuration & model discovery (override via environment variables)\n",
    "BASE_URL = os.getenv('FOUNDRY_LOCAL_ENDPOINT', discovered_endpoint if 'discovered_endpoint' in dir() and discovered_endpoint else 'http://127.0.0.1:59959')\n",
    "if not BASE_URL.endswith('/v1'):\n",
    "    BASE_URL = f\"{BASE_URL}/v1\"\n",
    "API_KEY = os.getenv('API_KEY','not-needed')\n",
    "\n",
    "_raw_models = os.getenv('BENCH_MODELS','').strip()\n",
    "requested_models = [m.strip() for m in _raw_models.split(',') if m.strip()] if _raw_models else []\n",
    "\n",
    "ROUNDS = int(os.getenv('BENCH_ROUNDS','3'))\n",
    "if ROUNDS < 1:\n",
    "    raise ValueError('BENCH_ROUNDS must be >= 1')\n",
    "PROMPT = os.getenv('BENCH_PROMPT','Explain retrieval augmented generation briefly.')\n",
    "MAX_TOKENS = int(os.getenv('BENCH_MAX_TOKENS','120'))\n",
    "TEMPERATURE = float(os.getenv('BENCH_TEMPERATURE','0.2'))\n",
    "\n",
    "def _discover_models():\n",
    "    try:\n",
    "        c = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "        data = c.models.list().data\n",
    "        return [m.id for m in data]\n",
    "    except Exception as e:\n",
    "        print(f\"Model discovery failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def _prefer_cpu_models(model_list):\n",
    "    \"\"\"Filter models to prefer CPU variants over CUDA for memory efficiency.\n",
    "    \n",
    "    Priority order:\n",
    "    1. CPU-optimized models (e.g., *-cpu, *-cpu-int4)\n",
    "    2. Quantized models without CUDA (e.g., *-q4, *-int4)\n",
    "    3. Other models (excluding CUDA variants if CPU available)\n",
    "    \"\"\"\n",
    "    # Group models by base name (removing variant suffixes)\n",
    "    from collections import defaultdict\n",
    "    model_groups = defaultdict(list)\n",
    "    \n",
    "    for model in model_list:\n",
    "        # Extract base name (before variant like -cpu, -cuda, -int4, etc.)\n",
    "        base_name = model.split('-cpu')[0].split('-cuda')[0].split('-int4')[0].split('-q4')[0]\n",
    "        model_groups[base_name].append(model)\n",
    "    \n",
    "    selected = []\n",
    "    for base_name, variants in model_groups.items():\n",
    "        # Prioritize CPU variants\n",
    "        cpu_variants = [m for m in variants if '-cpu' in m.lower()]\n",
    "        cuda_variants = [m for m in variants if '-cuda' in m.lower()]\n",
    "        other_variants = [m for m in variants if m not in cpu_variants and m not in cuda_variants]\n",
    "        \n",
    "        if cpu_variants:\n",
    "            # Prefer CPU variants\n",
    "            selected.extend(cpu_variants)\n",
    "            print(f\"‚úì Selected CPU variant for {base_name}: {cpu_variants[0]}\")\n",
    "        elif other_variants:\n",
    "            # Use non-CUDA variants if available\n",
    "            selected.extend(other_variants[:1])  # Take first one\n",
    "        elif cuda_variants:\n",
    "            # Only use CUDA if no other option\n",
    "            selected.extend(cuda_variants[:1])\n",
    "            print(f\"‚ö†Ô∏è  Using CUDA variant for {base_name}: {cuda_variants[0]} (no CPU variant found)\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "_discovered = _discover_models()\n",
    "if not _discovered:\n",
    "    print(\"Warning: No models discovered at BASE_URL. Ensure Foundry Local is running and models are loaded.\")\n",
    "\n",
    "if not requested_models or requested_models == ['auto'] or 'ALL' in requested_models:\n",
    "    # Auto mode: discover and prefer CPU models\n",
    "    MODELS = _prefer_cpu_models(_discovered)\n",
    "    if len(MODELS) < len(_discovered):\n",
    "        print(f\"üí° Memory-optimized: Using {len(MODELS)} CPU models instead of all {len(_discovered)} variants\")\n",
    "else:\n",
    "    # Filter requested models to those actually discovered\n",
    "    MODELS = [m for m in requested_models if m in _discovered] or requested_models  # fallback to requested even if not discovered\n",
    "    missing = [m for m in requested_models if m not in _discovered]\n",
    "    if missing:\n",
    "        print(f\"Notice: The following requested models were not discovered and may fail during benchmarking: {missing}\")\n",
    "\n",
    "MODELS = [m for m in MODELS if m]\n",
    "if not MODELS:\n",
    "    raise ValueError(\"No models available to benchmark. Start a model (e.g., 'foundry model run phi-4-mini') or set BENCH_MODELS.\")\n",
    "\n",
    "print(f\"Benchmarking models: {MODELS}\\nRounds: {ROUNDS}  Max Tokens: {MAX_TOKENS}  Temp: {TEMPERATURE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92895bf4",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÖ‡Æ£‡ØÅ‡Æï‡Æ≤‡Øç ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç (‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø-‡ÆÖ‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ÆÆ‡Øç ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ)\n",
    "`ensure_loaded(alias)` Foundry Local SDK ‡ÆÆ‡ØÅ‡Æ±‡Øà‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, CPU ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡ØÅ‡Æ∞‡Æø‡ÆÆ‡Øà‡ÆØ‡ØÅ‡Æü‡Æ©‡Øç:\n",
    "1. **FoundryLocalManager(alias)** - ‡Æ§‡Øá‡Æµ‡Øà‡ÆØ‡Ææ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æö‡Øá‡Æµ‡Øà‡ÆØ‡Øà ‡Æ§‡ØÅ‡Æµ‡Æï‡Øç‡Æï‡Æø, ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øà ‡Æè‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "2. **CPU ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡ØÅ‡Æ∞‡Æø‡ÆÆ‡Øà** - CUDA ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ ‡Æè‡Æ±‡Øç‡Æ±‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç ‡Æé‡Æö‡Øç‡Æö‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Ææ‡Æï CPU ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æµ‡Æ¥‡Æø‡ÆØ‡Øà ‡Æ™‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æ∞‡Øà‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "3. **‡Æ§‡Ææ‡Æ©‡Æø‡ÆØ‡Æô‡Øç‡Æï ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øç** - ‡Æé‡Æ£‡Øç‡Æü‡Øç‡Æ™‡Ææ‡ÆØ‡Æø‡Æ£‡Øç‡Æü‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡Øç‡Æü‡Øà ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "4. **OpenAI ‡Æï‡Æø‡Æ≥‡Øà‡ÆØ‡Æ©‡Øç‡Æü‡Øç** - ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æµ‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡ÆÖ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æï‡Æø‡Æ≥‡Øà‡ÆØ‡Æ©‡Øç‡Æü‡Øà ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "5. **‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æµ‡ØÅ** - alias ‡Æê ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Øç‡Æü ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ID ‡Æï‡Øç‡Æï‡ØÅ ‡Æ§‡ØÄ‡Æ∞‡Øç‡ÆÆ‡Ææ‡Æ©‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ\n",
    "\n",
    "**‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø-‡ÆÖ‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ÆÆ‡Øç ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ:** CPU ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Øç ‡Æ™‡Øä‡Æ§‡ØÅ‡Æµ‡Ææ‡Æï CUDA ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æí‡Æ™‡Øç‡Æ™‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç 30-50% ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§ ‡ÆÆ‡ØÜ‡ÆÆ‡Æ∞‡Æø‡ÆØ‡Øà ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©, ‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æ§‡Æ∞‡Æµ‡ØÅ‡Æ§‡Øç‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Øà ‡Æ™‡Æ∞‡Ææ‡ÆÆ‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©. ‡Æ§‡Ææ‡Æ©‡Æø‡ÆØ‡Æô‡Øç‡Æï ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øç ‡ÆÆ‡ØÅ‡Æ±‡Øà‡ÆØ‡Æø‡Æ≤‡Øç, ‡ÆÖ‡ÆÆ‡Øà‡Æ™‡Øç‡Æ™‡ØÅ ‡Æö‡ØÜ‡Æ≤‡Øç‡Æï‡Æ≥‡Øç CPU ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï‡Æµ‡Øá ‡Æµ‡Æü‡Æø‡Æï‡Æü‡Øç‡Æü‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c4ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_loaded(alias):\n",
    "    \"\"\"Return (manager, client, model_id) ensuring the alias is accessible.\n",
    "    \n",
    "    This follows the official Foundry Local SDK pattern with CPU preference:\n",
    "    1. FoundryLocalManager(alias) - Automatically starts service and loads model if needed\n",
    "    2. Prefers CPU variants over CUDA for memory efficiency\n",
    "    3. Create OpenAI client with manager's endpoint\n",
    "    4. Resolve model ID from alias\n",
    "    \n",
    "    Raises RuntimeError with guidance if the model cannot be accessed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize manager - this auto-starts service and loads model if needed\n",
    "        # Note: By default, Foundry Local may select CUDA if available\n",
    "        # For memory efficiency, we recommend using CPU-optimized aliases explicitly\n",
    "        m = FoundryLocalManager(alias)\n",
    "        \n",
    "        # Get resolved model ID\n",
    "        info = m.get_model_info(alias)\n",
    "        model_id = getattr(info, 'id', alias)\n",
    "        \n",
    "        # Warn if CUDA variant was loaded\n",
    "        if 'cuda' in model_id.lower():\n",
    "            print(f\"‚ö†Ô∏è  Loaded CUDA variant: '{alias}' -> '{model_id}'\")\n",
    "            print(f\"   üí° For lower memory usage, use CPU variant with: foundry model run {alias.split('-cuda')[0]}-cpu\")\n",
    "        else:\n",
    "            print(f\"‚úì Loaded model: '{alias}' -> '{model_id}' at {m.endpoint}\")\n",
    "            if 'cpu' in model_id.lower():\n",
    "                print(f\"   ‚úÖ Using memory-optimized CPU variant\")\n",
    "        \n",
    "        # Create OpenAI-compatible client for local Foundry service\n",
    "        c = OpenAI(base_url=m.endpoint, api_key=m.api_key or 'not-needed')\n",
    "        \n",
    "        return m, c, model_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to load model '{alias}'.\\n\"\n",
    "            f\"Original error: {e}\\n\\n\"\n",
    "            f\"üí° To fix:\\n\"\n",
    "            f\"   1. Ensure Foundry Local service is running: foundry service start\\n\"\n",
    "            f\"   2. Verify model is available: foundry model ls\\n\"\n",
    "            f\"   3. For CPU-optimized models: foundry model run {alias}\\n\"\n",
    "            f\"   4. Check available variants with: foundry model search {alias.split('-')[0]}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2944316",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡Æí‡Æ±‡Øç‡Æ±‡Øà ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Ææ‡Æü‡ØÅ\n",
    "`run_round` ‡Æí‡Æ∞‡ØÅ ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ®‡Æø‡Æ±‡Øà‡Æµ‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡Øà ‡ÆÆ‡Øá‡Æ±‡Øç‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ, ‡Æ§‡Ææ‡ÆÆ‡Æ§‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æ™‡ØÅ‡Æ≤‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡Æø ‡ÆÖ‡Æ©‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. API ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡Æï‡Øç‡Æï‡Øà‡Æï‡Æ≥‡Øà ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Ææ‡Æµ‡Æø‡Æü‡Øç‡Æü‡Ææ‡Æ≤‡Øç, ~4 ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æ≥‡Øç/‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç ‡Æé‡Æ©‡Øç‡Æ± ‡ÆÆ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÄ‡Æü‡Øç‡Æü‡ØÅ ‡ÆÆ‡ØÅ‡Æ±‡Øà‡ÆØ‡Øà ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø ‡ÆÖ‡Æµ‡Æ±‡Øç‡Æ±‡Øà ‡Æï‡Æ£‡Æï‡Øç‡Æï‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æá‡Æ§‡ØÅ ‡ÆÖ‡Æ©‡Øà‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Æï‡Øç‡Æï‡ØÇ‡Æü‡Æø‡ÆØ ‡ÆÖ‡Æ≥‡Æµ‡ØÅ‡Æï‡Øã‡Æ≤‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ±‡ØÅ‡Æ§‡Æø‡Æö‡ØÜ‡ÆØ‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6524ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_round(client, model_id, prompt):\n",
    "    \"\"\"Execute one chat completion round with comprehensive metric capture.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (latency_sec, total_tokens, prompt_tokens, completion_tokens, response_text)\n",
    "        Token counts are estimated if API doesn't provide them.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{'role':'user','content':prompt}],\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    \n",
    "    # Extract response content\n",
    "    content = resp.choices[0].message.content if resp.choices else \"\"\n",
    "    \n",
    "    # Try to get usage from API\n",
    "    usage = getattr(resp, 'usage', None)\n",
    "    prompt_tokens = getattr(usage, 'prompt_tokens', None) if usage else None\n",
    "    completion_tokens = getattr(usage, 'completion_tokens', None) if usage else None\n",
    "    total_tokens = getattr(usage, 'total_tokens', None) if usage else None\n",
    "    \n",
    "    # Estimate tokens if API doesn't provide them (~4 chars per token for English)\n",
    "    if prompt_tokens is None:\n",
    "        prompt_tokens = len(prompt) // 4\n",
    "    if completion_tokens is None:\n",
    "        completion_tokens = len(content) // 4\n",
    "    if total_tokens is None:\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "    \n",
    "    return latency, total_tokens, prompt_tokens, completion_tokens, content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d60c14",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æ≤‡ØÇ‡Æ™‡Øç & ‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ\n",
    "‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ:\n",
    "- ‡Æï‡ØÅ‡Æ≥‡Æø‡Æ∞‡Øç‡Æ®‡Øç‡Æ§ ‡Æ§‡Øä‡Æü‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Øà ‡Æï‡ØÅ‡Æ±‡Øà‡Æï‡Øç‡Æï ‡Æµ‡Øä‡Æ∞‡Øç‡ÆÆ‡Øç‡ÆÖ‡Æ™‡Øç (‡Æ™‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æø‡Æµ‡Æø‡Æµ‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æö‡Øá‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Ææ‡Æ§‡ØÅ).\n",
    "- ‡Æ™‡Æ≤ ‡ÆÆ‡ØÅ‡Æ±‡Øà ‡ÆÖ‡Æ≥‡Æµ‡Æø‡Æü‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æ≥‡Øç, ‡Æ§‡Ææ‡ÆÆ‡Æ§‡ÆÆ‡Øç + ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡Æ§‡Æø‡Æµ‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©.\n",
    "- ‡Æö‡Æ∞‡Ææ‡Æö‡Æ∞‡Æø, p95, ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Øç/‡Æµ‡Æø‡Æ©‡Ææ‡Æü‡Æø ‡ÆÜ‡Æï‡Æø‡ÆØ‡Æµ‡Æ±‡Øç‡Æ±‡Øà ‡Æ§‡Øä‡Æï‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
    "‡Æ™‡Æø‡Æ©‡Øç‡Æ©‡Æ∞‡Øç ‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡Ææ‡Æï ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Æø‡Æ©‡Øç ‡Æö‡ØÅ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ÆÆ‡Ææ‡Æ© ‡ÆÖ‡Æï‡Æ∞‡Ææ‡Æ§‡Æø‡Æï‡Æ≥‡Øà ‡Æö‡Øá‡ÆÆ‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a31f53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Loaded CUDA variant: 'phi-4-mini' -> 'Phi-4-mini-instruct-cuda-gpu:4'\n",
      "   üí° For lower memory usage, use CPU variant with: foundry model run phi-4-mini-cpu\n",
      "‚ö†Ô∏è  Loaded CUDA variant: 'gpt-oss-20b' -> 'gpt-oss-20b-cuda-gpu:1'\n",
      "   üí° For lower memory usage, use CPU variant with: foundry model run gpt-oss-20b-cpu\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for alias in MODELS:\n",
    "    try:\n",
    "        m, client, model_id = ensure_loaded(alias.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    # Warmup (not recorded)\n",
    "    try:\n",
    "        run_round(client, model_id, PROMPT)\n",
    "    except Exception as e:\n",
    "        print(f\"Warmup failed for {alias}: {e}\")\n",
    "        continue\n",
    "\n",
    "    latencies, tps = [], []\n",
    "    prompt_tokens_total = 0\n",
    "    completion_tokens_total = 0\n",
    "    total_tokens_sum = 0\n",
    "    sample_output = None\n",
    "\n",
    "    for round_num in range(ROUNDS):\n",
    "        try:\n",
    "            latency, total_tokens, p_tokens, c_tokens, content = run_round(client, model_id, PROMPT)\n",
    "        except Exception as e:\n",
    "            print(f\"Round {round_num+1} failed for {alias}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        prompt_tokens_total += p_tokens\n",
    "        completion_tokens_total += c_tokens\n",
    "        total_tokens_sum += total_tokens\n",
    "        \n",
    "        # Calculate tokens per second\n",
    "        if total_tokens and latency > 0:\n",
    "            tps.append(total_tokens / latency)\n",
    "        \n",
    "        # Capture first successful output as sample\n",
    "        if sample_output is None:\n",
    "            sample_output = content[:200]  # First 200 chars\n",
    "\n",
    "    if not latencies:\n",
    "        print(f\"Skipping {alias}: no successful rounds.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate statistics\n",
    "    rounds_ok = len(latencies)\n",
    "    latency_avg = statistics.mean(latencies)\n",
    "    latency_min = min(latencies)\n",
    "    latency_max = max(latencies)\n",
    "    latency_p95 = statistics.quantiles(latencies, n=20)[-1] if len(latencies) > 1 else latencies[0]\n",
    "    tokens_per_sec_avg = statistics.mean(tps) if tps else None\n",
    "    \n",
    "    # Average tokens per round\n",
    "    avg_prompt_tokens = prompt_tokens_total / rounds_ok if rounds_ok else 0\n",
    "    avg_completion_tokens = completion_tokens_total / rounds_ok if rounds_ok else 0\n",
    "    avg_total_tokens = total_tokens_sum / rounds_ok if rounds_ok else 0\n",
    "\n",
    "    summary.append({\n",
    "        'alias': alias,\n",
    "        'model_id': model_id,\n",
    "        'latency_avg_s': latency_avg,\n",
    "        'latency_min_s': latency_min,\n",
    "        'latency_max_s': latency_max,\n",
    "        'latency_p95_s': latency_p95,\n",
    "        'tokens_per_sec_avg': tokens_per_sec_avg,\n",
    "        'avg_prompt_tokens': avg_prompt_tokens,\n",
    "        'avg_completion_tokens': avg_completion_tokens,\n",
    "        'avg_total_tokens': avg_total_tokens,\n",
    "        'prompt_tokens_total': prompt_tokens_total,\n",
    "        'completion_tokens_total': completion_tokens_total,\n",
    "        'total_tokens_sum': total_tokens_sum,\n",
    "        'rounds_ok': rounds_ok,\n",
    "        'configured_rounds': ROUNDS,\n",
    "        'sample_output': sample_output,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e635d",
   "metadata": {},
   "source": [
    "### ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç: ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æµ‡ØÅ‡Æï‡Æ≥‡Øà ‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æ§‡Æ≤‡Øç\n",
    "JSON ‡Æö‡ØÅ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Øà (‡Æá‡ÆØ‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡ØÅ ‡Æâ‡Æï‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ) ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç Markdown ‡ÆÖ‡Æü‡Øç‡Æü‡Æµ‡Æ£‡Øà‡ÆØ‡Øà (‡ÆÆ‡Æ©‡Æø‡Æ§‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æâ‡Æï‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ) ‡Æá‡Æ£‡Øà‡Æ®‡Øç‡Æ§ ‡Æ®‡ØÜ‡Æü‡ØÅ‡Æµ‡Æ∞‡Æø‡Æö‡Øà‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç ‡Æµ‡ØÜ‡Æ≥‡Æø‡ÆØ‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡ÆÖ‡Æü‡Øç‡Æü‡Æµ‡Æ£‡Øà‡ÆØ‡Æø‡Æ≤‡Øç tail insights ‡Æï‡Øç‡Æï‡Ææ‡Æ© p95 latency ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡ØÅ ‡Æ§‡Æ∞‡Æµ‡ØÅ‡Æï‡Æ≥‡Øç ‡Æï‡Æø‡Æü‡Øà‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç tokens/sec ‡ÆÜ‡Æï‡Æø‡ÆØ‡Æµ‡Øà ‡ÆÖ‡Æü‡Æô‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93452b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä PERFORMANCE SUMMARY TABLE\n",
      "================================================================================\n",
      "Model       | Latency (avg) | Latency (P95) | Throughput | Tokens | Success | Rating\n",
      "-------------+---------------+---------------+------------+--------+---------+--------\n",
      "phi-4-mini  | üü¢ 38.815s     | 39.191s       | üü¢ 4.6      | 179    | 3/3     | ‚≠ê‚≠ê‚≠ê   \n",
      "gpt-oss-20b | üî¥ 160.754s    | 220.707s      | üî¥ 1.1      | 169    | 3/3     | ‚≠ê     \n",
      "\n",
      "================================================================================\n",
      "Legend: üü¢ Best  üü° Average  üî¥ Worst  |  Rating: ‚≠ê‚≠ê‚≠ê Excellent  ‚≠ê‚≠ê Good  ‚≠ê Needs Improvement\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED METRICS PER MODEL\n",
      "================================================================================\n",
      "\n",
      "üìä phi-4-mini (Phi-4-mini-instruct-cuda-gpu:4)\n",
      "   Latency:\n",
      "     Average: 38.815s\n",
      "     Min:     38.499s\n",
      "     Max:     39.057s\n",
      "     P95:     39.191s\n",
      "   Tokens:\n",
      "     Avg Prompt:     11\n",
      "     Avg Completion: 168\n",
      "     Avg Total:      179\n",
      "     Throughput:     4.6 tok/s\n",
      "   Rounds: 3/3 successful\n",
      "   Sample Output: Retrieval Augmented Generation (RAG) is a method that combines the capabilities of retrieval and generation to create more accurate and contextually r...\n",
      "\n",
      "üìä gpt-oss-20b (gpt-oss-20b-cuda-gpu:1)\n",
      "   Latency:\n",
      "     Average: 160.754s\n",
      "     Min:     134.951s\n",
      "     Max:     191.753s\n",
      "     P95:     220.707s\n",
      "   Tokens:\n",
      "     Avg Prompt:     11\n",
      "     Avg Completion: 158\n",
      "     Avg Total:      169\n",
      "     Throughput:     1.1 tok/s\n",
      "   Rounds: 3/3 successful\n",
      "   Sample Output: <|channel|>analysis<|message|>We need to explain retrieval augmented generation briefly. Provide concise explanation.<|end|><|start|>assistant<|channe...\n",
      "\n",
      "================================================================================\n",
      "üîç PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üìà Relative Performance (normalized to fastest model)\n",
      "--------------------------------------------------------------------------------\n",
      "Model         | Speed vs Fastest | Latency Delta | Throughput | Efficiency\n",
      "--------------+------------------+---------------+------------+-----------\n",
      "üöÄ phi-4-mini  | ‚ñà‚ñà‚ñà‚ñà‚ñà 100%       | baseline      | 4.6 tok/s  | 100%      \n",
      "üêå gpt-oss-20b | ‚ñà‚ñà‚ñë‚ñë‚ñë 40%        | +121.939s     | 1.1 tok/s  | 24%       \n",
      "\n",
      "================================================================================\n",
      "üìä KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "üèÉ Fastest Model: phi-4-mini\n",
      "   ‚îú‚îÄ Average latency: 38.815s\n",
      "   ‚îú‚îÄ P95 latency: 39.191s\n",
      "   ‚îî‚îÄ Throughput: 4.6 tok/s\n",
      "\n",
      "üêå Slowest Model: gpt-oss-20b\n",
      "   ‚îú‚îÄ Average latency: 160.754s\n",
      "   ‚îî‚îÄ Performance gap: 4.14x slower than fastest\n",
      "\n",
      "‚ö° Highest Throughput: phi-4-mini\n",
      "   ‚îú‚îÄ Throughput: 4.6 tok/s\n",
      "   ‚îî‚îÄ Latency: 38.815s\n",
      "\n",
      "üí° Throughput Range: 4.32x difference between best and worst\n",
      "\n",
      "üíæ Memory Efficiency:\n",
      "\n",
      "================================================================================\n",
      "JSON SUMMARY (for programmatic analysis)\n",
      "================================================================================\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"model_id\": \"Phi-4-mini-instruct-cuda-gpu:4\",\n",
      "    \"latency_avg_s\": 38.81543548901876,\n",
      "    \"latency_min_s\": 38.498725175857544,\n",
      "    \"latency_max_s\": 39.05744504928589,\n",
      "    \"latency_p95_s\": 39.19129209518432,\n",
      "    \"tokens_per_sec_avg\": 4.6117357291016745,\n",
      "    \"avg_prompt_tokens\": 11.0,\n",
      "    \"avg_completion_tokens\": 168.0,\n",
      "    \"avg_total_tokens\": 179.0,\n",
      "    \"prompt_tokens_total\": 33,\n",
      "    \"completion_tokens_total\": 504,\n",
      "    \"total_tokens_sum\": 537,\n",
      "    \"rounds_ok\": 3,\n",
      "    \"configured_rounds\": 3,\n",
      "    \"sample_output\": \"Retrieval Augmented Generation (RAG) is a method that combines the capabilities of retrieval and generation to create more accurate and contextually relevant responses. In this approach, a retrieval s\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"gpt-oss-20b\",\n",
      "    \"model_id\": \"gpt-oss-20b-cuda-gpu:1\",\n",
      "    \"latency_avg_s\": 160.75422271092734,\n",
      "    \"latency_min_s\": 134.9505536556244,\n",
      "    \"latency_max_s\": 191.7526979446411,\n",
      "    \"latency_p95_s\": 220.70732307434082,\n",
      "    \"tokens_per_sec_avg\": 1.0664144910569093,\n",
      "    \"avg_prompt_tokens\": 11.0,\n",
      "    \"avg_completion_tokens\": 157.66666666666666,\n",
      "    \"avg_total_tokens\": 168.66666666666666,\n",
      "    \"prompt_tokens_total\": 33,\n",
      "    \"completion_tokens_total\": 473,\n",
      "    \"total_tokens_sum\": 506,\n",
      "    \"rounds_ok\": 3,\n",
      "    \"configured_rounds\": 3,\n",
      "    \"sample_output\": \"<|channel|>analysis<|message|>We need to explain retrieval augmented generation briefly. Provide concise explanation.<|end|><|start|>assistant<|channel|>final<|message|>**Retrieval\\u2011Augmented Generatio\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "Benchmark completed: 2 models tested\n",
      "Configuration: 3 rounds, 120 max tokens, temp=0.2\n",
      "Prompt: Explain retrieval augmented generation briefly....\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Render results as JSON and markdown table\n",
    "import math\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not summary:\n",
    "    print(\"No results to display.\")\n",
    "else:\n",
    "    # Calculate best/worst for highlighting\n",
    "    if len(summary) > 0:\n",
    "        best_latency = min(r['latency_avg_s'] for r in summary)\n",
    "        worst_latency = max(r['latency_avg_s'] for r in summary)\n",
    "        best_tps = max((r['tokens_per_sec_avg'] for r in summary if r['tokens_per_sec_avg']), default=None)\n",
    "        worst_tps = min((r['tokens_per_sec_avg'] for r in summary if r['tokens_per_sec_avg']), default=None)\n",
    "    \n",
    "    # Enhanced comprehensive table with performance indicators\n",
    "    print(\"\\nüìä PERFORMANCE SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    headers = [\"Model\", \"Latency (avg)\", \"Latency (P95)\", \"Throughput\", \"Tokens\", \"Success\", \"Rating\"]\n",
    "    rows = []\n",
    "    \n",
    "    for r in summary:\n",
    "        # Performance indicators\n",
    "        lat_indicator = \"üü¢\" if r['latency_avg_s'] == best_latency else (\"üî¥\" if r['latency_avg_s'] == worst_latency else \"üü°\")\n",
    "        tps_indicator = \"\"\n",
    "        if r['tokens_per_sec_avg']:\n",
    "            if best_tps and r['tokens_per_sec_avg'] == best_tps:\n",
    "                tps_indicator = \"üü¢\"\n",
    "            elif worst_tps and r['tokens_per_sec_avg'] == worst_tps:\n",
    "                tps_indicator = \"üî¥\"\n",
    "            else:\n",
    "                tps_indicator = \"üü°\"\n",
    "        \n",
    "        # Overall rating based on latency and throughput\n",
    "        rating = \"\"\n",
    "        if r['latency_avg_s'] == best_latency or (r['tokens_per_sec_avg'] and r['tokens_per_sec_avg'] == best_tps):\n",
    "            rating = \"‚≠ê‚≠ê‚≠ê\"\n",
    "        elif r['latency_avg_s'] == worst_latency or (r['tokens_per_sec_avg'] and worst_tps and r['tokens_per_sec_avg'] == worst_tps):\n",
    "            rating = \"‚≠ê\"\n",
    "        else:\n",
    "            rating = \"‚≠ê‚≠ê\"\n",
    "        \n",
    "        rows.append([\n",
    "            r['alias'][:20],  # Truncate long names\n",
    "            f\"{lat_indicator} {r['latency_avg_s']:.3f}s\",\n",
    "            f\"{r['latency_p95_s']:.3f}s\",\n",
    "            f\"{tps_indicator} {r['tokens_per_sec_avg']:.1f}\" if r['tokens_per_sec_avg'] else '-',\n",
    "            f\"{r['avg_total_tokens']:.0f}\",\n",
    "            f\"{r['rounds_ok']}/{r['configured_rounds']}\",\n",
    "            rating\n",
    "        ])\n",
    "    \n",
    "    col_widths = [max(len(str(cell)) for cell in col) for col in zip(headers, *rows)]\n",
    "    def fmt_row(row):\n",
    "        return \" | \".join(str(c).ljust(w) for c, w in zip(row, col_widths))\n",
    "    \n",
    "    print(fmt_row(headers))\n",
    "    print(\"-\" + \"-+-\".join('-'*w for w in col_widths) + \"-\")\n",
    "    for row in rows:\n",
    "        print(fmt_row(row))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Legend: üü¢ Best  üü° Average  üî¥ Worst  |  Rating: ‚≠ê‚≠ê‚≠ê Excellent  ‚≠ê‚≠ê Good  ‚≠ê Needs Improvement\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Detailed metrics per model\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED METRICS PER MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    for r in summary:\n",
    "        print(f\"\\nüìä {r['alias']} ({r['model_id']})\")\n",
    "        print(f\"   Latency:\")\n",
    "        print(f\"     Average: {r['latency_avg_s']:.3f}s\")\n",
    "        print(f\"     Min:     {r['latency_min_s']:.3f}s\")\n",
    "        print(f\"     Max:     {r['latency_max_s']:.3f}s\")\n",
    "        print(f\"     P95:     {r['latency_p95_s']:.3f}s\")\n",
    "        print(f\"   Tokens:\")\n",
    "        print(f\"     Avg Prompt:     {r['avg_prompt_tokens']:.0f}\")\n",
    "        print(f\"     Avg Completion: {r['avg_completion_tokens']:.0f}\")\n",
    "        print(f\"     Avg Total:      {r['avg_total_tokens']:.0f}\")\n",
    "        if r['tokens_per_sec_avg']:\n",
    "            print(f\"     Throughput:     {r['tokens_per_sec_avg']:.1f} tok/s\")\n",
    "        print(f\"   Rounds: {r['rounds_ok']}/{r['configured_rounds']} successful\")\n",
    "        if r.get('sample_output'):\n",
    "            print(f\"   Sample Output: {r['sample_output'][:150]}...\")\n",
    "    \n",
    "    # Comparative analysis\n",
    "    if len(summary) > 1:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîç PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Sort by latency for speed comparison\n",
    "        sorted_by_speed = sorted(summary, key=lambda x: x['latency_avg_s'])\n",
    "        fastest = sorted_by_speed[0]\n",
    "        slowest = sorted_by_speed[-1]\n",
    "        \n",
    "        # Create performance comparison table\n",
    "        print(\"\\nüìà Relative Performance (normalized to fastest model)\")\n",
    "        print(\"-\" * 80)\n",
    "        comp_headers = [\"Model\", \"Speed vs Fastest\", \"Latency Delta\", \"Throughput\", \"Efficiency\"]\n",
    "        comp_rows = []\n",
    "        \n",
    "        for r in sorted_by_speed:\n",
    "            speedup = r['latency_avg_s'] / fastest['latency_avg_s']\n",
    "            latency_delta = r['latency_avg_s'] - fastest['latency_avg_s']\n",
    "            \n",
    "            # Speed indicator\n",
    "            if speedup <= 1.1:\n",
    "                speed_bar = \"‚ñà‚ñà‚ñà‚ñà‚ñà 100%\"\n",
    "                speed_emoji = \"üöÄ\"\n",
    "            elif speedup <= 1.5:\n",
    "                speed_bar = \"‚ñà‚ñà‚ñà‚ñà‚ñë 80%\"\n",
    "                speed_emoji = \"‚ö°\"\n",
    "            elif speedup <= 2.0:\n",
    "                speed_bar = \"‚ñà‚ñà‚ñà‚ñë‚ñë 60%\"\n",
    "                speed_emoji = \"üèÉ\"\n",
    "            else:\n",
    "                speed_bar = \"‚ñà‚ñà‚ñë‚ñë‚ñë 40%\"\n",
    "                speed_emoji = \"üêå\"\n",
    "            \n",
    "            # Efficiency score (lower is better: combines latency and throughput)\n",
    "            if r['tokens_per_sec_avg']:\n",
    "                efficiency = f\"{r['tokens_per_sec_avg']:.1f} tok/s\"\n",
    "            else:\n",
    "                efficiency = \"N/A\"\n",
    "            \n",
    "            comp_rows.append([\n",
    "                f\"{speed_emoji} {r['alias'][:18]}\",\n",
    "                speed_bar,\n",
    "                f\"+{latency_delta:.3f}s\" if latency_delta > 0 else \"baseline\",\n",
    "                efficiency,\n",
    "                f\"{(1/speedup)*100:.0f}%\"\n",
    "            ])\n",
    "        \n",
    "        comp_widths = [max(len(str(cell)) for cell in col) for col in zip(comp_headers, *comp_rows)]\n",
    "        def comp_fmt_row(row):\n",
    "            return \" | \".join(str(c).ljust(w) for c, w in zip(row, comp_widths))\n",
    "        \n",
    "        print(comp_fmt_row(comp_headers))\n",
    "        print(\"-+-\".join('-'*w for w in comp_widths))\n",
    "        for row in comp_rows:\n",
    "            print(comp_fmt_row(row))\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä KEY FINDINGS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nüèÉ Fastest Model: {fastest['alias']}\")\n",
    "        print(f\"   ‚îú‚îÄ Average latency: {fastest['latency_avg_s']:.3f}s\")\n",
    "        print(f\"   ‚îú‚îÄ P95 latency: {fastest['latency_p95_s']:.3f}s\")\n",
    "        if fastest['tokens_per_sec_avg']:\n",
    "            print(f\"   ‚îî‚îÄ Throughput: {fastest['tokens_per_sec_avg']:.1f} tok/s\")\n",
    "        \n",
    "        if len(summary) > 1:\n",
    "            print(f\"\\nüêå Slowest Model: {slowest['alias']}\")\n",
    "            print(f\"   ‚îú‚îÄ Average latency: {slowest['latency_avg_s']:.3f}s\")\n",
    "            speedup = slowest['latency_avg_s'] / fastest['latency_avg_s']\n",
    "            print(f\"   ‚îî‚îÄ Performance gap: {speedup:.2f}x slower than fastest\")\n",
    "        \n",
    "        # Throughput comparison\n",
    "        with_throughput = [r for r in summary if r['tokens_per_sec_avg']]\n",
    "        if len(with_throughput) > 1:\n",
    "            sorted_by_tps = sorted(with_throughput, key=lambda x: x['tokens_per_sec_avg'], reverse=True)\n",
    "            highest_tps = sorted_by_tps[0]\n",
    "            lowest_tps = sorted_by_tps[-1]\n",
    "            \n",
    "            print(f\"\\n‚ö° Highest Throughput: {highest_tps['alias']}\")\n",
    "            print(f\"   ‚îú‚îÄ Throughput: {highest_tps['tokens_per_sec_avg']:.1f} tok/s\")\n",
    "            print(f\"   ‚îî‚îÄ Latency: {highest_tps['latency_avg_s']:.3f}s\")\n",
    "            \n",
    "            if highest_tps['alias'] != lowest_tps['alias']:\n",
    "                throughput_gap = highest_tps['tokens_per_sec_avg'] / lowest_tps['tokens_per_sec_avg']\n",
    "                print(f\"\\nüí° Throughput Range: {throughput_gap:.2f}x difference between best and worst\")\n",
    "        \n",
    "        # Memory efficiency note\n",
    "        print(\"\\nüíæ Memory Efficiency:\")\n",
    "        cpu_models = [r for r in summary if 'cpu' in r['model_id'].lower()]\n",
    "        if cpu_models:\n",
    "            print(f\"   ‚îú‚îÄ {len(cpu_models)}/{len(summary)} models using CPU variants (30-50% memory savings)\")\n",
    "            print(f\"   ‚îî‚îÄ Recommended for systems with limited memory\")\n",
    "    \n",
    "    # Export JSON\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"JSON SUMMARY (for programmatic analysis)\")\n",
    "    print(\"=\"*80)\n",
    "    print(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Benchmark completed: {len(summary)} models tested\")\n",
    "print(f\"Configuration: {ROUNDS} rounds, {MAX_TOKENS} max tokens, temp={TEMPERATURE}\")\n",
    "print(f\"Prompt: {PROMPT[:60]}...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5468cfc",
   "metadata": {},
   "source": [
    "### ‡Æö‡ØÅ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æ™‡Æü‡Æø‡Æï‡Æ≥‡Øç\n",
    "\n",
    "‡Æá‡Æ®‡Øç‡Æ§ ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æ®‡Øã‡Æü‡Øç‡Æ™‡ØÅ‡Æï‡Øç ‡Æ™‡Æ≤ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç‡Æï‡Æ≥‡Øà Foundry Local ‡ÆÆ‡ØÇ‡Æ≤‡ÆÆ‡Øç ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡ØÅ‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡Ææ‡Æ© ‡Æµ‡Æø‡Æ∞‡Æø‡Æµ‡Ææ‡Æ© ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Øç ‡ÆÖ‡Æ≥‡Æµ‡ØÅ‡Æï‡Øã‡Æ≥‡Øç‡Æï‡Æ≥‡Øà ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ:\n",
    "\n",
    "**‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡ÆÖ‡Æ≥‡Æµ‡ØÅ‡Æï‡Øã‡Æ≥‡Øç‡Æï‡Æ≥‡Øç:**\n",
    "- ‚úÖ **Latency**: ‡Æö‡Æ∞‡Ææ‡Æö‡Æ∞‡Æø, ‡Æï‡ØÅ‡Æ±‡Øà‡Æ®‡Øç‡Æ§‡Æ™‡Æü‡Øç‡Æö‡ÆÆ‡Øç, ‡ÆÖ‡Æ§‡Æø‡Æï‡Æ™‡Æü‡Øç‡Æö‡ÆÆ‡Øç, ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç P95 (tail latency)\n",
    "- ‚úÖ **Throughput**: ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æµ‡Æø‡Æ©‡Ææ‡Æü‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡Æï‡Øç‡Æï‡Øà\n",
    "- ‚úÖ **Token Usage**: Prompt, completion, ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Øä‡Æ§‡Øç‡Æ§ ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Øç (‡Æï‡Æ£‡Æø‡Æ™‡Øç‡Æ™‡ØÅ fallback ‡Æâ‡Æü‡Æ©‡Øç)\n",
    "- ‚úÖ **Reliability**: ‡Æ™‡Æ≤ ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æµ‡Æø‡Æï‡Æø‡Æ§‡ÆÆ‡Øç\n",
    "- ‚úÖ **Sample Output**: ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡Æ™‡Æ§‡Æø‡Æ≤‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øã‡Æü‡Øç‡Æü‡ÆÆ‡Øç\n",
    "\n",
    "**‡Æö‡ØÇ‡Æ¥‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡Æø‡Æï‡Æ≥‡Øç ‡Æ§‡Æ©‡Æø‡Æ™‡Øç‡Æ™‡ÆØ‡Æ©‡Ææ‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æï:**\n",
    "- `BENCH_MODELS`: ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÖ‡Æ≤‡Æø‡ÆØ‡Ææ‡Æ∏‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æï‡Ææ‡ÆÆ‡Ææ-‡Æ™‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æ™‡Æü‡Øç‡Æü‡Æø‡ÆØ‡Æ≤‡Øç\n",
    "- `BENCH_ROUNDS`: ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æü‡Æ≤‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æö‡ØÅ‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡Æï‡Øç‡Æï‡Øà (default: 3)\n",
    "- `BENCH_PROMPT`: ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æö‡Øã‡Æ§‡Æ©‡Øà prompt\n",
    "- `BENCH_MAX_TOKENS`: ‡ÆÖ‡Æ§‡Æø‡Æï‡Æ™‡Æü‡Øç‡Æö ‡Æ™‡Æ§‡Æø‡Æ≤‡Øç ‡Æü‡Øã‡Æï‡Øç‡Æï‡Æ©‡Øç‡Æï‡Æ≥‡Øç (default: 120)\n",
    "- `BENCH_TEMPERATURE`: ‡Æö‡Ææ‡ÆÆ‡Øç‡Æ™‡Æø‡Æ≥‡Æø‡Æô‡Øç ‡Æµ‡ØÜ‡Æ™‡Øç‡Æ™‡Æ®‡Æø‡Æ≤‡Øà (default: 0.2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT`: ‡Æö‡Øá‡Æµ‡Øà ‡Æá‡Æ±‡ØÅ‡Æ§‡Æø‡Æ™‡Øç‡Æ™‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æø‡ÆØ‡Øà override ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æµ‡ØÅ‡ÆÆ‡Øç (default ‡ÆÜ‡Æï ‡Æ§‡Ææ‡Æ©‡Ææ‡Æï ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç)\n",
    "\n",
    "**‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æ™‡Æü‡Æø‡Æï‡Æ≥‡Øç:**\n",
    "1. ‡Æ™‡Æ≤ ‡Æö‡Æø‡Æï‡Øç‡Æï‡Æ≤‡Ææ‡Æ© ‡Æ®‡Æø‡Æ≤‡Øà‡Æï‡Æ≥‡Øà ‡Æö‡Øã‡Æ§‡Æø‡Æï‡Øç‡Æï ‡Æµ‡ØÜ‡Æµ‡Øç‡Æµ‡Øá‡Æ±‡ØÅ prompt-‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç ‡Æ™‡ØÜ‡Æû‡Øç‡Æö‡Øç‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æï‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç\n",
    "2. `BENCH_ROUNDS` ‡Æê ‡ÆÖ‡Æ§‡Æø‡Æï‡Æ∞‡Æø‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ™‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æø‡ÆØ‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æï‡Æ§‡Øç‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà‡ÆØ‡Øà ‡ÆÆ‡Øá‡ÆÆ‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æµ‡ØÅ‡ÆÆ‡Øç\n",
    "3. ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æµ‡ØÅ‡Æï‡Æ≥‡Øà ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø ‡Æµ‡Æ¥‡Æø‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æµ‡ØÅ‡Æï‡Æ≥‡Øà‡Æ§‡Øç ‡Æ§‡Æï‡Æµ‡ÆÆ‡Øà‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç (Session 06 ‡Æ®‡Øã‡Æü‡Øç‡Æ™‡ØÅ‡Æï‡Øç-‡Æï‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç)\n",
    "4. ‡ÆÆ‡Ææ‡Æü‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ±‡ØÅ‡Æ™‡Ææ‡Æü‡ØÅ‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æá‡Æü‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æï ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Ææ‡Æü‡ØÅ ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æπ‡Ææ‡Æ∞‡Øç‡Æü‡Øç‡Æµ‡Øá‡Æ∞‡Æø‡Æ©‡Øç ‡ÆÆ‡Øá‡ÆÆ‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡Øà ‡Æí‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Æµ‡ØÅ‡ÆÆ‡Øç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db617282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATION SUMMARY\n",
      "================================================================================\n",
      "‚úÖ Service Auto-Detection    Found at http://127.0.0.1:59959/v1\n",
      "‚úÖ Models Configuration      2 models configured: ['phi-4-mini', 'gpt-oss-20b']\n",
      "‚úÖ Benchmark Execution       2/2 models completed\n",
      "‚úÖ Metrics Completeness      All models have comprehensive metrics\n",
      "================================================================================\n",
      "\n",
      "üéâ ALL VALIDATIONS PASSED! Benchmark completed successfully.\n",
      "   Successfully benchmarked 2 models\n",
      "   Configuration: 3 rounds, 120 tokens, temp=0.2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_checks = []\n",
    "\n",
    "# Check service detection\n",
    "if 'discovered_endpoint' in dir() and discovered_endpoint:\n",
    "    validation_checks.append((\"‚úÖ\", \"Service Auto-Detection\", f\"Found at {discovered_endpoint}\"))\n",
    "else:\n",
    "    validation_checks.append((\"‚ö†Ô∏è\", \"Service Auto-Detection\", \"Not detected - using default\"))\n",
    "\n",
    "# Check configuration\n",
    "if 'MODELS' in dir() and MODELS:\n",
    "    validation_checks.append((\"‚úÖ\", \"Models Configuration\", f\"{len(MODELS)} models configured: {MODELS}\"))\n",
    "else:\n",
    "    validation_checks.append((\"‚ùå\", \"Models Configuration\", \"No models configured\"))\n",
    "\n",
    "# Check benchmark results\n",
    "if 'summary' in dir() and summary:\n",
    "    successful = [r for r in summary if r['rounds_ok'] > 0]\n",
    "    validation_checks.append((\"‚úÖ\", \"Benchmark Execution\", f\"{len(successful)}/{len(summary)} models completed\"))\n",
    "    \n",
    "    # Check all have complete metrics\n",
    "    all_have_metrics = all(\n",
    "        r.get('latency_avg_s') and \n",
    "        r.get('tokens_per_sec_avg') and \n",
    "        r.get('avg_total_tokens')\n",
    "        for r in successful\n",
    "    )\n",
    "    if all_have_metrics:\n",
    "        validation_checks.append((\"‚úÖ\", \"Metrics Completeness\", \"All models have comprehensive metrics\"))\n",
    "    else:\n",
    "        validation_checks.append((\"‚ö†Ô∏è\", \"Metrics Completeness\", \"Some metrics missing\"))\n",
    "else:\n",
    "    validation_checks.append((\"‚ùå\", \"Benchmark Execution\", \"No results yet\"))\n",
    "\n",
    "# Display validation results\n",
    "for icon, check_name, status in validation_checks:\n",
    "    print(f\"{icon} {check_name:<25} {status}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall status\n",
    "all_passed = all(icon == \"‚úÖ\" for icon, _, _ in validation_checks)\n",
    "if all_passed:\n",
    "    print(\"\\nüéâ ALL VALIDATIONS PASSED! Benchmark completed successfully.\")\n",
    "    if 'summary' in dir() and len(summary) > 0:\n",
    "        print(f\"   Successfully benchmarked {len(summary)} models\")\n",
    "        print(f\"   Configuration: {ROUNDS} rounds, {MAX_TOKENS} tokens, temp={TEMPERATURE}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some validations did not pass. Review the issues above.\")\n",
    "    print(\"\\nüí° Common fixes:\")\n",
    "    print(\"   1. Ensure Foundry Local service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-0.5b\")\n",
    "    print(\"   3. Check model availability: foundry model ls\")\n",
    "    print(\"   4. Re-run the benchmark cells\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡ØÅ**:  \n‡Æá‡Æ®‡Øç‡Æ§ ‡ÆÜ‡Æµ‡Æ£‡ÆÆ‡Øç [Co-op Translator](https://github.com/Azure/co-op-translator) ‡Æé‡Æ©‡Øç‡Æ± AI ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡ØÅ ‡Æö‡Øá‡Æµ‡Øà‡ÆØ‡Øà‡Æ™‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ. ‡Æ®‡Ææ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡ØÅ‡Æ≤‡Øç‡Æ≤‡Æø‡ÆØ‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡Ææ‡Æï ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Øã‡ÆÆ‡Øç, ‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æ§‡Ææ‡Æ©‡Æø‡ÆØ‡Æô‡Øç‡Æï‡Æø ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æ™‡Æø‡Æ¥‡Øà‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æ§‡Æµ‡Æ±‡Ææ‡Æ© ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æï‡Øç‡Æï‡ØÇ‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡Øà ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æï‡Æµ‡Æ©‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡ÆÖ‡Æ§‡Æ©‡Øç ‡Æ§‡Ææ‡ÆØ‡Øç‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡ÆÆ‡ØÇ‡Æ≤ ‡ÆÜ‡Æµ‡Æ£‡ÆÆ‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡Ææ‡Æ∞‡Æ™‡Øç‡Æ™‡ØÇ‡Æ∞‡Øç‡Æµ ‡ÆÜ‡Æ§‡Ææ‡Æ∞‡ÆÆ‡Ææ‡Æï ‡Æï‡Æ∞‡ØÅ‡Æ§‡Æ™‡Øç‡Æ™‡Æü ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç. ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Ææ‡Æ© ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ, ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡ÆÆ‡ØÅ‡Æ±‡Øà ‡ÆÆ‡Æ©‡Æø‡Æ§ ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡ØÅ ‡Æ™‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æ∞‡Øà‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æá‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡Øà‡Æ™‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Æ§‡Ææ‡Æ≤‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æé‡Æ®‡Øç‡Æ§ ‡Æ§‡Æµ‡Æ±‡Ææ‡Æ© ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ§‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æ§‡Æµ‡Æ±‡Ææ‡Æ© ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æ®‡Ææ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Øä‡Æ±‡ØÅ‡Æ™‡Øç‡Æ™‡Æ≤‡Øç‡Æ≤.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "1c3e21b3e210bdf6010fb0343b177eea",
   "translation_date": "2025-10-11T13:08:36+00:00",
   "source_file": "Workshop/notebooks/session03_benchmark_oss_models.ipynb",
   "language_code": "ta"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}