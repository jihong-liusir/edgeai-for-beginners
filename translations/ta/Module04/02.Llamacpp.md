<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-10-11T11:39:21+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "ta"
}
-->
# பகுதி 2 : Llama.cpp செயல்படுத்தல் வழிகாட்டி

## உள்ளடக்க அட்டவணை
1. [அறிமுகம்](../../../Module04)
2. [Llama.cpp என்றால் என்ன?](../../../Module04)
3. [நிறுவல்](../../../Module04)
4. [மூலத்திலிருந்து கட்டமைத்தல்](../../../Module04)
5. [மாதிரி அளவீடு](../../../Module04)
6. [அடிப்படை பயன்பாடு](../../../Module04)
7. [மேம்பட்ட அம்சங்கள்](../../../Module04)
8. [Python ஒருங்கிணைப்பு](../../../Module04)
9. [சிக்கல்களை சரிசெய்தல்](../../../Module04)
10. [சிறந்த நடைமுறைகள்](../../../Module04)

## அறிமுகம்

இந்த விரிவான பயிற்சியில் Llama.cpp பற்றிய அடிப்படை நிறுவலிலிருந்து மேம்பட்ட பயன்பாட்டு சூழல்களுக்கு வரை நீங்கள் அறிந்து கொள்ள வேண்டிய அனைத்தையும் வழிகாட்டுகிறது. Llama.cpp என்பது சக்திவாய்ந்த C++ செயல்படுத்தல் ஆகும், இது குறைந்த அமைப்புடன் மற்றும் பல்வேறு ஹார்ட்வேர்களுக்கான சிறந்த செயல்திறனுடன் பெரிய மொழி மாதிரிகளை (LLMs) திறம்பட செயல்படுத்த உதவுகிறது.

## Llama.cpp என்றால் என்ன?

Llama.cpp என்பது C/C++-ல் எழுதப்பட்ட LLM inference framework ஆகும், இது குறைந்த அமைப்புடன் பெரிய மொழி மாதிரிகளை உள்ளூர் ரீதியாக இயக்குவதற்கும், பல்வேறு ஹார்ட்வேர்களில் சிறந்த செயல்திறனுடன் செயல்படுவதற்கும் உதவுகிறது. முக்கிய அம்சங்கள்:

### முக்கிய அம்சங்கள்
- **சாதாரண C/C++ செயல்படுத்தல்** - எந்த சார்புகளும் இல்லாமல்
- **குறுக்குவெளி இணக்கத்தன்மை** (Windows, macOS, Linux)
- **ஹார்ட்வேர மேம்பாடு** பல்வேறு கட்டமைப்புகளுக்கு
- **அளவீட்டு ஆதரவு** (1.5-bit முதல் 8-bit integer quantization வரை)
- **CPU மற்றும் GPU வேகப்படுத்தல்** ஆதரவு
- **நினைவக திறன்** கட்டுப்படுத்தப்பட்ட சூழல்களுக்கு

### நன்மைகள்
- சிறப்பு ஹார்ட்வேரை தேவையில்லாமல் CPU-வில் திறம்பட இயங்குகிறது
- பல GPU பின்புலங்களை ஆதரிக்கிறது (CUDA, Metal, OpenCL, Vulkan)
- இலகுவானது மற்றும் எடுத்துச் செல்லக்கூடியது
- Apple silicon முக்கிய இடத்தைப் பெறுகிறது - ARM NEON, Accelerate மற்றும் Metal frameworks மூலம் மேம்படுத்தப்பட்டுள்ளது
- குறைந்த நினைவக பயன்பாட்டிற்கான பல அளவீட்டு நிலைகளை ஆதரிக்கிறது

## நிறுவல்

### முறை 1: முன்பே கட்டமைக்கப்பட்ட பைனரிகள் (தொடக்கத்திற்கான பரிந்துரை)

#### GitHub Releases-இல் இருந்து பதிவிறக்கவும்
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) ஐ பார்வையிடவும்
2. உங்கள் அமைப்பிற்கான சரியான பைனரியை பதிவிறக்கவும்:
   - Windows-க்கு `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS-க்கு `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux-க்கு `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. காப்பகத்தை பிரித்தெடுத்து உங்கள் அமைப்பின் PATH-க்கு அடைவைச் சேர்க்கவும்

#### தொகுப்பு மேலாளர்களைப் பயன்படுத்துதல்

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (பல்வேறு விநியோகங்கள்):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### முறை 2: Python தொகுப்பு (llama-cpp-python)

#### அடிப்படை நிறுவல்
```bash
pip install llama-cpp-python
```

#### ஹார்ட்வேர வேகப்படுத்தலுடன்
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## மூலத்திலிருந்து கட்டமைத்தல்

### முன்னோட்ட தேவைகள்

**அமைப்பு தேவைகள்:**
- C++ தொகுப்பாளர் (GCC, Clang, அல்லது MSVC)
- CMake (பதிப்பு 3.14 அல்லது அதற்கு மேல்)
- Git
- உங்கள் தளத்திற்கான கட்டமைப்பு கருவிகள்

**முன்னோட்ட தேவைகளை நிறுவுதல்:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022-ஐ C++ மேம்பாட்டு கருவிகளுடன் நிறுவவும்
- அதிகாரப்பூர்வ இணையதளத்திலிருந்து CMake-ஐ நிறுவவும்
- Git-ஐ நிறுவவும்

### அடிப்படை கட்டமைப்பு செயல்முறை

1. **காப்பகத்தை கிளோன் செய்யவும்:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **கட்டமைப்பை உள்ளமைக்கவும்:**
```bash
cmake -B build
```

3. **திட்டத்தை கட்டமைக்கவும்:**
```bash
cmake --build build --config Release
```

வேகமான தொகுப்புக்கு, இணை வேலைகளைப் பயன்படுத்தவும்:
```bash
cmake --build build --config Release -j 8
```

### ஹார்ட்வேர-குறிப்பிட்ட கட்டமைப்புகள்

#### CUDA ஆதரவு (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal ஆதரவு (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS ஆதரவு (CPU மேம்பாடு)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan ஆதரவு
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### மேம்பட்ட கட்டமைப்பு விருப்பங்கள்

#### Debug Build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### கூடுதல் அம்சங்களுடன்
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## மாதிரி அளவீடு

### GGUF வடிவத்தைப் புரிந்துகொள்வது

GGUF (Generalized GGML Unified Format) என்பது Llama.cpp மற்றும் பிற frameworks-களைப் பயன்படுத்தி பெரிய மொழி மாதிரிகளை திறம்பட இயக்குவதற்காக வடிவமைக்கப்பட்ட ஒரு மேம்பட்ட கோப்பு வடிவமாகும். இது வழங்குகிறது:

- மாதிரி எடை சேமிப்பின் தரநிலை
- தளங்களுக்கிடையிலான மேம்பட்ட இணக்கத்தன்மை
- மேம்பட்ட செயல்திறன்
- திறம்பட மெட்டாடேட்டா கையாளுதல்

### அளவீட்டு வகைகள்

Llama.cpp பல அளவீட்டு நிலைகளை ஆதரிக்கிறது:

| வகை | பிட்ஸ் | விளக்கம் | பயன்பாட்டு சூழல் |
|-----|-------|----------|------------------|
| F16 | 16 | அரை துல்லியம் | உயர் தரம், பெரிய நினைவகம் |
| Q8_0 | 8 | 8-bit அளவீடு | நல்ல சமநிலை |
| Q4_0 | 4 | 4-bit அளவீடு | மிதமான தரம், சிறிய அளவு |
| Q2_K | 2 | 2-bit அளவீடு | மிகச் சிறிய அளவு, குறைந்த தரம் |

### மாதிரிகளை மாற்றுதல்

#### PyTorch-இல் இருந்து GGUF-க்கு
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face-இல் இருந்து நேரடி பதிவிறக்கம்
பல மாதிரிகள் Hugging Face-இல் GGUF வடிவத்தில் கிடைக்கின்றன:
- "GGUF" என்ற பெயருடன் மாதிரிகளைத் தேடவும்
- சரியான அளவீட்டு நிலையை பதிவிறக்கவும்
- llama.cpp உடன் நேரடியாக பயன்படுத்தவும்

## அடிப்படை பயன்பாடு

### கட்டளை வரி இடைமுகம்

#### எளிய உரை உருவாக்கம்
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face-இல் இருந்து மாதிரிகளைப் பயன்படுத்துதல்
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### சர்வர் முறை
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### பொதுவான அளவுருக்கள்

| அளவுரு | விளக்கம் | உதாரணம் |
|--------|----------|---------|
| `-m` | மாதிரி கோப்பு பாதை | `-m model.gguf` |
| `-p` | முன்மொழிவு உரை | `-p "Hello world"` |
| `-n` | உருவாக்க வேண்டிய டோக்கன்களின் எண்ணிக்கை | `-n 100` |
| `-c` | சூழல் அளவு | `-c 4096` |
| `-t` | திருக்களின் எண்ணிக்கை | `-t 8` |
| `-ngl` | GPU அடுக்குகள் | `-ngl 32` |
| `-temp` | வெப்பநிலை | `-temp 0.7` |

### இடைமுக முறை

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## மேம்பட்ட அம்சங்கள்

### சர்வர் API

#### சர்வரை தொடங்குதல்
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API பயன்பாடு
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### செயல்திறன் மேம்பாடு

#### நினைவக மேலாண்மை
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### பல்திருக்கள்
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU வேகப்படுத்தல்
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python ஒருங்கிணைப்பு

### llama-cpp-python உடன் அடிப்படை பயன்பாடு

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### உரையாடல் இடைமுகம்

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### ஸ்ட்ரீமிங் பதில்கள்

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain உடன் ஒருங்கிணைப்பு

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## சிக்கல்களை சரிசெய்தல்

### பொதுவான சிக்கல்கள் மற்றும் தீர்வுகள்

#### கட்டமைப்பு பிழைகள்

**சிக்கல்: CMake காணப்படவில்லை**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**சிக்கல்: தொகுப்பாளர் காணப்படவில்லை**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### இயக்க பிழைகள்

**சிக்கல்: மாதிரி ஏற்றுதல் தோல்வி**
- மாதிரி கோப்பு பாதையை சரிபார்க்கவும்
- கோப்பு அனுமதிகளைச் சரிபார்க்கவும்
- போதுமான RAM உள்ளதா என்பதை உறுதிப்படுத்தவும்
- வேறு அளவீட்டு நிலைகளை முயற்சிக்கவும்

**சிக்கல்: குறைந்த செயல்திறன்**
- ஹார்ட்வேர வேகப்படுத்தலை இயக்கவும்
- திருக்களின் எண்ணிக்கையை அதிகரிக்கவும்
- சரியான அளவீட்டை பயன்படுத்தவும்
- GPU நினைவக பயன்பாட்டைச் சரிபார்க்கவும்

#### நினைவக சிக்கல்கள்

**சிக்கல்: நினைவகம் இல்லாமல் போகிறது**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### தள-குறிப்பிட்ட சிக்கல்கள்

#### Windows
- MinGW அல்லது Visual Studio தொகுப்பாளரைப் பயன்படுத்தவும்
- PATH அமைப்பை சரியாக உள்ளமைக்கவும்
- வைரஸ் எதிர்ப்பு தலையீட்டைச் சரிபார்க்கவும்

#### macOS
- Apple Silicon-க்கு Metal-ஐ இயக்கவும்
- தேவையானால் இணக்கத்தன்மைக்காக Rosetta 2-ஐ பயன்படுத்தவும்
- Xcode கட்டளை வரி கருவிகளைச் சரிபார்க்கவும்

#### Linux
- மேம்பாட்டு தொகுப்புகளை நிறுவவும்
- GPU இயக்கி பதிப்புகளைச் சரிபார்க்கவும்
- CUDA கருவி தொகுப்பைச் சரிபார்க்கவும்

## சிறந்த நடைமுறைகள்

### மாதிரி தேர்வு
1. **உங்கள் ஹார்ட்வேரை அடிப்படையாகக் கொண்டு சரியான அளவீட்டைத் தேர்ந்தெடுக்கவும்**
2. **மாதிரி அளவு மற்றும் தரத்தின் சமநிலையைப் பரிசீலிக்கவும்**
3. **உங்கள் குறிப்பிட்ட பயன்பாட்டிற்கான பல மாதிரிகளைச் சோதிக்கவும்**

### செயல்திறன் மேம்பாடு
1. **GPU வேகப்படுத்தலைப் பயன்படுத்தவும்** - கிடைக்கும்போது
2. **CPU-க்கு திருக்களின் எண்ணிக்கையை மேம்படுத்தவும்**
3. **உங்கள் பயன்பாட்டிற்கான சரியான சூழல் அளவை அமைக்கவும்**
4. **பெரிய மாதிரிகளுக்கு நினைவக வரைபடத்தை இயக்கவும்**

### உற்பத்தி பயன்பாடு
1. **API அணுகலுக்கான சர்வர் முறையைப் பயன்படுத்தவும்**
2. **சரியான பிழை கையாளுதலை செயல்படுத்தவும்**
3. **வள பயன்பாட்டை கண்காணிக்கவும்**
4. **பதிவேடு மற்றும் கண்காணிப்பை அமைக்கவும்**

### மேம்பாட்டு வேலைநிலை
1. **சோதனைக்கான சிறிய மாதிரிகளுடன் தொடங்கவும்**
2. **மாதிரி உள்ளமைப்புகளுக்கு பதிப்பு கட்டுப்பாட்டைப் பயன்படுத்தவும்**
3. **உங்கள் உள்ளமைப்புகளை ஆவணப்படுத்தவும்**
4. **பல்வேறு தளங்களில் சோதிக்கவும்**

### பாதுகாப்பு பரிசீலனைகள்
1. **உள்ளீட்டு முன்மொழிவுகளை சரிபார்க்கவும்**
2. **விகித வரையறையை செயல்படுத்தவும்**
3. **API இறுதிப்புள்ளிகளை பாதுகாக்கவும்**
4. **துஷ்பிரயோகம் முறைமைகளை கண்காணிக்கவும்**

## முடிவு

Llama.cpp பல்வேறு ஹார்ட்வேர அமைப்புகளில் உள்ளூர் ரீதியாக பெரிய மொழி மாதிரிகளை இயக்க ஒரு சக்திவாய்ந்த மற்றும் திறம்படமான வழியை வழங்குகிறது. நீங்கள் AI பயன்பாடுகளை உருவாக்குகிறீர்கள், ஆராய்ச்சி நடத்துகிறீர்கள், அல்லது LLM-களுடன் சோதனை செய்கிறீர்கள் என்றாலும், இந்த framework பல்வேறு பயன்பாட்டு சூழல்களுக்கு தேவையான நெகிழ்வுத்தன்மை மற்றும் செயல்திறனை வழங்குகிறது.

முக்கிய takeaway-கள்:
- உங்கள் தேவைகளுக்கு சிறந்ததாக பொருந்தும் நிறுவல் முறையைத் தேர்ந்தெடுக்கவும்
- உங்கள் குறிப்பிட்ட ஹார்ட்வேர அமைப்புக்கு மேம்படுத்தவும்
- அடிப்படை பயன்பாட்டுடன் தொடங்கி, تدريجமாக மேம்பட்ட அம்சங்களை ஆராயவும்
- Python பிணைப்புகளை எளிதான ஒருங்கிணைப்புக்காகப் பரிசீலிக்கவும்
- உற்பத்தி பயன்பாடுகளுக்கான சிறந்த நடைமுறைகளைப் பின்பற்றவும்

மேலும் தகவல்களுக்கும் புதுப்பிப்புகளுக்கும், [அதிகாரப்பூர்வ Llama.cpp repository](https://github.com/ggml-org/llama.cpp) ஐ பார்வையிடவும் மற்றும் விரிவான ஆவணங்கள் மற்றும் சமூக வளங்களைப் பயன்படுத்தவும்.

## ➡️ அடுத்தது என்ன?

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கின்றோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.