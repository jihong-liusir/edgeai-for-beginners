<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-10-11T11:40:48+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ta"
}
-->
# பகுதி 3 : Microsoft Olive Optimization Suite

## உள்ளடக்க அட்டவணை
1. [அறிமுகம்](../../../Module04)
2. [Microsoft Olive என்றால் என்ன?](../../../Module04)
3. [நிறுவல்](../../../Module04)
4. [விரைவான தொடக்க வழிகாட்டி](../../../Module04)
5. [உதாரணம்: Qwen3 ஐ ONNX INT4 ஆக மாற்றுதல்](../../../Module04)
6. [மேம்பட்ட பயன்பாடு](../../../Module04)
7. [சிறந்த நடைமுறைகள்](../../../Module04)
8. [சிக்கல்களை சரிசெய்தல்](../../../Module04)
9. [கூடுதல் வளங்கள்](../../../Module04)

## அறிமுகம்

Microsoft Olive என்பது சக்திவாய்ந்த, எளிதில் பயன்படுத்தக்கூடிய hardware-aware மாடல் மேம்பாட்டு கருவியாகும், இது இயந்திரக் கற்றல் மாடல்களை பல்வேறு hardware தளங்களில் பயன்படுத்துவதற்கான மேம்பாட்டு செயல்முறையை எளிமைப்படுத்துகிறது. நீங்கள் CPUs, GPUs, அல்லது சிறப்பு AI accelerators ஐ இலக்காகக் கொண்டிருந்தாலும், Olive உங்களுக்கு மாடல் துல்லியத்தை பராமரிக்கும்போது சிறந்த செயல்திறனை அடைய உதவுகிறது.

## Microsoft Olive என்றால் என்ன?

Olive என்பது hardware-aware மாடல் மேம்பாட்டு கருவியாகும், இது மாடல் சுருக்கம், மேம்பாடு மற்றும் தொகுப்பு ஆகியவற்றில் முன்னணி தொழில்நுட்பங்களை ஒருங்கிணைக்கிறது. இது ONNX Runtime உடன் E2E inference optimization தீர்வாக செயல்படுகிறது.

### முக்கிய அம்சங்கள்

- **Hardware-Aware Optimization**: உங்கள் இலக்கு hardware க்கான சிறந்த மேம்பாட்டு தொழில்நுட்பங்களை தானாகவே தேர்ந்தெடுக்கிறது
- **40+ உள்ளமைக்கப்பட்ட மேம்பாட்டு கூறுகள்**: மாடல் சுருக்கம், அளவீடு, graph optimization மற்றும் பலவற்றை உள்ளடக்குகிறது
- **எளிய CLI இடைமுகம்**: பொதுவான மேம்பாட்டு பணிகளுக்கான எளிய கட்டளைகள்
- **பல Framework ஆதரவு**: PyTorch, Hugging Face மாடல்கள் மற்றும் ONNX உடன் வேலை செய்கிறது
- **பிரபலமான மாடல் ஆதரவு**: Olive Llama, Phi, Qwen, Gemma போன்ற பிரபலமான மாடல் கட்டமைப்புகளை தானாகவே மேம்படுத்த முடியும்

### நன்மைகள்

- **மேம்பாட்டு நேரத்தை குறைத்தல்**: வெவ்வேறு மேம்பாட்டு தொழில்நுட்பங்களை கையால் சோதிக்க தேவையில்லை
- **செயல்திறன் மேம்பாடுகள்**: குறிப்பிடத்தக்க வேக மேம்பாடுகள் (சில நேரங்களில் 6x வரை)
- **Cross-Platform Deployment**: மேம்படுத்தப்பட்ட மாடல்கள் பல்வேறு hardware மற்றும் இயக்க முறைமைகளில் வேலை செய்கின்றன
- **துல்லியத்தை பராமரித்தல்**: மேம்பாடுகள் செயல்திறனை மேம்படுத்தும்போது மாடல் தரத்தை பாதுகாக்கின்றன

## நிறுவல்

### முன் தேவைகள்

- Python 3.8 அல்லது அதற்கு மேல்
- pip package manager
- மெய்நிகர் சூழல் (பரிந்துரைக்கப்படுகிறது)

### அடிப்படை நிறுவல்

மெய்நிகர் சூழலை உருவாக்கி, செயல்படுத்தவும்:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Olive ஐ auto-optimization அம்சங்களுடன் நிறுவவும்:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### விருப்ப Dependencies

Olive கூடுதல் அம்சங்களுக்கான பல விருப்ப dependencies ஐ வழங்குகிறது:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### நிறுவலை சரிபார்க்கவும்

```bash
olive --help
```

வெற்றிகரமாக இருந்தால், Olive CLI உதவி செய்தியை நீங்கள் காணலாம்.

## விரைவான தொடக்க வழிகாட்டி

### உங்கள் முதல் மேம்பாடு

Olive இன் auto-optimization அம்சத்தைப் பயன்படுத்தி ஒரு சிறிய மொழி மாடலை மேம்படுத்துவோம்:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### இந்த கட்டளை என்ன செய்கிறது

மேம்பாட்டு செயல்முறை: மாடலை உள்ளூர் cache இல் இருந்து பெறுதல், ONNX Graph ஐப் பிடித்தல் மற்றும் ONNX தரவுக் கோப்பில் எடைகளைச் சேமித்தல், ONNX Graph ஐ மேம்படுத்துதல் மற்றும் RTN முறையைப் பயன்படுத்தி மாடலை int4 ஆக அளவீடு செய்தல் ஆகியவற்றை உள்ளடக்குகிறது.

### கட்டளை அளவுருக்கள் விளக்கம்

- `--model_name_or_path`: Hugging Face மாடல் அடையாளம் அல்லது உள்ளூர் பாதை
- `--output_path`: மேம்படுத்தப்பட்ட மாடல் சேமிக்கப்படும் அடைவு
- `--device`: இலக்கு சாதனம் (cpu, gpu)
- `--provider`: செயல்பாட்டு வழங்குநர் (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: inference க்கான ONNX Runtime Generate AI ஐப் பயன்படுத்தவும்
- `--precision`: அளவீட்டு துல்லியம் (int4, int8, fp16)
- `--log_level`: பதிவு விவரக்குறிப்பு (0=minimal, 1=verbose)

## உதாரணம்: Qwen3 ஐ ONNX INT4 ஆக மாற்றுதல்

Hugging Face உதாரணத்தின் அடிப்படையில் [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), Qwen3 மாடலை எப்படி மேம்படுத்துவது என்பதை இங்கே காணலாம்:

### படி 1: மாடலை பதிவிறக்குதல் (விருப்பம்)

பதிவிறக்க நேரத்தை குறைக்க, முக்கிய கோப்புகளை மட்டுமே cache செய்யவும்:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### படி 2: Qwen3 மாடலை மேம்படுத்துதல்

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### படி 3: மேம்படுத்தப்பட்ட மாடலை சோதிக்கவும்

உங்கள் மேம்படுத்தப்பட்ட மாடலை சோதிக்க ஒரு எளிய Python script ஐ உருவாக்கவும்:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### வெளியீட்டு அமைப்பு

மேம்பாட்டுக்குப் பிறகு, உங்கள் வெளியீட்டு அடைவு கீழ்கண்டவற்றை உள்ளடக்கும்:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## மேம்பட்ட பயன்பாடு

### கட்டமைப்பு கோப்புகள்

மேம்பட்ட மேம்பாட்டு பணிகளுக்காக, JSON கட்டமைப்பு கோப்புகளைப் பயன்படுத்தலாம்:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

கட்டமைப்புடன் இயக்கவும்:

```bash
olive run --config config.json
```

### GPU மேம்பாடு

CUDA GPU மேம்பாட்டுக்காக:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) க்காக:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive உடன் Fine-tuning

Olive மாடல்களை fine-tuning செய்யவும் ஆதரவு அளிக்கிறது:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## சிறந்த நடைமுறைகள்

### 1. மாடல் தேர்வு
- சோதனைக்காக சிறிய மாடல்களுடன் தொடங்கவும் (எ.கா., 0.5B-7B அளவுகள்)
- உங்கள் இலக்கு மாடல் கட்டமைப்பு Olive மூலம் ஆதரிக்கப்படுகிறதா என்பதை உறுதிப்படுத்தவும்

### 2. Hardware கருத்துக்கள்
- உங்கள் மேம்பாட்டு இலக்கை உங்கள் பயன்பாட்டு hardware க்கு பொருந்துமாறு பொருத்தவும்
- CUDA-இயங்கும் hardware இருந்தால் GPU மேம்பாட்டைப் பயன்படுத்தவும்
- Windows இயங்கும் இயந்திரங்களுக்கான DirectML ஐ பரிசீலிக்கவும்

### 3. துல்லியத் தேர்வு
- **INT4**: அதிகபட்ச சுருக்கம், சிறிய துல்லிய இழப்பு
- **INT8**: அளவு மற்றும் துல்லியத்தின் நல்ல சமநிலை
- **FP16**: குறைந்த துல்லிய இழப்பு, மிதமான அளவு குறைப்பு

### 4. சோதனை மற்றும் சரிபார்ப்பு
- உங்கள் குறிப்பிட்ட பயன்பாடுகளுடன் மேம்படுத்தப்பட்ட மாடல்களை எப்போதும் சோதிக்கவும்
- செயல்திறன் அளவுகோள்களை ஒப்பிடவும் (latency, throughput, accuracy)
- மதிப்பீட்டுக்கான பிரதிநிதி உள்ளீட்டு தரவுகளைப் பயன்படுத்தவும்

### 5. மீளச்செயலாக்க மேம்பாடு
- விரைவான முடிவுகளுக்காக auto-optimization உடன் தொடங்கவும்
- நுணுக்கமான கட்டுப்பாட்டுக்காக கட்டமைப்பு கோப்புகளைப் பயன்படுத்தவும்
- வெவ்வேறு மேம்பாட்டு செயல்முறைகளை சோதிக்கவும்

## சிக்கல்களை சரிசெய்தல்

### பொதுவான சிக்கல்கள்

#### 1. நிறுவல் பிரச்சினைகள்
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU பிரச்சினைகள்
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. நினைவக பிரச்சினைகள்
- மேம்பாட்டின் போது சிறிய batch அளவுகளைப் பயன்படுத்தவும்
- முதலில் அதிக துல்லியத்துடன் அளவீட்டை முயற்சிக்கவும் (int8 bad int4)
- மாடல் cache க்கான போதுமான டிஸ்க் இடத்தை உறுதிப்படுத்தவும்

#### 4. மாடல் ஏற்றல் பிழைகள்
- மாடல் பாதை மற்றும் அணுகல் அனுமதிகளை சரிபார்க்கவும்
- மாடல் `trust_remote_code=True` ஐ தேவைப்படுகிறதா என்பதை சரிபார்க்கவும்
- தேவையான அனைத்து மாடல் கோப்புகளும் பதிவிறக்கப்பட்டுள்ளதா என்பதை உறுதிப்படுத்தவும்

### உதவி பெறுதல்

- **ஆவணங்கள்**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **உதாரணங்கள்**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## கூடுதல் வளங்கள்

### அதிகாரப்பூர்வ இணைப்புகள்
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime ஆவணங்கள்**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face உதாரணம்**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### சமூக உதாரணங்கள்
- **Jupyter Notebooks**: Olive GitHub Repository இல் கிடைக்கின்றன — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: VS Code க்கான AI Toolkit — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Posts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### தொடர்புடைய கருவிகள்
- **ONNX Runtime**: உயர் செயல்திறன் inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: பல இணக்கமான மாடல்களின் மூலமாக — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: மேக அடிப்படையிலான மேம்பாட்டு செயல்முறைகள் — https://learn.microsoft.com/azure/machine-learning/

## ➡️ அடுத்தது என்ன

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளுங்கள். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.