<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T15:25:54+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ne"
}
-->
# सत्र ३: ओपन-सोर्स मोडेल खोज र व्यवस्थापन

## अवलोकन

यो सत्रले Foundry Local मार्फत व्यावहारिक मोडेल खोज र व्यवस्थापनमा केन्द्रित छ। तपाईंले उपलब्ध मोडेलहरूको सूची बनाउने, विभिन्न विकल्पहरू परीक्षण गर्ने, र आधारभूत प्रदर्शन विशेषताहरू बुझ्ने सिक्नुहुनेछ। यो विधिले सही मोडेलहरू चयन गर्न मद्दत गर्न फाउन्ड्री CLI को प्रयोगमा जोड दिन्छ।

## सिक्ने उद्देश्यहरू

- मोडेल खोज र व्यवस्थापनका लागि फाउन्ड्री CLI आदेशहरूमा दक्षता हासिल गर्नुहोस्
- मोडेल क्यास र स्थानीय भण्डारण ढाँचाहरू बुझ्नुहोस्
- विभिन्न मोडेलहरू छिटो परीक्षण र तुलना गर्न सिक्नुहोस्
- मोडेल चयन र बेंचमार्किङका लागि व्यावहारिक कार्यप्रवाह स्थापना गर्नुहोस्
- Foundry Local मार्फत उपलब्ध मोडेलहरूको बढ्दो इकोसिस्टम अन्वेषण गर्नुहोस्

## पूर्वशर्तहरू

- सत्र १: Foundry Local को सुरुवात पूरा भएको छ
- Foundry Local CLI स्थापना गरिएको र पहुँचयोग्य छ
- मोडेल डाउनलोडका लागि पर्याप्त भण्डारण स्थान (मोडेलहरू १GB देखि २०GB+ सम्म हुन सक्छ)
- मोडेल प्रकार र प्रयोग केसहरूको आधारभूत समझ

## भाग ६: व्यावहारिक अभ्यास

### अभ्यास: मोडेल खोज र तुलना

Sample 03 को आधारमा आफ्नो मोडेल मूल्यांकन स्क्रिप्ट बनाउनुहोस्:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### तपाईंको कार्य

1. **Sample 03 स्क्रिप्ट चलाउनुहोस्**: `samples\03\list_and_bench.cmd`
2. **विभिन्न मोडेलहरू प्रयास गर्नुहोस्**: कम्तीमा ३ मोडेल परीक्षण गर्नुहोस्
3. **प्रदर्शन तुलना गर्नुहोस्**: गति र प्रतिक्रिया गुणस्तरमा भिन्नता नोट गर्नुहोस्
4. **नतिजा दस्तावेज गर्नुहोस्**: सरल तुलना चार्ट बनाउनुहोस्

### उदाहरण तुलना ढाँचा

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## भाग ७: समस्या समाधान र उत्तम अभ्यासहरू

### सामान्य समस्या र समाधानहरू

**मोडेल सुरु हुँदैन:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**अपर्याप्त मेमोरी:**
- साना मोडेलहरूबाट सुरु गर्नुहोस् (`phi-4-mini`)
- अन्य एप्लिकेसनहरू बन्द गर्नुहोस्
- बारम्बार सीमामा पुग्दा RAM अपग्रेड गर्नुहोस्

**ढिलो प्रदर्शन:**
- सुनिश्चित गर्नुहोस् कि मोडेल पूर्ण रूपमा लोड भएको छ (विस्तृत आउटपुट जाँच गर्नुहोस्)
- अनावश्यक पृष्ठभूमि एप्लिकेसनहरू बन्द गर्नुहोस्
- छिटो भण्डारण (SSD) विचार गर्नुहोस्

### उत्तम अभ्यासहरू

1. **सानोबाट सुरु गर्नुहोस्**: सेटअप मान्य गर्न `phi-4-mini` बाट सुरु गर्नुहोस्
2. **एक पटकमा एक मोडेल**: नयाँ मोडेल सुरु गर्नु अघि पुरानो मोडेल बन्द गर्नुहोस्
3. **स्रोतहरू अनुगमन गर्नुहोस्**: मेमोरी प्रयोगमा ध्यान दिनुहोस्
4. **नियमित परीक्षण गर्नुहोस्**: निष्पक्ष तुलना गर्न समान प्रॉम्प्टहरू प्रयोग गर्नुहोस्
5. **नतिजा दस्तावेज गर्नुहोस्**: तपाईंको प्रयोग केसहरूको लागि मोडेल प्रदर्शनमा नोटहरू राख्नुहोस्

## भाग ८: आगामी कदमहरू र सन्दर्भहरू

### सत्र ४ को तयारी

- **सत्र ४ केन्द्रबिन्दु**: अनुकूलन उपकरणहरू र प्रविधिहरू
- **पूर्वशर्तहरू**: मोडेल स्विचिङ र आधारभूत प्रदर्शन परीक्षणमा सहजता
- **सिफारिस गरिएको**: यस सत्रबाट २-३ मनपर्ने मोडेलहरू पहिचान गर्नुहोस्

### थप स्रोतहरू

- **[Foundry Local Documentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: आधिकारिक दस्तावेज
- **[CLI Reference](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: पूर्ण आदेश सन्दर्भ
- **[Model Mondays](https://aka.ms/model-mondays)**: साप्ताहिक मोडेल स्पटलाइटहरू
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: समुदाय र समस्याहरू
- **[Sample 03: Model Discovery](samples/03/README.md)**: व्यावहारिक उदाहरण स्क्रिप्ट

### मुख्य निष्कर्षहरू

✅ **मोडेल खोज**: उपलब्ध मोडेलहरू अन्वेषण गर्न `foundry model list` प्रयोग गर्नुहोस्  
✅ **छिटो परीक्षण**: छिटो मूल्यांकनका लागि `list_and_bench.cmd` ढाँचा  
✅ **प्रदर्शन अनुगमन**: आधारभूत स्रोत प्रयोग र प्रतिक्रिया समय मापन  
✅ **मोडेल चयन**: प्रयोग केसद्वारा मोडेल चयनका लागि व्यावहारिक दिशानिर्देशहरू  
✅ **क्यास व्यवस्थापन**: भण्डारण र सफाइ प्रक्रियाहरू बुझ्नुहोस्  

अब तपाईंले Foundry Local को सरल CLI दृष्टिकोण प्रयोग गरेर AI अनुप्रयोगहरूको लागि उपयुक्त मोडेलहरू खोज्न, परीक्षण गर्न, र चयन गर्न व्यावहारिक सीपहरू हासिल गर्नुभएको छ।

References:
- Foundry Local docs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## सिक्ने उद्देश्यहरू
- स्थानीय इनफरेन्सका लागि ओपन-सोर्स मोडेलहरू खोज र मूल्यांकन गर्नुहोस्
- Foundry Local भित्र चयनित Hugging Face मोडेलहरू कम्पाइल र चलाउनुहोस्
- शुद्धता, ढिलाइ, र स्रोत आवश्यकताहरूका लागि मोडेल चयन रणनीतिहरू लागू गर्नुहोस्
- क्यास र संस्करण व्यवस्थापनका साथ मोडेलहरू स्थानीय रूपमा व्यवस्थापन गर्नुहोस्

## भाग १: Foundry CLI मार्फत मोडेल खोज

### आधारभूत मोडेल व्यवस्थापन आदेशहरू

Foundry CLI ले मोडेल खोज र व्यवस्थापनका लागि सरल आदेशहरू प्रदान गर्दछ:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### तपाईंको पहिलो मोडेलहरू चलाउनुहोस्

प्रदर्शन विशेषताहरू बुझ्न लोकप्रिय, राम्रो परीक्षण गरिएका मोडेलहरूबाट सुरु गर्नुहोस्:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**नोट:** `--verbose` फ्ल्यागले विस्तृत स्टार्टअप जानकारी प्रदान गर्दछ, जसमा समावेश छ:
- मोडेल डाउनलोड प्रगति (पहिलो रनमा)
- मेमोरी आवंटन विवरण
- सेवा बाइन्डिङ जानकारी
- प्रदर्शन प्रारम्भिक मेट्रिक्स

### मोडेल श्रेणीहरू बुझ्नुहोस्

**सानो भाषा मोडेलहरू (SLMs):**
- `phi-4-mini`: छिटो, कुशल, सामान्य च्याटका लागि उत्कृष्ट
- `phi-4`: राम्रो तर्कसहितको अधिक सक्षम संस्करण

**मध्यम मोडेलहरू:**
- `qwen2.5-7b-instruct`: उत्कृष्ट तर्क र लामो सन्दर्भ
- `deepseek-r1-distill-qwen-7b`: कोड जेनेरेशनका लागि अनुकूलित

**ठूला मोडेलहरू:**
- `llama-3.2`: Meta को पछिल्लो ओपन-सोर्स मोडेल
- `qwen2.5-14b-instruct`: उद्यम-स्तरको तर्क

## भाग २: छिटो मोडेल परीक्षण र तुलना

### Sample 03 दृष्टिकोण: सरल सूची र बेंच

हाम्रो Sample 03 ढाँचाको आधारमा, यहाँ न्यूनतम कार्यप्रवाह छ:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### मोडेल प्रदर्शन परीक्षण

मोडेल चलिरहेको अवस्थामा, समान प्रॉम्प्टहरूसँग परीक्षण गर्नुहोस्:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell परीक्षण विकल्प

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## भाग ३: मोडेल क्यास र भण्डारण व्यवस्थापन

### मोडेल क्यास बुझ्नुहोस्

Foundry Local ले स्वचालित रूपमा मोडेल डाउनलोड र क्यासिङ व्यवस्थापन गर्दछ:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### मोडेल भण्डारण विचारहरू

**सामान्य मोडेल आकारहरू:**
- `phi-4-mini`: ~२.५ GB
- `qwen2.5-7b-instruct`: ~४.१ GB  
- `deepseek-r1-distill-qwen-7b`: ~४.३ GB
- `llama-3.2`: ~४.९ GB
- `qwen2.5-14b-instruct`: ~८.२ GB

**भण्डारण उत्तम अभ्यासहरू:**
- छिटो स्विचिङका लागि २-३ मोडेलहरू क्यास राख्नुहोस्
- स्थान खाली गर्न प्रयोग नगरिएका मोडेलहरू हटाउनुहोस्: `foundry cache clean`
- साना SSD मा विशेष गरी डिस्क प्रयोग अनुगमन गर्नुहोस्
- मोडेल आकार बनाम क्षमता व्यापार-अफहरू विचार गर्नुहोस्

### मोडेल प्रदर्शन अनुगमन

मोडेलहरू चलिरहेको अवस्थामा, प्रणाली स्रोतहरू अनुगमन गर्नुहोस्:

**Windows Task Manager:**
- मेमोरी प्रयोग हेर्नुहोस् (मोडेलहरू RAM मा लोड रहन्छन्)
- इनफरेन्सको समयमा CPU उपयोग अनुगमन गर्नुहोस्
- प्रारम्भिक मोडेल लोडिङको समयमा डिस्क I/O जाँच गर्नुहोस्

**कमाण्ड लाइन अनुगमन:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## भाग ४: व्यावहारिक मोडेल चयन दिशानिर्देशहरू

### प्रयोग केसद्वारा मोडेल चयन

**सामान्य च्याट र प्रश्नोत्तरका लागि:**
- सुरु गर्नुहोस्: `phi-4-mini` (छिटो, कुशल)
- अपग्रेड गर्नुहोस्: `phi-4` (राम्रो तर्क)
- उन्नत: `qwen2.5-7b-instruct` (लामो सन्दर्भ)

**कोड जेनेरेशनका लागि:**
- सिफारिस गरिएको: `deepseek-r1-distill-qwen-7b`
- विकल्प: `qwen2.5-7b-instruct` (कोडका लागि पनि राम्रो)

**जटिल तर्कका लागि:**
- उत्कृष्ट: `qwen2.5-7b-instruct` वा `qwen2.5-14b-instruct`
- बजेट विकल्प: `phi-4`

### हार्डवेयर आवश्यकताहरूको मार्गदर्शन

**न्यूनतम प्रणाली आवश्यकताहरू:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**उत्तम प्रदर्शनका लागि सिफारिस गरिएको:**
- ३२GB+ RAM सहज बहु-मोडेल स्विचिङका लागि
- छिटो मोडेल लोडिङका लागि SSD भण्डारण
- राम्रो सिंगल-थ्रेड प्रदर्शन भएको आधुनिक CPU
- एनपीयू समर्थन (Windows 11 Copilot+ PCs) गति बढाउनका लागि

### मोडेल स्विचिङ कार्यप्रवाह

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## भाग ५: सरल मोडेल बेंचमार्किङ

### आधारभूत प्रदर्शन परीक्षण

मोडेल प्रदर्शन तुलना गर्न यहाँ सरल दृष्टिकोण छ:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### म्यानुअल गुणस्तर मूल्यांकन

प्रत्येक मोडेलका लागि, समान प्रॉम्प्टहरूसँग परीक्षण गर्नुहोस् र म्यानुअल रूपमा मूल्यांकन गर्नुहोस्:

**परीक्षण प्रॉम्प्टहरू:**
1. "क्वान्टम कम्प्युटिङलाई सरल शब्दमा व्याख्या गर्नुहोस्।"
2. "सूचीलाई क्रमबद्ध गर्न Python फङ्सन लेख्नुहोस्।"
3. "दूरस्थ कामका फाइदा र बेफाइदाहरू के हुन्?"
4. "एज AI का फाइदाहरूको सारांश दिनुहोस्।"

**मूल्यांकन मापदण्ड:**
- **शुद्धता**: जानकारी सही छ कि छैन?
- **स्पष्टता**: व्याख्या बुझ्न सजिलो छ कि छैन?
- **पूर्णता**: यसले पूर्ण प्रश्नलाई सम्बोधन गर्छ कि छैन?
- **गति**: प्रतिक्रिया कति छिटो आउँछ?

### स्रोत प्रयोग अनुगमन

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## भाग ६: आगामी कदमहरू
- नयाँ मोडेलहरू र सुझावहरूको लागि Model Mondays सदस्यता लिनुहोस्: https://aka.ms/model-mondays
- तपाईंको टिमको `models.json` मा नतिजा योगदान गर्नुहोस्
- सत्र ४ को तयारी गर्नुहोस्: LLMs बनाम SLMs, स्थानीय बनाम क्लाउड इनफरेन्स, र व्यावहारिक डेमोहरू

---

