<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T23:55:26+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ne"
}
-->
# सत्र ३: ओपन-सोर्स मोडेल खोज र व्यवस्थापन

## अवलोकन

यो सत्रले Foundry Local प्रयोग गरेर व्यावहारिक मोडेल खोज र व्यवस्थापनमा केन्द्रित छ। तपाईंले उपलब्ध मोडेलहरूको सूची बनाउने, विभिन्न विकल्पहरू परीक्षण गर्ने, र आधारभूत प्रदर्शन विशेषताहरू बुझ्ने सिक्नुहुनेछ। यो दृष्टिकोणले सही मोडेलहरू चयन गर्न मद्दत गर्न फाउन्ड्री CLI प्रयोग गरेर व्यावहारिक अन्वेषणलाई जोड दिन्छ।

## सिक्ने उद्देश्यहरू

- मोडेल खोज र व्यवस्थापनका लागि फाउन्ड्री CLI आदेशहरूमा महारत हासिल गर्नुहोस्
- मोडेल क्यास र स्थानीय भण्डारण ढाँचाहरू बुझ्नुहोस्
- विभिन्न मोडेलहरू छिटो परीक्षण र तुलना गर्न सिक्नुहोस्
- मोडेल चयन र बेंचमार्किङका लागि व्यावहारिक कार्यप्रवाह स्थापना गर्नुहोस्
- Foundry Local मार्फत उपलब्ध मोडेलहरूको बढ्दो इकोसिस्टम अन्वेषण गर्नुहोस्

## पूर्वशर्तहरू

- सत्र १: Foundry Local सुरु गर्ने पूरा भएको छ
- Foundry Local CLI स्थापना गरिएको र पहुँचयोग्य छ
- मोडेल डाउनलोडको लागि पर्याप्त भण्डारण स्थान (मोडेलहरू १GB देखि २०GB+ सम्म हुन सक्छ)
- मोडेल प्रकार र प्रयोग केसहरूको आधारभूत समझ

## भाग ६: व्यावहारिक अभ्यास

### अभ्यास: मोडेल खोज र तुलना

Sample 03 मा आधारित आफ्नो मोडेल मूल्यांकन स्क्रिप्ट बनाउनुहोस्:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### तपाईंको कार्य

1. **Sample 03 स्क्रिप्ट चलाउनुहोस्**: `samples\03\list_and_bench.cmd`
2. **विभिन्न मोडेलहरू प्रयास गर्नुहोस्**: कम्तीमा ३ विभिन्न मोडेलहरू परीक्षण गर्नुहोस्
3. **प्रदर्शन तुलना गर्नुहोस्**: गति र प्रतिक्रिया गुणस्तरमा भिन्नता नोट गर्नुहोस्
4. **नतिजा दस्तावेज गर्नुहोस्**: सरल तुलना चार्ट बनाउनुहोस्

### उदाहरण तुलना ढाँचा

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## भाग ७: समस्या समाधान र उत्तम अभ्यासहरू

### सामान्य समस्या र समाधानहरू

**मोडेल सुरु हुँदैन:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**अपर्याप्त मेमोरी:**
- साना मोडेलहरूबाट सुरु गर्नुहोस् (`phi-4-mini`)
- अन्य एप्लिकेसनहरू बन्द गर्नुहोस्
- बारम्बार सीमामा पुग्दा RAM अपग्रेड गर्नुहोस्

**ढिलो प्रदर्शन:**
- सुनिश्चित गर्नुहोस् कि मोडेल पूर्ण रूपमा लोड भएको छ (विस्तृत आउटपुट जाँच गर्नुहोस्)
- अनावश्यक पृष्ठभूमि एप्लिकेसनहरू बन्द गर्नुहोस्
- छिटो भण्डारण (SSD) विचार गर्नुहोस्

### उत्तम अभ्यासहरू

1. **सानोबाट सुरु गर्नुहोस्**: सेटअप मान्य गर्न `phi-4-mini` बाट सुरु गर्नुहोस्
2. **एक पटकमा एक मोडेल**: नयाँ सुरु गर्नु अघि पुरानो मोडेल बन्द गर्नुहोस्
3. **स्रोतहरू अनुगमन गर्नुहोस्**: मेमोरी प्रयोगमा ध्यान दिनुहोस्
4. **निरन्तर परीक्षण गर्नुहोस्**: निष्पक्ष तुलना गर्न समान प्रम्प्टहरू प्रयोग गर्नुहोस्
5. **नतिजा दस्तावेज गर्नुहोस्**: तपाईंको प्रयोग केसहरूको लागि मोडेल प्रदर्शनमा नोटहरू राख्नुहोस्

## भाग ८: आगामी कदमहरू र सन्दर्भहरू

### सत्र ४ को तयारी

- **सत्र ४ केन्द्रबिन्दु**: अनुकूलन उपकरणहरू र प्रविधिहरू
- **पूर्वशर्तहरू**: मोडेल स्विचिङ र आधारभूत प्रदर्शन परीक्षणमा सहजता
- **सिफारिस गरिएको**: यस सत्रबाट २-३ मनपर्ने मोडेलहरू पहिचान गर्नुहोस्

### अतिरिक्त स्रोतहरू

- **[Foundry Local दस्तावेज](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: आधिकारिक दस्तावेज
- **[CLI सन्दर्भ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: पूर्ण आदेश सन्दर्भ
- **[Model Mondays](https://aka.ms/model-mondays)**: साप्ताहिक मोडेल स्पटलाइटहरू
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: समुदाय र समस्याहरू
- **[Sample 03: मोडेल खोज](samples/03/README.md)**: व्यावहारिक उदाहरण स्क्रिप्ट

### मुख्य निष्कर्षहरू

✅ **मोडेल खोज**: उपलब्ध मोडेलहरू अन्वेषण गर्न `foundry model list` प्रयोग गर्नुहोस्  
✅ **छिटो परीक्षण**: छिटो मूल्याङ्कनका लागि `list_and_bench.cmd` ढाँचा  
✅ **प्रदर्शन अनुगमन**: आधारभूत स्रोत प्रयोग र प्रतिक्रिया समय मापन  
✅ **मोडेल चयन**: प्रयोग केसद्वारा मोडेल चयनका लागि व्यावहारिक दिशानिर्देशहरू  
✅ **क्यास व्यवस्थापन**: भण्डारण र सफाइ प्रक्रियाहरू बुझ्नुहोस्  

अब तपाईंले Foundry Local को सरल CLI दृष्टिकोण प्रयोग गरेर AI अनुप्रयोगहरूको लागि उपयुक्त मोडेलहरू खोज्न, परीक्षण गर्न, र चयन गर्न व्यावहारिक सीपहरू प्राप्त गर्नुभएको छ।: समुदाय मोडेलहरू चयन गर्ने, Hugging Face सामग्री एकीकृत गर्ने, र "आफ्नो मोडेल ल्याउनुहोस्" (BYOM) रणनीतिहरू अपनाउने। तपाईंले निरन्तर सिकाइ र मोडेल खोजका लागि Model Mondays श्रृंखला पनि पत्ता लगाउनुहुनेछ।

सन्दर्भहरू:
- Foundry Local दस्तावेज: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face मोडेलहरू कम्पाइल गर्नुहोस्: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## सिक्ने उद्देश्यहरू
- स्थानीय इनफरेन्सका लागि ओपन-सोर्स मोडेलहरू पत्ता लगाउनुहोस् र मूल्याङ्कन गर्नुहोस्
- Foundry Local भित्र चयनित Hugging Face मोडेलहरू कम्पाइल र चलाउनुहोस्
- शुद्धता, विलम्बता, र स्रोत आवश्यकताहरूका लागि मोडेल चयन रणनीतिहरू लागू गर्नुहोस्
- क्यास र संस्करण व्यवस्थापनका साथ मोडेलहरू स्थानीय रूपमा व्यवस्थापन गर्नुहोस्

## भाग १: Foundry CLI प्रयोग गरेर मोडेल खोज

### आधारभूत मोडेल व्यवस्थापन आदेशहरू

Foundry CLI ले मोडेल खोज र व्यवस्थापनका लागि सरल आदेशहरू प्रदान गर्दछ:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### तपाईंको पहिलो मोडेलहरू चलाउँदै

प्रदर्शन विशेषताहरू बुझ्न लोकप्रिय, राम्रो परीक्षण गरिएका मोडेलहरूबाट सुरु गर्नुहोस्:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**नोट:** `--verbose` फ्ल्यागले विस्तृत स्टार्टअप जानकारी प्रदान गर्दछ, जसमा समावेश छ:
- मोडेल डाउनलोड प्रगति (पहिलो रनमा)
- मेमोरी आवंटन विवरणहरू
- सेवा बाइन्डिङ जानकारी
- प्रदर्शन प्रारम्भिककरण मेट्रिक्स

### मोडेल कोटीहरू बुझ्दै

**सानो भाषा मोडेलहरू (SLMs):**
- `phi-4-mini`: छिटो, कुशल, सामान्य च्याटका लागि उत्कृष्ट
- `phi-4`: राम्रो तर्कसहितको अधिक सक्षम संस्करण

**मध्यम मोडेलहरू:**
- `qwen2.5-7b`: उत्कृष्ट तर्क र लामो सन्दर्भ
- `deepseek-r1-7b`: कोड जेनेरेशनका लागि अनुकूलित

**ठूला मोडेलहरू:**
- `llama-3.2`: Meta को पछिल्लो ओपन-सोर्स मोडेल
- `qwen2.5-14b`: उद्यम-ग्रेड तर्क

## भाग २: छिटो मोडेल परीक्षण र तुलना

### Sample 03 दृष्टिकोण: सरल सूची र बेंच

हाम्रो Sample 03 ढाँचामा आधारित, यहाँ न्यूनतम कार्यप्रवाह छ:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### मोडेल प्रदर्शन परीक्षण गर्दै

एक मोडेल चलिरहेको अवस्थामा, समान प्रम्प्टहरूसँग परीक्षण गर्नुहोस्:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell परीक्षण विकल्प

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## भाग ३: मोडेल क्यास र भण्डारण व्यवस्थापन

### मोडेल क्यास बुझ्दै

Foundry Local ले स्वचालित रूपमा मोडेल डाउनलोड र क्यासिङ व्यवस्थापन गर्दछ:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### मोडेल भण्डारण विचारहरू

**सामान्य मोडेल आकारहरू:**
- `phi-4-mini`: ~२.५ GB
- `qwen2.5-7b`: ~४.१ GB  
- `deepseek-r1-7b`: ~४.३ GB
- `llama-3.2`: ~४.९ GB
- `qwen2.5-14b`: ~८.२ GB

**भण्डारण उत्तम अभ्यासहरू:**
- छिटो स्विचिङका लागि २-३ मोडेलहरू क्यास राख्नुहोस्
- प्रयोग नगरिएका मोडेलहरू हटाएर ठाउँ खाली गर्नुहोस्: `foundry cache clean`
- साना SSD मा विशेष गरी डिस्क प्रयोग अनुगमन गर्नुहोस्
- मोडेल आकार बनाम क्षमता व्यापार-अफ विचार गर्नुहोस्

### मोडेल प्रदर्शन अनुगमन

मोडेलहरू चलिरहेको अवस्थामा, प्रणाली स्रोतहरू अनुगमन गर्नुहोस्:

**Windows Task Manager:**
- मेमोरी प्रयोग हेर्नुहोस् (मोडेलहरू RAM मा लोड रहन्छन्)
- इनफरेन्सको समयमा CPU उपयोग अनुगमन गर्नुहोस्
- प्रारम्भिक मोडेल लोडिङको समयमा डिस्क I/O जाँच गर्नुहोस्

**कमाण्ड लाइन अनुगमन:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## भाग ४: व्यावहारिक मोडेल चयन दिशानिर्देशहरू

### प्रयोग केसद्वारा मोडेल चयन गर्दै

**सामान्य च्याट र Q&A का लागि:**
- सुरु गर्नुहोस्: `phi-4-mini` (छिटो, कुशल)
- अपग्रेड गर्नुहोस्: `phi-4` (राम्रो तर्क)
- उन्नत: `qwen2.5-7b` (लामो सन्दर्भ)

**कोड जेनेरेशनका लागि:**
- सिफारिस गरिएको: `deepseek-r1-7b`
- विकल्प: `qwen2.5-7b` (कोडका लागि पनि राम्रो)

**जटिल तर्कका लागि:**
- उत्कृष्ट: `qwen2.5-7b` वा `qwen2.5-14b`
- बजेट विकल्प: `phi-4`

### हार्डवेयर आवश्यकताहरू मार्गदर्शन

**न्यूनतम प्रणाली आवश्यकताहरू:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**उत्तम प्रदर्शनका लागि सिफारिस गरिएको:**
- ३२GB+ RAM सहज बहु-मोडेल स्विचिङका लागि
- छिटो मोडेल लोडिङका लागि SSD भण्डारण
- राम्रो सिंगल-थ्रेड प्रदर्शन भएको आधुनिक CPU
- एनपीयू समर्थन (Windows 11 Copilot+ PCs) गति बढाउनका लागि

### मोडेल स्विचिङ कार्यप्रवाह

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## भाग ५: सरल मोडेल बेंचमार्किङ

### आधारभूत प्रदर्शन परीक्षण

मोडेल प्रदर्शन तुलना गर्न यहाँ एक सरल दृष्टिकोण छ:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### म्यानुअल गुणस्तर मूल्याङ्कन

प्रत्येक मोडेलका लागि, समान प्रम्प्टहरूसँग परीक्षण गर्नुहोस् र म्यानुअल रूपमा मूल्याङ्कन गर्नुहोस्:

**परीक्षण प्रम्प्टहरू:**
1. "क्वान्टम कम्प्युटिङलाई सरल शब्दमा व्याख्या गर्नुहोस्।"
2. "सूचीलाई क्रमबद्ध गर्न Python फङ्सन लेख्नुहोस्।"
3. "दूरस्थ कामका फाइदा र बेफाइदाहरू के हुन्?"
4. "एज AI का फाइदाहरूको संक्षेपमा व्याख्या गर्नुहोस्।"

**मूल्याङ्कन मापदण्डहरू:**
- **शुद्धता**: जानकारी सही छ कि छैन?
- **स्पष्टता**: व्याख्या बुझ्न सजिलो छ कि छैन?
- **पूर्णता**: यसले पूर्ण प्रश्नलाई सम्बोधन गर्छ कि छैन?
- **गति**: यसले कति छिटो प्रतिक्रिया दिन्छ?

### स्रोत प्रयोग अनुगमन

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## भाग ६: आगामी कदमहरू
- नयाँ मोडेलहरू र सुझावहरूको लागि Model Mondays सदस्यता लिनुहोस्: https://aka.ms/model-mondays
- तपाईंको टिमको `models.json` मा नतिजा योगदान गर्नुहोस्
- सत्र ४ को तयारी गर्नुहोस्: LLMs बनाम SLMs, स्थानीय बनाम क्लाउड इनफरेन्स, र व्यावहारिक डेमोहरूको तुलना

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी यथासम्भव शुद्धता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। मूल दस्तावेज़ यसको मातृभाषामा आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।