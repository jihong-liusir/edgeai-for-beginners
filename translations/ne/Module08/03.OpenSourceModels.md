<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T17:43:35+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ne"
}
-->
# सत्र ३: फाउन्ड्री लोकलसँग ओपन-सोर्स मोडेलहरू

## अवलोकन

यस सत्रमा फाउन्ड्री लोकलमा ओपन-सोर्स मोडेलहरू ल्याउने तरिकाहरूको अन्वेषण गरिन्छ: समुदाय मोडेलहरू चयन गर्ने, Hugging Face सामग्री एकीकृत गर्ने, र "आफ्नो मोडेल ल्याउनुहोस्" (BYOM) रणनीतिहरू अपनाउने। तपाईंले निरन्तर सिकाइ र मोडेल खोजका लागि Model Mondays शृंखला पनि पत्ता लगाउनुहुनेछ।

सन्दर्भहरू:
- फाउन्ड्री लोकल डकहरू: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face मोडेलहरू कम्पाइल गर्नुहोस्: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- फाउन्ड्री लोकल GitHub: https://github.com/microsoft/Foundry-Local

## सिकाइ उद्देश्यहरू
- स्थानीय इनफरेन्सका लागि ओपन-सोर्स मोडेलहरू पत्ता लगाउनुहोस् र मूल्याङ्कन गर्नुहोस्
- फाउन्ड्री लोकलमा चयनित Hugging Face मोडेलहरू कम्पाइल गर्नुहोस् र चलाउनुहोस्
- शुद्धता, विलम्बता, र स्रोत आवश्यकताका लागि मोडेल चयन रणनीतिहरू लागू गर्नुहोस्
- क्यास र संस्करण व्यवस्थापनको साथ मोडेलहरू स्थानीय रूपमा व्यवस्थापन गर्नुहोस्

## भाग १: मोडेल खोज र चयन (चरण-दर-चरण)

चरण १) स्थानीय क्याटलगमा उपलब्ध मोडेलहरूको सूची बनाउनुहोस्  
```cmd
foundry model list
```
  
चरण २) दुई उम्मेदवारहरूको छिटो परीक्षण गर्नुहोस् (पहिलो पटक चलाउँदा स्वतः डाउनलोड हुन्छ)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
चरण ३) आधारभूत मेट्रिक्स नोट गर्नुहोस्  
- निश्चित प्रम्प्टका लागि विलम्बता (धारणा) र गुणस्तर अवलोकन गर्नुहोस्  
- प्रत्येक मोडेल चलाउँदा Task Manager मार्फत मेमोरी प्रयोग हेर्नुहोस्  

## भाग २: क्याटलग मोडेलहरू CLI मार्फत चलाउनुहोस् (चरण-दर-चरण)

चरण १) मोडेल सुरु गर्नुहोस्  
```cmd
foundry model run llama-3.2
```
  
चरण २) OpenAI-संगत अन्त बिन्दु मार्फत परीक्षण प्रम्प्ट पठाउनुहोस्  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## भाग ३: BYOM – Hugging Face मोडेलहरू कम्पाइल गर्नुहोस् (चरण-दर-चरण)

मोडेलहरू कम्पाइल गर्नको लागि आधिकारिक "कसरी गर्ने" मार्गदर्शन अनुसरण गर्नुहोस्। तल उच्च-स्तरीय प्रवाह दिइएको छ—Microsoft Learn लेखमा ठ्याक्कै आदेशहरू र समर्थित कन्फिगरेसनहरू हेर्नुहोस्।

चरण १) कार्य निर्देशिका तयार गर्नुहोस्  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
चरण २) समर्थित HF मोडेल कम्पाइल गर्नुहोस्  
- Learn डकका चरणहरू प्रयोग गरेर ONNX मोडेलमा रूपान्तरण गर्नुहोस् र कम्पाइल गरिएको मोडेललाई `models` निर्देशिकामा राख्नुहोस्  
- पुष्टि गर्नुहोस्:  
```cmd
foundry cache ls
```
  
तपाईंले कम्पाइल गरिएको मोडेलको नाम देख्नुहुनेछ (जस्तै, `llama-3.2`)।  

चरण ३) कम्पाइल गरिएको मोडेल चलाउनुहोस्  
```cmd
foundry model run llama-3.2 --verbose
```
  
नोटहरू:  
- कम्पाइल र चलाउन पर्याप्त डिस्क र RAM सुनिश्चित गर्नुहोस्  
- प्रवाह मान्य गर्न साना मोडेलहरूबाट सुरु गर्नुहोस्, त्यसपछि विस्तार गर्नुहोस्  

## भाग ४: व्यावहारिक मोडेल क्युरेसन (चरण-दर-चरण)

चरण १) `models.json` रजिस्ट्री सिर्जना गर्नुहोस्  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
चरण २) सानो चयनकर्ता स्क्रिप्ट  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## भाग ५: व्यावहारिक बेंचमार्कहरू (चरण-दर-चरण)

चरण १) साधारण विलम्बता बेंचमार्क  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
चरण २) गुणस्तर स्पट-जाँच  
- निश्चित प्रम्प्ट सेट प्रयोग गर्नुहोस्, आउटपुटहरूलाई CSV/JSON मा कैप्चर गर्नुहोस्  
- म्यानुअली फ्लुएन्सी, सान्दर्भिकता, र शुद्धता (१–५) मूल्याङ्कन गर्नुहोस्  

## भाग ६: आगामी कदमहरू
- नयाँ मोडेलहरू र सुझावहरूको लागि Model Mondays सदस्यता लिनुहोस्: https://aka.ms/model-mondays  
- आफ्नो टिमको `models.json` मा पत्ता लगाएका कुराहरू योगदान गर्नुहोस्  
- सत्र ४ को तयारी गर्नुहोस्: LLMs बनाम SLMs, स्थानीय बनाम क्लाउड इनफरेन्स, र व्यावहारिक डेमोहरूको तुलना  

---

