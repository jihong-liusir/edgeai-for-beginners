<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6503a980cb3bf2b2de2d2bc4ac6acc4c",
  "translation_date": "2025-09-24T15:26:31+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ne"
}
-->
# рд╕рддреНрд░ рез: Foundry Local рд╕реБрд░реБ рдЧрд░реНрдиреЗ

## рдкрд░рд┐рдЪрдп

Microsoft Foundry Local рд▓реЗ Azure AI Foundry рдХреНрд╖рдорддрд╛рд╣рд░реВ рд╕рд┐рдзреИ рддрдкрд╛рдИрдВрдХреЛ Windows 11 рд╡рд┐рдХрд╛рд╕ рд╡рд╛рддрд╛рд╡рд░рдгрдорд╛ рд▓реНрдпрд╛рдЙрдБрдЫ, рдЬрд╕рд▓реЗ рдЧреЛрдкрдиреАрдпрддрд╛ рд╕реБрд░рдХреНрд╖рд┐рдд рдЧрд░реНрдиреЗ, рдХрдо рд╡рд┐рд▓рдореНрдмрддрд╛ AI рд╡рд┐рдХрд╛рд╕рд▓рд╛рдИ рдЙрджреНрдпрдо-рд╕реНрддрд░рдХрд╛ рдЙрдкрдХрд░рдгрд╣рд░реВрд╕рдБрдЧ рд╕рдХреНрд╖рдо рдмрдирд╛рдЙрдБрдЫред рдпреЛ рд╕рддреНрд░рд▓реЗ рд▓реЛрдХрдкреНрд░рд┐рдп рдореЛрдбреЗрд▓рд╣рд░реВ рдЬрд╕реНрддреИ phi, qwen, deepseek, рд░ GPT-OSS-20B рдХреЛ рдкреВрд░реНрдг рд╕реНрдерд╛рдкрдирд╛, рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди, рд░ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдкрд░рд┐рдирд┐рдпреЛрдЬрди рд╕рдореЗрдЯреНрдЫред

## рд╕рд┐рдХрд╛рдЗ рдЙрджреНрджреЗрд╢реНрдпрд╣рд░реВ

рдпреЛ рд╕рддреНрд░рдХреЛ рдЕрдиреНрддреНрдпрд╕рдореНрдордорд╛, рддрдкрд╛рдИрдВ:
- Windows 11 рдорд╛ Foundry Local рд╕реНрдерд╛рдкрдирд╛ рд░ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░реНрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫ
- CLI рдХрдорд╛рдгреНрдбрд╣рд░реВ рд░ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рд╡рд┐рдХрд▓реНрдкрд╣рд░реВрдорд╛ рдорд╣рд╛рд░рдд рд╣рд╛рд╕рд┐рд▓ рдЧрд░реНрдиреБрд╣реБрдиреНрдЫ
- рдкреНрд░рджрд░реНрд╢рдирд▓рд╛рдИ рдЕрдиреБрдХреВрд▓ рдмрдирд╛рдЙрди рдореЛрдбреЗрд▓ рдХреНрдпрд╛рд╕рд┐рдЩ рд░рдгрдиреАрддрд┐рд╣рд░реВ рдмреБрдЭреНрдиреБрд╣реБрдиреНрдЫ
- phi, qwen, deepseek, рд░ GPT-OSS-20B рдореЛрдбреЗрд▓рд╣рд░реВ рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ рдЪрд▓рд╛рдЙрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫ
- Foundry Local рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЖрдлреНрдиреЛ рдкрд╣рд┐рд▓реЛ AI рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдмрдирд╛рдЙрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫ

## рдкреВрд░реНрд╡рд╢рд░реНрддрд╣рд░реВ

### рдкреНрд░рдгрд╛рд▓реА рдЖрд╡рд╢реНрдпрдХрддрд╛рд╣рд░реВ
- **Windows 11**: рд╕рдВрд╕реНрдХрд░рдг 22H2 рд╡рд╛ рдкрдЫрд┐рд▓реНрд▓реЛ
- **RAM**: рдиреНрдпреВрдирддрдо 16GB, рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХреЛ 32GB
- **рд╕реНрдЯреЛрд░реЗрдЬ**: рдореЛрдбреЗрд▓ рд░ рдХреНрдпрд╛рд╕рдХрд╛ рд▓рд╛рдЧрд┐ 50GB рдЦрд╛рд▓реА рдард╛рдЙрдБ
- **рд╣рд╛рд░реНрдбрд╡реЗрдпрд░**: NPU- рд╡рд╛ GPU-рд╕рдХреНрд╖рдо рдЙрдкрдХрд░рдг рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХреЛ (Copilot+ PC рд╡рд╛ NVIDIA GPU)
- **рдиреЗрдЯрд╡рд░реНрдХ**: рдореЛрдбреЗрд▓ рдбрд╛рдЙрдирд▓реЛрдбрдХрд╛ рд▓рд╛рдЧрд┐ рдЙрдЪреНрдЪ рдЧрддрд┐рдХреЛ рдЗрдиреНрдЯрд░рдиреЗрдЯ

### рд╡рд┐рдХрд╛рд╕ рд╡рд╛рддрд╛рд╡рд░рдг
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## рднрд╛рдЧ рез: рд╕реНрдерд╛рдкрдирд╛ рд░ рд╕реЗрдЯрдЕрдк

### рдЪрд░рдг рез: Foundry Local рд╕реНрдерд╛рдкрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН

Winget рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ Foundry Local рд╕реНрдерд╛рдкрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН рд╡рд╛ GitHub рдмрд╛рдЯ рдЗрдиреНрд╕реНрдЯрд▓рд░ рдбрд╛рдЙрдирд▓реЛрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### рдЪрд░рдг реи: рд╕реНрдерд╛рдкрдирд╛ рдкреНрд░рдорд╛рдгрд┐рдд рдЧрд░реНрдиреБрд╣реЛрд╕реН

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## рднрд╛рдЧ реи: CLI рдмреБрдЭреНрдиреЗ

### рдореБрдЦреНрдп рдХрдорд╛рдгреНрдб рд╕рдВрд░рдЪрдирд╛

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## рднрд╛рдЧ рей: рдореЛрдбреЗрд▓ рдХреНрдпрд╛рд╕рд┐рдЩ рд░ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди

Foundry Local рд▓реЗ рдкреНрд░рджрд░реНрд╢рди рд░ рд╕реНрдЯреЛрд░реЗрдЬрд▓рд╛рдИ рдЕрдиреБрдХреВрд▓ рдмрдирд╛рдЙрди рдмреБрджреНрдзрд┐рдорд╛рди рдореЛрдбреЗрд▓ рдХреНрдпрд╛рд╕рд┐рдЩ рд▓рд╛рдЧреВ рдЧрд░реНрджрдЫ:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## рднрд╛рдЧ рек: рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдореЛрдбреЗрд▓ рдкрд░рд┐рдирд┐рдпреЛрдЬрди

### Microsoft Phi рдореЛрдбреЗрд▓рд╣рд░реВ рдЪрд▓рд╛рдЙрдиреЗ

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Qwen рдореЛрдбреЗрд▓рд╣рд░реВрд╕рдБрдЧ рдХрд╛рдо рдЧрд░реНрдиреЗ

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b-instruct
foundry model run qwen2.5-14b-instruct
```

### DeepSeek рдореЛрдбреЗрд▓рд╣рд░реВ рдЪрд▓рд╛рдЙрдиреЗ

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-distill-qwen-7b
```

### GPT-OSS-20B рдЪрд▓рд╛рдЙрдиреЗ

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## рднрд╛рдЧ рел: рдЖрдлреНрдиреЛ рдкрд╣рд┐рд▓реЛ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдмрдирд╛рдЙрдиреЗ

### рдЖрдзреБрдирд┐рдХ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди (OpenAI SDK + Foundry Local)

OpenAI SDK рд░ Foundry Local рдПрдХреАрдХрд░рдг рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдмрдирд╛рдЙрдиреБрд╣реЛрд╕реН, рд╣рд╛рдореНрд░реЛ рдирдореВрдирд╛ режрез рдмрд╛рдЯ рдврд╛рдБрдЪрд╛рд╣рд░реВ рдЕрдиреБрд╕рд░рдг рдЧрд░реНрджреИред

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("тЪая╕П Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"ЁЯМР Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"ЁЯПа Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"тЪая╕П Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"ЁЯФз Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## рднрд╛рдЧ рем: рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди рд░ рдЙрддреНрдХреГрд╖реНрдЯ рдЕрднреНрдпрд╛рд╕рд╣рд░реВ

### рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛рд╣рд░реВ рд░ рд╕рдорд╛рдзрд╛рдирд╣рд░реВ

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### рдкреНрд░рдгрд╛рд▓реА рд╕реНрд░реЛрддрд╣рд░реВ рдЕрдиреБрдЧрдорди рдЧрд░реНрдиреЗ (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### рд╡рд╛рддрд╛рд╡рд░рдг рдЪрд░рд╣рд░реВ

| рдЪрд░ | рд╡рд┐рд╡рд░рдг | рдбрд┐рдлрд▓реНрдЯ | рдЖрд╡рд╢реНрдпрдХ |
|-----|--------|--------|---------|
| `MODEL` | рдореЛрдбреЗрд▓ рдЙрдкрдирд╛рдо рд╡рд╛ рдирд╛рдо | `phi-4-mini` | рд╣реЛрдЗрди |
| `BASE_URL` | Foundry Local рдЖрдзрд╛рд░ URL | `http://localhost:8000` | рд╣реЛрдЗрди |
| `API_KEY` | API рдХреБрдЮреНрдЬреА (рд╕рд╛рдорд╛рдиреНрдпрддрдпрд╛ рд╕реНрдерд╛рдиреАрдпрдХрд╛ рд▓рд╛рдЧрд┐ рдЖрд╡рд╢реНрдпрдХ рдЫреИрди) | `""` | рд╣реЛрдЗрди |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI рдЕрдиреНрдд рдмрд┐рдиреНрджреБ | - | Azure рдХрд╛ рд▓рд╛рдЧрд┐ |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API рдХреБрдЮреНрдЬреА | - | Azure рдХрд╛ рд▓рд╛рдЧрд┐ |
| `AZURE_OPENAI_API_VERSION` | Azure API рд╕рдВрд╕реНрдХрд░рдг | `2024-08-01-preview` | рд╣реЛрдЗрди |

### рдЙрддреНрдХреГрд╖реНрдЯ рдЕрднреНрдпрд╛рд╕рд╣рд░реВ

- **OpenAI SDK рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН**: рд░рд╛рдореНрд░реЛ рдорд░реНрдорддрдпреЛрдЧреНрдпрддрд╛рдХрд╛ рд▓рд╛рдЧрд┐ OpenAI SDK рд▓рд╛рдИ рдХрдЪреНрдЪрд╛ HTTP рдЕрдиреБрд░реЛрдзрд╣рд░реВ рднрдиреНрджрд╛ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рджрд┐рдиреБрд╣реЛрд╕реН
- **FoundryLocalManager**: рд╕реЗрд╡рд╛ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрдирдХрд╛ рд▓рд╛рдЧрд┐ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН рдЬрдм рдЙрдкрд▓рдмреНрдз рдЫ
- **рддреНрд░реБрдЯрд┐ рд╣реНрдпрд╛рдиреНрдбрд▓рд┐рдЩ**: рдЙрддреНрдкрд╛рджрди рдПрдкреНрд▓рд┐рдХреЗрд╕рдирд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рдЙрдЪрд┐рдд рдлрд▓рдмреНрдпрд╛рдХ рд░рдгрдиреАрддрд┐рд╣рд░реВ рд▓рд╛рдЧреВ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- **рдирд┐рдпрдорд┐рдд рд░реВрдкрдорд╛ рдЕрдкрдбреЗрдЯ рдЧрд░реНрдиреБрд╣реЛрд╕реН**: рдирдпрд╛рдБ рдореЛрдбреЗрд▓ рд░ рд╕реБрдзрд╛рд░рд╣рд░реВ рдкрд╣реБрдБрдЪ рдЧрд░реНрди Foundry Local рдЕрджреНрдпрд╛рд╡рдзрд┐рдХ рд░рд╛рдЦреНрдиреБрд╣реЛрд╕реН
- **рд╕рд╛рдиреЛрдмрд╛рдЯ рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН**: рд╕рд╛рдирд╛ рдореЛрдбреЗрд▓рд╣рд░реВ (Phi mini, Qwen 7B) рдмрд╛рдЯ рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН рд░ рд╡рд┐рд╕реНрддрд╛рд░ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- **рд╕реНрд░реЛрддрд╣рд░реВ рдЕрдиреБрдЧрдорди рдЧрд░реНрдиреБрд╣реЛрд╕реН**: CPU/GPU/рдореЗрдореЛрд░реА рдЯреНрд░реНрдпрд╛рдХ рдЧрд░реНрдиреБрд╣реЛрд╕реН рдЬрдм рдкреНрд░рдореНрдкреНрдЯрд╣рд░реВ рд░ рд╕реЗрдЯрд┐рдЩрд╣рд░реВ рдЯреНрдпреБрди рдЧрд░реНрджреИ

## рднрд╛рдЧ рен: рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЕрднреНрдпрд╛рд╕рд╣рд░реВ

### рдЕрднреНрдпрд╛рд╕ рез: рдЫрд┐рдЯреЛ рдмрд╣реБ-рдореЛрдбреЗрд▓ рдкрд░реАрдХреНрд╖рдг

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b-instruct"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### рдЕрднреНрдпрд╛рд╕ реи: OpenAI SDK рдПрдХреАрдХрд░рдг рдкрд░реАрдХреНрд╖рдг

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"тЬЕ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"тЭМ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b-instruct"]
for model in models_to_test:
    test_model_integration(model)
```

### рдЕрднреНрдпрд╛рд╕ рей: рд╡реНрдпрд╛рдкрдХ рд╕реЗрд╡рд╛ рд╕реНрд╡рд╛рд╕реНрдереНрдп рдЬрд╛рдБрдЪ

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"тЬЕ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"тЬЕ {model_id}: Working")
            except Exception as e:
                print(f"тЭМ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"тЭМ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## рд╕рдиреНрджрд░реНрднрд╣рд░реВ

- **Foundry Local рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **CLI рд╕рдиреНрджрд░реНрдн рд░ рдХрдорд╛рдгреНрдбрд╣рд░реВрдХреЛ рдЕрд╡рд▓реЛрдХрди**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **OpenAI SDK рдПрдХреАрдХрд░рдг**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Hugging Face рдореЛрдбреЗрд▓рд╣рд░реВ рдХрдореНрдкрд╛рдЗрд▓ рдЧрд░реНрдиреБрд╣реЛрд╕реН**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **рдирдореВрдирд╛ режрез: OpenAI SDK рдорд╛рд░реНрдлрдд рдЫрд┐рдЯреЛ рдЪреНрдпрд╛рдЯ**: samples/01/README.md
- **рдирдореВрдирд╛ режреи: рдЙрдиреНрдирдд SDK рдПрдХреАрдХрд░рдг**: samples/02/README.md

---

