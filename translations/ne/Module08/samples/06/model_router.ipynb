{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b0bff0",
   "metadata": {},
   "source": [
    "# नमूना ०६: बौद्धिक मोडेल राउटर - उपकरणको रूपमा मोडेलहरू\n",
    "\n",
    "यो नोटबुकले एक बौद्धिक राउटिङ प्रणाली प्रदर्शन गर्दछ जसले विभिन्न प्रकारका कार्यहरूको लागि स्वचालित रूपमा सबैभन्दा उपयुक्त AI मोडेल चयन गर्दछ, Microsoft Foundry Local सँग \"उपकरणको रूपमा मोडेलहरू\" दृष्टिकोणलाई देखाउँदै।\n",
    "\n",
    "## अवलोकन\n",
    "\n",
    "मोडेल राउटरले बौद्धिक कार्य वर्गीकरण र मोडेल चयन कार्यान्वयन गर्दछ:\n",
    "\n",
    "- 🎯 **कार्य वर्गीकरण**: प्रयोगकर्ताको प्रश्नलाई स्वचालित रूपमा वर्गीकृत गर्दछ\n",
    "- 🔧 **मोडेल चयन**: कार्यलाई विशेष मोडेलहरूसँग मिलाउँछ\n",
    "- ⚙️ **डायनामिक कन्फिगरेसन**: वातावरण-आधारित उपकरण रजिस्ट्री\n",
    "- 📊 **स्वास्थ्य अनुगमन**: सेवा स्थिति र मोडेल उपलब्धता\n",
    "- 🚀 **उत्पादन तयार**: उद्यम-स्तरको राउटिङ र त्रुटि व्यवस्थापन\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00acb1",
   "metadata": {},
   "source": [
    "## वास्तुकला अवलोकन\n",
    "\n",
    "```\n",
    "User Query → Task Classifier → Model Selector → Specialized Model → Response\n",
    "     ↓              ↓               ↓              ↓\n",
    "\"Fix this code\" → Code Task → qwen2.5-7b → Code Model → Fixed Code\n",
    "\"Why is...?\"   → Reasoning  → deepseek-r1 → Reasoning → Analysis\n",
    "\"Write story\"  → Creative   → phi-4-mini  → Creative → Story\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b102f",
   "metadata": {},
   "source": [
    "## आवश्यकताहरू र सेटअप\n",
    "\n",
    "उत्तम रुटिङको लागि, तपाईंले धेरै मोडेलहरू चलाइरहेको हुनुपर्छ। वातावरण सेटअप गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai foundry-local-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00eba5",
   "metadata": {},
   "source": [
    "## लाइब्रेरीहरू आयात गर्नुहोस् र कन्फिगरेसन\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e68b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "from openai import OpenAI\n",
    "\n",
    "try:\n",
    "    from foundry_local import FoundryLocalManager\n",
    "    FOUNDRY_SDK_AVAILABLE = True\n",
    "    print(\"✅ Foundry Local SDK is available\")\n",
    "except ImportError:\n",
    "    FOUNDRY_SDK_AVAILABLE = False\n",
    "    print(\"⚠️ Foundry Local SDK not available, will use manual configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c2b3",
   "metadata": {},
   "source": [
    "## मोडेल राउटर क्लास\n",
    "\n",
    "बुद्धिमानी मोडेल चयनको व्यवस्थापन गर्ने मुख्य राउटर:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRouter:\n",
    "    \"\"\"Intelligent model router that selects appropriate models for different task types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.base_url = None\n",
    "        self.tools = self._load_tool_registry()\n",
    "        self._initialize_client()\n",
    "    \n",
    "    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Load tool registry from environment or use defaults.\"\"\"\n",
    "        default_tools = {\n",
    "            \"general\": {\n",
    "                \"model\": \"phi-4-mini\",\n",
    "                \"notes\": \"Fast general-purpose chat and Q&A\",\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"model\": \"deepseek-r1-7b\",\n",
    "                \"notes\": \"Step-by-step analysis and logical reasoning\",\n",
    "                \"temperature\": 0.3\n",
    "            },\n",
    "            \"code\": {\n",
    "                \"model\": \"qwen2.5-7b\",\n",
    "                \"notes\": \"Code generation, debugging, and technical tasks\",\n",
    "                \"temperature\": 0.2\n",
    "            },\n",
    "            \"creative\": {\n",
    "                \"model\": \"phi-4-mini\",\n",
    "                \"notes\": \"Creative writing and storytelling\",\n",
    "                \"temperature\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Override with environment variables\n",
    "        if os.environ.get(\"GENERAL_MODEL\"):\n",
    "            default_tools[\"general\"][\"model\"] = os.environ[\"GENERAL_MODEL\"]\n",
    "        if os.environ.get(\"REASONING_MODEL\"):\n",
    "            default_tools[\"reasoning\"][\"model\"] = os.environ[\"REASONING_MODEL\"]\n",
    "        if os.environ.get(\"CODE_MODEL\"):\n",
    "            default_tools[\"code\"][\"model\"] = os.environ[\"CODE_MODEL\"]\n",
    "        if os.environ.get(\"CREATIVE_MODEL\"):\n",
    "            default_tools[\"creative\"][\"model\"] = os.environ[\"CREATIVE_MODEL\"]\n",
    "        \n",
    "        # Check for complete JSON override\n",
    "        tools_env = os.environ.get(\"TOOL_REGISTRY\")\n",
    "        if tools_env:\n",
    "            try:\n",
    "                custom_tools = json.loads(tools_env)\n",
    "                return custom_tools\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"⚠️ Invalid TOOL_REGISTRY JSON, using defaults\")\n",
    "        \n",
    "        return default_tools\n",
    "    \n",
    "    def _discover_base_url(self, default: str = \"http://localhost:8000\") -> str:\n",
    "        \"\"\"Discover Foundry Local service URL.\"\"\"\n",
    "        env_url = os.environ.get(\"BASE_URL\")\n",
    "        if env_url:\n",
    "            return env_url\n",
    "        \n",
    "        try:\n",
    "            # Try to get URL from Foundry CLI\n",
    "            result = subprocess.run(\n",
    "                [\"foundry\", \"service\", \"status\"],\n",
    "                capture_output=True, text=True, timeout=3\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                match = re.search(r\"https?://[\\w\\.-]+(?::\\d+)?\", result.stdout)\n",
    "                if match:\n",
    "                    return match.group(0)\n",
    "        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):\n",
    "            pass\n",
    "        \n",
    "        return default\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize OpenAI client with Foundry Local or fallback configuration.\"\"\"\n",
    "        if FOUNDRY_SDK_AVAILABLE:\n",
    "            try:\n",
    "                # Try to use any available model for client initialization\n",
    "                first_model = next(iter(self.tools.values()))[\"model\"]\n",
    "                print(f\"🔄 Initializing Foundry Local SDK with model: {first_model}...\")\n",
    "                manager = FoundryLocalManager(first_model)\n",
    "                \n",
    "                self.client = OpenAI(\n",
    "                    base_url=manager.endpoint,\n",
    "                    api_key=manager.api_key\n",
    "                )\n",
    "                self.base_url = manager.endpoint\n",
    "                print(f\"✅ Foundry Local SDK initialized\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not use Foundry SDK ({e}), falling back to manual configuration\")\n",
    "        \n",
    "        # Fallback to manual configuration\n",
    "        self.base_url = self._discover_base_url()\n",
    "        api_key = os.environ.get(\"API_KEY\", \"\")\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "        print(f\"🔧 Manual configuration initialized at {self.base_url}\")\n",
    "    \n",
    "    def check_service_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check Foundry Local service health and available models.\"\"\"\n",
    "        try:\n",
    "            # Try to list models\n",
    "            models_response = self.client.models.list()\n",
    "            available_models = [model.id for model in models_response.data]\n",
    "            \n",
    "            # Check which configured models are available\n",
    "            configured_models = [tool[\"model\"] for tool in self.tools.values()]\n",
    "            available_configured = [m for m in configured_models if m in available_models]\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"base_url\": self.base_url,\n",
    "                \"available_models\": available_models,\n",
    "                \"configured_models\": configured_models,\n",
    "                \"available_configured\": available_configured,\n",
    "                \"tools_configured\": list(self.tools.keys())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"base_url\": self.base_url,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def select_tool(self, user_query: str) -> str:\n",
    "        \"\"\"Select the most appropriate tool based on the user query.\"\"\"\n",
    "        query_lower = user_query.lower()\n",
    "        \n",
    "        # Code-related keywords\n",
    "        code_keywords = [\n",
    "            \"code\", \"python\", \"function\", \"class\", \"method\", \"bug\", \"debug\", \n",
    "            \"programming\", \"script\", \"algorithm\", \"implementation\", \"refactor\",\n",
    "            \"syntax\", \"compile\", \"error\", \"exception\", \"variable\", \"loop\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in code_keywords):\n",
    "            return \"code\"\n",
    "        \n",
    "        # Reasoning keywords\n",
    "        reasoning_keywords = [\n",
    "            \"why\", \"how\", \"explain\", \"step-by-step\", \"reason\", \"analyze\", \n",
    "            \"think\", \"logic\", \"because\", \"cause\", \"compare\", \"evaluate\",\n",
    "            \"pros and cons\", \"advantage\", \"disadvantage\", \"benefit\", \"drawback\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in reasoning_keywords):\n",
    "            return \"reasoning\"\n",
    "        \n",
    "        # Creative keywords\n",
    "        creative_keywords = [\n",
    "            \"story\", \"poem\", \"creative\", \"imagine\", \"write\", \"tale\", \n",
    "            \"narrative\", \"fiction\", \"character\", \"plot\", \"novel\", \"poetry\",\n",
    "            \"song\", \"lyrics\", \"dialogue\", \"script\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in creative_keywords):\n",
    "            return \"creative\"\n",
    "        \n",
    "        # Default to general\n",
    "        return \"general\"\n",
    "    \n",
    "    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:\n",
    "        \"\"\"Send chat completion request to the specified model.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "            \n",
    "            if temperature is not None:\n",
    "                params[\"temperature\"] = temperature\n",
    "            \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response with model {model}: {str(e)}\"\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route the prompt to the appropriate model and generate response.\"\"\"\n",
    "        tool_key = self.select_tool(prompt)\n",
    "        tool_config = self.tools[tool_key]\n",
    "        model = tool_config[\"model\"]\n",
    "        temperature = tool_config.get(\"temperature\", 0.7)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        answer = self.chat(\n",
    "            model=model, \n",
    "            content=prompt, \n",
    "            max_tokens=400, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"tool\": tool_key,\n",
    "            \"model\": model,\n",
    "            \"tool_description\": tool_config[\"notes\"],\n",
    "            \"temperature\": temperature,\n",
    "            \"processing_time\": end_time - start_time,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "# Initialize the router\n",
    "print(\"Initializing Model Router...\")\n",
    "print(\"=\" * 50)\n",
    "router = ModelRouter()\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Model Router initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7dfc2",
   "metadata": {},
   "source": [
    "## सेवा स्वास्थ्य जाँच\n",
    "\n",
    "हामी हाम्रो Foundry Local सेवाको स्वास्थ्य जाँच गरौं र कुन मोडेलहरू उपलब्ध छन् हेर्नुहोस्:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de120b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service health and available models\n",
    "health = router.check_service_health()\n",
    "\n",
    "print(\"🏥 **Service Health Check**\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"🚦 Status: {health['status']}\")\n",
    "print(f\"🔗 Base URL: {health['base_url']}\")\n",
    "\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"\\n📋 **Available Models:**\")\n",
    "    for i, model in enumerate(health['available_models'], 1):\n",
    "        print(f\"   {i}. {model}\")\n",
    "    \n",
    "    print(f\"\\n🎯 **Configured Models:**\")\n",
    "    for tool, config in router.tools.items():\n",
    "        model = config['model']\n",
    "        status = \"✅ Available\" if model in health['available_models'] else \"❌ Not Available\"\n",
    "        print(f\"   {tool.title()}: {model} - {status}\")\n",
    "    \n",
    "    available_tools = len(health['available_configured'])\n",
    "    total_tools = len(health['configured_models'])\n",
    "    print(f\"\\n📊 **Tools Ready:** {available_tools}/{total_tools}\")\n",
    "    \n",
    "    if available_tools < total_tools:\n",
    "        missing_models = set(health['configured_models']) - set(health['available_configured'])\n",
    "        print(f\"⚠️ **Missing Models:** {', '.join(missing_models)}\")\n",
    "        print(\"💡 To start missing models, run: foundry model run <model-name>\")\n",
    "else:\n",
    "    print(f\"❌ **Error:** {health.get('error', 'Unknown error')}\")\n",
    "    print(\"\\n🔧 **Troubleshooting:**\")\n",
    "    print(\"1. Ensure Foundry Local is running: foundry service status\")\n",
    "    print(\"2. Start a model: foundry model run phi-4-mini\")\n",
    "    print(\"3. Check the endpoint URL is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d58e64",
   "metadata": {},
   "source": [
    "## वर्गीकरण परीक्षण\n",
    "\n",
    "राउटरले विभिन्न प्रकारका प्रश्नहरू वर्गीकरण गर्न सक्ने क्षमता परीक्षण गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(test_queries: List[str]):\n",
    "    \"\"\"Test the classification system with various queries.\"\"\"\n",
    "    print(\"🔍 **Classification Testing**\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        tool = router.select_tool(query)\n",
    "        tool_config = router.tools[tool]\n",
    "        print(f\"\\n{i}. **Query:** {query}\")\n",
    "        print(f\"   🎯 **Classified as:** {tool.title()}\")\n",
    "        print(f\"   🤖 **Model:** {tool_config['model']}\")\n",
    "        print(f\"   🌡️ **Temperature:** {tool_config['temperature']}\")\n",
    "        print(f\"   📝 **Purpose:** {tool_config['notes']}\")\n",
    "\n",
    "# Test classification with diverse queries\n",
    "test_queries = [\n",
    "    # General queries\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Tell me about Microsoft Foundry Local\",\n",
    "    \n",
    "    # Code queries\n",
    "    \"Write a Python function to sort a list\",\n",
    "    \"Fix this bug in my JavaScript code\",\n",
    "    \"How do I implement a binary search algorithm?\",\n",
    "    \n",
    "    # Reasoning queries\n",
    "    \"Why is edge AI becoming important?\",\n",
    "    \"Explain step-by-step how neural networks work\",\n",
    "    \"Compare the pros and cons of local vs cloud inference\",\n",
    "    \n",
    "    # Creative queries\n",
    "    \"Write a short story about robots\",\n",
    "    \"Create a poem about technology\",\n",
    "    \"Write dialogue for a sci-fi movie\"\n",
    "]\n",
    "\n",
    "test_classification(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a911381",
   "metadata": {},
   "source": [
    "## लाइभ राउटिङ उदाहरणहरू\n",
    "\n",
    "अब वास्तविक प्रतिक्रियाहरूको साथ राउटरलाई क्रियाशील अवस्थामा हेर्नुहोस्:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb51e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_routing(example_queries: List[str]):\n",
    "    \"\"\"Demonstrate live routing with actual model responses.\"\"\"\n",
    "    print(\"🚀 **Live Routing Demonstration**\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, query in enumerate(example_queries, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Example {i}: {query}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Route and get response\n",
    "        result = router.route_and_run(query)\n",
    "        \n",
    "        # Display routing information\n",
    "        print(f\"🎯 **Tool Selected:** {result['tool'].title()}\")\n",
    "        print(f\"🤖 **Model Used:** {result['model']}\")\n",
    "        print(f\"🌡️ **Temperature:** {result['temperature']}\")\n",
    "        print(f\"⏱️ **Processing Time:** {result['processing_time']:.2f}s\")\n",
    "        print(f\"📝 **Tool Purpose:** {result['tool_description']}\")\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"\\n💬 **Response:**\")\n",
    "        print(result['answer'])\n",
    "\n",
    "# Example queries for live demonstration\n",
    "demo_queries = [\n",
    "    \"What are the main benefits of running AI models locally?\",\n",
    "    \"Write a Python function to calculate fibonacci numbers\",\n",
    "    \"Explain step-by-step why local AI inference is faster than cloud\",\n",
    "    \"Write a creative story about an AI assistant living on an edge device\"\n",
    "]\n",
    "\n",
    "demonstrate_routing(demo_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562549ee",
   "metadata": {},
   "source": [
    "## प्रदर्शन तुलना\n",
    "\n",
    "आउनुहोस्, एउटै कार्यमा विभिन्न मोडेलहरूले कसरी प्रदर्शन गर्छन् भनेर तुलना गरौं:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_performance(test_prompt: str, max_tokens: int = 150):\n",
    "    \"\"\"Compare how different models handle the same prompt.\"\"\"\n",
    "    print(f\"⚖️ **Model Performance Comparison**\")\n",
    "    print(f\"📝 **Test Prompt:** {test_prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test with each configured model\n",
    "    for tool_name, tool_config in router.tools.items():\n",
    "        model = tool_config['model']\n",
    "        temperature = tool_config['temperature']\n",
    "        \n",
    "        print(f\"\\n🔧 **{tool_name.title()} Tool ({model})**\")\n",
    "        print(f\"   🌡️ Temperature: {temperature}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = router.chat(\n",
    "            model=model, \n",
    "            content=test_prompt, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"⏱️ **Time:** {processing_time:.2f}s\")\n",
    "        print(f\"💬 **Response:** {response}\")\n",
    "        \n",
    "        # Calculate response length\n",
    "        response_length = len(response.split())\n",
    "        print(f\"📏 **Length:** {response_length} words\")\n",
    "        \n",
    "        if \"Error\" in response:\n",
    "            print(\"❌ **Status:** Error\")\n",
    "        else:\n",
    "            print(\"✅ **Status:** Success\")\n",
    "\n",
    "# Test with a neutral prompt that could work for any model\n",
    "comparison_prompt = \"Explain the concept of edge computing in simple terms.\"\n",
    "compare_model_performance(comparison_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5bb7c",
   "metadata": {},
   "source": [
    "## अन्तरक्रियात्मक राउटर परीक्षण\n",
    "\n",
    "आफ्नै अनुकूलित प्रश्नहरू प्रयोग गरेर राउटर परीक्षण गर्नुहोस्:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_test(custom_query: str, force_tool: str = None):\n",
    "    \"\"\"Test the router with a custom query, optionally forcing a specific tool.\"\"\"\n",
    "    print(f\"🎪 **Interactive Router Test**\")\n",
    "    print(f\"💭 **Your Query:** {custom_query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if force_tool:\n",
    "        if force_tool not in router.tools:\n",
    "            print(f\"❌ Invalid tool: {force_tool}. Available: {list(router.tools.keys())}\")\n",
    "            return\n",
    "        selected_tool = force_tool\n",
    "        print(f\"🔧 **Forced Tool:** {selected_tool.title()}\")\n",
    "    else:\n",
    "        selected_tool = router.select_tool(custom_query)\n",
    "        print(f\"🎯 **Auto-Selected Tool:** {selected_tool.title()}\")\n",
    "    \n",
    "    # Get full result\n",
    "    if force_tool:\n",
    "        # Manual routing\n",
    "        tool_config = router.tools[force_tool]\n",
    "        model = tool_config['model']\n",
    "        temperature = tool_config['temperature']\n",
    "        \n",
    "        start_time = time.time()\n",
    "        answer = router.chat(model, custom_query, temperature=temperature)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'tool': force_tool,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'processing_time': end_time - start_time,\n",
    "            'answer': answer\n",
    "        }\n",
    "    else:\n",
    "        # Automatic routing\n",
    "        result = router.route_and_run(custom_query)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n📊 **Routing Details:**\")\n",
    "    print(f\"   🔧 Tool: {result['tool'].title()}\")\n",
    "    print(f\"   🤖 Model: {result['model']}\")\n",
    "    print(f\"   🌡️ Temperature: {result['temperature']}\")\n",
    "    print(f\"   ⏱️ Time: {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n💬 **Response:**\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with your custom query\n",
    "custom_query = \"How can I optimize my Python code for better performance?\"\n",
    "# Uncomment the next line to force a specific tool\n",
    "# force_tool = \"code\"  # Options: \"general\", \"reasoning\", \"code\", \"creative\"\n",
    "force_tool = None\n",
    "\n",
    "result = interactive_test(custom_query, force_tool=force_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24aa28",
   "metadata": {},
   "source": [
    "## उन्नत कन्फिगरेसन\n",
    "\n",
    "राउटर कन्फिगरेसनलाई कसरी अनुकूलित गर्ने देखाउनुहोस्:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelRouter(ModelRouter):\n",
    "    \"\"\"Extended router with custom configuration and additional features.\"\"\"\n",
    "    \n",
    "    def __init__(self, custom_tools: Dict[str, Dict[str, Any]] = None):\n",
    "        self.custom_tools = custom_tools\n",
    "        super().__init__()\n",
    "    \n",
    "    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Load custom tool registry if provided.\"\"\"\n",
    "        if self.custom_tools:\n",
    "            return self.custom_tools\n",
    "        return super()._load_tool_registry()\n",
    "    \n",
    "    def add_custom_classifier(self, tool_name: str, keywords: List[str]):\n",
    "        \"\"\"Add a custom classification rule.\"\"\"\n",
    "        self.custom_keywords = getattr(self, 'custom_keywords', {})\n",
    "        self.custom_keywords[tool_name] = keywords\n",
    "    \n",
    "    def select_tool(self, user_query: str) -> str:\n",
    "        \"\"\"Enhanced tool selection with custom rules.\"\"\"\n",
    "        # Check custom keywords first\n",
    "        if hasattr(self, 'custom_keywords'):\n",
    "            query_lower = user_query.lower()\n",
    "            for tool_name, keywords in self.custom_keywords.items():\n",
    "                if any(keyword in query_lower for keyword in keywords):\n",
    "                    return tool_name\n",
    "        \n",
    "        # Fall back to default classification\n",
    "        return super().select_tool(user_query)\n",
    "    \n",
    "    def get_tool_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about tool usage.\"\"\"\n",
    "        if not hasattr(self, 'usage_stats'):\n",
    "            self.usage_stats = {tool: 0 for tool in self.tools.keys()}\n",
    "            self.total_requests = 0\n",
    "        \n",
    "        return {\n",
    "            'usage_by_tool': self.usage_stats.copy(),\n",
    "            'total_requests': self.total_requests,\n",
    "            'most_used_tool': max(self.usage_stats, key=self.usage_stats.get) if self.usage_stats else None\n",
    "        }\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced routing with usage tracking.\"\"\"\n",
    "        result = super().route_and_run(prompt)\n",
    "        \n",
    "        # Track usage\n",
    "        if not hasattr(self, 'usage_stats'):\n",
    "            self.usage_stats = {tool: 0 for tool in self.tools.keys()}\n",
    "            self.total_requests = 0\n",
    "        \n",
    "        self.usage_stats[result['tool']] += 1\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example: Create a router with custom configuration\n",
    "custom_config = {\n",
    "    \"general\": {\n",
    "        \"model\": \"phi-4-mini\",\n",
    "        \"notes\": \"General purpose model\",\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    \"technical\": {\n",
    "        \"model\": \"qwen2.5-7b\",\n",
    "        \"notes\": \"Technical documentation and analysis\",\n",
    "        \"temperature\": 0.3\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"model\": \"phi-4-mini\",\n",
    "        \"notes\": \"Creative writing and storytelling\",\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🔧 **Creating Custom Router Configuration**\")\n",
    "custom_router = CustomModelRouter(custom_tools=custom_config)\n",
    "\n",
    "# Add custom classification rules\n",
    "custom_router.add_custom_classifier(\"technical\", [\"documentation\", \"specification\", \"architecture\", \"design\"])\n",
    "\n",
    "print(\"✅ Custom router created with enhanced features\")\n",
    "print(f\"🎯 Available tools: {list(custom_router.tools.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a692983",
   "metadata": {},
   "source": [
    "## ब्याच प्रशोधन उदाहरण\n",
    "\n",
    "धेरै प्रश्नहरूलाई प्रभावकारी रूपमा प्रशोधन गर्नुहोस्:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_queries(queries: List[str], router_instance: ModelRouter = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple queries in batch and analyze results.\"\"\"\n",
    "    if router_instance is None:\n",
    "        router_instance = router\n",
    "    \n",
    "    print(f\"📦 **Batch Processing {len(queries)} Queries**\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    tool_usage = {}\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n{i}. Processing: {query[:50]}{'...' if len(query) > 50 else ''}\")\n",
    "        \n",
    "        result = router_instance.route_and_run(query)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Track statistics\n",
    "        total_time += result['processing_time']\n",
    "        tool = result['tool']\n",
    "        tool_usage[tool] = tool_usage.get(tool, 0) + 1\n",
    "        \n",
    "        print(f\"   🎯 Tool: {tool.title()} | ⏱️ {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n📊 **Batch Processing Summary**\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"📝 Total Queries: {len(queries)}\")\n",
    "    print(f\"⏱️ Total Time: {total_time:.2f}s\")\n",
    "    print(f\"🚀 Average Time: {total_time/len(queries):.2f}s per query\")\n",
    "    \n",
    "    print(f\"\\n🎯 **Tool Usage:**\")\n",
    "    for tool, count in sorted(tool_usage.items()):\n",
    "        percentage = (count / len(queries)) * 100\n",
    "        print(f\"   {tool.title()}: {count} queries ({percentage:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Batch test queries\n",
    "batch_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a Python function for quicksort\",\n",
    "    \"Why is data privacy important in AI?\",\n",
    "    \"Create a haiku about artificial intelligence\",\n",
    "    \"How do I debug a memory leak in my application?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "    \"Write a short story about a robot chef\",\n",
    "    \"What are the advantages of edge computing?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_process_queries(batch_queries)\n",
    "\n",
    "# Show detailed results for first few queries\n",
    "print(f\"\\n📋 **Sample Detailed Results:**\")\n",
    "for i, result in enumerate(batch_results[:3], 1):\n",
    "    print(f\"\\n{i}. **Tool:** {result['tool'].title()} | **Model:** {result['model']}\")\n",
    "    print(f\"   **Response:** {result['answer'][:100]}{'...' if len(result['answer']) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5d741",
   "metadata": {},
   "source": [
    "## उत्पादन अनुगमन\n",
    "\n",
    "उत्पादनको लागि तयार अनुगमन र लगिङको उदाहरण:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eace241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionModelRouter(ModelRouter):\n",
    "    \"\"\"Production-ready router with comprehensive monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'total_processing_time': 0,\n",
    "            'tool_usage': {},\n",
    "            'model_performance': {},\n",
    "            'error_log': []\n",
    "        }\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced routing with comprehensive monitoring.\"\"\"\n",
    "        self.metrics['total_requests'] += 1\n",
    "        request_id = self.metrics['total_requests']\n",
    "        \n",
    "        try:\n",
    "            result = super().route_and_run(prompt)\n",
    "            \n",
    "            # Track success metrics\n",
    "            self.metrics['successful_requests'] += 1\n",
    "            self.metrics['total_processing_time'] += result['processing_time']\n",
    "            \n",
    "            # Track tool usage\n",
    "            tool = result['tool']\n",
    "            if tool not in self.metrics['tool_usage']:\n",
    "                self.metrics['tool_usage'][tool] = {'count': 0, 'total_time': 0}\n",
    "            self.metrics['tool_usage'][tool]['count'] += 1\n",
    "            self.metrics['tool_usage'][tool]['total_time'] += result['processing_time']\n",
    "            \n",
    "            # Track model performance\n",
    "            model = result['model']\n",
    "            if model not in self.metrics['model_performance']:\n",
    "                self.metrics['model_performance'][model] = {'requests': 0, 'total_time': 0, 'avg_time': 0}\n",
    "            self.metrics['model_performance'][model]['requests'] += 1\n",
    "            self.metrics['model_performance'][model]['total_time'] += result['processing_time']\n",
    "            self.metrics['model_performance'][model]['avg_time'] = (\n",
    "                self.metrics['model_performance'][model]['total_time'] / \n",
    "                self.metrics['model_performance'][model]['requests']\n",
    "            )\n",
    "            \n",
    "            # Add monitoring metadata\n",
    "            result['request_id'] = request_id\n",
    "            result['status'] = 'success'\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Track failure metrics\n",
    "            self.metrics['failed_requests'] += 1\n",
    "            error_entry = {\n",
    "                'request_id': request_id,\n",
    "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'prompt': prompt[:100],  # Truncate for privacy\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.metrics['error_log'].append(error_entry)\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'status': 'error',\n",
    "                'error': str(e),\n",
    "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "    \n",
    "    def get_performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        total_requests = self.metrics['total_requests']\n",
    "        if total_requests == 0:\n",
    "            return {'message': 'No requests processed yet'}\n",
    "        \n",
    "        success_rate = (self.metrics['successful_requests'] / total_requests) * 100\n",
    "        avg_processing_time = (\n",
    "            self.metrics['total_processing_time'] / max(1, self.metrics['successful_requests'])\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overview': {\n",
    "                'total_requests': total_requests,\n",
    "                'successful_requests': self.metrics['successful_requests'],\n",
    "                'failed_requests': self.metrics['failed_requests'],\n",
    "                'success_rate': f\"{success_rate:.1f}%\",\n",
    "                'average_processing_time': f\"{avg_processing_time:.2f}s\"\n",
    "            },\n",
    "            'tool_usage': self.metrics['tool_usage'],\n",
    "            'model_performance': self.metrics['model_performance'],\n",
    "            'recent_errors': self.metrics['error_log'][-5:] if self.metrics['error_log'] else []\n",
    "        }\n",
    "\n",
    "# Create production router and test\n",
    "print(\"🏭 **Production Router Testing**\")\n",
    "prod_router = ProductionModelRouter()\n",
    "\n",
    "# Process several requests\n",
    "test_requests = [\n",
    "    \"Explain quantum computing\",\n",
    "    \"Write a function to reverse a string\",\n",
    "    \"Why is cybersecurity important?\",\n",
    "    \"Create a poem about the future\"\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(test_requests)} test requests...\")\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    result = prod_router.route_and_run(request)\n",
    "    status = \"✅\" if result['status'] == 'success' else \"❌\"\n",
    "    print(f\"{i}. {status} Request {result['request_id']}: {request[:30]}...\")\n",
    "\n",
    "# Generate performance report\n",
    "report = prod_router.get_performance_report()\n",
    "\n",
    "print(f\"\\n📊 **Performance Report**\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📈 **Overview:**\")\n",
    "for key, value in report['overview'].items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\n🎯 **Tool Usage:**\")\n",
    "for tool, stats in report['tool_usage'].items():\n",
    "    avg_time = stats['total_time'] / stats['count']\n",
    "    print(f\"   {tool.title()}: {stats['count']} requests (avg: {avg_time:.2f}s)\")\n",
    "\n",
    "print(f\"\\n🤖 **Model Performance:**\")\n",
    "for model, stats in report['model_performance'].items():\n",
    "    print(f\"   {model}: {stats['requests']} requests (avg: {stats['avg_time']:.2f}s)\")\n",
    "\n",
    "if report['recent_errors']:\n",
    "    print(f\"\\n❌ **Recent Errors:**\")\n",
    "    for error in report['recent_errors']:\n",
    "        print(f\"   Request {error['request_id']}: {error['error']}\")\n",
    "else:\n",
    "    print(f\"\\n✅ **No Recent Errors**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894acf23",
   "metadata": {},
   "source": [
    "## सारांश र उत्कृष्ट अभ्यासहरू\n",
    "\n",
    "यस नोटबुकले एक उन्नत बौद्धिक मोडेल राउटिङ प्रणाली प्रदर्शन गरेको छ:\n",
    "\n",
    "### ✅ प्रदर्शन गरिएका मुख्य विशेषताहरू\n",
    "\n",
    "1. **🎯 बौद्धिक वर्गीकरण**: कीवर्ड विश्लेषण प्रयोग गरेर स्वचालित कार्य वर्गीकरण\n",
    "2. **🔧 गतिशील मोडेल चयन**: विशेष मोडेलहरूमा प्रश्नहरू राउटिङ\n",
    "3. **⚙️ लचिलो कन्फिगरेसन**: वातावरण-आधारित र अनुकूलित उपकरण रजिस्ट्रीहरू\n",
    "4. **📊 स्वास्थ्य अनुगमन**: सेवा र मोडेल उपलब्धता जाँच\n",
    "5. **⚡ प्रदर्शन ट्र्याकिङ**: प्रतिक्रिया समय र प्रयोग विश्लेषण\n",
    "6. **🏭 उत्पादन तयार**: व्यापक अनुगमन र त्रुटि व्यवस्थापन\n",
    "\n",
    "### 🎨 मोडेल वर्गहरूको सारांश\n",
    "\n",
    "| वर्ग | डिफल्ट मोडेल | तापमान | उपयुक्त कार्यहरू |\n",
    "|------|--------------|---------|------------------|\n",
    "| **🌐 सामान्य** | phi-4-mini | 0.7 | प्रश्न उत्तर, च्याट, सामान्य कार्यहरू |\n",
    "| **🧠 तर्क** | deepseek-r1-7b | 0.3 | विश्लेषण, व्याख्या |\n",
    "| **💻 कोड** | qwen2.5-7b | 0.2 | प्रोग्रामिङ, डिबगिङ |\n",
    "| **🎨 सिर्जनात्मक** | phi-4-mini | 0.9 | कथाहरू, कविताहरू, सिर्जनात्मक लेखन |\n",
    "\n",
    "### 🔍 वर्गीकरण नियमहरू\n",
    "\n",
    "- **कोड पहिचान**: `code`, `python`, `function`, `class`, `bug`, `debug`, `programming`\n",
    "- **तर्क पहिचान**: `why`, `how`, `explain`, `step-by-step`, `reason`, `analyze`\n",
    "- **सिर्जनात्मक पहिचान**: `story`, `poem`, `creative`, `imagine`, `write`, `tale`\n",
    "- **डिफल्ट**: अन्य सबै प्रश्नहरू सामान्य मोडेल प्रयोग गर्छन्\n",
    "\n",
    "### 💡 उत्कृष्ट अभ्यासहरू\n",
    "\n",
    "1. **🎯 मोडेल विशेषज्ञता**: मोडेलहरूको बलमा आधारित प्रयोग गर्नुहोस्\n",
    "2. **🌡️ तापमान ट्युनिङ**: सटीकताका लागि कम, सिर्जनात्मकताका लागि उच्च\n",
    "3. **📊 निरन्तर अनुगमन**: प्रदर्शन र प्रयोगका ढाँचाहरू ट्र्याक गर्नुहोस्\n",
    "4. **🔄 फलब्याक रणनीतिहरू**: मोडेलहरू उपलब्ध नभएको अवस्थामा सहज डिग्रेडेसन\n",
    "5. **⚖️ लोड ब्यालेन्सिङ**: धेरै इन्स्ट्यान्सहरूमा लोड वितरण गर्नुहोस्\n",
    "6. **🔧 अनुकूलित वर्गीकरण**: डोमेन-विशिष्ट राउटिङ नियमहरू थप्नुहोस्\n",
    "\n",
    "### 🚀 प्रयोगका क्षेत्रहरू\n",
    "\n",
    "- **👨‍💻 विकास उपकरणहरू**: बौद्धिक मोडेल राउटिङसहितको IDE प्लगइनहरू\n",
    "- **🎧 ग्राहक समर्थन**: विशेष समर्थन मोडेलहरूमा प्रश्नहरू राउट गर्नुहोस्\n",
    "- **📝 सामग्री सिर्जना**: उपयुक्त सिर्जनात्मक मोडेलहरूमा कार्यहरू मिलाउनुहोस्\n",
    "- **📚 दस्तावेजीकरण**: प्राविधिक लेखन मोडेलहरूमा राउट गर्नुहोस्\n",
    "- **🎓 शैक्षिक प्लेटफर्महरू**: विषय-विशिष्ट मोडेल राउटिङ\n",
    "\n",
    "### 🔮 आगामी कदमहरू\n",
    "\n",
    "- **🤖 मेसिन लर्निङ**: राम्रो वर्गीकरणका लागि ML प्रयोग गर्नुहोस्\n",
    "- **🔧 फङ्सन कलिङ**: विशेष उपकरण क्षमतासहितका मोडेलहरूमा राउट गर्नुहोस्\n",
    "- **📸 मल्टि-मोडल**: इनपुट प्रकार (पाठ, छवि, अडियो) मा आधारित राउट गर्नुहोस्\n",
    "- **🌐 फेडरेटेड राउटिङ**: धेरै Foundry Local इन्स्ट्यान्सहरूमा राउट गर्नुहोस्\n",
    "- **💰 लागत अनुकूलन**: लागत र प्रदर्शन मेट्रिक्समा आधारित राउट गर्नुहोस्\n",
    "\n",
    "यो बौद्धिक राउटिङ प्रणालीले विभिन्न विशेष मोडेलहरूको मूल्य अधिकतम बनाउने तरिका प्रदर्शन गर्दछ, स्थानीय इनफरेन्सको गोपनीयता र प्रदर्शन लाभहरू कायम राख्दै। \"मोडेलहरू उपकरणको रूपमा\" दृष्टिकोणले प्रत्येक विशिष्ट कार्यका लागि उत्कृष्ट मोडेल स्वचालित रूपमा चयन गर्ने उन्नत AI अनुप्रयोगहरू निर्माण गर्न सक्षम बनाउँछ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी यथार्थताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "244af75414f6f2705d84855b890df6e9",
   "translation_date": "2025-09-30T22:31:33+00:00",
   "source_file": "Module08/samples/06/model_router.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}