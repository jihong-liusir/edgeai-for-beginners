<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T17:46:22+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ne"
}
-->
# सत्र ४: अत्याधुनिक मोडेलहरू – LLMs, SLMs, र उपकरणमा इन्फरेन्स

## अवलोकन

LLMs र SLMs तुलना गर्नुहोस्, स्थानीय बनाम क्लाउड इन्फरेन्सको व्यापार-सम्बन्धी पक्षहरू मूल्याङ्कन गर्नुहोस्, र Phi र ONNX Runtime प्रयोग गरेर EdgeAI परिदृश्यहरू प्रदर्शन गर्ने डेमोहरू कार्यान्वयन गर्नुहोस्। हामी Chainlit RAG, WebGPU इन्फरेन्स विकल्पहरू, र Open WebUI एकीकरणलाई पनि उजागर गर्नेछौं।

सन्दर्भहरू:
- Foundry Local दस्तावेज: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI कसरी गर्ने (Open WebUI संग च्याट एप): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## सिकाइ उद्देश्यहरू
- लागत, विलम्बता, र शुद्धताका लागि LLM बनाम SLM व्यापार-सम्बन्धी पक्षहरू बुझ्नुहोस्
- विशिष्ट व्यापार आवश्यकताहरूका लागि स्थानीय र क्लाउड इन्फरेन्स बीच चयन गर्नुहोस्
- Chainlit प्रयोग गरेर सानो RAG डेमो कार्यान्वयन गर्नुहोस्
- ब्राउजर-साइड एक्सेलेरेशनका लागि WebGPU अन्वेषण गर्नुहोस्
- Open WebUI लाई Foundry Local संग जडान गर्नुहोस्

## भाग १: LLM बनाम SLM – निर्णय म्याट्रिक्स

विचार गर्नुहोस्:
- विलम्बता: SLMs उपकरणमा अक्सर उप-सेकेन्ड प्रतिक्रिया दिन्छन्
- लागत: स्थानीय इन्फरेन्सले क्लाउड लागत घटाउँछ
- गोपनीयता: संवेदनशील डेटा उपकरणमै रहन्छ
- क्षमता: जटिल कार्यहरूमा LLMs SLMs भन्दा राम्रो हुन सक्छ
- विश्वसनीयता: हाइब्रिड रणनीतिले डाउनटाइम जोखिम कम गर्छ

## भाग २: स्थानीय बनाम क्लाउड – हाइब्रिड ढाँचाहरू

- ठूलो/जटिल प्रम्प्टहरूको लागि क्लाउड फलब्याक सहित स्थानीय-प्रथम
- गोपनीयता-संवेदनशील वा अफलाइन परिदृश्यहरूको लागि स्थानीय सहित क्लाउड-प्रथम
- कार्य प्रकारद्वारा मार्ग (कोड-जेनलाई DeepSeek मा, सामान्य च्याटलाई Phi/Qwen मा)

## भाग ३: Chainlit प्रयोग गरेर RAG च्याट एप (न्यूनतम)

आवश्यकताहरू स्थापना गर्नुहोस्:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

चलाउनुहोस्:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

विस्तार गर्नुहोस्: एक साधारण रिट्रिभर (स्थानीय फाइलहरू) थप्नुहोस् र प्रयोगकर्ता प्रम्प्टमा पुनःप्राप्त गरिएको सन्दर्भलाई अगाडि राख्नुहोस्।

## भाग ४: WebGPU इन्फरेन्स (सावधानी)

WebGPU प्रयोग गरेर साना मोडेलहरू ब्राउजरमै चलाउनुहोस्। यो गोपनीयता-प्रथम डेमोहरू र शून्य-स्थापना अनुभवहरूको लागि आदर्श हो। तल ONNX Runtime Web प्रयोग गरेर WebGPU कार्यान्वयन प्रदायकको साथ चरण-दर-चरण न्यूनतम उदाहरण छ।

१) WebGPU समर्थन जाँच गर्नुहोस्
- Chromium ब्राउजरहरू: chrome://gpu → “WebGPU” सक्षम भएको पुष्टि गर्नुहोस्
- प्रोग्रामेटिक जाँच (हामी कोडमा पनि जाँच गर्नेछौं): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

२) न्यूनतम प्रोजेक्ट बनाउनुहोस्
एक फोल्डर र दुई फाइलहरू बनाउनुहोस्: `index.html` र `main.js`।

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

३) स्थानीय रूपमा सेवा गर्नुहोस् (Windows cmd.exe)
ब्राउजरले मोडेल फेच गर्न सक्ने सरल स्थिर सर्भर प्रयोग गर्नुहोस्।

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

तपाईंको ब्राउजरमा http://localhost:5173 खोल्नुहोस्। तपाईंले इनिसियलाइजेसन लगहरू, WebGPU संग सत्र सिर्जना, र एक argmax भविष्यवाणी देख्नु पर्छ।

४) समस्या समाधान
- यदि WebGPU उपलब्ध छैन: Chrome/Edge अपडेट गर्नुहोस् र GPU ड्राइभरहरू हालको सुनिश्चित गर्नुहोस्, त्यसपछि chrome://flags मा “Enable WebGPU” जाँच गर्नुहोस्।
- यदि CORS वा फेच त्रुटिहरू हुन्छन्: सुनिश्चित गर्नुहोस् कि तपाईं फाइलहरू http:// (file:// होइन) मा सेवा गर्नुहुन्छ र मोडेल URL ले क्रस-ओरिजिन अनुरोधहरू अनुमति दिन्छ।
- CPU मा फलब्याक: `executionProviders: ['wasm']` परिवर्तन गरेर आधारभूत व्यवहार पुष्टि गर्नुहोस्।

५) अर्को चरणहरू
- डोमेन-विशिष्ट ONNX मोडेल (जस्तै, छवि वर्गीकरण वा सानो पाठ मोडेल) स्वाप गर्नुहोस्।
- वास्तविक इनपुटहरूको लागि प्रि-प्रोसेसिङ/पोस्ट-प्रोसेसिङ तर्क थप्नुहोस्।
- ठूला मोडेलहरू वा उत्पादन विलम्बताका लागि, Foundry Local वा ONNX Runtime Server प्राथमिकता दिनुहोस्।

## भाग ५: Open WebUI + Foundry Local (चरण-दर-चरण)

यो Open WebUI लाई Foundry Local को OpenAI-संगत अन्त बिन्दुमा जडान गर्दछ स्थानीय च्याट UI को लागि।

१) पूर्वापेक्षाहरू
- Foundry Local स्थापना गरिएको र काम गरिरहेको (`foundry --version`)
- एक मोडेल स्थानीय रूपमा चलाउन तयार (जस्तै, `phi-4-mini`)
- Docker Desktop स्थापना गरिएको (Open WebUI को लागि सिफारिस गरिएको)

२) Foundry Local संग मोडेल सुरु गर्नुहोस्
```powershell
foundry model run phi-4-mini
```
यसले `http://localhost:8000` मा OpenAI-संगत API उजागर गर्दछ।

३) Open WebUI सुरु गर्नुहोस् (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
नोटहरू:
- Windows मा, `host.docker.internal` ले कन्टेनरलाई तपाईंको होस्टमा `localhost` मा पुग्न दिन्छ।
- हामीले `OPENAI_API_BASE_URL` लाई Foundry Local को अन्त बिन्दुमा सेट गरेका छौं र एक डमी `OPENAI_API_KEY`।

४) Open WebUI UI बाट कन्फिगर गर्नुहोस् (वैकल्पिक)
- http://localhost:3000 मा ब्राउज गर्नुहोस्
- प्रारम्भिक सेटअप पूरा गर्नुहोस् (प्रशासन प्रयोगकर्ता)
- सेटिङ्स → मोडेलहरू/प्रदायकहरूमा जानुहोस्
- Base URL सेट गर्नुहोस्: `http://host.docker.internal:8000/v1`
- API Key सेट गर्नुहोस्: `local-key` (प्लेसहोल्डर)
- बचत गर्नुहोस्

५) परीक्षण प्रम्प्ट चलाउनुहोस्
- Open WebUI च्याटमा, मोडेल नाम `phi-4-mini` चयन गर्नुहोस् वा प्रविष्ट गर्नुहोस्
- प्रम्प्ट: “उपकरणमा AI इन्फरेन्सका पाँच फाइदाहरू सूचीबद्ध गर्नुहोस्।”
- तपाईंले आफ्नो स्थानीय मोडेलबाट स्ट्रिम गरिएको प्रतिक्रिया देख्नु पर्छ

६) समस्या समाधान
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

७) वैकल्पिक: Open WebUI डेटा कायम राख्नुहोस्
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## व्यावहारिक चेकलिस्ट
- [ ] स्थानीय रूपमा SLM र LLM बीचको प्रतिक्रिया/विलम्बता तुलना गर्नुहोस्
- [ ] Chainlit डेमो कम्तीमा दुई मोडेलहरू विरुद्ध चलाउनुहोस्
- [ ] Open WebUI लाई आफ्नो स्थानीय अन्त बिन्दुमा जडान गर्नुहोस् र परीक्षण गर्नुहोस्

## अर्को चरणहरू
- सत्र ५ मा एजेन्ट वर्कफ्लोहरूको तयारी गर्नुहोस्
- हाइब्रिड स्थानीय/क्लाउडले ROI सुधार गर्ने परिदृश्यहरू पहिचान गर्नुहोस्

---

