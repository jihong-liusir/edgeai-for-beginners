<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-24T15:27:43+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ne"
}
-->
# рд╕рддреНрд░ рек: Chainlit рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЙрддреНрдкрд╛рджрди рд╕реНрддрд░рдХреЛ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдирд┐рд░реНрдорд╛рдг

## рдЕрд╡рд▓реЛрдХрди

рдпреЛ рд╕рддреНрд░рд▓реЗ Chainlit рд░ Microsoft Foundry Local рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЙрддреНрдкрд╛рджрди рд╕реНрддрд░рдХреЛ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдирд┐рд░реНрдорд╛рдгрдорд╛ рдХреЗрдиреНрджреНрд░рд┐рдд рдЫред рддрдкрд╛рдИрдВрд▓реЗ рдПрдЖрдИ рд╕рдВрд╡рд╛рджрдХрд╛ рд▓рд╛рдЧрд┐ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм рдЗрдиреНрдЯрд░рдлреЗрд╕ рдирд┐рд░реНрдорд╛рдг рдЧрд░реНрди, рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрди, рд░ рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рддрдерд╛ рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рдбрд┐рдЬрд╛рдЗрдирд╕рд╣рд┐рдд рдмрд▓рд┐рдпреЛ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рддреИрдирд╛рдд рдЧрд░реНрди рд╕рд┐рдХреНрдиреБрд╣реБрдиреЗрдЫред

**рддрдкрд╛рдИрдВрд▓реЗ рдирд┐рд░реНрдорд╛рдг рдЧрд░реНрдиреЗ рдХреБрд░рд╛:**
- **Chainlit рдЪреНрдпрд╛рдЯ рдПрдк**: рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛рд╕рд╣рд┐рддрдХреЛ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм UI
- **WebGPU рдбреЗрдореЛ**: рдЧреЛрдкрдиреАрдпрддрд╛-рдкреНрд░рдердо рдПрдкреНрд▓рд┐рдХреЗрд╕рдирдХрд╛ рд▓рд╛рдЧрд┐ рдмреНрд░рд╛рдЙрдЬрд░-рдЖрдзрд╛рд░рд┐рдд рдЗрдирдлрд░реЗрдиреНрд╕  
- **Open WebUI рдПрдХреАрдХрд░рдг**: Foundry Local рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЪреНрдпрд╛рдЯ рдЗрдиреНрдЯрд░рдлреЗрд╕
- **рдЙрддреНрдкрд╛рджрди рдврд╛рдБрдЪрд╛**: рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди, рдирд┐рдЧрд░рд╛рдиреА, рд░ рддреИрдирд╛рддреА рд░рдгрдиреАрддрд┐рд╣рд░реВ

## рд╕рд┐рдХреНрдиреЗ рдЙрджреНрджреЗрд╢реНрдпрд╣рд░реВ

- Chainlit рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЙрддреНрдкрд╛рджрди рд╕реНрддрд░рдХреЛ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдирд┐рд░реНрдорд╛рдг рдЧрд░реНрдиреБрд╣реЛрд╕реН
- рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рд╕реБрдзрд╛рд░рдХрд╛ рд▓рд╛рдЧрд┐ рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН
- Foundry Local SDK рдПрдХреАрдХрд░рдг рдврд╛рдБрдЪрд╛рд╣рд░реВрдорд╛ рдорд╣рд╛рд░рдд рд╣рд╛рд╕рд┐рд▓ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- рдЙрдЪрд┐рдд рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рд░ рдЧреНрд░реЗрд╕рдлреБрд▓ рдбрд┐рдЧреНрд░реЗрдбреЗрд╕рди рд▓рд╛рдЧреВ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- рд╡рд┐рднрд┐рдиреНрди рд╡рд╛рддрд╛рд╡рд░рдгрдХрд╛ рд▓рд╛рдЧрд┐ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рддреИрдирд╛рдд рд░ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- рд╕рдВрд╡рд╛рджрд╛рддреНрдордХ рдПрдЖрдИрдХрд╛ рд▓рд╛рдЧрд┐ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм UI рдврд╛рдБрдЪрд╛рд╣рд░реВ рдмреБрдЭреНрдиреБрд╣реЛрд╕реН

## рдкреВрд░реНрд╡рд╢рд░реНрддрд╣рд░реВ

- **Foundry Local**: рд╕реНрдерд╛рдкрдирд╛ рдЧрд░рд┐рдПрдХреЛ рд░ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ ([рд╕реНрдерд╛рдкрдирд╛рдХреЛ рдорд╛рд░реНрдЧрджрд░реНрд╢рди](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: рей.резреж рд╡рд╛ рдкрдЫрд┐рд▓реНрд▓реЛ рд╕рдВрд╕реНрдХрд░рдг, рднрд░реНрдЪреБрдЕрд▓ рд╡рд╛рддрд╛рд╡рд░рдг рдХреНрд╖рдорддрд╛рд╕рд╣рд┐рдд
- **рдореЛрдбреЗрд▓**: рдХрдореНрддреАрдорд╛ рдПрдЙрдЯрд╛ рдореЛрдбреЗрд▓ рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ (`foundry model run phi-4-mini`)
- **рдмреНрд░рд╛рдЙрдЬрд░**: WebGPU рд╕рдорд░реНрдердирд╕рд╣рд┐рддрдХреЛ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм рдмреНрд░рд╛рдЙрдЬрд░ (Chrome/Edge)
- **Docker**: Open WebUI рдПрдХреАрдХрд░рдгрдХрд╛ рд▓рд╛рдЧрд┐ (рд╡реИрдХрд▓реНрдкрд┐рдХ)

## рднрд╛рдЧ рез: рдЖрдзреБрдирд┐рдХ рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рдирд╣рд░реВ рдмреБрдЭреНрджреИ

### рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЕрд╡рд▓реЛрдХрди

```
User Browser тЖРтЖТ Chainlit UI тЖРтЖТ Python Backend тЖРтЖТ Foundry Local тЖРтЖТ AI Model
      тЖУ              тЖУ              тЖУ              тЖУ            тЖУ
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### рдкреНрд░рдореБрдЦ рдкреНрд░рд╡рд┐рдзрд┐рд╣рд░реВ

**Foundry Local SDK рдврд╛рдБрдЪрд╛рд╣рд░реВ:**
- `FoundryLocalManager(alias)`: рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕реЗрд╡рд╛ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди
- `manager.endpoint` рд░ `manager.api_key`: рдЬрдбрд╛рди рд╡рд┐рд╡рд░рдгрд╣рд░реВ
- `manager.get_model_info(alias).id`: рдореЛрдбреЗрд▓ рдкрд╣рд┐рдЪрд╛рди

**Chainlit рдлреНрд░реЗрдорд╡рд░реНрдХ:**
- `@cl.on_chat_start`: рдЪреНрдпрд╛рдЯ рд╕рддреНрд░рд╣рд░реВ рдЖрд░рдореНрдн рдЧрд░реНрдиреБрд╣реЛрд╕реН
- `@cl.on_message`: рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛рдмрд╛рдЯ рдЖрдЙрдиреЗ рд╕рдиреНрджреЗрд╢рд╣рд░реВ рд╣реНрдпрд╛рдиреНрдбрд▓ рдЧрд░реНрдиреБрд╣реЛрд╕реН  
- `cl.Message().stream_token()`: рд╡рд╛рд╕реНрддрд╡рд┐рдХ рд╕рдордп рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ
- рд╕реНрд╡рдЪрд╛рд▓рд┐рдд UI рдирд┐рд░реНрдорд╛рдг рд░ WebSocket рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди

## рднрд╛рдЧ реи: рд╕реНрдерд╛рдиреАрдп рдмрдирд╛рдо рдХреНрд▓рд╛рдЙрдб рдирд┐рд░реНрдгрдп рдореНрдпрд╛рдЯреНрд░рд┐рдХреНрд╕

### рдкреНрд░рджрд░реНрд╢рди рд╡рд┐рд╢реЗрд╖рддрд╛рд╣рд░реВ

| рдкрдХреНрд╖ | рд╕реНрдерд╛рдиреАрдп (Foundry) | рдХреНрд▓рд╛рдЙрдб (Azure OpenAI) |
|------|-------------------|-----------------------|
| **рд▓реЗрдЯреЗрдиреНрд╕реА** | ЁЯЪА релреж-реирежрежms (рдиреЗрдЯрд╡рд░реНрдХ рдЫреИрди) | тП▒я╕П реирежреж-реирежрежрежms (рдиреЗрдЯрд╡рд░реНрдХ рдирд┐рд░реНрднрд░) |
| **рдЧреЛрдкрдиреАрдпрддрд╛** | ЁЯФТ рдбрд╛рдЯрд╛ рдХрд╣рд┐рд▓реНрдпреИ рдЙрдкрдХрд░рдгрдмрд╛рдЯ рдмрд╛рд╣рд┐рд░ рдЬрд╛рдБрджреИрди | тЪая╕П рдбрд╛рдЯрд╛ рдХреНрд▓рд╛рдЙрдбрдорд╛ рдкрдард╛рдЗрдиреНрдЫ |
| **рдЦрд░реНрдЪ** | ЁЯТ░ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рдкрдЫрд┐ рдирд┐рдГрд╢реБрд▓реНрдХ | ЁЯТ╕ рдкреНрд░рддрд┐ рдЯреЛрдХрди рддрд┐рд░реНрдиреБрд╣реЛрд╕реН |
| **рдЕрдлрд▓рд╛рдЗрди** | тЬЕ рдЗрдиреНрдЯрд░рдиреЗрдЯ рдмрд┐рдирд╛ рдХрд╛рдо рдЧрд░реНрдЫ | тЭМ рдЗрдиреНрдЯрд░рдиреЗрдЯ рдЪрд╛рд╣рд┐рдиреНрдЫ |
| **рдореЛрдбреЗрд▓ рдЖрдХрд╛рд░** | тЪая╕П рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рджреНрд╡рд╛рд░рд╛ рд╕реАрдорд┐рдд | тЬЕ рдареВрд▓рд╛ рдореЛрдбреЗрд▓рд╣рд░реВ рдкрд╣реБрдБрдЪрдпреЛрдЧреНрдп |
| **рд╕реНрдХреЗрд▓рд┐рдЩ** | тЪая╕П рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдирд┐рд░реНрднрд░ | тЬЕ рдЕрд╕реАрдорд┐рдд рд╕реНрдХреЗрд▓рд┐рдЩ |

### рд╣рд╛рдЗрдмреНрд░рд┐рдб рд░рдгрдиреАрддрд┐ рдврд╛рдБрдЪрд╛рд╣рд░реВ

**рд╕реНрдерд╛рдиреАрдп-рдкреНрд░рдердо рдлрд▓рдмреНрдпрд╛рдХрд╕рд╣рд┐рдд:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**рдХрд╛рд░реНрдп-рдЖрдзрд╛рд░рд┐рдд рд░реБрдЯрд┐рдЩ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## рднрд╛рдЧ рей: рдирдореВрдирд╛ режрек - Chainlit рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рди

### рдЫрд┐рдЯреЛ рд╕реБрд░реБ

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

рдПрдкреНрд▓рд┐рдХреЗрд╕рди `http://localhost:8080` рдорд╛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдЦреБрд▓реНрдЫ, рдЖрдзреБрдирд┐рдХ рдЪреНрдпрд╛рдЯ рдЗрдиреНрдЯрд░рдлреЗрд╕рд╕рд╣рд┐рддред

### рдореБрдЦреНрдп рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди

рдирдореВрдирд╛ режрек рдПрдкреНрд▓рд┐рдХреЗрд╕рдирд▓реЗ рдЙрддреНрдкрд╛рджрди рд╕реНрддрд░рдХреЛ рдврд╛рдБрдЪрд╛рд╣рд░реВ рдкреНрд░рджрд░реНрд╢рди рдЧрд░реНрджрдЫ:

**рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕реЗрд╡рд╛ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рдЪреНрдпрд╛рдЯ рд╣реНрдпрд╛рдиреНрдбрд▓рд░:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рд╡рд┐рдХрд▓реНрдкрд╣рд░реВ

**рдкрд░реНрдпрд╛рд╡рд░рдг рдЪрд░рд╣рд░реВ:**

| рдЪрд░ | рд╡рд┐рд╡рд░рдг | рдбрд┐рдлрд▓реНрдЯ | рдЙрджрд╛рд╣рд░рдг |
|----|-------|--------|--------|
| `MODEL` | рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рдореЛрдбреЗрд▓ рдЙрдкрдирд╛рдо | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local рдЕрдиреНрдд рдмрд┐рдиреНрджреБ | рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЗрдПрдХреЛ | `http://localhost:51211` |
| `API_KEY` | API рдХреБрдЮреНрдЬреА (рд╕реНрдерд╛рдиреАрдпрдХрд╛ рд▓рд╛рдЧрд┐ рд╡реИрдХрд▓реНрдкрд┐рдХ) | `""` | `your-api-key` |

**рдЙрдиреНрдирдд рдкреНрд░рдпреЛрдЧ:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## рднрд╛рдЧ рек: Jupyter рдиреЛрдЯрдмреБрдХрд╣рд░реВ рд╕рд┐рд░реНрдЬрдирд╛ рд░ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджреИ

### рдиреЛрдЯрдмреБрдХ рд╕рдорд░реНрдердирдХреЛ рдЕрд╡рд▓реЛрдХрди

рдирдореВрдирд╛ режрек рдорд╛ рд╡реНрдпрд╛рдкрдХ Jupyter рдиреЛрдЯрдмреБрдХ (`chainlit_app.ipynb`) рд╕рдорд╛рд╡реЗрд╢ рдЫ, рдЬрд╕рд▓реЗ рдкреНрд░рджрд╛рди рдЧрд░реНрджрдЫ:

- **ЁЯУЪ рд╢реИрдХреНрд╖рд┐рдХ рд╕рд╛рдордЧреНрд░реА**: рдЪрд░рдг-рджрд░-рдЪрд░рдг рд╕рд┐рдХрд╛рдЗ рд╕рд╛рдордЧреНрд░реА
- **ЁЯФм рдЕрдиреНрддрд░рдХреНрд░рд┐рдпрд╛рддреНрдордХ рдЕрдиреНрд╡реЗрд╖рдг**: рдХреЛрдб рд╕реЗрд▓рд╣рд░реВ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН рд░ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- **ЁЯУК рджреГрд╢реНрдп рдкреНрд░рджрд░реНрд╢рдирд╣рд░реВ**: рдЪрд╛рд░реНрдЯрд╣рд░реВ, рд░реЗрдЦрд╛рдЪрд┐рддреНрд░рд╣рд░реВ, рд░ рдЖрдЙрдЯрдкреБрдЯ рджреГрд╢реНрдп
- **ЁЯЫая╕П рд╡рд┐рдХрд╛рд╕ рдЙрдкрдХрд░рдгрд╣рд░реВ**: рдкрд░реАрдХреНрд╖рдг рд░ рдбрд┐рдмрдЧрд┐рдЩ рдХреНрд╖рдорддрд╛

### рдЖрдлреНрдиреИ рдиреЛрдЯрдмреБрдХрд╣рд░реВ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрджреИ

#### рдЪрд░рдг рез: Jupyter рд╡рд╛рддрд╛рд╡рд░рдг рд╕реЗрдЯ рдЕрдк рдЧрд░реНрдиреБрд╣реЛрд╕реН

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### рдЪрд░рдг реи: рдирдпрд╛рдБ рдиреЛрдЯрдмреБрдХ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН

**VS Code рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджреИ:**
1. Module08 рдбрд╛рдЗрд░реЗрдХреНрдЯрд░реАрдорд╛ VS Code рдЦреЛрд▓реНрдиреБрд╣реЛрд╕реН
2. `.ipynb` рдПрдХреНрд╕рдЯреЗрдиреНрд╕рдирд╕рд╣рд┐рдд рдирдпрд╛рдБ рдлрд╛рдЗрд▓ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН
3. "Foundry Local" рдХрд░реНрдиреЗрд▓ рдЪрдпрди рдЧрд░реНрдиреБрд╣реЛрд╕реН рдЬрдм рд╕реЛрдзрд┐рдиреНрдЫ
4. рдЖрдлреНрдиреЛ рд╕рд╛рдордЧреНрд░реАрд╕рд╣рд┐рдд рд╕реЗрд▓рд╣рд░реВ рдердкреНрди рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН

**Jupyter Lab рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджреИ:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### рдиреЛрдЯрдмреБрдХ рд╕рдВрд░рдЪрдирд╛ рдЙрддреНрддрдо рдЕрднреНрдпрд╛рд╕рд╣рд░реВ

#### рд╕реЗрд▓ рд╕рдВрдЧрдарди

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("тЬЕ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### рдЕрдиреНрддрд░рдХреНрд░рд┐рдпрд╛рддреНрдордХ рдЙрджрд╛рд╣рд░рдгрд╣рд░реВ рд░ рдЕрднреНрдпрд╛рд╕рд╣рд░реВ

#### рдЕрднреНрдпрд╛рд╕ рез: рдХреНрд▓рд╛рдЗрдиреНрдЯ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рдкрд░реАрдХреНрд╖рдг

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\nЁЯзк Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'тЬЕ Success' if result['status'] == 'ok' else 'тЭМ Failed'}")
```

#### рдЕрднреНрдпрд╛рд╕ реи: рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рд╕рд┐рдореБрд▓реЗрд╢рди

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ЁЯМК Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nтЬЕ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## рднрд╛рдЧ рел: WebGPU рдмреНрд░рд╛рдЙрдЬрд░ рдЗрдирдлрд░реЗрдиреНрд╕ рдбреЗрдореЛ

### рдЕрд╡рд▓реЛрдХрди

WebGPU рд▓реЗ рдЕрдзрд┐рдХрддрдо рдЧреЛрдкрдиреАрдпрддрд╛ рд░ рд╢реВрдиреНрдп-рд╕реНрдерд╛рдкрди рдЕрдиреБрднрд╡рдХрд╛ рд▓рд╛рдЧрд┐ рдПрдЖрдИ рдореЛрдбреЗрд▓рд╣рд░реВ рдмреНрд░рд╛рдЙрдЬрд░рдореИ рдЪрд▓рд╛рдЙрди рд╕рдХреНрд╖рдо рдмрдирд╛рдЙрдБрдЫред рдпреЛ рдирдореВрдирд╛рд▓реЗ ONNX Runtime Web рд╕рдБрдЧ WebGPU рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рдкреНрд░рджрд░реНрд╢рди рдЧрд░реНрджрдЫред

### рдЪрд░рдг рез: WebGPU рд╕рдорд░реНрдерди рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН

**рдмреНрд░рд╛рдЙрдЬрд░ рдЖрд╡рд╢реНрдпрдХрддрд╛рд╣рд░реВ:**
- Chrome/Edge резрезрей+ WebGPU рд╕рдХреНрд╖рдорд╕рд╣рд┐рдд
- рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН: `chrome://gpu` тЖТ "WebGPU" рд╕реНрдерд┐рддрд┐ рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрдиреБрд╣реЛрд╕реН
- рдкреНрд░реЛрдЧреНрд░рд╛рдореЗрдЯрд┐рдХ рдЬрд╛рдБрдЪ: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### рдЪрд░рдг реи: WebGPU рдбреЗрдореЛ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН

рдбрд╛рдЗрд░реЗрдХреНрдЯрд░реА рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ЁЯЪА WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'тЭМ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ЁЯФН WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('тЬЕ ONNX Runtime session created with WebGPU');
        log(`ЁЯУК Input names: ${session.inputNames.join(', ')}`);
        log(`ЁЯУК Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'тЬЕ WebGPU inference complete!';
        log(`ЁЯОп Predicted class: ${maxIdx}`);
        log(`ЁЯУИ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `тЭМ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### рдЪрд░рдг рей: рдбреЗрдореЛ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## рднрд╛рдЧ рем: Open WebUI рдПрдХреАрдХрд░рдг

### рдЕрд╡рд▓реЛрдХрди

Open WebUI рд▓реЗ Foundry Local рдХреЛ OpenAI-рд╕рдВрдЧрдд API рд╕рдБрдЧ рдЬрдбрд╛рди рдЧрд░реНрдиреЗ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ ChatGPT-рдЬрд╕реНрддреИ рдЗрдиреНрдЯрд░рдлреЗрд╕ рдкреНрд░рджрд╛рди рдЧрд░реНрджрдЫред

### рдЪрд░рдг рез: рдкреВрд░реНрд╡рд╢рд░реНрддрд╣рд░реВ

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### рдЪрд░рдг реи: Docker рд╕реЗрдЯрдЕрдк (рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХреЛ)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**рдиреЛрдЯ:** `host.docker.internal` рд▓реЗ Windows рдорд╛ Docker рдХрдиреНрдЯреЗрдирд░рд╣рд░реВрд▓рд╛рдИ рд╣реЛрд╕реНрдЯ рдореЗрд╕рд┐рдирдорд╛ рдкрд╣реБрдБрдЪ рдЧрд░реНрди рдЕрдиреБрдорддрд┐ рджрд┐рдиреНрдЫред

### рдЪрд░рдг рей: рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди

1. **рдмреНрд░рд╛рдЙрдЬрд░ рдЦреЛрд▓реНрдиреБрд╣реЛрд╕реН:** `http://localhost:3000` рдорд╛ рдЬрд╛рдиреБрд╣реЛрд╕реН
2. **рдкреНрд░рд╛рд░рдореНрднрд┐рдХ рд╕реЗрдЯрдЕрдк:** рдПрдбрдорд┐рди рдЦрд╛рддрд╛ рд╕рд┐рд░реНрдЬрдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реН
3. **рдореЛрдбреЗрд▓ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди:**
   - рд╕реЗрдЯрд┐рдЩреНрд╕ тЖТ рдореЛрдбреЗрд▓рд╣рд░реВ тЖТ OpenAI API  
   - рдмреЗрд╕ URL: `http://host.docker.internal:51211/v1`
   - API рдХреБрдЮреНрдЬреА: `foundry-local-key` (рдХреБрдиреИ рдкрдирд┐ рдорд╛рди рдХрд╛рдо рдЧрд░реНрдЫ)
4. **рдЬрдбрд╛рди рдкрд░реАрдХреНрд╖рдг рдЧрд░реНрдиреБрд╣реЛрд╕реН:** рдореЛрдбреЗрд▓рд╣рд░реВ рдбреНрд░рдкрдбрд╛рдЙрдирдорд╛ рджреЗрдЦрд┐рдиреБрдкрд░реНрдЫ

### рд╕рдорд╕реНрдпрд╛ рд╕рдорд╛рдзрд╛рди

**рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛рд╣рд░реВ:**

1. **рдЬрдбрд╛рди рдЕрд╕реНрд╡реАрдХреГрдд:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **рдореЛрдбреЗрд▓рд╣рд░реВ рджреЗрдЦрд┐рдБрджреИрдирдиреН:**
   - рдореЛрдбреЗрд▓ рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ рдЫ рднрдиреА рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрдиреБрд╣реЛрд╕реН: `foundry model list`
   - API рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН: `curl http://localhost:51211/v1/models`
   - Open WebUI рдХрдиреНрдЯреЗрдирд░ рдкреБрдирдГ рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН

## рднрд╛рдЧ рен: рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА рд╡рд┐рдЪрд╛рд░рд╣рд░реВ

### рд╡рд╛рддрд╛рд╡рд░рдг рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди

**рд╡рд┐рдХрд╛рд╕ рд╕реЗрдЯрдЕрдк:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### рд╕рд╛рдорд╛рдиреНрдп рдкреЛрд░реНрдЯ рд╕рдорд╕реНрдпрд╛рд╣рд░реВ рд░ рд╕рдорд╛рдзрд╛рдирд╣рд░реВ

**рдкреЛрд░реНрдЯ релрезреирезрез рджреНрд╡рдиреНрджреНрд╡ рд░реЛрдХрдерд╛рдо:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### рдкреНрд░рджрд░реНрд╢рди рдирд┐рдЧрд░рд╛рдиреА

**рд╕реНрд╡рд╛рд╕реНрдереНрдп рдЬрд╛рдБрдЪ рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## рд╕рд╛рд░рд╛рдВрд╢

рд╕рддреНрд░ рек рд▓реЗ Chainlit рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЙрддреНрдкрд╛рджрди рд╕реНрддрд░рдХреЛ рд╕рдВрд╡рд╛рджрд╛рддреНрдордХ рдПрдЖрдИ рдПрдкреНрд▓рд┐рдХреЗрд╕рди рдирд┐рд░реНрдорд╛рдгрдХреЛ рдмрд╛рд░реЗрдорд╛ рдЪрд░реНрдЪрд╛ рдЧрд░реНрдпреЛред рддрдкрд╛рдИрдВрд▓реЗ рд╕рд┐рдХреНрдиреБрднрдпреЛ:

- тЬЕ **Chainlit рдлреНрд░реЗрдорд╡рд░реНрдХ**: рдЪреНрдпрд╛рдЯ рдПрдкреНрд▓рд┐рдХреЗрд╕рдирдХрд╛ рд▓рд╛рдЧрд┐ рдЖрдзреБрдирд┐рдХ UI рд░ рд╕реНрдЯреНрд░рд┐рдорд┐рдЩ рд╕рдорд░реНрдерди
- тЬЕ **Foundry Local рдПрдХреАрдХрд░рдг**: SDK рдкреНрд░рдпреЛрдЧ рд░ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рдврд╛рдБрдЪрд╛рд╣рд░реВ  
- тЬЕ **WebGPU рдЗрдирдлрд░реЗрдиреНрд╕**: рдЕрдзрд┐рдХрддрдо рдЧреЛрдкрдиреАрдпрддрд╛рдХрд╛ рд▓рд╛рдЧрд┐ рдмреНрд░рд╛рдЙрдЬрд░-рдЖрдзрд╛рд░рд┐рдд рдПрдЖрдИ
- тЬЕ **Open WebUI рд╕реЗрдЯрдЕрдк**: рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЪреНрдпрд╛рдЯ рдЗрдиреНрдЯрд░рдлреЗрд╕ рддреИрдирд╛рддреА
- тЬЕ **рдЙрддреНрдкрд╛рджрди рдврд╛рдБрдЪрд╛рд╣рд░реВ**: рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди, рдирд┐рдЧрд░рд╛рдиреА, рд░ рд╕реНрдХреЗрд▓рд┐рдЩ

рдирдореВрдирд╛ режрек рдПрдкреНрд▓рд┐рдХреЗрд╕рдирд▓реЗ Microsoft Foundry Local рдорд╛рд░реНрдлрдд рд╕реНрдерд╛рдиреАрдп рдПрдЖрдИ рдореЛрдбреЗрд▓рд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджреИ рдЙрддреНрдХреГрд╖реНрдЯ рдкреНрд░рдпреЛрдЧрдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рдкреНрд░рджрд╛рди рдЧрд░реНрдиреЗ рдмрд▓рд┐рдпреЛ рдЪреНрдпрд╛рдЯ рдЗрдиреНрдЯрд░рдлреЗрд╕ рдирд┐рд░реНрдорд╛рдгрдХрд╛ рд▓рд╛рдЧрд┐ рдЙрддреНрддрдо рдЕрднреНрдпрд╛рд╕рд╣рд░реВ рдкреНрд░рджрд░реНрд╢рди рдЧрд░реНрджрдЫред

## рд╕рдиреНрджрд░реНрднрд╣рд░реВ

- **[рдирдореВрдирд╛ режрек: Chainlit рдПрдкреНрд▓рд┐рдХреЗрд╕рди](samples/04/README.md)**: рдкреВрд░реНрдг рдПрдкреНрд▓рд┐рдХреЗрд╕рди рд░ рджрд╕реНрддрд╛рд╡реЗрдЬ
- **[Chainlit рд╢реИрдХреНрд╖рд┐рдХ рдиреЛрдЯрдмреБрдХ](samples/04/chainlit_app.ipynb)**: рдЕрдиреНрддрд░рдХреНрд░рд┐рдпрд╛рддреНрдордХ рд╕рд┐рдХрд╛рдЗ рд╕рд╛рдордЧреНрд░реА
- **[Foundry Local рджрд╕реНрддрд╛рд╡реЗрдЬ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: рдкреВрд░реНрдг рдкреНрд▓реЗрдЯрдлрд░реНрдо рджрд╕реНрддрд╛рд╡реЗрдЬ
- **[Chainlit рджрд╕реНрддрд╛рд╡реЗрдЬ](https://docs.chainlit.io/)**: рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдлреНрд░реЗрдорд╡рд░реНрдХ рджрд╕реНрддрд╛рд╡реЗрдЬ
- **[Open WebUI рдПрдХреАрдХрд░рдг рдорд╛рд░реНрдЧрджрд░реНрд╢рди](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓

---

