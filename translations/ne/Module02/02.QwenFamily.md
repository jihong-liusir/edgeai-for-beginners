<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-17T19:47:39+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "ne"
}
-->
# खण्ड २: क्वेन परिवारको आधारभूत कुरा

क्वेन मोडेल परिवारले अलीबाबा क्लाउडको ठूलो भाषा मोडेल र मल्टिमोडल एआईमा व्यापक दृष्टिकोणलाई प्रतिनिधित्व गर्दछ। यसले देखाउँछ कि ओपन-सोर्स मोडेलले उल्लेखनीय प्रदर्शन हासिल गर्न सक्छन् र विभिन्न परिनियोजन परिदृश्यहरूमा पहुँचयोग्य रहन सक्छन्। यो बुझ्न महत्त्वपूर्ण छ कि क्वेन परिवारले शक्तिशाली एआई क्षमताहरू कसरी सक्षम बनाउँछ, लचिलो परिनियोजन विकल्पहरू प्रदान गर्दै विविध कार्यहरूमा प्रतिस्पर्धात्मक प्रदर्शन कायम राख्छ।

## विकासकर्ताहरूका लागि स्रोतहरू

### हगिङ फेस मोडेल रिपोजिटरी
चयन गरिएका क्वेन परिवारका मोडेलहरू [हगिङ फेस](https://huggingface.co/models?search=qwen) मार्फत उपलब्ध छन्। तपाईं यी मोडेलका भेरियन्टहरू अन्वेषण गर्न सक्नुहुन्छ, आफ्नो विशेष प्रयोगका लागि फाइन-ट्यून गर्न सक्नुहुन्छ, र विभिन्न फ्रेमवर्कहरू मार्फत परिनियोजन गर्न सक्नुहुन्छ।

### स्थानीय विकास उपकरणहरू
स्थानीय विकास र परीक्षणका लागि, तपाईं [माइक्रोसफ्ट फाउन्ड्री लोकल](https://github.com/microsoft/foundry-local) प्रयोग गर्न सक्नुहुन्छ। यसले उपलब्ध क्वेन मोडेलहरूलाई तपाईंको विकास मेसिनमा अनुकूलित प्रदर्शनका साथ चलाउन अनुमति दिन्छ।

### दस्तावेजीकरण स्रोतहरू
- [क्वेन मोडेल दस्तावेजीकरण](https://huggingface.co/docs/transformers/model_doc/qwen)
- [एज परिनियोजनका लागि क्वेन मोडेलहरू अनुकूलन गर्ने](https://github.com/microsoft/olive)

## परिचय

यस ट्युटोरियलमा, हामी अलीबाबाको क्वेन मोडेल परिवार र यसको आधारभूत अवधारणाहरू अन्वेषण गर्नेछौं। हामी क्वेन परिवारको विकास, प्रभावकारी बनाउने नवीन प्रशिक्षण पद्धतिहरू, परिवारका प्रमुख भेरियन्टहरू, र विभिन्न परिदृश्यहरूमा व्यावहारिक अनुप्रयोगहरू समेट्नेछौं।

## सिक्ने उद्देश्यहरू

यस ट्युटोरियलको अन्त्यसम्म, तपाईं सक्षम हुनुहुनेछ:

- अलीबाबाको क्वेन मोडेल परिवारको डिजाइन दर्शन र विकासलाई बुझ्न
- विभिन्न प्यारामिटर आकारहरूमा उच्च प्रदर्शन हासिल गर्न सक्षम बनाउने प्रमुख नवीनताहरू पहिचान गर्न
- क्वेन मोडेलका विभिन्न भेरियन्टहरूको फाइदा र सीमाहरू बुझ्न
- वास्तविक संसारका परिदृश्यहरूका लागि उपयुक्त भेरियन्टहरू चयन गर्न क्वेन मोडेलहरूको ज्ञान लागू गर्न

## आधुनिक एआई मोडेल परिदृश्य बुझ्दै

एआई परिदृश्यले महत्त्वपूर्ण रूपमा विकास गरेको छ, जहाँ विभिन्न संगठनहरूले भाषा मोडेल विकासका लागि विभिन्न दृष्टिकोणहरू अपनाएका छन्। केहीले मालिकाना बन्द-स्रोत मोडेलहरूमा ध्यान केन्द्रित गर्छन् भने अन्यले ओपन-सोर्स पहुँचयोग्यता र पारदर्शितामा जोड दिन्छन्। परम्परागत दृष्टिकोणमा प्रायः विशाल मालिकाना मोडेलहरू समावेश हुन्छन्, जुन केवल एपीआईहरू मार्फत पहुँचयोग्य हुन्छन्, वा ओपन-सोर्स मोडेलहरू, जसले क्षमतामा पछि पर्न सक्छन्।

यस परिदृश्यले शक्तिशाली एआई क्षमताहरू खोज्ने संगठनहरूका लागि चुनौतीहरू सिर्जना गर्दछ, जसले आफ्नो डेटा, लागत, र परिनियोजन लचिलोतामा नियन्त्रण कायम राख्न चाहन्छ। परम्परागत दृष्टिकोणले प्रायः अत्याधुनिक प्रदर्शन र व्यावहारिक परिनियोजन विचारहरू बीच छनोट गर्न आवश्यक बनाउँछ।

## पहुँचयोग्य एआई उत्कृष्टताको चुनौती

उच्च-गुणस्तरको, पहुँचयोग्य एआईको आवश्यकता विभिन्न परिदृश्यहरूमा बढ्दो महत्त्वपूर्ण भएको छ। लचिलो परिनियोजन विकल्पहरू आवश्यक पर्ने अनुप्रयोगहरू, जहाँ विभिन्न संगठनात्मक आवश्यकताहरूका लागि लागत-प्रभावकारी कार्यान्वयनहरू, एपीआई लागत महत्त्वपूर्ण हुन सक्छ, बहुभाषी क्षमताहरू विश्वव्यापी अनुप्रयोगहरूको लागि, वा कोडिङ र गणित जस्ता क्षेत्रहरूमा विशेष डोमेन विशेषज्ञता विचार गर्नुहोस्।

### प्रमुख परिनियोजन आवश्यकताहरू

आधुनिक एआई परिनियोजनहरूले व्यावहारिक प्रयोगलाई सीमित गर्ने केही आधारभूत आवश्यकताहरू सामना गर्छन्:

- **पहुंचयोग्यता**: पारदर्शिता र अनुकूलनका लागि ओपन-सोर्स उपलब्धता
- **लागत प्रभावकारिता**: विभिन्न बजेटका लागि उचित कम्प्युटेशनल आवश्यकताहरू
- **लचिलोता**: विभिन्न परिनियोजन परिदृश्यहरूको लागि बहु मोडेल आकारहरू
- **वैश्विक पहुँच**: बलियो बहुभाषी र क्रस-सांस्कृतिक क्षमताहरू
- **विशेषता**: विशेष प्रयोगका लागि डोमेन-विशिष्ट भेरियन्टहरू

## क्वेन मोडेल दर्शन

क्वेन मोडेल परिवारले एआई मोडेल विकासमा व्यापक दृष्टिकोणलाई प्रतिनिधित्व गर्दछ, जसले ओपन-सोर्स पहुँचयोग्यता, बहुभाषी क्षमताहरू, र व्यावहारिक परिनियोजनलाई प्राथमिकता दिन्छ, विविध कार्यहरूमा प्रतिस्पर्धात्मक प्रदर्शन विशेषताहरू कायम राख्छ। क्वेन मोडेलहरूले विविध मोडेल आकारहरू, उच्च-गुणस्तरको प्रशिक्षण पद्धतिहरू, र विभिन्न डोमेनहरूको लागि विशेष भेरियन्टहरू मार्फत यो हासिल गर्छ।

क्वेन परिवारले प्रदर्शन-दक्षता स्पेक्ट्रममा विकल्पहरू प्रदान गर्न डिजाइन गरिएका विभिन्न दृष्टिकोणहरू समेट्छ, मोबाइल उपकरणहरूदेखि उद्यम सर्भरहरूमा परिनियोजन सक्षम बनाउँछ, जबकि अर्थपूर्ण एआई क्षमताहरू प्रदान गर्दछ। लक्ष्य भनेको उच्च-गुणस्तरको एआईमा पहुँचलाई लोकतान्त्रिक बनाउनु हो, परिनियोजन विकल्पहरूमा लचिलोता प्रदान गर्दै।

### क्वेन डिजाइनका मुख्य सिद्धान्तहरू

क्वेन मोडेलहरू केही आधारभूत सिद्धान्तहरूमा निर्माण गरिएका छन्, जसले तिनीहरूलाई अन्य भाषा मोडेल परिवारहरूबाट फरक बनाउँछ:

- **पहिले ओपन-सोर्स**: अनुसन्धान र व्यावसायिक प्रयोगका लागि पूर्ण पारदर्शिता र पहुँचयोग्यता
- **व्यापक प्रशिक्षण**: बहुभाषा र डोमेनहरू समेट्ने विशाल, विविध डेटासेटमा प्रशिक्षण
- **स्केलेबल आर्किटेक्चर**: विभिन्न कम्प्युटेशनल आवश्यकताहरू मिलाउन बहु मोडेल आकारहरू
- **विशेष उत्कृष्टता**: विशेष कार्यहरूको लागि अनुकूलित डोमेन-विशिष्ट भेरियन्टहरू

## क्वेन परिवारलाई सक्षम बनाउने प्रमुख प्रविधिहरू

### विशाल स्केल प्रशिक्षण

क्वेन परिवारको परिभाषित पक्ष भनेको मोडेल विकासमा लगानी गरिएको प्रशिक्षण डेटा र कम्प्युटेशनल स्रोतहरूको विशाल स्केल हो। क्वेन मोडेलहरूले सावधानीपूर्वक क्युरेट गरिएका, बहुभाषी डेटासेटहरू प्रयोग गर्छन्, जसले ट्रिलियन टोकनहरू समेट्छ, व्यापक विश्व ज्ञान र तर्क क्षमताहरू प्रदान गर्न डिजाइन गरिएको।

यो दृष्टिकोणले उच्च-गुणस्तरको वेब सामग्री, शैक्षिक साहित्य, कोड रिपोजिटरीहरू, र बहुभाषी स्रोतहरूलाई संयोजन गरेर काम गर्छ। प्रशिक्षण पद्धतिले विभिन्न डोमेन र भाषाहरूमा ज्ञानको चौडाइ र गहिराइ दुवैलाई जोड दिन्छ।

### उन्नत तर्क र सोच

हालका क्वेन मोडेलहरूले जटिल बहु-चरण समस्या समाधान सक्षम गर्ने परिष्कृत तर्क क्षमताहरू समेट्छन्:

**थिंकिङ मोड (क्वेन३)**: मोडेलहरूले अन्तिम उत्तरहरू प्रदान गर्नु अघि विस्तृत चरण-दर-चरण तर्कमा संलग्न हुन सक्छन्, मानव समस्या समाधान दृष्टिकोणहरू जस्तै।

**डुअल-मोड अपरेशन**: साधारण प्रश्नहरूको लागि छिटो प्रतिक्रिया मोड र जटिल समस्याहरूको लागि गहिरो सोच मोड बीच स्विच गर्ने क्षमता।

**चेन-ऑफ-थट एकीकरण**: जटिल कार्यहरूमा पारदर्शिता र शुद्धता सुधार गर्ने तर्क चरणहरूको प्राकृतिक समावेश।

### आर्किटेक्चरल नवीनताहरू

क्वेन परिवारले प्रदर्शन र दक्षताको लागि डिजाइन गरिएका केही आर्किटेक्चरल अनुकूलनहरू समेट्छ:

**स्केलेबल डिजाइन**: मोडेल आकारहरूमा सजिलो स्केलिङ र तुलना सक्षम बनाउने सुसंगत आर्किटेक्चर।

**मल्टिमोडल एकीकरण**: एकीकृत आर्किटेक्चरहरू भित्र पाठ, दृष्टि, र अडियो प्रशोधन क्षमताहरूको सहज एकीकरण।

**परिनियोजन अनुकूलन**: विभिन्न हार्डवेयर कन्फिगरेसनहरूको लागि बहु क्वान्टाइजेशन विकल्पहरू र परिनियोजन ढाँचाहरू।

## मोडेल आकार र परिनियोजन विकल्पहरू

आधुनिक परिनियोजन वातावरणहरूले विभिन्न कम्प्युटेशनल आवश्यकताहरूमा क्वेन मोडेलहरूको लचकता लाभ उठाउँछन्:

### साना मोडेलहरू (०.५बी-३बी)

क्वेनले एज परिनियोजन, मोबाइल अनुप्रयोगहरू, र स्रोत-सीमित वातावरणहरूको लागि उपयुक्त कुशल साना मोडेलहरू प्रदान गर्दछ, प्रभावशाली क्षमताहरू कायम राख्दै।

### मध्यम मोडेलहरू (७बी-३२बी)

मध्य-स्तरका मोडेलहरूले व्यावसायिक अनुप्रयोगहरूको लागि उन्नत क्षमताहरू प्रदान गर्छन्, प्रदर्शन र कम्प्युटेशनल आवश्यकताहरू बीच उत्कृष्ट सन्तुलन प्रदान गर्दै।

### ठूला मोडेलहरू (७२बी+)

पूर्ण-स्तरका मोडेलहरूले अत्याधुनिक प्रदर्शन प्रदान गर्छन्, माग गर्ने अनुप्रयोगहरू, अनुसन्धान, र उद्यम परिनियोजनहरूको लागि अधिकतम क्षमता आवश्यक पर्दछ।

## क्वेन मोडेल परिवारका फाइदाहरू

### ओपन-सोर्स पहुँचयोग्यता

क्वेन मोडेलहरूले पूर्ण पारदर्शिता र अनुकूलन क्षमताहरू प्रदान गर्छन्, जसले संगठनहरूलाई मोडेलहरू बुझ्न, परिमार्जन गर्न, र आफ्नो विशेष आवश्यकताहरूमा अनुकूलन गर्न अनुमति दिन्छ, विक्रेता-लॉक-इन बिना।

### परिनियोजन लचिलोता

मोडेल आकारहरूको दायरा विभिन्न हार्डवेयर कन्फिगरेसनहरूमा परिनियोजन सक्षम बनाउँछ, मोबाइल उपकरणहरूदेखि उच्च-स्तरीय सर्भरहरूसम्म, संगठनहरूलाई आफ्नो एआई पूर्वाधार विकल्पहरूमा लचिलोता प्रदान गर्दै।

### बहुभाषी उत्कृष्टता

क्वेन मोडेलहरूले बहुभाषी बुझाइ र उत्पादनमा उत्कृष्ट प्रदर्शन गर्छन्, दर्जनौं भाषाहरूलाई समर्थन गर्दै, विशेष गरी अंग्रेजी र चिनियाँमा बलियो, जसले तिनीहरूलाई विश्वव्यापी अनुप्रयोगहरूको लागि उपयुक्त बनाउँछ।

### प्रतिस्पर्धात्मक प्रदर्शन

क्वेन मोडेलहरूले लगातार बेंचमार्कमा प्रतिस्पर्धात्मक परिणामहरू हासिल गर्छन्, ओपन-सोर्स पहुँचयोग्यता प्रदान गर्दै, जसले देखाउँछ कि ओपन मोडेलहरूले मालिकाना विकल्पहरूलाई मेल खान सक्छन्।

### विशेष क्षमताहरू

क्वेन-कोडर र क्वेन-म्याथ जस्ता डोमेन-विशिष्ट भेरियन्टहरूले सामान्य भाषा बुझाइ क्षमताहरू कायम राख्दै विशेष विशेषज्ञता प्रदान गर्छन्।

## व्यावहारिक उदाहरणहरू र प्रयोगका केसहरू

प्राविधिक विवरणमा जानु अघि, क्वेन मोडेलहरूले के गर्न सक्छन् भन्ने केही ठोस उदाहरणहरू अन्वेषण गरौं:

### गणितीय तर्क उदाहरण

क्वेन-म्याथले चरण-दर-चरण गणितीय समस्या समाधानमा उत्कृष्ट प्रदर्शन गर्छ। उदाहरणका लागि, जटिल क्यालकुलस समस्या समाधान गर्न सोध्दा:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### बहुभाषी समर्थन उदाहरण

क्वेन मोडेलहरूले विभिन्न भाषाहरूमा बलियो बहुभाषी क्षमताहरू प्रदर्शन गर्छन्:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### मल्टिमोडल क्षमताहरू उदाहरण

क्वेन-वीएलले पाठ र छविहरूलाई एकसाथ प्रशोधन गर्न सक्छ:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### कोड उत्पादन उदाहरण

क्वेन-कोडरले विभिन्न प्रोग्रामिङ भाषाहरूमा कोड उत्पादन र व्याख्या गर्न उत्कृष्ट प्रदर्शन गर्छ:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Perform binary search on a sorted array to find the target value.
    
    Args:
        arr (list): A sorted list of comparable elements
        target: The value to search for
        
    Returns:
        int: Index of target if found, -1 if not found
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # Initialize left and right pointers
    left, right = 0, len(arr) - 1
    
    # Continue searching while the search space is valid
    while left <= right:
        # Calculate middle index to avoid integer overflow
        mid = left + (right - left) // 2
        
        # Check if we found the target
        if arr[mid] == target:
            return mid
        
        # If target is smaller, search left half
        elif arr[mid] > target:
            right = mid - 1
        
        # If target is larger, search right half
        else:
            left = mid + 1
    
    # Target not found
    return -1

# Example usage:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index of 7: {result}")  # Output: Index of 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### एज परिनियोजन उदाहरण

क्वेन मोडेलहरू विभिन्न एज उपकरणहरूमा अनुकूलित कन्फिगरेसनहरूसँग परिनियोजन गर्न सकिन्छ:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## क्वेन परिवारको विकास

### क्वेन १.० र १.५: आधारभूत मोडेलहरू

प्रारम्भिक क्वेन मोडेलहरूले व्यापक प्रशिक्षण र ओपन-सोर्स पहुँचयोग्यताको आधारभूत सिद्धान्तहरू स्थापना गरे:

- **क्वेन-७बी (७बी प्यारामिटरहरू)**: चिनियाँ र अंग्रेजी भाषा बुझाइमा केन्द्रित प्रारम्भिक संस्करण
- **क्वेन-१४बी (१४बी प्यारामिटरहरू)**: तर्क र ज्ञानमा सुधार गरिएको उन्नत क्षमता
- **क्वेन-७२बी (७२बी प्यारामिटरहरू)**: ठूलो-स्तरको मोडेलले अत्याधुनिक प्रदर्शन प्रदान गर्दै
- **क्वेन१.५ श्रृंखला**: लामो-सन्दर्भ ह्यान्डलिङ सुधार गरिएको बहु आकारहरू (०.५बी देखि ११०बी) मा विस्तार

### क्वेन२ परिवार: मल्टिमोडल विस्तार

क्वेन२ श्रृंखलाले भाषा र मल्टिमोडल क्षमताहरूमा महत्त्वपूर्ण प्रगति चिन्हित गर्‍यो:

- **क्वेन२-०.५बी देखि ७२बी**: विभिन्न परिनियोजन आवश्यकताहरूका लागि व्यापक भाषा मोडेलहरूको दायरा
- **क्वेन२-५७बी-ए१४बी (MoE)**: दक्ष प्यारामिटर प्रयोगका लागि मिश्रण-ऑफ-एक्सपर्ट्स आर्किटेक्चर
- **क्वेन२-वीएल**: छवि बुझाइका लागि उन्नत दृष्टि-भाषा क्षमताहरू
- **क्वेन२-अडियो**: अडियो प्रशोधन र बुझाइ क्षमताहरू
- **क्वेन२-म्याथ**: विशेष गणितीय तर्क र समस्या समाधान

### क्वेन२.५ परिवार: सुधारिएको प्रदर्शन

क्वेन२.५ श्रृंखलाले सबै आयामहरूमा महत्त्वपूर्ण सुधार ल्यायो:

- **विस्तारित प्रशिक्षण**: १८ ट्रिलियन टोकनको प्रशिक्षण डेटा सुधारिएको क्षमताहरूका लागि
- **विस्तारित सन्दर्भ**: १२८के टोकन सन्दर्भ लम्बाइ, टर्बो भेरियन्टले १एम टोकन समर्थन गर्दै
- **उन्नत विशेषता**: सुधारिएको क्वेन२.५-कोडर र क्वेन२.५-म्याथ भेरियन्टहरू
- **बेहतर बहुभाषी समर्थन**: २७+ भाषाहरूमा सुधारिएको प्रदर्शन

### क्वेन३ परिवार: उन्नत तर्क

नवीनतम पुस्ताले तर्क र सोच क्षमताहरूको सीमा धकेल्छ:

- **क्वेन३-२३५बी-ए२२बी**: २३५बी कुल प्यारामिटरहरूसँग फ्ल्यागशिप मिश्रण-ऑफ-एक्सपर्ट्स मोडेल
- **क्वेन३-३०बी-ए३बी**: सक्रिय प्यारामिटर प्रति बलियो प्रदर्शनको साथ कुशल MoE मोडेल
- **घनत्व मोडेलहरू**: विभिन्न परिनियोजन परिदृश्यहरूको लागि क्वेन३-३२बी, १४बी, ८बी, ४बी, १.७बी, ०.६बी
- **थिंकिङ मोड**: छिटो प्रतिक्रियाहरू र गहिरो सोच समर्थन गर्ने हाइब्रिड तर्क दृष्टिकोण
- **बहुभाषी उत्कृष्टता**: ११९ भाषाहरू र बोलियाँको समर्थन
- **उन्नत प्रशिक्षण**: ३६ ट्रिलियन टोकनको विविध, उच्च-गुणस्तरको प्रशिक्षण डेटा

## क्वेन मोडेलहरूको अनुप्रयोगहरू

### उद्यम अनुप्रयोगहरू

संगठनहरूले दस्तावेज विश्लेषण, ग्राहक सेवा स्वचालन, कोड उत्पादन सहयोग, र व्यावसायिक बुद्धिमत्ता अनुप्रयोगहरूको लागि क्वेन मोडेलहरू प्रयोग गर्छन्। ओपन-सो
Here's how to get started with Qwen models using the Hugging Face Transformers library:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Qwen2.5 मोडेलहरू प्रयोग गर्दै

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### विशेष मोडेल प्रयोग

**Qwen-Coder मार्फत कोड निर्माण:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**गणितीय समस्या समाधान:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**भिजन-भाषा कार्यहरू:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### थिंकिङ मोड (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 मोबाइल र एज डिप्लोयमेन्ट

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### API डिप्लोयमेन्ट उदाहरण

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## प्रदर्शन मापन र उपलब्धिहरू

Qwen मोडेल परिवारले विभिन्न मापनहरूमा उत्कृष्ट प्रदर्शन हासिल गरेको छ, खुला स्रोत पहुँच कायम राख्दै:

### प्रमुख प्रदर्शन हाइलाइटहरू

**तर्क गर्ने क्षमता:**
- Qwen3-235B-A22B ले कोडिङ, गणित, सामान्य क्षमता मापनमा DeepSeek-R1, o1, o3-mini, Grok-3, र Gemini-2.5-Pro जस्ता शीर्ष मोडेलहरूसँग प्रतिस्पर्धात्मक नतिजा हासिल गरेको छ।
- Qwen3-30B-A3B ले QwQ-32B भन्दा १० गुणा सक्रिय प्यारामिटरहरूसँग उत्कृष्ट प्रदर्शन गरेको छ।
- Qwen3-4B ले Qwen2.5-72B-Instruct को प्रदर्शनसँग प्रतिस्पर्धा गर्न सक्छ।

**क्षमतामा सुधार:**
- Qwen3-MoE आधार मोडेलहरूले Qwen2.5 घनत्व आधार मोडेलहरूको समान प्रदर्शन हासिल गरेको छ, केवल १०% सक्रिय प्यारामिटर प्रयोग गर्दै।
- घनत्व मोडेलहरूको तुलनामा प्रशिक्षण र अनुमान लागतमा महत्वपूर्ण बचत।

**बहुभाषी क्षमता:**
- Qwen3 मोडेलहरूले ११९ भाषा र उपभाषाहरूलाई समर्थन गर्दछ।
- विविध भाषिक र सांस्कृतिक सन्दर्भहरूमा बलियो प्रदर्शन।

**प्रशिक्षण स्केल:**
- Qwen3 ले लगभग ३६ ट्रिलियन टोकन प्रयोग गर्दछ, जसले ११९ भाषा र उपभाषाहरू समेट्छ, जबकि Qwen2.5 ले १८ ट्रिलियन टोकन प्रयोग गर्दछ।

### मोडेल तुलना म्याट्रिक्स

| मोडेल श्रृंखला | प्यारामिटर दायरा | सन्दर्भ लम्बाइ | प्रमुख विशेषताहरू | उत्तम प्रयोग केसहरू |
|----------------|------------------|----------------|-------------------|---------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | सन्तुलित प्रदर्शन, बहुभाषी | सामान्य अनुप्रयोगहरू, उत्पादन डिप्लोयमेन्ट |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | कोड निर्माण, प्रोग्रामिङ | सफ्टवेयर विकास, कोडिङ सहयोग |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | गणितीय तर्क | शैक्षिक प्लेटफर्महरू, STEM अनुप्रयोगहरू |
| **Qwen2.5-VL** | विभिन्न | परिवर्तनशील | भिजन-भाषा बुझाइ | मल्टिमोडल अनुप्रयोगहरू, छवि विश्लेषण |
| **Qwen3** | 0.6B-235B | परिवर्तनशील | उन्नत तर्क, थिंकिङ मोड | जटिल तर्क, अनुसन्धान अनुप्रयोगहरू |
| **Qwen3 MoE** | 30B-235B कुल | परिवर्तनशील | प्रभावकारी ठूलो स्केल प्रदर्शन | उद्यम अनुप्रयोगहरू, उच्च प्रदर्शन आवश्यकताहरू |

## मोडेल चयन मार्गदर्शन

### आधारभूत अनुप्रयोगहरूको लागि
- **Qwen2.5-0.5B/1.5B**: मोबाइल एप्स, एज उपकरणहरू, वास्तविक समय अनुप्रयोगहरू
- **Qwen2.5-3B/7B**: सामान्य च्याटबोटहरू, सामग्री निर्माण, प्रश्न उत्तर प्रणालीहरू

### गणितीय र तर्क कार्यहरूको लागि
- **Qwen2.5-Math**: गणितीय समस्या समाधान र STEM शिक्षा
- **Qwen3 थिंकिङ मोडसँग**: चरण-दर-चरण विश्लेषण आवश्यक पर्ने जटिल तर्क

### प्रोग्रामिङ र विकासको लागि
- **Qwen2.5-Coder**: कोड निर्माण, डिबगिङ, प्रोग्रामिङ सहयोग
- **Qwen3**: तर्क क्षमतासहित उन्नत प्रोग्रामिङ कार्यहरू

### मल्टिमोडल अनुप्रयोगहरूको लागि
- **Qwen2.5-VL**: छवि बुझाइ, दृश्य प्रश्न उत्तर
- **Qwen-Audio**: अडियो प्रशोधन र भाषण बुझाइ

### उद्यम डिप्लोयमेन्टको लागि
- **Qwen2.5-32B/72B**: उच्च प्रदर्शन भाषा बुझाइ
- **Qwen3-235B-A22B**: माग गर्ने अनुप्रयोगहरूको लागि अधिकतम क्षमता

## डिप्लोयमेन्ट प्लेटफर्महरू र पहुँच

### क्लाउड प्लेटफर्महरू
- **Hugging Face Hub**: समुदाय समर्थनसहित व्यापक मोडेल रिपोजिटरी
- **ModelScope**: Alibaba को मोडेल प्लेटफर्म अनुकूलन उपकरणहरूसहित
- **विभिन्न क्लाउड प्रदायकहरू**: मानक ML प्लेटफर्महरू मार्फत समर्थन

### स्थानीय विकास फ्रेमवर्कहरू
- **Transformers**: सजिलो डिप्लोयमेन्टको लागि मानक Hugging Face एकीकरण
- **vLLM**: उत्पादन वातावरणहरूको लागि उच्च प्रदर्शन सर्भिङ
- **Ollama**: स्थानीय डिप्लोयमेन्ट र व्यवस्थापनको लागि सरल
- **ONNX Runtime**: विभिन्न हार्डवेयरको लागि क्रस-प्लेटफर्म अनुकूलन
- **llama.cpp**: विविध प्लेटफर्महरूको लागि प्रभावकारी C++ कार्यान्वयन

### सिकाइ स्रोतहरू
- **Qwen दस्तावेजीकरण**: आधिकारिक दस्तावेजीकरण र मोडेल कार्डहरू
- **Hugging Face Model Hub**: अन्तरक्रियात्मक डेमोहरू र समुदाय उदाहरणहरू
- **अनुसन्धान पत्रहरू**: arxiv मा प्राविधिक पत्रहरू गहिरो बुझाइको लागि
- **समुदाय फोरमहरू**: सक्रिय समुदाय समर्थन र छलफलहरू

### Qwen मोडेलहरूसँग सुरु गर्दै

#### विकास प्लेटफर्महरू
1. **Hugging Face Transformers**: मानक Python एकीकरणबाट सुरु गर्नुहोस्
2. **ModelScope**: Alibaba को अनुकूलित डिप्लोयमेन्ट उपकरणहरू अन्वेषण गर्नुहोस्
3. **स्थानीय डिप्लोयमेन्ट**: Ollama वा प्रत्यक्ष ट्रान्सफर्मरहरू स्थानीय परीक्षणको लागि प्रयोग गर्नुहोस्

#### सिकाइ मार्ग
1. **मुख्य अवधारणाहरू बुझ्नुहोस्**: Qwen परिवारको वास्तुकला र क्षमताहरू अध्ययन गर्नुहोस्
2. **भिन्नताहरूको प्रयोग गर्नुहोस्**: प्रदर्शन व्यापार-अफहरू बुझ्न विभिन्न मोडेल आकारहरू प्रयास गर्नुहोस्
3. **कार्यान्वयन अभ्यास गर्नुहोस्**: विकास वातावरणहरूमा मोडेलहरू डिप्लोय गर्नुहोस्
4. **डिप्लोयमेन्ट अनुकूलन गर्नुहोस्**: उत्पादन प्रयोग केसहरूको लागि फाइन-ट्यून गर्नुहोस्

#### उत्तम अभ्यासहरू
- **सानोबाट सुरु गर्नुहोस्**: प्रारम्भिक विकासको लागि साना मोडेलहरू (1.5B-7B) बाट सुरु गर्नुहोस्
- **च्याट टेम्प्लेटहरू प्रयोग गर्नुहोस्**: इष्टतम नतिजाको लागि उचित ढाँचा लागू गर्नुहोस्
- **स्रोतहरू अनुगमन गर्नुहोस्**: मेमोरी प्रयोग र अनुमान गति ट्र्याक गर्नुहोस्
- **विशेषीकरण विचार गर्नुहोस्**: उपयुक्त हुँदा डोमेन-विशिष्ट भेरियन्टहरू चयन गर्नुहोस्

## उन्नत प्रयोग ढाँचाहरू

### फाइन-ट्यूनिङ उदाहरणहरू

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### विशेष प्रम्प्ट इन्जिनियरिङ

**जटिल तर्क कार्यहरूको लागि:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**सन्दर्भसहित कोड निर्माणको लागि:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### बहुभाषी अनुप्रयोगहरू

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 उत्पादन डिप्लोयमेन्ट ढाँचाहरू

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## प्रदर्शन अनुकूलन रणनीतिहरू

### मेमोरी अनुकूलन

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### अनुमान अनुकूलन

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## उत्तम अभ्यासहरू र दिशानिर्देशहरू

### सुरक्षा र गोपनीयता

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### अनुगमन र मूल्याङ्कन

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## निष्कर्ष

Qwen मोडेल परिवारले विविध अनुप्रयोगहरूमा प्रतिस्पर्धात्मक प्रदर्शन कायम राख्दै AI प्रविधिको लोकतान्त्रीकरणको लागि व्यापक दृष्टिकोण प्रस्तुत गर्दछ। यसको खुला स्रोत पहुँच, बहुभाषी क्षमता, र लचिलो डिप्लोयमेन्ट विकल्पहरूको प्रतिबद्धताले Qwen लाई संगठनहरू र विकासकर्ताहरूलाई उनीहरूको स्रोतहरू वा विशिष्ट आवश्यकताहरूको परवाह नगरी शक्तिशाली AI क्षमताहरू प्रयोग गर्न सक्षम बनाउँछ।

### मुख्य निष्कर्षहरू

**खुला स्रोत उत्कृष्टता**: Qwen ले देखाउँछ कि खुला स्रोत मोडेलहरूले पारदर्शिता, अनुकूलन, र नियन्त्रण प्रदान गर्दै मालिकाना विकल्पहरूसँग प्रतिस्पर्धात्मक प्रदर्शन हासिल गर्न सक्छ।

**स्केलेबल वास्तुकला**: 0.5B देखि 235B प्यारामिटरहरूको दायरा मोबाइल उपकरणहरूदेखि उद्यम क्लस्टरहरूसम्म कम्प्युटेशनल वातावरणहरूको पूर्ण स्पेक्ट्रममा डिप्लोयमेन्ट सक्षम बनाउँछ।

**विशेषीकृत क्षमता**: Qwen-Coder, Qwen-Math, र Qwen-VL जस्ता डोमेन-विशेष भेरियन्टहरूले सामान्य भाषा बुझाइ कायम राख्दै विशेष विशेषज्ञता प्रदान गर्दछ।

**वैश्विक पहुँचयोग्यता**: ११९+ भाषाहरूमा बलियो बहुभाषी समर्थनले Qwen लाई अन्तर्राष्ट्रिय अनुप्रयोगहरू र विविध प्रयोगकर्ता आधारहरूको लागि उपयुक्त बनाउँछ।

**निरन्तर नवप्रवर्तन**: Qwen 1.0 देखि Qwen3 सम्मको विकासले क्षमताहरू, दक्षता, र डिप्लोयमेन्ट विकल्पहरूमा निरन्तर सुधार देखाउँछ।

### भविष्यको दृष्टिकोण

जसरी Qwen परिवार विकसित हुँदैछ, हामीले अपेक्षा गर्न सक्छौं:

- **दक्षतामा सुधार**: प्यारामिटर अनुपातहरू प्रति प्रदर्शनको लागि निरन्तर अनुकूलन
- **विस्तारित मल्टिमोडल क्षमता**: थप परिष्कृत भिजन, अडियो, र पाठ प्रशोधनको एकीकरण
- **तर्क सुधार**: उन्नत सोच संयन्त्रहरू र बहु-चरण समस्या समाधान क्षमता
- **डिप्लोयमेन्ट उपकरणहरू सुधार**: विविध डिप्लोयमेन्ट परिदृश्यहरूको लागि उन्नत फ्रेमवर्कहरू र अनुकूलन उपकरणहरू
- **समुदाय वृद्धि**: उपकरणहरू, अनुप्रयोगहरू, र समुदाय योगदानहरूको विस्तारित पारिस्थितिकी तन्त्र

### आगामी कदमहरू

च्याटबोट निर्माण गर्दै, शैक्षिक उपकरणहरू विकास गर्दै, कोडिङ सहायकहरू सिर्जना गर्दै, वा बहुभाषी अनुप्रयोगहरूमा काम गर्दै, Qwen परिवारले बलियो समुदाय समर्थन र व्यापक दस्तावेजीकरणसहित स्केलेबल समाधानहरू प्रदान गर्दछ।

नवीनतम अपडेटहरू, मोडेल रिलीजहरू, र विस्तृत प्राविधिक दस्तावेजीकरणको लागि, Hugging Face मा आधिकारिक Qwen रिपोजिटरीहरू भ्रमण गर्नुहोस् र सक्रिय समुदाय छलफलहरू र उदाहरणहरू अन्वेषण गर्नुहोस्।

AI विकासको भविष्य पहुँचयोग्य, पारदर्शी, र शक्तिशाली उपकरणहरूमा निहित छ जसले सबै क्षेत्रहरू र स्केलहरूमा नवप्रवर्तन सक्षम बनाउँछ। Qwen परिवारले यो दृष्टिकोणलाई उदाहरण दिन्छ, संगठनहरू र विकासकर्ताहरूलाई AI-संचालित अनुप्रयोगहरूको अर्को पुस्ता निर्माण गर्न आधार प्रदान गर्दै।

## थप स्रोतहरू

- **आधिकारिक दस्तावेजीकरण**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **मोडेल हब**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **प्राविधिक पत्रहरू**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **समुदाय**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope प्लेटफर्म**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## सिकाइ परिणामहरू

यो मोड्युल पूरा गरेपछि, तपाईं सक्षम हुनुहुनेछ:

1. Qwen मोडेल परिवारको वास्तुकला लाभ र यसको खुला स्रोत दृष्टिकोण व्याख्या गर्नुहोस्।
2. विशिष्ट अनुप्रयोग आवश्यकताहरू र स्रोत सीमाहरूको आधारमा उपयुक्त Qwen भेरियन्ट चयन गर्नुहोस्।
3. विभिन्न डिप्लोयमेन्ट परिदृश्यहरूमा अनुकूलित कन्फिगरेसनहरूसँग Qwen मोडेलहरू कार्यान्वयन गर्नुहोस्।
4. Qwen मोडेल प्रदर्शन सुधार गर्न क्वान्टाइजेशन र अनुकूलन प्रविधिहरू लागू गर्नुहोस्।
5. Qwen परिवारभर मोडेल आकार, प्रदर्शन, र क्षमताहरू बीचको व्यापार-अफहरू मूल्याङ्कन गर्नुहोस्।

## के आउँदैछ

- [03: Gemma Family Fundamentals](03.GemmaFamily.md)

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।