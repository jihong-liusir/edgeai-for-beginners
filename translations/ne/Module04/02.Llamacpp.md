<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T21:08:00+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "ne"
}
-->
# खण्ड २ : Llama.cpp कार्यान्वयन मार्गदर्शन

## सामग्री तालिका
1. [परिचय](../../../Module04)
2. [Llama.cpp के हो?](../../../Module04)
3. [स्थापना](../../../Module04)
4. [स्रोतबाट निर्माण](../../../Module04)
5. [मोडेल क्वान्टाइजेशन](../../../Module04)
6. [आधारभूत प्रयोग](../../../Module04)
7. [उन्नत सुविधाहरू](../../../Module04)
8. [Python एकीकरण](../../../Module04)
9. [समस्या समाधान](../../../Module04)
10. [सर्वोत्तम अभ्यासहरू](../../../Module04)

## परिचय

यो विस्तृत ट्यूटोरियलले Llama.cpp को बारेमा सबै जानकारी प्रदान गर्दछ, आधारभूत स्थापना देखि उन्नत प्रयोग परिदृश्यसम्म। Llama.cpp एक शक्तिशाली C++ कार्यान्वयन हो जसले न्यूनतम सेटअप र विभिन्न हार्डवेयर कन्फिगरेसनहरूमा उत्कृष्ट प्रदर्शनको साथ ठूलो भाषा मोडेलहरूको कुशल अनुमान सक्षम गर्दछ।

## Llama.cpp के हो?

Llama.cpp एक LLM अनुमान फ्रेमवर्क हो जुन C/C++ मा लेखिएको छ। यसले न्यूनतम सेटअपको साथ ठूलो भाषा मोडेलहरू स्थानीय रूपमा चलाउन सक्षम बनाउँछ र विभिन्न हार्डवेयरमा अत्याधुनिक प्रदर्शन प्रदान गर्दछ। मुख्य सुविधाहरू समावेश छन्:

### मुख्य सुविधाहरू
- **साधारण C/C++ कार्यान्वयन** कुनै निर्भरता बिना
- **क्रस-प्ल्याटफर्म अनुकूलता** (Windows, macOS, Linux)
- **हार्डवेयर अनुकूलन** विभिन्न आर्किटेक्चरहरूको लागि
- **क्वान्टाइजेशन समर्थन** (1.5-बिट देखि 8-बिट इन्टिजर क्वान्टाइजेशन)
- **CPU र GPU एक्सेलेरेशन** समर्थन
- **स्मृति दक्षता** सीमित वातावरणहरूको लागि

### फाइदाहरू
- CPU मा कुशलतापूर्वक चल्छ, विशेष हार्डवेयर आवश्यक छैन
- GPU ब्याकएन्डहरू समर्थन गर्दछ (CUDA, Metal, OpenCL, Vulkan)
- हल्का र पोर्टेबल
- Apple सिलिकनलाई प्राथमिकता दिइएको छ - ARM NEON, Accelerate र Metal फ्रेमवर्कहरू मार्फत अनुकूलित
- कम स्मृति प्रयोगको लागि विभिन्न क्वान्टाइजेशन स्तरहरू समर्थन गर्दछ

## स्थापना

### विधि १: पूर्व-निर्मित बाइनरीहरू (सुरुवातकर्ताहरूको लागि सिफारिस)

#### GitHub Releases बाट डाउनलोड गर्नुहोस्
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) मा जानुहोस्
2. आफ्नो प्रणालीको लागि उपयुक्त बाइनरी डाउनलोड गर्नुहोस्:
   - Windows को लागि `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS को लागि `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux को लागि `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. आर्काइभलाई अनप्याक गर्नुहोस् र आफ्नो प्रणालीको PATH मा डाइरेक्टरी थप्नुहोस्

#### प्याकेज म्यानेजरहरू प्रयोग गर्दै

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (विभिन्न वितरणहरू):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### विधि २: Python प्याकेज (llama-cpp-python)

#### आधारभूत स्थापना
```bash
pip install llama-cpp-python
```

#### हार्डवेयर एक्सेलेरेशनको साथ
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## स्रोतबाट निर्माण

### पूर्वापेक्षाहरू

**प्रणाली आवश्यकताहरू:**
- C++ कम्पाइलर (GCC, Clang, वा MSVC)
- CMake (संस्करण 3.14 वा उच्च)
- Git
- आफ्नो प्लेटफर्मको लागि निर्माण उपकरणहरू

**पूर्वापेक्षाहरू स्थापना गर्दै:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022 स्थापना गर्नुहोस् C++ विकास उपकरणहरूसँग
- आधिकारिक वेबसाइटबाट CMake स्थापना गर्नुहोस्
- Git स्थापना गर्नुहोस्

### आधारभूत निर्माण प्रक्रिया

1. **रिपोजिटरी क्लोन गर्नुहोस्:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **निर्माण कन्फिगर गर्नुहोस्:**
```bash
cmake -B build
```

3. **प्रोजेक्ट निर्माण गर्नुहोस्:**
```bash
cmake --build build --config Release
```

छिटो कम्पाइलको लागि, समानान्तर कामहरू प्रयोग गर्नुहोस्:
```bash
cmake --build build --config Release -j 8
```

### हार्डवेयर-विशिष्ट निर्माणहरू

#### CUDA समर्थन (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal समर्थन (Apple सिलिकन)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS समर्थन (CPU अनुकूलन)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan समर्थन
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### उन्नत निर्माण विकल्पहरू

#### डिबग निर्माण
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### थप सुविधाहरूको साथ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## मोडेल क्वान्टाइजेशन

### GGUF ढाँचाको समझ

GGUF (Generalized GGML Unified Format) एक अनुकूलित फाइल ढाँचा हो जुन Llama.cpp र अन्य फ्रेमवर्कहरू प्रयोग गरेर ठूलो भाषा मोडेलहरू कुशलतापूर्वक चलाउन डिजाइन गरिएको हो। यसले प्रदान गर्दछ:

- मानकीकृत मोडेल वजन भण्डारण
- प्लेटफर्महरूमा सुधारिएको अनुकूलता
- उन्नत प्रदर्शन
- कुशल मेटाडाटा ह्यान्डलिंग

### क्वान्टाइजेशन प्रकारहरू

Llama.cpp विभिन्न क्वान्टाइजेशन स्तरहरू समर्थन गर्दछ:

| प्रकार | बिट्स | विवरण | प्रयोग केस |
|-------|-------|--------|-----------|
| F16 | 16 | आधा परिशुद्धता | उच्च गुणस्तर, ठूलो स्मृति |
| Q8_0 | 8 | 8-बिट क्वान्टाइजेशन | राम्रो सन्तुलन |
| Q4_0 | 4 | 4-बिट क्वान्टाइजेशन | मध्यम गुणस्तर, सानो आकार |
| Q2_K | 2 | 2-बिट क्वान्टाइजेशन | सबैभन्दा सानो आकार, कम गुणस्तर |

### मोडेल रूपान्तरण

#### PyTorch बाट GGUF मा
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face बाट सीधा डाउनलोड
धेरै मोडेलहरू GGUF ढाँचामा Hugging Face मा उपलब्ध छन्:
- "GGUF" नाम भएका मोडेलहरू खोज्नुहोस्
- उपयुक्त क्वान्टाइजेशन स्तर डाउनलोड गर्नुहोस्
- llama.cpp सँग सिधै प्रयोग गर्नुहोस्

## आधारभूत प्रयोग

### कमाण्ड लाइन इन्टरफेस

#### साधारण पाठ उत्पादन
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face बाट मोडेलहरू प्रयोग गर्दै
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### सर्भर मोड
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### सामान्य प्यारामिटरहरू

| प्यारामिटर | विवरण | उदाहरण |
|-----------|--------|--------|
| `-m` | मोडेल फाइल पथ | `-m model.gguf` |
| `-p` | प्रम्प्ट पाठ | `-p "Hello world"` |
| `-n` | उत्पादन गर्नुपर्ने टोकनहरूको संख्या | `-n 100` |
| `-c` | सन्दर्भ आकार | `-c 4096` |
| `-t` | थ्रेडहरूको संख्या | `-t 8` |
| `-ngl` | GPU तहहरू | `-ngl 32` |
| `-temp` | तापमान | `-temp 0.7` |

### अन्तरक्रियात्मक मोड

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## उन्नत सुविधाहरू

### सर्भर API

#### सर्भर सुरु गर्दै
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API प्रयोग
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### प्रदर्शन अनुकूलन

#### स्मृति व्यवस्थापन
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### बहु-थ्रेडिङ
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU एक्सेलेरेशन
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python एकीकरण

### llama-cpp-python सँग आधारभूत प्रयोग

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### च्याट इन्टरफेस

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### स्ट्रिमिङ प्रतिक्रियाहरू

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain सँग एकीकरण

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## समस्या समाधान

### सामान्य समस्याहरू र समाधानहरू

#### निर्माण त्रुटिहरू

**समस्या: CMake फेला परेन**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**समस्या: कम्पाइलर फेला परेन**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### रनटाइम समस्याहरू

**समस्या: मोडेल लोड गर्न असफल**
- मोडेल फाइल पथ प्रमाणित गर्नुहोस्
- फाइल अनुमति जाँच गर्नुहोस्
- पर्याप्त RAM सुनिश्चित गर्नुहोस्
- विभिन्न क्वान्टाइजेशन स्तरहरू प्रयास गर्नुहोस्

**समस्या: कमजोर प्रदर्शन**
- हार्डवेयर एक्सेलेरेशन सक्षम गर्नुहोस्
- थ्रेड संख्या बढाउनुहोस्
- उपयुक्त क्वान्टाइजेशन प्रयोग गर्नुहोस्
- GPU स्मृति प्रयोग जाँच गर्नुहोस्

#### स्मृति समस्याहरू

**समस्या: स्मृति सकियो**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### प्लेटफर्म-विशिष्ट समस्याहरू

#### Windows
- MinGW वा Visual Studio कम्पाइलर प्रयोग गर्नुहोस्
- उचित PATH कन्फिगरेसन सुनिश्चित गर्नुहोस्
- एन्टिभाइरस हस्तक्षेप जाँच गर्नुहोस्

#### macOS
- Apple सिलिकनको लागि Metal सक्षम गर्नुहोस्
- आवश्यक परेमा अनुकूलताका लागि Rosetta 2 प्रयोग गर्नुहोस्
- Xcode कमाण्ड लाइन उपकरणहरू जाँच गर्नुहोस्

#### Linux
- विकास प्याकेजहरू स्थापना गर्नुहोस्
- GPU ड्राइभर संस्करणहरू जाँच गर्नुहोस्
- CUDA टूलकिट स्थापना प्रमाणित गर्नुहोस्

## सर्वोत्तम अभ्यासहरू

### मोडेल चयन
1. **आफ्नो हार्डवेयरको आधारमा उपयुक्त क्वान्टाइजेशन चयन गर्नुहोस्**
2. **मोडेल आकार बनाम गुणस्तरको व्यापार-अफ विचार गर्नुहोस्**
3. **आफ्नो विशिष्ट प्रयोग केसको लागि विभिन्न मोडेलहरू परीक्षण गर्नुहोस्**

### प्रदर्शन अनुकूलन
1. **GPU एक्सेलेरेशन प्रयोग गर्नुहोस्** जब उपलब्ध छ
2. **आफ्नो CPU को लागि थ्रेड संख्या अनुकूलित गर्नुहोस्**
3. **आफ्नो प्रयोग केसको लागि उपयुक्त सन्दर्भ आकार सेट गर्नुहोस्**
4. **ठूलो मोडेलहरूको लागि स्मृति म्यापिङ सक्षम गर्नुहोस्**

### उत्पादन परिनियोजन
1. **API पहुँचको लागि सर्भर मोड प्रयोग गर्नुहोस्**
2. **उचित त्रुटि ह्यान्डलिङ कार्यान्वयन गर्नुहोस्**
3. **संसाधन प्रयोगको निगरानी गर्नुहोस्**
4. **लगिङ र निगरानी सेटअप गर्नुहोस्**

### विकास कार्यप्रवाह
1. **परीक्षणको लागि साना मोडेलहरूसँग सुरु गर्नुहोस्**
2. **मोडेल कन्फिगरेसनहरूको लागि संस्करण नियन्त्रण प्रयोग गर्नुहोस्**
3. **आफ्नो कन्फिगरेसनहरू दस्तावेज गर्नुहोस्**
4. **विभिन्न प्लेटफर्महरूमा परीक्षण गर्नुहोस्**

### सुरक्षा विचारहरू
1. **इनपुट प्रम्प्टहरू प्रमाणित गर्नुहोस्**
2. **दर सीमितता कार्यान्वयन गर्नुहोस्**
3. **API अन्त बिन्दुहरू सुरक्षित गर्नुहोस्**
4. **दुरुपयोग ढाँचाहरूको निगरानी गर्नुहोस्**

## निष्कर्ष

Llama.cpp ले विभिन्न हार्डवेयर कन्फिगरेसनहरूमा स्थानीय रूपमा ठूलो भाषा मोडेलहरू चलाउन शक्तिशाली र कुशल तरिका प्रदान गर्दछ। चाहे तपाईं AI अनुप्रयोगहरू विकास गर्दै हुनुहुन्छ, अनुसन्धान गर्दै हुनुहुन्छ, वा LLMs संग प्रयोग गर्दै हुनुहुन्छ, यो फ्रेमवर्कले विभिन्न प्रयोग केसहरूको लागि आवश्यक लचिलोपन र प्रदर्शन प्रदान गर्दछ।

मुख्य निष्कर्षहरू:
- आफ्नो आवश्यकताहरूमा फिट हुने स्थापना विधि चयन गर्नुहोस्
- आफ्नो विशिष्ट हार्डवेयर कन्फिगरेसनको लागि अनुकूलन गर्नुहोस्
- आधारभूत प्रयोगबाट सुरु गर्नुहोस् र क्रमिक रूपमा उन्नत सुविधाहरू अन्वेषण गर्नुहोस्
- सजिलो एकीकरणको लागि Python बाइन्डिङहरू विचार गर्नुहोस्
- उत्पादन परिनियोजनहरूको लागि सर्वोत्तम अभ्यासहरू पालना गर्नुहोस्

थप जानकारी र अपडेटहरूको लागि, [आधिकारिक Llama.cpp रिपोजिटरी](https://github.com/ggml-org/llama.cpp) मा जानुहोस् र उपलब्ध विस्तृत दस्तावेजहरू र समुदाय स्रोतहरूलाई सन्दर्भ गर्नुहोस्।

## ➡️ अब के गर्ने

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव सटीकता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।  