<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T21:02:43+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ne"
}
-->
# खण्ड ३ : माइक्रोसफ्ट ओलिभ अप्टिमाइजेसन सुइट

## सामग्री सूची
1. [परिचय](../../../Module04)
2. [माइक्रोसफ्ट ओलिभ के हो?](../../../Module04)
3. [इन्स्टलेसन](../../../Module04)
4. [द्रुत सुरुवात मार्गदर्शन](../../../Module04)
5. [उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने](../../../Module04)
6. [उन्नत प्रयोग](../../../Module04)
7. [सर्वोत्तम अभ्यासहरू](../../../Module04)
8. [समस्या समाधान](../../../Module04)
9. [थप स्रोतहरू](../../../Module04)

## परिचय

माइक्रोसफ्ट ओलिभ एक शक्तिशाली, प्रयोग गर्न सजिलो, हार्डवेयर-अवगत मोडेल अप्टिमाइजेसन उपकरण हो जसले विभिन्न हार्डवेयर प्लेटफर्महरूमा मेसिन लर्निङ मोडेलहरू तैनाथ गर्नको लागि अप्टिमाइजेसन प्रक्रियालाई सरल बनाउँछ। तपाईं CPU, GPU, वा विशेष AI एक्सेलेरेटरहरू लक्षित गर्दै हुनुहुन्छ भने पनि, ओलिभले मोडेलको शुद्धता कायम राख्दै इष्टतम प्रदर्शन प्राप्त गर्न मद्दत गर्दछ।

## माइक्रोसफ्ट ओलिभ के हो?

ओलिभ एक प्रयोग गर्न सजिलो हार्डवेयर-अवगत मोडेल अप्टिमाइजेसन उपकरण हो जसले मोडेल कम्प्रेसन, अप्टिमाइजेसन, र कम्पाइलेसनका लागि उद्योग-अग्रणी प्रविधिहरूलाई संयोजन गर्दछ। यो ONNX Runtime सँग E2E इन्फरेन्स अप्टिमाइजेसन समाधानको रूपमा काम गर्दछ।

### मुख्य विशेषताहरू

- **हार्डवेयर-अवगत अप्टिमाइजेसन**: तपाईंको लक्षित हार्डवेयरका लागि स्वचालित रूपमा उत्तम अप्टिमाइजेसन प्रविधिहरू चयन गर्दछ
- **४०+ बिल्ट-इन अप्टिमाइजेसन कम्पोनेन्टहरू**: मोडेल कम्प्रेसन, क्वान्टाइजेसन, ग्राफ अप्टिमाइजेसन, र थप कभर गर्दछ
- **सजिलो CLI इन्टरफेस**: सामान्य अप्टिमाइजेसन कार्यहरूको लागि सरल आदेशहरू
- **बहु-फ्रेमवर्क समर्थन**: PyTorch, Hugging Face मोडेलहरू, र ONNX सँग काम गर्दछ
- **लोकप्रिय मोडेल समर्थन**: ओलिभले Llama, Phi, Qwen, Gemma जस्ता लोकप्रिय मोडेल आर्किटेक्चरहरूलाई स्वचालित रूपमा अप्टिमाइज गर्न सक्छ

### फाइदाहरू

- **विकास समय घटाउँछ**: विभिन्न अप्टिमाइजेसन प्रविधिहरू म्यानुअल रूपमा परीक्षण गर्न आवश्यक पर्दैन
- **प्रदर्शन सुधार**: महत्वपूर्ण गति सुधार (केही अवस्थामा ६ गुणासम्म)
- **क्रस-प्लेटफर्म तैनाथीकरण**: अप्टिमाइज गरिएको मोडेलहरू विभिन्न हार्डवेयर र अपरेटिङ सिस्टमहरूमा काम गर्छन्
- **शुद्धता कायम राख्छ**: अप्टिमाइजेसनले प्रदर्शन सुधार गर्दा मोडेलको गुणस्तर सुरक्षित राख्छ

## इन्स्टलेसन

### पूर्वआवश्यकताहरू

- Python 3.8 वा उच्च
- pip प्याकेज म्यानेजर
- भर्चुअल वातावरण (सिफारिस गरिन्छ)

### आधारभूत इन्स्टलेसन

भर्चुअल वातावरण सिर्जना र सक्रिय गर्नुहोस्:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ओलिभलाई स्वचालित अप्टिमाइजेसन सुविधाहरूको साथ इन्स्टल गर्नुहोस्:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### वैकल्पिक निर्भरताहरू

ओलिभले थप सुविधाहरूका लागि विभिन्न वैकल्पिक निर्भरताहरू प्रदान गर्दछ:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### इन्स्टलेसन प्रमाणित गर्नुहोस्

```bash
olive --help
```

यदि सफल भयो भने, तपाईंले ओलिभ CLI सहायता सन्देश देख्नुहुनेछ।

## द्रुत सुरुवात मार्गदर्शन

### तपाईंको पहिलो अप्टिमाइजेसन

ओलिभको स्वचालित अप्टिमाइजेसन सुविधाको प्रयोग गरेर सानो भाषा मोडेल अप्टिमाइज गरौं:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### यो आदेशले के गर्छ

अप्टिमाइजेसन प्रक्रियामा समावेश छ: स्थानीय क्यासबाट मोडेल प्राप्त गर्नु, ONNX ग्राफ क्याप्चर गर्नु र तौलहरू ONNX डेटा फाइलमा भण्डारण गर्नु, ONNX ग्राफ अप्टिमाइज गर्नु, र RTN विधि प्रयोग गरेर मोडेललाई int4 मा क्वान्टाइज गर्नु।

### आदेश प्यारामिटरहरूको व्याख्या

- `--model_name_or_path`: Hugging Face मोडेल पहिचानकर्ता वा स्थानीय पथ
- `--output_path`: अप्टिमाइज गरिएको मोडेल सुरक्षित गरिने डाइरेक्टरी
- `--device`: लक्षित उपकरण (cpu, gpu)
- `--provider`: कार्यान्वयन प्रदायक (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: इन्फरेन्सका लागि ONNX Runtime Generate AI प्रयोग गर्नुहोस्
- `--precision`: क्वान्टाइजेसन शुद्धता (int4, int8, fp16)
- `--log_level`: लगिङ्गको स्तर (0=न्यूनतम, 1=विस्तृत)

## उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने

Hugging Face मा दिइएको उदाहरण [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) को आधारमा, Qwen3 मोडेललाई कसरी अप्टिमाइज गर्ने:

### चरण १: मोडेल डाउनलोड गर्नुहोस् (वैकल्पिक)

डाउनलोड समय घटाउन, केवल आवश्यक फाइलहरू क्यास गर्नुहोस्:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### चरण २: Qwen3 मोडेल अप्टिमाइज गर्नुहोस्

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### चरण ३: अप्टिमाइज गरिएको मोडेल परीक्षण गर्नुहोस्

तपाईंको अप्टिमाइज गरिएको मोडेल परीक्षण गर्न एक साधारण Python स्क्रिप्ट बनाउनुहोस्:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### आउटपुट संरचना

अप्टिमाइजेसन पछि, तपाईंको आउटपुट डाइरेक्टरीमा निम्न सामग्री हुनेछ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## उन्नत प्रयोग

### कन्फिगरेसन फाइलहरू

अझ जटिल अप्टिमाइजेसन कार्यप्रवाहहरूको लागि, तपाईं JSON कन्फिगरेसन फाइलहरू प्रयोग गर्न सक्नुहुन्छ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

कन्फिगरेसनको साथ चलाउनुहोस्:

```bash
olive run --config config.json
```

### GPU अप्टिमाइजेसन

CUDA GPU अप्टिमाइजेसनको लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) को लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ओलिभसँग फाइन-ट्युनिङ

ओलिभले मोडेलहरू फाइन-ट्युनिङ गर्न पनि समर्थन गर्दछ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## सर्वोत्तम अभ्यासहरू

### १. मोडेल चयन
- परीक्षणका लागि साना मोडेलहरूबाट सुरु गर्नुहोस् (जस्तै, 0.5B-7B प्यारामिटरहरू)
- सुनिश्चित गर्नुहोस् कि तपाईंको लक्षित मोडेल आर्किटेक्चर ओलिभद्वारा समर्थित छ

### २. हार्डवेयर विचारहरू
- तपाईंको अप्टिमाइजेसन लक्ष्यलाई तपाईंको तैनाथीकरण हार्डवेयरसँग मिलाउनुहोस्
- CUDA-संगत हार्डवेयर भएमा GPU अप्टिमाइजेसन प्रयोग गर्नुहोस्
- Windows मेसिनहरूमा DirectML विचार गर्नुहोस्

### ३. शुद्धता चयन
- **INT4**: अधिकतम कम्प्रेसन, थोरै शुद्धता ह्रास
- **INT8**: आकार र शुद्धताको राम्रो सन्तुलन
- **FP16**: न्यूनतम शुद्धता ह्रास, मध्यम आकार घटाउने

### ४. परीक्षण र मान्यता
- तपाईंको विशेष प्रयोग केसहरूसँग अप्टिमाइज गरिएको मोडेलहरू सधैं परीक्षण गर्नुहोस्
- प्रदर्शन मेट्रिक्स तुलना गर्नुहोस् (लेटेन्सी, थ्रुपुट, शुद्धता)
- मूल्याङ्कनका लागि प्रतिनिधि इनपुट डेटा प्रयोग गर्नुहोस्

### ५. पुनरावृत्त अप्टिमाइजेसन
- छिटो नतिजाहरूका लागि स्वचालित अप्टिमाइजेसनबाट सुरु गर्नुहोस्
- विस्तृत नियन्त्रणका लागि कन्फिगरेसन फाइलहरू प्रयोग गर्नुहोस्
- विभिन्न अप्टिमाइजेसन पासहरूसँग प्रयोग गर्नुहोस्

## समस्या समाधान

### सामान्य समस्याहरू

#### १. इन्स्टलेसन समस्याहरू
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### २. CUDA/GPU समस्याहरू
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ३. मेमोरी समस्याहरू
- अप्टिमाइजेसनको क्रममा साना ब्याच साइजहरू प्रयोग गर्नुहोस्
- पहिले उच्च शुद्धतासहित क्वान्टाइजेसन प्रयास गर्नुहोस् (int8 को सट्टा int4)
- मोडेल क्यासिङका लागि पर्याप्त डिस्क स्पेस सुनिश्चित गर्नुहोस्

#### ४. मोडेल लोडिङ त्रुटिहरू
- मोडेल पथ र पहुँच अनुमति प्रमाणित गर्नुहोस्
- जाँच गर्नुहोस् कि मोडेललाई `trust_remote_code=True` चाहिन्छ कि छैन
- सबै आवश्यक मोडेल फाइलहरू डाउनलोड गरिएको सुनिश्चित गर्नुहोस्

### सहयोग प्राप्त गर्ने

- **डकुमेन्टेसन**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **उदाहरणहरू**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## थप स्रोतहरू

### आधिकारिक लिङ्कहरू
- **GitHub रिपोजिटरी**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime डकुमेन्टेसन**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face उदाहरण**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### समुदायका उदाहरणहरू
- **Jupyter नोटबुकहरू**: ओलिभ GitHub रिपोजिटरीमा उपलब्ध
- **VS Code एक्सटेन्सन**: AI Toolkit एक्सटेन्सनले मोडेल अप्टिमाइजेसनका लागि ओलिभ प्रयोग गर्दछ
- **ब्लग पोस्टहरू**: माइक्रोसफ्ट ओपन सोर्स ब्लगमा विस्तृत ओलिभ ट्युटोरियलहरू छन्

### सम्बन्धित उपकरणहरू
- **ONNX Runtime**: उच्च-प्रदर्शन इन्फरेन्स इन्जिन
- **Hugging Face Transformers**: धेरै उपयुक्त मोडेलहरूको स्रोत
- **Azure Machine Learning**: क्लाउड-आधारित अप्टिमाइजेसन कार्यप्रवाहहरू

## ➡️ अब के गर्ने

- [०४: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।