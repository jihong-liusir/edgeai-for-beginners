<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T17:35:35+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ne"
}
-->
# Section 3 : Microsoft Olive Optimization Suite

## सामग्री सूची
1. [परिचय](../../../Module04)
2. [Microsoft Olive के हो?](../../../Module04)
3. [स्थापना](../../../Module04)
4. [द्रुत सुरुवात मार्गदर्शन](../../../Module04)
5. [उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने](../../../Module04)
6. [उन्नत प्रयोग](../../../Module04)
7. [सर्वोत्तम अभ्यासहरू](../../../Module04)
8. [समस्या समाधान](../../../Module04)
9. [थप स्रोतहरू](../../../Module04)

## परिचय

Microsoft Olive एक शक्तिशाली, प्रयोग गर्न सजिलो हार्डवेयर-उन्मुख मोडेल अनुकूलन उपकरण हो जसले विभिन्न हार्डवेयर प्लेटफर्महरूमा मेसिन लर्निङ मोडेलहरू तैनाथ गर्नको लागि अनुकूलन प्रक्रिया सरल बनाउँछ। चाहे तपाईं CPUs, GPUs, वा विशेष AI एक्सेलेरेटरहरू लक्षित गर्दै हुनुहुन्छ, Olive ले मोडेलको शुद्धता कायम राख्दै उत्कृष्ट प्रदर्शन प्राप्त गर्न मद्दत गर्दछ।

## Microsoft Olive के हो?

Olive एक हार्डवेयर-उन्मुख मोडेल अनुकूलन उपकरण हो जसले मोडेल कम्प्रेसन, अनुकूलन, र कम्पाइलको क्षेत्रमा उद्योग-अग्रणी प्रविधिहरू समेट्छ। यो ONNX Runtime सँग E2E इन्फरेन्स अनुकूलन समाधानको रूपमा काम गर्दछ।

### मुख्य विशेषताहरू

- **हार्डवेयर-उन्मुख अनुकूलन**: तपाईंको लक्षित हार्डवेयरको लागि स्वतः उत्कृष्ट अनुकूलन प्रविधिहरू चयन गर्दछ
- **40+ निर्मित अनुकूलन घटकहरू**: मोडेल कम्प्रेसन, क्वान्टाइजेशन, ग्राफ अनुकूलन, र अन्य समेट्छ
- **सजिलो CLI इन्टरफेस**: सामान्य अनुकूलन कार्यहरूको लागि सरल आदेशहरू
- **बहु-फ्रेमवर्क समर्थन**: PyTorch, Hugging Face मोडेलहरू, र ONNX सँग काम गर्दछ
- **लोकप्रिय मोडेल समर्थन**: Olive ले Llama, Phi, Qwen, Gemma जस्ता लोकप्रिय मोडेल आर्किटेक्चरहरूलाई स्वतः अनुकूलन गर्न सक्छ

### फाइदाहरू

- **विकास समय घटाउँछ**: विभिन्न अनुकूलन प्रविधिहरू म्यानुअल रूपमा परीक्षण गर्न आवश्यक छैन
- **प्रदर्शन सुधार**: महत्वपूर्ण गति सुधार (केही अवस्थामा 6x सम्म)
- **क्रस-प्लेटफर्म तैनाथी**: अनुकूलित मोडेलहरू विभिन्न हार्डवेयर र अपरेटिङ सिस्टमहरूमा काम गर्दछ
- **शुद्धता कायम राख्छ**: अनुकूलनहरूले प्रदर्शन सुधार गर्दै मोडेलको गुणस्तर सुरक्षित राख्छ

## स्थापना

### पूर्वापेक्षाहरू

- Python 3.8 वा उच्च संस्करण
- pip प्याकेज म्यानेजर
- भर्चुअल वातावरण (सिफारिस गरिएको)

### आधारभूत स्थापना

भर्चुअल वातावरण सिर्जना र सक्रिय गर्नुहोस्:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Olive लाई स्वतः अनुकूलन सुविधाहरू सहित स्थापना गर्नुहोस्:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### वैकल्पिक निर्भरताहरू

Olive ले थप सुविधाहरूको लागि विभिन्न वैकल्पिक निर्भरताहरू प्रदान गर्दछ:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### स्थापना पुष्टि गर्नुहोस्

```bash
olive --help
```

यदि सफल भयो भने, तपाईंले Olive CLI सहायता सन्देश देख्नुहुनेछ।

## द्रुत सुरुवात मार्गदर्शन

### तपाईंको पहिलो अनुकूलन

Olive को स्वतः अनुकूलन सुविधा प्रयोग गरेर सानो भाषा मोडेल अनुकूलन गरौं:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### यो आदेशले के गर्छ

अनुकूलन प्रक्रियामा समावेश छ: स्थानीय क्यासबाट मोडेल प्राप्त गर्नु, ONNX ग्राफ कब्जा गर्नु र तौलहरू ONNX डेटा फाइलमा भण्डारण गर्नु, ONNX ग्राफ अनुकूलन गर्नु, र RTN विधि प्रयोग गरेर मोडेललाई int4 मा क्वान्टाइज गर्नु।

### आदेश प्यारामिटरहरूको व्याख्या

- `--model_name_or_path`: Hugging Face मोडेल पहिचानकर्ता वा स्थानीय पथ
- `--output_path`: अनुकूलित मोडेल बचत गरिने निर्देशिका
- `--device`: लक्षित उपकरण (cpu, gpu)
- `--provider`: कार्यान्वयन प्रदायक (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI प्रयोग गरेर इन्फरेन्स गर्नुहोस्
- `--precision`: क्वान्टाइजेशन शुद्धता (int4, int8, fp16)
- `--log_level`: लगिङ्गको स्तर (0=न्यूनतम, 1=विस्तृत)

## उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने

Hugging Face मा प्रदान गरिएको उदाहरण [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) को आधारमा, यहाँ Qwen3 मोडेल अनुकूलन गर्ने तरिका छ:

### चरण 1: मोडेल डाउनलोड गर्नुहोस् (वैकल्पिक)

डाउनलोड समय कम गर्न, केवल आवश्यक फाइलहरू क्यास गर्नुहोस्:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### चरण 2: Qwen3 मोडेल अनुकूलन गर्नुहोस्

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### चरण 3: अनुकूलित मोडेल परीक्षण गर्नुहोस्

अनुकूलित मोडेल परीक्षण गर्न एक साधारण Python स्क्रिप्ट बनाउनुहोस्:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### आउटपुट संरचना

अनुकूलन पछि, तपाईंको आउटपुट निर्देशिकामा समावेश हुनेछ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## उन्नत प्रयोग

### कन्फिगरेसन फाइलहरू

अधिक जटिल अनुकूलन कार्यप्रवाहहरूको लागि, तपाईं JSON कन्फिगरेसन फाइलहरू प्रयोग गर्न सक्नुहुन्छ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

कन्फिगरेसनको साथ चलाउनुहोस्:

```bash
olive run --config config.json
```

### GPU अनुकूलन

CUDA GPU अनुकूलनको लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) को लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive सँग फाइन-ट्युनिङ

Olive ले मोडेलहरू फाइन-ट्युनिङ गर्न पनि समर्थन गर्दछ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## सर्वोत्तम अभ्यासहरू

### 1. मोडेल चयन
- परीक्षणको लागि साना मोडेलहरूबाट सुरु गर्नुहोस् (जस्तै, 0.5B-7B प्यारामिटरहरू)
- सुनिश्चित गर्नुहोस् कि तपाईंको लक्षित मोडेल आर्किटेक्चर Olive द्वारा समर्थित छ

### 2. हार्डवेयर विचारहरू
- तपाईंको अनुकूलन लक्ष्यलाई तपाईंको तैनाथी हार्डवेयरसँग मिलाउनुहोस्
- CUDA-संगत हार्डवेयर भएमा GPU अनुकूलन प्रयोग गर्नुहोस्
- Windows मेसिनहरूमा DirectML विचार गर्नुहोस्

### 3. शुद्धता चयन
- **INT4**: अधिकतम कम्प्रेसन, थोरै शुद्धता ह्रास
- **INT8**: आकार र शुद्धताको राम्रो सन्तुलन
- **FP16**: न्यूनतम शुद्धता ह्रास, मध्यम आकार घटाउने

### 4. परीक्षण र मान्यता
- अनुकूलित मोडेलहरू तपाईंको विशिष्ट प्रयोग केसहरूसँग परीक्षण गर्नुहोस्
- प्रदर्शन मेट्रिक्स तुलना गर्नुहोस् (लेटेन्सी, थ्रुपुट, शुद्धता)
- मूल्याङ्कनको लागि प्रतिनिधि इनपुट डेटा प्रयोग गर्नुहोस्

### 5. पुनरावृत्त अनुकूलन
- छिटो नतिजाको लागि स्वतः अनुकूलनबाट सुरु गर्नुहोस्
- सूक्ष्म नियन्त्रणको लागि कन्फिगरेसन फाइलहरू प्रयोग गर्नुहोस्
- विभिन्न अनुकूलन पासहरूसँग प्रयोग गर्नुहोस्

## समस्या समाधान

### सामान्य समस्याहरू

#### 1. स्थापना समस्याहरू
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU समस्याहरू
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. मेमोरी समस्याहरू
- अनुकूलनको क्रममा साना ब्याच साइजहरू प्रयोग गर्नुहोस्
- पहिले उच्च शुद्धतासँग क्वान्टाइजेशन प्रयास गर्नुहोस् (int8 को सट्टा int4)
- मोडेल क्यासिङको लागि पर्याप्त डिस्क स्पेस सुनिश्चित गर्नुहोस्

#### 4. मोडेल लोडिङ त्रुटिहरू
- मोडेल पथ र पहुँच अनुमति जाँच गर्नुहोस्
- मोडेललाई `trust_remote_code=True` आवश्यक छ कि छैन जाँच गर्नुहोस्
- सबै आवश्यक मोडेल फाइलहरू डाउनलोड गरिएको सुनिश्चित गर्नुहोस्

### सहयोग प्राप्त गर्नुहोस्

- **डकुमेन्टेसन**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **उदाहरणहरू**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## थप स्रोतहरू

### आधिकारिक लिंकहरू
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### समुदाय उदाहरणहरू
- **Jupyter Notebooks**: Olive GitHub Repository मा उपलब्ध — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: VS Code को लागि AI Toolkit अवलोकन — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ब्लग पोस्टहरू**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### सम्बन्धित उपकरणहरू
- **ONNX Runtime**: उच्च प्रदर्शन इन्फरेन्स इन्जिन — https://onnxruntime.ai/
- **Hugging Face Transformers**: धेरै उपयुक्त मोडेलहरूको स्रोत — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: क्लाउड-आधारित अनुकूलन कार्यप्रवाहहरू — https://learn.microsoft.com/azure/machine-learning/

## ➡️ अब के गर्ने

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

