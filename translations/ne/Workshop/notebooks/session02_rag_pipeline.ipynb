{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeb64bf",
   "metadata": {},
   "source": [
    "# सत्र २ – न्यूनतम RAG पाइपलाइन\n",
    "\n",
    "Foundry Local + sentence-transformers embeddings प्रयोग गरेर हल्का Retrieval-Augmented Generation पाइपलाइन निर्माण गर्नुहोस्।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a46eea",
   "metadata": {},
   "source": [
    "### व्याख्या: निर्भरता स्थापना\n",
    "यो पाइपलाइनका लागि न्यूनतम प्याकेजहरू स्थापना गर्दछ:\n",
    "- `foundry-local-sdk` स्थानीय मोडेल व्यवस्थापनका लागि (यदि शुद्ध BASE_URL पथ प्रयोग गरिँदैन भने)।\n",
    "- `openai` उपयुक्त SDK संरचनाहरूका लागि (केही उपयोगिताहरू)।\n",
    "- `sentence-transformers` एम्बेडिङका लागि।\n",
    "- `numpy` भेक्टर गणितका लागि।\n",
    "पुन: चलाउन सुरक्षित; यदि वातावरण पहिले नै सन्तुष्ट छ भने छोड्न सकिन्छ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857bce7",
   "metadata": {},
   "source": [
    "# परिदृश्य\n",
    "यो नोटबुकले पूर्ण रूपमा स्थानीय रूपमा चल्ने न्यूनतम Retrieval-Augmented Generation (RAG) पाइपलाइन निर्माण गर्दछ:\n",
    "- Foundry Local मोडेलसँग जडान गर्दछ (SDK वा BASE_URL मार्फत स्वचालित रूपमा पत्ता लगाउँछ)।\n",
    "- सानो इन-मेमोरी कागजात संग्रह सिर्जना गर्दछ र यसलाई Sentence Transformers को प्रयोग गरेर एम्बेड गर्दछ।\n",
    "- पारदर्शिताका लागि साधारण भेक्टर समानता पुनःप्राप्ति (बाह्य इन्डेक्स बिना) कार्यान्वयन गर्दछ।\n",
    "- धेरै HTTP फलब्याक मार्गहरू (`/v1/chat/completions`, `/v1/completions`, `/v1/responses`) मार्फत आधारभूत जेनेरेसन अनुरोधहरू जारी गर्दछ।\n",
    "- `answer()` सहायक प्रदान गर्दछ, जसले प्रारम्भिक प्रयासहरू असफल हुँदा वैकल्पिक मोडेल रूपहरू पुन: प्रयास गर्दछ।\n",
    "\n",
    "ठूलो कर्पस, स्थायी भेक्टर स्टोरहरू, वा मूल्याङ्कन मेट्रिक्समा विस्तार गर्नु अघि यसलाई एक डायग्नोस्टिक टेम्प्लेटको रूपमा प्रयोग गर्नुहोस् (RAG मूल्याङ्कन नोटबुक हेर्नुहोस्)।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2c5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed3131",
   "metadata": {},
   "source": [
    "### व्याख्या: कोर आयातहरू\n",
    "एम्बेडिङ + स्थानीय अनुमानको लागि आवश्यक कोर पुस्तकालयहरू लोड गर्दछ:\n",
    "- SentenceTransformer घनत्व भेक्टर एम्बेडिङको लागि।\n",
    "- FoundryLocalManager (वैकल्पिक) स्थानीय सेवा व्यवस्थापन गर्न।\n",
    "- OpenAI क्लाइन्ट परिचित वस्तु आकारहरूको लागि (यद्यपि पछि हामी HTTP सिधै प्रयोग गर्छौं)।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf135fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725143b8",
   "metadata": {},
   "source": [
    "### व्याख्या: खेलौना दस्तावेज संग्रह\n",
    "डोमेन कथनहरूको सानो इन-मेमोरी सूची परिभाषित गर्दछ। पुनरावृत्ति छिटो र नियन्त्रणमा राख्छ ताकि ध्यान पाइपलाइन यान्त्रिकी (पुनःप्राप्ति + ग्राउन्डिङ) मा रहोस्, डाटा व्यवस्थापनमा होइन।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6d493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    "    'Foundry Local provides an OpenAI-compatible local inference endpoint.',\n",
    "    'Retrieval Augmented Generation improves answer grounding by injecting relevant context.',\n",
    "    'Edge AI reduces latency and preserves privacy via local execution.',\n",
    "    'Small Language Models can offer competitive quality with lower resource usage.',\n",
    "    'Vector similarity search retrieves semantically relevant documents.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e66b3",
   "metadata": {},
   "source": [
    "### व्याख्या: जडान, मोडेल चयन र एम्बेडिङ सुरुवात\n",
    "मजबुत जडान तर्क:\n",
    "1. वैकल्पिक रूपमा स्पष्ट `BASE_URL` (शुद्ध HTTP पथ) प्रयोग गर्दछ, अन्यथा FoundryLocalManager मा फर्किन्छ।\n",
    "2. `/v1/models` मा जाँच गर्दछ र सबैभन्दा उपयुक्त ठोस मोडेल आईडी चयन गर्दछ (ठ्याक्कै उपनाम > क्यानोनिकल परिवार > पहिलो उपलब्ध)।\n",
    "3. पुन: प्रयास लूप लागू गर्दछ, जसमा `FOUNDRY_CONNECT_RETRIES` र ढिलाइ समायोज्य छ।\n",
    "4. टय खेल सामग्रीको लागि SentenceTransformer एम्बेडिङ (सामान्यीकृत भेक्टरहरू) सुरुवात गर्दछ।\n",
    "5. पुन: उत्पादनयोग्यता सुनिश्चित गर्न OpenAI SDK संस्करण समेट्छ।\n",
    "यदि सेवा अनुपस्थित छ भने, क्र्यास नगरी यसलाई सुरु गर्न मार्गदर्शन प्रिन्ट गर्दछ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae7f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Foundry Local manager endpoint: http://127.0.0.1:59778/v1 | base=http://127.0.0.1:59778 | alias=phi-4-mini\n",
      "[OK] Model resolved: deepseek-r1-distill-qwen-7b-cuda-gpu:0 (total_models=11)\n",
      "[OK] Embedded 5 docs using sentence-transformers/all-MiniLM-L6-v2 shape=(5, 384)\n",
      "OpenAI SDK version: 1.109.1\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, requests, re\n",
    "# Native Foundry Local SDK preferred; fall back to explicit BASE_URL if provided\n",
    "os.environ.setdefault('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini')\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS', os.getenv('TARGET_MODEL', 'phi-4-mini'))\n",
    "base_url_env = os.getenv('BASE_URL', '').strip()\n",
    "manager = None\n",
    "client = None\n",
    "endpoint = None\n",
    "\n",
    "def _canonicalize(model_id: str) -> str:\n",
    "    \"\"\"Remove CUDA suffix and version tags from model name.\"\"\"\n",
    "    b = model_id.split(':')[0]\n",
    "    return re.sub(r'-cuda.*', '', b)\n",
    "\n",
    "try:\n",
    "    if base_url_env:\n",
    "        # Allow user override; normalize by removing trailing / and optional /v1\n",
    "        root = base_url_env.rstrip('/')\n",
    "        if root.endswith('/v1'):\n",
    "            root = root[:-3]\n",
    "        endpoint = root\n",
    "        print(f'[INFO] Using explicit BASE_URL override: {endpoint}')\n",
    "    else:\n",
    "        from foundry_local import FoundryLocalManager\n",
    "        manager = FoundryLocalManager(alias)\n",
    "        # Manager endpoint already includes /v1 - remove it for our base\n",
    "        raw_endpoint = manager.endpoint.rstrip('/')\n",
    "        if raw_endpoint.endswith('/v1'):\n",
    "            endpoint = raw_endpoint[:-3]\n",
    "        else:\n",
    "            endpoint = raw_endpoint\n",
    "        print(f'[OK] Foundry Local manager endpoint: {manager.endpoint} | base={endpoint} | alias={alias}')\n",
    "    \n",
    "    # Probe models list (endpoint does NOT include /v1 here)\n",
    "    models_resp = requests.get(endpoint + '/v1/models', timeout=5)\n",
    "    models_resp.raise_for_status()\n",
    "    payload = models_resp.json() if models_resp.headers.get('content-type','').startswith('application/json') else {}\n",
    "    data = payload.get('data', []) if isinstance(payload, dict) else []\n",
    "    ids = [m.get('id') for m in data if isinstance(m, dict)]\n",
    "    \n",
    "    # Select best matching model\n",
    "    chosen = None\n",
    "    if alias in ids:\n",
    "        chosen = alias\n",
    "    else:\n",
    "        for mid in ids:\n",
    "            if _canonicalize(mid) == _canonicalize(alias):\n",
    "                chosen = mid\n",
    "                break\n",
    "    if not chosen and ids:\n",
    "        chosen = ids[0]\n",
    "    model_name = chosen or alias\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    from openai import OpenAI as _OpenAI\n",
    "    client = _OpenAI(\n",
    "        base_url=endpoint + '/v1',  # OpenAI client needs full base URL with /v1\n",
    "        api_key=(getattr(manager, 'api_key', None) or os.getenv('API_KEY') or 'not-needed')\n",
    "    )\n",
    "    print(f'[OK] Model resolved: {model_name} (total_models={len(ids)})')\n",
    "except Exception as e:\n",
    "    print('[ERROR] Failed to initialize Foundry Local client:', e)\n",
    "    client = None\n",
    "    model_name = alias\n",
    "\n",
    "# Expose BASE for downstream compatibility (without /v1)\n",
    "BASE = endpoint\n",
    "\n",
    "# Embeddings setup\n",
    "embed_model_name = os.getenv('EMBED_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer(embed_model_name)\n",
    "    doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f'[OK] Embedded {len(DOCS)} docs using {embed_model_name} shape={doc_emb.shape}')\n",
    "except Exception as e:\n",
    "    print('[ERROR] Embedding init failed:', e)\n",
    "    embedder = None\n",
    "    doc_emb = None\n",
    "\n",
    "try:\n",
    "    import openai as _openai\n",
    "    openai_version = getattr(_openai, '__version__', 'unknown')\n",
    "    print('OpenAI SDK version:', openai_version)\n",
    "except Exception:\n",
    "    openai_version = 'unknown'\n",
    "\n",
    "if client is None:\n",
    "    print('\\nNEXT: Start/verify service then re-run this cell:')\n",
    "    print('  foundry service start')\n",
    "    print('  foundry model run phi-4-mini')\n",
    "    print('  (optional) set BASE_URL=http://127.0.0.1:57127')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6537337",
   "metadata": {},
   "source": [
    "### व्याख्या: Retrieve Function (Vector Similarity)\n",
    "`retrieve(query, k=3)` ले सोधपुछलाई एन्कोड गर्छ, कोसाइन समानता (सामान्यीकृत भेक्टरहरूमा डट प्रोडक्ट) गणना गर्छ, र शीर्ष-k डक इंडेक्सहरू फर्काउँछ। यो पारदर्शिताका लागि न्यूनतम र इन-मेमोरीमा रहन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373d8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return sims.argsort()[::-1][:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42bed1",
   "metadata": {},
   "source": [
    "### व्याख्या: SDK-आधारित जेनेरेसन र उत्तर सहायक\n",
    "Foundry Local SDK र OpenAI-सँग मिल्दो क्लाइन्ट विधिहरू प्रयोग गरेर पुनःनिर्माण गरिएको छ, जसले कच्चा HTTP पोस्टहरूको सट्टा प्रयोग गर्दछ:\n",
    "- प्राथमिक मार्ग: `client.chat.completions.create` (संरचित सन्देशहरू)।\n",
    "- वैकल्पिक उपायहरू: `client.completions.create` (पुरानो प्रम्प्ट) त्यसपछि `client.responses.create` (सरलीकृत प्रतिक्रिया API)।\n",
    "- वैकल्पिक मोडेल आईडीहरू (RAW बनाम हटाइएको ALT) सामान्यीकरण गरेर अनुकूलता विस्तार गर्दछ।\n",
    "- `answer()` ले शीर्ष-k पुनःप्राप्त कागजातहरूबाट आधारित प्रम्प्ट निर्माण गर्दछ र प्रयासहरूको क्रमबद्ध ट्रेसहरू रेकर्ड गर्दछ।\n",
    "यसले तर्कलाई पठनीय राख्छ जबकि OpenAI-सँग मिल्दो अन्तर्क्रियात्मक बिन्दुहरूको विकास हुँदै गर्दा सहज रूपमा कार्य गर्न सक्षम बनाउँछ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526983ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SDK generation mode active.\n",
      "       RAW_MODEL = deepseek-r1-distill-qwen-7b-cuda-gpu:0\n",
      "       ALT_MODEL = deepseek-r1-distill-qwen-7b\n"
     ]
    }
   ],
   "source": [
    "# SDK-based generation (Foundry Local manager + OpenAI client methods)\n",
    "import re, time, json\n",
    "\n",
    "def _strip_model_name(name: str) -> str:\n",
    "    \"\"\"Strip CUDA suffix and version tags from model name.\"\"\"\n",
    "    base = name.split(':')[0]\n",
    "    base = re.sub(r'-cuda.*', '', base)\n",
    "    return base\n",
    "\n",
    "# Use the actual resolved model name from connection cell\n",
    "RAW_MODEL = model_name\n",
    "ALT_MODEL = _strip_model_name(RAW_MODEL)\n",
    "\n",
    "def _try_via_client(messages, prompt, model_id: str, max_tokens=220, temperature=0.2):\n",
    "    \"\"\"Try generating response using OpenAI client with multiple fallback routes.\"\"\"\n",
    "    attempts = []\n",
    "    \n",
    "    # 1. Try chat.completions endpoint (preferred for chat models)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model_id, \n",
    "            messages=messages, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        content = resp.choices[0].message.content\n",
    "        attempts.append(('chat.completions', 200, (content or '')[:160]))\n",
    "        if content and content.strip():\n",
    "            return content, attempts\n",
    "    except Exception as e:\n",
    "        attempts.append(('chat.completions', None, str(e)[:160]))\n",
    "    \n",
    "    # 2. Try legacy completions endpoint\n",
    "    try:\n",
    "        comp = client.completions.create(\n",
    "            model=model_id, \n",
    "            prompt=prompt, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        txt = comp.choices[0].text if comp.choices else ''\n",
    "        attempts.append(('completions', 200, (txt or '')[:160]))\n",
    "        if txt and txt.strip():\n",
    "            return txt, attempts\n",
    "    except Exception as e:\n",
    "        attempts.append(('completions', None, str(e)[:160]))\n",
    "    \n",
    "    return None, attempts\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    \"\"\"Retrieve top-k most similar documents using cosine similarity.\"\"\"\n",
    "    if embedder is None or doc_emb is None:\n",
    "        raise RuntimeError(\"Embeddings not initialized.\")\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True)[0]\n",
    "    scores = doc_emb @ q_emb\n",
    "    idxs = np.argsort(scores)[::-1][:k]\n",
    "    return idxs\n",
    "\n",
    "def answer(query, k=3, max_tokens=220, temperature=0.2, try_alternate=True):\n",
    "    \"\"\"\n",
    "    Answer a query using RAG pipeline:\n",
    "    1. Retrieve relevant documents using vector similarity\n",
    "    2. Generate grounded response using Foundry Local model via OpenAI SDK\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        k: Number of documents to retrieve\n",
    "        max_tokens: Maximum tokens for generation\n",
    "        temperature: Sampling temperature\n",
    "        try_alternate: Whether to try alternate model name on failure\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with query, answer, docs, context, route, and tried attempts\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError('Model client not initialized. Re-run connection cell after starting Foundry Local.')\n",
    "    if embedder is None or doc_emb is None:\n",
    "        raise RuntimeError('Embeddings not initialized.')\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    idxs = retrieve(query, k=k)\n",
    "    context = '\\n'.join(f'Doc {i}: {DOCS[i]}' for i in idxs)\n",
    "    \n",
    "    # Construct grounded generation prompt\n",
    "    system_content = 'Use ONLY provided context. If insufficient, say \"I\\'m not sure.\"'\n",
    "    user_content = f'Context:\\n{context}\\n\\nQuestion: {query}'\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_content},\n",
    "        {'role': 'user', 'content': user_content}\n",
    "    ]\n",
    "    prompt = f'System: {system_content}\\n{user_content}\\nAnswer:'\n",
    "    \n",
    "    # Try generation with primary model\n",
    "    tried = []\n",
    "    ans, attempts = _try_via_client(messages, prompt, RAW_MODEL, max_tokens=max_tokens, temperature=temperature)\n",
    "    tried.append({'model': RAW_MODEL, 'attempts': attempts})\n",
    "    \n",
    "    if ans and ans.strip():\n",
    "        return {\n",
    "            'query': query, \n",
    "            'answer': ans.strip(), \n",
    "            'docs': idxs.tolist(), \n",
    "            'context': context, \n",
    "            'route': 'chat-first', \n",
    "            'tried': tried\n",
    "        }\n",
    "    \n",
    "    # Try alternate model name if available\n",
    "    if try_alternate and ALT_MODEL != RAW_MODEL:\n",
    "        ans2, attempts2 = _try_via_client(messages, prompt, ALT_MODEL, max_tokens=max_tokens, temperature=temperature)\n",
    "        tried.append({'model': ALT_MODEL, 'attempts': attempts2})\n",
    "        if ans2 and ans2.strip():\n",
    "            return {\n",
    "                'query': query, \n",
    "                'answer': ans2.strip(), \n",
    "                'docs': idxs.tolist(), \n",
    "                'context': context, \n",
    "                'route': 'chat-alt', \n",
    "                'tried': tried\n",
    "            }\n",
    "    \n",
    "    # All routes failed\n",
    "    return {\n",
    "        'query': query, \n",
    "        'answer': 'I\\'m not sure. (All SDK routes failed)', \n",
    "        'docs': idxs.tolist(), \n",
    "        'context': context, \n",
    "        'route': 'failed', \n",
    "        'tried': tried\n",
    "    }\n",
    "\n",
    "print('[INFO] SDK generation mode active.')\n",
    "print(f'       RAW_MODEL = {RAW_MODEL}')\n",
    "print(f'       ALT_MODEL = {ALT_MODEL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19a21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alt_model': 'deepseek-r1-distill-qwen-7b',\n",
      " 'answer_preview': 'Okay, so I need to figure out why someone would use '\n",
      "                   'Retrieval Augmented Generation (RAG) with local inference. '\n",
      "                   'Let me start by understanding each part of the qu',\n",
      " 'base': 'http://127.0.0.1:59778',\n",
      " 'raw_model': 'deepseek-r1-distill-qwen-7b-cuda-gpu:0',\n",
      " 'retrieved_indices': [0, 3, 1],\n",
      " 'route': 'chat-first'}\n"
     ]
    }
   ],
   "source": [
    "# Self-test cell: validates connectivity, embeddings, and answer() basic functionality (SDK mode)\n",
    "import math, pprint\n",
    "\n",
    "def rag_self_test(sample_query: str = 'Why use RAG with local inference?', expect_docs: int = 3):\n",
    "    report = {'base': BASE, 'raw_model': RAW_MODEL, 'alt_model': ALT_MODEL}\n",
    "    if not BASE:\n",
    "        report['error'] = 'BASE not resolved'\n",
    "        return report\n",
    "    if embedder is None or doc_emb is None:\n",
    "        report['error'] = 'Embeddings not initialized'\n",
    "        return report\n",
    "    if getattr(doc_emb, 'shape', (0,))[0] != len(DOCS):\n",
    "        report['warning_embeddings'] = f\"doc_emb count {getattr(doc_emb,'shape',('?'))} mismatch DOCS {len(DOCS)}\"\n",
    "    try:\n",
    "        idxs = retrieve(sample_query, k=expect_docs)\n",
    "        report['retrieved_indices'] = idxs.tolist() if hasattr(idxs, 'tolist') else list(idxs)\n",
    "    except Exception as e:\n",
    "        report['error_retrieve'] = str(e)\n",
    "        return report\n",
    "    try:\n",
    "        ans = answer(sample_query, k=expect_docs, max_tokens=80, temperature=0.2)\n",
    "        report['route'] = ans.get('route')\n",
    "        report['answer_preview'] = ans.get('answer','')[:160]\n",
    "        if ans.get('route') == 'failed':\n",
    "            report['warning_generation'] = 'All SDK routes failed for sample query'\n",
    "    except Exception as e:\n",
    "        report['error_generation'] = str(e)\n",
    "    return report\n",
    "\n",
    "pprint.pprint(rag_self_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa474",
   "metadata": {},
   "source": [
    "### व्याख्या: ब्याच क्वेरी स्मोक टेस्ट\n",
    "`answer()` मार्फत केही प्रतिनिधि प्रयोगकर्ता प्रश्नहरू चलाएर निम्न कुराहरू प्रमाणित गरिन्छ:\n",
    "- पुनःप्राप्ति सूचकांकहरू सम्भावित समर्थन गर्ने कागजातहरूसँग मेल खान्छन्।\n",
    "- फलब्याक राउटिङ काम गर्छ (राउट मान 'failed' छैन)।\n",
    "- उत्तरहरूले आधारभूत निर्देशनको पालना गर्छन् (कुनै भ्रम सिर्जना गर्दैनन्)।\n",
    "अन्तिम परिणाम वस्तुलाई अनियमित निरीक्षणको लागि कैद गरिन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c38a64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Why use RAG with local inference?\n",
      "A: Okay, so I need to figure out why someone would use Retrieval Augmented Generation (RAG) with local inference. Let me start by understanding each part of the question.\n",
      "\n",
      "First, RAG. From the context given, Doc 1 says that RAG improves answer grounding by injecting relevant context. So RAG is a method that uses retrieval techniques to find the most relevant parts of a document or corpus to augment the generation process. This probably helps in making the generated answers more accurate because they're backed by real data.\n",
      "\n",
      "Then, local inference. Doc 0 mentions that Foundry Local provides an OpenAI-compatible local inference endpoint. So local inference means running the model on the user's device rather than sending the request to a remote server. This is good for privacy and reducing latency, but it might have limitations in terms of model size or capabilities compared to cloud-based options.\n",
      "\n",
      "Now, combining RAG with local inference. The context says that small language models can offer competitive quality with lower resource usage (Doc 3). So using a smaller model might be more efficient, but maybe RAG\n",
      "Docs: [0, 3, 1]\n",
      "---\n",
      "Q: What does vector similarity search do?\n",
      "A: Okay, so I need to figure out what vector similarity search does based on the provided context. Let me start by looking at the given documents.\n",
      "\n",
      "Doc 4 says, \"Vector similarity search retrieves semantically relevant documents.\" Hmm, that seems straightforward. So vector similarity search is a method that retrieves documents based on their semantic relevance. It probably uses some kind of vector representation for the documents, maybe through techniques like word embeddings or TF-IDF vectors. Then, when a query comes in, it's converted into a vector, and the system finds documents with similar vectors, meaning they share similar meanings or topics.\n",
      "\n",
      "I should make sure I'm not missing anything. The other documents mention edge AI and retrieval augmented generation, but they don't directly relate to vector similarity search. So, the key point here is that it retrieves semantically relevant documents. That makes sense because vector-based methods are commonly used in information retrieval to find documents that are not just syntactically similar but actually about the same topic.\n",
      "\n",
      "I think that's the main idea. Vector similarity search focuses on semantic relevance, using vector representations to\n",
      "Docs: [4, 2, 1]\n",
      "---\n",
      "Q: Explain privacy benefits.\n",
      "A: Okay, so I need to explain the privacy benefits mentioned in the provided context. Let me look at the context again. The context includes three documents:\n",
      "\n",
      "Doc 2 says Edge AI reduces latency and preserves privacy via local execution.\n",
      "Doc 3 mentions Small Language Models can offer competitive quality with lower resource usage.\n",
      "Doc 1 states Retrieval Augmented Generation improves answer grounding by injecting relevant context.\n",
      "\n",
      "The question is about explaining the privacy benefits. So, I should focus on the parts of the context that talk about privacy. \n",
      "\n",
      "Looking at Doc 2, it mentions Edge AI reduces latency and preserves privacy via local execution. That seems directly related to privacy. I think \"local execution\" means that the AI processes data on the device itself rather than sending it to a server. This could mean that data doesn't have to be transmitted, which might help protect user privacy because it avoids centralizing data that could be intercepted.\n",
      "\n",
      "Doc 3 talks about small language models with lower resource usage. I'm not sure how this relates to privacy. It might be more about efficiency rather than privacy benefits.\n",
      "\n",
      "Doc 1\n",
      "Docs: [2, 3, 1]\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain privacy benefits.',\n",
       " 'answer': 'Okay, so I need to explain the privacy benefits mentioned in the provided context. Let me look at the context again. The context includes three documents:\\n\\nDoc 2 says Edge AI reduces latency and preserves privacy via local execution.\\nDoc 3 mentions Small Language Models can offer competitive quality with lower resource usage.\\nDoc 1 states Retrieval Augmented Generation improves answer grounding by injecting relevant context.\\n\\nThe question is about explaining the privacy benefits. So, I should focus on the parts of the context that talk about privacy. \\n\\nLooking at Doc 2, it mentions Edge AI reduces latency and preserves privacy via local execution. That seems directly related to privacy. I think \"local execution\" means that the AI processes data on the device itself rather than sending it to a server. This could mean that data doesn\\'t have to be transmitted, which might help protect user privacy because it avoids centralizing data that could be intercepted.\\n\\nDoc 3 talks about small language models with lower resource usage. I\\'m not sure how this relates to privacy. It might be more about efficiency rather than privacy benefits.\\n\\nDoc 1',\n",
       " 'docs': [2, 3, 1],\n",
       " 'context': 'Doc 2: Edge AI reduces latency and preserves privacy via local execution.\\nDoc 3: Small Language Models can offer competitive quality with lower resource usage.\\nDoc 1: Retrieval Augmented Generation improves answer grounding by injecting relevant context.',\n",
       " 'route': 'chat-first',\n",
       " 'tried': [{'model': 'deepseek-r1-distill-qwen-7b-cuda-gpu:0',\n",
       "   'attempts': [('chat.completions',\n",
       "     200,\n",
       "     'Okay, so I need to explain the privacy benefits mentioned in the provided context. Let me look at the context again. The context includes three documents:\\n\\nDoc ')]}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test queries\n",
    "\n",
    "queries = [\n",
    "\n",
    "    \"Why use RAG with local inference?\",\n",
    "\n",
    "    \"What does vector similarity search do?\",\n",
    "\n",
    "    \"Explain privacy benefits.\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "last_result = None\n",
    "\n",
    "for q in queries:\n",
    "\n",
    "    try:\n",
    "\n",
    "        r = answer(q)\n",
    "\n",
    "        last_result = r\n",
    "\n",
    "        print(f\"Q: {q}\\nA: {r['answer']}\\nDocs: {r['docs']}\\n---\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Failed answering '{q}': {e}\")\n",
    "\n",
    "\n",
    "\n",
    "last_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8282fc",
   "metadata": {},
   "source": [
    "### व्याख्या: एकल उत्तर सुविधा कल\n",
    "सजिलो प्रतिलिपि/पेस्ट प्रयोग वा पछि सन्दर्भको लागि अन्तिम छिटो एकल-प्रश्न कल। `answer()` को पूर्व-तयारी सोधपुछहरू पछि पुनः प्रयोगको लागि समान परिणाम सुनिश्चित गर्ने प्रदर्शन गर्दछ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d25165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Why use RAG with local inference?',\n",
       " 'answer': \"Okay, so I need to figure out why someone would use Retrieval Augmented Generation (RAG) with local inference. Let me start by understanding each part of the question.\\n\\nFirst, RAG. From the context given, Doc 1 says that RAG improves answer grounding by injecting relevant context. So RAG is a method that uses retrieval techniques to find the most relevant parts of a document or corpus to augment the generation process. This probably helps in making the generated answers more accurate because they're backed by real data.\\n\\nThen, local inference. Doc 0 mentions that Foundry Local provides an OpenAI-compatible local inference endpoint. So local inference means running the model on the user's device rather than sending the request to a remote server. This is good for privacy and reducing latency, but it might have limitations in terms of model size or capabilities compared to cloud-based options.\\n\\nNow, combining RAG with local inference. The context says that small language models can offer competitive quality with lower resource usage (Doc 3). So using a smaller model might be more efficient, but maybe RAG\",\n",
       " 'docs': [0, 3, 1],\n",
       " 'context': 'Doc 0: Foundry Local provides an OpenAI-compatible local inference endpoint.\\nDoc 3: Small Language Models can offer competitive quality with lower resource usage.\\nDoc 1: Retrieval Augmented Generation improves answer grounding by injecting relevant context.',\n",
       " 'route': 'chat-first',\n",
       " 'tried': [{'model': 'deepseek-r1-distill-qwen-7b-cuda-gpu:0',\n",
       "   'attempts': [('chat.completions',\n",
       "     200,\n",
       "     'Okay, so I need to figure out why someone would use Retrieval Augmented Generation (RAG) with local inference. Let me start by understanding each part of the qu')]}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = answer('Why use RAG with local inference?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी यथासम्भव शुद्धता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। मूल दस्तावेज़ यसको मातृभाषामा आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "9f009936a177f8847d250fe1771d25f6",
   "translation_date": "2025-10-09T09:56:22+00:00",
   "source_file": "Workshop/notebooks/session02_rag_pipeline.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}