{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# рд╕рддреНрд░ рек тАУ SLM рдмрдирд╛рдо LLM рддреБрд▓рдирд╛\n",
    "\n",
    "рдПрдХ рд╕рд╛рдиреЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ рд░ Foundry Local рдорд╛рд░реНрдлрдд рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдареВрд▓реЛ рдореЛрдбреЗрд▓рдХреЛ рдмреАрдЪрдорд╛ рд╡рд┐рд▓рдореНрдмрддрд╛ рд░ рдирдореВрдирд╛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рдЧреБрдгрд╕реНрддрд░ рддреБрд▓рдирд╛ рдЧрд░реНрдиреБрд╣реЛрд╕реНред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## тЪб рдЫрд┐рдЯреЛ рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "**рд╕реНрдореГрддрд┐-рдЕрдиреБрдХреВрд▓ рд╕реЗрдЯрдЕрдк (рдЕрдкрдбреЗрдЯ рдЧрд░рд┐рдПрдХреЛ):**\n",
    "1. рдореЛрдбреЗрд▓рд╣рд░реВрд▓реЗ CPU рднреЗрд░рд┐рдпрдиреНрдЯрд╣рд░реВ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдЪрдпрди рдЧрд░реНрдЫрдиреН (рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рдорд╛ рдХрд╛рдо рдЧрд░реНрдЫ)\n",
    "2. `qwen2.5-3b` рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрдЫ 7B рдХреЛ рд╕рдЯреНрдЯрд╛ (рдЭрдиреНрдбреИ 4GB RAM рдмрдЪрдд рдЧрд░реНрдЫ)\n",
    "3. рдкреЛрд░реНрдЯ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ рд╕реБрд╡рд┐рдзрд╛ (рдореНрдпрд╛рдиреБрдЕрд▓ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рдЖрд╡рд╢реНрдпрдХ рдЫреИрди)\n",
    "4. рдХреБрд▓ RAM рдЖрд╡рд╢реНрдпрдХ: ~8GB рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдПрдХреЛ (рдореЛрдбреЗрд▓рд╣рд░реВ + OS)\n",
    "\n",
    "**рдЯрд░реНрдорд┐рдирд▓ рд╕реЗрдЯрдЕрдк (рейреж рд╕реЗрдХреЗрдиреНрдб):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "рддреНрдпрд╕рдкрдЫрд┐ рдпреЛ рдиреЛрдЯрдмреБрдХ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН! ЁЯЪА\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдирд┐рд░реНрднрд░рддрд╛ рд╕реНрдерд╛рдкрдирд╛\n",
    "рд╕рдордп рд░ рдЪреНрдпрд╛рдЯ рдЕрдиреБрд░реЛрдзрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рдЖрд╡рд╢реНрдпрдХ рдиреНрдпреВрдирддрдо рдкреНрдпрд╛рдХреЗрдЬрд╣рд░реВ (`foundry-local-sdk`, `openai`, `numpy`) рд╕реНрдерд╛рдкрдирд╛ рдЧрд░реНрджрдЫред рдпреЛ рдкреБрди: рдЪрд▓рд╛рдЙрди рд╕реБрд░рдХреНрд╖рд┐рдд рдЫред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# рдкрд░рд┐рджреГрд╢реНрдп\n",
    "рдПрдЙрдЯрд╛ рдкреНрд░рддрд┐рдирд┐рдзрд┐ рд╕рд╛рдиреЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ (SLM) рд░ рдареВрд▓реЛ рдореЛрдбреЗрд▓рд▓рд╛рдИ рдПрдЙрдЯреИ рдкреНрд░рдореНрдкреНрдЯрдорд╛ рддреБрд▓рдирд╛ рдЧрд░реЗрд░ рд╡реНрдпрд╛рдкрд╛рд░тАСрдЕрдлрд╣рд░реВ рджреЗрдЦрд╛рдЙрдиреБрд╣реЛрд╕реН:\n",
    "- **рдврд┐рд▓рд╛рдЗрдХреЛ рднрд┐рдиреНрдирддрд╛** (рд╡рд╛рд▓ рдХреНрд▓рдХ рд╕реЗрдХреЗрдиреНрдбрдорд╛)\n",
    "- **рдЯреЛрдХрди рдкреНрд░рдпреЛрдЧ** (рдпрджрд┐ рдЙрдкрд▓рдмреНрдз рдЫ) throughput рдХреЛ рд╕рдВрдХреЗрддрдХреЛ рд░реВрдкрдорд╛\n",
    "- **рдирдореВрдирд╛ рдЧреБрдгрд╛рддреНрдордХ рдЖрдЙрдЯрдкреБрдЯ** рдЫрд┐рдЯреЛ рдореВрд▓реНрдпрд╛рдЩреНрдХрдирдХрд╛ рд▓рд╛рдЧрд┐\n",
    "- **рдЧрддрд┐рдХреЛ рдЧрдгрдирд╛** рдкреНрд░рджрд░реНрд╢рди рд▓рд╛рднрд▓рд╛рдИ рдорд╛рдкрди рдЧрд░реНрди\n",
    "\n",
    "**рдкрд░реНрдпрд╛рд╡рд░рдг рдЪрд░рд╣рд░реВ:**\n",
    "- `SLM_ALIAS` - рд╕рд╛рдиреЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ (рдкреВрд░реНрд╡рдирд┐рд░реНрдзрд╛рд░рд┐рдд: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - рдареВрд▓реЛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓ (рдкреВрд░реНрд╡рдирд┐рд░реНрдзрд╛рд░рд┐рдд: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - рддреБрд▓рдирд╛ рдкрд░реАрдХреНрд╖рдгрдХрд╛ рд▓рд╛рдЧрд┐ рдкреНрд░рдореНрдкреНрдЯ\n",
    "- `COMPARE_RETRIES` - рд▓рдЪрд┐рд▓реЛрддрд╛рдХрд╛ рд▓рд╛рдЧрд┐ рдкреБрди: рдкреНрд░рдпрд╛рд╕ (рдкреВрд░реНрд╡рдирд┐рд░реНрдзрд╛рд░рд┐рдд: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - рд╕реЗрд╡рд╛ рдЕрдиреНрддрд░реНрдХреНрд░рд┐рдпрд╛рд▓рд╛рдИ рдУрднрд░рд░рд╛рдЗрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН (рд╕реЗрдЯ рдирдЧрд░рд┐рдПрдорд╛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЗрдиреНрдЫ)\n",
    "\n",
    "**рдпрд╕рд▓реЗ рдХрд╕рд░реА рдХрд╛рдо рдЧрд░реНрдЫ (рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдврд╛рдБрдЪрд╛):**\n",
    "1. **FoundryLocalManager** рд▓реЗ Foundry Local рд╕реЗрд╡рд╛рдХреЛ рд╕реБрд░реБрд╡рд╛рдд рд░ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЧрд░реНрдЫ\n",
    "2. рд╕реЗрд╡рд╛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рд╕реБрд░реБ рд╣реБрдиреНрдЫ рдпрджрд┐ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗ (рдореНрдпрд╛рдиреБрдЕрд▓ рд╕реЗрдЯрдЕрдк рдЖрд╡рд╢реНрдпрдХ рдЫреИрди)\n",
    "3. рдореЛрдбреЗрд▓рд╣рд░реВ рдЙрдкрдирд╛рдордмрд╛рдЯ рдареЛрд╕ ID рд╣рд░реВрдорд╛ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рд╕рдорд╛рдзрд╛рди рдЧрд░рд┐рдиреНрдЫ\n",
    "4. рд╣рд╛рд░реНрдбрд╡реЗрдпрд░-рдЕрдиреБрдХреВрд▓рд┐рдд рднреЗрд░рд┐рдпрдиреНрдЯрд╣рд░реВ рдЪрдпрди рдЧрд░рд┐рдиреНрдЫ (CUDA, NPU, рд╡рд╛ CPU)\n",
    "5. OpenAI-рд╕рдВрдЧрдд рдЧреНрд░рд╛рд╣рдХрд▓реЗ рдЪреНрдпрд╛рдЯ рдкреВрд░рд╛ рдЧрд░реНрджрдЫ\n",
    "6. рдореЗрдЯреНрд░рд┐рдХреНрд╕рд╣рд░реВ рдХреНрдпрд╛рдкреНрдЪрд░ рдЧрд░рд┐рдиреНрдЫ: рдврд┐рд▓рд╛рдЗ, рдЯреЛрдХрди, рдЖрдЙрдЯрдкреБрдЯ рдЧреБрдгрд╕реНрддрд░\n",
    "7. рдирддрд┐рдЬрд╛рд╣рд░реВ рддреБрд▓рдирд╛ рдЧрд░рд┐рдиреНрдЫ рдЧрддрд┐рдХреЛ рдЕрдиреБрдкрд╛рдд рдЧрдгрдирд╛ рдЧрд░реНрди\n",
    "\n",
    "рдпреЛ рд╕рд╛рдиреЛ рддреБрд▓рдирд╛ рддрдкрд╛рдИрдВрдХреЛ рдкреНрд░рдпреЛрдЧрдХреЛ рдХреЗрд╕рдХрд╛ рд▓рд╛рдЧрд┐ рдареВрд▓реЛ рдореЛрдбреЗрд▓рдорд╛ рд░реБрдЯрд┐рдЩ рдХрд╣рд┐рд▓реЗ рдЙрдЪрд┐рдд рд╣реБрдиреНрдЫ рднрдиреЗрд░ рдирд┐рд░реНрдгрдп рдЧрд░реНрди рдорджреНрджрдд рдЧрд░реНрдЫред\n",
    "\n",
    "**SDK рд╕рдиреНрджрд░реНрдн:**\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- рдХрд╛рд░реНрдпрд╢рд╛рд▓рд╛ рдЙрдкрдпреЛрдЧрд┐рддрд╛рд╣рд░реВ: ../samples/workshop_utils.py рдмрд╛рдЯ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рдврд╛рдБрдЪрд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрдЫ\n",
    "\n",
    "**рдореБрдЦреНрдп рдлрд╛рдЗрджрд╛рд╣рд░реВ:**\n",
    "- тЬЕ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд╕реЗрд╡рд╛ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдиреЗ рд░ рд╕реБрд░реБрд╡рд╛рдд рдЧрд░реНрдиреЗ\n",
    "- тЬЕ рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рд░реВрдкрдорд╛ рд╕реБрд░реБ рдЧрд░реНрдиреЗ\n",
    "- тЬЕ рдореЛрдбреЗрд▓ рд╕рдорд╛рдзрд╛рди рд░ рдХреНрдпрд╛рд╕рд┐рдЩрдорд╛ рдирд┐рд░реНрдорд┐рдд\n",
    "- тЬЕ рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдЕрдиреБрдХреВрд▓рди (CUDA/NPU/CPU)\n",
    "- тЬЕ OpenAI SDK рд╕рдВрдЧрддрддрд╛\n",
    "- тЬЕ рдкреБрди: рдкреНрд░рдпрд╛рд╕рд╕рд╣рд┐рддрдХреЛ рдмрд▓рд┐рдпреЛ рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди\n",
    "- тЬЕ рд╕реНрдерд╛рдиреАрдп рдЕрдиреБрдорд╛рди (рдХреНрд▓рд╛рдЙрдб API рдЖрд╡рд╢реНрдпрдХ рдЫреИрди)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ЁЯЪи рдкреВрд░реНрд╡рд╢рд░реНрдд: Foundry Local рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рд╣реБрдиреБрдкрд░реНрдЫ!\n",
    "\n",
    "**рдпреЛ рдиреЛрдЯрдмреБрдХ рдЪрд▓рд╛рдЙрдиреБ рдЕрдШрд┐**, Foundry Local рд╕реЗрд╡рд╛ рд╕реЗрдЯрдЕрдк рдЧрд░рд┐рдПрдХреЛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрдиреБрд╣реЛрд╕реН:\n",
    "\n",
    "### рдЫрд┐рдЯреЛ рд╕реБрд░реБ рдЧрд░реНрдиреЗ рдХрдорд╛рдгреНрдбрд╣рд░реВ (рдЯрд░реНрдорд┐рдирд▓рдорд╛ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### рд╡реИрдХрд▓реНрдкрд┐рдХ рдореЛрдбреЗрд▓рд╣рд░реВ (рдпрджрд┐ рдбрд┐рдлрд▓реНрдЯ рдЙрдкрд▓рдмреНрдз рдЫреИрдирдиреН рднрдиреЗ):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "тЪая╕П **рдпрджрд┐ рддрдкрд╛рдИрдВрд▓реЗ рдпреА рдЪрд░рдгрд╣рд░реВ рдЫреЛрдбреНрдиреБрднрдпреЛ рднрдиреЗ**, рддрд▓рдХрд╛ рдиреЛрдЯрдмреБрдХ рд╕реЗрд▓рд╣рд░реВ рдЪрд▓рд╛рдЙрдБрджрд╛ `APIConnectionError` рджреЗрдЦрд┐рдиреЗрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдХреЛрд░ рдЖрдпрд╛рддрд╣рд░реВ\n",
    "рд╕рдордп рдЙрдкрдпреЛрдЧрд┐рддрд╛рд╣рд░реВ рд░ Foundry Local / OpenAI рдХреНрд▓рд╛рдЗрдиреНрдЯрд╣рд░реВ рд▓реНрдпрд╛рдЙрдБрдЫ, рдЬрд╕рд▓реЗ рдореЛрдбреЗрд▓ рдЬрд╛рдирдХрд╛рд░реА рдкреНрд░рд╛рдкреНрдд рдЧрд░реНрди рд░ рдЪреНрдпрд╛рдЯ рдкреВрд░рд╛ рдЧрд░реНрди рдкреНрд░рдпреЛрдЧ рдЧрд░рд┐рдиреНрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдЙрдкрдирд╛рдорд╣рд░реВ рд░ рдкреНрд░реЙрдореНрдкреНрдЯ рд╕реЗрдЯрдЕрдк  \n",
    "рд╕рд╛рдиреЛ рд░ рдареВрд▓реЛ рдореЛрдбреЗрд▓рдХрд╛ рд▓рд╛рдЧрд┐ рд╡рд╛рддрд╛рд╡рд░рдг-рдЕрдиреБрдХреВрд▓рди рдпреЛрдЧреНрдп рдЙрдкрдирд╛рдорд╣рд░реВ рд░ рддреБрд▓рдирд╛ рдкреНрд░реЙрдореНрдкреНрдЯ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдЧрд░реНрджрдЫред рд╡рд┐рднрд┐рдиреНрди рдореЛрдбреЗрд▓ рдкрд░рд┐рд╡рд╛рд░рд╣рд░реВ рд╡рд╛ рдХрд╛рд░реНрдпрд╣рд░реВрдорд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рд╡рд╛рддрд╛рд╡рд░рдгреАрдп рднреЗрд░рд┐рдПрдмрд▓рд╣рд░реВ рд╕рдорд╛рдпреЛрдЬрди рдЧрд░реНрдиреБрд╣реЛрд╕реНред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ЁЯТб рдореЗрдореЛрд░реА-рдЕрдкреНрдЯрд┐рдорд╛рдЗрдЬреНрдб рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди\n",
    "\n",
    "**рдпреЛ рдиреЛрдЯрдмреБрдХрд▓реЗ рдбрд┐рдлрд▓реНрдЯ рд░реВрдкрдорд╛ рдореЗрдореЛрд░реА-рдХреБрд╢рд▓ рдореЛрдбреЗрд▓рд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрдЫ:**\n",
    "- `phi-4-mini` тЖТ ~4GB RAM (Foundry Local CPU рднреЗрд░рд┐рдпрдиреНрдЯрд▓рд╛рдИ рд╕реНрд╡рддрдГ рдЪрдпрди рдЧрд░реНрджрдЫ)\n",
    "- `qwen2.5-3b` тЖТ ~3GB RAM (7B рдХреЛ рд╕рдЯреНрдЯрд╛, рдЬрд╕рд▓рд╛рдИ ~7GB+ рдЖрд╡рд╢реНрдпрдХ рдкрд░реНрдЫ)\n",
    "\n",
    "**рдкреЛрд░реНрдЯ рд╕реНрд╡рддрдГ-рдкрд╣рд┐рдЪрд╛рди:**\n",
    "- Foundry Local рд▓реЗ рд╡рд┐рднрд┐рдиреНрди рдкреЛрд░реНрдЯрд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрди рд╕рдХреНрдЫ (рд╕рд╛рдорд╛рдиреНрдпрддрдпрд╛ 55769 рд╡рд╛ 59959)\n",
    "- рддрд▓рдХреЛ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓рд▓реЗ рд╕рд╣реА рдкреЛрд░реНрдЯ рд╕реНрд╡рддрдГ рдкрд╣рд┐рдЪрд╛рди рдЧрд░реНрджрдЫ\n",
    "- рдХреБрдиреИ рдкрдирд┐ рдореНрдпрд╛рдиреБрдЕрд▓ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди рдЖрд╡рд╢реНрдпрдХ рдЫреИрди!\n",
    "\n",
    "**рдпрджрд┐ рддрдкрд╛рдИрдВрдХреЛ RAM рд╕реАрдорд┐рдд рдЫ (<8GB), рдЕрдЭ рд╕рд╛рдирд╛ рдореЛрдбреЗрд▓рд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ЁЯТб Using official Foundry SDK pattern from workshop_utils\n",
      "   тЖТ FoundryLocalManager handles service lifecycle\n",
      "   тЖТ Automatic model resolution and hardware optimization\n",
      "   тЖТ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nЁЯТб Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   тЖТ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   тЖТ Automatic model resolution and hardware optimization\")\n",
    "print(\"   тЖТ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рд╕рд╣рд╛рдпрдХ (Foundry SDK рдврд╛рдБрдЪрд╛)\n",
    "Workshop рдирдореВрдирд╛рд╣рд░реВрдорд╛ рджрд╕реНрддрд╛рд╡реЗрдЬ рдЧрд░рд┐рдПрдХреЛ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Foundry Local SDK рдврд╛рдБрдЪрд╛рдХреЛ рдкреНрд░рдпреЛрдЧ:\n",
    "\n",
    "**рдкрджреНрдзрддрд┐:**\n",
    "- **FoundryLocalManager** - Foundry Local рд╕реЗрд╡рд╛ рдЖрд░рдореНрдн рд░ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЧрд░реНрджрдЫ\n",
    "- **рд╕реНрд╡рдд: рдкрд╣рд┐рдЪрд╛рди** - рд╕реНрд╡рдд: рдЕрдиреНрддрд░реНрдХреНрд░рд┐рдпрд╛рд▓рд╛рдИ рдкрддреНрддрд╛ рд▓рдЧрд╛рдЙрдБрдЫ рд░ рд╕реЗрд╡рд╛ рдЬреАрд╡рдирдЪрдХреНрд░рд▓рд╛рдИ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЧрд░реНрджрдЫ\n",
    "- **рдореЛрдбреЗрд▓ рд╕рдорд╛рдзрд╛рди** - рдЙрдкрдирд╛рдорд▓рд╛рдИ рдкреВрд░реНрдг рдореЛрдбреЗрд▓ ID рдорд╛ рд░реВрдкрд╛рдиреНрддрд░рдг рдЧрд░реНрджрдЫ (рдЬрд╕реНрддреИ, phi-4-mini тЖТ phi-4-mini-instruct-cpu)\n",
    "- **рд╣рд╛рд░реНрдбрд╡реЗрдпрд░ рдЕрдиреБрдХреВрд▓рди** - рдЙрдкрд▓рдмреНрдз рд╣рд╛рд░реНрдбрд╡реЗрдпрд░рдХрд╛ рд▓рд╛рдЧрд┐ рдЙрддреНрддрдо рднреЗрд░рд┐рдпрдиреНрдЯ рдЪрдпрди рдЧрд░реНрджрдЫ (CUDA, NPU, рд╡рд╛ CPU)\n",
    "- **OpenAI рдХреНрд▓рд╛рдЗрдиреНрдЯ** - OpenAI-рд╕рдВрдЧрдд API рдкрд╣реБрдБрдЪрдХреЛ рд▓рд╛рдЧрд┐ рдкреНрд░рдмрдиреНрдзрдХрдХреЛ рдЕрдиреНрддрд░реНрдХреНрд░рд┐рдпрд╛рд╕рдБрдЧ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░рд┐рдПрдХреЛ\n",
    "\n",
    "**рд▓рдЪрд┐рд▓реЛрдкрди рд╕реБрд╡рд┐рдзрд╛рд╣рд░реВ:**\n",
    "- рдПрдХреНрд╕реНрдкреЛрдиреЗрдВрд╢рд┐рдпрд▓ рдмреНрдпрд╛рдХрдЕрдл рдкреБрди: рдкреНрд░рдпрд╛рд╕ рддрд░реНрдХ (рдкрд░реНрдпрд╛рд╡рд░рдг рдорд╛рд░реНрдлрдд рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░реНрди рд╕рдХрд┐рдиреЗ)\n",
    "- рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗ рд╕реНрд╡рдд: рд╕реЗрд╡рд╛ рд╕реБрд░реБ\n",
    "- рдЖрд░рдореНрдн рдкрдЫрд┐ рдЬрдбрд╛рдирдХреЛ рд╕рддреНрдпрд╛рдкрди\n",
    "- рд╡рд┐рд╕реНрддреГрдд рддреНрд░реБрдЯрд┐ рд░рд┐рдкреЛрд░реНрдЯрд┐рдЩрд╕рд╣рд┐рддрдХреЛ рд╕рд╣рдЬ рддреНрд░реБрдЯрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди\n",
    "- рдмрд╛рд░рдореНрдмрд╛рд░ рдЖрд░рдореНрднрд▓рд╛рдИ рд░реЛрдХреНрди рдореЛрдбреЗрд▓ рдХреНрдпрд╛рд╕рд┐рдЩ\n",
    "\n",
    "**рдкрд░рд┐рдгрд╛рдо рд╕рдВрд░рдЪрдирд╛:**\n",
    "- рд╡рд┐рд▓рдореНрдм рдорд╛рдкрди (рд╡рд╛рд▓ рдШрдбреА рд╕рдордп)\n",
    "- рдЯреЛрдХрди рдкреНрд░рдпреЛрдЧ рдЯреНрд░реНрдпрд╛рдХрд┐рдЩ (рдпрджрд┐ рдЙрдкрд▓рдмреНрдз рдЫ рднрдиреЗ)\n",
    "- рдирдореВрдирд╛ рдЖрдЙрдЯрдкреБрдЯ (рдкрдвреНрди рд╕рдЬрд┐рд▓реЛ рдмрдирд╛рдЙрди рдЫреЛрдЯреНрдпрд╛рдЗрдПрдХреЛ)\n",
    "- рдЕрд╕рдлрд▓ рдЕрдиреБрд░реЛрдзрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рддреНрд░реБрдЯрд┐ рд╡рд┐рд╡рд░рдг\n",
    "\n",
    "рдпреЛ рдврд╛рдБрдЪрд╛рд▓реЗ workshop_utils рдореЛрдбреНрдпреБрд▓рд▓рд╛рдИ рдЙрдкрдпреЛрдЧ рдЧрд░реНрджрдЫ, рдЬрд╕рд▓реЗ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдврд╛рдБрдЪрд╛рдХреЛ рдкрд╛рд▓рдирд╛ рдЧрд░реНрджрдЫред\n",
    "\n",
    "**SDK рд╕рдиреНрджрд░реНрдн:**\n",
    "- рдореБрдЦреНрдп рд░рд┐рдкреЛрдЬрд┐рдЯрд░реА: https://github.com/microsoft/Foundry-Local\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- Workshop Utils: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Execution helpers defined: setup(), run()\n",
      "   тЖТ Uses workshop_utils for proper SDK integration\n",
      "   тЖТ setup() initializes with FoundryLocalManager\n",
      "   тЖТ run() executes inference via OpenAI-compatible API\n",
      "   тЖТ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        тЖТ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        тЖТ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        тЖТ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        тЖТ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nтЭМ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nЁЯТб Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      тЖТ foundry service status\")\n",
    "    print(f\"      тЖТ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      тЖТ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      тЖТ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"тЬЕ Execution helpers defined: setup(), run()\")\n",
    "print(\"   тЖТ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   тЖТ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   тЖТ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   тЖТ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рдкреНрд░рд┐-рдлреНрд▓рд╛рдЗрдЯ рд╕реЗрд▓реНрдл-рдЯреЗрд╕реНрдЯ\n",
    "FoundryLocalManager рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рджреБрд╡реИ рдореЛрдбреЗрд▓рд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рд╣рд▓реНрдХрд╛ рдХрдиреЗрдХреНрдЯрд┐рднрд┐рдЯреА рдЬрд╛рдБрдЪ рдЪрд▓рд╛рдЙрдБрдЫред рдпрд╕рд▓реЗ рдирд┐рдореНрди рдХреБрд░рд╛рд╣рд░реВ рдкреНрд░рдорд╛рдгрд┐рдд рдЧрд░реНрдЫ:\n",
    "- рд╕реЗрд╡рд╛ рдкрд╣реБрдБрдЪрдпреЛрдЧреНрдп рдЫ\n",
    "- рдореЛрдбреЗрд▓рд╣рд░реВ рд╕реБрд░реБ рдЧрд░реНрди рд╕рдХрд┐рдиреНрдЫ\n",
    "- рдЙрдкрдирд╛рдорд╣рд░реВ рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдореЛрдбреЗрд▓ рдЖрдИрдбреАрд╣рд░реВрдорд╛ рд╕рдорд╛рдзрд╛рди рд╣реБрдиреНрдЫрдиреН\n",
    "- рддреБрд▓рдирд╛ рдЪрд▓рд╛рдЙрдиреБ рдЕрдШрд┐ рдЬрдбрд╛рди рд╕реНрдерд┐рд░ рдЫ\n",
    "\n",
    "setup() рдлрдЩреНрд╕рдирд▓реЗ workshop_utils рдмрд╛рдЯ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ SDK рдврд╛рдБрдЪрд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "тЭМ Foundry Local service not found!\n",
      "\n",
      "ЁЯТб To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "тЪая╕П  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"тЬЕ Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"тЬЕ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"тЪая╕П  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"тЪая╕П  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nтЭМ Foundry Local service not found!\")\n",
    "    print(\"\\nЁЯТб To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nтЬЕ Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nтЪая╕П  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪая╕П  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ЁЯТб Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"тЪая╕П  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ЁЯТб Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ЁЯЫая╕П рдЫрд┐рдЯреЛ рд╕рдорд╛рдзрд╛рди: рдиреЛрдЯрдмреБрдХрдмрд╛рдЯ рд╕реНрдерд╛рдиреАрдп рдлрд╛рдЙрдиреНрдбреНрд░реА рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН (рд╡реИрдХрд▓реНрдкрд┐рдХ)\n",
    "\n",
    "рдпрджрд┐ рдорд╛рдерд┐рдХреЛ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХрд▓реЗ рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗрд░ рджреЗрдЦрд╛рдЙрдБрдЫ рднрдиреЗ, рддрдкрд╛рдИрдВ рдпрд╕рд▓рд╛рдИ рдпрд╣рд╛рдБрдмрд╛рдЯ рд╕реБрд░реБ рдЧрд░реНрди рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫ:\n",
    "\n",
    "**рдиреЛрдЯ:** рдпреЛ рд╡рд┐рдиреНрдбреЛрдЬрдорд╛ рд╕рдмреИрднрдиреНрджрд╛ рд░рд╛рдореНрд░реЛ рдХрд╛рдо рдЧрд░реНрдЫред рдЕрдиреНрдп рдкреНрд▓реЗрдЯрдлрд░реНрдорд╣рд░реВрдорд╛, рдЯрд░реНрдорд┐рдирд▓ рдХрдорд╛рдгреНрдбрд╣рд░реВ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реНред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### тЪая╕П рдЬрдбрд╛рди рддреНрд░реБрдЯрд┐рд╣рд░реВ рд╕рдорд╛рдзрд╛рди рдЧрд░реНрдиреЗ\n",
    "\n",
    "рдпрджрд┐ рддрдкрд╛рдИрдВрд▓реЗ `APIConnectionError` рджреЗрдЦреНрдиреБрднрдПрдХреЛ рдЫ рднрдиреЗ, Foundry Local рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рд╡рд╛ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ рдЫреИрдиред рдпреА рдЪрд░рдгрд╣рд░реВ рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреБрд╣реЛрд╕реН:\n",
    "\n",
    "**1. рд╕реЗрд╡рд╛ рд╕реНрдерд┐рддрд┐ рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**2. рд╕реЗрд╡рд╛ рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реН (рдпрджрд┐ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди рднрдиреЗ):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**3. рдЖрд╡рд╢реНрдпрдХ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**4. рдореЛрдбреЗрд▓рд╣рд░реВ рдЙрдкрд▓рдмреНрдз рдЫрдиреН рдХрд┐ рдЫреИрдирдиреН рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрдиреБрд╣реЛрд╕реН:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛рд╣рд░реВ:**\n",
    "- тЭМ рд╕реЗрд╡рд╛ рдЪрд▓рд┐рд░рд╣реЗрдХреЛ рдЫреИрди тЖТ `foundry service start` рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН\n",
    "- тЭМ рдореЛрдбреЗрд▓рд╣рд░реВ рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ рдЫреИрди тЖТ `foundry model run <model-name>` рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН\n",
    "- тЭМ рдкреЛрд░реНрдЯрдХреЛ рджреНрд╡рдиреНрджреНрд╡ тЖТ рдЕрд░реНрдХреЛ рд╕реЗрд╡рд╛ рдкреЛрд░реНрдЯ рдкреНрд░рдпреЛрдЧ рдЧрд░рд┐рд░рд╣реЗрдХреЛ рдЫ рдХрд┐ рдЫреИрди рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "- тЭМ рдлрд╛рдпрд░рд╡рд╛рд▓рд▓реЗ рд░реЛрдХрд┐рд░рд╣реЗрдХреЛ рдЫ тЖТ рд╕реНрдерд╛рдиреАрдп рдЬрдбрд╛рдирд╣рд░реВ рдЕрдиреБрдорддрд┐ рджрд┐рдЗрдПрдХреЛ рдЫ рдХрд┐ рдЫреИрди рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "**рдЫрд┐рдЯреЛ рд╕рдорд╛рдзрд╛рди:** рдкреНрд░рд┐-рдлреНрд▓рд╛рдЗрдЯ рдЬрд╛рдБрдЪ рдЕрдШрд┐ рддрд▓рдХреЛ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓ рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реНред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  тЬЕ phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  тЬЕ qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'тЬЕ' if details['status'] == 'success' else 'тЭМ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### тЬЕ рдкреНрд░рд┐-рдлреНрд▓рд╛рдЗрдЯ рдЬрд╛рдБрдЪ: рдореЛрдбреЗрд▓ рдЙрдкрд▓рдмреНрдзрддрд╛\n",
    "\n",
    "рдпреЛ рд╕реЗрд▓рд▓реЗ рддреБрд▓рдирд╛ рд╕реБрд░реБ рдЧрд░реНрдиреБ рдЕрдШрд┐ рджреБрд╡реИ рдореЛрдбреЗрд▓рд╣рд░реВ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░рд┐рдПрдХреЛ рдЕрдиреНрдд рдмрд┐рдиреНрджреБрдорд╛ рдкрд╣реБрдБрдЪрдпреЛрдЧреНрдп рдЫрдиреН рдХрд┐ рдЫреИрдирдиреН рднрдиреЗрд░ рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрджрдЫред\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### рд╡реНрдпрд╛рдЦреНрдпрд╛: рд░рди рддреБрд▓рдирд╛ рд░ рдкрд░рд┐рдгрд╛рдо рд╕рдВрдХрд▓рди\n",
    "рджреБрд╡реИ рдЙрдкрдирд╛рдорд╣рд░реВрдорд╛ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ Foundry SDK рдврд╛рдБрдЪрд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджреИ рдкреБрдирд░рд╛рд╡реГрддреНрддрд┐ рдЧрд░реНрджрдЫ:\n",
    "1. рдкреНрд░рддреНрдпреЗрдХ рдореЛрдбреЗрд▓рд▓рд╛рдИ setup() рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЖрд░рдореНрдн рдЧрд░реНрдиреБрд╣реЛрд╕реН (FoundryLocalManager рдкреНрд░рдпреЛрдЧ рдЧрд░реНрджрдЫ)\n",
    "2. OpenAI-рд╕рдВрдЧ рдорд┐рд▓реНрдиреЗ API рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдЕрдиреБрдорд╛рди рдЪрд▓рд╛рдЙрдиреБрд╣реЛрд╕реН\n",
    "3. рд╡рд┐рд▓рдореНрдмрддрд╛, рдЯреЛрдХрдирд╣рд░реВ, рд░ рдирдореВрдирд╛ рдЖрдЙрдЯрдкреБрдЯ рдХреИрдж рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "4. рддреБрд▓рдирд╛рддреНрдордХ рд╡рд┐рд╢реНрд▓реЗрд╖рдгрд╕рд╣рд┐рдд JSON рд╕рд╛рд░рд╛рдВрд╢ рдЙрддреНрдкрд╛рджрди рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "рдпреЛ session04/model_compare.py рдорд╛ рдХрд╛рд░реНрдпрд╢рд╛рд▓рд╛рдХрд╛ рдирдореВрдирд╛рд╣рд░реВрдХреЛ рд╕рдорд╛рди рдврд╛рдБрдЪрд╛рд▓рд╛рдИ рдкрдЫреНрдпрд╛рдЙрдБрдЫред\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "тЬЕ phi-4-mini         success         51.975          ~247 (est.)    \n",
      "тЬЕ qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ЁЯТб SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'тЬЕ' if status == 'success' else 'тЭМ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nЁЯТб SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nтЪая╕П  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### рдирддрд┐рдЬрд╛ рд╡реНрдпрд╛рдЦреНрдпрд╛ рдЧрд░реНрджреИ\n",
    "\n",
    "**рдореБрдЦреНрдп рдорд╛рдкрджрдгреНрдбрд╣рд░реВ:**\n",
    "- **рд▓реЗрдЯреЗрдиреНрд╕реА**: рдХрдо рд╣реБрдиреБ рд░рд╛рдореНрд░реЛ - рдЫрд┐рдЯреЛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛ рд╕рдордп рджреЗрдЦрд╛рдЙрдБрдЫ\n",
    "- **рдЯреЛрдХрдирд╣рд░реВ**: рдЙрдЪреНрдЪ рдереНрд░реБрдкреБрдЯ = рдмрдвреА рдЯреЛрдХрдирд╣рд░реВ рдкреНрд░рд╢реЛрдзрди рдЧрд░рд┐рдиреНрдЫ\n",
    "- **рд░реВрдЯ**: рдХреБрди API рдЕрдиреНрдд рдмрд┐рдиреНрджреБ рдкреНрд░рдпреЛрдЧ рдЧрд░рд┐рдПрдХреЛ рдерд┐рдпреЛ рднрдиреЗрд░ рдкреБрд╖реНрдЯрд┐ рдЧрд░реНрдЫ\n",
    "\n",
    "**SLM рдмрдирд╛рдо LLM рдХрд╣рд┐рд▓реЗ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреЗ:**\n",
    "- **SLM (рд╕рд╛рдирд╛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓)**: рдЫрд┐рдЯреЛ рдкреНрд░рддрд┐рдХреНрд░рд┐рдпрд╛, рдХрдо рд╕реНрд░реЛрддрдХреЛ рдкреНрд░рдпреЛрдЧ, рд╕рд╛рдзрд╛рд░рдг рдХрд╛рд░реНрдпрд╣рд░реВрдХреЛ рд▓рд╛рдЧрд┐ рдЙрдкрдпреБрдХреНрдд\n",
    "- **LLM (рдареВрд▓рд╛ рднрд╛рд╖рд╛ рдореЛрдбреЗрд▓)**: рдЙрдЪреНрдЪ рдЧреБрдгрд╕реНрддрд░, рд░рд╛рдореНрд░реЛ рддрд░реНрдХ рдХреНрд╖рдорддрд╛, рдЬрдм рдЧреБрдгрд╕реНрддрд░ рд╕рдмреИрднрдиреНрджрд╛ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рд╣реБрдиреНрдЫ рддрдм рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "\n",
    "**рдЕрдЧрд╛рдбрд┐ рдХреЗ рдЧрд░реНрдиреЗ:**\n",
    "1. рд╡рд┐рднрд┐рдиреНрди рдкреНрд░реЙрдореНрдкреНрдЯрд╣рд░реВ рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреБрд╣реЛрд╕реН рд░ рдЬрдЯрд┐рд▓рддрд╛рд▓реЗ рддреБрд▓рдирд╛ рдХрд╕рд░реА рдкреНрд░рднрд╛рд╡рд┐рдд рдЧрд░реНрдЫ рд╣реЗрд░реНрдиреБрд╣реЛрд╕реН\n",
    "2. рдЕрдиреНрдп рдореЛрдбреЗрд▓ рдЬреЛрдбреАрд╣рд░реВрдорд╛ рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n",
    "3. рдХрд╛рд░реНрдпрдХреЛ рдЬрдЯрд┐рд▓рддрд╛рдХрд╛ рдЖрдзрд╛рд░рдорд╛ рдмреБрджреНрдзрд┐рдорд╛рдиреАрдкреВрд░реНрд╡рдХ рд░реВрдЯ рдЧрд░реНрди Workshop рд░рд╛рдЙрдЯрд░ рдирдореВрдирд╛рд╣рд░реВ (Session 06) рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреБрд╣реЛрд╕реН\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "тЬЕ SLM Model: phi-4-mini\n",
      "тЬЕ LLM Model: qwen2.5-7b\n",
      "тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "тЬЕ Pre-flight passed: True\n",
      "тЬЕ Comparison completed: True\n",
      "тЬЕ Both models responded: True\n",
      "======================================================================\n",
      "ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"тЬЕ SLM Model: {SLM}\")\n",
    "print(f\"тЬЕ LLM Model: {LLM}\")\n",
    "print(f\"тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"тЬЕ Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"тЪая╕П  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"тЪая╕П  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"тЪая╕П  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nтЪая╕П  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nЁЯТб Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nЁЯТб Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрдпреЛ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ AI рдЕрдиреБрд╡рд╛рдж рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдкреНрд░рдпреЛрдЧ рдЧрд░реА рдЕрдиреБрд╡рд╛рдж рдЧрд░рд┐рдПрдХреЛ рд╣реЛред рд╣рд╛рдореА рдпрдерд╛рд╕рдореНрднрд╡ рд╕рдЯреАрдХрддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдЧрд░реНрди рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдЫреМрдВ, рддрд░ рдХреГрдкрдпрд╛ рдзреНрдпрд╛рди рджрд┐рдиреБрд╣реЛрд╕реН рдХрд┐ рд╕реНрд╡рдЪрд╛рд▓рд┐рдд рдЕрдиреБрд╡рд╛рджрд╣рд░реВрдорд╛ рддреНрд░реБрдЯрд┐ рд╡рд╛ рдЕрд╢реБрджреНрдзрддрд╛ рд╣реБрди рд╕рдХреНрдЫред рдореВрд▓ рднрд╛рд╖рд╛рдорд╛ рд░рд╣реЗрдХреЛ рдореВрд▓ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝рд▓рд╛рдИ рдЖрдзрд┐рдХрд╛рд░рд┐рдХ рд╕реНрд░реЛрдд рдорд╛рдирд┐рдиреБрдкрд░реНрдЫред рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рдЬрд╛рдирдХрд╛рд░реАрдХрд╛ рд▓рд╛рдЧрд┐, рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡ рдЕрдиреБрд╡рд╛рдж рд╕рд┐рдлрд╛рд░рд┐рд╕ рдЧрд░рд┐рдиреНрдЫред рдпрд╕ рдЕрдиреБрд╡рд╛рджрдХреЛ рдкреНрд░рдпреЛрдЧрдмрд╛рдЯ рдЙрддреНрдкрдиреНрди рд╣реБрдиреЗ рдХреБрдиреИ рдкрдирд┐ рдЧрд▓рддрдлрд╣рдореА рд╡рд╛ рдЧрд▓рдд рд╡реНрдпрд╛рдЦреНрдпрд╛рдХреЛ рд▓рд╛рдЧрд┐ рд╣рд╛рдореА рдЬрд┐рдореНрдореЗрд╡рд╛рд░ рд╣реБрдиреЗ рдЫреИрдиреМрдВред\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-09T10:04:59+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}