<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-17T20:37:12+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "ne"
}
-->
# खण्ड ३: व्यावहारिक कार्यान्वयन मार्गदर्शिका

## अवलोकन

यो विस्तृत मार्गदर्शिकाले EdgeAI पाठ्यक्रमको तयारीमा सहयोग गर्नेछ, जसले किनार उपकरणहरूमा प्रभावकारी रूपमा चल्ने व्यावहारिक AI समाधानहरू निर्माणमा केन्द्रित छ। पाठ्यक्रमले आधुनिक फ्रेमवर्कहरू र किनारमा तैनातीका लागि अनुकूलित अत्याधुनिक मोडेलहरूको प्रयोगमा आधारित व्यावहारिक विकासलाई प्राथमिकता दिन्छ।

## १. विकास वातावरण सेटअप

### प्रोग्रामिङ भाषा र फ्रेमवर्कहरू

**Python वातावरण**
- **संस्करण**: Python 3.10 वा उच्च (सिफारिस: Python 3.11)
- **प्याकेज प्रबन्धक**: pip वा conda
- **भर्चुअल वातावरण**: अलगावका लागि venv वा conda वातावरण प्रयोग गर्नुहोस्
- **मुख्य पुस्तकालयहरू**: EdgeAI पुस्तकालयहरू पाठ्यक्रमको क्रममा स्थापना गरिनेछ

**Microsoft .NET वातावरण**
- **संस्करण**: .NET 8 वा उच्च
- **IDE**: Visual Studio 2022, Visual Studio Code, वा JetBrains Rider
- **SDK**: क्रस-प्ल्याटफर्म विकासका लागि .NET SDK स्थापना सुनिश्चित गर्नुहोस्

### विकास उपकरणहरू

**कोड सम्पादकहरू र IDEs**
- Visual Studio Code (क्रस-प्ल्याटफर्म विकासका लागि सिफारिस गरिएको)
- PyCharm वा Visual Studio (भाषा-विशिष्ट विकासका लागि)
- Jupyter Notebooks अन्तर्क्रियात्मक विकास र प्रोटोटाइपका लागि

**संस्करण नियन्त्रण**
- Git (नवीनतम संस्करण)
- GitHub खाता रिपोजिटरीहरू पहुँच गर्न र सहकार्यका लागि

## २. हार्डवेयर आवश्यकताहरू र सिफारिसहरू

### न्यूनतम प्रणाली आवश्यकताहरू
- **CPU**: मल्टी-कोर प्रोसेसर (Intel i5/AMD Ryzen 5 वा समकक्ष)
- **RAM**: न्यूनतम ८GB, सिफारिस गरिएको १६GB
- **स्टोरेज**: मोडेलहरू र विकास उपकरणहरूको लागि ५०GB उपलब्ध स्थान
- **OS**: Windows 10/11, macOS 10.15+, वा Linux (Ubuntu 20.04+)

### कम्प्युट स्रोत रणनीति
पाठ्यक्रम विभिन्न हार्डवेयर कन्फिगरेसनहरूमा पहुँचयोग्य बनाउन डिजाइन गरिएको छ:

**स्थानीय विकास (CPU/NPU केन्द्रित)**
- प्राथमिक विकास CPU र NPU एक्सेलेरेशन प्रयोग गर्नेछ
- अधिकांश आधुनिक ल्यापटप र डेस्कटपका लागि उपयुक्त
- दक्षता र व्यावहारिक तैनाती परिदृश्यहरूमा केन्द्रित

**क्लाउड GPU स्रोतहरू (वैकल्पिक)**
- **Azure Machine Learning**: गहन प्रशिक्षण र प्रयोगका लागि
- **Google Colab**: शैक्षिक प्रयोजनका लागि निःशुल्क स्तर उपलब्ध
- **Kaggle Notebooks**: वैकल्पिक क्लाउड कम्प्युटिङ प्लेटफर्म

### किनार उपकरण विचारहरू
- ARM-आधारित प्रोसेसरहरूको समझ
- मोबाइल र IoT हार्डवेयर सीमाहरूको ज्ञान
- पावर खपत अनुकूलनको परिचय

## ३. मुख्य मोडेल परिवारहरू र स्रोतहरू

### प्राथमिक मोडेल परिवारहरू

**Microsoft Phi-4 परिवार**
- **विवरण**: किनार तैनातीका लागि डिजाइन गरिएका कम्प्याक्ट, दक्ष मोडेलहरू
- **शक्तिहरू**: प्रदर्शन-देखि-आकार अनुपात उत्कृष्ट, तर्क कार्यहरूको लागि अनुकूलित
- **स्रोत**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **प्रयोग केसहरू**: कोड उत्पादन, गणितीय तर्क, सामान्य संवाद

**Qwen-3 परिवार**
- **विवरण**: Alibaba को बहुभाषी मोडेलहरूको नवीनतम पुस्ता
- **शक्तिहरू**: बलियो बहुभाषी क्षमता, दक्ष वास्तुकला
- **स्रोत**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **प्रयोग केसहरू**: बहुभाषी अनुप्रयोगहरू, सांस्कृतिक क्रस-एआई समाधानहरू

**Google Gemma-3n परिवार**
- **विवरण**: किनार तैनातीका लागि अनुकूलित Google का हल्का मोडेलहरू
- **शक्तिहरू**: छिटो अनुमान, मोबाइल-अनुकूल वास्तुकला
- **स्रोत**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **प्रयोग केसहरू**: मोबाइल अनुप्रयोगहरू, वास्तविक-समय प्रशोधन

### मोडेल चयन मापदण्ड
- **प्रदर्शन बनाम आकार व्यापार-सम्झौता**: सानो वा ठूलो मोडेलहरू कहिले चयन गर्ने बुझ्न
- **कार्य-विशिष्ट अनुकूलन**: विशिष्ट प्रयोग केसहरूमा मोडेलहरू मिलाउन
- **तैनाती सीमाहरू**: मेमोरी, विलम्बता, र पावर खपत विचारहरू

## ४. क्वान्टाइजेशन र अनुकूलन उपकरणहरू

### Llama.cpp फ्रेमवर्क
- **रिपोजिटरी**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **उद्देश्य**: LLMs का लागि उच्च प्रदर्शन अनुमान इन्जिन
- **मुख्य विशेषताहरू**:
  - CPU-अनुकूलित अनुमान
  - बहु क्वान्टाइजेशन ढाँचाहरू (Q4, Q5, Q8)
  - क्रस-प्ल्याटफर्म अनुकूलता
  - मेमोरी-कुशल कार्यान्वयन
- **स्थापना र आधारभूत प्रयोग**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **रिपोजिटरी**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **उद्देश्य**: किनार तैनातीका लागि मोडेल अनुकूलन टूलकिट
- **मुख्य विशेषताहरू**:
  - स्वचालित मोडेल अनुकूलन वर्कफ्लोहरू
  - हार्डवेयर-अनुकूल अनुकूलन
  - ONNX Runtime सँग एकीकरण
  - प्रदर्शन बेंचमार्किङ उपकरणहरू
- **स्थापना र आधारभूत प्रयोग**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # मोडेल र अनुकूलन कन्फिग परिभाषित गर्नुहोस्
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # अनुकूलन वर्कफ्लो चलाउनुहोस्
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # अनुकूलित मोडेल बचत गर्नुहोस्
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # MLX स्थापना गर्नुहोस्
  pip install mlx
  
  # मोडेल लोड र अनुकूलनका लागि उदाहरण Python स्क्रिप्ट
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **रिपोजिटरी**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **उद्देश्य**: ONNX मोडेलहरूको लागि क्रस-प्ल्याटफर्म अनुमान एक्सेलेरेशन
- **मुख्य विशेषताहरू**:
  - हार्डवेयर-विशिष्ट अनुकूलनहरू (CPU, GPU, NPU)
  - अनुमानका लागि ग्राफ अनुकूलनहरू
  - क्वान्टाइजेशन समर्थन
  - क्रस-भाषा समर्थन (Python, C++, C#, JavaScript)
- **स्थापना र आधारभूत प्रयोग**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```

## ५. सिफारिस गरिएका पढाइ र स्रोतहरू

### आवश्यक दस्तावेजहरू
- **ONNX Runtime दस्तावेजहरू**: क्रस-प्ल्याटफर्म अनुमान बुझ्न
- **Hugging Face Transformers मार्गदर्शिका**: मोडेल लोड र अनुमान
- **Edge AI डिजाइन ढाँचाहरू**: किनार तैनातीका लागि उत्कृष्ट अभ्यासहरू

### प्राविधिक कागजातहरू
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### समुदाय स्रोतहरू
- **EdgeAI Slack/Discord समुदायहरू**: सहकर्मी समर्थन र छलफल
- **GitHub रिपोजिटरीहरू**: उदाहरण कार्यान्वयनहरू र ट्यूटोरियलहरू
- **YouTube च्यानलहरू**: प्राविधिक गहिराइ र ट्यूटोरियलहरू

## ६. मूल्यांकन र प्रमाणीकरण

### पूर्व-पाठ्यक्रम चेकलिस्ट
- [ ] Python 3.10+ स्थापना गरिएको र प्रमाणित
- [ ] .NET 8+ स्थापना गरिएको र प्रमाणित
- [ ] विकास वातावरण कन्फिगर गरिएको
- [ ] Hugging Face खाता सिर्जना गरिएको
- [ ] लक्ष्य मोडेल परिवारहरूको आधारभूत परिचय
- [ ] क्वान्टाइजेशन उपकरणहरू स्थापना गरिएको र परीक्षण गरिएको
- [ ] हार्डवेयर आवश्यकताहरू पूरा गरिएको
- [ ] क्लाउड कम्प्युटिङ खाता सेटअप गरिएको (यदि आवश्यक छ)

## प्रमुख सिकाइ उद्देश्यहरू

यस मार्गदर्शिकाको अन्त्यसम्म, तपाईं सक्षम हुनुहुनेछ:

1. EdgeAI अनुप्रयोग विकासका लागि पूर्ण विकास वातावरण सेटअप गर्नुहोस्
2. मोडेल अनुकूलनका लागि आवश्यक उपकरणहरू र फ्रेमवर्कहरू स्थापना र कन्फिगर गर्नुहोस्
3. EdgeAI परियोजनाहरूका लागि उपयुक्त हार्डवेयर र सफ्टवेयर कन्फिगरेसनहरू चयन गर्नुहोस्
4. किनार उपकरणहरूमा AI मोडेल तैनातीका लागि प्रमुख विचारहरू बुझ्नुहोस्
5. पाठ्यक्रमका व्यावहारिक अभ्यासहरूको तयारी गर्नुहोस्

## थप स्रोतहरू

### आधिकारिक दस्तावेजहरू
- **Python दस्तावेजहरू**: आधिकारिक Python भाषा दस्तावेजहरू
- **Microsoft .NET दस्तावेजहरू**: आधिकारिक .NET विकास स्रोतहरू
- **ONNX Runtime दस्तावेजहरू**: ONNX Runtime को व्यापक मार्गदर्शिका
- **TensorFlow Lite दस्तावेजहरू**: आधिकारिक TensorFlow Lite दस्तावेजहरू

### विकास उपकरणहरू
- **Visual Studio Code**: AI विकास एक्सटेन्सनसहित हल्का कोड सम्पादक
- **Jupyter Notebooks**: ML प्रयोगका लागि अन्तर्क्रियात्मक कम्प्युटिङ वातावरण
- **Docker**: स्थिर विकास वातावरणका लागि कन्टेनराइजेसन प्लेटफर्म
- **Git**: कोड व्यवस्थापनका लागि संस्करण नियन्त्रण प्रणाली

### सिकाइ स्रोतहरू
- **EdgeAI अनुसन्धान कागजातहरू**: दक्ष मोडेलहरूमा नवीनतम शैक्षिक अनुसन्धान
- **अनलाइन पाठ्यक्रमहरू**: AI अनुकूलनमा पूरक सिकाइ सामग्रीहरू
- **समुदाय फोरमहरू**: EdgeAI विकास चुनौतीहरूको लागि Q&A प्लेटफर्महरू
- **बेंचमार्क डेटासेटहरू**: मोडेल प्रदर्शन मूल्याङ्कनका लागि मानक डेटासेटहरू

## सिकाइ परिणामहरू

यो तयारी मार्गदर्शिका पूरा गरेपछि, तपाईं:

1. EdgeAI विकासका लागि पूर्ण रूपमा कन्फिगर गरिएको विकास वातावरण हुनेछ
2. विभिन्न तैनाती परिदृश्यहरूको लागि हार्डवेयर र सफ्टवेयर आवश्यकताहरू बुझ्नुहुनेछ
3. पाठ्यक्रमभर प्रयोग गरिने प्रमुख फ्रेमवर्क र उपकरणहरूसँग परिचित हुनुहुनेछ
4. उपकरण सीमाहरू र आवश्यकताहरूको आधारमा उपयुक्त मोडेलहरू चयन गर्न सक्षम हुनुहुनेछ
5. किनार तैनातीका लागि अनुकूलन प्रविधिहरूको आवश्यक ज्ञान हुनेछ

## ➡️ अब के गर्ने

- [04: EdgeAI हार्डवेयर र तैनाती](04.EdgeDeployment.md)

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव सटीकता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।