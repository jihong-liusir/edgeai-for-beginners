<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-17T21:18:06+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "ne"
}
-->
# खण्ड २: मोडेल डिस्टिलेशन - सिद्धान्तदेखि व्यवहारसम्म

## सामग्री सूची
1. [मोडेल डिस्टिलेशनको परिचय](../../../Module05)
2. [डिस्टिलेशन किन महत्त्वपूर्ण छ](../../../Module05)
3. [डिस्टिलेशन प्रक्रिया](../../../Module05)
4. [व्यावहारिक कार्यान्वयन](../../../Module05)
5. [Azure ML डिस्टिलेशन उदाहरण](../../../Module05)
6. [सर्वोत्तम अभ्यास र अनुकूलन](../../../Module05)
7. [वास्तविक-विश्व अनुप्रयोगहरू](../../../Module05)
8. [निष्कर्ष](../../../Module05)

## मोडेल डिस्टिलेशनको परिचय {#introduction}

मोडेल डिस्टिलेशन एक शक्तिशाली प्रविधि हो जसले हामीलाई सानो, अधिक कुशल मोडेलहरू निर्माण गर्न अनुमति दिन्छ, जबकि ठूला, जटिल मोडेलहरूको प्रदर्शनलाई धेरै हदसम्म कायम राख्छ। यो प्रक्रियाले एक संकुचित "विद्यार्थी" मोडेललाई ठूला "शिक्षक" मोडेलको व्यवहारलाई अनुकरण गर्न प्रशिक्षण दिन्छ।

**मुख्य फाइदाहरू:**
- **कम कम्प्युटेशनल आवश्यकताहरू** अनुमानका लागि
- **कम मेमोरी प्रयोग** र भण्डारण आवश्यकताहरू
- **छिटो अनुमान समय** यथोचित शुद्धता कायम राख्दै
- **स्रोत-सीमित वातावरणमा लागत-प्रभावकारी परिनियोजन**

## डिस्टिलेशन किन महत्त्वपूर्ण छ {#why-distillation-matters}

ठूला भाषा मोडेलहरू (LLMs) दिन प्रतिदिन शक्तिशाली हुँदै गइरहेका छन्, तर स्रोत-गहन पनि। जबकि अर्बौं प्यारामिटर भएको मोडेलले उत्कृष्ट परिणाम दिन सक्छ, धेरै वास्तविक-विश्व अनुप्रयोगहरूको लागि यो व्यावहारिक नहुन सक्छ निम्न कारणले:

### स्रोत सीमाहरू
- **कम्प्युटेशनल ओभरहेड**: ठूला मोडेलहरूले धेरै GPU मेमोरी र प्रशोधन शक्ति आवश्यक पर्छ
- **अनुमान ढिलाइ**: जटिल मोडेलहरूले प्रतिक्रिया उत्पन्न गर्न समय लिन्छ
- **ऊर्जा खपत**: ठूला मोडेलहरूले धेरै शक्ति खपत गर्छन्, जसले सञ्चालन लागत बढाउँछ
- **पूर्वाधार लागत**: ठूला मोडेल होस्ट गर्न महँगो हार्डवेयर आवश्यक पर्छ

### व्यावहारिक सीमाहरू
- **मोबाइल परिनियोजन**: ठूला मोडेलहरू मोबाइल उपकरणहरूमा कुशलतापूर्वक चल्न सक्दैनन्
- **वास्तविक-समय अनुप्रयोगहरू**: कम ढिलाइ चाहिने अनुप्रयोगहरूले ढिलो अनुमानलाई समायोजन गर्न सक्दैनन्
- **एज कम्प्युटिङ**: IoT र एज उपकरणहरूमा सीमित कम्प्युटेशनल स्रोतहरू हुन्छन्
- **लागत विचारहरू**: धेरै संस्थाहरूले ठूला मोडेल परिनियोजनको लागि पूर्वाधार खर्च गर्न सक्दैनन्

## डिस्टिलेशन प्रक्रिया {#the-distillation-process}

मोडेल डिस्टिलेशनले शिक्षक मोडेलबाट विद्यार्थी मोडेलमा ज्ञान स्थानान्तरण गर्ने दुई-चरणीय प्रक्रिया अनुसरण गर्दछ:

### चरण १: कृत्रिम डाटा उत्पादन

शिक्षक मोडेलले तपाईंको प्रशिक्षण डाटासेटको लागि प्रतिक्रिया उत्पन्न गर्छ, जसले शिक्षकको ज्ञान र तर्क गर्ने ढाँचालाई समेट्ने उच्च-गुणस्तरको कृत्रिम डाटा सिर्जना गर्छ।

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**यो चरणका मुख्य पक्षहरू:**
- शिक्षक मोडेलले प्रत्येक प्रशिक्षण उदाहरणलाई प्रक्रिया गर्छ
- उत्पन्न प्रतिक्रिया विद्यार्थी प्रशिक्षणको लागि "ग्राउन्ड ट्रुथ" बन्छ
- यो प्रक्रियाले शिक्षकको निर्णय गर्ने ढाँचालाई समेट्छ
- कृत्रिम डाटाको गुणस्तरले विद्यार्थी मोडेलको प्रदर्शनलाई प्रत्यक्ष रूपमा असर गर्छ

### चरण २: विद्यार्थी मोडेलको फाइन-ट्युनिङ

विद्यार्थी मोडेललाई कृत्रिम डाटासेटमा प्रशिक्षण दिइन्छ, जसले शिक्षकको व्यवहार र प्रतिक्रियालाई पुन: उत्पादन गर्न सिक्छ।

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**प्रशिक्षण उद्देश्यहरू:**
- विद्यार्थी र शिक्षकको उत्पादनहरू बीचको भिन्नतालाई न्यूनतम बनाउनु
- शिक्षकको ज्ञानलाई सानो प्यारामिटर स्पेसमा सुरक्षित गर्नु
- मोडेलको जटिलता घटाउँदै प्रदर्शन कायम राख्नु

## व्यावहारिक कार्यान्वयन {#practical-implementation}

### शिक्षक र विद्यार्थी मोडेल चयन

**शिक्षक मोडेल चयन:**
- तपाईंको विशिष्ट कार्यमा प्रमाणित प्रदर्शन भएका ठूला-स्तरका LLMs (१००B+ प्यारामिटर) चयन गर्नुहोस्
- लोकप्रिय शिक्षक मोडेलहरू समावेश छन्:
  - **DeepSeek V3** (६७१B प्यारामिटर) - तर्क र कोड उत्पादनका लागि उत्कृष्ट
  - **Meta Llama 3.1 405B Instruct** - व्यापक सामान्य-उद्देश्य क्षमताहरू
  - **GPT-4** - विविध कार्यहरूमा बलियो प्रदर्शन
  - **Claude 3.5 Sonnet** - जटिल तर्क कार्यहरूको लागि उत्कृष्ट
- सुनिश्चित गर्नुहोस् कि शिक्षक मोडेलले तपाईंको डोमेन-विशिष्ट डाटामा राम्रो प्रदर्शन गर्छ

**विद्यार्थी मोडेल चयन:**
- मोडेल आकार र प्रदर्शन आवश्यकताहरू बीच सन्तुलन कायम गर्नुहोस्
- कुशल, साना मोडेलहरूमा ध्यान दिनुहोस्, जस्तै:
  - **Microsoft Phi-4-mini** - तर्क क्षमताहरूमा बलियो नवीनतम कुशल मोडेल
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K र 128K भेरियन्टहरू)
  - Microsoft Phi-3.5 Mini Instruct

### कार्यान्वयन चरणहरू

1. **डाटा तयारी**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **शिक्षक मोडेल सेटअप**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **कृत्रिम डाटा उत्पादन**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **विद्यार्थी मोडेल प्रशिक्षण**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Azure ML डिस्टिलेशन उदाहरण {#azure-ml-example}

Azure Machine Learning मोडेल डिस्टिलेशन कार्यान्वयनको लागि व्यापक प्लेटफर्म प्रदान गर्दछ। यहाँ Azure ML लाई तपाईंको डिस्टिलेशन वर्कफ्लोको लागि कसरी उपयोग गर्ने भन्ने विवरण छ:

### पूर्वापेक्षाहरू

1. **Azure ML Workspace**: उपयुक्त क्षेत्रमा तपाईंको कार्यक्षेत्र सेटअप गर्नुहोस्
   - ठूला-स्तरका शिक्षक मोडेलहरूमा पहुँच सुनिश्चित गर्नुहोस् (DeepSeek V3, Llama 405B)
   - मोडेल उपलब्धताका आधारमा क्षेत्रहरू कन्फिगर गर्नुहोस्

2. **कम्प्युट स्रोतहरू**: प्रशिक्षणका लागि उपयुक्त कम्प्युट इन्स्ट्यान्सहरू कन्फिगर गर्नुहोस्
   - शिक्षक मोडेल अनुमानका लागि उच्च-मेमोरी इन्स्ट्यान्सहरू
   - विद्यार्थी मोडेल फाइन-ट्युनिङका लागि GPU-सक्षम कम्प्युट

### समर्थित कार्य प्रकारहरू

Azure ML विभिन्न कार्यहरूको लागि डिस्टिलेशनलाई समर्थन गर्दछ:

- **प्राकृतिक भाषा व्याख्या (NLI)**
- **वार्तालापात्मक AI**
- **प्रश्न र उत्तर (QA)**
- **गणितीय तर्क**
- **पाठ संक्षेपण**

### नमूना कार्यान्वयन

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### अनुगमन र मूल्याङ्कन

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## सर्वोत्तम अभ्यास र अनुकूलन {#best-practices}

### डाटा गुणस्तर

**उच्च-गुणस्तरको प्रशिक्षण डाटा महत्त्वपूर्ण छ:**
- विविध र प्रतिनिधि प्रशिक्षण उदाहरण सुनिश्चित गर्नुहोस्
- सम्भव भएमा डोमेन-विशिष्ट डाटा प्रयोग गर्नुहोस्
- विद्यार्थी प्रशिक्षणको लागि शिक्षक मोडेल आउटपुटहरू प्रयोग गर्नु अघि मान्य गर्नुहोस्
- विद्यार्थी मोडेल सिकाइमा पूर्वाग्रह रोक्न डाटासेट सन्तुलन गर्नुहोस्

### हाइपरप्यारामिटर ट्युनिङ

**अनुकूलन गर्नुपर्ने मुख्य प्यारामिटरहरू:**
- **लर्निङ दर**: फाइन-ट्युनिङका लागि साना दरहरू (१e-५ देखि ५e-५) बाट सुरु गर्नुहोस्
- **ब्याच साइज**: मेमोरी सीमाहरू र प्रशिक्षण स्थिरताबीच सन्तुलन कायम गर्नुहोस्
- **एपोक्सको संख्या**: ओभरफिटिङको लागि निगरानी गर्नुहोस्; सामान्यतया २-५ एपोक्स पर्याप्त हुन्छ
- **तापमान स्केलिङ**: राम्रो ज्ञान स्थानान्तरणका लागि शिक्षक आउटपुटको नरमतालाई समायोजन गर्नुहोस्

### मोडेल आर्किटेक्चर विचारहरू

**शिक्षक-विद्यार्थी अनुकूलता:**
- शिक्षक र विद्यार्थी मोडेलहरू बीच आर्किटेक्चरल अनुकूलता सुनिश्चित गर्नुहोस्
- राम्रो ज्ञान स्थानान्तरणका लागि मध्यवर्ती तह मिलान विचार गर्नुहोस्
- लागू हुँदा ध्यान स्थानान्तरण प्रविधिहरू प्रयोग गर्नुहोस्

### मूल्याङ्कन रणनीतिहरू

**व्यापक मूल्याङ्कन दृष्टिकोण:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## वास्तविक-विश्व अनुप्रयोगहरू {#real-world-applications}

### मोबाइल र एज परिनियोजन

डिस्टिल गरिएको मोडेलहरूले स्रोत-सीमित उपकरणहरूमा AI क्षमताहरू सक्षम बनाउँछ:
- **स्मार्टफोन अनुप्रयोगहरू** वास्तविक-समय पाठ प्रशोधनका साथ
- **IoT उपकरणहरू** स्थानीय अनुमान प्रदर्शन गर्दै
- **एम्बेडेड प्रणालीहरू** सीमित कम्प्युटेशनल स्रोतहरूसँग

### लागत-प्रभावकारी उत्पादन प्रणालीहरू

संस्थाहरूले सञ्चालन लागत घटाउन डिस्टिलेशन प्रयोग गर्छन्:
- **ग्राहक सेवा च्याटबोटहरू** छिटो प्रतिक्रिया समयका साथ
- **सामग्री मोडरेशन प्रणालीहरू** उच्च मात्रा कुशलतापूर्वक प्रक्रिया गर्दै
- **वास्तविक-समय अनुवाद सेवाहरू** कम ढिलाइ आवश्यकताका साथ

### डोमेन-विशिष्ट अनुप्रयोगहरू

डिस्टिलेशनले विशेष मोडेलहरू सिर्जना गर्न मद्दत गर्छ:
- **चिकित्सा निदान सहयोग** गोपनीयता-संरक्षण स्थानीय अनुमानका साथ
- **कानूनी दस्तावेज विश्लेषण** विशिष्ट कानूनी डोमेनका लागि अनुकूलित
- **वित्तीय जोखिम मूल्याङ्कन** छिटो निर्णय गर्ने क्षमताका साथ

### केस अध्ययन: ग्राहक समर्थन DeepSeek V3 → Phi-4-mini

एक प्रविधि कम्पनीले आफ्नो ग्राहक समर्थन प्रणालीको लागि डिस्टिलेशन कार्यान्वयन गर्‍यो:

**कार्यान्वयन विवरण:**
- **शिक्षक मोडेल**: DeepSeek V3 (६७१B प्यारामिटर) - जटिल ग्राहक प्रश्नहरूको लागि उत्कृष्ट तर्क
- **विद्यार्थी मोडेल**: Phi-4-mini - छिटो अनुमान र परिनियोजनका लागि अनुकूलित
- **प्रशिक्षण डाटा**: ५०,००० ग्राहक समर्थन वार्तालापहरू
- **कार्य**: बहु-टर्न वार्तालाप समर्थन प्राविधिक समस्या समाधानका साथ

**प्राप्त परिणामहरू:**
- **८५% कमी** अनुमान समयमा (३.२ सेकेन्डबाट ०.४८ सेकेन्ड प्रति प्रतिक्रिया)
- **९५% कमी** मेमोरी आवश्यकतामा (१.२TB बाट ६०GB)
- **९२% प्रतिधारण** मूल मोडेल शुद्धताको ग्राहक समर्थन कार्यहरूमा
- **६०% कमी** सञ्चालन लागतमा
- **सुधारिएको स्केलेबिलिटी** - अब १०x बढी समवर्ती प्रयोगकर्ताहरूलाई सम्हाल्न सक्षम

**प्रदर्शन विवरण:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## निष्कर्ष {#conclusion}

मोडेल डिस्टिलेशनले उन्नत AI क्षमताहरूको पहुँचलाई लोकतान्त्रिक बनाउन महत्त्वपूर्ण प्रविधि प्रतिनिधित्व गर्छ। ठूला मोडेलहरूको प्रदर्शनलाई धेरै हदसम्म कायम राख्दै साना, अधिक कुशल मोडेलहरू सिर्जना गर्न सक्षम बनाउँदै, डिस्टिलेशनले व्यावहारिक AI परिनियोजनको बढ्दो आवश्यकतालाई सम्बोधन गर्छ।

### मुख्य निष्कर्षहरू

1. **डिस्टिलेशनले पुल बनाउँछ** मोडेल प्रदर्शन र व्यावहारिक सीमाहरू बीच
2. **दुई-चरणीय प्रक्रिया** शिक्षकबाट विद्यार्थीमा प्रभावकारी ज्ञान स्थानान्तरण सुनिश्चित गर्छ
3. **Azure ML बलियो पूर्वाधार प्रदान गर्छ** डिस्टिलेशन वर्कफ्लो कार्यान्वयनका लागि
4. **उचित मूल्याङ्कन र अनुकूलन** सफल डिस्टिलेशनका लागि आवश्यक छ
5. **वास्तविक-विश्व अनुप्रयोगहरू** लागत, गति, र पहुँचमा महत्त्वपूर्ण फाइदाहरू देखाउँछन्

### भविष्यको दिशा

जसरी क्षेत्र विकसित हुँदै जान्छ, हामीले अपेक्षा गर्न सक्छौं:
- **उन्नत डिस्टिलेशन प्रविधिहरू** राम्रो ज्ञान स्थानान्तरण विधिहरूसँग
- **बहु-शिक्षक डिस्टिलेशन** विद्यार्थी मोडेल क्षमताहरू सुधार गर्न
- **डिस्टिलेशन प्रक्रियाको स्वचालित अनुकूलन**
- **विभिन्न आर्किटेक्चर र डोमेनहरूमा व्यापक मोडेल समर्थन**

मोडेल डिस्टिलेशनले संस्थाहरूलाई उन्नत भाषा मोडेलहरूको पहुँच प्रदान गर्दै व्यावहारिक परिनियोजन सीमाहरू कायम राख्न सशक्त बनाउँछ, जसले उन्नत AI क्षमताहरूलाई अनुप्रयोग र वातावरणहरूको विस्तृत दायरामा पहुँचयोग्य बनाउँछ।

## ➡️ अब के गर्ने

- [03: फाइन-ट्युनिङ - विशिष्ट कार्यहरूको लागि मोडेलहरू अनुकूलित गर्दै](./03.SLMOps-Finetuing.md)

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।