<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T20:33:45+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ne"
}
-->
# खण्ड ०३ - मोडल कन्टेक्स्ट प्रोटोकल (MCP) एकीकरण

## MCP (मोडल कन्टेक्स्ट प्रोटोकल) को परिचय

मोडल कन्टेक्स्ट प्रोटोकल (MCP) एक क्रान्तिकारी ढाँचा हो जसले भाषा मोडेलहरूलाई बाह्य उपकरण र प्रणालीहरूसँग मानकीकृत तरिकामा अन्तरक्रिया गर्न सक्षम बनाउँछ। परम्परागत दृष्टिकोणहरूमा जहाँ मोडेलहरू अलग-थलग हुन्छन्, MCP ले एक स्पष्ट परिभाषित प्रोटोकल मार्फत AI मोडेलहरू र वास्तविक संसारबीच पुल निर्माण गर्दछ।

### MCP के हो?

MCP एक सञ्चार प्रोटोकलको रूपमा कार्य गर्दछ जसले भाषा मोडेलहरूलाई निम्न कार्यहरू गर्न अनुमति दिन्छ:
- बाह्य डाटा स्रोतहरूसँग जडान गर्न
- उपकरण र कार्यहरू कार्यान्वयन गर्न
- API र सेवाहरूसँग अन्तरक्रिया गर्न
- वास्तविक-समय जानकारी पहुँच गर्न
- जटिल बहु-चरणीय कार्यहरू प्रदर्शन गर्न

यस प्रोटोकलले स्थिर भाषा मोडेलहरूलाई गतिशील एजेन्टहरूमा रूपान्तरण गर्दछ, जसले पाठ उत्पादनभन्दा पर व्यावहारिक कार्यहरू गर्न सक्षम बनाउँछ।

## साना भाषा मोडेलहरू (SLMs) MCP मा

साना भाषा मोडेलहरूले AI परिनियोजनको लागि एक कुशल दृष्टिकोण प्रस्तुत गर्दछ, जसले धेरै फाइदाहरू प्रदान गर्दछ:

### SLMs का फाइदाहरू
- **स्रोत दक्षता**: कम कम्प्युटेशनल आवश्यकताहरू
- **छिटो प्रतिक्रिया समय**: वास्तविक-समय अनुप्रयोगहरूको लागि कम विलम्बता  
- **लागत प्रभावकारिता**: न्यूनतम पूर्वाधार आवश्यकताहरू
- **गोपनीयता**: स्थानीय रूपमा चलाउन सकिने, डाटा प्रसारण बिना
- **अनुकूलन**: विशिष्ट डोमेनहरूको लागि सजिलै परिमार्जन गर्न सकिने

### MCP सँग SLMs किन राम्रोसँग काम गर्छन्

SLMs र MCP को संयोजनले एक शक्तिशाली संयोजन सिर्जना गर्दछ, जहाँ मोडेलको तर्क क्षमताहरू बाह्य उपकरणहरूले बढाउँछन्, साना प्यारामिटरहरूको सीमालाई कार्यक्षमताको माध्यमबाट पूर्ति गर्छन्।

## Python MCP SDK को अवलोकन

Python MCP SDK ले MCP-सक्षम अनुप्रयोगहरू निर्माण गर्नको लागि आधार प्रदान गर्दछ। SDK मा समावेश छन्:

- **क्लाइन्ट पुस्तकालयहरू**: MCP सर्भरहरूसँग जडान गर्नका लागि
- **सर्भर फ्रेमवर्क**: अनुकूल MCP सर्भरहरू सिर्जना गर्नका लागि
- **प्रोटोकल ह्यान्डलरहरू**: सञ्चार व्यवस्थापन गर्नका लागि
- **उपकरण एकीकरण**: बाह्य कार्यहरू कार्यान्वयन गर्नका लागि

## व्यावहारिक कार्यान्वयन: Phi-4 MCP क्लाइन्ट

Microsoft को Phi-4 मिनी मोडेललाई MCP क्षमताहरूको साथ एकीकृत गरेर वास्तविक-विश्व कार्यान्वयन अन्वेषण गरौं।

### प्रणाली वास्तुकला

कार्यान्वयनले तहगत वास्तुकलालाई पछ्याउँछ:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### मुख्य घटकहरू

#### १. MCP क्लाइन्ट कक्षाहरू

**BaseMCPClient**: सामान्य कार्यक्षमता प्रदान गर्ने अमूर्त आधार
- Async कन्टेक्स्ट म्यानेजर प्रोटोकल
- मानक इन्टरफेस परिभाषा
- स्रोत व्यवस्थापन

**Phi4MiniMCPClient**: STDIO-आधारित कार्यान्वयन
- स्थानीय प्रक्रिया सञ्चार
- मानक इनपुट/आउटपुट ह्यान्डलिङ
- सबप्रोसेस व्यवस्थापन

**Phi4MiniSSEMCPClient**: सर्भर-सेन्ट इभेन्ट्स कार्यान्वयन
- HTTP स्ट्रिमिङ सञ्चार
- वास्तविक-समय इभेन्ट ह्यान्डलिङ
- वेब-आधारित सर्भर जडान

#### २. LLM एकीकरण

**OllamaClient**: स्थानीय मोडेल होस्टिङ
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: उच्च-प्रदर्शन सेवा
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### ३. उपकरण प्रशोधन पाइपलाइन

उपकरण प्रशोधन पाइपलाइनले MCP उपकरणहरूलाई भाषा मोडेलहरूसँग उपयुक्त ढाँचामा रूपान्तरण गर्दछ:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## सुरु गर्दै: चरण-दर-चरण मार्गदर्शन

### चरण १: वातावरण सेटअप

आवश्यक निर्भरताहरू स्थापना गर्नुहोस्:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### चरण २: आधारभूत कन्फिगरेसन

आफ्नो वातावरण चरहरू सेट गर्नुहोस्:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### चरण ३: आफ्नो पहिलो MCP क्लाइन्ट चलाउनुहोस्

**आधारभूत Ollama सेटअप:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM ब्याकएन्ड प्रयोग गर्दै:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**सर्भर-सेन्ट इभेन्ट्स जडान:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**अनुकूल MCP सर्भर:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### चरण ४: प्रोग्रामेटिक प्रयोग

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## उन्नत सुविधाहरू

### बहु-ब्याकएन्ड समर्थन

कार्यान्वयनले Ollama र vLLM दुबै ब्याकएन्डहरूलाई समर्थन गर्दछ, जसले तपाईंलाई आवश्यकताहरूको आधारमा छनोट गर्न अनुमति दिन्छ:

- **Ollama**: स्थानीय विकास र परीक्षणको लागि राम्रो
- **vLLM**: उत्पादन र उच्च-थ्रुपुट परिदृश्यहरूको लागि अनुकूलित

### लचिलो जडान प्रोटोकलहरू

दुई जडान मोडहरू समर्थित छन्:

**STDIO मोड**: प्रत्यक्ष प्रक्रिया सञ्चार
- कम विलम्बता
- स्थानीय उपकरणहरूको लागि उपयुक्त
- सरल सेटअप

**SSE मोड**: HTTP-आधारित स्ट्रिमिङ
- नेटवर्क-सक्षम
- वितरित प्रणालीहरूको लागि राम्रो
- वास्तविक-समय अपडेटहरू

### उपकरण एकीकरण क्षमताहरू

प्रणालीले विभिन्न उपकरणहरूसँग एकीकरण गर्न सक्छ:
- वेब स्वचालन (Playwright)
- फाइल अपरेसनहरू
- API अन्तरक्रिया
- प्रणाली आदेशहरू
- अनुकूल कार्यहरू

## त्रुटि व्यवस्थापन र उत्तम अभ्यासहरू

### व्यापक त्रुटि व्यवस्थापन

कार्यान्वयनले निम्नका लागि बलियो त्रुटि व्यवस्थापन समावेश गर्दछ:

**जडान त्रुटिहरू:**
- MCP सर्भर असफलताहरू
- नेटवर्क टाइमआउटहरू
- जडान समस्याहरू

**उपकरण कार्यान्वयन त्रुटिहरू:**
- हराएका उपकरणहरू
- प्यारामिटर मान्यता
- कार्यान्वयन असफलताहरू

**प्रतिक्रिया प्रशोधन त्रुटिहरू:**
- JSON पार्सिङ समस्याहरू
- ढाँचा असंगतताहरू
- LLM प्रतिक्रिया असामान्यताहरू

### उत्तम अभ्यासहरू

1. **स्रोत व्यवस्थापन**: Async कन्टेक्स्ट म्यानेजरहरू प्रयोग गर्नुहोस्
2. **त्रुटि व्यवस्थापन**: व्यापक try-catch ब्लकहरू कार्यान्वयन गर्नुहोस्
3. **लगिङ**: उपयुक्त लगिङ स्तरहरू सक्षम गर्नुहोस्
4. **सुरक्षा**: इनपुटहरू मान्यता गर्नुहोस् र आउटपुटहरू सफा गर्नुहोस्
5. **प्रदर्शन**: जडान पूलिङ र क्यासिङ प्रयोग गर्नुहोस्

## वास्तविक-विश्व अनुप्रयोगहरू

### वेब स्वचालन
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### डाटा प्रशोधन
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API एकीकरण
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## प्रदर्शन अनुकूलन

### मेमोरी व्यवस्थापन
- सन्देश इतिहासको कुशल ह्यान्डलिङ
- उचित स्रोत सफाइ
- जडान पूलिङ

### नेटवर्क अनुकूलन
- Async HTTP अपरेसनहरू
- कन्फिगरेबल टाइमआउटहरू
- अनुग्रहपूर्ण त्रुटि पुन: प्राप्ति

### समवर्ती प्रशोधन
- गैर-अवरोधक I/O
- समानान्तर उपकरण कार्यान्वयन
- कुशल Async ढाँचाहरू

## सुरक्षा विचारहरू

### डाटा सुरक्षा
- सुरक्षित API कुञ्जी व्यवस्थापन
- इनपुट मान्यता
- आउटपुट सफाइ

### नेटवर्क सुरक्षा
- HTTPS समर्थन
- स्थानीय अन्तबिन्दु पूर्वनिर्धारित
- सुरक्षित टोकन ह्यान्डलिङ

### कार्यान्वयन सुरक्षा
- उपकरण फिल्टरिङ
- स्यान्डबक्स वातावरण
- अडिट लगिङ

## निष्कर्ष

MCP सँग एकीकृत SLMs ले AI अनुप्रयोग विकासमा एक नयाँ दृष्टिकोण प्रस्तुत गर्दछ। साना मोडेलहरूको दक्षतालाई बाह्य उपकरणहरूको शक्तिसँग संयोजन गरेर, विकासकर्ताहरूले स्रोत-कुशल र अत्यधिक सक्षम बौद्धिक प्रणालीहरू निर्माण गर्न सक्छन्।

Phi-4 MCP क्लाइन्ट कार्यान्वयनले यो एकीकरण कसरी व्यवहारमा हासिल गर्न सकिन्छ भन्ने देखाउँछ, जसले परिष्कृत AI-सक्षम अनुप्रयोगहरू निर्माण गर्नको लागि ठोस आधार प्रदान गर्दछ।

मुख्य बुँदाहरू:
- MCP ले भाषा मोडेलहरू र बाह्य प्रणालीहरू बीचको खाडललाई पूर्ति गर्दछ
- SLMs ले उपकरणहरूको साथ बढाउँदा दक्षता बिना क्षमतामा सम्झौता गर्दैन
- मोड्युलर वास्तुकलाले सजिलो विस्तार र अनुकूलन सक्षम गर्दछ
- उत्पादन प्रयोगको लागि उचित त्रुटि व्यवस्थापन र सुरक्षा उपायहरू आवश्यक छन्

यो ट्युटोरियलले SLM-सक्षम MCP अनुप्रयोगहरू निर्माण गर्नको लागि आधार प्रदान गर्दछ, जसले स्वचालन, डाटा प्रशोधन, र बौद्धिक प्रणाली एकीकरणको लागि सम्भावनाहरू खोल्छ।

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव सटीकता सुनिश्चित गर्न प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटि वा अशुद्धता हुन सक्छ। यसको मूल भाषामा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।  