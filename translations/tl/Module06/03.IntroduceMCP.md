<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T14:22:31+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "tl"
}
-->
# Seksyon 03 - Integrasyon ng Model Context Protocol (MCP)

## Panimula sa MCP (Model Context Protocol)

Ang Model Context Protocol (MCP) ay isang makabagong framework na nagbibigay-daan sa mga language model na makipag-ugnayan sa mga panlabas na tool at sistema sa isang standardized na paraan. Hindi tulad ng tradisyunal na mga pamamaraan kung saan ang mga modelo ay hiwalay, ang MCP ay lumilikha ng tulay sa pagitan ng mga AI model at ng totoong mundo sa pamamagitan ng isang maayos na protocol.

### Ano ang MCP?

Ang MCP ay nagsisilbing komunikasyon na protocol na nagbibigay-daan sa mga language model na:
- Kumonekta sa mga panlabas na pinagmulan ng datos
- Magpatakbo ng mga tool at function
- Makipag-ugnayan sa mga API at serbisyo
- Mag-access ng real-time na impormasyon
- Magsagawa ng mga kumplikadong multi-step na operasyon

Binabago ng protocol na ito ang mga static na language model sa mga dynamic na ahente na may kakayahang magsagawa ng mga praktikal na gawain na lampas sa simpleng text generation.

## Maliit na Language Model (SLMs) sa MCP

Ang Maliit na Language Model ay kumakatawan sa isang epektibong paraan ng AI deployment, na may maraming benepisyo:

### Mga Benepisyo ng SLMs
- **Pagiging Epektibo sa Resource**: Mas mababang computational na pangangailangan
- **Mas Mabilis na Tugon**: Nabawasang latency para sa real-time na aplikasyon  
- **Pagiging Matipid**: Minimal na pangangailangan sa imprastraktura
- **Privacy**: Maaaring patakbuhin nang lokal nang walang data transmission
- **Pag-customize**: Mas madaling i-fine-tune para sa partikular na mga domain

### Bakit Maganda ang SLMs sa MCP

Ang pagsasama ng SLMs at MCP ay lumilikha ng isang makapangyarihang kombinasyon kung saan ang kakayahan ng modelo sa pag-reason ay pinapalakas ng mga panlabas na tool, na bumabawi sa mas maliit na bilang ng parameter sa pamamagitan ng pinahusay na functionality.

## Python MCP SDK Overview

Ang Python MCP SDK ay nagbibigay ng pundasyon para sa pagbuo ng mga MCP-enabled na aplikasyon. Kasama sa SDK ang:

- **Client Libraries**: Para sa pagkonekta sa MCP servers
- **Server Framework**: Para sa paglikha ng custom na MCP servers
- **Protocol Handlers**: Para sa pamamahala ng komunikasyon
- **Tool Integration**: Para sa pagpapatakbo ng mga panlabas na function

## Praktikal na Implementasyon: Phi-4 MCP Client

Tuklasin natin ang isang real-world na implementasyon gamit ang Phi-4 mini model ng Microsoft na may integrasyon ng MCP capabilities.

### Arkitektura ng Sistema

Ang implementasyon ay sumusunod sa isang layered na arkitektura:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Mga Pangunahing Komponent

#### 1. MCP Client Classes

**BaseMCPClient**: Abstract na pundasyon na nagbibigay ng karaniwang functionality
- Async context manager protocol
- Standard na interface definition
- Resource management

**Phi4MiniMCPClient**: STDIO-based na implementasyon
- Lokal na komunikasyon ng proseso
- Pamamahala ng standard input/output
- Subprocess management

**Phi4MiniSSEMCPClient**: Server-Sent Events na implementasyon
- HTTP streaming na komunikasyon
- Real-time na event handling
- Web-based na koneksyon sa server

#### 2. LLM Integration

**OllamaClient**: Lokal na pagho-host ng modelo
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: High-performance na serving
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Tool Processing Pipeline

Ang tool processing pipeline ay nagta-transform ng MCP tools sa mga format na compatible sa language models:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Pagsisimula: Step-by-Step na Gabay

### Hakbang 1: Environment Setup

I-install ang mga kinakailangang dependencies:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Hakbang 2: Pangunahing Konfigurasyon

I-set up ang iyong environment variables:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Hakbang 3: Pagpapatakbo ng Iyong Unang MCP Client

**Basic Ollama Setup:**
```bash
python ghmodel_mcp_demo.py
```

**Gamit ang vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events Connection:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Custom MCP Server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Hakbang 4: Programmatic Usage

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Mga Advanced na Tampok

### Multi-Backend Support

Ang implementasyon ay sumusuporta sa parehong Ollama at vLLM backends, na nagbibigay-daan sa iyo na pumili base sa iyong pangangailangan:

- **Ollama**: Mas maganda para sa lokal na development at testing
- **vLLM**: Na-optimize para sa production at high-throughput na mga scenario

### Flexible Connection Protocols

Dalawang mode ng koneksyon ang sinusuportahan:

**STDIO Mode**: Direktang komunikasyon ng proseso
- Mas mababang latency
- Angkop para sa lokal na tools
- Simpleng setup

**SSE Mode**: HTTP-based na streaming
- Network-capable
- Mas maganda para sa distributed systems
- Real-time na updates

### Tool Integration Capabilities

Ang sistema ay maaaring mag-integrate sa iba't ibang tools:
- Web automation (Playwright)
- File operations
- API interactions
- System commands
- Custom functions

## Error Handling at Best Practices

### Comprehensive Error Management

Ang implementasyon ay may matibay na error handling para sa:

**Connection Errors:**
- MCP server failures
- Network timeouts
- Connectivity issues

**Tool Execution Errors:**
- Nawawalang tools
- Parameter validation
- Execution failures

**Response Processing Errors:**
- JSON parsing issues
- Format inconsistencies
- LLM response anomalies

### Best Practices

1. **Resource Management**: Gumamit ng async context managers
2. **Error Handling**: Mag-implement ng comprehensive try-catch blocks
3. **Logging**: I-enable ang tamang logging levels
4. **Security**: I-validate ang inputs at i-sanitize ang outputs
5. **Performance**: Gumamit ng connection pooling at caching

## Mga Real-World na Aplikasyon

### Web Automation
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Data Processing
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API Integration
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Performance Optimization

### Memory Management
- Epektibong pamamahala ng message history
- Tamang resource cleanup
- Connection pooling

### Network Optimization
- Async HTTP operations
- Configurable timeouts
- Graceful error recovery

### Concurrent Processing
- Non-blocking I/O
- Parallel tool execution
- Epektibong async patterns

## Mga Pagsasaalang-alang sa Seguridad

### Proteksyon ng Datos
- Secure na pamamahala ng API key
- Input validation
- Output sanitization

### Network Security
- HTTPS support
- Lokal na endpoint defaults
- Secure token handling

### Kaligtasan sa Pagpapatakbo
- Tool filtering
- Sandboxed environments
- Audit logging

## Konklusyon

Ang SLMs na may integrasyon ng MCP ay kumakatawan sa isang pagbabago sa pag-develop ng AI application. Sa pamamagitan ng pagsasama ng pagiging epektibo ng maliit na modelo at ang kapangyarihan ng mga panlabas na tool, maaaring lumikha ang mga developer ng mga intelligent na sistema na parehong resource-efficient at lubos na may kakayahan.

Ang implementasyon ng Phi-4 MCP client ay nagpapakita kung paano maaaring makamit ang integrasyong ito sa praktika, na nagbibigay ng matibay na pundasyon para sa pagbuo ng mga sopistikadong AI-powered na aplikasyon.

Mga pangunahing puntos:
- Ang MCP ay lumilikha ng tulay sa pagitan ng language models at panlabas na sistema
- Ang SLMs ay nag-aalok ng pagiging epektibo nang hindi isinasakripisyo ang kakayahan kapag pinalakas ng mga tool
- Ang modular na arkitektura ay nagbibigay-daan sa madaling extension at customization
- Ang tamang error handling at mga hakbang sa seguridad ay mahalaga para sa production use

Ang tutorial na ito ay nagbibigay ng pundasyon para sa pagbuo ng iyong sariling SLM-powered MCP applications, na nagbubukas ng mga posibilidad para sa automation, data processing, at intelligent system integration.

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.