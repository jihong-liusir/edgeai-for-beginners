<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T14:52:42+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "tl"
}
-->
# Seksyon 4: Deployment - Pagsasakatuparan ng Modelong Handa na para sa Produksyon

## Pangkalahatang-ideya

Ang detalyadong tutorial na ito ay gagabay sa iyo sa buong proseso ng pag-deploy ng mga fine-tuned na quantized na modelo gamit ang Foundry Local. Saklaw nito ang conversion ng modelo, optimization ng quantization, at configuration ng deployment mula simula hanggang matapos.

## Mga Kinakailangan

Bago magsimula, tiyakin na mayroon ka ng mga sumusunod:

- ‚úÖ Isang fine-tuned na onnx model na handa para sa deployment
- ‚úÖ Windows o Mac na computer
- ‚úÖ Python 3.10 o mas mataas
- ‚úÖ Hindi bababa sa 8GB na available na RAM
- ‚úÖ Foundry Local na naka-install sa iyong sistema

## Bahagi 1: Pagsasaayos ng Kapaligiran

### Pag-install ng Mga Kinakailangang Tool

Buksan ang iyong terminal (Command Prompt sa Windows, Terminal sa Mac) at patakbuhin ang mga sumusunod na command nang sunod-sunod:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Mahalagang Paalala**: Kailangan mo rin ng CMake bersyon 3.31 o mas bago, na maaaring ma-download mula sa [cmake.org](https://cmake.org/download/).

## Bahagi 2: Conversion ng Modelo at Quantization

### Pagpili ng Tamang Format

Para sa mga fine-tuned na maliliit na language model, inirerekomenda naming gamitin ang **ONNX format** dahil ito ay nag-aalok ng:

- üöÄ Mas mahusay na performance optimization
- üîß Deployment na hindi nakadepende sa hardware
- üè≠ Kakayahang handa para sa produksyon
- üì± Compatibility sa iba't ibang platform

### Paraan 1: Conversion gamit ang Isang Command (Inirerekomenda)

Gamitin ang sumusunod na command upang direktang i-convert ang iyong fine-tuned na modelo:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Paliwanag ng Parameter:**
- `--model_name_or_path`: Path ng iyong fine-tuned na modelo
- `--device cpu`: Gamitin ang CPU para sa optimization
- `--precision int4`: Gamitin ang INT4 quantization (humigit-kumulang 75% na pagbabawas sa laki)
- `--output_path`: Output path para sa na-convert na modelo

### Paraan 2: Diskarte gamit ang Configuration File (Para sa Advanced na User)

Gumawa ng configuration file na pinangalanang `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Pagkatapos, patakbuhin:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Paghahambing ng Mga Opsyon sa Quantization

| Precision | Laki ng File | Bilis ng Inference | Kalidad ng Modelo | Inirerekomendang Paggamit |
|-----------|--------------|--------------------|-------------------|--------------------------|
| FP16      | Baseline √ó 0.5 | Mabilis | Pinakamahusay | High-end na hardware |
| INT8      | Baseline √ó 0.25 | Napakabilis | Maganda | Balanseng pagpipilian |
| INT4      | Baseline √ó 0.125 | Pinakamabilis | Katanggap-tanggap | Limitadong resources |

üí° **Rekomendasyon**: Magsimula sa INT4 quantization para sa iyong unang deployment. Kung hindi sapat ang kalidad, subukan ang INT8 o FP16.

## Bahagi 3: Configuration ng Deployment gamit ang Foundry Local

### Paglikha ng Configuration ng Modelo

Pumunta sa Foundry Local models directory:

```bash
foundry cache cd ./models/
```

Gumawa ng istruktura ng directory para sa iyong modelo:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Gumawa ng `inference_model.json` na configuration file sa iyong model directory:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Mga Template ng Configuration na Tukoy sa Modelo

#### Para sa Qwen Series Models:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Bahagi 4: Pagsubok at Optimization ng Modelo

### Pag-verify ng Pagkakabit ng Modelo

Suriin kung makikilala ng Foundry Local ang iyong modelo:

```bash
foundry cache ls
```

Makikita mo ang `your-finetuned-model-int4` sa listahan.

### Pagsisimula ng Pagsubok ng Modelo

```bash
foundry model run your-finetuned-model-int4
```

### Benchmarking ng Performance

Subaybayan ang mga pangunahing sukatan habang nagsusubok:

1. **Response Time**: Sukatin ang average na oras bawat tugon
2. **Memory Usage**: Subaybayan ang konsumo ng RAM
3. **CPU Utilization**: Suriin ang load ng processor
4. **Output Quality**: Suriin ang kaugnayan at coherence ng tugon

### Checklist para sa Pagpapatunay ng Kalidad

- ‚úÖ Ang modelo ay tumutugon nang naaayon sa mga query sa fine-tuned na domain
- ‚úÖ Ang format ng tugon ay tumutugma sa inaasahang istruktura ng output
- ‚úÖ Walang memory leaks sa mahabang paggamit
- ‚úÖ Konsistent na performance sa iba't ibang haba ng input
- ‚úÖ Tamang paghawak sa mga edge cases at invalid na input

## Buod

Binabati kita! Matagumpay mong natapos ang:

- ‚úÖ Conversion ng format ng fine-tuned na modelo
- ‚úÖ Optimization ng quantization ng modelo
- ‚úÖ Configuration ng deployment gamit ang Foundry Local
- ‚úÖ Pagsasaayos ng performance at troubleshooting

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.