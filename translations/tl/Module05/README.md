<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2db7a2f6e9873c3cd09fea6736bf360b",
  "translation_date": "2025-09-18T14:46:48+00:00",
  "source_file": "Module05/README.md",
  "language_code": "tl"
}
-->
# Kabanata 05: SLMOps - Isang Komprehensibong Gabay sa Operasyon ng Maliit na Language Model

## Pangkalahatang-ideya

Ang SLMOps (Small Language Model Operations) ay kumakatawan sa isang rebolusyonaryong paraan ng pag-deploy ng AI na inuuna ang kahusayan, pagiging cost-effective, at kakayahan sa edge computing. Ang komprehensibong gabay na ito ay sumasaklaw sa buong lifecycle ng operasyon ng SLM, mula sa pag-unawa sa mga pangunahing konsepto hanggang sa pagpapatupad ng mga deployment na handa na para sa produksyon.

---

## [Seksyon 1: Panimula sa SLMOps](./01.IntroduceSLMOps.md)

**Pagbabago ng Operasyon ng AI sa Edge**

Ang pundasyong kabanata na ito ay nagpapakilala sa pagbabago mula sa tradisyunal na malakihang operasyon ng AI patungo sa Small Language Model Operations (SLMOps). Matutuklasan mo kung paano tinutugunan ng SLMOps ang mga kritikal na hamon sa pag-deploy ng AI sa malakihang sukat habang pinapanatili ang pagiging cost-efficient at pagsunod sa privacy.

**Ano ang Iyong Matututunan:**
- Ang pag-usbong at kahalagahan ng SLMOps sa modernong estratehiya ng AI
- Paano binubuo ng SLMs ang tulay sa pagitan ng performance at kahusayan sa resources
- Mga pangunahing prinsipyo ng operasyon kabilang ang matalinong pamamahala ng resources at arkitektura na inuuna ang privacy
- Mga hamon sa aktwal na implementasyon at ang kanilang mga solusyon
- Estratehikong epekto sa negosyo at mga competitive na bentahe

**Pangunahing Aral:** Ang SLMOps ay nagbibigay-daan sa democratization ng AI deployment sa pamamagitan ng paggawa ng advanced na kakayahan sa language processing na maa-access ng mga organisasyon na may limitadong teknikal na imprastraktura, na nagreresulta sa mas mabilis na development cycles at mas predictable na operational costs.

---

## [Seksyon 2: Model Distillation - Mula Teorya Hanggang Praktika](./02.SLMOps-Distillation.md)

**Paglikha ng Mahuhusay na Modelo sa Pamamagitan ng Knowledge Transfer**

Ang model distillation ay ang pangunahing teknik para sa paglikha ng mas maliit, mas mahusay na mga modelo na pinapanatili ang performance ng kanilang mas malalaking katapat. Ang kabanatang ito ay nagbibigay ng komprehensibong gabay sa pagpapatupad ng distillation workflows na naglilipat ng kaalaman mula sa malalaking teacher models patungo sa compact student models.

**Ano ang Iyong Matututunan:**
- Ang mga pangunahing konsepto at benepisyo ng model distillation
- Dalawang-yugto ng proseso ng distillation: synthetic data generation at student model training
- Mga estratehiya sa praktikal na implementasyon gamit ang mga state-of-the-art na modelo tulad ng DeepSeek V3 at Phi-4-mini
- Mga workflow ng Azure ML distillation na may mga hands-on na halimbawa
- Mga pinakamahusay na kasanayan para sa hyperparameter tuning at mga estratehiya sa pagsusuri
- Mga case study sa totoong mundo na nagpapakita ng makabuluhang pagbuti sa gastos at performance

**Pangunahing Aral:** Ang model distillation ay nagbibigay-daan sa mga organisasyon na makamit ang 85% na pagbawas sa inference time at 95% na pagbaba sa memory requirements habang pinapanatili ang 92% ng orihinal na model accuracy, na ginagawang praktikal ang advanced na kakayahan ng AI.

---

## [Seksyon 3: Fine-Tuning - Pag-customize ng Mga Modelo para sa Tiyak na Gawain](./03.SLMOps-Finetuing.md)

**Pag-aangkop ng Pre-trained Models sa Iyong Natatanging Pangangailangan**

Ang fine-tuning ay nagbabago ng mga general-purpose models sa mga espesyal na solusyon na iniayon sa iyong partikular na use cases at domains. Ang kabanatang ito ay sumasaklaw sa lahat mula sa simpleng pag-aayos ng parameter hanggang sa mga advanced na teknik tulad ng LoRA at QLoRA para sa mahusay na pag-customize ng modelo.

**Ano ang Iyong Matututunan:**
- Komprehensibong overview ng mga metodolohiya ng fine-tuning at ang kanilang mga aplikasyon
- Iba't ibang uri ng fine-tuning: full fine-tuning, parameter-efficient fine-tuning (PEFT), at mga task-specific na approach
- Hands-on na implementasyon gamit ang Microsoft Olive na may mga praktikal na halimbawa
- Mga advanced na teknik kabilang ang multi-adapter training at hyperparameter optimization
- Mga pinakamahusay na kasanayan para sa data preparation, training configuration, at resource management
- Mga karaniwang hamon at napatunayang solusyon para sa matagumpay na fine-tuning projects

**Pangunahing Aral:** Ang fine-tuning gamit ang mga tool tulad ng Microsoft Olive ay nagbibigay-daan sa mga organisasyon na mahusay na iangkop ang mga pre-trained models sa partikular na pangangailangan habang ina-optimize ang performance at resource constraints, na ginagawang maa-access ang state-of-the-art na AI sa iba't ibang aplikasyon.

---

## [Seksyon 4: Deployment - Implementasyon ng Modelong Handa na para sa Produksyon](./04.SLMOps.Deployment.md)

**Pagdadala ng Fine-tuned Models sa Produksyon gamit ang Foundry Local**

Ang huling kabanata ay nakatuon sa kritikal na yugto ng deployment, na sumasaklaw sa model conversion, quantization, at production configuration. Matutunan mo kung paano i-deploy ang fine-tuned quantized models gamit ang Foundry Local para sa optimal na performance at paggamit ng resources.

**Ano ang Iyong Matututunan:**
- Kumpletong setup ng environment at mga pamamaraan sa pag-install ng tool
- Mga teknik sa model conversion at quantization para sa iba't ibang deployment scenarios
- Foundry Local deployment configuration na may mga optimizations na partikular sa modelo
- Mga metodolohiya sa performance benchmarking at quality validation
- Pag-troubleshoot ng mga karaniwang isyu sa deployment at mga estratehiya sa optimization
- Mga pinakamahusay na kasanayan sa production monitoring at maintenance

**Pangunahing Aral:** Ang tamang deployment configuration gamit ang quantization techniques ay maaaring makamit ang hanggang 75% na pagbawas sa laki habang pinapanatili ang katanggap-tanggap na kalidad ng modelo, na nagbibigay-daan sa mahusay na production deployments sa iba't ibang hardware configurations.

---

## Pagsisimula

Ang gabay na ito ay idinisenyo upang dalhin ka sa buong paglalakbay ng SLMOps, mula sa pag-unawa sa mga pundasyong konsepto hanggang sa pagpapatupad ng mga deployment na handa na para sa produksyon. Ang bawat kabanata ay bumubuo sa nauna, na nagbibigay ng parehong teoretikal na pag-unawa at praktikal na kasanayan sa implementasyon.

Kung ikaw ay isang data scientist na naghahanap ng paraan upang i-optimize ang model deployment, isang DevOps engineer na nagpapatupad ng AI operations, o isang technical leader na nag-e-evaluate ng SLMOps para sa iyong organisasyon, ang komprehensibong gabay na ito ay nagbibigay ng kaalaman at mga tool na kinakailangan upang matagumpay na maipatupad ang Small Language Model Operations.

**Handa ka na bang magsimula?** Simulan sa Kabanata 1 upang maunawaan ang mga pangunahing prinsipyo ng SLMOps at bumuo ng pundasyon para sa mga advanced na teknik sa implementasyon na saklaw sa mga susunod na kabanata.

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.