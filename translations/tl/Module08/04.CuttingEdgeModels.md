<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-10-01T01:04:21+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "tl"
}
-->
# Session 4: Paggawa ng Production Chat Applications gamit ang Chainlit

## Pangkalahatang-ideya

Ang sesyon na ito ay nakatuon sa paggawa ng mga production-ready chat applications gamit ang Chainlit at Microsoft Foundry Local. Matututuhan mong lumikha ng modernong web interface para sa mga AI na pag-uusap, magpatupad ng streaming responses, at mag-deploy ng matibay na chat applications na may tamang error handling at disenyo ng user experience.

**Ano ang Iyong Itatayo:**
- **Chainlit Chat App**: Modernong web UI na may streaming responses
- **WebGPU Demo**: Pagpapatakbo ng inference sa browser para sa privacy-first na mga aplikasyon  
- **Open WebUI Integration**: Propesyonal na chat interface gamit ang Foundry Local
- **Production Patterns**: Error handling, monitoring, at deployment strategies

## Mga Layunin sa Pag-aaral

- Gumawa ng production-ready chat applications gamit ang Chainlit
- Magpatupad ng streaming responses para sa mas mahusay na user experience
- Masterin ang Foundry Local SDK integration patterns
- Mag-apply ng tamang error handling at graceful degradation
- Mag-deploy at mag-configure ng chat applications para sa iba't ibang environment
- Maunawaan ang modernong web UI patterns para sa conversational AI

## Mga Kinakailangan

- **Foundry Local**: Naka-install at tumatakbo ([Installation Guide](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Bersyon 3.10 o mas bago na may kakayahan sa virtual environment
- **Model**: Hindi bababa sa isang model na naka-load (`foundry model run phi-4-mini`)
- **Browser**: Modernong web browser na may suporta sa WebGPU (Chrome/Edge)
- **Docker**: Para sa Open WebUI integration (opsyonal)

## Bahagi 1: Pag-unawa sa Modernong Chat Applications

### Pangkalahatang-ideya ng Arkitektura

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Mga Pangunahing Teknolohiya

**Foundry Local SDK Patterns:**
- `FoundryLocalManager(alias)`: Automatic service management
- `manager.endpoint` at `manager.api_key`: Mga detalye ng koneksyon
- `manager.get_model_info(alias).id`: Pagkilala sa model

**Chainlit Framework:**
- `@cl.on_chat_start`: Pag-initialize ng chat sessions
- `@cl.on_message`: Pag-handle ng mga mensahe ng user  
- `cl.Message().stream_token()`: Real-time streaming
- Automatic UI generation at WebSocket management

## Bahagi 2: Local vs Cloud Decision Matrix

### Mga Katangian ng Performance

| Aspeto | Lokal (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latency** | 🚀 50-200ms (walang network) | ⏱️ 200-2000ms (depende sa network) |
| **Privacy** | 🔒 Hindi umaalis ang data sa device | ⚠️ Ang data ay ipinapadala sa cloud |
| **Gastos** | 💰 Libre pagkatapos ng hardware | 💸 Bayad kada token |
| **Offline** | ✅ Gumagana nang walang internet | ❌ Nangangailangan ng internet |
| **Laki ng Model** | ⚠️ Limitado ng hardware | ✅ Access sa pinakamalalaking model |
| **Scaling** | ⚠️ Depende sa hardware | ✅ Walang limitasyong scaling |

### Hybrid Strategy Patterns

**Local-First with Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Task-Based Routing:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Bahagi 3: Sample 04 - Chainlit Chat Application

### Mabilisang Simula

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Awtomatikong magbubukas ang application sa `http://localhost:8080` na may modernong chat interface.

### Pangunahing Implementasyon

Ang Sample 04 application ay nagpapakita ng production-ready patterns:

**Automatic Service Discovery:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Streaming Chat Handler:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Mga Opsyon sa Konfigurasyon

**Environment Variables:**

| Variable | Deskripsyon | Default | Halimbawa |
|----------|-------------|---------|----------|
| `MODEL` | Model alias na gagamitin | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local endpoint | Auto-detected | `http://localhost:51211` |
| `API_KEY` | API key (opsyonal para sa lokal) | `""` | `your-api-key` |

**Advanced Usage:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Bahagi 4: Paglikha at Paggamit ng Jupyter Notebooks

### Pangkalahatang-ideya ng Notebook Support

Ang Sample 04 ay may kasamang komprehensibong Jupyter notebook (`chainlit_app.ipynb`) na nagbibigay ng:

- **📚 Educational Content**: Step-by-step na materyales sa pag-aaral
- **🔬 Interactive Exploration**: Pagpapatakbo at eksperimento sa mga code cell
- **📊 Visual Demonstrations**: Mga chart, diagram, at output visualization
- **🛠️ Development Tools**: Testing at debugging capabilities

### Paglikha ng Sariling Notebook

#### Hakbang 1: I-set Up ang Jupyter Environment

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Hakbang 2: Gumawa ng Bagong Notebook

**Gamit ang VS Code:**
1. Buksan ang VS Code sa Module08 directory
2. Gumawa ng bagong file na may `.ipynb` extension
3. Piliin ang "Foundry Local" kernel kapag na-prompt
4. Simulan ang pagdagdag ng mga cell na may iyong content

**Gamit ang Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Mga Best Practices sa Notebook Structure

#### Organisasyon ng Cell

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Mga Interactive na Halimbawa at Ehersisyo

#### Ehersisyo 1: Pagsubok sa Client Configuration

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Ehersisyo 2: Simulation ng Streaming Response

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Bahagi 5: WebGPU Browser Inference Demo

### Pangkalahatang-ideya

Ang WebGPU ay nagbibigay-daan sa pagpapatakbo ng AI models direkta sa browser para sa maximum privacy at zero-install na mga karanasan. Ang sample na ito ay nagpapakita ng ONNX Runtime Web na may WebGPU execution.

### Hakbang 1: Suriin ang Suporta ng WebGPU

**Mga Kinakailangan sa Browser:**
- Chrome/Edge 113+ na may WebGPU enabled
- Suriin: `chrome://gpu` → kumpirmahin ang "WebGPU" status
- Programmatic na pagsusuri: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Hakbang 2: Gumawa ng WebGPU Demo

Gumawa ng directory: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Hakbang 3: Patakbuhin ang Demo

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Bahagi 6: Open WebUI Integration

### Pangkalahatang-ideya

Ang Open WebUI ay nagbibigay ng propesyonal na ChatGPT-like interface na kumokonekta sa Foundry Local's OpenAI-compatible API.

### Hakbang 1: Mga Kinakailangan

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Hakbang 2: Docker Setup (Inirerekomenda)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Tandaan:** Ang `host.docker.internal` ay nagbibigay-daan sa mga Docker container na ma-access ang host machine sa Windows.

### Hakbang 3: Konfigurasyon

1. **Buksan ang Browser:** Pumunta sa `http://localhost:3000`
2. **Initial Setup:** Gumawa ng admin account
3. **Model Configuration:**
   - Settings → Models → OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (anumang halaga ay gagana)
4. **Test Connection:** Dapat lumitaw ang mga model sa dropdown

### Troubleshooting

**Karaniwang Problema:**

1. **Connection Refused:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Hindi Lumilitaw ang Mga Model:**
   - Suriin kung naka-load ang model: `foundry model list`
   - Suriin ang API response: `curl http://localhost:51211/v1/models`
   - I-restart ang Open WebUI container

## Bahagi 7: Mga Pagsasaalang-alang sa Production Deployment

### Konfigurasyon ng Environment

**Development Setup:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Production Deployment:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Mga Karaniwang Problema sa Port at Solusyon

**Pag-iwas sa Port 51211 Conflict:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Performance Monitoring

**Health Check Implementation:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Buod

Ang Session 4 ay tumalakay sa paggawa ng production-ready Chainlit applications para sa conversational AI. Natutunan mo ang tungkol sa:

- ✅ **Chainlit Framework**: Modernong UI at streaming support para sa chat applications
- ✅ **Foundry Local Integration**: Paggamit ng SDK at mga pattern ng konfigurasyon  
- ✅ **WebGPU Inference**: AI sa browser para sa maximum privacy
- ✅ **Open WebUI Setup**: Deployment ng propesyonal na chat interface
- ✅ **Production Patterns**: Error handling, monitoring, at scaling

Ang Sample 04 application ay nagpapakita ng best practices para sa paggawa ng matibay na chat interfaces na gumagamit ng lokal na AI models sa pamamagitan ng Microsoft Foundry Local habang nagbibigay ng mahusay na user experiences.

## Mga Sanggunian

- **[Sample 04: Chainlit Application](samples/04/README.md)**: Kumpletong application na may dokumentasyon
- **[Chainlit Educational Notebook](samples/04/chainlit_app.ipynb)**: Interactive na materyales sa pag-aaral
- **[Foundry Local Documentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Kumpletong dokumentasyon ng platform
- **[Chainlit Documentation](https://docs.chainlit.io/)**: Opisyal na dokumentasyon ng framework
- **[Open WebUI Integration Guide](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Opisyal na tutorial

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, mangyaring tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.