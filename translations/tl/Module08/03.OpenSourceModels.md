<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-25T00:53:42+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "tl"
}
-->
# Session 3: Pagdiskubre at Pamamahala ng Open-Source Model

## Pangkalahatang-ideya

Ang sesyon na ito ay nakatuon sa praktikal na pagdiskubre at pamamahala ng modelo gamit ang Foundry Local. Matututuhan mo kung paano ilista ang mga available na modelo, subukan ang iba't ibang opsyon, at maunawaan ang mga pangunahing katangian ng performance. Ang approach ay nagbibigay-diin sa hands-on na eksplorasyon gamit ang foundry CLI upang matulungan kang pumili ng tamang modelo para sa iyong mga pangangailangan.

## Mga Layunin sa Pagkatuto

- Masterin ang mga foundry CLI command para sa pagdiskubre at pamamahala ng modelo
- Maunawaan ang model cache at mga pattern ng lokal na storage
- Matutong mabilis na subukan at ikumpara ang iba't ibang modelo
- Magtatag ng praktikal na workflow para sa pagpili at benchmarking ng modelo
- Tuklasin ang lumalaking ecosystem ng mga modelo na available sa Foundry Local

## Mga Kinakailangan

- Natapos ang Session 1: Pagsisimula sa Foundry Local
- Nakainstall at accessible ang Foundry Local CLI
- Sapat na storage space para sa pag-download ng mga modelo (ang mga modelo ay maaaring umabot mula 1GB hanggang 20GB+)
- Pangunahing kaalaman sa mga uri ng modelo at mga use case

## Part 6: Hands-On Exercise

### Ehersisyo: Pagdiskubre at Paghahambing ng Modelo

Gumawa ng sarili mong script para sa pagsusuri ng modelo batay sa Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Ang Iyong Gawain

1. **Patakbuhin ang Sample 03 script**: `samples\03\list_and_bench.cmd`
2. **Subukan ang iba't ibang modelo**: Subukan ang hindi bababa sa 3 iba't ibang modelo
3. **Ihambing ang performance**: Pansinin ang pagkakaiba sa bilis at kalidad ng tugon
4. **Idokumento ang mga natuklasan**: Gumawa ng simpleng comparison chart

### Halimbawa ng Format ng Paghahambing

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Part 7: Troubleshooting at Mga Pinakamahusay na Praktika

### Karaniwang Isyu at Solusyon

**Hindi Nag-iistart ang Modelo:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Kulang ang Memorya:**
- Magsimula sa mas maliliit na modelo (`phi-4-mini`)
- Isara ang ibang mga application
- Mag-upgrade ng RAM kung madalas na nauubusan

**Mabagal na Performance:**
- Siguraduhing ganap na na-load ang modelo (suriin ang verbose output)
- Isara ang mga hindi kinakailangang background application
- Isaalang-alang ang mas mabilis na storage (SSD)

### Mga Pinakamahusay na Praktika

1. **Magsimula sa Maliit**: Simulan sa `phi-4-mini` upang i-validate ang setup
2. **Isang Modelo sa Isang Oras**: Itigil ang nakaraang modelo bago magsimula ng bago
3. **I-monitor ang Resources**: Bantayan ang paggamit ng memorya
4. **Subukan nang Konsistent**: Gumamit ng parehong mga prompt para sa patas na paghahambing
5. **Idokumento ang Resulta**: Magtala ng performance ng modelo para sa iyong mga use case

## Part 8: Mga Susunod na Hakbang at Sanggunian

### Paghahanda para sa Session 4

- **Pokusan ng Session 4**: Mga tool at teknik sa optimization
- **Mga Kinakailangan**: Kumportable sa pag-switch ng modelo at pangunahing pagsusuri ng performance
- **Inirerekomenda**: Magkaroon ng 2-3 paboritong modelo na natukoy mula sa sesyon na ito

### Karagdagang Mga Sanggunian

- **[Foundry Local Documentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Opisyal na dokumentasyon
- **[CLI Reference](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Kumpletong command reference
- **[Model Mondays](https://aka.ms/model-mondays)**: Lingguhang spotlight ng modelo
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Komunidad at mga isyu
- **[Sample 03: Model Discovery](samples/03/README.md)**: Hands-on na halimbawa ng script

### Mga Pangunahing Takeaway

✅ **Pagdiskubre ng Modelo**: Gamitin ang `foundry model list` upang tuklasin ang mga available na modelo  
✅ **Mabilis na Pagsubok**: Ang pattern na `list_and_bench.cmd` para sa mabilisang pagsusuri  
✅ **Pag-monitor ng Performance**: Pangunahing paggamit ng resources at pagsukat ng oras ng tugon  
✅ **Pagpili ng Modelo**: Praktikal na gabay para sa pagpili ng mga modelo batay sa use case  
✅ **Pamamahala ng Cache**: Pag-unawa sa storage at mga proseso ng paglilinis  

Ngayon ay mayroon ka nang praktikal na kasanayan upang tuklasin, subukan, at pumili ng angkop na mga modelo para sa iyong AI applications gamit ang simpleng CLI approach ng Foundry Local.

## Mga Layunin sa Pagkatuto

- Tuklasin at suriin ang mga open-source na modelo para sa lokal na inference
- I-compile at patakbuhin ang mga napiling Hugging Face na modelo sa Foundry Local
- Mag-apply ng mga estratehiya sa pagpili ng modelo para sa accuracy, latency, at resource needs
- Pamahalaan ang mga modelo nang lokal gamit ang cache at versioning

## Part 1: Pagdiskubre ng Modelo gamit ang Foundry CLI

### Mga Pangunahing Command sa Pamamahala ng Modelo

Nagbibigay ang foundry CLI ng mga simpleng command para sa pagdiskubre at pamamahala ng modelo:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Pagpapatakbo ng Iyong Unang Mga Modelo

Magsimula sa mga popular at subok na modelo upang maunawaan ang mga katangian ng performance:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Tandaan:** Ang `--verbose` flag ay nagbibigay ng detalyadong impormasyon sa startup, kabilang ang:
- Progress ng pag-download ng modelo (sa unang run)
- Mga detalye ng memory allocation
- Impormasyon sa service binding
- Mga metrics ng performance initialization

### Pag-unawa sa Mga Kategorya ng Modelo

**Small Language Models (SLMs):**
- `phi-4-mini`: Mabilis, mahusay, magaling para sa general chat
- `phi-4`: Mas kapable na bersyon na may mas mahusay na reasoning

**Medium Models:**
- `qwen2.5-7b-instruct`: Mahusay sa reasoning at mas mahabang context
- `deepseek-r1-distill-qwen-7b`: Optimized para sa code generation

**Larger Models:**
- `llama-3.2`: Pinakabagong open-source na modelo ng Meta
- `qwen2.5-14b-instruct`: Pang-enterprise na reasoning

## Part 2: Mabilis na Pagsubok at Paghahambing ng Modelo

### Sample 03 Approach: Simple List and Bench

Batay sa aming Sample 03 pattern, narito ang minimal na workflow:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Pagsubok ng Performance ng Modelo

Kapag tumatakbo na ang modelo, subukan ito gamit ang konsistent na mga prompt:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternatibo sa Pagsubok gamit ang PowerShell

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Part 3: Pamamahala ng Model Cache at Storage

### Pag-unawa sa Model Cache

Ang Foundry Local ay awtomatikong namamahala ng pag-download at pag-cache ng modelo:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Mga Pagsasaalang-alang sa Model Storage

**Karaniwang Laki ng Modelo:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Mga Pinakamahusay na Praktika sa Storage:**
- Panatilihin ang 2-3 modelo na naka-cache para sa mabilisang switching
- Alisin ang mga hindi ginagamit na modelo upang magpalaya ng espasyo: `foundry cache clean`
- I-monitor ang disk usage, lalo na sa mas maliit na SSDs
- Isaalang-alang ang trade-off sa laki ng modelo at kakayahan

### Pag-monitor ng Performance ng Modelo

Habang tumatakbo ang mga modelo, i-monitor ang system resources:

**Windows Task Manager:**
- Bantayan ang paggamit ng memorya (ang mga modelo ay nananatiling naka-load sa RAM)
- I-monitor ang CPU utilization habang nag-i-inference
- Suriin ang disk I/O sa unang pag-load ng modelo

**Command Line Monitoring:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Part 4: Praktikal na Gabay sa Pagpili ng Modelo

### Pagpili ng Modelo Batay sa Use Case

**Para sa General Chat at Q&A:**
- Magsimula sa: `phi-4-mini` (mabilis, mahusay)
- Mag-upgrade sa: `phi-4` (mas mahusay na reasoning)
- Advanced: `qwen2.5-7b-instruct` (mas mahabang context)

**Para sa Code Generation:**
- Inirerekomenda: `deepseek-r1-distill-qwen-7b`
- Alternatibo: `qwen2.5-7b-instruct` (magaling din sa code)

**Para sa Complex Reasoning:**
- Pinakamahusay: `qwen2.5-7b-instruct` o `qwen2.5-14b-instruct`
- Budget option: `phi-4`

### Gabay sa Hardware Requirements

**Minimum System Requirements:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Inirerekomenda para sa Pinakamahusay na Performance:**
- 32GB+ RAM para sa komportableng multi-model switching
- SSD storage para sa mas mabilis na pag-load ng modelo
- Modernong CPU na may mahusay na single-thread performance
- NPU support (Windows 11 Copilot+ PCs) para sa acceleration

### Workflow sa Pag-switch ng Modelo

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Part 5: Simpleng Benchmarking ng Modelo

### Pangunahing Pagsubok ng Performance

Narito ang simpleng approach para ikumpara ang performance ng modelo:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manual na Pagsusuri ng Kalidad

Para sa bawat modelo, subukan gamit ang konsistent na mga prompt at manu-manong suriin:

**Test Prompts:**
1. "Ipaliwanag ang quantum computing sa simpleng paraan."
2. "Gumawa ng Python function para mag-sort ng list."
3. "Ano ang mga pros at cons ng remote work?"
4. "I-summarize ang mga benepisyo ng edge AI."

**Mga Pamantayan sa Pagsusuri:**
- **Accuracy**: Tama ba ang impormasyon?
- **Clarity**: Madali bang maunawaan ang paliwanag?
- **Completeness**: Natugunan ba ang buong tanong?
- **Speed**: Gaano kabilis ang tugon?

### Pag-monitor ng Paggamit ng Resources

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Part 6: Mga Susunod na Hakbang

- Mag-subscribe sa Model Mondays para sa mga bagong modelo at tips: https://aka.ms/model-mondays
- Mag-ambag ng mga natuklasan sa `models.json` ng iyong team
- Maghanda para sa Session 4: paghahambing ng LLMs vs SLMs, lokal vs cloud inference, at hands-on na demo

---

