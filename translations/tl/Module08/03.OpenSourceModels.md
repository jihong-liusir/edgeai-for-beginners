<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T22:37:59+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "tl"
}
-->
# Session 3: Mga Open-Source Model gamit ang Foundry Local

## Pangkalahatang-ideya

Sa sesyon na ito, tatalakayin kung paano magdala ng mga open-source model sa Foundry Local: pagpili ng mga model mula sa komunidad, integrasyon ng nilalaman mula sa Hugging Face, at paggamit ng “bring your own model” (BYOM) na mga estratehiya. Malalaman mo rin ang tungkol sa serye ng Model Mondays para sa patuloy na pag-aaral at pagtuklas ng mga model.

Mga Sanggunian:
- Foundry Local docs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Mga Layunin sa Pag-aaral
- Tuklasin at suriin ang mga open-source model para sa lokal na inference
- I-compile at patakbuhin ang mga napiling Hugging Face model sa Foundry Local
- Mag-apply ng mga estratehiya sa pagpili ng model para sa katumpakan, bilis, at pangangailangan sa resources
- Pamahalaan ang mga model nang lokal gamit ang cache at versioning

## Bahagi 1: Pagtuklas at Pagpili ng Model (Hakbang-hakbang)

Hakbang 1) Ilista ang mga available na model sa lokal na katalogo  
```cmd
foundry model list
```
  
Hakbang 2) Subukan ang dalawang kandidato (auto-download sa unang pagtakbo)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Hakbang 3) Tandaan ang mga pangunahing sukatan  
- Obserbahan ang bilis (subjective) at kalidad para sa isang nakatakdang prompt  
- Tingnan ang paggamit ng memorya sa Task Manager habang tumatakbo ang bawat model  

## Bahagi 2: Pagpapatakbo ng Catalog Models gamit ang CLI (Hakbang-hakbang)

Hakbang 1) Simulan ang isang model  
```cmd
foundry model run llama-3.2
```
  
Hakbang 2) Magpadala ng test prompt gamit ang OpenAI-compatible endpoint  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Bahagi 3: BYOM – Pag-compile ng Hugging Face Models (Hakbang-hakbang)

Sundin ang opisyal na how-to para sa pag-compile ng mga model. Narito ang pangkalahatang daloy—tingnan ang Microsoft Learn article para sa eksaktong mga utos at suportadong mga configuration.

Hakbang 1) Ihanda ang working directory  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Hakbang 2) I-compile ang isang suportadong HF model  
- Gamitin ang mga hakbang mula sa Learn doc para i-convert at ilagay ang na-compile na ONNX model sa iyong `models` directory  
- Kumpirmahin gamit ang:  
```cmd
foundry cache ls
```
  
Makikita mo ang pangalan ng iyong na-compile na model (halimbawa, `llama-3.2`).  

Hakbang 3) Patakbuhin ang na-compile na model  
```cmd
foundry model run llama-3.2 --verbose
```
  
Mga Tala:  
- Siguraduhing may sapat na disk at RAM para sa pag-compile at pagtakbo  
- Magsimula sa mas maliliit na model para i-validate ang daloy, pagkatapos ay mag-scale up  

## Bahagi 4: Praktikal na Pag-curate ng Model (Hakbang-hakbang)

Hakbang 1) Gumawa ng `models.json` registry  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Hakbang 2) Maliit na selector script  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Bahagi 5: Hands-On Benchmarks (Hakbang-hakbang)

Hakbang 1) Simpleng latency benchmark  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Hakbang 2) Quality spot-check  
- Gumamit ng nakatakdang prompt set, i-capture ang mga output sa isang CSV/JSON  
- Manu-manong i-rate ang fluency, relevance, at correctness (1–5)  

## Bahagi 6: Mga Susunod na Hakbang
- Mag-subscribe sa Model Mondays para sa mga bagong model at tips: https://aka.ms/model-mondays  
- Ibahagi ang mga natuklasan sa `models.json` ng iyong team  
- Maghanda para sa Session 4: paghahambing ng LLMs vs SLMs, lokal vs cloud inference, at hands-on demos  

---

