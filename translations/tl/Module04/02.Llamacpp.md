<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T14:43:07+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "tl"
}
-->
# Seksyon 2: Gabay sa Implementasyon ng Llama.cpp

## Talaan ng Nilalaman
1. [Introduksyon](../../../Module04)
2. [Ano ang Llama.cpp?](../../../Module04)
3. [Pag-install](../../../Module04)
4. [Pagbuo mula sa Source](../../../Module04)
5. [Model Quantization](../../../Module04)
6. [Pangunahing Paggamit](../../../Module04)
7. [Mga Advanced na Tampok](../../../Module04)
8. [Python Integration](../../../Module04)
9. [Pag-aayos ng Problema](../../../Module04)
10. [Mga Pinakamahusay na Praktika](../../../Module04)

## Introduksyon

Ang komprehensibong tutorial na ito ay magbibigay sa iyo ng lahat ng kailangan mong malaman tungkol sa Llama.cpp, mula sa simpleng pag-install hanggang sa mga advanced na senaryo ng paggamit. Ang Llama.cpp ay isang makapangyarihang implementasyon sa C++ na nagbibigay-daan sa mahusay na inference ng Large Language Models (LLMs) na may minimal na setup at mahusay na performance sa iba't ibang hardware configurations.

## Ano ang Llama.cpp?

Ang Llama.cpp ay isang framework para sa LLM inference na isinulat sa C/C++ na nagbibigay-daan sa pagtakbo ng malalaking language models nang lokal na may minimal na setup at state-of-the-art na performance sa iba't ibang hardware. Ang mga pangunahing tampok nito ay:

### Mga Pangunahing Tampok
- **Plain C/C++ implementation** na walang dependencies
- **Cross-platform compatibility** (Windows, macOS, Linux)
- **Hardware optimization** para sa iba't ibang arkitektura
- **Suporta sa Quantization** (1.5-bit hanggang 8-bit integer quantization)
- **Suporta sa CPU at GPU acceleration**
- **Memory efficiency** para sa mga limitadong environment

### Mga Bentahe
- Tumatakbo nang mahusay sa CPU nang hindi nangangailangan ng specialized hardware
- Sinusuportahan ang iba't ibang GPU backends (CUDA, Metal, OpenCL, Vulkan)
- Magaan at portable
- Ang Apple silicon ay isang first-class citizen - na-optimize gamit ang ARM NEON, Accelerate, at Metal frameworks
- Sinusuportahan ang iba't ibang antas ng quantization para sa mas mababang memory usage

## Pag-install

### Paraan 1: Pre-built Binaries (Inirerekomenda para sa mga Baguhan)

#### I-download mula sa GitHub Releases
1. Bisitahin ang [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. I-download ang tamang binary para sa iyong sistema:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` para sa Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` para sa macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` para sa Linux

3. I-extract ang archive at idagdag ang direktoryo sa PATH ng iyong sistema

#### Gamit ang Package Managers

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Iba't ibang distribusyon):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Paraan 2: Python Package (llama-cpp-python)

#### Pangunahing Pag-install
```bash
pip install llama-cpp-python
```

#### Gamit ang Hardware Acceleration
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Pagbuo mula sa Source

### Mga Kinakailangan

**Mga Kinakailangan sa Sistema:**
- C++ compiler (GCC, Clang, o MSVC)
- CMake (bersyon 3.14 o mas mataas)
- Git
- Mga build tools para sa iyong platform

**Pag-install ng Mga Kinakailangan:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- I-install ang Visual Studio 2022 na may C++ development tools
- I-install ang CMake mula sa opisyal na website
- I-install ang Git

### Pangunahing Proseso ng Pagbuo

1. **I-clone ang repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **I-configure ang build:**
```bash
cmake -B build
```

3. **I-build ang proyekto:**
```bash
cmake --build build --config Release
```

Para sa mas mabilis na compilation, gumamit ng parallel jobs:
```bash
cmake --build build --config Release -j 8
```

### Mga Build na Specific sa Hardware

#### Suporta sa CUDA (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Suporta sa Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Suporta sa OpenBLAS (CPU Optimization)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Suporta sa Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Mga Advanced na Opsyon sa Build

#### Debug Build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### May Karagdagang Mga Tampok
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Model Quantization

### Pag-unawa sa GGUF Format

Ang GGUF (Generalized GGML Unified Format) ay isang optimized na file format na idinisenyo para sa mahusay na pagtakbo ng malalaking language models gamit ang Llama.cpp at iba pang frameworks. Nagbibigay ito ng:

- Standardized na storage ng model weights
- Pinahusay na compatibility sa iba't ibang platform
- Mas mahusay na performance
- Efficient na metadata handling

### Mga Uri ng Quantization

Sinusuportahan ng Llama.cpp ang iba't ibang antas ng quantization:

| Uri | Bits | Deskripsyon | Gamit |
|-----|------|-------------|-------|
| F16 | 16 | Half precision | Mataas na kalidad, malaking memory |
| Q8_0 | 8 | 8-bit quantization | Magandang balanse |
| Q4_0 | 4 | 4-bit quantization | Katamtamang kalidad, mas maliit na laki |
| Q2_K | 2 | 2-bit quantization | Pinakamaliit na laki, mas mababang kalidad |

### Pag-convert ng Models

#### Mula PyTorch patungong GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Direktang Pag-download mula sa Hugging Face
Maraming models ang available sa GGUF format sa Hugging Face:
- Maghanap ng models na may "GGUF" sa pangalan
- I-download ang tamang antas ng quantization
- Gamitin nang direkta sa llama.cpp

## Pangunahing Paggamit

### Command Line Interface

#### Simpleng Text Generation
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Paggamit ng Models mula sa Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Server Mode
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Karaniwang Parameters

| Parameter | Deskripsyon | Halimbawa |
|-----------|-------------|----------|
| `-m` | Path ng model file | `-m model.gguf` |
| `-p` | Prompt text | `-p "Hello world"` |
| `-n` | Bilang ng tokens na gagawin | `-n 100` |
| `-c` | Context size | `-c 4096` |
| `-t` | Bilang ng threads | `-t 8` |
| `-ngl` | GPU layers | `-ngl 32` |
| `-temp` | Temperature | `-temp 0.7` |

### Interactive Mode

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Mga Advanced na Tampok

### Server API

#### Pagsisimula ng Server
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Paggamit ng API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optimization ng Performance

#### Memory Management
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multi-threading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU Acceleration
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python Integration

### Pangunahing Paggamit gamit ang llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chat Interface

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streaming Responses

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integration sa LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Pag-aayos ng Problema

### Karaniwang Isyu at Solusyon

#### Mga Error sa Build

**Isyu: CMake hindi natagpuan**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Isyu: Compiler hindi natagpuan**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Mga Isyu sa Runtime

**Isyu: Nabigo ang pag-load ng model**
- Suriin ang path ng model file
- I-check ang file permissions
- Siguraduhing sapat ang RAM
- Subukan ang iba't ibang antas ng quantization

**Isyu: Mahinang performance**
- I-enable ang hardware acceleration
- Dagdagan ang bilang ng threads
- Gumamit ng tamang quantization
- I-check ang GPU memory usage

#### Mga Isyu sa Memory

**Isyu: Kulang sa memory**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Mga Isyu Specific sa Platform

#### Windows
- Gumamit ng MinGW o Visual Studio compiler
- Siguraduhing tama ang PATH configuration
- I-check ang interference ng antivirus

#### macOS
- I-enable ang Metal para sa Apple Silicon
- Gumamit ng Rosetta 2 para sa compatibility kung kinakailangan
- I-check ang Xcode command line tools

#### Linux
- I-install ang development packages
- I-check ang GPU driver versions
- Suriin ang pag-install ng CUDA toolkit

## Mga Pinakamahusay na Praktika

### Pagpili ng Model
1. **Pumili ng tamang quantization** batay sa iyong hardware
2. **Isaalang-alang ang laki ng model** kumpara sa trade-offs ng kalidad
3. **Subukan ang iba't ibang models** para sa iyong partikular na gamit

### Optimization ng Performance
1. **Gumamit ng GPU acceleration** kung available
2. **I-optimize ang bilang ng threads** para sa iyong CPU
3. **Itakda ang tamang context size** para sa iyong gamit
4. **I-enable ang memory mapping** para sa malalaking models

### Deployment sa Produksyon
1. **Gumamit ng server mode** para sa API access
2. **Mag-implement ng tamang error handling**
3. **I-monitor ang resource usage**
4. **Mag-set up ng logging at monitoring**

### Workflow sa Pag-develop
1. **Magsimula sa mas maliit na models** para sa testing
2. **Gumamit ng version control** para sa model configurations
3. **I-dokumenta ang iyong configurations**
4. **Subukan sa iba't ibang platform**

### Mga Pagsasaalang-alang sa Seguridad
1. **I-validate ang input prompts**
2. **Mag-implement ng rate limiting**
3. **I-secure ang API endpoints**
4. **I-monitor ang abuse patterns**

## Konklusyon

Ang Llama.cpp ay nagbibigay ng makapangyarihan at mahusay na paraan upang patakbuhin ang malalaking language models nang lokal sa iba't ibang hardware configurations. Kung ikaw ay nagde-develop ng AI applications, nagsasagawa ng pananaliksik, o simpleng nag-eeksperimento sa LLMs, ang framework na ito ay nag-aalok ng flexibility at performance na kinakailangan para sa iba't ibang gamit.

Mga pangunahing puntos:
- Pumili ng paraan ng pag-install na pinakaangkop sa iyong pangangailangan
- I-optimize para sa iyong partikular na hardware configuration
- Magsimula sa pangunahing paggamit at unti-unting tuklasin ang mga advanced na tampok
- Isaalang-alang ang paggamit ng Python bindings para sa mas madaling integration
- Sundin ang pinakamahusay na praktika para sa deployment sa produksyon

Para sa karagdagang impormasyon at updates, bisitahin ang [opisyal na repository ng Llama.cpp](https://github.com/ggml-org/llama.cpp) at sumangguni sa komprehensibong dokumentasyon at mga resources ng komunidad.

## ➡️ Ano ang susunod

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.