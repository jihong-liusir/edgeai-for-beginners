<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T14:35:39+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "tl"
}
-->
# Seksyon 4: OpenVINO Toolkit Optimization Suite

## Talaan ng Nilalaman
1. [Introduksyon](../../../Module04)
2. [Ano ang OpenVINO?](../../../Module04)
3. [Pag-install](../../../Module04)
4. [Mabilisang Gabay sa Pagsisimula](../../../Module04)
5. [Halimbawa: Pag-convert at Pag-optimize ng Mga Modelo gamit ang OpenVINO](../../../Module04)
6. [Advanced na Paggamit](../../../Module04)
7. [Pinakamahusay na Praktika](../../../Module04)
8. [Pag-aayos ng Problema](../../../Module04)
9. [Karagdagang Mga Mapagkukunan](../../../Module04)

## Introduksyon

Ang OpenVINO (Open Visual Inference and Neural Network Optimization) ay ang open-source toolkit ng Intel para sa pag-deploy ng mga AI solution na may mataas na performance sa cloud, on-premises, at edge environments. Kahit anong hardware ang target mo—CPUs, GPUs, VPUs, o mga espesyal na AI accelerators—nagbibigay ang OpenVINO ng komprehensibong kakayahan sa pag-optimize habang pinapanatili ang katumpakan ng modelo at sinusuportahan ang cross-platform deployment.

## Ano ang OpenVINO?

Ang OpenVINO ay isang open-source toolkit na nagbibigay-daan sa mga developer na mag-optimize, mag-convert, at mag-deploy ng mga AI model nang epektibo sa iba't ibang hardware platforms. Binubuo ito ng tatlong pangunahing bahagi: OpenVINO Runtime para sa inference, Neural Network Compression Framework (NNCF) para sa pag-optimize ng modelo, at OpenVINO Model Server para sa scalable deployment.

### Pangunahing Katangian

- **Cross-Platform Deployment**: Sinusuportahan ang Linux, Windows, at macOS gamit ang Python, C++, at C APIs
- **Hardware Acceleration**: Awtomatikong pagtuklas ng device at pag-optimize para sa CPU, GPU, VPU, at AI accelerators
- **Model Compression Framework**: Advanced na mga teknika sa quantization, pruning, at optimization gamit ang NNCF
- **Framework Compatibility**: Direktang suporta para sa TensorFlow, ONNX, PaddlePaddle, at PyTorch models
- **Generative AI Support**: Espesyal na OpenVINO GenAI para sa pag-deploy ng malalaking language models at generative AI applications

### Mga Benepisyo

- **Performance Optimization**: Malaking bilis ng pagtakbo na may minimal na pagkawala ng katumpakan
- **Mas Maliit na Deployment Footprint**: Minimal na external dependencies para sa mas madaling pag-install at deployment
- **Pinahusay na Start-up Time**: Optimized na pag-load ng modelo at caching para sa mas mabilis na initialization ng application
- **Scalable Deployment**: Mula sa edge devices hanggang sa cloud infrastructure gamit ang consistent APIs
- **Handa para sa Produksyon**: Enterprise-grade na reliability na may komprehensibong dokumentasyon at suporta mula sa komunidad

## Pag-install

### Mga Kinakailangan

- Python 3.8 o mas mataas
- pip package manager
- Virtual environment (inirerekomenda)
- Compatible na hardware (inirerekomenda ang Intel CPUs, ngunit sinusuportahan ang iba't ibang arkitektura)

### Pangunahing Pag-install

Gumawa at i-activate ang virtual environment:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

I-install ang OpenVINO Runtime:

```bash
pip install openvino
```

I-install ang NNCF para sa pag-optimize ng modelo:

```bash
pip install nncf
```

### Pag-install ng OpenVINO GenAI

Para sa generative AI applications:

```bash
pip install openvino-genai
```

### Opsyonal na Dependencies

Karagdagang mga package para sa partikular na mga use case:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### I-verify ang Pag-install

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Kapag matagumpay, makikita mo ang impormasyon ng bersyon ng OpenVINO.

## Mabilisang Gabay sa Pagsisimula

### Ang Iyong Unang Model Optimization

I-convert at i-optimize ang isang Hugging Face model gamit ang OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Ano ang Ginagawa ng Proseso na Ito

Ang workflow ng optimization ay kinabibilangan ng: pag-load ng orihinal na modelo mula sa Hugging Face, pag-convert sa OpenVINO Intermediate Representation (IR) format, pag-aaplay ng default optimizations, at pag-compile para sa target hardware.

### Paliwanag ng Mga Pangunahing Parameter

- `export=True`: Iko-convert ang modelo sa OpenVINO IR format
- `compile=False`: Ipinagpapaliban ang compilation hanggang runtime para sa flexibility
- `device`: Target hardware ("CPU", "GPU", "AUTO" para sa awtomatikong pagpili)
- `save_pretrained()`: Sine-save ang optimized na modelo para sa muling paggamit

## Halimbawa: Pag-convert at Pag-optimize ng Mga Modelo gamit ang OpenVINO

### Hakbang 1: Model Conversion gamit ang NNCF Quantization

Narito kung paano mag-aplay ng post-training quantization gamit ang NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Hakbang 2: Advanced Optimization gamit ang Weight Compression

Para sa mga transformer-based models, mag-aplay ng weight compression:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Hakbang 3: Inference gamit ang Optimized Model

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Output Structure

Pagkatapos ng optimization, ang iyong model directory ay maglalaman ng:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Advanced na Paggamit

### Configuration gamit ang NNCF YAML

Para sa mas kumplikadong optimization workflows, gumamit ng NNCF configuration files:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Mag-aplay ng configuration:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU Optimization

Para sa GPU acceleration:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Batch Processing Optimization

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Model Server Deployment

I-deploy ang optimized models gamit ang OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Client code para sa model server:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Pinakamahusay na Praktika

### 1. Pagpili at Paghahanda ng Modelo
- Gumamit ng mga modelo mula sa mga suportadong frameworks (PyTorch, TensorFlow, ONNX)
- Siguraduhing ang mga input ng modelo ay may fixed o kilalang dynamic shapes
- Subukan gamit ang representative datasets para sa calibration

### 2. Pagpili ng Optimization Strategy
- **Post-training Quantization**: Magsimula dito para sa mabilisang optimization
- **Weight Compression**: Perpekto para sa malalaking language models at transformers
- **Quantization-aware Training**: Gamitin kapag kritikal ang katumpakan

### 3. Hardware-Specific Optimization
- **CPU**: Gumamit ng INT8 quantization para sa balanseng performance
- **GPU**: Gamitin ang FP16 precision at batch processing
- **VPU**: Mag-focus sa simplification ng modelo at layer fusion

### 4. Performance Tuning
- **Throughput Mode**: Para sa high-volume batch processing
- **Latency Mode**: Para sa real-time interactive applications
- **AUTO Device**: Hayaan ang OpenVINO na pumili ng optimal hardware

### 5. Memory Management
- Gamitin ang dynamic shapes nang maingat upang maiwasan ang memory overhead
- Magpatupad ng model caching para sa mas mabilis na pag-load sa susunod
- I-monitor ang memory usage habang nag-o-optimize

### 6. Pag-validate ng Katumpakan
- Palaging i-validate ang optimized models laban sa orihinal na performance
- Gumamit ng representative test datasets para sa evaluation
- Isaalang-alang ang gradual optimization (magsimula sa konserbatibong settings)

## Pag-aayos ng Problema

### Karaniwang Mga Isyu

#### 1. Mga Problema sa Pag-install
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Mga Error sa Model Conversion
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Mga Isyu sa Performance
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Mga Problema sa Memory
- Bawasan ang batch size ng modelo habang nag-o-optimize
- Gumamit ng streaming para sa malalaking datasets
- I-enable ang model caching: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Pagbaba ng Katumpakan
- Gumamit ng mas mataas na precision (INT8 sa halip na INT4)
- Palakihin ang laki ng calibration dataset
- Mag-aplay ng mixed precision optimization

### Performance Monitoring

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Pagkuha ng Tulong

- **Dokumentasyon**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community Forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Karagdagang Mga Mapagkukunan

### Opisyal na Mga Link
- **OpenVINO Homepage**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Mga Mapagkukunan sa Pag-aaral
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Mabilisang Gabay sa Pagsisimula**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimization Guide**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Mga Tool sa Integrasyon
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Performance Benchmarks
- **Opisyal na Benchmarks**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Mga Halimbawa mula sa Komunidad
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Komprehensibong tutorials na makikita sa OpenVINO notebooks repository
- **Sample Applications**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Mga halimbawa sa totoong mundo para sa iba't ibang domain (computer vision, NLP, audio)
- **Blog Posts**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Mga blog post mula sa Intel AI at komunidad na may detalyadong use cases

### Mga Kaugnay na Tool
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Karagdagang mga teknika sa optimization para sa Intel hardware
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Para sa mga mobile at edge deployment comparisons
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Mga alternatibong cross-platform inference engine

## ➡️ Ano ang susunod

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.