<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T14:39:58+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "tl"
}
-->
# Seksyon 3: Microsoft Olive Optimization Suite

## Talaan ng Nilalaman
1. [Panimula](../../../Module04)
2. [Ano ang Microsoft Olive?](../../../Module04)
3. [Pag-install](../../../Module04)
4. [Mabilisang Gabay sa Pagsisimula](../../../Module04)
5. [Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4](../../../Module04)
6. [Advanced na Paggamit](../../../Module04)
7. [Mga Pinakamahusay na Praktika](../../../Module04)
8. [Pag-aayos ng Problema](../../../Module04)
9. [Karagdagang Mga Mapagkukunan](../../../Module04)

## Panimula

Ang Microsoft Olive ay isang makapangyarihan at madaling gamiting hardware-aware na toolkit para sa pag-optimize ng modelo na nagpapadali sa proseso ng pag-optimize ng mga machine learning model para sa deployment sa iba't ibang hardware platform. Kung ang target mo ay CPUs, GPUs, o mga espesyal na AI accelerators, tinutulungan ka ng Olive na makamit ang pinakamainam na performance habang pinapanatili ang katumpakan ng modelo.

## Ano ang Microsoft Olive?

Ang Olive ay isang madaling gamiting hardware-aware na tool para sa pag-optimize ng modelo na gumagamit ng mga nangungunang teknolohiya sa industriya para sa compression, optimization, at compilation ng modelo. Gumagana ito kasama ang ONNX Runtime bilang isang E2E inference optimization solution.

### Pangunahing Katangian

- **Hardware-Aware Optimization**: Awtomatikong pinipili ang pinakamahusay na mga teknik sa pag-optimize para sa iyong target na hardware
- **40+ Built-in Optimization Components**: Saklaw ang compression ng modelo, quantization, optimization ng graph, at marami pa
- **Madaling CLI Interface**: Simpleng mga utos para sa karaniwang mga gawain sa pag-optimize
- **Suporta sa Multi-Framework**: Gumagana sa PyTorch, Hugging Face models, at ONNX
- **Suporta sa Mga Popular na Modelo**: Awtomatikong na-optimize ng Olive ang mga sikat na arkitektura ng modelo tulad ng Llama, Phi, Qwen, Gemma, at iba pa

### Mga Benepisyo

- **Mas Mabilis na Pag-develop**: Hindi na kailangang manu-manong mag-eksperimento sa iba't ibang teknik sa pag-optimize
- **Pagtaas ng Performance**: Malaking bilis ng pagpapabuti (hanggang 6x sa ilang kaso)
- **Deployment sa Iba't Ibang Platform**: Gumagana ang mga na-optimize na modelo sa iba't ibang hardware at operating systems
- **Pinapanatili ang Katumpakan**: Pinapanatili ng mga optimization ang kalidad ng modelo habang pinapabuti ang performance

## Pag-install

### Mga Kinakailangan

- Python 3.8 o mas mataas
- pip package manager
- Virtual environment (inirerekomenda)

### Pangunahing Pag-install

Gumawa at i-activate ang virtual environment:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

I-install ang Olive na may auto-optimization features:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opsyonal na Dependencies

Nag-aalok ang Olive ng iba't ibang opsyonal na dependencies para sa karagdagang mga tampok:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### I-verify ang Pag-install

```bash
olive --help
```

Kung matagumpay, makikita mo ang Olive CLI help message.

## Mabilisang Gabay sa Pagsisimula

### Ang Iyong Unang Optimization

I-optimize ang isang maliit na language model gamit ang auto-optimization feature ng Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ano ang Ginagawa ng Utos na Ito

Ang proseso ng optimization ay kinabibilangan ng: pagkuha ng modelo mula sa lokal na cache, pag-capture ng ONNX Graph at pag-store ng weights sa ONNX data file, pag-optimize ng ONNX Graph, at pag-quantize ng modelo sa int4 gamit ang RTN method.

### Paliwanag ng Mga Parameter ng Utos

- `--model_name_or_path`: Hugging Face model identifier o lokal na path
- `--output_path`: Direktoryo kung saan ise-save ang na-optimize na modelo
- `--device`: Target na device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gumamit ng ONNX Runtime Generate AI para sa inference
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4

Batay sa ibinigay na halimbawa ng Hugging Face sa [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), narito kung paano i-optimize ang isang Qwen3 model:

### Hakbang 1: I-download ang Modelo (Opsyonal)

Upang mabawasan ang oras ng pag-download, i-cache lamang ang mahahalagang file:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Hakbang 2: I-optimize ang Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hakbang 3: Subukan ang Na-optimize na Modelo

Gumawa ng simpleng Python script upang subukan ang iyong na-optimize na modelo:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Istruktura ng Output

Pagkatapos ng optimization, ang iyong output directory ay maglalaman ng:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced na Paggamit

### Mga Configuration File

Para sa mas kumplikadong workflows ng optimization, maaari kang gumamit ng JSON configuration files:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Patakbuhin gamit ang configuration:

```bash
olive run --config config.json
```

### GPU Optimization

Para sa CUDA GPU optimization:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para sa DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning gamit ang Olive

Sinusuportahan din ng Olive ang fine-tuning ng mga modelo:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mga Pinakamahusay na Praktika

### 1. Pagpili ng Modelo
- Magsimula sa mas maliliit na modelo para sa testing (hal., 0.5B-7B parameters)
- Siguraduhing suportado ng Olive ang target na arkitektura ng modelo

### 2. Mga Pagsasaalang-alang sa Hardware
- I-match ang iyong optimization target sa iyong deployment hardware
- Gumamit ng GPU optimization kung mayroon kang CUDA-compatible hardware
- Isaalang-alang ang DirectML para sa Windows machines na may integrated graphics

### 3. Pagpili ng Precision
- **INT4**: Pinakamataas na compression, bahagyang pagkawala ng katumpakan
- **INT8**: Magandang balanse ng laki at katumpakan
- **FP16**: Minimal na pagkawala ng katumpakan, katamtamang pagbabawas ng laki

### 4. Pagsubok at Pagpapatunay
- Laging subukan ang mga na-optimize na modelo gamit ang iyong partikular na use cases
- Ihambing ang mga performance metrics (latency, throughput, accuracy)
- Gumamit ng representative input data para sa evaluation

### 5. Iterative Optimization
- Magsimula sa auto-optimization para sa mabilisang resulta
- Gumamit ng configuration files para sa mas detalyadong kontrol
- Mag-eksperimento sa iba't ibang optimization passes

## Pag-aayos ng Problema

### Karaniwang Mga Isyu

#### 1. Mga Problema sa Pag-install
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Mga Isyu sa CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Mga Problema sa Memory
- Gumamit ng mas maliit na batch sizes sa panahon ng optimization
- Subukan ang quantization na may mas mataas na precision muna (int8 sa halip na int4)
- Siguraduhing may sapat na disk space para sa model caching

#### 4. Mga Error sa Pag-load ng Modelo
- I-verify ang path ng modelo at mga access permissions
- Tingnan kung ang modelo ay nangangailangan ng `trust_remote_code=True`
- Siguraduhing na-download ang lahat ng kinakailangang file ng modelo

### Pagkuha ng Tulong

- **Dokumentasyon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Mga Halimbawa**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Karagdagang Mga Mapagkukunan

### Opisyal na Mga Link
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Mga Halimbawa ng Komunidad
- **Jupyter Notebooks**: Available sa Olive GitHub repository
- **VS Code Extension**: Ang AI Toolkit extension ay gumagamit ng Olive para sa pag-optimize ng modelo
- **Mga Blog Post**: Ang Microsoft Open Source Blog ay may detalyadong Olive tutorials

### Kaugnay na Mga Tool
- **ONNX Runtime**: High-performance inference engine
- **Hugging Face Transformers**: Pinagmulan ng maraming compatible na modelo
- **Azure Machine Learning**: Cloud-based optimization workflows

## ➡️ Ano ang susunod

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.