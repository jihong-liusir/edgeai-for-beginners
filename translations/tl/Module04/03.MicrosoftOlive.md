<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T22:32:26+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "tl"
}
-->
# Seksyon 3: Microsoft Olive Optimization Suite

## Talaan ng Nilalaman
1. [Introduksyon](../../../Module04)
2. [Ano ang Microsoft Olive?](../../../Module04)
3. [Pag-install](../../../Module04)
4. [Mabilisang Gabay sa Pagsisimula](../../../Module04)
5. [Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4](../../../Module04)
6. [Advanced na Paggamit](../../../Module04)
7. [Pinakamahusay na Praktika](../../../Module04)
8. [Pag-aayos ng Problema](../../../Module04)
9. [Karagdagang Mga Mapagkukunan](../../../Module04)

## Introduksyon

Ang Microsoft Olive ay isang makapangyarihan at madaling gamitin na hardware-aware na toolkit para sa pag-optimize ng modelo na nagpapadali sa proseso ng pag-optimize ng mga machine learning na modelo para sa deployment sa iba't ibang hardware platform. Kung ang target mo ay CPUs, GPUs, o mga espesyal na AI accelerators, tinutulungan ka ng Olive na makamit ang pinakamainam na performance habang pinapanatili ang katumpakan ng modelo.

## Ano ang Microsoft Olive?

Ang Olive ay isang madaling gamitin na hardware-aware na tool para sa pag-optimize ng modelo na pinagsasama ang mga nangungunang teknolohiya sa industriya sa compression ng modelo, optimization, at compilation. Gumagana ito kasama ang ONNX Runtime bilang isang E2E inference optimization solution.

### Pangunahing Katangian

- **Hardware-Aware Optimization**: Awtomatikong pinipili ang pinakamahusay na mga teknik sa pag-optimize para sa iyong target na hardware
- **40+ Built-in Optimization Components**: Saklaw ang compression ng modelo, quantization, graph optimization, at iba pa
- **Madaling CLI Interface**: Simpleng mga utos para sa karaniwang mga gawain sa pag-optimize
- **Multi-Framework Support**: Gumagana sa PyTorch, Hugging Face models, at ONNX
- **Suporta sa Mga Popular na Modelo**: Awtomatikong na-optimize ng Olive ang mga sikat na arkitektura ng modelo tulad ng Llama, Phi, Qwen, Gemma, at iba pa

### Mga Benepisyo

- **Mas Maikling Oras ng Pag-develop**: Hindi na kailangang manu-manong mag-eksperimento sa iba't ibang teknik sa pag-optimize
- **Pagtaas ng Performance**: Malaking bilis ng pagpapabuti (hanggang 6x sa ilang kaso)
- **Cross-Platform Deployment**: Ang mga na-optimize na modelo ay gumagana sa iba't ibang hardware at operating systems
- **Pinanatiling Katumpakan**: Pinapanatili ng mga optimization ang kalidad ng modelo habang pinapabuti ang performance

## Pag-install

### Mga Kinakailangan

- Python 3.8 o mas mataas
- pip package manager
- Virtual environment (inirerekomenda)

### Pangunahing Pag-install

Gumawa at i-activate ang virtual environment:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

I-install ang Olive na may auto-optimization features:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opsyonal na Dependencies

Nag-aalok ang Olive ng iba't ibang opsyonal na dependencies para sa karagdagang mga tampok:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### I-verify ang Pag-install

```bash
olive --help
```

Kung matagumpay, makikita mo ang Olive CLI help message.

## Mabilisang Gabay sa Pagsisimula

### Ang Iyong Unang Optimization

I-optimize ang isang maliit na language model gamit ang auto-optimization feature ng Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ano ang Ginagawa ng Utos na Ito

Ang proseso ng optimization ay kinabibilangan ng: pagkuha ng modelo mula sa lokal na cache, pag-capture ng ONNX Graph at pag-store ng weights sa ONNX data file, pag-optimize ng ONNX Graph, at pag-quantize ng modelo sa int4 gamit ang RTN method.

### Paliwanag ng Mga Parameter ng Utos

- `--model_name_or_path`: Hugging Face model identifier o lokal na path
- `--output_path`: Direktoryo kung saan ise-save ang na-optimize na modelo
- `--device`: Target na device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gumamit ng ONNX Runtime Generate AI para sa inference
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4

Batay sa ibinigay na halimbawa ng Hugging Face sa [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), narito kung paano i-optimize ang isang Qwen3 model:

### Hakbang 1: I-download ang Modelo (Opsyonal)

Upang mabawasan ang oras ng pag-download, i-cache lamang ang mahahalagang file:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Hakbang 2: I-optimize ang Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hakbang 3: Subukan ang Na-optimize na Modelo

Gumawa ng simpleng Python script upang subukan ang iyong na-optimize na modelo:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Istruktura ng Output

Pagkatapos ng optimization, ang iyong output directory ay maglalaman ng:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced na Paggamit

### Mga Configuration File

Para sa mas kumplikadong workflows ng optimization, maaari kang gumamit ng JSON configuration files:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Patakbuhin gamit ang configuration:

```bash
olive run --config config.json
```

### GPU Optimization

Para sa CUDA GPU optimization:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para sa DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning gamit ang Olive

Sinusuportahan din ng Olive ang fine-tuning ng mga modelo:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Pinakamahusay na Praktika

### 1. Pagpili ng Modelo
- Magsimula sa mas maliliit na modelo para sa testing (hal., 0.5B-7B parameters)
- Siguraduhing suportado ng Olive ang target na arkitektura ng modelo

### 2. Mga Pagsasaalang-alang sa Hardware
- Itugma ang iyong optimization target sa iyong deployment hardware
- Gumamit ng GPU optimization kung mayroon kang CUDA-compatible hardware
- Isaalang-alang ang DirectML para sa mga Windows machine na may integrated graphics

### 3. Pagpili ng Precision
- **INT4**: Pinakamataas na compression, bahagyang pagkawala ng katumpakan
- **INT8**: Magandang balanse ng laki at katumpakan
- **FP16**: Minimal na pagkawala ng katumpakan, katamtamang pagbabawas ng laki

### 4. Pagsubok at Pagpapatunay
- Laging subukan ang mga na-optimize na modelo gamit ang iyong partikular na use cases
- Ihambing ang mga performance metrics (latency, throughput, katumpakan)
- Gumamit ng representative input data para sa evaluation

### 5. Iterative Optimization
- Magsimula sa auto-optimization para sa mabilisang resulta
- Gumamit ng configuration files para sa mas detalyadong kontrol
- Mag-eksperimento sa iba't ibang optimization passes

## Pag-aayos ng Problema

### Karaniwang Mga Isyu

#### 1. Mga Problema sa Pag-install
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Mga Isyu sa CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Mga Problema sa Memorya
- Gumamit ng mas maliit na batch sizes sa panahon ng optimization
- Subukan ang quantization na may mas mataas na precision muna (int8 sa halip na int4)
- Siguraduhing may sapat na disk space para sa model caching

#### 4. Mga Error sa Pag-load ng Modelo
- I-verify ang model path at access permissions
- Tingnan kung ang modelo ay nangangailangan ng `trust_remote_code=True`
- Siguraduhing na-download ang lahat ng kinakailangang model files

### Pagkuha ng Tulong

- **Dokumentasyon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Mga Halimbawa**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Karagdagang Mga Mapagkukunan

### Opisyal na Mga Link
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Mga Halimbawa ng Komunidad
- **Jupyter Notebooks**: Available sa Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit para sa VS Code overview — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Mga Blog Post**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Kaugnay na Mga Tool
- **ONNX Runtime**: High-performance inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Pinagmulan ng maraming compatible na modelo — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-based optimization workflows — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Ano ang Susunod

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

