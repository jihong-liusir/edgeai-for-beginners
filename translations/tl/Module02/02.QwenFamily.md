<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9b66db9742653af2dbeada08d98716e7",
  "translation_date": "2025-09-18T13:54:07+00:00",
  "source_file": "Module02/02.QwenFamily.md",
  "language_code": "tl"
}
-->
# Seksyon 2: Mga Pangunahing Kaalaman sa Pamilya ng Qwen

Ang pamilya ng modelo ng Qwen ay kumakatawan sa komprehensibong diskarte ng Alibaba Cloud sa malalaking modelo ng wika at multimodal AI, na nagpapakita na ang mga open-source na modelo ay maaaring makamit ang kahanga-hangang pagganap habang naa-access sa iba't ibang mga senaryo ng deployment. Mahalagang maunawaan kung paano nagbibigay ang pamilya ng Qwen ng makapangyarihang kakayahan sa AI na may mga flexible na opsyon sa deployment habang pinapanatili ang kompetitibong pagganap sa iba't ibang gawain.

## Mga Mapagkukunan para sa mga Developer

### Hugging Face Model Repository
Ang mga napiling modelo ng pamilya ng Qwen ay magagamit sa [Hugging Face](https://huggingface.co/models?search=qwen), na nagbibigay ng access sa ilang mga variant ng mga modelong ito. Maaari mong tuklasin ang mga magagamit na variant, i-fine-tune ang mga ito para sa iyong partikular na mga kaso ng paggamit, at i-deploy ang mga ito sa pamamagitan ng iba't ibang mga framework.

### Mga Tool para sa Lokal na Pag-develop
Para sa lokal na pag-develop at testing, maaari mong gamitin ang [Microsoft Foundry Local](https://github.com/microsoft/foundry-local) upang patakbuhin ang mga magagamit na modelo ng Qwen sa iyong development machine na may optimized na pagganap.

### Mga Mapagkukunan ng Dokumentasyon
- [Dokumentasyon ng Modelo ng Qwen](https://huggingface.co/docs/transformers/model_doc/qwen)
- [Pag-optimize ng Mga Modelo ng Qwen para sa Edge Deployment](https://github.com/microsoft/olive)

## Panimula

Sa tutorial na ito, ating susuriin ang pamilya ng modelo ng Qwen ng Alibaba at ang mga pangunahing konsepto nito. Tatalakayin natin ang ebolusyon ng pamilya ng Qwen, ang mga makabagong pamamaraan ng pagsasanay na nagpapahusay sa mga modelo ng Qwen, mga pangunahing variant sa pamilya, at mga praktikal na aplikasyon sa iba't ibang mga senaryo.

## Mga Layunin sa Pag-aaral

Sa pagtatapos ng tutorial na ito, magagawa mong:

- Maunawaan ang pilosopiya ng disenyo at ebolusyon ng pamilya ng modelo ng Qwen ng Alibaba
- Tukuyin ang mga pangunahing inobasyon na nagpapahintulot sa mga modelo ng Qwen na makamit ang mataas na pagganap sa iba't ibang laki ng parameter
- Kilalanin ang mga benepisyo at limitasyon ng iba't ibang variant ng modelo ng Qwen
- Ilapat ang kaalaman tungkol sa mga modelo ng Qwen upang pumili ng naaangkop na variant para sa mga totoong senaryo

## Pag-unawa sa Modernong Landscape ng AI Model

Ang landscape ng AI ay malaki ang pagbabago, kung saan ang iba't ibang organisasyon ay sumusunod sa iba't ibang diskarte sa pag-develop ng modelo ng wika. Habang ang ilan ay nakatuon sa proprietary closed-source na mga modelo, ang iba ay binibigyang-diin ang open-source na accessibility at transparency. Ang tradisyunal na diskarte ay kinabibilangan ng alinman sa malalaking proprietary na mga modelo na naa-access lamang sa pamamagitan ng mga API o open-source na mga modelo na maaaring kulang sa kakayahan.

Ang paradigm na ito ay lumilikha ng mga hamon para sa mga organisasyong naghahanap ng makapangyarihang kakayahan sa AI habang pinapanatili ang kontrol sa kanilang data, gastos, at flexibility sa deployment. Ang tradisyunal na diskarte ay madalas na nangangailangan ng pagpili sa pagitan ng cutting-edge na pagganap at mga praktikal na konsiderasyon sa deployment.

## Ang Hamon ng Accessible AI Excellence

Ang pangangailangan para sa mataas na kalidad, accessible na AI ay nagiging mas mahalaga sa iba't ibang mga senaryo. Isaalang-alang ang mga aplikasyon na nangangailangan ng flexible na opsyon sa deployment para sa iba't ibang pangangailangan ng organisasyon, cost-effective na mga implementasyon kung saan ang mga gastos sa API ay maaaring maging mahalaga, multilingual na kakayahan para sa mga global na aplikasyon, o specialized na domain expertise sa mga lugar tulad ng coding at matematika.

### Mga Pangunahing Pangangailangan sa Deployment

Ang mga modernong deployment ng AI ay nahaharap sa ilang mga pangunahing pangangailangan na naglilimita sa praktikal na applicability:

- **Accessibility**: Open-source na availability para sa transparency at customization
- **Cost Effectiveness**: Makatuwirang mga kinakailangan sa computational para sa iba't ibang badyet
- **Flexibility**: Maramihang laki ng modelo para sa iba't ibang senaryo ng deployment
- **Global Reach**: Malakas na multilingual at cross-cultural na kakayahan
- **Specialization**: Mga domain-specific na variant para sa partikular na mga kaso ng paggamit

## Ang Pilosopiya ng Modelo ng Qwen

Ang pamilya ng modelo ng Qwen ay kumakatawan sa isang komprehensibong diskarte sa pag-develop ng modelo ng AI, na inuuna ang open-source na accessibility, multilingual na kakayahan, at praktikal na deployment habang pinapanatili ang kompetitibong mga katangian ng pagganap. Ang mga modelo ng Qwen ay nakamit ito sa pamamagitan ng iba't ibang laki ng modelo, mataas na kalidad na mga pamamaraan ng pagsasanay, at mga specialized na variant para sa iba't ibang domain.

Ang pamilya ng Qwen ay sumasaklaw sa iba't ibang mga diskarte na idinisenyo upang magbigay ng mga opsyon sa buong spectrum ng performance-efficiency, na nagpapahintulot sa deployment mula sa mga mobile device hanggang sa mga enterprise server habang nagbibigay ng makabuluhang kakayahan sa AI. Ang layunin ay gawing accessible ang mataas na kalidad na AI habang nagbibigay ng flexibility sa mga pagpipilian sa deployment.

### Mga Pangunahing Prinsipyo ng Disenyo ng Qwen

Ang mga modelo ng Qwen ay binuo sa ilang mga pangunahing prinsipyo na nagtatangi sa kanila mula sa iba pang mga pamilya ng modelo ng wika:

- **Open Source First**: Kumpletong transparency at accessibility para sa pananaliksik at komersyal na paggamit
- **Comprehensive Training**: Pagsasanay sa malalaking, magkakaibang dataset na sumasaklaw sa maraming wika at domain
- **Scalable Architecture**: Maramihang laki ng modelo upang tumugma sa iba't ibang mga kinakailangan sa computational
- **Specialized Excellence**: Mga domain-specific na variant na na-optimize para sa partikular na mga gawain

## Mga Pangunahing Teknolohiya na Nagpapagana sa Pamilya ng Qwen

### Malakihang Pagsasanay

Isa sa mga defining na aspeto ng pamilya ng Qwen ay ang malakihang pagsasanay ng data at mga computational resource na ginugol sa pag-develop ng modelo. Ang mga modelo ng Qwen ay gumagamit ng maingat na curated, multilingual na mga dataset na sumasaklaw sa trilyon-trilyong mga token, na idinisenyo upang magbigay ng komprehensibong kaalaman sa mundo at kakayahan sa pangangatwiran.

Ang diskarte na ito ay gumagana sa pamamagitan ng pagsasama ng mataas na kalidad na nilalaman sa web, akademikong literatura, mga repository ng code, at mga multilingual na mapagkukunan. Ang pamamaraan ng pagsasanay ay binibigyang-diin ang parehong lawak ng kaalaman at lalim ng pag-unawa sa iba't ibang domain at wika.

### Advanced na Pangangatwiran at Pag-iisip

Ang mga kamakailang modelo ng Qwen ay nagsasama ng sopistikadong kakayahan sa pangangatwiran na nagpapahintulot sa kumplikadong multi-step na paglutas ng problema:

**Thinking Mode (Qwen3)**: Ang mga modelo ay maaaring makisali sa detalyado, hakbang-hakbang na pangangatwiran bago magbigay ng mga huling sagot, katulad ng mga diskarte sa paglutas ng problema ng tao.

**Dual-Mode Operation**: Kakayahang lumipat sa pagitan ng mabilis na mode ng pagtugon para sa mga simpleng query at mas malalim na mode ng pag-iisip para sa mga kumplikadong problema.

**Chain-of-Thought Integration**: Natural na pagsasama ng mga hakbang sa pangangatwiran na nagpapabuti sa transparency at katumpakan sa mga kumplikadong gawain.

### Mga Inobasyon sa Arkitektura

Ang pamilya ng Qwen ay nagsasama ng ilang mga optimisasyon sa arkitektura na idinisenyo para sa parehong pagganap at kahusayan:

**Scalable Design**: Pare-parehong arkitektura sa iba't ibang laki ng modelo na nagpapadali sa scaling at paghahambing.

**Multimodal Integration**: Seamless na pagsasama ng text, vision, at audio processing capabilities sa loob ng unified na mga arkitektura.

**Deployment Optimization**: Maramihang mga opsyon sa quantization at mga format ng deployment para sa iba't ibang hardware configurations.

## Laki ng Modelo at Mga Opsyon sa Deployment

Ang mga modernong environment ng deployment ay nakikinabang sa flexibility ng mga modelo ng Qwen sa iba't ibang mga kinakailangan sa computational:

### Maliit na Mga Modelo (0.5B-3B)

Nagbibigay ang Qwen ng mga efficient na maliit na modelo na angkop para sa edge deployment, mga mobile application, at mga environment na may limitadong resources habang pinapanatili ang kahanga-hangang kakayahan.

### Katamtamang Mga Modelo (7B-32B)

Ang mga mid-range na modelo ay nag-aalok ng pinahusay na kakayahan para sa mga propesyonal na aplikasyon, na nagbibigay ng mahusay na balanse sa pagitan ng pagganap at mga kinakailangan sa computational.

### Malalaking Mga Modelo (72B+)

Ang mga full-scale na modelo ay naghahatid ng state-of-the-art na pagganap para sa mga demanding na aplikasyon, pananaliksik, at mga enterprise deployment na nangangailangan ng maximum na kakayahan.

## Mga Benepisyo ng Pamilya ng Modelo ng Qwen

### Open Source Accessibility

Ang mga modelo ng Qwen ay nagbibigay ng kumpletong transparency at kakayahan sa customization, na nagpapahintulot sa mga organisasyon na maunawaan, baguhin, at i-adapt ang mga modelo sa kanilang partikular na pangangailangan nang walang vendor lock-in.

### Deployment Flexibility

Ang hanay ng laki ng modelo ay nagpapahintulot sa deployment sa iba't ibang hardware configurations, mula sa mga mobile device hanggang sa high-end na mga server, na nagbibigay sa mga organisasyon ng flexibility sa kanilang mga pagpipilian sa AI infrastructure.

### Multilingual Excellence

Ang mga modelo ng Qwen ay mahusay sa multilingual na pag-unawa at pagbuo, na sumusuporta sa dose-dosenang mga wika na may partikular na lakas sa Ingles at Tsino, na ginagawa silang angkop para sa mga global na aplikasyon.

### Kompetitibong Pagganap

Ang mga modelo ng Qwen ay patuloy na nakakamit ng kompetitibong resulta sa mga benchmark habang nagbibigay ng open-source na accessibility, na nagpapakita na ang mga open na modelo ay maaaring tumapat sa mga proprietary na alternatibo.

### Specialized Capabilities

Ang mga domain-specific na variant tulad ng Qwen-Coder at Qwen-Math ay nagbibigay ng specialized na expertise habang pinapanatili ang pangkalahatang kakayahan sa pag-unawa sa wika.

## Mga Praktikal na Halimbawa at Mga Kaso ng Paggamit

Bago sumisid sa mga teknikal na detalye, tuklasin natin ang ilang konkretong halimbawa ng kung ano ang maaaring magawa ng mga modelo ng Qwen:

### Halimbawa ng Pangangatwiran sa Matematika

Ang Qwen-Math ay mahusay sa hakbang-hakbang na paglutas ng problema sa matematika. Halimbawa, kapag tinanong na lutasin ang isang kumplikadong problema sa calculus:

```
User: Find the derivative of f(x) = x³ + 2x² - 5x + 3

Qwen-Math: I'll solve this step by step using the power rule for derivatives.

For f(x) = x³ + 2x² - 5x + 3, I need to find f'(x).

Using the power rule: d/dx(xⁿ) = n·xⁿ⁻¹

Breaking down each term:
- d/dx(x³) = 3x²
- d/dx(2x²) = 2 · 2x¹ = 4x  
- d/dx(-5x) = -5
- d/dx(3) = 0 (constant term)

Therefore: f'(x) = 3x² + 4x - 5
```

### Halimbawa ng Multilingual na Suporta

Ang mga modelo ng Qwen ay nagpapakita ng malakas na kakayahan sa multilingual sa iba't ibang wika:

```
User: Explain the concept of artificial intelligence in both English and Chinese.

Qwen: **English:**
Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human cognitive abilities.

**中文:**
人工智能（AI）是指机器（特别是计算机系统）对人类智能过程的模拟。这些过程包括学习、推理、解决问题、感知和语言理解。人工智能使机器能够执行通常需要人类认知能力的任务。
```

### Halimbawa ng Multimodal na Kakayahan

Ang Qwen-VL ay maaaring magproseso ng parehong text at mga imahe nang sabay:

```
User: [Image of a chart showing sales data] What trends do you see in this sales chart?

Qwen-VL: Looking at this sales chart, I can identify several key trends:

1. **Seasonal Pattern**: There's a clear seasonal cycle with peaks typically occurring in Q4, likely due to holiday sales.

2. **Overall Growth**: Despite fluctuations, there's an upward trend over the time period shown, indicating business growth.

3. **Quarterly Variations**: Q1 and Q3 show consistently lower performance compared to Q2 and Q4.

4. **Recent Acceleration**: The most recent data points suggest accelerating growth, with steeper increases than historical averages.

5. **Volatility Changes**: Earlier periods show more stability, while recent periods display increased volatility, possibly indicating market expansion or increased competition.
```

### Halimbawa ng Pagbuo ng Code

Ang Qwen-Coder ay mahusay sa pagbuo at pagpapaliwanag ng code sa iba't ibang programming languages:

```
User: Create a Python function to implement a binary search algorithm with detailed comments.

Qwen-Coder: Here's a Python implementation of binary search with comprehensive comments:

```python
def binary_search(arr, target):
    """
    Magpatupad ng binary search sa isang sorted na array upang mahanap ang target na halaga.
    
    Args:
        arr (list): Isang sorted na listahan ng mga maihahambing na elemento
        target: Ang halaga na hinahanap
        
    Returns:
        int: Index ng target kung natagpuan, -1 kung hindi natagpuan
        
    Time Complexity: O(log n)
    Space Complexity: O(1)
    """
    # I-initialize ang left at right pointers
    left, right = 0, len(arr) - 1
    
    # Magpatuloy sa paghahanap habang valid ang search space
    while left <= right:
        # Kalkulahin ang middle index upang maiwasan ang integer overflow
        mid = left + (right - left) // 2
        
        # Suriin kung natagpuan ang target
        if arr[mid] == target:
            return mid
        
        # Kung mas maliit ang target, hanapin ang kaliwang bahagi
        elif arr[mid] > target:
            right = mid - 1
        
        # Kung mas malaki ang target, hanapin ang kanang bahagi
        else:
            left = mid + 1
    
    # Hindi natagpuan ang target
    return -1

# Halimbawa ng paggamit:
sorted_list = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
result = binary_search(sorted_list, 7)
print(f"Index ng 7: {result}")  # Output: Index ng 7: 3
```

This implementation follows best practices with clear variable names, comprehensive documentation, and efficient logic.
```

### Halimbawa ng Edge Deployment

Ang mga modelo ng Qwen ay maaaring i-deploy sa iba't ibang edge devices na may optimized na configurations:

```
# Example deployment on mobile device with quantization
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load quantized model for mobile deployment
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # 8-bit quantization for efficiency
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Mobile-optimized inference
def mobile_inference(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## Ebolusyon ng Pamilya ng Qwen

### Qwen 1.0 at 1.5: Mga Foundation Models

Ang mga unang modelo ng Qwen ay nagtatag ng mga pangunahing prinsipyo ng komprehensibong pagsasanay at open-source na accessibility:

- **Qwen-7B (7B parameters)**: Paunang release na nakatuon sa pag-unawa sa wikang Tsino at Ingles
- **Qwen-14B (14B parameters)**: Pinahusay na kakayahan na may mas mahusay na pangangatwiran at kaalaman
- **Qwen-72B (72B parameters)**: Malakihang modelo na nagbibigay ng state-of-the-art na pagganap
- **Qwen1.5 Series**: Pinalawak sa maramihang laki (0.5B hanggang 110B) na may pinahusay na long-context handling

### Qwen2 Family: Multimodal na Pagpapalawak

Ang serye ng Qwen2 ay nagmarka ng makabuluhang pag-unlad sa parehong wika at multimodal na kakayahan:

- **Qwen2-0.5B hanggang 72B**: Komprehensibong hanay ng mga modelo ng wika para sa iba't ibang pangangailangan sa deployment
- **Qwen2-57B-A14B (MoE)**: Mixture-of-experts na arkitektura para sa efficient na paggamit ng parameter
- **Qwen2-VL**: Advanced na vision-language na kakayahan para sa pag-unawa sa imahe
- **Qwen2-Audio**: Kakayahan sa pagproseso at pag-unawa sa audio
- **Qwen2-Math**: Specialized na pangangatwiran sa matematika at paglutas ng problema

### Qwen2.5 Family: Pinahusay na Pagganap

Ang serye ng Qwen2.5 ay nagdala ng makabuluhang pagpapabuti sa lahat ng dimensyon:

- **Pinalawak na Pagsasanay**: 18 trilyong token ng data ng pagsasanay para sa pinahusay na kakayahan
- **Pinalawak na Context**: Hanggang 128K na haba ng token ng context, na may Turbo variant na sumusuporta sa 1M na token
- **Pinahusay na Specialization**: Pinahusay na Qwen2.5-Coder at Qwen2.5-Math na mga variant
- **Mas Mahusay na Multilingual na Suporta**: Pinahusay na pagganap sa 27+ na wika

### Qwen3 Family: Advanced na Pangangatwiran

Ang pinakabagong henerasyon ay nagtutulak sa mga hangganan ng pangangatwiran at kakayahan sa pag-iisip:

- **Qwen3-235B-A22B**: Flagship na mixture-of-experts na modelo na may kabuuang 235B na mga parameter
- **Qwen3-30B-A3B**: Efficient na MoE na modelo na may malakas na pagganap bawat aktibong parameter
- **Dense Models**: Qwen3-32B, 14B, 8B, 4B, 1.7B, 0.6B para sa iba't ibang senaryo ng deployment
- **Thinking Mode**: Hybrid na diskarte sa pangangatwiran na sumusuporta sa parehong mabilis na mga tugon at malalim na pag-iisip
- **Multilingual Excellence**: Suporta para sa 119 na wika at diyalekto
- **Pinahusay na Pagsasanay**: 36 trilyong token ng magkakaibang, mataas na kalidad na data ng pagsasanay

## Mga Aplikasyon ng Mga Modelo ng Qwen

### Mga Aplikasyon sa Enterprise

Ginagamit ng mga organisasyon ang mga modelo ng Qwen para sa pagsusuri ng dokumento, automation ng customer service, tulong sa pagbuo ng code, at mga aplikasyon sa business intelligence. Ang open-source na kalikasan ay nagbibigay-daan sa customization para sa mga partikular na pangangailangan ng negosyo habang pinapanatili ang privacy at kontrol sa data.

### Mobile at Edge Computing

Ang mga mobile application ay gumagamit ng mga modelo ng Qwen para sa real-time na pagsasalin, intelligent na mga assistant, pagbuo ng nilalaman, at personalized na mga rekomendasyon. Ang hanay ng laki ng modelo ay nagpapahintulot sa deployment mula sa mga mobile device hanggang sa mga edge server.

### Teknolohiya sa Edukasyon

Ang mga platform sa edukasyon ay gumagamit ng mga modelo ng Qwen para sa personalized na pagtuturo,
Narito kung paano magsimula sa paggamit ng mga Qwen model gamit ang Hugging Face Transformers library:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load Qwen3-8B model
model_name = "Qwen/Qwen3-8B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Prepare conversation with chat template
messages = [
    {"role": "user", "content": "Give me a short introduction to large language models."}
]

# Apply chat template and generate response
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

# Extract and display response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### Paggamit ng Qwen2.5 Models

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example with Qwen2.5-7B-Instruct
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Structured conversation example
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms."}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Generate response with optimized settings
model_inputs = tokenizer([text], return_tensors="pt")
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05
)

response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### Espesyal na Paggamit ng Model

**Paglikha ng Code gamit ang Qwen-Coder:**
```python
# Using Qwen2.5-Coder for programming tasks
model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """
Create a Python function that:
1. Takes a list of numbers as input
2. Returns the median value
3. Handles edge cases like empty lists
4. Include proper documentation and type hints
"""

messages = [{"role": "user", "content": prompt}]
# Process with model to generate code solution
```

**Paglutas ng Problema sa Matematika:**
```python
# Using Qwen2.5-Math for mathematical reasoning
model_name = "Qwen/Qwen2.5-Math-7B-Instruct"

prompt = """
Solve this step by step:
Find the derivative of f(x) = x³ + 2x² - 5x + 3
and then find the critical points.
"""

messages = [{"role": "user", "content": prompt}]
# Generate mathematical solution with step-by-step reasoning
```

**Mga Gawain sa Vision-Language:**
```python
# For image understanding with Qwen-VL
from qwen_vl_utils import process_vision_info

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "path/to/image.jpg"},
            {"type": "text", "text": "Describe what's happening in this image and identify any text present."}
        ]
    }
]

# Process image and generate comprehensive description
```

### Thinking Mode (Qwen3)

```python
# Using Qwen3 with thinking mode for complex reasoning
model_name = "Qwen/Qwen3-8B"

# Enable thinking mode for complex problems
prompt = """
Analyze the following business scenario and provide a strategic recommendation:

A startup has developed an innovative AI-powered educational app. They have limited funding, 
strong technical capabilities, but no marketing experience. They're deciding between:
1. Focusing on B2B sales to schools
2. Direct-to-consumer marketing
3. Partnering with existing educational publishers

Consider market dynamics, resource constraints, and growth potential.
"""

messages = [{"role": "user", "content": prompt}]

# The model will generate <think>...</think> reasoning before final answer
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Generate with thinking mode
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024,
    thinking_budget=512  # Allow extended reasoning
)

# Parse thinking content and final response
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# Extract thinking process and final answer
try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True)
final_response = tokenizer.decode(output_ids[index:], skip_special_tokens=True)

print("Thinking Process:", thinking_content)
print("Final Recommendation:", final_response)
```

### 📱 Deployment sa Mobile at Edge

```python
# Optimized deployment for resource-constrained environments
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load smallest efficient model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # Reduce memory usage
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

def efficient_inference(prompt, max_length=256):
    """Optimized inference for mobile/edge deployment"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=True
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# Example mobile-optimized usage
quick_response = efficient_inference("What is machine learning?", max_length=100)
print(quick_response)
```

### Halimbawa ng API Deployment

```python
# Deploy Qwen model as API using vLLM
from vllm import LLM, SamplingParams

# Initialize model for API serving
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8
)

# Configure sampling parameters
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=512
)

def api_generate(prompts):
    """API endpoint for text generation"""
    # Format prompts with chat template
    formatted_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        formatted_prompts.append(formatted_prompt)
    
    # Generate responses
    outputs = llm.generate(formatted_prompts, sampling_params)
    
    # Extract and return responses
    responses = []
    for output in outputs:
        response = output.outputs[0].text.strip()
        responses.append(response)
    
    return responses

# Example API usage
user_prompts = [
    "Explain the benefits of renewable energy",
    "Write a Python function to calculate factorial"
]
responses = api_generate(user_prompts)
for prompt, response in zip(user_prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response}\n")
```

## Mga Benchmark ng Performance at Mga Nakamit

Ang pamilya ng Qwen model ay nakamit ang kahanga-hangang performance sa iba't ibang benchmark habang nananatiling accessible bilang open-source:

### Mga Pangunahing Highlight ng Performance

**Kahusayan sa Pangangatwiran:**
- Ang Qwen3-235B-A22B ay nakamit ang kompetitibong resulta sa mga benchmark ng coding, matematika, at pangkalahatang kakayahan kumpara sa iba pang nangungunang modelo tulad ng DeepSeek-R1, o1, o3-mini, Grok-3, at Gemini-2.5-Pro
- Ang Qwen3-30B-A3B ay mas mahusay kaysa sa QwQ-32B na may 10 beses na aktibong mga parameter
- Ang Qwen3-4B ay maaring makipagsabayan sa performance ng Qwen2.5-72B-Instruct

**Mga Nakamit sa Kahusayan:**
- Ang mga base model ng Qwen3-MoE ay nakamit ang katulad na performance sa mga dense base model ng Qwen2.5 habang gumagamit lamang ng 10% ng aktibong mga parameter
- Malaking pagtitipid sa gastos sa parehong training at inference kumpara sa dense models

**Multilingual na Kakayahan:**
- Ang mga Qwen3 model ay sumusuporta sa 119 na wika at diyalekto
- Malakas na performance sa iba't ibang lingguwistiko at kultural na konteksto

**Saklaw ng Training:**
- Ang Qwen3 ay gumagamit ng halos doble ng dami, na may humigit-kumulang 36 trilyong token na sumasaklaw sa 119 na wika at diyalekto kumpara sa 18 trilyong token ng Qwen2.5

### Matrix ng Paghahambing ng Model

| Serye ng Model | Saklaw ng Parameter | Haba ng Konteksto | Pangunahing Lakas | Pinakamahusay na Gamit |
|----------------|---------------------|-------------------|-------------------|------------------------|
| **Qwen2.5** | 0.5B-72B | 32K-128K | Balanseng performance, multilingual | Pangkalahatang aplikasyon, production deployment |
| **Qwen2.5-Coder** | 1.5B-32B | 128K | Paglikha ng code, programming | Software development, coding assistance |
| **Qwen2.5-Math** | 1.5B-72B | 4K-128K | Pangangatwiran sa matematika | Mga platform ng edukasyon, STEM applications |
| **Qwen2.5-VL** | Iba't-iba | Variable | Pag-unawa sa vision-language | Multimodal applications, image analysis |
| **Qwen3** | 0.6B-235B | Variable | Advanced na pangangatwiran, thinking mode | Kumplikadong pangangatwiran, research applications |
| **Qwen3 MoE** | 30B-235B total | Variable | Mahusay na performance sa malakihang scale | Enterprise applications, high-performance needs |

## Gabay sa Pagpili ng Model

### Para sa Pangunahing Aplikasyon
- **Qwen2.5-0.5B/1.5B**: Mga mobile app, edge devices, real-time applications
- **Qwen2.5-3B/7B**: Pangkalahatang chatbots, content generation, Q&A systems

### Para sa Matematika at Pangangatwiran
- **Qwen2.5-Math**: Paglutas ng problema sa matematika at STEM education
- **Qwen3 na may Thinking Mode**: Kumplikadong pangangatwiran na nangangailangan ng step-by-step analysis

### Para sa Programming at Development
- **Qwen2.5-Coder**: Paglikha ng code, debugging, programming assistance
- **Qwen3**: Advanced na mga programming task na may kakayahan sa pangangatwiran

### Para sa Multimodal Applications
- **Qwen2.5-VL**: Pag-unawa sa imahe, visual question answering
- **Qwen-Audio**: Pagproseso ng audio at pag-unawa sa pagsasalita

### Para sa Enterprise Deployment
- **Qwen2.5-32B/72B**: Mataas na performance sa pag-unawa sa wika
- **Qwen3-235B-A22B**: Pinakamataas na kakayahan para sa mga demanding applications

## Mga Platform ng Deployment at Accessibility
### Mga Cloud Platform
- **Hugging Face Hub**: Komprehensibong repository ng model na may suporta mula sa komunidad
- **ModelScope**: Platform ng model ng Alibaba na may mga optimization tools
- **Iba't-ibang Cloud Providers**: Suporta sa pamamagitan ng standard ML platforms

### Mga Framework para sa Lokal na Development
- **Transformers**: Standard na Hugging Face integration para sa madaling deployment
- **vLLM**: Mataas na performance na serving para sa production environments
- **Ollama**: Simplified na lokal na deployment at management
- **ONNX Runtime**: Cross-platform optimization para sa iba't-ibang hardware
- **llama.cpp**: Mahusay na C++ implementation para sa iba't-ibang platform

### Mga Learning Resources
- **Qwen Documentation**: Opisyal na dokumentasyon at model cards
- **Hugging Face Model Hub**: Interactive na demos at mga halimbawa mula sa komunidad
- **Research Papers**: Mga teknikal na papel sa arxiv para sa mas malalim na pag-unawa
- **Community Forums**: Aktibong suporta mula sa komunidad at mga diskusyon

### Pagsisimula sa Qwen Models

#### Mga Platform ng Development
1. **Hugging Face Transformers**: Magsimula sa standard na Python integration
2. **ModelScope**: Tuklasin ang mga optimized deployment tools ng Alibaba
3. **Lokal na Deployment**: Gumamit ng Ollama o direktang transformers para sa lokal na testing

#### Learning Path
1. **Unawain ang Core Concepts**: Pag-aralan ang arkitektura at kakayahan ng pamilya ng Qwen
2. **Mag-eksperimento sa mga Variant**: Subukan ang iba't-ibang laki ng model para maunawaan ang trade-offs sa performance
3. **Magpraktis ng Implementasyon**: I-deploy ang mga model sa development environments
4. **I-optimize ang Deployment**: Fine-tune para sa production use cases

#### Best Practices
- **Magsimula sa Maliit**: Simulan sa mas maliliit na model (1.5B-7B) para sa paunang development
- **Gumamit ng Chat Templates**: Mag-apply ng tamang formatting para sa optimal na resulta
- **Subaybayan ang Resources**: I-track ang memory usage at inference speed
- **Isaalang-alang ang Espesyalisasyon**: Pumili ng domain-specific variants kung kinakailangan

## Mga Advanced na Pattern ng Paggamit

### Mga Halimbawa ng Fine-tuning

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset

# Load base model for fine-tuning
model_name = "Qwen/Qwen2.5-7B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration
training_args = TrainingArguments(
    output_dir="./qwen-finetuned",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False
)

# Load and prepare dataset
def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['output']}<|im_end|>"

dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(
    lambda x: {"text": format_instruction(x)},
    remove_columns=dataset["train"].column_names
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()
```

### Espesyal na Prompt Engineering

**Para sa Kumplikadong Pangangatwiran:**
```python
def create_reasoning_prompt(problem, context=""):
    """Create structured prompt for complex reasoning"""
    prompt = f"""<|im_start|>system
You are Qwen, a helpful AI assistant. When solving complex problems, break down your reasoning into clear steps.

Instructions:
1. Analyze the problem carefully
2. Identify key components and relationships
3. Work through the solution step by step
4. Verify your answer
5. Provide a clear final answer

{context}
<|im_end|>
<|im_start|>user
{problem}

Please solve this step by step, showing your reasoning process.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
complex_problem = """
A company's revenue grows by 15% each year. If they had $2 million in revenue in 2020, 
and they want to reach $5 million by 2025, will they achieve this goal? 
If not, what growth rate would they need?
"""

reasoning_prompt = create_reasoning_prompt(complex_problem)
```

**Para sa Paglikha ng Code na may Konteksto:**
```python
def create_coding_prompt(task, language="Python", context="", constraints=""):
    """Create structured prompt for code generation"""
    prompt = f"""<|im_start|>system
You are Qwen-Coder, an expert programming assistant. Generate clean, efficient, and well-documented code.

Requirements:
- Use {language} programming language
- Include comprehensive docstrings
- Add type hints where appropriate
- Follow best practices and conventions
- Include example usage

{context}
<|im_end|>
<|im_start|>user
Task: {task}

{f"Constraints: {constraints}" if constraints else ""}

Please provide a complete, production-ready solution.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
coding_task = """
Create a class that manages a simple in-memory cache with TTL (time-to-live) support.
The cache should support get, set, delete operations and automatically expire entries.
"""

constraints = """
- Thread-safe operations
- Configurable default TTL
- Memory-efficient cleanup of expired entries
- Support for custom serialization
"""

coding_prompt = create_coding_prompt(coding_task, "Python", constraints=constraints)
```

### Multilingual na Aplikasyon

```python
def create_multilingual_prompt(query, target_languages=["en", "zh", "es"]):
    """Create prompt for multilingual responses"""
    language_names = {
        "en": "English",
        "zh": "Chinese (中文)",
        "es": "Spanish (Español)",
        "fr": "French (Français)",
        "de": "German (Deutsch)",
        "ja": "Japanese (日本語)"
    }
    
    lang_list = [language_names.get(lang, lang) for lang in target_languages]
    lang_str = ", ".join(lang_list)
    
    prompt = f"""<|im_start|>system
You are Qwen, a multilingual AI assistant. Provide responses in multiple languages as requested.
Ensure cultural appropriateness and natural expression in each language.
<|im_end|>
<|im_start|>user
Please answer the following question in {lang_str}:

{query}

Provide clear, culturally appropriate responses in each requested language.
<|im_end|>
<|im_start|>assistant"""
    
    return prompt

# Example usage
multilingual_query = "What are the benefits of renewable energy for the environment?"
multilingual_prompt = create_multilingual_prompt(
    multilingual_query, 
    target_languages=["en", "zh", "es"]
)
```

### 🔧 Mga Pattern ng Production Deployment

```python
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

class QwenService:
    """Production-ready Qwen model service"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        )
        
        # Optimize for inference
        self.model.eval()
        if hasattr(self.model, 'generation_config'):
            self.model.generation_config.pad_token_id = self.tokenizer.eos_token_id
    
    def format_chat(self, messages: List[Dict[str, str]]) -> str:
        """Format messages using chat template"""
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig()
    ) -> str:
        """Async generation for high-throughput applications"""
        formatted_prompt = self.format_chat(messages)
        
        # Tokenize input
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    max_new_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    repetition_penalty=config.repetition_penalty,
                    do_sample=config.do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )
        
        # Extract generated text
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_batch(
        self, 
        batch_messages: List[List[Dict[str, str]]], 
        config: GenerationConfig = GenerationConfig()
    ) -> List[str]:
        """Batch generation for efficiency"""
        formatted_prompts = [self.format_chat(messages) for messages in batch_messages]
        
        # Tokenize batch
        inputs = self.tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Extract all generated texts
        responses = []
        for i, output in enumerate(outputs):
            generated_text = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(generated_text.strip())
        
        return responses

# Example usage
async def main():
    # Initialize service
    qwen_service = QwenService("Qwen/Qwen2.5-7B-Instruct")
    
    # Single generation
    messages = [
        {"role": "user", "content": "Explain machine learning in simple terms"}
    ]
    response = await qwen_service.generate_async(messages)
    print("Single Response:", response)
    
    # Batch generation
    batch_messages = [
        [{"role": "user", "content": "What is artificial intelligence?"}],
        [{"role": "user", "content": "How does deep learning work?"}],
        [{"role": "user", "content": "What are neural networks?"}]
    ]
    
    batch_responses = qwen_service.generate_batch(batch_messages)
    for i, response in enumerate(batch_responses):
        print(f"Batch Response {i+1}:", response)

# Run the example
# asyncio.run(main())
```

## Mga Estratehiya sa Optimization ng Performance

### Optimization ng Memory

```python
# Memory-efficient loading strategies
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit quantization for memory efficiency
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# 4-bit quantization for maximum efficiency
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

efficient_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

### Optimization ng Inference

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel

# Optimized inference configuration
def optimized_inference_setup():
    """Configure optimizations for inference"""
    
    # Enable optimized attention mechanisms
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    
    # Set optimal threading
    torch.set_num_threads(4)  # Adjust based on your CPU
    
    # Enable JIT compilation for repeated patterns
    torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])

def fast_generate(model, tokenizer, prompt, max_tokens=256):
    """Optimized generation function"""
    with torch.no_grad():
        # Use optimized attention backend
        with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # Generate with optimizations
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                use_cache=True,  # Enable KV caching
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
            
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
    return response.strip()
```

## Mga Best Practices at Gabay

### Seguridad at Privacy

```python
import hashlib
import time
from typing import Optional

class SecureQwenService:
    """Security-focused Qwen service implementation"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self._load_model()
    
    def _sanitize_input(self, text: str) -> str:
        """Sanitize user input to prevent injection attacks"""
        # Remove or escape potentially harmful patterns
        dangerous_patterns = [
            "<script>", "</script>", 
            "javascript:", "data:",
            "<iframe>", "</iframe>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern, "")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str, max_requests: int = 100, window: int = 3600) -> bool:
        """Simple rate limiting implementation"""
        current_time = time.time()
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window
        ]
        
        # Check rate limit
        if len(self.request_logs[user_id]) >= max_requests:
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Optional[str]:
        """Generate with security measures"""
        
        # Rate limiting
        if not self._rate_limit_check(user_id):
            return "Rate limit exceeded. Please try again later."
        
        # Input sanitization
        sanitized_messages = []
        for message in messages:
            sanitized_content = self._sanitize_input(message.get("content", ""))
            sanitized_messages.append({
                "role": message.get("role", "user"),
                "content": sanitized_content
            })
        
        # Content length validation
        total_content_length = sum(len(msg["content"]) for msg in sanitized_messages)
        if total_content_length > 8192:  # Reasonable limit
            return "Input too long. Please reduce the content length."
        
        # Log request (with hashed sensitive data)
        content_hash = self._hash_sensitive_data(str(sanitized_messages))
        print(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
        
        # Generate response
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                sanitized_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=min(max_tokens, 1024),  # Enforce reasonable limits
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return "An error occurred while processing your request."
```

### Monitoring at Evaluation

```python
import time
import psutil
import torch
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class PerformanceMetrics:
    """Performance metrics for monitoring"""
    response_time: float
    memory_usage: float
    gpu_usage: float
    token_count: int
    tokens_per_second: float

class QwenMonitor:
    """Monitor Qwen model performance and health"""
    
    def __init__(self):
        self.metrics_history = []
    
    def measure_performance(self, model, tokenizer, prompt: str) -> PerformanceMetrics:
        """Measure comprehensive performance metrics"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics (if available)
        gpu_usage = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Calculate metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        token_count = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = token_count / response_time if response_time > 0 else 0
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            gpu_usage=gpu_usage,
            token_count=token_count,
            tokens_per_second=tokens_per_second
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def get_average_metrics(self, last_n: int = 10) -> Dict[str, float]:
        """Get average metrics from recent measurements"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = self.metrics_history[-last_n:]
        
        return {
            "avg_response_time": sum(m.response_time for m in recent_metrics) / len(recent_metrics),
            "avg_memory_usage": sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            "avg_gpu_usage": sum(m.gpu_usage for m in recent_metrics) / len(recent_metrics),
            "avg_tokens_per_second": sum(m.tokens_per_second for m in recent_metrics) / len(recent_metrics)
        }
    
    def health_check(self, model, tokenizer) -> Dict[str, Any]:
        """Perform comprehensive health check"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "recommendations": []
        }
        
        try:
            # Test basic functionality
            test_prompt = "Hello, how are you?"
            metrics = self.measure_performance(model, tokenizer, test_prompt)
            
            # Check response time
            if metrics.response_time > 10.0:  # seconds
                health_status["checks"]["response_time"] = "slow"
                health_status["recommendations"].append("Consider model optimization or hardware upgrade")
            else:
                health_status["checks"]["response_time"] = "good"
            
            # Check memory usage
            if metrics.memory_usage > 1000:  # MB
                health_status["checks"]["memory_usage"] = "high"
                health_status["recommendations"].append("Monitor memory usage and consider cleanup")
            else:
                health_status["checks"]["memory_usage"] = "good"
            
            # Check token generation rate
            if metrics.tokens_per_second < 5:
                health_status["checks"]["generation_speed"] = "slow"
                health_status["recommendations"].append("Optimize inference configuration")
            else:
                health_status["checks"]["generation_speed"] = "good"
            
            # Overall status
            if any(check in ["slow", "high"] for check in health_status["checks"].values()):
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
            health_status["recommendations"].append("Check model loading and configuration")
        
        return health_status

# Example usage
monitor = QwenMonitor()

# Regular performance monitoring
def monitor_model_performance(model, tokenizer, test_prompts: List[str]):
    """Monitor model performance with various prompts"""
    for prompt in test_prompts:
        metrics = monitor.measure_performance(model, tokenizer, prompt)
        print(f"Prompt: {prompt[:50]}...")
        print(f"Response time: {metrics.response_time:.2f}s")
        print(f"Tokens/sec: {metrics.tokens_per_second:.1f}")
        print(f"Memory usage: {metrics.memory_usage:.1f}MB")
        print("-" * 50)
    
    # Show average metrics
    avg_metrics = monitor.get_average_metrics()
    print("Average Performance Metrics:")
    for metric, value in avg_metrics.items():
        print(f"{metric}: {value:.2f}")
```

## Konklusyon

Ang pamilya ng Qwen model ay kumakatawan sa isang komprehensibong approach sa democratization ng AI technology habang nananatiling kompetitibo sa performance sa iba't-ibang aplikasyon. Sa pamamagitan ng commitment nito sa open-source accessibility, multilingual capabilities, at flexible deployment options, binibigyang-daan ng Qwen ang mga organisasyon at developer na magamit ang makapangyarihang AI capabilities anuman ang kanilang resources o partikular na pangangailangan.

### Mga Pangunahing Takeaways

**Kahusayan sa Open Source**: Ipinapakita ng Qwen na ang mga open-source model ay maaring makipagsabayan sa mga proprietary na alternatibo habang nagbibigay ng transparency, customization, at kontrol.

**Scalable na Arkitektura**: Ang saklaw mula 0.5B hanggang 235B na parameter ay nagbibigay-daan sa deployment sa buong spectrum ng computational environments, mula sa mobile devices hanggang sa enterprise clusters.

**Espesyal na Kakayahan**: Ang mga domain-specific variants tulad ng Qwen-Coder, Qwen-Math, at Qwen-VL ay nagbibigay ng espesyal na expertise habang nananatili ang pangkalahatang pag-unawa sa wika.

**Global na Accessibility**: Malakas na multilingual na suporta sa 119+ na wika ang ginagawa ang Qwen na angkop para sa mga international na aplikasyon at magkakaibang user base.

**Patuloy na Inobasyon**: Ang ebolusyon mula Qwen 1.0 hanggang Qwen3 ay nagpapakita ng tuloy-tuloy na pagpapabuti sa kakayahan, kahusayan, at mga opsyon sa deployment.

### Hinaharap na Perspektibo

Habang patuloy na umuunlad ang pamilya ng Qwen, maaring asahan ang:
- **Pinahusay na Kahusayan**: Patuloy na optimization para sa mas mahusay na performance-per-parameter ratios
- **Pinalawak na Multimodal na Kakayahan**: Integrasyon ng mas sopistikadong vision, audio, at text processing
- **Pinahusay na Pangangatwiran**: Advanced na mekanismo ng pag-iisip at multi-step na kakayahan sa paglutas ng problema
- **Mas Mahusay na Deployment Tools**: Pinahusay na mga framework at optimization tools para sa iba't-ibang deployment scenarios
- **Paglago ng Komunidad**: Pinalawak na ecosystem ng tools, applications, at kontribusyon mula sa komunidad

### Mga Susunod na Hakbang

Kung ikaw ay gumagawa ng chatbot, nagde-develop ng mga educational tools, lumilikha ng coding assistants, o nagtatrabaho sa multilingual applications, ang pamilya ng Qwen ay nagbibigay ng scalable na solusyon na may malakas na suporta mula sa komunidad at komprehensibong dokumentasyon.

Para sa pinakabagong updates, model releases, at detalyadong teknikal na dokumentasyon, bisitahin ang opisyal na repositories ng Qwen sa Hugging Face at tuklasin ang aktibong diskusyon at mga halimbawa mula sa komunidad.

Ang hinaharap ng AI development ay nakasalalay sa accessible, transparent, at makapangyarihang tools na nagbibigay-daan sa inobasyon sa lahat ng sektor at scale. Ang pamilya ng Qwen ay naglalarawan ng pananaw na ito, nagbibigay sa mga organisasyon at developer ng pundasyon para sa pagbuo ng susunod na henerasyon ng AI-powered applications.

## Karagdagang Resources

- **Opisyal na Dokumentasyon**: [Qwen Documentation](https://qwen.readthedocs.io/)
- **Model Hub**: [Hugging Face Qwen Collections](https://huggingface.co/collections/Qwen/)
- **Mga Teknikal na Papel**: [Qwen Research Publications](https://arxiv.org/search/?query=Qwen&searchtype=all)
- **Komunidad**: [GitHub Discussions and Issues](https://github.com/QwenLM/)
- **ModelScope Platform**: [Alibaba ModelScope](https://modelscope.cn/models?page=1&tasks=natural-language-processing&type=1)

## Mga Learning Outcomes

Pagkatapos makumpleto ang module na ito, magagawa mong:

1. Ipaliwanag ang mga arkitektural na bentahe ng pamilya ng Qwen model at ang open-source approach nito
2. Piliin ang angkop na variant ng Qwen base sa partikular na pangangailangan ng aplikasyon at limitasyon ng resources
3. I-implement ang mga Qwen model sa iba't-ibang deployment scenarios na may optimized configurations
4. Mag-apply ng quantization at optimization techniques para mapabuti ang performance ng Qwen model
5. Suriin ang trade-offs sa pagitan ng laki ng model, performance, at kakayahan sa buong pamilya ng Qwen

## Ano ang susunod

- [03: Mga Pangunahing Konsepto ng Gemma Family](03.GemmaFamily.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.