<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "27be883865b4bad1e3c7e02c696da642",
  "translation_date": "2025-09-18T14:58:21+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "tl"
}
-->
# Seksyon 1: SLM Advanced Learning - Mga Batayan at Pag-optimize

Ang Small Language Models (SLMs) ay kumakatawan sa mahalagang pag-unlad sa EdgeAI, na nagbibigay-daan sa sopistikadong kakayahan sa natural language processing sa mga device na may limitadong mapagkukunan. Ang pag-unawa kung paano epektibong i-deploy, i-optimize, at gamitin ang SLMs ay mahalaga para sa pagbuo ng praktikal na mga solusyon sa edge-based AI.

## Panimula

Sa araling ito, tatalakayin natin ang Small Language Models (SLMs) at ang kanilang mga advanced na estratehiya sa implementasyon. Saklawin natin ang mga pangunahing konsepto ng SLMs, ang kanilang mga limitasyon sa parameter at klasipikasyon, mga teknik sa pag-optimize, at mga praktikal na estratehiya sa pag-deploy para sa mga edge computing environment.

## Mga Layunin sa Pagkatuto

Sa pagtatapos ng araling ito, magagawa mo ang sumusunod:

- üî¢ Maunawaan ang mga limitasyon sa parameter at klasipikasyon ng Small Language Models.
- üõ†Ô∏è Tukuyin ang mga pangunahing teknik sa pag-optimize para sa pag-deploy ng SLM sa mga edge device.
- üöÄ Matutunan ang implementasyon ng mga advanced na estratehiya sa quantization at compression para sa SLMs.

## Pag-unawa sa Mga Limitasyon sa Parameter at Klasipikasyon ng SLM

Ang Small Language Models (SLMs) ay mga AI model na idinisenyo upang magproseso, umunawa, at bumuo ng natural na nilalaman ng wika na may mas kaunting mga parameter kumpara sa kanilang mas malalaking katapat. Habang ang Large Language Models (LLMs) ay naglalaman ng daan-daang bilyon hanggang trilyon na mga parameter, ang SLMs ay partikular na idinisenyo para sa kahusayan at edge deployment.

Ang framework ng klasipikasyon ng parameter ay tumutulong sa atin na maunawaan ang iba't ibang kategorya ng SLMs at ang kanilang naaangkop na mga kaso ng paggamit. Ang klasipikasyong ito ay mahalaga para sa pagpili ng tamang modelo para sa mga partikular na edge computing scenario.

### Framework ng Klasipikasyon ng Parameter

Ang pag-unawa sa mga limitasyon ng parameter ay tumutulong sa pagpili ng naaangkop na mga modelo para sa iba't ibang edge computing scenario:

- **üî¨ Micro SLMs**: 100M - 1.4B na mga parameter (napakagaan para sa mga mobile device)
- **üì± Small SLMs**: 1.5B - 13.9B na mga parameter (balanse sa performance at kahusayan)
- **‚öñÔ∏è Medium SLMs**: 14B - 30B na mga parameter (malapit sa kakayahan ng LLM habang pinapanatili ang kahusayan)

Ang eksaktong hangganan ay nananatiling pabago-bago sa komunidad ng pananaliksik, ngunit karamihan sa mga practitioner ay itinuturing ang mga modelo na may mas mababa sa 30 bilyong parameter bilang "maliit," na may ilang mga mapagkukunan na nagtatakda ng threshold na mas mababa pa sa 10 bilyong parameter.

### Mga Pangunahing Bentahe ng SLMs

Ang SLMs ay nag-aalok ng ilang mga pangunahing bentahe na ginagawang perpekto ang mga ito para sa mga aplikasyon ng edge computing:

**Kahusayan sa Operasyon**: Ang SLMs ay nagbibigay ng mas mabilis na oras ng inference dahil sa mas kaunting mga parameter na kailangang iproseso, na ginagawang perpekto ang mga ito para sa mga real-time na aplikasyon. Nangangailangan sila ng mas mababang computational resources, na nagbibigay-daan sa pag-deploy sa mga device na may limitadong mapagkukunan habang kumokonsumo ng mas kaunting enerhiya at pinapanatili ang mas mababang carbon footprint.

**Kakayahang Mag-deploy**: Ang mga modelong ito ay nagbibigay-daan sa on-device AI capabilities nang walang pangangailangan sa koneksyon sa internet, pinapahusay ang privacy at seguridad sa pamamagitan ng lokal na pagproseso, maaaring i-customize para sa mga aplikasyon na partikular sa domain, at angkop para sa iba't ibang edge computing environment.

**Pagiging Cost-Effective**: Ang SLMs ay nag-aalok ng cost-effective na training at deployment kumpara sa LLMs, na may mas mababang operational costs at mas mababang bandwidth requirements para sa mga edge application.

## Mga Advanced na Estratehiya sa Pagkuha ng Modelo

### Hugging Face Ecosystem

Ang Hugging Face ay nagsisilbing pangunahing hub para sa pagtuklas at pag-access sa mga state-of-the-art na SLMs. Ang platform ay nagbibigay ng komprehensibong resources para sa pagtuklas at pag-deploy ng modelo:

**Mga Tampok sa Pagtuklas ng Modelo**: Ang platform ay nag-aalok ng advanced na pag-filter batay sa bilang ng parameter, uri ng lisensya, at mga sukatan ng performance. Maaaring ma-access ng mga user ang mga tool sa side-by-side na paghahambing ng modelo, mga real-time na benchmark ng performance at resulta ng pagsusuri, at mga WebGPU demo para sa agarang pagsubok.

**Mga Curated na Koleksyon ng SLM**: Kasama sa mga sikat na modelo ang Phi-4-mini-3.8B para sa mga advanced na gawain sa pangangatwiran, serye ng Qwen3 (0.6B/1.7B/4B) para sa mga multilingual na aplikasyon, Google Gemma3 para sa mahusay na mga pangkalahatang gawain, at mga experimental na modelo tulad ng BitNET para sa ultra-low precision deployment. Itinatampok din ng platform ang mga koleksyon na pinamamahalaan ng komunidad na may mga espesyal na modelo para sa mga partikular na domain at mga pre-trained at instruction-tuned na variant na na-optimize para sa iba't ibang kaso ng paggamit.

### Azure AI Foundry Model Catalog

Ang Azure AI Foundry Model Catalog ay nagbibigay ng enterprise-grade na access sa SLMs na may pinahusay na kakayahan sa integrasyon:

**Integrasyon ng Enterprise**: Kasama sa catalog ang mga modelong direktang ibinebenta ng Azure na may enterprise-grade na suporta at SLAs, na nagtatampok ng Phi-4-mini-3.8B para sa mga advanced na kakayahan sa pangangatwiran at Llama 3-8B para sa production deployment. Kasama rin dito ang mga modelo tulad ng Qwen3 8B mula sa mga pinagkakatiwalaang third-party na open source na modelo.

**Mga Benepisyo ng Enterprise**: Ang mga built-in na tool para sa fine-tuning, observability, at responsible AI ay isinama sa fungible Provisioned Throughput sa mga pamilya ng modelo. Ang direktang suporta ng Microsoft na may enterprise SLAs, mga integrated na tampok sa seguridad at pagsunod, at komprehensibong mga workflow sa pag-deploy ay nagpapahusay sa karanasan ng enterprise.

## Mga Advanced na Teknik sa Quantization at Optimization

### Llama.cpp Optimization Framework

Ang Llama.cpp ay nagbibigay ng mga cutting-edge na teknik sa quantization para sa maximum na kahusayan sa edge deployment:

**Mga Paraan ng Quantization**: Sinusuportahan ng framework ang iba't ibang antas ng quantization kabilang ang Q4_0 (4-bit quantization na may mahusay na size reduction - perpekto para sa Qwen3-0.6B mobile deployment), Q5_1 (5-bit quantization na nagbabalanse ng kalidad at compression - angkop para sa Phi-4-mini-3.8B edge inference), at Q8_0 (8-bit quantization para sa halos orihinal na kalidad - inirerekomenda para sa Google Gemma3 production use). Ang BitNET ay kumakatawan sa cutting edge na may 1-bit quantization para sa mga extreme compression scenario.

**Mga Benepisyo ng Implementasyon**: Ang CPU-optimized inference na may SIMD acceleration ay nagbibigay ng memory-efficient na pag-load at pag-execute ng modelo. Ang cross-platform compatibility sa x86, ARM, at Apple Silicon architectures ay nagbibigay-daan sa hardware-agnostic na kakayahan sa pag-deploy.

**Halimbawa ng Praktikal na Implementasyon**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Paghahambing ng Memory Footprint**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive Optimization Suite

Ang Microsoft Olive ay nag-aalok ng komprehensibong mga workflow sa pag-optimize ng modelo na idinisenyo para sa mga production environment:

**Mga Teknik sa Optimization**: Kasama sa suite ang dynamic quantization para sa awtomatikong pagpili ng precision (partikular na epektibo sa mga modelo ng Qwen3 series), graph optimization at operator fusion (na-optimize para sa Google Gemma3 architecture), hardware-specific optimizations para sa CPU, GPU, at NPU (na may espesyal na suporta para sa Phi-4-mini-3.8B sa mga ARM device), at multi-stage optimization pipelines. Ang mga modelo ng BitNET ay nangangailangan ng mga espesyal na workflow sa 1-bit quantization sa loob ng Olive framework.

**Automation ng Workflow**: Ang awtomatikong benchmarking sa mga variant ng optimization ay nagsisiguro ng preservation ng quality metrics sa panahon ng optimization. Ang integrasyon sa mga sikat na ML framework tulad ng PyTorch at ONNX ay nagbibigay ng cloud at edge deployment optimization capabilities.

**Halimbawa ng Praktikal na Implementasyon**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX Framework

Ang Apple MLX ay nagbibigay ng native optimization na partikular na idinisenyo para sa mga Apple Silicon device:

**Optimization para sa Apple Silicon**: Ang framework ay gumagamit ng unified memory architecture na may Metal Performance Shaders integration, awtomatikong mixed precision inference (partikular na epektibo sa Google Gemma3), at na-optimize na memory bandwidth utilization. Ang Phi-4-mini-3.8B ay nagpapakita ng exceptional performance sa M-series chips, habang ang Qwen3-1.7B ay nagbibigay ng optimal na balanse para sa MacBook Air deployments.

**Mga Tampok sa Pag-develop**: Ang suporta sa Python at Swift API na may NumPy-compatible na mga operasyon ng array, mga kakayahan sa awtomatikong differentiation, at seamless integration sa mga Apple development tools ay nagbibigay ng komprehensibong development environment.

**Halimbawa ng Praktikal na Implementasyon**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Mga Estratehiya sa Production Deployment at Inference

### Ollama: Pinadaling Lokal na Deployment

Ang Ollama ay nagpapadali sa deployment ng SLM na may mga enterprise-ready na tampok para sa lokal at edge environment:

**Mga Kakayahan sa Deployment**: Isang-command na pag-install at pag-execute ng modelo na may awtomatikong pag-pull at pag-cache ng modelo. Suporta para sa Phi-4-mini-3.8B, buong serye ng Qwen3 (0.6B/1.7B/4B), at Google Gemma3 na may REST API para sa application integration at multi-model management at switching capabilities. Ang mga modelo ng BitNET ay nangangailangan ng experimental build configurations para sa 1-bit quantization support.

**Mga Advanced na Tampok**: Suporta sa custom na fine-tuning ng modelo, pagbuo ng Dockerfile para sa containerized deployment, GPU acceleration na may awtomatikong detection, at mga opsyon sa model quantization at optimization na nagbibigay ng komprehensibong flexibility sa deployment.

### VLLM: Mataas na Performance na Inference

Ang VLLM ay naghahatid ng production-grade na inference optimization para sa mga high-throughput na scenario:

**Mga Optimization sa Performance**: Ang PagedAttention para sa memory-efficient na computation ng attention (partikular na kapaki-pakinabang para sa transformer architecture ng Phi-4-mini-3.8B), dynamic batching para sa throughput optimization (na-optimize para sa parallel processing ng Qwen3 series), tensor parallelism para sa multi-GPU scaling (suporta para sa Google Gemma3), at speculative decoding para sa latency reduction. Ang mga modelo ng BitNET ay nangangailangan ng mga espesyal na inference kernel para sa 1-bit operations.

**Integrasyon ng Enterprise**: Ang mga OpenAI-compatible na API endpoint, suporta sa Kubernetes deployment, integrasyon sa monitoring at observability, at auto-scaling capabilities ay nagbibigay ng mga enterprise-grade na solusyon sa deployment.

### Foundry Local: Solusyon ng Microsoft para sa Edge

Ang Foundry Local ay nagbibigay ng komprehensibong kakayahan sa edge deployment para sa mga enterprise environment:

**Mga Tampok sa Edge Computing**: Offline-first na disenyo ng arkitektura na may optimization sa resource constraint, lokal na pamamahala ng model registry, at mga kakayahan sa edge-to-cloud synchronization na nagsisiguro ng maaasahang edge deployment.

**Seguridad at Pagsunod**: Lokal na pagproseso ng data para sa privacy preservation, mga kontrol sa seguridad ng enterprise, audit logging at compliance reporting, at role-based access management na nagbibigay ng komprehensibong seguridad para sa mga edge deployment.

## Mga Best Practice para sa Implementasyon ng SLM

### Mga Alituntunin sa Pagpili ng Modelo

Kapag pumipili ng SLMs para sa edge deployment, isaalang-alang ang mga sumusunod na salik:

**Mga Pagsasaalang-alang sa Parameter Count**: Pumili ng micro SLMs tulad ng Qwen3-0.6B para sa ultra-lightweight na mga mobile application, small SLMs tulad ng Qwen3-1.7B o Google Gemma3 para sa mga balanse na performance scenario, at medium SLMs tulad ng Phi-4-mini-3.8B o Qwen3-4B kapag papalapit sa kakayahan ng LLM habang pinapanatili ang kahusayan. Ang mga modelo ng BitNET ay nag-aalok ng experimental ultra-compression para sa mga partikular na aplikasyon sa pananaliksik.

**Pag-align ng Kaso ng Paggamit**: Itugma ang mga kakayahan ng modelo sa mga partikular na kinakailangan ng aplikasyon, isinasaalang-alang ang mga salik tulad ng kalidad ng tugon, bilis ng inference, mga limitasyon sa memorya, at mga kinakailangan sa offline na operasyon.

### Pagpili ng Estratehiya sa Optimization

**Diskarte sa Quantization**: Pumili ng naaangkop na antas ng quantization batay sa mga kinakailangan sa kalidad at mga limitasyon sa hardware. Isaalang-alang ang Q4_0 para sa maximum na compression (perpekto para sa Qwen3-0.6B mobile deployment), Q5_1 para sa balanseng trade-off sa kalidad-compression (angkop para sa Phi-4-mini-3.8B at Google Gemma3), at Q8_0 para sa halos orihinal na preservation ng kalidad (inirerekomenda para sa Qwen3-4B production environment). Ang 1-bit quantization ng BitNET ay kumakatawan sa extreme compression frontier para sa mga espesyal na aplikasyon.

**Pagpili ng Framework**: Pumili ng mga optimization framework batay sa target na hardware at mga kinakailangan sa deployment. Gamitin ang Llama.cpp para sa CPU-optimized na deployment, Microsoft Olive para sa komprehensibong mga workflow sa optimization, at Apple MLX para sa mga Apple Silicon device.

## Mga Praktikal na Halimbawa ng Modelo at Kaso ng Paggamit

### Mga Real-World Deployment Scenario

**Mga Mobile Application**: Ang Qwen3-0.6B ay mahusay sa mga smartphone chatbot application na may minimal na memory footprint, habang ang Google Gemma3 ay nagbibigay ng balanseng performance para sa mga tablet-based na educational tools. Ang Phi-4-mini-3.8B ay nag-aalok ng superior reasoning capabilities para sa mga mobile productivity application.

**Desktop at Edge Computing**: Ang Qwen3-1.7B ay naghahatid ng optimal na performance para sa mga desktop assistant application, ang Phi-4-mini-3.8B ay nagbibigay ng advanced na kakayahan sa code generation para sa mga developer tools, at ang Qwen3-4B ay nagbibigay-daan sa sopistikadong pagsusuri ng dokumento sa mga workstation environment.

**Pananaliksik at Eksperimento**: Ang mga modelo ng BitNET ay nagbibigay-daan sa paggalugad ng ultra-low precision inference para sa akademikong pananaliksik at mga proof-of-concept application na nangangailangan ng extreme resource constraints.

### Mga Benchmark sa Performance at Paghahambing

**Bilis ng Inference**: Ang Qwen3-0.6B ay nakakamit ng pinakamabilis na oras ng inference sa mga mobile CPU, ang Google Gemma3 ay nagbibigay ng balanseng speed-quality ratio para sa mga pangkalahatang aplikasyon, ang Phi-4-mini-3.8B ay nag-aalok ng superior reasoning speed para sa mga kumplikadong gawain, at ang BitNET ay naghahatid ng theoretical maximum throughput na may specialized hardware.

**Mga Kinakailangan sa Memorya**: Ang mga memory footprint ng modelo ay nag-iiba mula sa Qwen3-0.6B (mas mababa sa 1GB na quantized) hanggang sa Phi-4-mini-3.8B (humigit-kumulang 3-4GB na quantized), na may BitNET na nakakamit ng sub-500MB na footprint sa mga experimental configuration.

## Mga Hamon at Pagsasaalang-alang

### Mga Trade-off sa Performance

Ang deployment ng SLM ay nangangailangan ng maingat na pagsasaalang-alang sa mga trade-off sa pagitan ng laki ng modelo, bilis ng inference, at kalidad ng output. Halimbawa, habang ang Qwen3-0.6B ay nag-aalok ng exceptional na bilis at kahusayan, ang Phi-4-mini-3.8B ay nagbibigay ng superior reasoning capabilities kapalit ng mas mataas na mga kinakailangan sa mapagkukunan. Ang Google Gemma3 ay nagbibigay ng balanseng opsyon na angkop para sa karamihan ng mga pangkalahatang aplikasyon.

### Compatibility ng Hardware

Ang iba't ibang edge device ay may iba't ibang kakayahan at limitasyon. Ang Qwen3-0.6B ay tumatakbo nang mahusay sa mga basic ARM processor, ang Google Gemma3 ay nangangailangan ng katamtamang computational resources, at ang Phi-4-mini-3.8B ay nakikinabang sa mas mataas na-end na edge hardware. Ang mga modelo ng BitNET ay nangangailangan ng specialized hardware o software implementations para sa optimal na 1-bit operations.

### Seguridad at Privacy

Habang ang SLMs ay nagbibigay-daan sa lokal na pagproseso para sa pinahusay na privacy, ang tamang mga hakbang sa seguridad ay dapat ipatupad upang protektahan ang mga modelo at data sa mga edge environment. Ito ay partikular

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.