<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T15:00:20+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "tl"
}
-->
# Containerized Cloud Deployment - Mga Solusyon para sa Produksyon

Ang komprehensibong tutorial na ito ay tumatalakay sa tatlong pangunahing paraan ng pag-deploy ng Phi-4-mini-instruct model ng Microsoft sa mga containerized na kapaligiran: vLLM, Ollama, at SLM Engine gamit ang ONNX Runtime. Ang modelong ito na may 3.8B parameter ay isang optimal na pagpipilian para sa mga reasoning task habang pinapanatili ang kahusayan para sa edge deployment.

## Talaan ng Nilalaman

1. [Panimula sa Phi-4-mini Container Deployment](../../../Module03)
2. [Mga Layunin sa Pag-aaral](../../../Module03)
3. [Pag-unawa sa Phi-4-mini Classification](../../../Module03)
4. [vLLM Container Deployment](../../../Module03)
5. [Ollama Container Deployment](../../../Module03)
6. [SLM Engine gamit ang ONNX Runtime](../../../Module03)
7. [Framework ng Paghahambing](../../../Module03)
8. [Mga Pinakamahusay na Kasanayan](../../../Module03)

## Panimula sa Phi-4-mini Container Deployment

Ang Small Language Models (SLMs) ay kumakatawan sa mahalagang pag-unlad sa EdgeAI, na nagbibigay-daan sa mga advanced na kakayahan sa natural language processing sa mga device na may limitadong resources. Ang tutorial na ito ay nakatuon sa mga estratehiya ng containerized deployment para sa Phi-4-mini-instruct ng Microsoft, isang makabagong reasoning model na nagbabalanse ng kakayahan at kahusayan.

### Itinatampok na Modelo: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B parameters)**: Ang pinakabagong lightweight instruction-tuned model ng Microsoft na idinisenyo para sa mga kapaligirang may limitadong memorya/compute na may pambihirang kakayahan sa:
- **Mathematical reasoning at mga komplikadong kalkulasyon**
- **Code generation, debugging, at analysis**
- **Logical problem solving at step-by-step reasoning**
- **Mga aplikasyon sa edukasyon na nangangailangan ng detalyadong paliwanag**
- **Function calling at tool integration**

Bahagi ng kategoryang "Small SLMs" (1.5B - 13.9B parameters), ang Phi-4-mini ay nag-aalok ng optimal na balanse sa pagitan ng kakayahan sa reasoning at kahusayan sa resources.

### Mga Benepisyo ng Containerized Phi-4-mini Deployment

- **Operational Efficiency**: Mabilis na inference para sa mga reasoning task na may mas mababang computational requirements
- **Deployment Flexibility**: On-device AI capabilities na may pinahusay na privacy sa pamamagitan ng lokal na pagproseso
- **Cost Effectiveness**: Mas mababang operational costs kumpara sa mas malalaking modelo habang pinapanatili ang kalidad
- **Isolation**: Malinis na paghihiwalay sa pagitan ng mga model instance at secure na execution environments
- **Scalability**: Madaling horizontal scaling para sa mas mataas na throughput ng reasoning

## Mga Layunin sa Pag-aaral

Sa pagtatapos ng tutorial na ito, magagawa mong:

- I-deploy at i-optimize ang Phi-4-mini-instruct sa iba't ibang containerized na kapaligiran
- Magpatupad ng advanced na quantization at compression strategies para sa iba't ibang deployment scenarios
- I-configure ang production-ready container orchestration para sa mga reasoning workload
- Suriin at piliin ang angkop na deployment frameworks batay sa mga partikular na pangangailangan ng use case
- Mag-apply ng security, monitoring, at scaling best practices para sa containerized SLM deployments

## Pag-unawa sa Phi-4-mini Classification

### Mga Detalye ng Modelo

**Teknikal na Detalye:**
- **Parameters**: 3.8 bilyon (Small SLM category)
- **Arkitektura**: Dense decoder-only Transformer na may grouped-query attention
- **Context Length**: 128K tokens (32K inirerekomenda para sa optimal na performance)
- **Vocabulary**: 200K tokens na may multilingual support
- **Training Data**: 5T tokens ng mataas na kalidad na reasoning-dense content

### Mga Pangangailangan sa Resources

| Uri ng Deployment | Min RAM | Inirerekomendang RAM | VRAM (GPU) | Storage | Karaniwang Gamit |
|-------------------|---------|---------------------|------------|---------|------------------|
| **Development** | 6GB | 8GB | - | 8GB | Lokal na testing, prototyping |
| **Production CPU** | 8GB | 12GB | - | 10GB | Edge servers, cost-optimized deployment |
| **Production GPU** | 6GB | 8GB | 4-6GB | 8GB | High-throughput reasoning services |
| **Edge Optimized** | 4GB | 6GB | - | 6GB | Quantized deployment, IoT gateways |

### Mga Kakayahan ng Phi-4-mini

- **Mathematical Excellence**: Advanced arithmetic, algebra, at calculus problem solving
- **Code Intelligence**: Python, JavaScript, at multi-language code generation na may debugging
- **Logical Reasoning**: Step-by-step na pag-decompose ng problema at pagbuo ng solusyon
- **Educational Support**: Detalyadong paliwanag na angkop para sa pag-aaral at pagtuturo
- **Function Calling**: Native na suporta para sa tool integration at API interactions

## vLLM Container Deployment

Ang vLLM ay nagbibigay ng mahusay na suporta para sa Phi-4-mini-instruct na may optimized inference performance at OpenAI-compatible APIs, na ginagawang ideal para sa production reasoning services.

### Mga Halimbawa ng Mabilisang Pagsisimula

#### Basic CPU Deployment (Development)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-Accelerated Production Deployment
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Production Configuration

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pagsubok sa Kakayahan ng Phi-4-mini Reasoning

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama Container Deployment

Ang Ollama ay nagbibigay ng mahusay na suporta para sa Phi-4-mini-instruct na may pinasimpleng deployment at management, na ginagawang ideal para sa development at balanced production deployments.

### Mabilisang Setup

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Production Configuration

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Model Optimization at Mga Variant

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Mga Halimbawa ng API Usage

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine gamit ang ONNX Runtime

Ang ONNX Runtime ay nagbibigay ng optimal na performance para sa edge deployment ng Phi-4-mini-instruct na may advanced optimization at cross-platform compatibility.

### Basic Setup

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Pinasimpleng Server Implementation

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Model Conversion Script

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Production Configuration

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Pagsubok sa ONNX Deployment

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Framework ng Paghahambing

### Paghahambing ng Framework para sa Phi-4-mini

| Feature | vLLM | Ollama | ONNX Runtime |
|---------|------|--------|--------------|
| **Setup Complexity** | Katamtaman | Madali | Kumplikado |
| **Performance (GPU)** | Napakahusay (~25 tok/s) | Napakaganda (~20 tok/s) | Maganda (~15 tok/s) |
| **Performance (CPU)** | Maganda (~8 tok/s) | Napakaganda (~12 tok/s) | Napakahusay (~15 tok/s) |
| **Memory Usage** | 8-12GB | 6-10GB | 4-8GB |
| **API Compatibility** | OpenAI Compatible | Custom REST | Custom FastAPI |
| **Function Calling** | ✅ Native | ✅ Supported | ⚠️ Custom Implementation |
| **Quantization Support** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX Quantization |
| **Production Ready** | ✅ Napakahusay | ✅ Napakaganda | ✅ Maganda |
| **Edge Deployment** | Maganda | Napakahusay | Napakahusay |

## Karagdagang Resources

### Opisyal na Dokumentasyon
- **Microsoft Phi-4 Model Card**: Detalyadong mga detalye at gabay sa paggamit
- **vLLM Documentation**: Advanced na configuration at optimization options
- **Ollama Model Library**: Mga community model at halimbawa ng customization
- **ONNX Runtime Guides**: Mga estratehiya sa performance optimization at deployment

### Mga Tool sa Pag-develop
- **Hugging Face Transformers**: Para sa model interaction at customization
- **OpenAI API Specification**: Para sa vLLM compatibility testing
- **Docker Best Practices**: Mga gabay sa container security at optimization
- **Kubernetes Deployment**: Mga pattern ng orchestration para sa production scaling

### Mga Resources sa Pag-aaral
- **SLM Performance Benchmarking**: Mga metodolohiya sa comparative analysis
- **Edge AI Deployment**: Mga pinakamahusay na kasanayan para sa mga kapaligirang may limitadong resources
- **Reasoning Task Optimization**: Mga estratehiya sa prompting para sa mathematical at logical problems
- **Container Security**: Mga hardening practices para sa AI model deployments

## Mga Resulta sa Pag-aaral

Pagkatapos makumpleto ang module na ito, magagawa mong:

1. I-deploy ang Phi-4-mini-instruct model sa containerized na kapaligiran gamit ang iba't ibang frameworks
2. I-configure at i-optimize ang SLM deployments para sa iba't ibang hardware environments
3. Magpatupad ng mga pinakamahusay na kasanayan sa seguridad para sa containerized AI deployments
4. Ihambing at piliin ang angkop na deployment frameworks batay sa mga partikular na pangangailangan ng use case
5. Mag-apply ng monitoring at scaling strategies para sa production-grade SLM services

## Ano ang susunod

- Bumalik sa [Module 1](../Module01/README.md)
- Bumalik sa [Module 2](../Module02/README.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.