<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-17T13:29:23+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "de"
}
-->
# Abschnitt 3: Microsoft Olive Optimierungssuite

## Inhaltsverzeichnis
1. [Einführung](../../../Module04)
2. [Was ist Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Schnellstartanleitung](../../../Module04)
5. [Beispiel: Qwen3 in ONNX INT4 konvertieren](../../../Module04)
6. [Erweiterte Nutzung](../../../Module04)
7. [Best Practices](../../../Module04)
8. [Fehlerbehebung](../../../Module04)
9. [Zusätzliche Ressourcen](../../../Module04)

## Einführung

Microsoft Olive ist ein leistungsstarkes, benutzerfreundliches Toolkit zur hardwarebewussten Modelloptimierung, das den Prozess der Optimierung von Machine-Learning-Modellen für die Bereitstellung auf verschiedenen Hardwareplattformen vereinfacht. Egal, ob Sie CPUs, GPUs oder spezialisierte KI-Beschleuniger anvisieren – Olive hilft Ihnen, optimale Leistung zu erzielen, während die Modellgenauigkeit erhalten bleibt.

## Was ist Microsoft Olive?

Olive ist ein benutzerfreundliches, hardwarebewusstes Modelloptimierungstool, das branchenführende Techniken für Modellkompression, Optimierung und Kompilierung kombiniert. Es arbeitet mit ONNX Runtime als End-to-End-Lösung für Inferenzoptimierung.

### Hauptmerkmale

- **Hardwarebewusste Optimierung**: Wählt automatisch die besten Optimierungstechniken für Ihre Zielhardware aus
- **Über 40 integrierte Optimierungskomponenten**: Umfasst Modellkompression, Quantisierung, Graphoptimierung und mehr
- **Einfache CLI-Schnittstelle**: Einfache Befehle für gängige Optimierungsaufgaben
- **Unterstützung mehrerer Frameworks**: Funktioniert mit PyTorch, Hugging Face-Modellen und ONNX
- **Unterstützung beliebter Modelle**: Olive kann beliebte Modellarchitekturen wie Llama, Phi, Qwen, Gemma usw. automatisch optimieren

### Vorteile

- **Reduzierte Entwicklungszeit**: Kein manuelles Experimentieren mit verschiedenen Optimierungstechniken erforderlich
- **Leistungssteigerungen**: Signifikante Geschwindigkeitsverbesserungen (bis zu 6x in einigen Fällen)
- **Plattformübergreifende Bereitstellung**: Optimierte Modelle funktionieren auf verschiedenen Hardware- und Betriebssystemen
- **Erhaltene Genauigkeit**: Optimierungen bewahren die Modellqualität und verbessern gleichzeitig die Leistung

## Installation

### Voraussetzungen

- Python 3.8 oder höher
- pip-Paketmanager
- Virtuelle Umgebung (empfohlen)

### Basisinstallation

Erstellen und aktivieren Sie eine virtuelle Umgebung:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installieren Sie Olive mit Auto-Optimierungsfunktionen:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Optionale Abhängigkeiten

Olive bietet verschiedene optionale Abhängigkeiten für zusätzliche Funktionen:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Installation überprüfen

```bash
olive --help
```

Wenn erfolgreich, sollte die Olive-CLI-Hilfenachricht angezeigt werden.

## Schnellstartanleitung

### Ihre erste Optimierung

Optimieren wir ein kleines Sprachmodell mit der Auto-Optimierungsfunktion von Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Was dieser Befehl bewirkt

Der Optimierungsprozess umfasst: Abrufen des Modells aus dem lokalen Cache, Erfassen des ONNX-Graphen und Speichern der Gewichte in einer ONNX-Datendatei, Optimieren des ONNX-Graphen und Quantisieren des Modells auf INT4 mit der RTN-Methode.

### Erklärung der Befehlsparameter

- `--model_name_or_path`: Hugging Face-Modellkennung oder lokaler Pfad
- `--output_path`: Verzeichnis, in dem das optimierte Modell gespeichert wird
- `--device`: Zielgerät (cpu, gpu)
- `--provider`: Ausführungsanbieter (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI für Inferenz verwenden
- `--precision`: Quantisierungspräzision (int4, int8, fp16)
- `--log_level`: Protokollierungsdetails (0=minimal, 1=ausführlich)

## Beispiel: Qwen3 in ONNX INT4 konvertieren

Basierend auf dem bereitgestellten Hugging Face-Beispiel unter [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) können Sie ein Qwen3-Modell wie folgt optimieren:

### Schritt 1: Modell herunterladen (optional)

Um die Downloadzeit zu minimieren, cachen Sie nur die wesentlichen Dateien:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Schritt 2: Qwen3-Modell optimieren

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Schritt 3: Optimiertes Modell testen

Erstellen Sie ein einfaches Python-Skript, um Ihr optimiertes Modell zu testen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur der Ausgabe

Nach der Optimierung enthält Ihr Ausgabeverzeichnis:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Erweiterte Nutzung

### Konfigurationsdateien

Für komplexere Optimierungsabläufe können Sie JSON-Konfigurationsdateien verwenden:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Ausführung mit Konfiguration:

```bash
olive run --config config.json
```

### GPU-Optimierung

Für CUDA-GPU-Optimierung:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Für DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Feinabstimmung mit Olive

Olive unterstützt auch die Feinabstimmung von Modellen:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Modellauswahl
- Beginnen Sie mit kleineren Modellen für Tests (z. B. 0,5B-7B Parameter)
- Stellen Sie sicher, dass Ihre Zielmodellarchitektur von Olive unterstützt wird

### 2. Hardwareüberlegungen
- Passen Sie Ihr Optimierungsziel an Ihre Bereitstellungshardware an
- Verwenden Sie GPU-Optimierung, wenn Sie CUDA-kompatible Hardware haben
- Ziehen Sie DirectML für Windows-Maschinen mit integrierter Grafik in Betracht

### 3. Präzisionsauswahl
- **INT4**: Maximale Kompression, leichte Genauigkeitsverluste
- **INT8**: Gute Balance zwischen Größe und Genauigkeit
- **FP16**: Minimale Genauigkeitsverluste, moderate Größenreduzierung

### 4. Tests und Validierung
- Testen Sie optimierte Modelle immer mit Ihren spezifischen Anwendungsfällen
- Vergleichen Sie Leistungskennzahlen (Latenz, Durchsatz, Genauigkeit)
- Verwenden Sie repräsentative Eingabedaten für die Bewertung

### 5. Iterative Optimierung
- Beginnen Sie mit Auto-Optimierung für schnelle Ergebnisse
- Verwenden Sie Konfigurationsdateien für eine feinere Steuerung
- Experimentieren Sie mit verschiedenen Optimierungsschritten

## Fehlerbehebung

### Häufige Probleme

#### 1. Installationsprobleme
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-Probleme
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Speicherprobleme
- Verwenden Sie kleinere Batchgrößen während der Optimierung
- Versuchen Sie zuerst Quantisierung mit höherer Präzision (int8 statt int4)
- Stellen Sie sicher, dass ausreichend Speicherplatz für das Modell-Caching vorhanden ist

#### 4. Modellladefehler
- Überprüfen Sie den Modellpfad und die Zugriffsberechtigungen
- Prüfen Sie, ob das Modell `trust_remote_code=True` erfordert
- Stellen Sie sicher, dass alle erforderlichen Modelfiles heruntergeladen wurden

### Hilfe erhalten

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Beispiele**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Zusätzliche Ressourcen

### Offizielle Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Beispiel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community-Beispiele
- **Jupyter Notebooks**: Verfügbar im Olive GitHub Repository
- **VS Code Erweiterung**: Die AI Toolkit-Erweiterung verwendet Olive zur Modelloptimierung
- **Blogbeiträge**: Der Microsoft Open Source Blog enthält detaillierte Olive-Tutorials

### Verwandte Tools
- **ONNX Runtime**: Hochleistungsfähige Inferenz-Engine
- **Hugging Face Transformers**: Quelle vieler kompatibler Modelle
- **Azure Machine Learning**: Cloud-basierte Optimierungsabläufe

## ➡️ Was kommt als Nächstes?

- [04: OpenVINO Toolkit Optimierungssuite](./04.openvino.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.