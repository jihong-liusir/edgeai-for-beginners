<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-17T13:25:01+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "de"
}
-->
# Abschnitt 4: OpenVINO Toolkit Optimierungssuite

## Inhaltsverzeichnis
1. [Einführung](../../../Module04)
2. [Was ist OpenVINO?](../../../Module04)
3. [Installation](../../../Module04)
4. [Schnellstartanleitung](../../../Module04)
5. [Beispiel: Modelle mit OpenVINO konvertieren und optimieren](../../../Module04)
6. [Erweiterte Nutzung](../../../Module04)
7. [Best Practices](../../../Module04)
8. [Fehlerbehebung](../../../Module04)
9. [Zusätzliche Ressourcen](../../../Module04)

## Einführung

OpenVINO (Open Visual Inference and Neural Network Optimization) ist Intels Open-Source-Toolkit zur Bereitstellung leistungsstarker KI-Lösungen in Cloud-, On-Premises- und Edge-Umgebungen. Egal, ob Sie CPUs, GPUs, VPUs oder spezialisierte KI-Beschleuniger anvisieren, OpenVINO bietet umfassende Optimierungsmöglichkeiten, ohne die Modellgenauigkeit zu beeinträchtigen, und ermöglicht plattformübergreifende Bereitstellung.

## Was ist OpenVINO?

OpenVINO ist ein Open-Source-Toolkit, das Entwicklern ermöglicht, KI-Modelle effizient zu optimieren, zu konvertieren und auf verschiedenen Hardwareplattformen bereitzustellen. Es besteht aus drei Hauptkomponenten: OpenVINO Runtime für Inferenz, Neural Network Compression Framework (NNCF) für Modelloptimierung und OpenVINO Model Server für skalierbare Bereitstellung.

### Hauptmerkmale

- **Plattformübergreifende Bereitstellung**: Unterstützt Linux, Windows und macOS mit Python-, C++- und C-APIs
- **Hardware-Beschleunigung**: Automatische Geräteerkennung und Optimierung für CPU, GPU, VPU und KI-Beschleuniger
- **Modellkompressions-Framework**: Fortschrittliche Quantisierungs-, Pruning- und Optimierungstechniken durch NNCF
- **Framework-Kompatibilität**: Direkte Unterstützung für TensorFlow-, ONNX-, PaddlePaddle- und PyTorch-Modelle
- **Generative KI-Unterstützung**: Spezialisierte OpenVINO GenAI für die Bereitstellung großer Sprachmodelle und generativer KI-Anwendungen

### Vorteile

- **Leistungsoptimierung**: Deutliche Geschwindigkeitsverbesserungen bei minimalem Genauigkeitsverlust
- **Reduzierter Bereitstellungsaufwand**: Minimale externe Abhängigkeiten vereinfachen Installation und Bereitstellung
- **Verbesserte Startzeit**: Optimiertes Modell-Laden und Caching für schnellere Anwendungsinitialisierung
- **Skalierbare Bereitstellung**: Von Edge-Geräten bis hin zu Cloud-Infrastrukturen mit konsistenten APIs
- **Produktionsreife**: Unternehmensgerechte Zuverlässigkeit mit umfassender Dokumentation und Community-Support

## Installation

### Voraussetzungen

- Python 3.8 oder höher
- pip-Paketmanager
- Virtuelle Umgebung (empfohlen)
- Kompatible Hardware (Intel-CPUs empfohlen, unterstützt jedoch verschiedene Architekturen)

### Basisinstallation

Erstellen und aktivieren Sie eine virtuelle Umgebung:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Installieren Sie OpenVINO Runtime:

```bash
pip install openvino
```

Installieren Sie NNCF für die Modelloptimierung:

```bash
pip install nncf
```

### OpenVINO GenAI Installation

Für generative KI-Anwendungen:

```bash
pip install openvino-genai
```

### Optionale Abhängigkeiten

Zusätzliche Pakete für spezifische Anwendungsfälle:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Installation überprüfen

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Wenn erfolgreich, sollten Sie die OpenVINO-Versionsinformationen sehen.

## Schnellstartanleitung

### Ihr erstes Modell optimieren

Lassen Sie uns ein Hugging Face-Modell mit OpenVINO konvertieren und optimieren:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Was dieser Prozess macht

Der Optimierungsworkflow umfasst: Laden des ursprünglichen Modells von Hugging Face, Konvertieren in das OpenVINO Intermediate Representation (IR)-Format, Anwenden von Standardoptimierungen und Kompilieren für die Zielhardware.

### Wichtige Parameter erklärt

- `export=True`: Konvertiert das Modell in das OpenVINO IR-Format
- `compile=False`: Verzögert die Kompilierung bis zur Laufzeit für mehr Flexibilität
- `device`: Zielhardware ("CPU", "GPU", "AUTO" für automatische Auswahl)
- `save_pretrained()`: Speichert das optimierte Modell zur Wiederverwendung

## Beispiel: Modelle mit OpenVINO konvertieren und optimieren

### Schritt 1: Modellkonvertierung mit NNCF-Quantisierung

So wenden Sie die Quantisierung nach dem Training mit NNCF an:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Schritt 2: Erweiterte Optimierung mit Gewichtskompression

Für transformerbasierte Modelle Gewichtskompression anwenden:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Schritt 3: Inferenz mit optimiertem Modell

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Struktur der Ausgabe

Nach der Optimierung enthält Ihr Modellverzeichnis:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Erweiterte Nutzung

### Konfiguration mit NNCF YAML

Für komplexe Optimierungsworkflows verwenden Sie NNCF-Konfigurationsdateien:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Konfiguration anwenden:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-Optimierung

Für GPU-Beschleunigung:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Batch-Verarbeitungsoptimierung

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Modellserver-Bereitstellung

Optimierte Modelle mit OpenVINO Model Server bereitstellen:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Client-Code für den Modellserver:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Best Practices

### 1. Modellauswahl und -vorbereitung
- Verwenden Sie Modelle aus unterstützten Frameworks (PyTorch, TensorFlow, ONNX)
- Stellen Sie sicher, dass die Modell-Eingaben feste oder bekannte dynamische Formen haben
- Testen Sie mit repräsentativen Datensätzen zur Kalibrierung

### 2. Auswahl der Optimierungsstrategie
- **Quantisierung nach dem Training**: Beginnen Sie hier für schnelle Optimierung
- **Gewichtskompression**: Ideal für große Sprachmodelle und Transformer
- **Quantisierungsbewusstes Training**: Verwenden Sie dies, wenn Genauigkeit entscheidend ist

### 3. Hardware-spezifische Optimierung
- **CPU**: Verwenden Sie INT8-Quantisierung für ausgewogene Leistung
- **GPU**: Nutzen Sie FP16-Präzision und Batch-Verarbeitung
- **VPU**: Konzentrieren Sie sich auf Modellvereinfachung und Layer-Fusion

### 4. Leistungstuning
- **Durchsatzmodus**: Für hochvolumige Batch-Verarbeitung
- **Latenzmodus**: Für Echtzeit-Interaktive Anwendungen
- **AUTO-Gerät**: Lassen Sie OpenVINO die optimale Hardware auswählen

### 5. Speicherverwaltung
- Verwenden Sie dynamische Formen mit Bedacht, um Speicherüberlastung zu vermeiden
- Implementieren Sie Modell-Caching für schnellere nachfolgende Ladevorgänge
- Überwachen Sie die Speichernutzung während der Optimierung

### 6. Genauigkeitsvalidierung
- Validieren Sie optimierte Modelle immer gegenüber der ursprünglichen Leistung
- Verwenden Sie repräsentative Testdatensätze zur Bewertung
- Ziehen Sie schrittweise Optimierung in Betracht (beginnen Sie mit konservativen Einstellungen)

## Fehlerbehebung

### Häufige Probleme

#### 1. Installationsprobleme
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Modellkonvertierungsfehler
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Leistungsprobleme
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Speicherprobleme
- Reduzieren Sie die Modell-Batchgröße während der Optimierung
- Verwenden Sie Streaming für große Datensätze
- Aktivieren Sie Modell-Caching: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Genauigkeitsverlust
- Verwenden Sie höhere Präzision (INT8 statt INT4)
- Erhöhen Sie die Größe des Kalibrierungsdatensatzes
- Wenden Sie gemischte Präzisionsoptimierung an

### Leistungsüberwachung

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Hilfe erhalten

- **Dokumentation**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community-Forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Zusätzliche Ressourcen

### Offizielle Links
- **OpenVINO Homepage**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Lernressourcen
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Schnellstartanleitung**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimierungsleitfaden**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integrationstools
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Leistungsbenchmarks
- **Offizielle Benchmarks**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Community-Beispiele
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Umfassende Tutorials im OpenVINO Notebooks Repository
- **Beispielanwendungen**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Praxisnahe Beispiele für verschiedene Bereiche (Computer Vision, NLP, Audio)
- **Blogbeiträge**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI und Community-Blogbeiträge mit detaillierten Anwendungsfällen

### Verwandte Tools
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Zusätzliche Optimierungstechniken für Intel-Hardware
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Für mobile und Edge-Bereitstellungsvergleiche
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternativen für plattformübergreifende Inferenz-Engines

## ➡️ Was kommt als Nächstes?

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.