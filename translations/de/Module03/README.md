<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6cf75ae5b01949656a3ad41425c7ffe4",
  "translation_date": "2025-09-17T13:44:36+00:00",
  "source_file": "Module03/README.md",
  "language_code": "de"
}
-->
# Kapitel 03: Bereitstellung von Small Language Models (SLMs)

Dieses umfassende Kapitel beleuchtet den gesamten Lebenszyklus der Bereitstellung von Small Language Models (SLMs), einschließlich theoretischer Grundlagen, praktischer Implementierungsstrategien und produktionsreifer containerisierter Lösungen. Das Kapitel ist in drei aufeinander aufbauende Abschnitte gegliedert, die die Leser von grundlegenden Konzepten bis hin zu fortgeschrittenen Bereitstellungsszenarien führen.

## Kapitelstruktur und Lernreise

### **[Abschnitt 1: Fortgeschrittenes Lernen mit SLMs - Grundlagen und Optimierung](./01.SLMAdvancedLearning.md)**
Der erste Abschnitt legt die theoretischen Grundlagen für das Verständnis von Small Language Models und deren strategische Bedeutung in Edge-AI-Bereitstellungen. Dieser Abschnitt behandelt:

- **Parameterklassifizierungsrahmen**: Detaillierte Untersuchung der SLM-Kategorien von Micro SLMs (100M-1,4B Parameter) bis hin zu Medium SLMs (14B-30B Parameter), mit besonderem Fokus auf Modelle wie Phi-4-mini-3.8B, Qwen3-Serie und Google Gemma3, einschließlich Hardwareanforderungen und Speicherbedarfsanalyse für jede Modellklasse
- **Fortgeschrittene Optimierungstechniken**: Umfassende Darstellung von Quantisierungsmethoden mit Llama.cpp, Microsoft Olive und Apple MLX Frameworks, einschließlich modernster BitNET 1-Bit-Quantisierung mit praktischen Codebeispielen, die Quantisierungspipelines und Benchmarking-Ergebnisse zeigen
- **Strategien zur Modellbeschaffung**: Tiefgehende Analyse des Hugging Face-Ökosystems und des Azure AI Foundry Model Catalog für unternehmensgerechte SLM-Bereitstellungen, mit Codebeispielen für programmgesteuertes Herunterladen, Validieren und Konvertieren von Modellen
- **Entwickler-APIs**: Codebeispiele in Python, C++ und C#, die zeigen, wie Modelle geladen, Inferenz durchgeführt und mit beliebten Frameworks wie PyTorch, TensorFlow und ONNX Runtime integriert werden können

Dieser grundlegende Abschnitt betont die Balance zwischen Betriebseffizienz, Bereitstellungsflexibilität und Kosteneffektivität, die SLMs ideal für Edge-Computing-Szenarien macht, mit praktischen Codebeispielen, die Entwickler direkt in ihren Projekten umsetzen können.

### **[Abschnitt 2: Lokale Bereitstellung - Datenschutzorientierte Lösungen](./02.DeployingSLMinLocalEnv.md)**
Der zweite Abschnitt geht von der Theorie zur praktischen Umsetzung über und konzentriert sich auf lokale Bereitstellungsstrategien, die Datensouveränität und operative Unabhängigkeit priorisieren. Wichtige Themen sind:

- **Ollama Universal Platform**: Umfassende Untersuchung der plattformübergreifenden Bereitstellung mit Schwerpunkt auf entwicklerfreundlichen Workflows, Modelllebenszyklusmanagement und Anpassung durch Modelfiles, einschließlich vollständiger REST-API-Integrationsbeispiele und CLI-Automatisierungsskripte
- **Microsoft Foundry Local**: Unternehmensgerechte Bereitstellungslösungen mit ONNX-basierter Optimierung, Windows ML-Integration und umfassenden Sicherheitsfunktionen, mit C#- und Python-Codebeispielen für native Anwendungsintegration
- **Vergleichsanalyse**: Detaillierter Vergleich von Frameworks, einschließlich technischer Architektur, Leistungseigenschaften und Optimierungsrichtlinien für Anwendungsfälle, mit Benchmark-Code zur Bewertung der Inferenzgeschwindigkeit und des Speicherverbrauchs auf verschiedenen Hardwareplattformen
- **API-Integration**: Beispielanwendungen, die zeigen, wie Webservices, Chat-Anwendungen und Datenverarbeitungspipelines mit lokalen SLM-Bereitstellungen erstellt werden können, mit Codebeispielen in Node.js, Python Flask/FastAPI und ASP.NET Core
- **Testframeworks**: Automatisierte Testansätze für die Qualitätssicherung von Modellen, einschließlich Unit- und Integrationstestbeispielen für SLM-Implementierungen

Dieser Abschnitt bietet praktische Anleitungen für Organisationen, die datenschutzfreundliche KI-Lösungen implementieren möchten, während sie die volle Kontrolle über ihre Bereitstellungsumgebung behalten, mit einsatzbereiten Codebeispielen, die Entwickler an ihre spezifischen Anforderungen anpassen können.

### **[Abschnitt 3: Containerisierte Cloud-Bereitstellung - Lösungen im Produktionsmaßstab](./03.DeployingSLMinCloud.md)**
Der letzte Abschnitt gipfelt in fortgeschrittenen containerisierten Bereitstellungsstrategien, wobei Microsofts Phi-4-mini-instruct als Hauptfallstudie dient. Dieser Abschnitt behandelt:

- **vLLM-Bereitstellung**: Hochleistungsfähige Inferenzoptimierung mit OpenAI-kompatiblen APIs, fortschrittlicher GPU-Beschleunigung und produktionsgerechter Konfiguration, einschließlich vollständiger Dockerfiles, Kubernetes-Manifeste und Parameter für Leistungstuning
- **Ollama Container Orchestration**: Vereinfachte Bereitstellungsworkflows mit Docker Compose, Modelloptimierungsvarianten und Web-UI-Integration, mit CI/CD-Pipeline-Beispielen für automatisierte Bereitstellung und Tests
- **ONNX Runtime Implementierung**: Edge-optimierte Bereitstellung mit umfassender Modellkonvertierung, Quantisierungsstrategien und plattformübergreifender Kompatibilität, einschließlich detaillierter Codebeispiele für Modelloptimierung und Bereitstellung
- **Überwachung & Beobachtbarkeit**: Implementierung von Prometheus/Grafana-Dashboards mit benutzerdefinierten Metriken zur Leistungsüberwachung von SLMs, einschließlich Konfigurationen für Warnungen und Log-Aggregation
- **Lastverteilung & Skalierung**: Praktische Beispiele für horizontale und vertikale Skalierungsstrategien mit Autoscaling-Konfigurationen basierend auf CPU/GPU-Auslastung und Anforderungsmustern
- **Sicherheitsverstärkung**: Best Practices für Containersicherheit, einschließlich Privilegienreduktion, Netzwerkrichtlinien und Geheimnisverwaltung für API-Schlüssel und Modellzugriffsberechtigungen

Jeder Bereitstellungsansatz wird mit vollständigen Konfigurationsbeispielen, Testverfahren, Produktionsbereitschafts-Checklisten und Infrastructure-as-Code-Vorlagen präsentiert, die Entwickler direkt in ihre Bereitstellungsworkflows integrieren können.

## Wichtige Lernergebnisse

Nach Abschluss dieses Kapitels werden die Leser folgende Fähigkeiten beherrschen:

1. **Strategische Modellauswahl**: Verständnis der Parametergrenzen und Auswahl geeigneter SLMs basierend auf Ressourcenbeschränkungen und Leistungsanforderungen
2. **Optimierungsbeherrschung**: Implementierung fortgeschrittener Quantisierungstechniken über verschiedene Frameworks hinweg, um ein optimales Gleichgewicht zwischen Leistung und Effizienz zu erreichen
3. **Bereitstellungsflexibilität**: Auswahl zwischen lokalen datenschutzorientierten Lösungen und skalierbaren containerisierten Bereitstellungen basierend auf den Bedürfnissen der Organisation
4. **Produktionsbereitschaft**: Konfiguration von Überwachungs-, Sicherheits- und Skalierungssystemen für unternehmensgerechte SLM-Bereitstellungen

## Praktischer Fokus und Anwendungen aus der Praxis

Das Kapitel behält durchgehend eine starke praktische Orientierung bei und bietet:

- **Praxisnahe Beispiele**: Vollständige Konfigurationsdateien, API-Testverfahren und Bereitstellungsskripte
- **Leistungsbenchmarking**: Detaillierte Vergleiche von Inferenzgeschwindigkeit, Speicherverbrauch und Ressourcenanforderungen
- **Sicherheitsüberlegungen**: Sicherheitspraktiken auf Unternehmensniveau, Compliance-Rahmenwerke und Strategien zum Schutz von Daten
- **Best Practices**: Produktionsbewährte Richtlinien für Überwachung, Skalierung und Wartung

## Zukunftsorientierte Perspektive

Das Kapitel schließt mit zukunftsweisenden Einblicken in aufkommende Trends, darunter:

- Fortschrittliche Modellarchitekturen mit verbesserten Effizienzverhältnissen
- Tiefere Hardwareintegration mit spezialisierten KI-Beschleunigern
- Weiterentwicklung des Ökosystems hin zu Standardisierung und Interoperabilität
- Muster der Unternehmensadoption, die durch Datenschutz- und Compliance-Anforderungen vorangetrieben werden

Dieser umfassende Ansatz stellt sicher, dass die Leser sowohl aktuelle Herausforderungen bei der SLM-Bereitstellung als auch zukünftige technologische Entwicklungen meistern können, um fundierte Entscheidungen zu treffen, die mit ihren spezifischen organisatorischen Anforderungen und Einschränkungen übereinstimmen.

Das Kapitel dient sowohl als praktischer Leitfaden für die sofortige Umsetzung als auch als strategische Ressource für die langfristige Planung der KI-Bereitstellung und betont die kritische Balance zwischen Fähigkeit, Effizienz und operativer Exzellenz, die erfolgreiche SLM-Bereitstellungen auszeichnet.

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.