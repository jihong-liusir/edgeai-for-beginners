<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-17T13:50:43+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "de"
}
-->
# Containerisierte Cloud-Bereitstellung - Lösungen im Produktionsmaßstab

Dieses umfassende Tutorial behandelt drei Hauptansätze zur Bereitstellung des Phi-4-mini-instruct-Modells von Microsoft in containerisierten Umgebungen: vLLM, Ollama und SLM Engine mit ONNX Runtime. Dieses Modell mit 3,8 Milliarden Parametern ist eine optimale Wahl für Aufgaben im Bereich des logischen Denkens und bietet gleichzeitig Effizienz für Edge-Bereitstellungen.

## Inhaltsverzeichnis

1. [Einführung in die containerisierte Bereitstellung von Phi-4-mini](../../../Module03)
2. [Lernziele](../../../Module03)
3. [Verständnis der Phi-4-mini-Klassifikation](../../../Module03)
4. [vLLM-Container-Bereitstellung](../../../Module03)
5. [Ollama-Container-Bereitstellung](../../../Module03)
6. [SLM Engine mit ONNX Runtime](../../../Module03)
7. [Vergleichsrahmenwerk](../../../Module03)
8. [Best Practices](../../../Module03)

## Einführung in die containerisierte Bereitstellung von Phi-4-mini

Small Language Models (SLMs) stellen einen entscheidenden Fortschritt in der EdgeAI dar, da sie fortschrittliche Fähigkeiten zur Verarbeitung natürlicher Sprache auf ressourcenbeschränkten Geräten ermöglichen. Dieses Tutorial konzentriert sich auf Strategien zur containerisierten Bereitstellung von Microsofts Phi-4-mini-instruct, einem hochmodernen Modell für logisches Denken, das Leistungsfähigkeit und Effizienz in Einklang bringt.

### Vorgestelltes Modell: Phi-4-mini-instruct

**Phi-4-mini-instruct (3,8 Milliarden Parameter)**: Microsofts neuestes leichtgewichtiges, instruktionstaugliches Modell, das für speicher- und rechenbeschränkte Umgebungen entwickelt wurde und außergewöhnliche Fähigkeiten bietet in:
- **Mathematischem Denken und komplexen Berechnungen**
- **Code-Generierung, Debugging und Analyse**
- **Logischem Problemlösen und schrittweisem Denken**
- **Bildungsanwendungen mit detaillierten Erklärungen**
- **Funktionsaufrufen und Tool-Integration**

Als Teil der Kategorie "Small SLMs" (1,5 bis 13,9 Milliarden Parameter) bietet Phi-4-mini eine optimale Balance zwischen Denkfähigkeit und Ressourceneffizienz.

### Vorteile der containerisierten Bereitstellung von Phi-4-mini

- **Betriebseffizienz**: Schnelle Inferenz für Denkaufgaben mit geringeren Rechenanforderungen
- **Bereitstellungsflexibilität**: KI-Fähigkeiten direkt auf dem Gerät mit verbesserter Privatsphäre durch lokale Verarbeitung
- **Kosteneffizienz**: Reduzierte Betriebskosten im Vergleich zu größeren Modellen bei gleichbleibender Qualität
- **Isolation**: Klare Trennung zwischen Modellinstanzen und sichere Ausführungsumgebungen
- **Skalierbarkeit**: Einfache horizontale Skalierung für erhöhte Verarbeitungskapazität

## Lernziele

Am Ende dieses Tutorials werden Sie in der Lage sein:

- Phi-4-mini-instruct in verschiedenen containerisierten Umgebungen bereitzustellen und zu optimieren
- Fortgeschrittene Quantisierungs- und Komprimierungsstrategien für unterschiedliche Bereitstellungsszenarien umzusetzen
- Produktionsreife Container-Orchestrierung für Denkaufgaben zu konfigurieren
- Geeignete Bereitstellungsframeworks basierend auf spezifischen Anwendungsanforderungen zu bewerten und auszuwählen
- Sicherheits-, Überwachungs- und Skalierungs-Best-Practices für containerisierte SLM-Bereitstellungen anzuwenden

## Verständnis der Phi-4-mini-Klassifikation

### Modellspezifikationen

**Technische Details:**
- **Parameter**: 3,8 Milliarden (Kategorie Small SLM)
- **Architektur**: Dichtes Decoder-Only-Transformer mit gruppierter Abfrageaufmerksamkeit
- **Kontextlänge**: 128K Tokens (32K empfohlen für optimale Leistung)
- **Vokabular**: 200K Tokens mit mehrsprachiger Unterstützung
- **Trainingsdaten**: 5T Tokens hochwertiger, denkintensiver Inhalte

### Ressourcenanforderungen

| Bereitstellungstyp | Min. RAM | Empfohlener RAM | VRAM (GPU) | Speicher | Typische Anwendungsfälle |
|--------------------|----------|-----------------|------------|----------|--------------------------|
| **Entwicklung**    | 6GB      | 8GB             | -          | 8GB      | Lokale Tests, Prototyping |
| **Produktion CPU** | 8GB      | 12GB            | -          | 10GB     | Edge-Server, kostenoptimierte Bereitstellung |
| **Produktion GPU** | 6GB      | 8GB             | 4-6GB      | 8GB      | Hochdurchsatz-Denkdienste |
| **Edge-Optimiert** | 4GB      | 6GB             | -          | 6GB      | Quantisierte Bereitstellung, IoT-Gateways |

### Fähigkeiten von Phi-4-mini

- **Mathematische Exzellenz**: Fortgeschrittene Arithmetik, Algebra und Kalkül-Problemlösung
- **Code-Intelligenz**: Python-, JavaScript- und mehrsprachige Code-Generierung mit Debugging
- **Logisches Denken**: Schrittweise Problemanalyse und Lösungsentwicklung
- **Bildungsunterstützung**: Detaillierte Erklärungen, geeignet für Lern- und Lehrszenarien
- **Funktionsaufrufe**: Native Unterstützung für Tool-Integration und API-Interaktionen

## vLLM-Container-Bereitstellung

vLLM bietet hervorragende Unterstützung für Phi-4-mini-instruct mit optimierter Inferenzleistung und OpenAI-kompatiblen APIs, was es ideal für produktionsreife Denkdienste macht.

### Schnellstart-Beispiele

#### Grundlegende CPU-Bereitstellung (Entwicklung)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-beschleunigte Produktionsbereitstellung
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Produktionskonfiguration

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testen der Denkfähigkeiten von Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama-Container-Bereitstellung

Ollama bietet eine hervorragende Unterstützung für Phi-4-mini-instruct mit vereinfachter Bereitstellung und Verwaltung, was es ideal für Entwicklungs- und ausgewogene Produktionsbereitstellungen macht.

### Schnelle Einrichtung

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Produktionskonfiguration

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Modelloptimierung und Varianten

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API-Nutzungsbeispiele

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine mit ONNX Runtime

ONNX Runtime bietet optimale Leistung für die Edge-Bereitstellung von Phi-4-mini-instruct mit fortschrittlicher Optimierung und plattformübergreifender Kompatibilität.

### Grundlegende Einrichtung

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Vereinfachte Server-Implementierung

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Modellkonvertierungsskript

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Produktionskonfiguration

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testen der ONNX-Bereitstellung

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Vergleichsrahmenwerk

### Vergleich der Frameworks für Phi-4-mini

| Merkmal              | vLLM         | Ollama       | ONNX Runtime   |
|----------------------|--------------|--------------|----------------|
| **Einrichtungsaufwand** | Mittel       | Einfach       | Komplex        |
| **Leistung (GPU)**   | Hervorragend (~25 Tok/s) | Sehr gut (~20 Tok/s) | Gut (~15 Tok/s) |
| **Leistung (CPU)**   | Gut (~8 Tok/s) | Sehr gut (~12 Tok/s) | Hervorragend (~15 Tok/s) |
| **Speichernutzung**  | 8-12GB       | 6-10GB       | 4-8GB          |
| **API-Kompatibilität** | OpenAI-kompatibel | Benutzerdefinierte REST | Benutzerdefinierte FastAPI |
| **Funktionsaufrufe** | ✅ Native     | ✅ Unterstützt | ⚠️ Benutzerdefinierte Implementierung |
| **Quantisierungsunterstützung** | AWQ, GPTQ    | Q4_0, Q5_1, Q8_0 | ONNX-Quantisierung |
| **Produktionsreife** | ✅ Hervorragend | ✅ Sehr gut   | ✅ Gut         |
| **Edge-Bereitstellung** | Gut          | Hervorragend  | Überragend     |

## Zusätzliche Ressourcen

### Offizielle Dokumentation
- **Microsoft Phi-4 Model Card**: Detaillierte Spezifikationen und Nutzungshinweise
- **vLLM-Dokumentation**: Erweiterte Konfigurations- und Optimierungsoptionen
- **Ollama-Modellbibliothek**: Community-Modelle und Anpassungsbeispiele
- **ONNX Runtime Guides**: Leistungsoptimierung und Bereitstellungsstrategien

### Entwicklungstools
- **Hugging Face Transformers**: Für Modellinteraktion und Anpassung
- **OpenAI API-Spezifikation**: Für vLLM-Kompatibilitätstests
- **Docker Best Practices**: Sicherheit und Optimierung von Containern
- **Kubernetes-Bereitstellung**: Orchestrierungsmuster für Produktionsskalierung

### Lernressourcen
- **SLM-Leistungsbenchmarking**: Methoden zur vergleichenden Analyse
- **Edge-AI-Bereitstellung**: Best Practices für ressourcenbeschränkte Umgebungen
- **Optimierung von Denkaufgaben**: Strategien für mathematische und logische Probleme
- **Containersicherheit**: Härtungspraktiken für KI-Modellbereitstellungen

## Lernergebnisse

Nach Abschluss dieses Moduls werden Sie in der Lage sein:

1. Das Phi-4-mini-instruct-Modell in containerisierten Umgebungen mit verschiedenen Frameworks bereitzustellen
2. SLM-Bereitstellungen für unterschiedliche Hardwareumgebungen zu konfigurieren und zu optimieren
3. Sicherheits-Best-Practices für containerisierte KI-Bereitstellungen umzusetzen
4. Geeignete Bereitstellungsframeworks basierend auf spezifischen Anwendungsanforderungen zu vergleichen und auszuwählen
5. Überwachungs- und Skalierungsstrategien für produktionsreife SLM-Dienste anzuwenden

## Was kommt als Nächstes?

- Zurück zu [Modul 1](../Module01/README.md)
- Zurück zu [Modul 2](../Module02/README.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.