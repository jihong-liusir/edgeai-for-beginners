<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-22T12:56:08+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "de"
}
-->
# Sitzung 6: Foundry Local – Modelle als Werkzeuge

## Überblick

Behandeln Sie KI-Modelle als modulare, anpassbare Werkzeuge, die direkt auf dem Gerät mit Foundry Local ausgeführt werden. Diese Sitzung konzentriert sich auf praktische Workflows für datenschutzfreundliche, latenzarme Inferenz und darauf, wie diese Werkzeuge über SDKs, APIs oder CLI integriert werden können. Außerdem lernen Sie, wie Sie bei Bedarf auf Azure AI Foundry skalieren können.

Referenzen:
- Foundry Local-Dokumentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integration mit Inferenz-SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Hugging Face-Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Lernziele
- Modell-als-Werkzeug-Muster auf dem Gerät entwerfen
- Integration über OpenAI-kompatible REST-API oder SDKs
- Modelle an domänenspezifische Anwendungsfälle anpassen
- Planung für hybride Skalierung zu Azure AI Foundry

## Teil 1: Werkzeugabstraktionen (Schritt-für-Schritt)

Ziel: Modelle als Werkzeuge mit klaren Schnittstellen und einem einfachen Router darstellen.

Schritt 1) Werkzeug-Schnittstelle und -Registry definieren  
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```
  
Schritt 2) Zwei Werkzeuge implementieren, die von Foundry Local unterstützt werden  
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```
  
Schritt 3) Router nach Aufgabe  
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```
  

## Teil 2: SDK- und API-Integration (Schritt-für-Schritt)

Ziel: Das OpenAI Python SDK gegen den Foundry Local-Endpunkt verwenden.

Schritt 1) Installation  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```
  
Schritt 2) Umgebungsvariablen konfigurieren  
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```
  
Schritt 3) Die Chat-API aufrufen  
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```
  

## Teil 3: Domänenanpassung (Schritt-für-Schritt)

Ziel: Ausgaben für eine Domäne mithilfe von Prompt-Vorlagen und JSON-Schema anpassen.

Schritt 1) Eine Domänen-Prompt-Vorlage erstellen  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Schritt 2) JSON-Ausgabe erzwingen  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Teil 4: Offline- und Sicherheitsstrategie (Schritt-für-Schritt)

Ziel: Datenschutz und Widerstandsfähigkeit sicherstellen, wenn Modelle lokal als Werkzeuge ausgeführt werden.

Schritt 1) Lokalen Endpunkt vorwärmen und validieren  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Schritt 2) Eingaben bereinigen  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Schritt 3) Nur-lokal-Flag und Logging  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Teil 5: Skalierung zu Azure AI Foundry (Schritt-für-Schritt)

Ziel: Lokale Modelle mit Azure-Endpunkten für Überlaufkapazität spiegeln.

Schritt 1) Routing-Strategie festlegen  
- Lokal zuerst für Datenschutz/Latenz, Azure-Fallback bei Fehlern oder großen Prompts  

Schritt 2) Einfachen Router-Stub implementieren  
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```
  

## Praktische Checkliste
- [ ] Mindestens zwei Werkzeuge registrieren und Anfragen routen
- [ ] Foundry Local über OpenAI SDK und rohe REST aufrufen
- [ ] JSON-Ausgaben für eine Domänenvorlage erzwingen
- [ ] Anfragen lokal bereinigen und protokollieren
- [ ] Einfachen hybriden Router mit Azure-Fallback implementieren

## Abschluss

Foundry Local ermöglicht robuste KI direkt auf dem Gerät, bei der Modelle zu zusammensetzbaren Werkzeugen werden. Mit klaren Schnittstellen, Governance und hybrider Skalierung können Teams Echtzeit-, sichere KI-Anwendungen entwickeln, die die Privatsphäre der Nutzer respektieren und gleichzeitig unternehmensbereit bleiben.

---

