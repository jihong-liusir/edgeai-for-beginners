<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e3d7e45c04a9946ff6f9a3358db9ba74",
  "translation_date": "2025-09-30T22:59:48+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "de"
}
-->
# Sitzung 3: Entdeckung und Verwaltung von Open-Source-Modellen

## Überblick

Diese Sitzung konzentriert sich auf die praktische Entdeckung und Verwaltung von Modellen mit Foundry Local. Sie lernen, wie Sie verfügbare Modelle auflisten, verschiedene Optionen testen und grundlegende Leistungsmerkmale verstehen. Der Ansatz betont die praktische Erkundung mit der Foundry CLI, um Ihnen bei der Auswahl der richtigen Modelle für Ihre Anwendungsfälle zu helfen.

## Lernziele

- Beherrschen der Foundry CLI-Befehle zur Modellentdeckung und -verwaltung
- Verständnis von Modell-Cache- und lokalen Speicherstrukturen
- Schnelles Testen und Vergleichen verschiedener Modelle
- Etablierung praktischer Workflows für Modellauswahl und Benchmarking
- Erkundung des wachsenden Ökosystems von Modellen, die über Foundry Local verfügbar sind

## Voraussetzungen

- Abschluss der Sitzung 1: Einführung in Foundry Local
- Foundry Local CLI installiert und zugänglich
- Ausreichender Speicherplatz für Modell-Downloads (Modelle können zwischen 1GB und 20GB+ variieren)
- Grundlegendes Verständnis von Modelltypen und Anwendungsfällen

## Teil 6: Praktische Übung

### Übung: Modellentdeckung und Vergleich

Erstellen Sie Ihr eigenes Skript zur Modellevaluierung basierend auf Sample 03:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Ihre Aufgabe

1. **Führen Sie das Sample 03-Skript aus**: `samples\03\list_and_bench.cmd`
2. **Testen Sie verschiedene Modelle**: Testen Sie mindestens 3 verschiedene Modelle
3. **Vergleichen Sie die Leistung**: Notieren Sie Unterschiede in Geschwindigkeit und Antwortqualität
4. **Dokumentieren Sie Ihre Ergebnisse**: Erstellen Sie eine einfache Vergleichstabelle

### Beispiel für ein Vergleichsformat

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Teil 7: Fehlerbehebung und bewährte Verfahren

### Häufige Probleme und Lösungen

**Modell startet nicht:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Unzureichender Speicher:**
- Beginnen Sie mit kleineren Modellen (`phi-4-mini`)
- Schließen Sie andere Anwendungen
- Erweitern Sie den Arbeitsspeicher, wenn häufig Speichergrenzen erreicht werden

**Langsame Leistung:**
- Stellen Sie sicher, dass das Modell vollständig geladen ist (prüfen Sie die ausführliche Ausgabe)
- Schließen Sie unnötige Hintergrundanwendungen
- Ziehen Sie schnelleren Speicher (SSD) in Betracht

### Bewährte Verfahren

1. **Klein anfangen**: Beginnen Sie mit `phi-4-mini`, um die Einrichtung zu validieren
2. **Ein Modell zur Zeit**: Stoppen Sie vorherige Modelle, bevor Sie neue starten
3. **Ressourcen überwachen**: Behalten Sie die Speichernutzung im Auge
4. **Konsistent testen**: Verwenden Sie dieselben Eingaben für faire Vergleiche
5. **Ergebnisse dokumentieren**: Notieren Sie die Modellleistung für Ihre Anwendungsfälle

## Teil 8: Nächste Schritte und Referenzen

### Vorbereitung auf Sitzung 4

- **Schwerpunkt der Sitzung 4**: Optimierungstools und -techniken
- **Voraussetzungen**: Vertrautheit mit Modellwechsel und grundlegenden Leistungstests
- **Empfehlung**: Identifizieren Sie 2-3 bevorzugte Modelle aus dieser Sitzung

### Zusätzliche Ressourcen

- **[Foundry Local Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Offizielle Dokumentation
- **[CLI-Referenz](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Vollständige Befehlsreferenz
- **[Model Mondays](https://aka.ms/model-mondays)**: Wöchentliche Modellvorstellungen
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Community und Probleme
- **[Sample 03: Modellentdeckung](samples/03/README.md)**: Praktisches Beispielskript

### Wichtige Erkenntnisse

✅ **Modellentdeckung**: Verwenden Sie `foundry model list`, um verfügbare Modelle zu erkunden  
✅ **Schnelles Testen**: Das Muster `list_and_bench.cmd` für schnelle Evaluierung  
✅ **Leistungsüberwachung**: Grundlegende Ressourcennutzung und Antwortzeitmessung  
✅ **Modellauswahl**: Praktische Richtlinien zur Auswahl von Modellen nach Anwendungsfall  
✅ **Cache-Verwaltung**: Verständnis von Speicher- und Bereinigungsverfahren  

Sie haben nun die praktischen Fähigkeiten, geeignete Modelle für Ihre KI-Anwendungen mit der einfachen CLI-Methode von Foundry Local zu entdecken, zu testen und auszuwählen.

## Lernziele

- Entdecken und bewerten Sie Open-Source-Modelle für lokale Inferenz
- Kompilieren und führen Sie ausgewählte Hugging Face-Modelle innerhalb von Foundry Local aus
- Anwenden von Modellauswahlstrategien für Genauigkeit, Latenz und Ressourcenbedarf
- Verwalten Sie Modelle lokal mit Cache und Versionierung

## Teil 1: Modellentdeckung mit Foundry CLI

### Grundlegende Befehle zur Modellverwaltung

Die Foundry CLI bietet einfache Befehle zur Modellentdeckung und -verwaltung:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Ausführen Ihrer ersten Modelle

Beginnen Sie mit beliebten, gut getesteten Modellen, um Leistungsmerkmale zu verstehen:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-7b --verbose
```

**Hinweis:** Der `--verbose`-Flag liefert detaillierte Startinformationen, einschließlich:
- Fortschritt des Modell-Downloads (beim ersten Start)
- Details zur Speicherzuweisung
- Informationen zur Dienstbindung
- Leistungsinitialisierungsmetriken

### Verständnis der Modellkategorien

**Kleine Sprachmodelle (SLMs):**
- `phi-4-mini`: Schnell, effizient, ideal für allgemeine Chats
- `phi-4`: Leistungsfähigere Version mit besserem logischen Denken

**Mittlere Modelle:**
- `qwen2.5-7b`: Hervorragendes logisches Denken und längerer Kontext
- `deepseek-r1-7b`: Optimiert für Codegenerierung

**Größere Modelle:**
- `llama-3.2`: Metas neuestes Open-Source-Modell
- `qwen2.5-14b`: Unternehmensgerechtes logisches Denken

## Teil 2: Schnelles Testen und Vergleichen von Modellen

### Ansatz Sample 03: Einfaches Listen und Benchmarken

Basierend auf unserem Muster Sample 03 hier der minimale Workflow:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Testen der Modellleistung

Sobald ein Modell läuft, testen Sie es mit konsistenten Eingaben:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### Alternative PowerShell-Testmethode

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Teil 3: Modell-Cache- und Speicherverwaltung

### Verständnis des Modell-Caches

Foundry Local verwaltet Modell-Downloads und Cache automatisch:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Überlegungen zur Modell-Speicherung

**Typische Modellgrößen:**
- `phi-4-mini`: ~2,5 GB
- `qwen2.5-7b`: ~4,1 GB  
- `deepseek-r1-7b`: ~4,3 GB
- `llama-3.2`: ~4,9 GB
- `qwen2.5-14b`: ~8,2 GB

**Bewährte Speicherpraktiken:**
- Halten Sie 2-3 Modelle im Cache für schnellen Wechsel
- Entfernen Sie nicht verwendete Modelle, um Platz zu schaffen: `foundry cache clean`
- Überwachen Sie die Festplattennutzung, insbesondere bei kleineren SSDs
- Berücksichtigen Sie den Kompromiss zwischen Modellgröße und Fähigkeiten

### Leistungsüberwachung von Modellen

Während Modelle laufen, überwachen Sie die Systemressourcen:

**Windows Task-Manager:**
- Beobachten Sie die Speichernutzung (Modelle bleiben im RAM geladen)
- Überwachen Sie die CPU-Auslastung während der Inferenz
- Prüfen Sie die Festplatten-I/O während des initialen Modellladens

**Überwachung über die Befehlszeile:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Teil 4: Praktische Richtlinien zur Modellauswahl

### Auswahl von Modellen nach Anwendungsfall

**Für allgemeine Chats und Q&A:**
- Starten mit: `phi-4-mini` (schnell, effizient)
- Upgrade auf: `phi-4` (besseres logisches Denken)
- Fortgeschritten: `qwen2.5-7b` (längerer Kontext)

**Für Codegenerierung:**
- Empfohlen: `deepseek-r1-7b`
- Alternative: `qwen2.5-7b` (auch gut für Code)

**Für komplexes logisches Denken:**
- Beste Wahl: `qwen2.5-7b` oder `qwen2.5-14b`
- Budgetoption: `phi-4`

### Leitfaden für Hardwareanforderungen

**Minimale Systemanforderungen:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Empfohlen für beste Leistung:**
- 32GB+ RAM für komfortables Umschalten zwischen Modellen
- SSD-Speicher für schnelleres Modellladen
- Moderner Prozessor mit guter Single-Thread-Leistung
- NPU-Unterstützung (Windows 11 Copilot+ PCs) für Beschleunigung

### Workflow für Modellwechsel

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b

REM Verify model is running
foundry service status
```

## Teil 5: Einfaches Benchmarking von Modellen

### Grundlegende Leistungstests

Hier ist ein einfacher Ansatz, um die Leistung von Modellen zu vergleichen:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b", "deepseek-r1-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuelle Qualitätsbewertung

Testen Sie jedes Modell mit konsistenten Eingaben und bewerten Sie es manuell:

**Testeingaben:**
1. "Erklären Sie Quantencomputing in einfachen Worten."
2. "Schreiben Sie eine Python-Funktion, um eine Liste zu sortieren."
3. "Was sind die Vor- und Nachteile von Remote-Arbeit?"
4. "Fassen Sie die Vorteile von Edge-KI zusammen."

**Bewertungskriterien:**
- **Genauigkeit**: Sind die Informationen korrekt?
- **Klarheit**: Ist die Erklärung leicht verständlich?
- **Vollständigkeit**: Wird die gesamte Frage beantwortet?
- **Geschwindigkeit**: Wie schnell erfolgt die Antwort?

### Überwachung der Ressourcennutzung

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Teil 6: Nächste Schritte

- Abonnieren Sie Model Mondays für neue Modelle und Tipps: https://aka.ms/model-mondays
- Tragen Sie Ihre Ergebnisse in die `models.json` Ihres Teams ein
- Bereiten Sie sich auf Sitzung 4 vor: Vergleich von LLMs vs. SLMs, lokale vs. Cloud-Inferenz und praktische Demos

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.