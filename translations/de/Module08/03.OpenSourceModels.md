<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T12:54:19+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "de"
}
-->
# Sitzung 3: Open-Source-Modelle mit Foundry Local

## Überblick

In dieser Sitzung geht es darum, wie man Open-Source-Modelle in Foundry Local integriert: Auswahl von Community-Modellen, Integration von Hugging Face-Inhalten und Nutzung von „Bring your own model“ (BYOM)-Strategien. Außerdem lernen Sie die Serie „Model Mondays“ kennen, die kontinuierliches Lernen und Modellentdeckung fördert.

Referenzen:
- Foundry Local-Dokumentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face-Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Lernziele
- Open-Source-Modelle für lokale Inferenz entdecken und bewerten
- Ausgewählte Hugging Face-Modelle in Foundry Local kompilieren und ausführen
- Modell-Auswahlstrategien für Genauigkeit, Latenz und Ressourcenbedarf anwenden
- Modelle lokal mit Cache und Versionierung verwalten

## Teil 1: Modellentdeckung und -auswahl (Schritt-für-Schritt)

Schritt 1) Verfügbare Modelle im lokalen Katalog auflisten  
```cmd
foundry model list
```
  
Schritt 2) Zwei Kandidaten schnell ausprobieren (automatischer Download beim ersten Ausführen)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Schritt 3) Grundlegende Metriken notieren  
- Latenz (subjektiv) und Qualität für einen festen Prompt beobachten  
- Speicherverbrauch im Task-Manager überwachen, während jedes Modell läuft  

## Teil 2: Katalogmodelle über CLI ausführen (Schritt-für-Schritt)

Schritt 1) Ein Modell starten  
```cmd
foundry model run llama-3.2
```
  
Schritt 2) Einen Test-Prompt über den OpenAI-kompatiblen Endpunkt senden  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Teil 3: BYOM – Hugging Face-Modelle kompilieren (Schritt-für-Schritt)

Folgen Sie der offiziellen Anleitung zum Kompilieren von Modellen. Nachfolgend ein Überblick – die genauen Befehle und unterstützten Konfigurationen finden Sie im Microsoft Learn-Artikel.

Schritt 1) Arbeitsverzeichnis vorbereiten  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Schritt 2) Ein unterstütztes HF-Modell kompilieren  
- Verwenden Sie die Schritte aus der Learn-Dokumentation, um das Modell zu konvertieren und das kompilierte ONNX-Modell in Ihrem `models`-Verzeichnis zu platzieren  
- Bestätigen Sie mit:  
```cmd
foundry cache ls
```
  
Sie sollten den Namen Ihres kompilierten Modells sehen (zum Beispiel `llama-3.2`).  

Schritt 3) Das kompilierte Modell ausführen  
```cmd
foundry model run llama-3.2 --verbose
```
  
Hinweise:  
- Stellen Sie sicher, dass ausreichend Festplatten- und RAM-Kapazität für Kompilierung und Ausführung vorhanden ist  
- Beginnen Sie mit kleineren Modellen, um den Ablauf zu validieren, und skalieren Sie dann hoch  

## Teil 4: Praktische Modellkuratierung (Schritt-für-Schritt)

Schritt 1) Eine `models.json`-Registry erstellen  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Schritt 2) Kleines Auswahlskript  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Teil 5: Praktische Benchmarks (Schritt-für-Schritt)

Schritt 1) Einfacher Latenz-Benchmark  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Schritt 2) Qualitätsprüfung  
- Einen festen Prompt-Satz verwenden, Ausgaben in einer CSV/JSON erfassen  
- Manuell Fluenz, Relevanz und Korrektheit bewerten (1–5)  

## Teil 6: Nächste Schritte
- Abonnieren Sie Model Mondays für neue Modelle und Tipps: https://aka.ms/model-mondays  
- Teilen Sie Ihre Ergebnisse in der `models.json` Ihres Teams  
- Bereiten Sie sich auf Sitzung 4 vor: Vergleich von LLMs vs. SLMs, lokale vs. Cloud-Inferenz und praktische Demos  

---

