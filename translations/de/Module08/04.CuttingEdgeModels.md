<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:00:15+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "de"
}
-->
# Sitzung 4: Produktionsreife Chat-Anwendungen mit Chainlit erstellen

## Überblick

Diese Sitzung konzentriert sich auf die Erstellung von produktionsreifen Chat-Anwendungen mit Chainlit und Microsoft Foundry Local. Sie lernen, moderne Weboberflächen für KI-Konversationen zu erstellen, Streaming-Antworten zu implementieren und robuste Chat-Anwendungen mit ordnungsgemäßer Fehlerbehandlung und Benutzererlebnisgestaltung bereitzustellen.

**Was Sie erstellen werden:**
- **Chainlit Chat-App**: Moderne Weboberfläche mit Streaming-Antworten
- **WebGPU-Demo**: Browserbasierte Inferenz für datenschutzorientierte Anwendungen  
- **Open WebUI-Integration**: Professionelle Chat-Oberfläche mit Foundry Local
- **Produktionsmuster**: Fehlerbehandlung, Überwachung und Bereitstellungsstrategien

## Lernziele

- Produktionsreife Chat-Anwendungen mit Chainlit erstellen
- Streaming-Antworten für ein verbessertes Benutzererlebnis implementieren
- Foundry Local SDK-Integrationsmuster meistern
- Fehlerbehandlung und schrittweise Degradierung anwenden
- Chat-Anwendungen für verschiedene Umgebungen bereitstellen und konfigurieren
- Moderne Web-UI-Muster für konversationelle KI verstehen

## Voraussetzungen

- **Foundry Local**: Installiert und ausgeführt ([Installationsanleitung](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Version 3.10 oder höher mit virtueller Umgebung
- **Modell**: Mindestens ein geladenes Modell (`foundry model run phi-4-mini`)
- **Browser**: Moderner Webbrowser mit WebGPU-Unterstützung (Chrome/Edge)
- **Docker**: Für die Open WebUI-Integration (optional)

## Teil 1: Moderne Chat-Anwendungen verstehen

### Architekturübersicht

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Schlüsseltechnologien

**Foundry Local SDK-Muster:**
- `FoundryLocalManager(alias)`: Automatische Dienstverwaltung
- `manager.endpoint` und `manager.api_key`: Verbindungsdetails
- `manager.get_model_info(alias).id`: Modellidentifikation

**Chainlit-Framework:**
- `@cl.on_chat_start`: Chat-Sitzungen initialisieren
- `@cl.on_message`: Eingehende Benutzer-Nachrichten verarbeiten  
- `cl.Message().stream_token()`: Echtzeit-Streaming
- Automatische UI-Generierung und WebSocket-Verwaltung

## Teil 2: Entscheidungsmatrix Lokal vs. Cloud

### Leistungsmerkmale

| Aspekt | Lokal (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latenz** | 🚀 50-200ms (kein Netzwerk) | ⏱️ 200-2000ms (netzwerkabhängig) |
| **Datenschutz** | 🔒 Daten verlassen das Gerät nie | ⚠️ Daten werden in die Cloud gesendet |
| **Kosten** | 💰 Kostenlos nach Hardware | 💸 Zahlung pro Token |
| **Offline** | ✅ Funktioniert ohne Internet | ❌ Erfordert Internet |
| **Modellgröße** | ⚠️ Begrenzung durch Hardware | ✅ Zugriff auf größte Modelle |
| **Skalierung** | ⚠️ Hardwareabhängig | ✅ Unbegrenzte Skalierung |

### Hybridstrategiemuster

**Lokal zuerst mit Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Aufgabenbasierte Weiterleitung:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Teil 3: Beispiel 04 - Chainlit Chat-Anwendung

### Schnellstart

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Die Anwendung öffnet sich automatisch unter `http://localhost:8080` mit einer modernen Chat-Oberfläche.

### Kernimplementierung

Die Beispielanwendung 04 demonstriert produktionsreife Muster:

**Automatische Dienstentdeckung:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Streaming-Chat-Handler:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Konfigurationsoptionen

**Umgebungsvariablen:**

| Variable | Beschreibung | Standard | Beispiel |
|----------|-------------|---------|----------|
| `MODEL` | Zu verwendender Modellalias | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local-Endpunkt | Automatisch erkannt | `http://localhost:51211` |
| `API_KEY` | API-Schlüssel (optional für lokal) | `""` | `your-api-key` |

**Erweiterte Nutzung:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Teil 4: Jupyter-Notebooks erstellen und verwenden

### Überblick über Notebook-Unterstützung

Das Beispiel 04 enthält ein umfassendes Jupyter-Notebook (`chainlit_app.ipynb`), das Folgendes bietet:

- **📚 Lerninhalte**: Schritt-für-Schritt-Lernmaterialien
- **🔬 Interaktive Erkundung**: Codezellen ausführen und experimentieren
- **📊 Visuelle Demonstrationen**: Diagramme, Grafiken und Ergebnisvisualisierung
- **🛠️ Entwicklungstools**: Test- und Debugging-Funktionen

### Eigene Notebooks erstellen

#### Schritt 1: Jupyter-Umgebung einrichten

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Schritt 2: Neues Notebook erstellen

**Mit VS Code:**
1. Öffnen Sie VS Code im Verzeichnis Module08
2. Erstellen Sie eine neue Datei mit der Endung `.ipynb`
3. Wählen Sie den "Foundry Local"-Kernel aus, wenn Sie dazu aufgefordert werden
4. Beginnen Sie mit dem Hinzufügen von Zellen mit Ihrem Inhalt

**Mit Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Best Practices für Notebook-Struktur

#### Zellorganisation

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Interaktive Beispiele und Übungen

#### Übung 1: Client-Konfiguration testen

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Übung 2: Streaming-Antwort-Simulation

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Teil 5: WebGPU-Browser-Inferenz-Demo

### Überblick

WebGPU ermöglicht das Ausführen von KI-Modellen direkt im Browser für maximalen Datenschutz und eine Installation-freie Erfahrung. Dieses Beispiel demonstriert ONNX Runtime Web mit WebGPU-Ausführung.

### Schritt 1: WebGPU-Unterstützung prüfen

**Browseranforderungen:**
- Chrome/Edge 113+ mit aktivierter WebGPU
- Überprüfen: `chrome://gpu` → "WebGPU"-Status bestätigen
- Programmatische Überprüfung: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Schritt 2: WebGPU-Demo erstellen

Verzeichnis erstellen: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Schritt 3: Demo ausführen

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Teil 6: Open WebUI-Integration

### Überblick

Open WebUI bietet eine professionelle ChatGPT-ähnliche Oberfläche, die mit der OpenAI-kompatiblen API von Foundry Local verbunden ist.

### Schritt 1: Voraussetzungen

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Schritt 2: Docker-Setup (empfohlen)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Hinweis:** `host.docker.internal` ermöglicht Docker-Containern den Zugriff auf den Host-Rechner unter Windows.

### Schritt 3: Konfiguration

1. **Browser öffnen:** Navigieren Sie zu `http://localhost:3000`
2. **Erstkonfiguration:** Admin-Konto erstellen
3. **Modellkonfiguration:**
   - Einstellungen → Modelle → OpenAI API  
   - Basis-URL: `http://host.docker.internal:51211/v1`
   - API-Schlüssel: `foundry-local-key` (beliebiger Wert funktioniert)
4. **Verbindung testen:** Modelle sollten im Dropdown-Menü erscheinen

### Fehlerbehebung

**Häufige Probleme:**

1. **Verbindung verweigert:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modelle erscheinen nicht:**
   - Überprüfen, ob Modell geladen ist: `foundry model list`
   - API-Antwort prüfen: `curl http://localhost:51211/v1/models`
   - Open WebUI-Container neu starten

## Teil 7: Überlegungen zur Produktionsbereitstellung

### Umgebungskonfiguration

**Entwicklungs-Setup:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Produktionsbereitstellung:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Häufige Portprobleme und Lösungen

**Port 51211 Konfliktvermeidung:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Leistungsüberwachung

**Implementierung von Gesundheitschecks:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Zusammenfassung

Sitzung 4 behandelte die Erstellung produktionsreifer Chainlit-Anwendungen für konversationelle KI. Sie haben gelernt:

- ✅ **Chainlit-Framework**: Moderne UI und Streaming-Unterstützung für Chat-Anwendungen
- ✅ **Foundry Local-Integration**: SDK-Nutzung und Konfigurationsmuster  
- ✅ **WebGPU-Inferenz**: Browserbasierte KI für maximalen Datenschutz
- ✅ **Open WebUI-Setup**: Bereitstellung einer professionellen Chat-Oberfläche
- ✅ **Produktionsmuster**: Fehlerbehandlung, Überwachung und Skalierung

Die Beispielanwendung 04 demonstriert Best Practices für die Erstellung robuster Chat-Oberflächen, die lokale KI-Modelle über Microsoft Foundry Local nutzen und gleichzeitig ein hervorragendes Benutzererlebnis bieten.

## Referenzen

- **[Beispiel 04: Chainlit-Anwendung](samples/04/README.md)**: Vollständige Anwendung mit Dokumentation
- **[Chainlit Bildungs-Notebook](samples/04/chainlit_app.ipynb)**: Interaktive Lernmaterialien
- **[Foundry Local Dokumentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Vollständige Plattformdokumentation
- **[Chainlit Dokumentation](https://docs.chainlit.io/)**: Offizielle Framework-Dokumentation
- **[Open WebUI-Integrationsleitfaden](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Offizielles Tutorial

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.