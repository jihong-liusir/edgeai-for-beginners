<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T12:56:53+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "de"
}
-->
# Sitzung 4: Moderne Modelle – LLMs, SLMs und On-Device-Inferenz

## Überblick

Vergleichen Sie LLMs und SLMs, bewerten Sie die Vor- und Nachteile von lokaler und Cloud-Inferenz und implementieren Sie Demos, die EdgeAI-Szenarien mit Phi und ONNX Runtime demonstrieren. Wir werden auch Chainlit RAG, WebGPU-Inferenzoptionen und die Integration von Open WebUI hervorheben.

Referenzen:
- Foundry Local-Dokumentation: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI Anleitung (Chat-App mit Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Lernziele
- Verstehen der Vor- und Nachteile von LLMs vs. SLMs in Bezug auf Kosten, Latenz und Genauigkeit
- Auswahl zwischen lokaler und Cloud-Inferenz für spezifische Geschäftsanforderungen
- Implementierung einer kleinen RAG-Demo mit Chainlit
- Erkundung von WebGPU für browserseitige Beschleunigung
- Verbindung von Open WebUI mit Foundry Local

## Teil 1: LLM vs. SLM – Entscheidungsmatrix

Berücksichtigen Sie:
- Latenz: SLMs auf dem Gerät liefern oft Antworten in unter einer Sekunde
- Kosten: Lokale Inferenz senkt Cloud-Kosten
- Datenschutz: Sensible Daten bleiben auf dem Gerät
- Fähigkeiten: LLMs können SLMs bei komplexen Aufgaben übertreffen
- Zuverlässigkeit: Hybride Strategien reduzieren das Risiko von Ausfallzeiten

## Teil 2: Lokal vs. Cloud – Hybride Muster

- Lokal zuerst mit Cloud-Fallback für große/komplexe Eingaben
- Cloud zuerst mit lokalem Ansatz für datenschutzsensible oder Offline-Szenarien
- Routing nach Aufgabentyp (Code-Generierung zu DeepSeek, allgemeiner Chat zu Phi/Qwen)

## Teil 3: RAG-Chat-App mit Chainlit (Minimal)

Installieren Sie Abhängigkeiten:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Ausführen:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Erweitern: Fügen Sie einen einfachen Retriever (lokale Dateien) hinzu und setzen Sie den abgerufenen Kontext vor die Benutzereingabe.

## Teil 4: WebGPU-Inferenz (Hinweis)

Führen Sie kleine Modelle direkt im Browser mit WebGPU aus. Dies ist ideal für datenschutzorientierte Demos und Erfahrungen ohne Installation. Unten finden Sie ein minimales, schrittweises Beispiel mit ONNX Runtime Web und dem WebGPU-Ausführungsanbieter.

1) Überprüfen Sie die WebGPU-Unterstützung
- Chromium-Browser: chrome://gpu → bestätigen Sie, dass „WebGPU“ aktiviert ist
- Programmatische Überprüfung (wir prüfen dies auch im Code): `if (!('gpu' in navigator)) { /* kein WebGPU */ }`

2) Erstellen Sie ein minimales Projekt
Erstellen Sie einen Ordner und zwei Dateien: `index.html` und `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Lokal bereitstellen (Windows cmd.exe)
Verwenden Sie einen einfachen statischen Server, damit der Browser das Modell abrufen kann.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Öffnen Sie http://localhost:5173 in Ihrem Browser. Sie sollten Initialisierungsprotokolle, Sitzungscreation mit WebGPU und eine Argmax-Vorhersage sehen.

4) Fehlerbehebung
- Wenn WebGPU nicht verfügbar ist: Aktualisieren Sie Chrome/Edge und stellen Sie sicher, dass die GPU-Treiber aktuell sind, und überprüfen Sie chrome://flags auf „WebGPU aktivieren“.
- Bei CORS- oder Abruffehlern: Stellen Sie sicher, dass Sie Dateien über http:// (nicht file://) bereitstellen und die Modell-URL Cross-Origin-Anfragen zulässt.
- Fallback auf CPU: Ändern Sie `executionProviders: ['wasm']`, um das Basisverhalten zu überprüfen.

5) Nächste Schritte
- Tauschen Sie ein domänenspezifisches ONNX-Modell aus (z. B. Bildklassifikation oder ein kleines Textmodell).
- Fügen Sie Vorverarbeitungs-/Nachverarbeitungslogik für echte Eingaben hinzu.
- Für größere Modelle oder Produktionslatenz bevorzugen Sie Foundry Local oder ONNX Runtime Server.

## Teil 5: Open WebUI + Foundry Local (Schritt-für-Schritt)

Dies verbindet Open WebUI mit Foundry Locals OpenAI-kompatiblem Endpunkt für eine lokale Chat-Benutzeroberfläche.

1) Voraussetzungen
- Foundry Local installiert und funktionsfähig (`foundry --version`)
- Ein Modell, das lokal ausgeführt werden kann (z. B. `phi-4-mini`)
- Docker Desktop installiert (empfohlen für Open WebUI)

2) Starten Sie ein Modell mit Foundry Local
```powershell
foundry model run phi-4-mini
```
Dies stellt eine OpenAI-kompatible API unter `http://localhost:8000` bereit.

3) Starten Sie Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Hinweise:
- Unter Windows ermöglicht `host.docker.internal`, dass der Container Ihren Host unter `localhost` erreicht.
- Wir setzen `OPENAI_API_BASE_URL` auf den Endpunkt von Foundry Local und einen Dummy-`OPENAI_API_KEY`.

4) Konfiguration über die Open WebUI-Benutzeroberfläche (Alternative)
- Gehen Sie zu http://localhost:3000
- Schließen Sie die Ersteinrichtung ab (Admin-Benutzer)
- Gehen Sie zu Einstellungen → Modelle/Anbieter
- Basis-URL festlegen: `http://host.docker.internal:8000/v1`
- API-Schlüssel festlegen: `local-key` (Platzhalter)
- Speichern

5) Testen Sie eine Eingabeaufforderung
- Wählen oder geben Sie in Open WebUI Chat den Modellnamen `phi-4-mini` ein
- Eingabeaufforderung: „Nennen Sie fünf Vorteile der KI-Inferenz auf dem Gerät.“
- Sie sollten eine Antwort sehen, die von Ihrem lokalen Modell gestreamt wird.

6) Fehlerbehebung
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Optional: Open WebUI-Daten persistent speichern
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Praktische Checkliste
- [ ] Vergleichen Sie Antworten/Latenz zwischen SLM und LLM lokal
- [ ] Führen Sie die Chainlit-Demo mit mindestens zwei Modellen aus
- [ ] Verbinden Sie Open WebUI mit Ihrem lokalen Endpunkt und testen Sie

## Nächste Schritte
- Vorbereitung auf Agenten-Workflows in Sitzung 5
- Identifizieren Sie Szenarien, in denen hybride lokale/Cloud-Ansätze den ROI verbessern

---

