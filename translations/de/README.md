<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c817161ba08864340737d623f761b9ae",
  "translation_date": "2025-09-17T12:21:19+00:00",
  "source_file": "README.md",
  "language_code": "de"
}
-->
# EdgeAI f√ºr Einsteiger

![Kurs Titelbild](../../translated_images/cover.eb18d1b9605d754b30973f4e17c6e11ea4f8473d9686ee378d6e7b44e3c70ac7.de.png)

[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/edgeai-for-beginners.svg)](https://GitHub.com/microsoft/edgeai-for-beginners/graphs/contributors)
[![GitHub issues](https://img.shields.io/github/issues/microsoft/edgeai-for-beginners.svg)](https://GitHub.com/microsoft/edgeai-for-beginners/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/edgeai-for-beginners.svg)](https://GitHub.com/microsoft/edgeai-for-beginners/pulls)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/edgeai-for-beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/edgeai-for-beginners/watchers)
[![GitHub forks](https://img.shields.io/github/forks/microsoft/edgeai-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/edgeai-for-beginners/fork)
[![GitHub stars](https://img.shields.io/github/stars/microsoft/edgeai-for-beginners?style=social&label=Star)](https://GitHub.com/microsoft/edgeai-for-beginners/stargazers)

[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/ByRwuEEgH4)](https://discord.com/invite/ByRwuEEgH4)

Folgen Sie diesen Schritten, um mit diesen Ressourcen zu starten:

1. **Forken Sie das Repository**: Klicken Sie auf [![GitHub forks](https://img.shields.io/github/forks/microsoft/edgeai-for-beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/edgeai-for-beginners/fork)
2. **Klonen Sie das Repository**: `git clone https://github.com/microsoft/edgeai-for-beginners.git`
3. [**Treten Sie dem Azure AI Foundry Discord bei und tauschen Sie sich mit Experten und anderen Entwicklern aus**](https://discord.com/invite/ByRwuEEgH4)

### üåê Mehrsprachige Unterst√ºtzung

#### Unterst√ºtzt durch GitHub Action (Automatisiert & Immer aktuell)

[Arabisch](../ar/README.md) | [Bengalisch](../bn/README.md) | [Bulgarisch](../bg/README.md) | [Birmanisch (Myanmar)](../my/README.md) | [Chinesisch (Vereinfacht)](../zh/README.md) | [Chinesisch (Traditionell, Hongkong)](../hk/README.md) | [Chinesisch (Traditionell, Macau)](../mo/README.md) | [Chinesisch (Traditionell, Taiwan)](../tw/README.md) | [Kroatisch](../hr/README.md) | [Tschechisch](../cs/README.md) | [D√§nisch](../da/README.md) | [Niederl√§ndisch](../nl/README.md) | [Finnisch](../fi/README.md) | [Franz√∂sisch](../fr/README.md) | [Deutsch](./README.md) | [Griechisch](../el/README.md) | [Hebr√§isch](../he/README.md) | [Hindi](../hi/README.md) | [Ungarisch](../hu/README.md) | [Indonesisch](../id/README.md) | [Italienisch](../it/README.md) | [Japanisch](../ja/README.md) | [Koreanisch](../ko/README.md) | [Malaiisch](../ms/README.md) | [Marathi](../mr/README.md) | [Nepalesisch](../ne/README.md) | [Norwegisch](../no/README.md) | [Persisch (Farsi)](../fa/README.md) | [Polnisch](../pl/README.md) | [Portugiesisch (Brasilien)](../br/README.md) | [Portugiesisch (Portugal)](../pt/README.md) | [Punjabi (Gurmukhi)](../pa/README.md) | [Rum√§nisch](../ro/README.md) | [Russisch](../ru/README.md) | [Serbisch (Kyrillisch)](../sr/README.md) | [Slowakisch](../sk/README.md) | [Slowenisch](../sl/README.md) | [Spanisch](../es/README.md) | [Swahili](../sw/README.md) | [Schwedisch](../sv/README.md) | [Tagalog (Filipino)](../tl/README.md) | [Thail√§ndisch](../th/README.md) | [T√ºrkisch](../tr/README.md) | [Ukrainisch](../uk/README.md) | [Urdu](../ur/README.md) | [Vietnamesisch](../vi/README.md)

**Falls Sie zus√§tzliche √úbersetzungen w√ºnschen, finden Sie die unterst√ºtzten Sprachen [hier](https://github.com/Azure/co-op-translator/blob/main/getting_started/supported-languages.md)**

## Einf√ºhrung

Willkommen bei **EdgeAI f√ºr Einsteiger** ‚Äì Ihre umfassende Reise in die transformative Welt der Edge-K√ºnstlichen Intelligenz. Dieser Kurs schl√§gt die Br√ºcke zwischen leistungsstarken KI-F√§higkeiten und deren praktischer, realer Anwendung auf Edge-Ger√§ten, sodass Sie das Potenzial der KI direkt dort nutzen k√∂nnen, wo Daten generiert und Entscheidungen getroffen werden m√ºssen.

### Was Sie lernen werden

Dieser Kurs f√ºhrt Sie von den grundlegenden Konzepten bis hin zu produktionsreifen Implementierungen und behandelt:
- **Kleine Sprachmodelle (SLMs)**, die f√ºr den Einsatz auf Edge-Ger√§ten optimiert sind
- **Hardware-bewusste Optimierung** f√ºr verschiedene Plattformen
- **Echtzeit-Inferenz** mit datenschutzfreundlichen Funktionen
- **Produktionsbereitstellung** f√ºr Unternehmensanwendungen

### Warum EdgeAI wichtig ist

Edge AI stellt einen Paradigmenwechsel dar, der zentrale Herausforderungen der modernen Welt adressiert:
- **Datenschutz & Sicherheit**: Verarbeitung sensibler Daten lokal, ohne Cloud-Exposition
- **Echtzeit-Performance**: Keine Netzwerklatenz f√ºr zeitkritische Anwendungen
- **Kosteneffizienz**: Reduzierung von Bandbreiten- und Cloud-Computing-Kosten
- **Robuste Betriebsf√§higkeit**: Funktionalit√§t auch bei Netzwerkausf√§llen
- **Regulatorische Konformit√§t**: Einhaltung von Datenschutzbestimmungen

### Edge AI

Edge AI bezieht sich auf das Ausf√ºhren von KI-Algorithmen und Sprachmodellen lokal auf Hardware ‚Äì in der N√§he der Datenquelle ‚Äì ohne auf Cloud-Ressourcen f√ºr Inferenz zur√ºckzugreifen. Es reduziert Latenz, verbessert den Datenschutz und erm√∂glicht Echtzeit-Entscheidungen.

### Kernprinzipien:
- **On-Device-Inferenz**: KI-Modelle laufen auf Edge-Ger√§ten (Smartphones, Router, Mikrocontroller, Industrie-PCs)
- **Offline-F√§higkeit**: Funktioniert ohne dauerhafte Internetverbindung
- **Niedrige Latenz**: Sofortige Reaktionen f√ºr Echtzeitsysteme
- **Datensouver√§nit√§t**: Sensible Daten bleiben lokal, was Sicherheit und Compliance verbessert

### Kleine Sprachmodelle (SLMs)

SLMs wie Phi-4, Mistral-7B und Gemma sind optimierte Versionen gr√∂√üerer LLMs, die f√ºr folgende Zwecke trainiert oder destilliert wurden:
- **Reduzierter Speicherbedarf**: Effiziente Nutzung des begrenzten Speichers von Edge-Ger√§ten
- **Geringerer Rechenaufwand**: Optimiert f√ºr CPU- und Edge-GPU-Leistung
- **Schnellere Startzeiten**: Schnelle Initialisierung f√ºr reaktionsschnelle Anwendungen

Sie erm√∂glichen leistungsstarke NLP-Funktionen und erf√ºllen gleichzeitig die Anforderungen von:
- **Eingebetteten Systemen**: IoT-Ger√§te und industrielle Steuerungen
- **Mobilger√§ten**: Smartphones und Tablets mit Offline-F√§higkeiten
- **IoT-Ger√§ten**: Sensoren und intelligente Ger√§te mit begrenzten Ressourcen
- **Edge-Servern**: Lokale Verarbeitungseinheiten mit begrenzten GPU-Ressourcen
- **Personal Computern**: Desktop- und Laptop-Bereitstellungsszenarien

## Kursstruktur

### [Modul 01: Grundlagen und Transformation von EdgeAI](./Module01/README.md)
**Thema**: Der transformative Wandel durch Edge-AI-Bereitstellung

#### Kapitelstruktur:
- [**Abschnitt 1: Grundlagen von EdgeAI**](./Module01/01.EdgeAIFundamentals.md)
  - Vergleich von traditioneller Cloud-KI und Edge-KI
  - Herausforderungen und Einschr√§nkungen des Edge-Computings
  - Schl√ºsseltechnologien: Modell-Quantisierung, Kompressionsoptimierung, Kleine Sprachmodelle (SLMs)
  - Hardware-Beschleunigung: NPUs, GPU-Optimierung, CPU-Optimierung
  - Vorteile: Datenschutz, niedrige Latenz, Offline-F√§higkeiten, Kosteneffizienz

- [**Abschnitt 2: Praxisbeispiele aus der realen Welt**](./Module01/02.RealWorldCaseStudies.md)
  - Microsoft Phi- & Mu-Modell-√ñkosystem
  - Fallstudie: KI-Berichtssystem von Japan Airlines
  - Marktauswirkungen und zuk√ºnftige Entwicklungen
  - Bereitstellungs√ºberlegungen und Best Practices

- [**Abschnitt 3: Leitfaden f√ºr praktische Implementierung**](./Module01/03.PracticalImplementationGuide.md)
  - Einrichtung der Entwicklungsumgebung (Python 3.10+, .NET 8+)
  - Hardwareanforderungen und empfohlene Konfigurationen
  - Ressourcen der Kernmodellfamilie
  - Quantisierungs- und Optimierungstools (Llama.cpp, Microsoft Olive, Apple MLX)
  - Bewertungs- und Verifizierungscheckliste

- [**Abschnitt 4: Hardwareplattformen f√ºr Edge-AI-Bereitstellung**](./Module01/04.EdgeDeployment.md)
  - √úberlegungen und Anforderungen f√ºr Edge-AI-Bereitstellungen
  - Intel Edge-AI-Hardware und Optimierungstechniken
  - Qualcomm KI-L√∂sungen f√ºr mobile und eingebettete Systeme
  - NVIDIA Jetson und Edge-Computing-Plattformen
  - Windows KI-PC-Plattformen mit NPU-Beschleunigung
  - Hardware-spezifische Optimierungsstrategien

---

### [Modul 02: Grundlagen kleiner Sprachmodelle](./Module02/README.md)
**Thema**: Theoretische Prinzipien, Implementierungsstrategien und produktionsreife Bereitstellung von SLMs

#### Kapitelstruktur:
- [**Abschnitt 1: Grundlagen der Microsoft Phi-Modellfamilie**](./Module02/01.PhiFamily.md)
  - Entwicklung der Designphilosophie (Phi-1 bis Phi-4)
  - Architekturdesign mit Fokus auf Effizienz
  - Spezialisierte F√§higkeiten (Schlussfolgerungen, multimodal, Edge-Bereitstellung)

- [**Abschnitt 2: Grundlagen der Qwen-Modellfamilie**](./Module02/02.QwenFamily.md)
  - Open-Source-Exzellenz (Qwen 1.0 bis Qwen3) ‚Äì verf√ºgbar √ºber Hugging Face
  - Fortgeschrittene Architektur f√ºr Schlussfolgerungen mit Denkmodus-Funktionen
  - Skalierbare Bereitstellungsoptionen (0,5B-235B Parameter)

- [**Abschnitt 3: Grundlagen der Gemma-Modellfamilie**](./Module02/03.GemmaFamily.md)
  - Forschungsgetriebene Innovation (Gemma 3 & 3n)
  - Multimodale Exzellenz
  - Mobile-First-Architektur

- [**Abschnitt 4: Grundlagen der BitNET-Modellfamilie**](./Module02/04.BitNETFamily.md)
  - Revolution√§re Quantisierungstechnologie (1,58-Bit)
  - Spezielles Inferenz-Framework von https://github.com/microsoft/BitNet
  - Nachhaltige KI-F√ºhrung durch extreme Effizienz

- [**Abschnitt 5: Grundlagen der Microsoft Mu-Modelle**](./Module02/05.mumodel.md)
  - Ger√§teorientierte Architektur, integriert in Windows 11
  - Systemintegration mit den Windows 11-Einstellungen
  - Datenschutzfreundlicher Offline-Betrieb

- [**Abschnitt 6: Grundlagen von Phi-Silica**](./Module02/06.phisilica.md)
  - NPU-optimierte Architektur, integriert in Windows 11 Copilot+ PCs
  - Au√üergew√∂hnliche Effizienz (650 Tokens/Sekunde bei 1,5W)
  - Entwicklerintegration mit Windows App SDK

---

### [Modul 03: Bereitstellung kleiner Sprachmodelle](./Module03/README.md)
**Thema**: Vollst√§ndiger Lebenszyklus der SLM-Bereitstellung, von der Theorie bis zur Produktionsumgebung

#### Kapitelstruktur:
- [**Abschnitt 1: Fortgeschrittenes Lernen mit SLMs**](./Module03/01.SLMAdvancedLearning.md)
  - Klassifikationsrahmen f√ºr Parameter (Micro SLM 100M-1,4B, Medium SLM 14B-30B)
  - Fortgeschrittene Optimierungstechniken (Quantisierungsmethoden, BitNET 1-Bit-Quantisierung)
  - Strategien zur Modellerfassung (Azure AI Foundry f√ºr Phi-Modelle, Hugging Face f√ºr ausgew√§hlte Modelle)

- [**Abschnitt 2: Bereitstellung in lokaler Umgebung**](./Module03/02.DeployingSLMinLocalEnv.md)
  - Ollama universelle Plattformbereitstellung
  - Microsoft Foundry lokale Unternehmensl√∂sungen
  - Vergleich von Frameworks

- [**Abschnitt 3: Containerisierte Cloud-Bereitstellung**](./Module03/03.DeployingSLMinCloud.md)
  - vLLM Hochleistungs-Inferenzbereitstellung
  - Ollama Container-Orchestrierung
  - ONNX Runtime edge-optimierte Implementierung

---

### [Modul 04: Modellformatkonvertierung und Quantisierung](./Module04/README.md)
**Thema**: Vollst√§ndiges Toolkit zur Modelloptimierung f√ºr Edge-Bereitstellungen auf verschiedenen Plattformen

#### Kapitelstruktur:
- [**Abschnitt 1: Grundlagen der Modellformatkonvertierung und Quantisierung**](./Module04/01.Introduce.md)
  - Klassifikationsrahmen f√ºr Pr√§zision (ultra-niedrig, niedrig, mittel)
  - Vorteile und Anwendungsf√§lle von GGUF- und ONNX-Formaten
  - Vorteile der Quantisierung f√ºr betriebliche Effizienz
  - Leistungsbenchmarks und Speicherplatzvergleiche
- [**Abschnitt 2: Llama.cpp Implementierungsleitfaden**](./Module04/02.Llamacpp.md)
  - Plattform√ºbergreifende Installation (Windows, macOS, Linux)
  - GGUF-Formatkonvertierung und Quantisierungsstufen (Q2_K bis Q8_0)
  - Hardwarebeschleunigung (CUDA, Metal, OpenCL, Vulkan)
  - Python-Integration und REST-API-Bereitstellung

- [**Abschnitt 3: Microsoft Olive Optimierungssuite**](./Module04/03.MicrosoftOlive.md)
  - Hardwarebewusste Modelloptimierung mit √ºber 40 integrierten Komponenten
  - Automatische Optimierung mit dynamischer und statischer Quantisierung
  - Unternehmensintegration mit Azure ML-Workflows
  - Unterst√ºtzung beliebter Modelle (Llama, Phi, ausgew√§hlte Qwen-Modelle, Gemma)

- [**Abschnitt 4: OpenVINO Toolkit Optimierungssuite**](./Module04/04.openvino.md)
  - Intels Open-Source-Toolkit f√ºr plattform√ºbergreifende KI-Bereitstellung
  - Neural Network Compression Framework (NNCF) f√ºr fortgeschrittene Optimierung
  - OpenVINO GenAI f√ºr die Bereitstellung gro√üer Sprachmodelle
  - Hardwarebeschleunigung √ºber CPU, GPU, VPU und KI-Beschleuniger

- [**Abschnitt 5: Apple MLX Framework im Detail**](./Module04/05.AppleMLX.md)
  - Einheitliche Speicherarchitektur f√ºr Apple Silicon
  - Unterst√ºtzung f√ºr LLaMA, Mistral, Phi-3, ausgew√§hlte Qwen-Modelle
  - LoRA-Feinabstimmung und Modellanpassung
  - Integration mit Hugging Face und 4-Bit/8-Bit-Quantisierung

- [**Abschnitt 6: Synthese des Edge-AI-Entwicklungsworkflows**](./Module04/06.workflow-synthesis.md)
  - Einheitliche Workflow-Architektur, die mehrere Optimierungsframeworks integriert
  - Entscheidungsb√§ume zur Framework-Auswahl und Analyse von Leistungsabstrichen
  - Validierung der Produktionsbereitschaft und umfassende Bereitstellungsstrategien
  - Zukunftssichere Strategien f√ºr aufkommende Hardware- und Modellarchitekturen

---

### [Modul 05: SLMOps - Small Language Model Operations](./Module05/README.md)
**Thema**: Vollst√§ndige SLM-Lebenszyklusoperationen von der Destillation bis zur Produktionsbereitstellung

#### Kapitelstruktur:
- [**Abschnitt 1: Einf√ºhrung in SLMOps**](./Module05/01.IntroduceSLMOps.md)
  - Paradigmenwechsel durch SLMOps in der KI-Operationswelt
  - Kostenersparnis und datenschutzorientierte Architektur
  - Strategische Gesch√§ftsauswirkungen und Wettbewerbsvorteile
  - Herausforderungen und L√∂sungen bei der Implementierung in der Praxis

- [**Abschnitt 2: Modelldestillation - Von der Theorie zur Praxis**](./Module05/02.SLMOps-Distillation.md)
  - Wissenstransfer von Lehrer- zu Sch√ºlermodellen
  - Implementierung des zweistufigen Destillationsprozesses
  - Azure ML-Destillationsworkflows mit praktischen Beispielen
  - 85 % Reduktion der Inferenzzeit bei 92 % Genauigkeitsbeibehaltung

- [**Abschnitt 3: Feinabstimmung - Anpassung von Modellen f√ºr spezifische Aufgaben**](./Module05/03.SLMOps-Finetuing.md)
  - Parameter-effiziente Feinabstimmungstechniken (PEFT)
  - Fortgeschrittene Methoden wie LoRA und QLoRA
  - Implementierung der Feinabstimmung mit Microsoft Olive
  - Multi-Adapter-Training und Hyperparameter-Optimierung

- [**Abschnitt 4: Bereitstellung - Produktionsreife Implementierung**](./Module05/04.SLMOps.Deployment.md)
  - Modellkonvertierung und Quantisierung f√ºr die Produktion
  - Konfiguration der Foundry Local-Bereitstellung
  - Leistungsbenchmarking und Qualit√§tsvalidierung
  - 75 % Gr√∂√üenreduktion mit Produktions√ºberwachung

---

### [Modul 06: SLM Agentensysteme - KI-Agenten und Funktionsaufrufe](./Module06/README.md)
**Thema**: Implementierung von SLM-Agentensystemen von der Basis bis zu fortgeschrittenen Funktionsaufrufen und Model Context Protocol-Integration

#### Kapitelstruktur:
- [**Abschnitt 1: KI-Agenten und Grundlagen kleiner Sprachmodelle**](./Module06/01.IntroduceAgent.md)
  - Klassifikationsrahmen f√ºr Agenten (Reflex-, modellbasierte, zielbasierte, lernende Agenten)
  - Grundlagen und Optimierungsstrategien von SLMs (GGUF, Quantisierung, Edge-Frameworks)
  - Analyse der Abw√§gungen zwischen SLMs und LLMs (10-30√ó Kostenreduktion, 70-80 % Aufgabenwirksamkeit)
  - Praktische Bereitstellung mit Ollama, VLLM und Microsoft Edge-L√∂sungen

- [**Abschnitt 2: Funktionsaufrufe in kleinen Sprachmodellen**](./Module06/02.FunctionCalling.md)
  - Systematische Workflow-Implementierung (Absichtserkennung, JSON-Ausgabe, externe Ausf√ºhrung)
  - Plattform-spezifische Implementierungen (Phi-4-mini, ausgew√§hlte Qwen-Modelle, Microsoft Foundry Local)
  - Fortgeschrittene Beispiele (Zusammenarbeit mehrerer Agenten, dynamische Werkzeugauswahl)
  - Produktions√ºberlegungen (Ratenbegrenzung, Protokollierung, Sicherheitsma√ünahmen)

- [**Abschnitt 3: Integration des Model Context Protocol (MCP)**](./Module06/03.IntroduceMCP.md)
  - Protokollarchitektur und schichtbasiertes Systemdesign
  - Unterst√ºtzung mehrerer Backends (Ollama f√ºr Entwicklung, vLLM f√ºr Produktion)
  - Verbindungsprotokolle (STDIO- und SSE-Modi)
  - Anwendungen in der Praxis (Webautomatisierung, Datenverarbeitung, API-Integration)

---

### [Modul 07: EdgeAI Implementierungsbeispiele](./Module07/README.md)
**Thema**: Umfassende EdgeAI-Implementierungen √ºber verschiedene Plattformen und Frameworks hinweg

#### Kapitelstruktur:
- [**AI-Toolkit f√ºr Visual Studio Code**](./Module07/aitoolkit.md)
  - Umfassende Edge-AI-Entwicklungsumgebung innerhalb von VS Code
  - Modellkatalog und -entdeckung f√ºr Edge-Bereitstellung
  - Lokale Tests, Optimierung und Agentenentwicklungsworkflows
  - Leistungs√ºberwachung und Bewertung f√ºr Edge-Szenarien

- [**Windows EdgeAI Entwicklungsleitfaden**](./Module07/windowdeveloper.md)
  - Umfassender √úberblick √ºber die Windows AI Foundry-Plattform
  - Phi Silica API f√ºr effiziente NPU-Inferenz
  - Computer Vision APIs f√ºr Bildverarbeitung und OCR
  - Foundry Local CLI f√ºr lokale Entwicklung und Tests

- [**EdgeAI in NVIDIA Jetson Orin Nano**](./Module07/README.md#1-edgeai-in-nvidia-jetson-orin-nano)
  - 67 TOPS KI-Leistung in kreditkartengro√üem Formfaktor
  - Unterst√ºtzung generativer KI-Modelle (Vision-Transformer, LLMs, Vision-Language-Modelle)
  - Anwendungen in Robotik, Drohnen, intelligenten Kameras, autonomen Ger√§ten
  - Erschwingliche Plattform f√ºr $249 zur Demokratisierung der KI-Entwicklung

- [**EdgeAI in mobilen Anwendungen mit .NET MAUI und ONNX Runtime GenAI**](./Module07/README.md#2-edgeai-in-mobile-applications-with-net-maui-and-onnx-runtime-genai)
  - Plattform√ºbergreifende mobile KI mit einer einzigen C#-Codebasis
  - Unterst√ºtzung f√ºr Hardwarebeschleunigung (CPU, GPU, mobile KI-Prozessoren)
  - Plattform-spezifische Optimierungen (CoreML f√ºr iOS, NNAPI f√ºr Android)
  - Vollst√§ndige Implementierung des generativen KI-Zyklus

- [**EdgeAI in Azure mit Small Language Models Engine**](./Module07/README.md#3-edgeai-in-azure-with-small-language-models-engine)
  - Hybridarchitektur f√ºr Cloud-Edge-Bereitstellung
  - Integration von Azure AI-Diensten mit ONNX Runtime
  - Bereitstellung im Unternehmensma√üstab und kontinuierliches Modellmanagement
  - Hybride KI-Workflows f√ºr intelligente Dokumentenverarbeitung

- [**EdgeAI mit Windows ML**](./Module07/README.md#4-edgeai-with-windows-ml)
  - Grundlage der Windows AI Foundry f√ºr leistungsstarke On-Device-Inferenz
  - Universelle Hardwareunterst√ºtzung (AMD, Intel, NVIDIA, Qualcomm-Silizium)
  - Automatische Hardwareabstraktion und Optimierung
  - Einheitliches Framework f√ºr diverse Windows-Hardware-√ñkosysteme

- [**EdgeAI mit Foundry Local Anwendungen**](./Module07/README.md#5-edgeai-with-foundry-local-applications)
  - Datenschutzorientierte RAG-Implementierung mit lokalen Ressourcen
  - Integration des Phi-3 Sprachmodells mit semantischer Suche (nur Phi-Modelle)
  - Unterst√ºtzung lokaler Vektordatenbanken (SQLite, Qdrant)
  - Datenhoheit und Offline-Betriebsf√§higkeit

## Kurslernziele

Durch den Abschluss dieses umfassenden EdgeAI-Kurses entwickeln Sie die Expertise, um produktionsreife EdgeAI-L√∂sungen zu entwerfen, zu implementieren und bereitzustellen. Unser strukturierter Ansatz stellt sicher, dass Sie sowohl theoretische Grundlagen als auch praktische Implementierungsf√§higkeiten beherrschen.

### Technische Kompetenzen

**Grundlagenwissen**
- Verstehen der grundlegenden Unterschiede zwischen cloudbasierten und edgebasierten KI-Architekturen
- Beherrschen der Prinzipien der Modellquantisierung, -kompression und -optimierung f√ºr ressourcenbeschr√§nkte Umgebungen
- Verst√§ndnis der Optionen zur Hardwarebeschleunigung (NPUs, GPUs, CPUs) und deren Auswirkungen auf die Bereitstellung

**Implementierungsf√§higkeiten**
- Bereitstellung kleiner Sprachmodelle auf verschiedenen Edge-Plattformen (mobil, eingebettet, IoT, Edge-Server)
- Anwendung von Optimierungsframeworks wie Llama.cpp, Microsoft Olive, ONNX Runtime und Apple MLX
- Implementierung von Echtzeit-Inferenzsystemen mit Reaktionszeiten unter einer Sekunde

**Produktionskompetenz**
- Entwurf skalierbarer EdgeAI-Architekturen f√ºr Unternehmensanwendungen
- Implementierung von √úberwachungs-, Wartungs- und Aktualisierungsstrategien f√ºr bereitgestellte Systeme
- Anwendung von Sicherheitsbest Practices f√ºr datenschutzfreundliche EdgeAI-Implementierungen

### Strategische F√§higkeiten

**Entscheidungsfindungsrahmen**
- Bewertung von EdgeAI-M√∂glichkeiten und Identifizierung geeigneter Anwendungsf√§lle f√ºr Gesch√§ftsanwendungen
- Abw√§gung zwischen Modellgenauigkeit, Inferenzgeschwindigkeit, Stromverbrauch und Hardwarekosten
- Auswahl geeigneter SLM-Familien und Konfigurationen basierend auf spezifischen Bereitstellungsanforderungen

**Systemarchitektur**
- Entwurf von End-to-End-EdgeAI-L√∂sungen, die in bestehende Infrastrukturen integriert sind
- Planung hybrider Edge-Cloud-Architekturen f√ºr optimale Leistung und Kosteneffizienz
- Implementierung von Datenfluss- und Verarbeitungspipelines f√ºr Echtzeit-KI-Anwendungen

### Branchenanwendungen

**Praktische Bereitstellungsszenarien**
- **Fertigung**: Qualit√§tssicherungssysteme, vorausschauende Wartung und Prozessoptimierung
- **Gesundheitswesen**: Datenschutzfreundliche Diagnosetools und Patienten√ºberwachungssysteme
- **Transport**: Entscheidungsfindung f√ºr autonome Fahrzeuge und Verkehrsmanagement
- **Smart Cities**: Intelligente Infrastruktur- und Ressourcenmanagementsysteme
- **Elektronik f√ºr Verbraucher**: KI-gest√ºtzte mobile Anwendungen und Smart-Home-Ger√§te

## √úberblick √ºber die Lernergebnisse

### Modul 01 Lernergebnisse:
- Verstehen der grundlegenden Unterschiede zwischen Cloud- und Edge-KI-Architekturen
- Beherrschen grundlegender Optimierungstechniken f√ºr Edge-Bereitstellungen
- Erkennen von Anwendungen und Erfolgsgeschichten aus der Praxis
- Erwerb praktischer F√§higkeiten zur Implementierung von EdgeAI-L√∂sungen

### Modul 02 Lernergebnisse:
- Tiefes Verst√§ndnis verschiedener SLM-Designphilosophien und deren Auswirkungen auf die Bereitstellung
- Beherrschen strategischer Entscheidungsf√§higkeiten basierend auf Rechenbeschr√§nkungen und Leistungsanforderungen
- Verst√§ndnis der Flexibilit√§tsabstriche bei der Bereitstellung
- Zukunftsorientierte Einblicke in effiziente KI-Architekturen

### Modul 03 Lernergebnisse:
- Strategische Modellwahlf√§higkeiten
- Beherrschen von Optimierungstechniken
- Beherrschen der Flexibilit√§t bei der Bereitstellung
- Produktionsreife Konfigurationsf√§higkeiten

### Modul 04 Lernergebnisse:
- Tiefes Verst√§ndnis von Quantisierungsgrenzen und praktischen Anwendungen
- Praktische Erfahrung mit mehreren Optimierungsframeworks (Llama.cpp, Olive, OpenVINO, MLX)
- Beherrschen der Intel-Hardwareoptimierung mit OpenVINO und NNCF
- Auswahlf√§higkeiten f√ºr hardwarebewusste Optimierungen √ºber verschiedene Plattformen hinweg
- Produktionsbereitstellungsf√§higkeiten f√ºr plattform√ºbergreifende Edge-Computing-Umgebungen
- Strategische Framework-Auswahl und Workflow-Synthese f√ºr optimale EdgeAI-L√∂sungen

### Modul 05 Lernergebnisse:
- Beherrschen des SLMOps-Paradigmas und der Betriebsprinzipien
- Implementierung der Modelldestillation f√ºr Wissenstransfer und Effizienzoptimierung
- Anwendung von Feinabstimmungstechniken f√ºr dom√§nenspezifische Modellanpassung
- Bereitstellung produktionsreifer SLM-L√∂sungen mit √úberwachungs- und Wartungsstrategien

### Modul 06 Lernergebnisse:
- Verst√§ndnis der grundlegenden Konzepte von KI-Agenten und der Architektur kleiner Sprachmodelle
- Beherrschen der Implementierung von Funktionsaufrufen √ºber mehrere Plattformen und Frameworks hinweg
- Integration des Model Context Protocol (MCP) f√ºr standardisierte externe Werkzeuginteraktion
- Aufbau fortschrittlicher Agentensysteme mit minimalen Anforderungen an menschliche Eingriffe

### Modul 07 Lernergebnisse:
- Beherrschen des AI-Toolkits f√ºr Visual Studio Code f√ºr umfassende Edge-AI-Entwicklungsworkflows
- Expertise in der Windows AI Foundry-Plattform und NPU-Optimierungsstrategien
- Praktische Erfahrung mit verschiedenen EdgeAI-Plattformen und Implementierungsstrategien
- Beherrschen hardware-spezifischer Optimierungstechniken √ºber NVIDIA-, mobile-, Azure- und Windows-Plattformen hinweg
- Verst√§ndnis der Bereitstellungsabstriche zwischen Leistung, Kosten und Datenschutzanforderungen
- Entwicklung praktischer F√§higkeiten f√ºr den Aufbau realer EdgeAI-Anwendungen √ºber verschiedene √ñkosysteme hinweg

## Erwartete Kursauswirkungen

Nach erfolgreichem Abschluss dieses Kurses sind Sie mit dem Wissen, den F√§higkeiten und dem Selbstvertrauen ausgestattet, um EdgeAI-Initiativen in professionellen Umgebungen zu leiten.

### Berufliche Einsatzbereitschaft

**Technische F√ºhrung**
- **L√∂sungsarchitektur**: Entwurf umfassender EdgeAI-Systeme, die Unternehmensanforderungen erf√ºllen
- **Leistungsoptimierung**: Erreichen eines optimalen Gleichgewichts zwischen Genauigkeit, Geschwindigkeit und Ressourcenverbrauch
- **Plattform√ºbergreifende Bereitstellung**: Implementierung von L√∂sungen √ºber Windows-, Linux-, mobile- und eingebettete Plattformen hinweg
- **Produktionsbetrieb**: Wartung und Skalierung von EdgeAI-Systemen mit Unternehmenszuverl√§ssigkeit

**Branchenexpertise**
- **Technologiebewertung**: Bewertung und Empfehlung von EdgeAI-L√∂sungen f√ºr spezifische gesch√§ftliche Herausforderungen
- **Implementierungsplanung**: Entwicklung realistischer Zeitpl√§ne und Ressourcenanforderungen f√ºr EdgeAI-Projekte
- **Risikomanagement**: Identifizierung und Minderung technischer und betrieblicher Risiken bei EdgeAI-Bereitstellungen
- **ROI-Optimierung**: Nachweis messbarer Gesch√§ftswerte durch EdgeAI-Implementierungen

### Karrierechancen

**Berufliche Rollen**
- EdgeAI-L√∂sungsarchitekt
- Machine Learning Engineer (Edge-Spezialisierung)
- IoT-KI-Entwickler
- Entwickler mobiler KI-Anwendungen
- Unternehmens-KI-Berater

**Branchen**
- Intelligente Fertigung und Industrie 4.0
- Autonome Fahrzeuge und Transport
- Gesundheitstechnologie und medizinische Ger√§te
- Finanztechnologie und Sicherheit
- Elektronik f√ºr Verbraucher und mobile Anwendungen

### Zertifizierung und Validierung

**Portfolioentwicklung**
- Abschluss von End-to-End-EdgeAI-Projekten, die praktische Kompetenz demonstrieren
- Bereitstellung produktionsreifer L√∂sungen √ºber mehrere Hardwareplattformen hinweg
- Dokumentation von Optimierungsstrategien und erzielten Leistungsverbesserungen

**Kontinuierlicher Lernpfad**
- Grundlage f√ºr fortgeschrittene KI-Spezialisierungen
- Vorbereitung auf hybride Cloud-Edge-Architekturen
- Einstieg in aufkommende KI-Technologien und Frameworks
Dieser Kurs positioniert Sie an der Spitze der KI-Technologie-Einf√ºhrung, bei der intelligente Funktionen nahtlos in die Ger√§te und Systeme integriert werden, die das moderne Leben antreiben.

## Dateistruktur-Baumdiagramm

```
edgeai-for-beginners/
‚îú‚îÄ‚îÄ imgs/
‚îÇ   ‚îî‚îÄ‚îÄ cover.png
‚îú‚îÄ‚îÄ Module01/ (EdgeAI Fundamentals and Transformation)
‚îÇ   ‚îú‚îÄ‚îÄ 01.EdgeAIFundamentals.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.RealWorldCaseStudies.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.PracticalImplementationGuide.md
‚îÇ   ‚îú‚îÄ‚îÄ 04.EdgeDeployment.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module02/ (Small Language Model Foundations)
‚îÇ   ‚îú‚îÄ‚îÄ 01.PhiFamily.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.QwenFamily.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.GemmaFamily.md
‚îÇ   ‚îú‚îÄ‚îÄ 04.BitNETFamily.md
‚îÇ   ‚îú‚îÄ‚îÄ 05.mumodel.md
‚îÇ   ‚îú‚îÄ‚îÄ 06.phisilica.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module03/ (SLM Deployment Practice)
‚îÇ   ‚îú‚îÄ‚îÄ 01.SLMAdvancedLearning.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.DeployingSLMinLocalEnv.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.DeployingSLMinCloud.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module04/ (Model Format Conversion and Quantization)
‚îÇ   ‚îú‚îÄ‚îÄ 01.Introduce.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.Llamacpp.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.MicrosoftOlive.md
‚îÇ   ‚îú‚îÄ‚îÄ 04.openvino.md
‚îÇ   ‚îú‚îÄ‚îÄ 05.AppleMLX.md
‚îÇ   ‚îú‚îÄ‚îÄ 06.workflow-synthesis.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module05/ (SLMOps - Small Language Model Operations)
‚îÇ   ‚îú‚îÄ‚îÄ 01.IntroduceSLMOps.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.SLMOps-Distillation.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.SLMOps-Finetuing.md
‚îÇ   ‚îú‚îÄ‚îÄ 04.SLMOps.Deployment.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module06/ (SLM Agentic Systems)
‚îÇ   ‚îú‚îÄ‚îÄ 01.IntroduceAgent.md
‚îÇ   ‚îú‚îÄ‚îÄ 02.FunctionCalling.md
‚îÇ   ‚îú‚îÄ‚îÄ 03.IntroduceMCP.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Module07/ (EdgeAI Implementation Samples)
‚îÇ   ‚îú‚îÄ‚îÄ aitoolkit.md
‚îÇ   ‚îú‚îÄ‚îÄ windowdeveloper.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md (This file)
‚îú‚îÄ‚îÄ SECURITY.md
‚îú‚îÄ‚îÄ STUDY_GUIDE.md
‚îî‚îÄ‚îÄ SUPPORT.md
```

## Kursmerkmale

- **Progressives Lernen**: Schrittweises Voranschreiten von grundlegenden Konzepten bis hin zur fortgeschrittenen Implementierung
- **Integration von Theorie und Praxis**: Jedes Modul enth√§lt sowohl theoretische Grundlagen als auch praktische Anwendungen
- **Echte Fallstudien**: Basierend auf realen F√§llen von Microsoft, Alibaba, Google und anderen
- **Praktische √úbungen**: Vollst√§ndige Konfigurationsdateien, API-Testverfahren und Bereitstellungsskripte
- **Leistungsbenchmarks**: Detaillierte Vergleiche von Inferenzgeschwindigkeit, Speicherverbrauch und Ressourcenanforderungen
- **Unternehmensrelevante Aspekte**: Sicherheitspraktiken, Compliance-Rahmenwerke und Strategien zum Datenschutz

## Erste Schritte

Empfohlener Lernpfad:
1. Beginnen Sie mit **Modul01**, um ein grundlegendes Verst√§ndnis von EdgeAI aufzubauen
2. Fahren Sie mit **Modul02** fort, um die verschiedenen SLM-Modellfamilien tiefgehend zu verstehen
3. Lernen Sie **Modul03**, um praktische Implementierungsf√§higkeiten zu beherrschen
4. Setzen Sie mit **Modul04** fort, um fortgeschrittene Modelloptimierung, Formatkonvertierung und Framework-Synthese zu erlernen
5. Schlie√üen Sie **Modul05** ab, um SLMOps f√ºr produktionsreife Implementierungen zu meistern
6. Erkunden Sie **Modul06**, um SLM-agentische Systeme und Funktionserweiterungen zu verstehen
7. Beenden Sie mit **Modul07**, um praktische Erfahrungen mit dem AI Toolkit und verschiedenen EdgeAI-Implementierungsbeispielen zu sammeln

Jedes Modul ist so konzipiert, dass es eigenst√§ndig abgeschlossen werden kann, aber ein sequentielles Lernen liefert die besten Ergebnisse.

## Studienleitfaden

Ein umfassender [Studienleitfaden](STUDY_GUIDE.md) steht zur Verf√ºgung, um Ihnen zu helfen, das Beste aus Ihrem Lernerlebnis herauszuholen. Der Studienleitfaden bietet:

- **Strukturierte Lernpfade**: Optimierte Zeitpl√§ne, um den Kurs in 20 Stunden abzuschlie√üen
- **Zeitmanagement-Empfehlungen**: Konkrete Vorschl√§ge, wie Sie Lesen, √úbungen und Projekte ausbalancieren k√∂nnen
- **Fokus auf Schl√ºsselkonzepte**: Priorisierte Lernziele f√ºr jedes Modul
- **Selbstbewertungs-Tools**: Fragen und √úbungen, um Ihr Verst√§ndnis zu testen
- **Mini-Projektideen**: Praktische Anwendungen, um Ihr Lernen zu festigen

Der Studienleitfaden ist sowohl f√ºr intensives Lernen (1 Woche) als auch f√ºr Teilzeitstudium (3 Wochen) ausgelegt und bietet klare Anleitungen, wie Sie Ihre Zeit effektiv einteilen k√∂nnen, selbst wenn Sie nur 10 Stunden f√ºr den Kurs aufbringen k√∂nnen.

---

**Die Zukunft von EdgeAI liegt in der kontinuierlichen Verbesserung von Modellarchitekturen, Quantisierungstechniken und Bereitstellungsstrategien, die Effizienz und Spezialisierung √ºber allgemeine F√§higkeiten priorisieren. Organisationen, die diesen Paradigmenwechsel annehmen, werden gut positioniert sein, um das transformative Potenzial der KI zu nutzen und gleichzeitig die Kontrolle √ºber ihre Daten und Abl√§ufe zu behalten.**

## Weitere Kurse

Unser Team bietet weitere Kurse an! Schauen Sie sich an:

- [MCP for Beginners](https://github.com/microsoft/mcp-for-beginners)
- [AI Agents For Beginners](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using JavaScript](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)
- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)
- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)
- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)
- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)
- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)
- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)
- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)
- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.