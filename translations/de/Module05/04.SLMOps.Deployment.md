<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-17T13:43:10+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "de"
}
-->
# Abschnitt 4: Bereitstellung - Produktionsreife Modellimplementierung

## √úberblick

Dieses umfassende Tutorial f√ºhrt Sie durch den gesamten Prozess der Bereitstellung feinabgestimmter quantisierter Modelle mit Foundry Local. Wir behandeln die Modellkonvertierung, Optimierung durch Quantisierung und die Konfigurationsschritte f√ºr die Bereitstellung von Anfang bis Ende.

## Voraussetzungen

Bevor Sie beginnen, stellen Sie sicher, dass Sie Folgendes haben:

- ‚úÖ Ein feinabgestimmtes ONNX-Modell, das bereit f√ºr die Bereitstellung ist
- ‚úÖ Einen Windows- oder Mac-Computer
- ‚úÖ Python 3.10 oder h√∂her
- ‚úÖ Mindestens 8 GB verf√ºgbarer RAM
- ‚úÖ Foundry Local auf Ihrem System installiert

## Teil 1: Einrichtung der Umgebung

### Installation der erforderlichen Tools

√ñffnen Sie Ihr Terminal (Eingabeaufforderung unter Windows, Terminal unter Mac) und f√ºhren Sie die folgenden Befehle der Reihe nach aus:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

‚ö†Ô∏è **Wichtiger Hinweis**: Sie ben√∂tigen au√üerdem CMake Version 3.31 oder neuer, das Sie von [cmake.org](https://cmake.org/download/) herunterladen k√∂nnen.

## Teil 2: Modellkonvertierung und Quantisierung

### Das richtige Format w√§hlen

F√ºr feinabgestimmte kleine Sprachmodelle empfehlen wir die Verwendung des **ONNX-Formats**, da es folgende Vorteile bietet:

- üöÄ Bessere Leistungsoptimierung
- üîß Hardware-unabh√§ngige Bereitstellung
- üè≠ Produktionsreife F√§higkeiten
- üì± Plattform√ºbergreifende Kompatibilit√§t

### Methode 1: Ein-Befehl-Konvertierung (Empfohlen)

Verwenden Sie den folgenden Befehl, um Ihr feinabgestimmtes Modell direkt zu konvertieren:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Parametererkl√§rung:**
- `--model_name_or_path`: Pfad zu Ihrem feinabgestimmten Modell
- `--device cpu`: CPU f√ºr die Optimierung verwenden
- `--precision int4`: INT4-Quantisierung verwenden (ca. 75 % Gr√∂√üenreduktion)
- `--output_path`: Ausgabepfad f√ºr das konvertierte Modell

### Methode 2: Ansatz mit Konfigurationsdatei (F√ºr Fortgeschrittene)

Erstellen Sie eine Konfigurationsdatei mit dem Namen `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

F√ºhren Sie dann aus:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Vergleich der Quantisierungsoptionen

| Pr√§zision | Dateigr√∂√üe | Inferenzgeschwindigkeit | Modellqualit√§t | Empfohlene Nutzung |
|-----------|------------|-------------------------|----------------|---------------------|
| FP16      | Basiswert √ó 0,5 | Schnell | Beste | Hochleistungs-Hardware |
| INT8      | Basiswert √ó 0,25 | Sehr schnell | Gut | Ausgewogene Wahl |
| INT4      | Basiswert √ó 0,125 | Am schnellsten | Akzeptabel | Ressourcenbeschr√§nkt |

üí° **Empfehlung**: Beginnen Sie mit INT4-Quantisierung f√ºr Ihre erste Bereitstellung. Falls die Qualit√§t nicht zufriedenstellend ist, probieren Sie INT8 oder FP16.

## Teil 3: Foundry Local Bereitstellungskonfiguration

### Erstellung der Modellkonfiguration

Navigieren Sie zum Foundry Local Modellverzeichnis:

```bash
foundry cache cd ./models/
```

Erstellen Sie die Verzeichnisstruktur f√ºr Ihr Modell:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Erstellen Sie die Konfigurationsdatei `inference_model.json` in Ihrem Modellverzeichnis:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Modellspezifische Vorlagenkonfigurationen

#### F√ºr Qwen-Serienmodelle:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Teil 4: Modelltest und Optimierung

### √úberpr√ºfung der Modellinstallation

Pr√ºfen Sie, ob Foundry Local Ihr Modell erkennt:

```bash
foundry cache ls
```

Sie sollten `your-finetuned-model-int4` in der Liste sehen.

### Start des Modelltests

```bash
foundry model run your-finetuned-model-int4
```

### Leistungsbenchmarking

√úberwachen Sie w√§hrend des Tests die folgenden Schl√ºsselmetriken:

1. **Antwortzeit**: Durchschnittliche Zeit pro Antwort messen
2. **Speichernutzung**: RAM-Verbrauch √ºberwachen
3. **CPU-Auslastung**: Prozessorbelastung pr√ºfen
4. **Ausgabequalit√§t**: Relevanz und Koh√§renz der Antworten bewerten

### Qualit√§tsvalidierungs-Checkliste

- ‚úÖ Modell reagiert angemessen auf dom√§nenspezifische Anfragen
- ‚úÖ Antwortformat entspricht der erwarteten Ausgabenstruktur
- ‚úÖ Keine Speicherlecks bei l√§ngerer Nutzung
- ‚úÖ Konsistente Leistung bei unterschiedlichen Eingabel√§ngen
- ‚úÖ Korrekte Handhabung von Randf√§llen und ung√ºltigen Eingaben

## Zusammenfassung

Herzlichen Gl√ºckwunsch! Sie haben erfolgreich abgeschlossen:

- ‚úÖ Konvertierung des feinabgestimmten Modells in das richtige Format
- ‚úÖ Optimierung durch Modellquantisierung
- ‚úÖ Konfiguration der Foundry Local Bereitstellung
- ‚úÖ Leistungstuning und Fehlerbehebung

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.