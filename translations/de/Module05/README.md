<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2db7a2f6e9873c3cd09fea6736bf360b",
  "translation_date": "2025-09-17T13:36:23+00:00",
  "source_file": "Module05/README.md",
  "language_code": "de"
}
-->
# Kapitel 05: SLMOps - Ein umfassender Leitfaden für den Betrieb von Small Language Models

## Überblick

SLMOps (Small Language Model Operations) stellt einen revolutionären Ansatz für die Bereitstellung von KI dar, der Effizienz, Kosteneffektivität und Edge-Computing-Fähigkeiten in den Vordergrund stellt. Dieser umfassende Leitfaden behandelt den gesamten Lebenszyklus von SLM-Operationen, von den grundlegenden Konzepten bis hin zur Implementierung einsatzbereiter Lösungen.

---

## [Abschnitt 1: Einführung in SLMOps](./01.IntroduceSLMOps.md)

**Revolutionierung von KI-Operationen am Edge**

Dieses grundlegende Kapitel stellt den Paradigmenwechsel von traditionellen groß angelegten KI-Operationen hin zu Small Language Model Operations (SLMOps) vor. Sie erfahren, wie SLMOps die entscheidenden Herausforderungen bei der Bereitstellung von KI im großen Maßstab adressiert und dabei Kosteneffizienz und Datenschutz gewährleistet.

**Was Sie lernen werden:**
- Die Entstehung und Bedeutung von SLMOps in der modernen KI-Strategie
- Wie SLMs die Lücke zwischen Leistung und Ressourceneffizienz schließen
- Grundlegende Betriebsprinzipien wie intelligentes Ressourcenmanagement und datenschutzorientierte Architektur
- Herausforderungen bei der Implementierung in der Praxis und deren Lösungen
- Strategische Geschäftsauswirkungen und Wettbewerbsvorteile

**Wichtigste Erkenntnis:** SLMOps demokratisiert die Bereitstellung von KI, indem es fortschrittliche Sprachverarbeitungsfähigkeiten für Organisationen mit begrenzter technischer Infrastruktur zugänglich macht, schnellere Entwicklungszyklen ermöglicht und die Betriebskosten besser vorhersagbar macht.

---

## [Abschnitt 2: Modell-Distillation - Von der Theorie zur Praxis](./02.SLMOps-Distillation.md)

**Effiziente Modelle durch Wissenstransfer erstellen**

Die Modell-Distillation ist die zentrale Technik zur Erstellung kleinerer, effizienterer Modelle, die die Leistung ihrer größeren Gegenstücke beibehalten. Dieses Kapitel bietet eine umfassende Anleitung zur Implementierung von Distillations-Workflows, die Wissen von großen Lehrermodellen auf kompakte Schülermodelle übertragen.

**Was Sie lernen werden:**
- Die grundlegenden Konzepte und Vorteile der Modell-Distillation
- Zwei-Stufen-Distillationsprozess: synthetische Datengenerierung und Training des Schülermodells
- Praktische Implementierungsstrategien mit modernsten Modellen wie DeepSeek V3 und Phi-4-mini
- Azure ML Distillations-Workflows mit praxisnahen Beispielen
- Best Practices für Hyperparameter-Tuning und Evaluierungsstrategien
- Fallstudien aus der Praxis, die erhebliche Kosten- und Leistungsverbesserungen demonstrieren

**Wichtigste Erkenntnis:** Die Modell-Distillation ermöglicht Organisationen eine Reduzierung der Inferenzzeit um 85 % und des Speicherbedarfs um 95 %, während 92 % der ursprünglichen Modellgenauigkeit erhalten bleiben, wodurch fortschrittliche KI-Fähigkeiten praktisch einsetzbar werden.

---

## [Abschnitt 3: Fine-Tuning - Modelle für spezifische Aufgaben anpassen](./03.SLMOps-Finetuing.md)

**Vortrainierte Modelle an Ihre individuellen Anforderungen anpassen**

Fine-Tuning verwandelt allgemeine Modelle in spezialisierte Lösungen, die auf Ihre spezifischen Anwendungsfälle und Domänen zugeschnitten sind. Dieses Kapitel behandelt alles, von der grundlegenden Parameteranpassung bis hin zu fortgeschrittenen Techniken wie LoRA und QLoRA für effiziente Modellanpassung.

**Was Sie lernen werden:**
- Umfassender Überblick über Fine-Tuning-Methoden und deren Anwendungen
- Verschiedene Arten des Fine-Tunings: vollständiges Fine-Tuning, parameter-effizientes Fine-Tuning (PEFT) und aufgabenspezifische Ansätze
- Praktische Implementierung mit Microsoft Olive und praxisnahen Beispielen
- Fortgeschrittene Techniken wie Multi-Adapter-Training und Hyperparameter-Optimierung
- Best Practices für Datenvorbereitung, Trainingskonfiguration und Ressourcenmanagement
- Häufige Herausforderungen und bewährte Lösungen für erfolgreiche Fine-Tuning-Projekte

**Wichtigste Erkenntnis:** Fine-Tuning mit Tools wie Microsoft Olive ermöglicht es Organisationen, vortrainierte Modelle effizient an spezifische Anforderungen anzupassen und dabei Leistung und Ressourcennutzung zu optimieren, wodurch modernste KI für vielfältige Anwendungen zugänglich wird.

---

## [Abschnitt 4: Bereitstellung - Produktionsreife Modellimplementierung](./04.SLMOps.Deployment.md)

**Feinabgestimmte Modelle mit Foundry Local in die Produktion bringen**

Das abschließende Kapitel konzentriert sich auf die entscheidende Bereitstellungsphase und behandelt Modellkonvertierung, Quantisierung und Produktionskonfiguration. Sie lernen, wie Sie feinabgestimmte quantisierte Modelle mit Foundry Local für optimale Leistung und Ressourcennutzung bereitstellen.

**Was Sie lernen werden:**
- Vollständige Einrichtung der Umgebung und Installationsverfahren für Tools
- Modellkonvertierungs- und Quantisierungstechniken für verschiedene Bereitstellungsszenarien
- Foundry Local-Bereitstellungskonfiguration mit modell-spezifischen Optimierungen
- Methoden zur Leistungsbewertung und Qualitätsvalidierung
- Fehlerbehebung bei häufigen Bereitstellungsproblemen und Optimierungsstrategien
- Best Practices für Produktionsüberwachung und Wartung

**Wichtigste Erkenntnis:** Eine ordnungsgemäße Bereitstellungskonfiguration mit Quantisierungstechniken kann eine Größenreduzierung um bis zu 75 % erreichen, während die Modellqualität akzeptabel bleibt, wodurch effiziente Produktionsbereitstellungen auf verschiedenen Hardwarekonfigurationen ermöglicht werden.

---

## Erste Schritte

Dieser Leitfaden soll Sie durch die gesamte SLMOps-Reise führen, von der Vermittlung der grundlegenden Konzepte bis hin zur Implementierung einsatzbereiter Lösungen. Jedes Kapitel baut auf dem vorherigen auf und bietet sowohl theoretisches Verständnis als auch praktische Umsetzungskompetenzen.

Egal, ob Sie ein Data Scientist sind, der die Modellbereitstellung optimieren möchte, ein DevOps-Ingenieur, der KI-Operationen implementiert, oder ein technischer Leiter, der SLMOps für Ihre Organisation bewertet – dieser umfassende Leitfaden bietet das Wissen und die Werkzeuge, die Sie benötigen, um Small Language Model Operations erfolgreich umzusetzen.

**Bereit anzufangen?** Beginnen Sie mit Kapitel 1, um die grundlegenden Prinzipien von SLMOps zu verstehen und Ihre Grundlage für die fortgeschrittenen Implementierungstechniken zu schaffen, die in den folgenden Kapiteln behandelt werden.

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.