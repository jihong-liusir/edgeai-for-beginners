<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef04a48f3f1428fa008738033017e0e8",
  "translation_date": "2025-09-24T11:38:40+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "de"
}
-->
# EdgeAI für Anfänger: Lernpfade und Studienplan

### Intensiver Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 2 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 3 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 4-5 | Modul 4: Modelloptimierung (6 Frameworks) | 4 Stunden |
| Tag 6 | Modul 5: SLMOps | 3 Stunden |
| Tag 7 | Modul 6-7: KI-Agenten & Entwicklungstools | 4 Stunden |
| Tag 8 | Modul 8: Foundry Local Toolkit (Moderne Implementierung) | 1 Stunde |

### Intensiver Lernpfad (2 Wochen)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: KI-Agenten | 2 Stunden |
| Tag 13-14 | Modul 7: Entwicklungstools | 3 Stunden |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: SLM-Agentensysteme | 2 Stunden |
| Tag 13-14 | Modul 7: EdgeAI-Implementierungsbeispiele | 2 Stunden |

| Modul | Abschlussdatum | Aufgewendete Stunden | Wichtige Erkenntnisse |
|-------|----------------|----------------------|-----------------------|
| Modul 1: Grundlagen von EdgeAI | | | |
| Modul 2: SLM-Grundlagen | | | |
| Modul 3: SLM-Bereitstellung | | | |
| Modul 4: Modelloptimierung (6 Frameworks) | | | |
| Modul 5: SLMOps | | | |
| Modul 6: SLM-Agentensysteme | | | |
| Modul 7: EdgeAI-Implementierungsbeispiele | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

## Einführung

Willkommen zum Studienleitfaden für EdgeAI für Anfänger! Dieses Dokument soll Ihnen helfen, die Kursmaterialien effektiv zu nutzen und Ihr Lernerlebnis zu maximieren. Es bietet strukturierte Lernpfade, empfohlene Studienpläne, Zusammenfassungen wichtiger Konzepte und ergänzende Ressourcen, um Ihr Verständnis der EdgeAI-Technologien zu vertiefen.

Dies ist ein kompakter 20-Stunden-Kurs, der essenzielles Wissen über EdgeAI in einem zeiteffizienten Format vermittelt – ideal für vielbeschäftigte Fachleute und Studierende, die schnell praktische Fähigkeiten in diesem aufstrebenden Bereich erwerben möchten.

## Kursübersicht

Der Kurs ist in sieben umfassende Module unterteilt:

1. **Grundlagen und Transformation von EdgeAI** – Verständnis der Kernkonzepte und des technologischen Wandels
2. **Grundlagen von Small Language Models (SLM)** – Erforschung verschiedener SLM-Familien und ihrer Architekturen
3. **Bereitstellung von Small Language Models** – Praktische Implementierungsstrategien
4. **Modellformatkonvertierung und Quantisierung** – Fortgeschrittene Optimierung mit 6 Frameworks, einschließlich OpenVINO
5. **SLMOps – Betrieb von Small Language Models** – Produktionslebenszyklusmanagement und Bereitstellung
6. **SLM-Agentensysteme** – KI-Agenten, Funktionsaufrufe und Model Context Protocol
7. **EdgeAI-Implementierungsbeispiele** – KI-Toolkit, Windows-Entwicklung und plattformspezifische Implementierungen
8. **Microsoft Foundry Local – Komplettes Entwickler-Toolkit** – Lokale Entwicklung mit hybrider Azure-Integration (Modul 08)

## So nutzen Sie diesen Studienleitfaden

- **Progressives Lernen**: Folgen Sie den Modulen in der vorgegebenen Reihenfolge für ein kohärentes Lernerlebnis.
- **Wissenscheckpunkte**: Nutzen Sie die Selbstbewertungsfragen nach jedem Abschnitt.
- **Praktische Übungen**: Schließen Sie die vorgeschlagenen Übungen ab, um theoretische Konzepte zu festigen.
- **Ergänzende Ressourcen**: Erkunden Sie zusätzliche Materialien zu Themen, die Sie besonders interessieren.

## Empfehlungen für den Studienplan

### Intensiver Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 6 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 8 Stunden |
| Tag 5 | Modul 3: SLM-Bereitstellung | 3 Stunden |
| Tag 6 | Modul 8: Foundry Local Toolkit | 3 Stunden |

### Teilzeitstudium (3 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1: Grundlagen von EdgeAI | 6-7 Stunden |
| Woche 2 | Modul 2: Grundlagen von SLM | 7-8 Stunden |
| Woche 3 | Modul 3: SLM-Bereitstellung (3h) + Modul 8: Foundry Local Toolkit (2-3h) | 5-6 Stunden |

## Modul 1: Grundlagen und Transformation von EdgeAI

### Wichtige Lernziele

- Unterschiede zwischen cloudbasiertem und edgebasiertem KI verstehen
- Kernoptimierungstechniken für ressourcenbeschränkte Umgebungen beherrschen
- Analyse von realen Anwendungen der EdgeAI-Technologien
- Einrichtung einer Entwicklungsumgebung für EdgeAI-Projekte

### Studienbereiche

#### Abschnitt 1: Grundlagen von EdgeAI
- **Wichtige Konzepte**: 
  - Paradigmen von Edge- vs. Cloud-Computing
  - Modellquantisierungstechniken
  - Hardware-Beschleunigungsoptionen (NPUs, GPUs, CPUs)
  - Vorteile in Bezug auf Datenschutz und Sicherheit

- **Ergänzende Materialien**:
  - [TensorFlow Lite Dokumentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Dokumentation](https://docs.edgeimpulse.com)

#### Abschnitt 2: Fallstudien aus der Praxis
- **Wichtige Konzepte**: 
  - Microsoft Phi & Mu Modell-Ökosystem
  - Praktische Implementierungen in verschiedenen Branchen
  - Bereitstellungsüberlegungen

#### Abschnitt 3: Praktischer Implementierungsleitfaden
- **Wichtige Konzepte**: 
  - Einrichtung der Entwicklungsumgebung
  - Quantisierungs- und Optimierungstools
  - Bewertungsmethoden für EdgeAI-Implementierungen

#### Abschnitt 4: Hardware für Edge-Bereitstellung
- **Wichtige Konzepte**: 
  - Vergleich von Hardwareplattformen
  - Optimierungsstrategien für spezifische Hardware
  - Bereitstellungsüberlegungen

### Selbstbewertungsfragen

1. Vergleichen Sie cloudbasierte KI mit edgebasierter KI-Implementierung.
2. Erklären Sie drei Schlüsseltechniken zur Optimierung von Modellen für die Edge-Bereitstellung.
3. Was sind die Hauptvorteile der Ausführung von KI-Modellen am Edge?
4. Beschreiben Sie den Prozess der Modellquantisierung und wie er die Leistung beeinflusst.
5. Erklären Sie, wie verschiedene Hardware-Beschleuniger (NPUs, GPUs, CPUs) die EdgeAI-Bereitstellung beeinflussen.

### Praktische Übungen

1. **Schnelle Einrichtung der Umgebung**: Konfigurieren Sie eine minimale Entwicklungsumgebung mit den wesentlichen Paketen (30 Minuten)
2. **Modelluntersuchung**: Laden Sie ein vortrainiertes Small Language Model herunter und analysieren Sie es (1 Stunde)
3. **Grundlegende Quantisierung**: Probieren Sie einfache Quantisierung an einem kleinen Modell aus (1 Stunde)

## Modul 2: Grundlagen von Small Language Models

### Wichtige Lernziele

- Verständnis der Architekturprinzipien verschiedener SLM-Familien
- Vergleich der Modellfähigkeiten über verschiedene Parametergrößen hinweg
- Bewertung von Modellen basierend auf Effizienz, Fähigkeiten und Bereitstellungsanforderungen
- Erkennen geeigneter Anwendungsfälle für verschiedene Modellfamilien

### Studienbereiche

#### Abschnitt 1: Microsoft Phi Modellfamilie
- **Wichtige Konzepte**: 
  - Entwicklung der Designphilosophie
  - Architektur mit Fokus auf Effizienz
  - Spezialisierte Fähigkeiten

#### Abschnitt 2: Qwen Familie
- **Wichtige Konzepte**: 
  - Beiträge aus der Open-Source-Community
  - Skalierbare Bereitstellungsoptionen
  - Architektur für fortgeschrittenes logisches Denken

#### Abschnitt 3: Gemma Familie
- **Wichtige Konzepte**: 
  - Innovationsgetriebene Forschung
  - Multimodale Fähigkeiten
  - Optimierung für mobile Geräte

#### Abschnitt 4: BitNET Familie
- **Wichtige Konzepte**: 
  - 1-Bit-Quantisierungstechnologie
  - Framework für Inferenzoptimierung
  - Nachhaltigkeitsüberlegungen

#### Abschnitt 5: Microsoft Mu Modell
- **Wichtige Konzepte**: 
  - Geräteorientierte Architektur
  - Systemintegration mit Windows
  - Datenschutzfreundlicher Betrieb

#### Abschnitt 6: Phi-Silica
- **Wichtige Konzepte**: 
  - NPU-optimierte Architektur
  - Leistungskennzahlen
  - Entwicklerintegration

### Selbstbewertungsfragen

1. Vergleichen Sie die Architekturansätze der Phi- und Qwen-Modellfamilien.
2. Erklären Sie, wie sich die Quantisierungstechnologie von BitNET von herkömmlicher Quantisierung unterscheidet.
3. Was sind die einzigartigen Vorteile des Mu-Modells für die Windows-Integration?
4. Beschreiben Sie, wie Phi-Silica NPU-Hardware für Leistungsoptimierung nutzt.
5. Für eine mobile Anwendung mit eingeschränkter Konnektivität: Welche Modellfamilie wäre am besten geeignet und warum?

### Praktische Übungen

1. **Modellvergleich**: Schneller Benchmark von zwei verschiedenen SLM-Modellen (1 Stunde)
2. **Einfache Textgenerierung**: Grundlegende Implementierung der Textgenerierung mit einem kleinen Modell (1 Stunde)
3. **Schnelle Optimierung**: Anwenden einer Optimierungstechnik zur Verbesserung der Inferenzgeschwindigkeit (1 Stunde)

## Modul 3: Bereitstellung von Small Language Models

### Wichtige Lernziele

- Auswahl geeigneter Modelle basierend auf Bereitstellungsbeschränkungen
- Beherrschung von Optimierungstechniken für verschiedene Bereitstellungsszenarien
- Implementierung von SLMs in lokalen und Cloud-Umgebungen
- Gestaltung produktionsreifer Konfigurationen für EdgeAI-Anwendungen

### Studienbereiche

#### Abschnitt 1: Fortgeschrittenes Lernen mit SLM
- **Wichtige Konzepte**: 
  - Parameterklassifizierungsrahmen
  - Fortgeschrittene Optimierungstechniken
  - Strategien zur Modellbeschaffung

#### Abschnitt 2: Lokale Bereitstellung
- **Wichtige Konzepte**: 
  - Bereitstellung auf der Ollama-Plattform
  - Microsoft Foundry Local Lösungen
  - Vergleichende Analyse von Frameworks

#### Abschnitt 3: Containerisierte Cloud-Bereitstellung
- **Wichtige Konzepte**: 
  - vLLM Hochleistungsinferenz
  - Container-Orchestrierung
  - Implementierung von ONNX Runtime

### Selbstbewertungsfragen

1. Welche Faktoren sollten bei der Auswahl zwischen lokaler Bereitstellung und Cloud-Bereitstellung berücksichtigt werden?
2. Vergleichen Sie Ollama und Microsoft Foundry Local als Bereitstellungsoptionen.
3. Erklären Sie die Vorteile der Containerisierung für die SLM-Bereitstellung.
4. Welche Leistungskennzahlen sollten für ein Edge-bereitgestelltes SLM überwacht werden?
5. Beschreiben Sie einen vollständigen Bereitstellungsworkflow von der Modellauswahl bis zur Produktionsimplementierung.

### Praktische Übungen

1. **Grundlegende lokale Bereitstellung**: Bereitstellung eines einfachen SLM mit Ollama (1 Stunde)
2. **Leistungsüberprüfung**: Schneller Benchmark Ihres bereitgestellten Modells (30 Minuten)
3. **Einfache Integration**: Erstellen Sie eine minimale Anwendung, die Ihr bereitgestelltes Modell verwendet (1 Stunde)

## Modul 4: Modellformatkonvertierung und Quantisierung

### Wichtige Lernziele

- Beherrschung fortgeschrittener Quantisierungstechniken von 1-Bit bis 8-Bit Präzision
- Verständnis von Strategien zur Formatkonvertierung (GGUF, ONNX)
- Implementierung von Optimierungen über sechs Frameworks (Llama.cpp, Olive, OpenVINO, MLX, Workflow-Synthese)
- Bereitstellung optimierter Modelle für produktive Edge-Umgebungen auf Intel-, Apple- und plattformübergreifender Hardware

### Studienbereiche

#### Abschnitt 1: Grundlagen der Quantisierung
- **Wichtige Konzepte**: 
  - Präzisionsklassifizierungsrahmen
  - Abwägung zwischen Leistung und Genauigkeit
  - Optimierung des Speicherbedarfs

#### Abschnitt 2: Llama.cpp Implementierung
- **Wichtige Konzepte**: 
  - Plattformübergreifende Bereitstellung
  - GGUF-Formatoptimierung
  - Hardware-Beschleunigungstechniken

#### Abschnitt 3: Microsoft Olive Suite
- **Wichtige Konzepte**: 
  - Hardwarebewusste Optimierung
  - Unternehmensgerechte Bereitstellung
  - Automatisierte Optimierungsworkflows

#### Abschnitt 4: OpenVINO Toolkit
- **Wichtige Konzepte**: 
  - Intel-Hardwareoptimierung
  - Neural Network Compression Framework (NNCF)
  - Plattformübergreifende Inferenzbereitstellung
- OpenVINO GenAI für LLM-Bereitstellung

#### Abschnitt 5: Apple MLX Framework
- **Wichtige Konzepte**: 
  - Optimierung für Apple Silicon
  - Einheitliche Speicherarchitektur
  - LoRA-Fine-Tuning-Funktionen

#### Abschnitt 6: Workflow-Synthese für Edge AI-Entwicklung
- **Wichtige Konzepte**: 
  - Einheitliche Workflow-Architektur
  - Entscheidungsbäume für Framework-Auswahl
  - Validierung der Produktionsbereitschaft
  - Strategien zur Zukunftssicherung

### Selbstbewertungsfragen

1. Vergleichen Sie Quantisierungsstrategien über verschiedene Präzisionsstufen (1-Bit bis 8-Bit).
2. Erklären Sie die Vorteile des GGUF-Formats für Edge-Bereitstellungen.
3. Wie verbessert hardwarebewusste Optimierung in Microsoft Olive die Effizienz der Bereitstellung?
4. Was sind die Hauptvorteile von OpenVINOs NNCF für die Modellkompression?
5. Beschreiben Sie, wie Apple MLX die einheitliche Speicherarchitektur für Optimierungen nutzt.
6. Wie hilft die Workflow-Synthese bei der Auswahl optimaler Optimierungs-Frameworks?

### Praktische Übungen

1. **Modell-Quantisierung**: Wenden Sie verschiedene Quantisierungsstufen auf ein Modell an und vergleichen Sie die Ergebnisse (1 Stunde)
2. **OpenVINO-Optimierung**: Nutzen Sie NNCF, um ein Modell für Intel-Hardware zu komprimieren (1 Stunde)
3. **Framework-Vergleich**: Testen Sie dasselbe Modell in drei verschiedenen Optimierungs-Frameworks (1 Stunde)
4. **Leistungs-Benchmarking**: Messen Sie den Einfluss der Optimierung auf die Inferenzgeschwindigkeit und den Speicherverbrauch (1 Stunde)

## Modul 5: SLMOps - Betrieb von Small Language Models

### Wichtige Lernziele

- Verstehen der Prinzipien des Lebenszyklusmanagements von SLMOps
- Beherrschen von Distillations- und Fine-Tuning-Techniken für Edge-Bereitstellungen
- Implementieren von Produktionsstrategien mit Monitoring
- Aufbau von unternehmensgerechten Workflows für Betrieb und Wartung von SLMs

### Studienfokusbereiche

#### Abschnitt 1: Einführung in SLMOps
- **Wichtige Konzepte**: 
  - Paradigmenwechsel von SLMOps in der KI-Betriebsführung
  - Kosten- und Datenschutzorientierte Architektur
  - Strategische Geschäftsauswirkungen und Wettbewerbsvorteile

#### Abschnitt 2: Modell-Distillation
- **Wichtige Konzepte**: 
  - Techniken des Wissenstransfers
  - Implementierung des zweistufigen Distillationsprozesses
  - Distillations-Workflows mit Azure ML

#### Abschnitt 3: Fine-Tuning-Strategien
- **Wichtige Konzepte**: 
  - Parameter-effizientes Fine-Tuning (PEFT)
  - Fortgeschrittene Methoden wie LoRA und QLoRA
  - Multi-Adapter-Training und Hyperparameter-Optimierung

#### Abschnitt 4: Produktionsbereitstellung
- **Wichtige Konzepte**: 
  - Modellkonvertierung und Quantisierung für die Produktion
  - Konfiguration der Foundry Local-Bereitstellung
  - Leistungs-Benchmarking und Qualitätsvalidierung

### Selbstbewertungsfragen

1. Wie unterscheidet sich SLMOps von traditionellem MLOps?
2. Erklären Sie die Vorteile der Modell-Distillation für Edge-Bereitstellungen.
3. Was sind die wichtigsten Überlegungen für das Fine-Tuning von SLMs in ressourcenbeschränkten Umgebungen?
4. Beschreiben Sie eine vollständige Produktionsbereitstellungspipeline für Edge-KI-Anwendungen.

### Praktische Übungen

1. **Grundlegende Distillation**: Erstellen Sie ein kleineres Modell aus einem größeren Lehrermodell (1 Stunde)
2. **Fine-Tuning-Experiment**: Fine-Tunen Sie ein Modell für einen spezifischen Anwendungsbereich (1 Stunde)
3. **Bereitstellungspipeline**: Richten Sie eine grundlegende CI/CD-Pipeline für die Modellbereitstellung ein (1 Stunde)

## Modul 6: SLM Agentic Systems - KI-Agenten und Funktionsaufrufe

### Wichtige Lernziele

- Intelligente KI-Agenten für Edge-Umgebungen mit Small Language Models entwickeln
- Funktionsaufruf-Fähigkeiten mit systematischen Workflows implementieren
- Model Context Protocol (MCP) für standardisierte Tool-Interaktion integrieren
- Komplexe agentische Systeme mit minimaler menschlicher Intervention erstellen

### Studienfokusbereiche

#### Abschnitt 1: KI-Agenten und SLM-Grundlagen
- **Wichtige Konzepte**: 
  - Klassifikationsrahmen für Agenten (Reflex-, Modell-, Ziel-, Lernagenten)
  - Analyse der Kompromisse zwischen SLM und LLM
  - Edge-spezifische Designmuster für Agenten
  - Ressourcenoptimierung für Agenten

#### Abschnitt 2: Funktionsaufrufe in Small Language Models
- **Wichtige Konzepte**: 
  - Systematische Workflow-Implementierung (Intent-Erkennung, JSON-Ausgabe, externe Ausführung)
  - Plattform-spezifische Implementierungen (Phi-4-mini, ausgewählte Qwen-Modelle, Microsoft Foundry Local)
  - Fortgeschrittene Beispiele (Multi-Agent-Zusammenarbeit, dynamische Tool-Auswahl)
  - Produktionsüberlegungen (Rate-Limiting, Audit-Logging, Sicherheitsmaßnahmen)

#### Abschnitt 3: Model Context Protocol (MCP) Integration
- **Wichtige Konzepte**: 
  - Protokollarchitektur und schichtbasiertes Systemdesign
  - Unterstützung für mehrere Backends (Ollama für Entwicklung, vLLM für Produktion)
  - Verbindungsprotokolle (STDIO- und SSE-Modi)
  - Anwendungen in der Praxis (Web-Automatisierung, Datenverarbeitung, API-Integration)

### Selbstbewertungsfragen

1. Was sind die wichtigsten architektonischen Überlegungen für Edge-KI-Agenten?
2. Wie verbessern Funktionsaufrufe die Fähigkeiten von Agenten?
3. Erklären Sie die Rolle des Model Context Protocols in der Agentenkommunikation.

### Praktische Übungen

1. **Einfacher Agent**: Erstellen Sie einen grundlegenden KI-Agenten mit Funktionsaufrufen (1 Stunde)
2. **MCP-Integration**: Implementieren Sie MCP in einer Agentenanwendung (30 Minuten)

## Modul 7: EdgeAI Implementierungsbeispiele

### Wichtige Lernziele

- Beherrschen des AI Toolkits für Visual Studio Code für umfassende EdgeAI-Entwicklungs-Workflows
- Expertise in der Windows AI Foundry-Plattform und NPU-Optimierungsstrategien gewinnen
- EdgeAI auf verschiedenen Hardwareplattformen und Bereitstellungsszenarien implementieren
- Produktionsreife EdgeAI-Anwendungen mit plattformspezifischen Optimierungen erstellen

### Studienfokusbereiche

#### Abschnitt 1: AI Toolkit für Visual Studio Code
- **Wichtige Konzepte**: 
  - Umfassende Edge-KI-Entwicklungsumgebung innerhalb von VS Code
  - Modellkatalog und -entdeckung für Edge-Bereitstellungen
  - Lokale Test-, Optimierungs- und Agentenentwicklungs-Workflows
  - Leistungsüberwachung und Bewertung für Edge-Szenarien

#### Abschnitt 2: Windows EdgeAI Entwicklungsleitfaden
- **Wichtige Konzepte**: 
  - Umfassender Überblick über die Windows AI Foundry-Plattform
  - Phi Silica API für effiziente NPU-Inferenz
  - Computer Vision APIs für Bildverarbeitung und OCR
  - Foundry Local CLI für lokale Entwicklung und Tests

#### Abschnitt 3: Plattform-spezifische Implementierungen
- **Wichtige Konzepte**: 
  - NVIDIA Jetson Orin Nano-Bereitstellung (67 TOPS KI-Leistung)
  - Mobile Anwendungen mit .NET MAUI und ONNX Runtime GenAI
  - Azure EdgeAI-Lösungen mit Cloud-Edge-Hybridarchitektur
  - Windows ML-Optimierung mit universeller Hardwareunterstützung
  - Foundry Local-Anwendungen mit datenschutzorientierter RAG-Implementierung

### Selbstbewertungsfragen

1. Wie vereinfacht das AI Toolkit den EdgeAI-Entwicklungsworkflow?
2. Vergleichen Sie Bereitstellungsstrategien auf verschiedenen Hardwareplattformen.
3. Was sind die Vorteile der Windows AI Foundry für die Edge-Entwicklung?
4. Erklären Sie die Rolle der NPU-Optimierung in modernen Edge-KI-Anwendungen.
5. Wie nutzt die Phi Silica API NPU-Hardware für Leistungsoptimierung?
6. Vergleichen Sie die Vorteile von lokaler vs. Cloud-Bereitstellung für datenschutzsensible Anwendungen.

### Praktische Übungen

1. **AI Toolkit Setup**: Konfigurieren Sie das AI Toolkit und optimieren Sie ein Modell (1 Stunde)
2. **Windows AI Foundry**: Erstellen Sie eine einfache Windows-KI-Anwendung mit der Phi Silica API (1 Stunde)
3. **Cross-Plattform-Bereitstellung**: Bereitstellen Sie dasselbe Modell auf zwei verschiedenen Plattformen (1 Stunde)
4. **NPU-Optimierung**: Testen Sie die NPU-Leistung mit Windows AI Foundry-Tools (30 Minuten)

## Modul 8: Microsoft Foundry Local – Komplettes Entwickler-Toolkit (Modernisiert)

### Wichtige Lernziele

- Foundry Local mit moderner SDK-Integration installieren und konfigurieren
- Fortgeschrittene Multi-Agent-Systeme mit Koordinator-Mustern implementieren
- Intelligente Modell-Router mit automatischer aufgabenbasierter Auswahl erstellen
- Produktionsreife KI-Lösungen mit umfassendem Monitoring bereitstellen
- Integration mit Azure AI Foundry für hybride Bereitstellungsszenarien
- Moderne SDK-Muster mit FoundryLocalManager und OpenAI-Client beherrschen

### Studienfokusbereiche

#### Abschnitt 1: Moderne Installation und Konfiguration
- **Wichtige Konzepte**: 
  - FoundryLocalManager SDK-Integration
  - Automatische Dienstentdeckung und Gesundheitsüberwachung
  - Konfigurationsmuster basierend auf Umgebungen
  - Überlegungen zur Produktionsbereitstellung

#### Abschnitt 2: Fortgeschrittene Multi-Agent-Systeme
- **Wichtige Konzepte**: 
  - Koordinator-Muster mit spezialisierten Agenten
  - Spezialisierung auf Abruf-, Argumentations- und Ausführungsagenten
  - Feedback-Mechanismen zur Verfeinerung
  - Leistungsüberwachung und Statistik-Tracking

#### Abschnitt 3: Intelligente Modell-Routing
- **Wichtige Konzepte**: 
  - Schlüsselwortbasierte Modell-Auswahlalgorithmen
  - Unterstützung für mehrere Modelle (allgemein, Argumentation, Code, kreativ)
  - Konfiguration von Umgebungsvariablen für Flexibilität
  - Dienstgesundheitsprüfung und Fehlerbehandlung

#### Abschnitt 4: Produktionsreife Implementierung
- **Wichtige Konzepte**: 
  - Umfassende Fehlerbehandlung und Fallback-Mechanismen
  - Anfragen-Monitoring und Leistungs-Tracking
  - Interaktive Jupyter-Notebook-Beispiele mit Benchmarks
  - Integrationsmuster mit bestehenden Anwendungen

### Selbstbewertungsfragen

1. Wie unterscheidet sich der moderne Ansatz von FoundryLocalManager von manuellen REST-Aufrufen?
2. Erklären Sie das Koordinator-Muster und wie es spezialisierte Agenten orchestriert.
3. Wie wählt der intelligente Router geeignete Modelle basierend auf dem Inhalt der Anfrage aus?
4. Was sind die Hauptkomponenten eines produktionsreifen KI-Agentensystems?
5. Wie implementieren Sie umfassendes Gesundheitsmonitoring für Foundry Local-Dienste?
6. Vergleichen Sie die Vorteile des modernisierten Ansatzes gegenüber traditionellen Implementierungsmustern.

### Praktische Übungen

1. **Modernes SDK-Setup**: Konfigurieren Sie FoundryLocalManager mit automatischer Dienstentdeckung (30 Minuten)
2. **Multi-Agent-System**: Führen Sie den fortgeschrittenen Koordinator mit spezialisierten Agenten aus (30 Minuten)
3. **Intelligentes Routing**: Testen Sie den Modell-Router mit verschiedenen Anfragearten (30 Minuten)
4. **Interaktive Erkundung**: Nutzen Sie die Jupyter-Notebooks, um fortgeschrittene Funktionen zu erkunden (45 Minuten)
5. **Produktionsbereitstellung**: Implementieren Sie Monitoring- und Fehlerbehandlungsmuster (30 Minuten)
6. **Hybride Integration**: Konfigurieren Sie Azure AI Foundry-Fallback-Szenarien (30 Minuten)

## Zeitplan-Leitfaden

Um die 20 Stunden Kurszeit optimal zu nutzen, hier eine empfohlene Aufteilung:

| Aktivität | Zeitaufwand | Beschreibung |
|-----------|-------------|--------------|
| Lesen der Kernmaterialien | 9 Stunden | Fokus auf die wesentlichen Konzepte in jedem Modul |
| Praktische Übungen | 6 Stunden | Praktische Umsetzung der wichtigsten Techniken |
| Selbstbewertung | 2 Stunden | Testen des Verständnisses durch Fragen und Reflexion |
| Mini-Projekt | 3 Stunden | Anwendung des Wissens in einer kleinen praktischen Umsetzung |

### Fokusbereiche nach Zeitbeschränkung

**Wenn Sie nur 10 Stunden haben:**
- Abschließen der Module 1, 2 und 3 (Kernkonzepte von EdgeAI)
- Mindestens eine praktische Übung pro Modul durchführen
- Fokus auf das Verständnis der Kernkonzepte statt auf Implementierungsdetails

**Wenn Sie die vollen 20 Stunden investieren können:**
- Alle sieben Module abschließen
- Wichtige praktische Übungen aus jedem Modul durchführen
- Ein Mini-Projekt aus Modul 7 abschließen
- Mindestens 2-3 ergänzende Ressourcen erkunden

**Wenn Sie mehr als 20 Stunden haben:**
- Alle Module mit detaillierten Übungen abschließen
- Mehrere Mini-Projekte erstellen
- Fortgeschrittene Optimierungstechniken in Modul 4 erkunden
- Produktionsbereitstellung aus Modul 5 implementieren

## Wichtige Ressourcen

Diese sorgfältig ausgewählten Ressourcen bieten den größten Nutzen für Ihre begrenzte Lernzeit:

### Unverzichtbare Dokumentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Das effizienteste Modelloptimierungstool
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Schnellster Weg zur lokalen Bereitstellung von SLMs
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referenz für ein führendes Edge-optimiertes Modell
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intels umfassendes Optimierungstoolkit
- [AI Toolkit für VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrierte EdgeAI-Entwicklungsumgebung
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-spezifische EdgeAI-Entwicklungsplattform

### Zeitersparende Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Schneller Zugriff auf Modelle und Bereitstellung
- [Gradio](https://www.gradio.app/docs/interface) - Schnelle UI-Entwicklung für KI-Demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Vereinfachte Modelloptimierung
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Effiziente CPU-Inferenz
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework für neuronale Netzwerkkompression
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit für die Bereitstellung großer Sprachmodelle

## Fortschritts-Tracking-Vorlage

Nutzen Sie diese vereinfachte Vorlage, um Ihren Lernfortschritt im 20-Stunden-Kurs zu verfolgen:

| Modul | Abschlussdatum | Zeitaufwand | Wichtige Erkenntnisse |
|-------|----------------|-------------|-----------------------|
| Modul 1: EdgeAI Grundlagen | | | |
| Modul 2: SLM Grundlagen | | | |
| Modul 3: SLM Bereitstellung | | | |
| Modul 4: Modelloptimierung | | | |
| Modul 5: SLMOps | | | |
| Modul 6: KI-Agenten | | | |
| Modul 7: Entwicklungstools | | | |
| Modul 8: Foundry Local Toolkit | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

## Mini-Projektideen

Überlege, eines dieser Projekte abzuschließen, um EdgeAI-Konzepte zu üben (jedes ist so konzipiert, dass es 2-4 Stunden dauert):

### Anfängerprojekte (jeweils 2-3 Stunden)
1. **Edge Text Assistant**: Erstelle ein einfaches Offline-Tool zur Textvervollständigung mit einem kleinen Sprachmodell
2. **Modellvergleichs-Dashboard**: Baue eine grundlegende Visualisierung von Leistungsmetriken verschiedener SLMs
3. **Optimierungsexperiment**: Messe die Auswirkungen unterschiedlicher Quantisierungsstufen auf dasselbe Basismodell

### Projekte für Fortgeschrittene (jeweils 3-4 Stunden)
4. **AI Toolkit Workflow**: Nutze das VS Code AI Toolkit, um ein Modell von Anfang bis Ende zu optimieren und bereitzustellen
5. **Windows AI Foundry Anwendung**: Erstelle eine Windows-App mit Phi Silica API und NPU-Optimierung
6. **Plattformübergreifende Bereitstellung**: Stelle dasselbe optimierte Modell auf Windows (OpenVINO) und Mobilgeräten (.NET MAUI) bereit
7. **Funktionsaufruf-Agent**: Baue einen KI-Agenten mit Funktionsaufruf-Fähigkeiten für Edge-Szenarien

### Projekte zur fortgeschrittenen Integration (jeweils 4-5 Stunden)
8. **OpenVINO-Optimierungspipeline**: Implementiere eine vollständige Modelloptimierung mit NNCF und GenAI Toolkit
9. **SLMOps-Pipeline**: Implementiere einen vollständigen Modelllebenszyklus von der Schulung bis zur Edge-Bereitstellung
10. **Multi-Modell-Edge-System**: Stelle mehrere spezialisierte Modelle bereit, die zusammen auf Edge-Hardware arbeiten
11. **MCP-Integrationssystem**: Baue ein agentenbasiertes System mit Model Context Protocol für Werkzeuginteraktionen

## Referenzen

- Microsoft Learn (Foundry Local)
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Erste Schritte: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - CLI-Referenz: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integration mit Inferenz-SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Open WebUI Anleitung: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Hugging Face Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenten (Übersicht): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Optimierungs- und Inferenz-Tools
  - Microsoft Olive (Dokumentation): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (Erste Schritte): https://onnxruntime.ai/docs/get-started/with-python.html
  - ONNX Runtime Olive Integration: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (Dokumentation): https://docs.openvino.ai/2025/index.html
  - Apple MLX (Dokumentation): https://ml-explore.github.io/mlx/build/html/index.html
- Bereitstellungs-Frameworks und Modelle
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (Dokumentation): https://docs.vllm.ai/
  - Ollama (Schnellstart): https://github.com/ollama/ollama#get-started
- Entwickler-Tools (Windows und VS Code)
  - AI Toolkit für VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (Übersicht): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Lern-Community

Tritt der Diskussion bei und vernetze dich mit anderen Lernenden:
- GitHub-Diskussionen im [EdgeAI for Beginners Repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Fazit

EdgeAI repräsentiert die Zukunft der Implementierung künstlicher Intelligenz, indem leistungsstarke Fähigkeiten direkt auf Geräte gebracht werden und gleichzeitig wichtige Themen wie Datenschutz, Latenz und Konnektivität adressiert werden. Dieser 20-stündige Kurs vermittelt dir das grundlegende Wissen und die praktischen Fähigkeiten, um sofort mit EdgeAI-Technologien zu arbeiten.

Der Kurs ist bewusst prägnant und konzentriert sich auf die wichtigsten Konzepte, sodass du schnell wertvolle Expertise erlangen kannst, ohne eine überwältigende Zeitinvestition. Denke daran, dass praktische Übungen, selbst mit einfachen Beispielen, der Schlüssel sind, um das Gelernte zu festigen.

Viel Erfolg beim Lernen!

---

