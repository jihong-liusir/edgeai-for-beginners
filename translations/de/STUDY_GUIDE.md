<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-10-08T20:36:10+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "de"
}
-->
# EdgeAI für Anfänger: Lernpfade und Studienplan

### Intensiver Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 2 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 3 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 4-5 | Modul 4: Modelloptimierung (6 Frameworks) | 4 Stunden |
| Tag 6 | Modul 5: SLMOps | 3 Stunden |
| Tag 7 | Modul 6-7: KI-Agenten & Entwicklungstools | 4 Stunden |
| Tag 8 | Modul 8: Foundry Local Toolkit (Moderne Implementierung) | 1 Stunde |

### Intensiver Lernpfad (2 Wochen)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: KI-Agenten | 2 Stunden |
| Tag 13-14 | Modul 7: Entwicklungstools | 3 Stunden |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: SLM-Agentensysteme | 2 Stunden |
| Tag 13-14 | Modul 7: EdgeAI-Implementierungsbeispiele | 2 Stunden |

| Modul | Abschlussdatum | Aufgewendete Stunden | Wichtige Erkenntnisse |
|-------|----------------|----------------------|-----------------------|
| Modul 0: Einführung in EdgeAI | | | |
| Modul 1: Grundlagen von EdgeAI | | | |
| Modul 2: SLM-Grundlagen | | | |
| Modul 3: SLM-Bereitstellung | | | |
| Modul 4: Modelloptimierung (6 Frameworks) | | | |
| Modul 5: SLMOps | | | |
| Modul 6: SLM-Agentensysteme | | | |
| Modul 7: EdgeAI-Implementierungsbeispiele | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

## Einführung

Willkommen beim Studienleitfaden für EdgeAI für Anfänger! Dieses Dokument soll Ihnen helfen, die Kursmaterialien effektiv zu nutzen und Ihr Lernerlebnis zu maximieren. Es bietet strukturierte Lernpfade, empfohlene Studienpläne, Zusammenfassungen wichtiger Konzepte und ergänzende Ressourcen, um Ihr Verständnis der EdgeAI-Technologien zu vertiefen.

Dies ist ein kompakter 20-Stunden-Kurs, der essenzielles Wissen über EdgeAI in einem zeiteffizienten Format vermittelt – ideal für vielbeschäftigte Fachleute und Studierende, die schnell praktische Fähigkeiten in diesem aufstrebenden Bereich erwerben möchten.

## Kursübersicht

Dieser Kurs ist in acht umfassende Module unterteilt:

0. **Einführung in EdgeAI** – Grundlagen und Kontext mit Branchenanwendungen und Lernzielen
1. **Grundlagen und Transformation von EdgeAI** – Verständnis der Kernkonzepte und technologischen Veränderungen
2. **Grundlagen von Small Language Models (SLM)** – Untersuchung verschiedener SLM-Familien und ihrer Architekturen
3. **Bereitstellung von Small Language Models** – Praktische Implementierungsstrategien
4. **Modellformatkonvertierung und Quantisierung** – Fortgeschrittene Optimierung mit 6 Frameworks, einschließlich OpenVINO
5. **SLMOps – Betrieb von Small Language Models** – Produktionslebenszyklusmanagement und Bereitstellung
6. **SLM-Agentensysteme** – KI-Agenten, Funktionsaufrufe und Model Context Protocol
7. **EdgeAI-Implementierungsbeispiele** – KI-Toolkit, Windows-Entwicklung und plattformspezifische Implementierungen
8. **Microsoft Foundry Local – Komplettes Entwickler-Toolkit** – Lokale Entwicklung mit hybrider Azure-Integration (Modul 08)

## So nutzen Sie diesen Studienleitfaden

- **Progressives Lernen**: Folgen Sie den Modulen der Reihe nach für ein kohärentes Lernerlebnis
- **Wissens-Checkpoints**: Nutzen Sie die Selbstbewertungsfragen nach jedem Abschnitt
- **Praktische Übungen**: Schließen Sie die vorgeschlagenen Übungen ab, um theoretische Konzepte zu festigen
- **Ergänzende Ressourcen**: Erkunden Sie zusätzliche Materialien zu Themen, die Sie besonders interessieren

## Empfehlungen für den Studienplan

### Intensiver Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 6 Stunden |
| Tag 3-4 | Modul 2: SLM-Grundlagen | 8 Stunden |
| Tag 5 | Modul 3: SLM-Bereitstellung | 3 Stunden |
| Tag 6 | Modul 8: Foundry Local Toolkit | 3 Stunden |

### Teilzeitstudium (3 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 0: Einführung + Modul 1: Grundlagen von EdgeAI | 7-9 Stunden |
| Woche 2 | Modul 2: SLM-Grundlagen | 7-8 Stunden |
| Woche 3 | Modul 3: SLM-Bereitstellung (3h) + Modul 8: Foundry Local Toolkit (2-3h) | 5-6 Stunden |

## Modul 0: Einführung in EdgeAI

### Wichtige Lernziele

- Verstehen, was EdgeAI ist und warum es in der heutigen Technologielandschaft wichtig ist
- Wichtige Branchen identifizieren, die durch EdgeAI transformiert wurden, und deren spezifische Anwendungsfälle
- Die Vorteile von Small Language Models (SLMs) für Edge-Bereitstellungen verstehen
- Klare Lernziele und Ergebnisse für den gesamten Kurs festlegen
- Karrierechancen und erforderliche Fähigkeiten im Bereich EdgeAI erkennen

### Studien-Schwerpunkte

#### Abschnitt 1: EdgeAI-Paradigma und Definition
- **Wichtige Konzepte**: 
  - EdgeAI vs. traditionelle Cloud-AI-Verarbeitung
  - Die Konvergenz von Hardware, Modelloptimierung und geschäftlichen Anforderungen
  - Echtzeit-, datenschutzfreundliche und kosteneffiziente KI-Bereitstellung

#### Abschnitt 2: Branchenanwendungen
- **Wichtige Konzepte**: 
  - Fertigung & Industrie 4.0: Prädiktive Wartung und Qualitätskontrolle
  - Gesundheitswesen: Diagnostische Bildgebung und Patientenüberwachung
  - Autonome Systeme: Selbstfahrende Fahrzeuge und Transportwesen
  - Smart Cities: Verkehrsmanagement und öffentliche Sicherheit
  - Verbrauchertechnologie: Smartphones, Wearables und Smart Homes

#### Abschnitt 3: Grundlagen von Small Language Models
- **Wichtige Konzepte**: 
  - Eigenschaften und Leistungsmerkmale von SLMs
  - Parameter-Effizienz vs. Fähigkeit-Abwägungen
  - Einschränkungen und Optimierungsstrategien für Edge-Bereitstellungen

#### Abschnitt 4: Lernrahmen und Karriereweg
- **Wichtige Konzepte**: 
  - Kursstruktur und progressiver Lernansatz
  - Technische Fähigkeiten und praktische Implementierungsziele
  - Karriereentwicklungsmöglichkeiten und Branchenanwendungen

### Selbstbewertungsfragen

1. Welche drei Haupttechnologietrends haben EdgeAI ermöglicht?
2. Vergleichen Sie die Vorteile und Herausforderungen von EdgeAI gegenüber Cloud-basierter KI.
3. Nennen Sie drei Branchen, in denen EdgeAI einen entscheidenden Geschäftswert bietet, und erklären Sie warum.
4. Wie machen Small Language Models EdgeAI für reale Anwendungen praktikabel?
5. Welche wichtigen technischen Fähigkeiten werden Sie in diesem Kurs entwickeln?
6. Beschreiben Sie den vierphasigen Lernansatz, der in diesem Kurs verwendet wird.

### Praktische Übungen

1. **Branchenforschung**: Wählen Sie eine Branchenanwendung aus und recherchieren Sie eine reale EdgeAI-Implementierung (30 Minuten)
2. **Modell-Erkundung**: Durchsuchen Sie verfügbare Small Language Models auf Hugging Face und vergleichen Sie deren Parameteranzahl und Fähigkeiten (30 Minuten)
3. **Lernplanung**: Überprüfen Sie die gesamte Kursstruktur und erstellen Sie Ihren persönlichen Studienplan (15 Minuten)

### Ergänzende Materialien

- [EdgeAI Marktübersicht - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Small Language Models Übersicht - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Edge Computing Foundation](https://www.edgecomputing.org/)

## Modul 1: Grundlagen und Transformation von EdgeAI

### Wichtige Lernziele

- Unterschiede zwischen Cloud-basierter und Edge-basierter KI verstehen
- Kernoptimierungstechniken für ressourcenbeschränkte Umgebungen beherrschen
- Reale Anwendungen von EdgeAI-Technologien analysieren
- Eine Entwicklungsumgebung für EdgeAI-Projekte einrichten

### Studien-Schwerpunkte

#### Abschnitt 1: Grundlagen von EdgeAI
- **Wichtige Konzepte**: 
  - Edge- vs. Cloud-Computing-Paradigmen
  - Modell-Quantisierungstechniken
  - Hardware-Beschleunigungsoptionen (NPUs, GPUs, CPUs)
  - Datenschutz- und Sicherheitsvorteile

- **Ergänzende Materialien**:
  - [TensorFlow Lite Dokumentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Dokumentation](https://docs.edgeimpulse.com)

#### Abschnitt 2: Fallstudien aus der Praxis
- **Wichtige Konzepte**: 
  - Microsoft Phi & Mu Modell-Ökosystem
  - Praktische Implementierungen in verschiedenen Branchen
  - Bereitstellungsüberlegungen

#### Abschnitt 3: Praktischer Implementierungsleitfaden
- **Wichtige Konzepte**: 
  - Einrichtung der Entwicklungsumgebung
  - Quantisierungs- und Optimierungstools
  - Bewertungsmethoden für EdgeAI-Implementierungen

#### Abschnitt 4: Edge-Bereitstellungshardware
- **Wichtige Konzepte**: 
  - Vergleich von Hardware-Plattformen
  - Optimierungsstrategien für spezifische Hardware
  - Bereitstellungsüberlegungen

### Selbstbewertungsfragen

1. Vergleichen und kontrastieren Sie Cloud-basierte KI mit Edge-basierten KI-Implementierungen.
2. Erklären Sie drei Schlüsseltechniken zur Optimierung von Modellen für Edge-Bereitstellungen.
3. Was sind die Hauptvorteile der Ausführung von KI-Modellen am Edge?
4. Beschreiben Sie den Prozess der Modell-Quantisierung und wie er die Leistung beeinflusst.
5. Erklären Sie, wie verschiedene Hardware-Beschleuniger (NPUs, GPUs, CPUs) die EdgeAI-Bereitstellung beeinflussen.

### Praktische Übungen

1. **Schnelle Einrichtung der Umgebung**: Konfigurieren Sie eine minimale Entwicklungsumgebung mit den wesentlichen Paketen (30 Minuten)
2. **Modell-Erkundung**: Laden Sie ein vortrainiertes Small Language Model herunter und untersuchen Sie es (1 Stunde)
3. **Grundlegende Quantisierung**: Probieren Sie einfache Quantisierung an einem kleinen Modell aus (1 Stunde)

## Modul 2: Grundlagen von Small Language Models

### Wichtige Lernziele

- Architektonische Prinzipien verschiedener SLM-Familien verstehen
- Modellfähigkeiten über verschiedene Parameter-Skalen hinweg vergleichen
- Modelle basierend auf Effizienz, Fähigkeiten und Bereitstellungsanforderungen bewerten
- Geeignete Anwendungsfälle für verschiedene Modellfamilien erkennen

### Studien-Schwerpunkte

#### Abschnitt 1: Microsoft Phi Modellfamilie
- **Wichtige Konzepte**: 
  - Designphilosophie und Entwicklung
  - Effizienzorientierte Architektur
  - Spezialisierte Fähigkeiten

#### Abschnitt 2: Qwen Familie
- **Wichtige Konzepte**: 
  - Beiträge aus der Open-Source-Community
  - Skalierbare Bereitstellungsoptionen
  - Fortgeschrittene Argumentationsarchitektur

#### Abschnitt 3: Gemma Familie
- **Wichtige Konzepte**: 
  - Forschungsgetriebene Innovation
  - Multimodale Fähigkeiten
  - Optimierung für mobile Geräte

#### Abschnitt 4: BitNET Familie
- **Wichtige Konzepte**: 
  - 1-Bit-Quantisierungstechnologie
  - Inferenzoptimierungs-Framework
  - Nachhaltigkeitsüberlegungen

#### Abschnitt 5: Microsoft Mu Modell
- **Wichtige Konzepte**: 
  - Geräteorientierte Architektur
  - Systemintegration mit Windows
  - Datenschutzfreundlicher Betrieb

#### Abschnitt 6: Phi-Silica
- **Wichtige Konzepte**: 
  - NPU-optimierte Architektur
  - Leistungskennzahlen
  - Entwicklerintegration

### Selbstbewertungsfragen

1. Vergleichen Sie die architektonischen Ansätze der Phi- und Qwen-Modellfamilien.
2. Erklären Sie, wie sich die Quantisierungstechnologie von BitNET von traditioneller Quantisierung unterscheidet.
3. Welche einzigartigen Vorteile bietet das Mu-Modell für die Integration in Windows?
4. Beschreiben Sie, wie Phi-Silica NPU-Hardware zur Leistungsoptimierung nutzt.
5. Welche Modellfamilie wäre für eine mobile Anwendung mit eingeschränkter Konnektivität am geeignetsten und warum?

### Praktische Übungen

1. **Modellvergleich**: Schneller Benchmark von zwei verschiedenen SLM-Modellen (1 Stunde)
2. **Einfache Textgenerierung**: Grundlegende Implementierung der Textgenerierung mit einem kleinen Modell (1 Stunde)
3. **Schnelle Optimierung**: Anwenden einer Optimierungstechnik zur Verbesserung der Inferenzgeschwindigkeit (1 Stunde)

## Modul 3: Bereitstellung von Small Language Models

### Wichtige Lernziele

- Auswahl geeigneter Modelle basierend auf Bereitstellungsbeschränkungen
- Beherrschung von Optimierungstechniken für verschiedene Bereitstellungsszenarien
- Implementierung von SLMs in lokalen und Cloud-Umgebungen
- Entwurf produktionsreifer Konfigurationen für EdgeAI-Anwendungen

### Schwerpunktbereiche des Studiums

#### Abschnitt 1: Fortgeschrittenes Lernen mit SLMs
- **Prioritätskonzepte**: 
  - Parameterklassifizierungs-Framework
  - Fortgeschrittene Optimierungstechniken
  - Strategien zur Modellbeschaffung

#### Abschnitt 2: Bereitstellung in lokalen Umgebungen
- **Prioritätskonzepte**: 
  - Ollama-Plattform-Bereitstellung
  - Lokale Lösungen von Microsoft Foundry
  - Vergleichende Analyse von Frameworks

#### Abschnitt 3: Containerisierte Cloud-Bereitstellung
- **Prioritätskonzepte**: 
  - vLLM Hochleistungsinferenz
  - Container-Orchestrierung
  - Implementierung von ONNX Runtime

### Selbstbewertungsfragen

1. Welche Faktoren sollten bei der Auswahl zwischen lokaler Bereitstellung und Cloud-Bereitstellung berücksichtigt werden?
2. Vergleichen Sie Ollama und Microsoft Foundry Local als Bereitstellungsoptionen.
3. Erklären Sie die Vorteile der Containerisierung für die Bereitstellung von SLMs.
4. Welche Leistungskennzahlen sollten für ein Edge-bereitgestelltes SLM überwacht werden?
5. Beschreiben Sie einen vollständigen Bereitstellungsworkflow von der Modellauswahl bis zur Produktionsimplementierung.

### Praktische Übungen

1. **Grundlegende lokale Bereitstellung**: Bereitstellung eines einfachen SLM mit Ollama (1 Stunde)
2. **Leistungsprüfung**: Führen Sie einen schnellen Benchmark für Ihr bereitgestelltes Modell durch (30 Minuten)
3. **Einfache Integration**: Erstellen Sie eine minimale Anwendung, die Ihr bereitgestelltes Modell verwendet (1 Stunde)

## Modul 4: Modellformatkonvertierung und Quantisierung

### Wichtige Lernziele

- Beherrschung fortgeschrittener Quantisierungstechniken von 1-Bit bis 8-Bit Präzision
- Verständnis von Strategien zur Formatkonvertierung (GGUF, ONNX)
- Optimierung über sechs Frameworks (Llama.cpp, Olive, OpenVINO, MLX, Workflow-Synthese)
- Bereitstellung optimierter Modelle für produktionsreife Edge-Umgebungen auf Intel-, Apple- und plattformübergreifender Hardware

### Schwerpunktbereiche des Studiums

#### Abschnitt 1: Grundlagen der Quantisierung
- **Prioritätskonzepte**: 
  - Präzisionsklassifizierungs-Framework
  - Abwägung zwischen Leistung und Genauigkeit
  - Optimierung des Speicherbedarfs

#### Abschnitt 2: Implementierung von Llama.cpp
- **Prioritätskonzepte**: 
  - Plattformübergreifende Bereitstellung
  - GGUF-Formatoptimierung
  - Hardware-Beschleunigungstechniken

#### Abschnitt 3: Microsoft Olive Suite
- **Prioritätskonzepte**: 
  - Hardwarebewusste Optimierung
  - Unternehmensgerechte Bereitstellung
  - Automatisierte Optimierungsworkflows

#### Abschnitt 4: OpenVINO Toolkit
- **Prioritätskonzepte**: 
  - Optimierung für Intel-Hardware
  - Neural Network Compression Framework (NNCF)
  - Plattformübergreifende Inferenzbereitstellung
  - OpenVINO GenAI für LLM-Bereitstellung

#### Abschnitt 5: Apple MLX Framework
- **Prioritätskonzepte**: 
  - Optimierung für Apple Silicon
  - Einheitliche Speicherarchitektur
  - LoRA-Fine-Tuning-Fähigkeiten

#### Abschnitt 6: Workflow-Synthese für Edge AI-Entwicklung
- **Prioritätskonzepte**: 
  - Einheitliche Workflow-Architektur
  - Entscheidungsbäume für die Auswahl von Frameworks
  - Validierung der Produktionsreife
  - Zukunftssichere Strategien

### Selbstbewertungsfragen

1. Vergleichen Sie Quantisierungsstrategien über verschiedene Präzisionsstufen (1-Bit bis 8-Bit).
2. Erklären Sie die Vorteile des GGUF-Formats für die Edge-Bereitstellung.
3. Wie verbessert hardwarebewusste Optimierung in Microsoft Olive die Bereitstellungseffizienz?
4. Was sind die Hauptvorteile von OpenVINOs NNCF für die Modellkompression?
5. Beschreiben Sie, wie Apple MLX die einheitliche Speicherarchitektur für die Optimierung nutzt.
6. Wie hilft die Workflow-Synthese bei der Auswahl optimaler Optimierungsframeworks?

### Praktische Übungen

1. **Modellquantisierung**: Wenden Sie verschiedene Quantisierungsstufen auf ein Modell an und vergleichen Sie die Ergebnisse (1 Stunde)
2. **OpenVINO-Optimierung**: Verwenden Sie NNCF, um ein Modell für Intel-Hardware zu komprimieren (1 Stunde)
3. **Framework-Vergleich**: Testen Sie dasselbe Modell in drei verschiedenen Optimierungsframeworks (1 Stunde)
4. **Leistungsbenchmarking**: Messen Sie den Einfluss der Optimierung auf Inferenzgeschwindigkeit und Speicherverbrauch (1 Stunde)

## Modul 5: SLMOps - Small Language Model Operations

### Wichtige Lernziele

- Verständnis der Prinzipien des Lebenszyklusmanagements von SLMOps
- Beherrschung von Distillations- und Fine-Tuning-Techniken für Edge-Bereitstellungen
- Implementierung von Produktionsbereitstellungsstrategien mit Monitoring
- Aufbau von unternehmensgerechten Workflows für SLM-Betrieb und Wartung

### Schwerpunktbereiche des Studiums

#### Abschnitt 1: Einführung in SLMOps
- **Prioritätskonzepte**: 
  - Paradigmenwechsel von SLMOps in der KI-Betriebsführung
  - Kosten- und Datenschutzorientierte Architektur
  - Strategische Geschäftsauswirkungen und Wettbewerbsvorteile

#### Abschnitt 2: Modell-Distillation
- **Prioritätskonzepte**: 
  - Techniken des Wissenstransfers
  - Implementierung des zweistufigen Distillationsprozesses
  - Distillationsworkflows mit Azure ML

#### Abschnitt 3: Fine-Tuning-Strategien
- **Prioritätskonzepte**: 
  - Parameter-effizientes Fine-Tuning (PEFT)
  - Fortgeschrittene Methoden wie LoRA und QLoRA
  - Multi-Adapter-Training und Hyperparameter-Optimierung

#### Abschnitt 4: Produktionsbereitstellung
- **Prioritätskonzepte**: 
  - Modellkonvertierung und Quantisierung für die Produktion
  - Konfiguration der Foundry Local-Bereitstellung
  - Leistungsbenchmarking und Qualitätsvalidierung

### Selbstbewertungsfragen

1. Wie unterscheidet sich SLMOps von traditionellem MLOps?
2. Erklären Sie die Vorteile der Modell-Distillation für Edge-Bereitstellungen.
3. Welche wichtigen Überlegungen gibt es beim Fine-Tuning von SLMs in ressourcenbeschränkten Umgebungen?
4. Beschreiben Sie eine vollständige Produktionsbereitstellungspipeline für Edge-AI-Anwendungen.

### Praktische Übungen

1. **Grundlegende Distillation**: Erstellen Sie ein kleineres Modell aus einem größeren Lehrermodell (1 Stunde)
2. **Fine-Tuning-Experiment**: Fine-Tuning eines Modells für einen spezifischen Bereich (1 Stunde)
3. **Bereitstellungspipeline**: Einrichten einer grundlegenden CI/CD-Pipeline für die Modellbereitstellung (1 Stunde)

## Modul 6: SLM Agentensysteme - KI-Agenten und Funktionsaufrufe

### Wichtige Lernziele

- Aufbau intelligenter KI-Agenten für Edge-Umgebungen mit Small Language Models
- Implementierung von Funktionsaufrufen mit systematischen Workflows
- Beherrschung der Integration des Model Context Protocol (MCP) für standardisierte Werkzeuginteraktion
- Erstellung anspruchsvoller Agentensysteme mit minimaler menschlicher Intervention

### Schwerpunktbereiche des Studiums

#### Abschnitt 1: KI-Agenten und SLM-Grundlagen
- **Prioritätskonzepte**: 
  - Klassifizierungsframework für Agenten (Reflex-, Modell-, Ziel- und Lernagenten)
  - Analyse der Abwägungen zwischen SLM und LLM
  - Edge-spezifische Entwurfsmuster für Agenten
  - Ressourcenoptimierung für Agenten

#### Abschnitt 2: Funktionsaufrufe in Small Language Models
- **Prioritätskonzepte**: 
  - Implementierung systematischer Workflows (Absichtserkennung, JSON-Ausgabe, externe Ausführung)
  - Plattform-spezifische Implementierungen (Phi-4-mini, ausgewählte Qwen-Modelle, Microsoft Foundry Local)
  - Fortgeschrittene Beispiele (Zusammenarbeit mehrerer Agenten, dynamische Werkzeugauswahl)
  - Produktionsüberlegungen (Rate-Limiting, Audit-Logging, Sicherheitsmaßnahmen)

#### Abschnitt 3: Integration des Model Context Protocol (MCP)
- **Prioritätskonzepte**: 
  - Protokollarchitektur und schichtweises Systemdesign
  - Unterstützung für mehrere Backends (Ollama für Entwicklung, vLLM für Produktion)
  - Verbindungsprotokolle (STDIO- und SSE-Modi)
  - Anwendungen in der Praxis (Web-Automatisierung, Datenverarbeitung, API-Integration)

### Selbstbewertungsfragen

1. Was sind die wichtigsten architektonischen Überlegungen für Edge-KI-Agenten?
2. Wie verbessern Funktionsaufrufe die Fähigkeiten von Agenten?
3. Erklären Sie die Rolle des Model Context Protocol in der Agentenkommunikation.

### Praktische Übungen

1. **Einfacher Agent**: Erstellen Sie einen grundlegenden KI-Agenten mit Funktionsaufrufen (1 Stunde)
2. **MCP-Integration**: Implementieren Sie MCP in einer Agentenanwendung (30 Minuten)

## Workshop: Praxisorientierter Lernpfad

### Wichtige Lernziele

- Aufbau produktionsreifer KI-Anwendungen mit Foundry Local SDK und Best Practices
- Implementierung umfassender Fehlerbehandlungs- und Benutzerfeedback-Muster
- Erstellung von RAG-Pipelines mit Qualitätsbewertung und Leistungsüberwachung
- Entwicklung von Multi-Agenten-Systemen mit Koordinator-Mustern
- Beherrschung intelligenter Modell-Routing-Strategien für aufgabenbasierte Modellauswahl
- Bereitstellung lokal orientierter KI-Lösungen mit datenschutzfreundlichen Architekturen

### Schwerpunktbereiche des Studiums

#### Sitzung 01: Einstieg in Foundry Local
- **Prioritätskonzepte**:
  - Integration des FoundryLocalManager SDK und automatische Dienstentdeckung
  - Grundlegende und Streaming-Chat-Implementierungen
  - Fehlerbehandlungsmuster und Benutzerfeedback
  - Konfiguration basierend auf der Umgebung

#### Sitzung 02: Aufbau von KI-Lösungen mit RAG
- **Prioritätskonzepte**:
  - In-Memory-Vektor-Embeddings mit Sentence-Transformers
  - Implementierung der RAG-Pipeline (Abrufen → Generieren)
  - Qualitätsbewertung mit RAGAS-Metriken
  - Import-Sicherheit für optionale Abhängigkeiten

#### Sitzung 03: Open-Source-Modelle
- **Prioritätskonzepte**:
  - Multi-Modell-Benchmarking-Strategien
  - Messung von Latenz und Durchsatz
  - Sanfte Degradierung und Fehlerbehebung
  - Leistungsvergleich zwischen Modellfamilien

#### Sitzung 04: Spitzentechnologie-Modelle
- **Prioritätskonzepte**:
  - Vergleichsmethodik zwischen SLM und LLM
  - Typ-Hinweise und umfassende Ausgabeformatierung
  - Fehlerbehandlung pro Modell
  - Strukturierte Ergebnisse für Analysen

#### Sitzung 05: KI-gestützte Agenten
- **Prioritätskonzepte**:
  - Multi-Agenten-Orchestrierung mit Koordinator-Muster
  - Speicherverwaltung und Statusverfolgung für Agenten
  - Fehlerbehandlung in der Pipeline und Protokollierung der Phasen
  - Leistungsüberwachung und Statistiken

#### Sitzung 06: Modelle als Werkzeuge
- **Prioritätskonzepte**:
  - Absichtserkennung und Musterabgleich
  - Schlüsselwortbasierte Modell-Routing-Algorithmen
  - Mehrstufige Pipelines (Planen → Ausführen → Verfeinern)
  - Umfassende Funktionsdokumentation

### Selbstbewertungsfragen

1. Wie vereinfacht FoundryLocalManager das Servicemanagement im Vergleich zu manuellen REST-Aufrufen?
2. Erklären Sie die Bedeutung von Import-Gards für optionale Abhängigkeiten wie Sentence-Transformers.
3. Welche Strategien gewährleisten eine sanfte Degradierung im Multi-Modell-Benchmarking?
4. Wie orchestriert das Koordinator-Muster mehrere spezialisierte Agenten?
5. Beschreiben Sie die Komponenten eines intelligenten Modell-Routers.
6. Was sind die Schlüsselelemente einer produktionsreifen Fehlerbehandlung?

### Praktische Übungen

1. **Chat-Anwendung**: Implementieren Sie Streaming-Chat mit Fehlerbehandlung (45 Minuten)
2. **RAG-Pipeline**: Erstellen Sie eine minimale RAG mit Qualitätsbewertung (1 Stunde)
3. **Modell-Benchmarking**: Vergleichen Sie 3+ Modelle hinsichtlich Leistung (1 Stunde)
4. **Multi-Agenten-System**: Erstellen Sie einen Koordinator mit 2 spezialisierten Agenten (1,5 Stunden)
5. **Intelligenter Router**: Erstellen Sie eine aufgabenbasierte Modellauswahl (1 Stunde)
6. **Produktionsbereitstellung**: Fügen Sie Monitoring und umfassende Fehlerbehandlung hinzu (45 Minuten)

### Zeitaufteilung

**Konzentriertes Lernen (1 Woche)**:
- Tag 1: Sitzung 01-02 (Chat + RAG) - 3 Stunden
- Tag 2: Sitzung 03-04 (Benchmarking + Vergleich) - 3 Stunden
- Tag 3: Sitzung 05-06 (Agenten + Routing) - 3 Stunden
- Tag 4: Praktische Übungen und Validierung - 2 Stunden

**Teilzeitstudium (2 Wochen)**:
- Woche 1: Sitzungen 01-03 (6 Stunden insgesamt)
- Woche 2: Sitzungen 04-06 + Übungen (5 Stunden insgesamt)

## Modul 7: EdgeAI-Implementierungsbeispiele

### Wichtige Lernziele

- Beherrschung des AI Toolkit für Visual Studio Code für umfassende EdgeAI-Entwicklungsworkflows
- Expertise in der Windows AI Foundry-Plattform und NPU-Optimierungsstrategien
- Implementierung von EdgeAI auf verschiedenen Hardwareplattformen und Bereitstellungsszenarien
- Aufbau produktionsreifer EdgeAI-Anwendungen mit plattform-spezifischen Optimierungen

### Schwerpunktbereiche des Studiums

#### Abschnitt 1: AI Toolkit für Visual Studio Code
- **Prioritätskonzepte**: 
  - Umfassende EdgeAI-Entwicklungsumgebung innerhalb von VS Code
  - Modellkatalog und -entdeckung für Edge-Bereitstellungen
  - Lokale Tests, Optimierung und Agentenentwicklungsworkflows
  - Leistungsüberwachung und Bewertung für Edge-Szenarien

#### Abschnitt 2: Windows EdgeAI-Entwicklungsleitfaden
- **Prioritätskonzepte**: 
  - Umfassender Überblick über die Windows AI Foundry-Plattform
  - Phi Silica API für effiziente NPU-Inferenz
  - Computer Vision APIs für Bildverarbeitung und OCR
  - Foundry Local CLI für lokale Entwicklung und Tests

#### Abschnitt 3: Plattform-spezifische Implementierungen
- **Prioritätskonzepte**: 
  - NVIDIA Jetson Orin Nano-Bereitstellung (67 TOPS KI-Leistung)
  - Mobile Anwendungen mit .NET MAUI und ONNX Runtime GenAI
  - Azure EdgeAI-Lösungen mit Cloud-Edge-Hybridarchitektur
  - Windows ML-Optimierung mit universeller Hardwareunterstützung
  - Foundry Local-Anwendungen mit datenschutzorientierter RAG-Implementierung

### Selbstbewertungsfragen

1. Wie vereinfacht das AI Toolkit den EdgeAI-Entwicklungsworkflow?
2. Vergleichen Sie Bereitstellungsstrategien auf verschiedenen Hardwareplattformen.
3. Welche Vorteile bietet Windows AI Foundry für die Edge-Entwicklung?
4. Erklären Sie die Rolle der NPU-Optimierung in modernen Edge-AI-Anwendungen.  
5. Wie nutzt die Phi Silica API NPU-Hardware zur Leistungsoptimierung?  
6. Vergleichen Sie die Vorteile von lokaler und Cloud-Bereitstellung für datenschutzsensible Anwendungen.  

### Praktische Übungen  

1. **AI Toolkit Einrichtung**: Konfigurieren Sie das AI Toolkit und optimieren Sie ein Modell (1 Stunde)  
2. **Windows AI Foundry**: Erstellen Sie eine einfache Windows-AI-Anwendung mit der Phi Silica API (1 Stunde)  
3. **Plattformübergreifende Bereitstellung**: Stellen Sie dasselbe Modell auf zwei verschiedenen Plattformen bereit (1 Stunde)  
4. **NPU-Optimierung**: Testen Sie die NPU-Leistung mit den Windows AI Foundry Tools (30 Minuten)  

## Modul 8: Microsoft Foundry Local – Komplettes Entwickler-Toolkit (Modernisiert)  

### Wichtige Lernziele  

- Foundry Local mit moderner SDK-Integration installieren und konfigurieren  
- Fortgeschrittene Multi-Agenten-Systeme mit Koordinator-Mustern implementieren  
- Intelligente Modell-Router mit automatischer, auf Aufgaben basierender Auswahl erstellen  
- Produktionsreife KI-Lösungen mit umfassendem Monitoring bereitstellen  
- Integration mit Azure AI Foundry für hybride Bereitstellungsszenarien  
- Moderne SDK-Muster mit FoundryLocalManager und OpenAI-Client meistern  

### Studien-Schwerpunkte  

#### Abschnitt 1: Moderne Installation und Konfiguration  
- **Prioritätskonzepte**:  
  - FoundryLocalManager SDK-Integration  
  - Automatische Dienst-Erkennung und Gesundheitsüberwachung  
  - Konfigurationsmuster basierend auf Umgebungen  
  - Überlegungen zur Produktionsbereitstellung  

#### Abschnitt 2: Fortgeschrittene Multi-Agenten-Systeme  
- **Prioritätskonzepte**:  
  - Koordinator-Muster mit spezialisierten Agenten  
  - Spezialisierung von Abruf-, Analyse- und Ausführungsagenten  
  - Feedback-Mechanismen zur Verfeinerung  
  - Leistungsüberwachung und Statistik-Tracking  

#### Abschnitt 3: Intelligente Modell-Routing  
- **Prioritätskonzepte**:  
  - Schlüsselwortbasierte Modell-Auswahlalgorithmen  
  - Unterstützung mehrerer Modelle (allgemein, Analyse, Code, kreativ)  
  - Konfiguration von Umgebungsvariablen für Flexibilität  
  - Dienst-Gesundheitsprüfung und Fehlerbehandlung  

#### Abschnitt 4: Produktionsreife Implementierung  
- **Prioritätskonzepte**:  
  - Umfassende Fehlerbehandlung und Fallback-Mechanismen  
  - Anfragen-Monitoring und Leistungs-Tracking  
  - Interaktive Jupyter-Notebook-Beispiele mit Benchmarks  
  - Integrationsmuster mit bestehenden Anwendungen  

### Selbstbewertungsfragen  

1. Wie unterscheidet sich der moderne Ansatz von FoundryLocalManager von manuellen REST-Aufrufen?  
2. Erklären Sie das Koordinator-Muster und wie es spezialisierte Agenten orchestriert.  
3. Wie wählt der intelligente Router geeignete Modelle basierend auf dem Inhalt der Anfrage aus?  
4. Was sind die Hauptkomponenten eines produktionsreifen KI-Agenten-Systems?  
5. Wie implementieren Sie umfassendes Gesundheits-Monitoring für Foundry Local-Dienste?  
6. Vergleichen Sie die Vorteile des modernisierten Ansatzes mit traditionellen Implementierungsmustern.  

### Praktische Übungen  

1. **Moderne SDK-Einrichtung**: Konfigurieren Sie FoundryLocalManager mit automatischer Dienst-Erkennung (30 Minuten)  
2. **Multi-Agenten-System**: Führen Sie den fortgeschrittenen Koordinator mit spezialisierten Agenten aus (30 Minuten)  
3. **Intelligentes Routing**: Testen Sie den Modell-Router mit verschiedenen Anfrage-Typen (30 Minuten)  
4. **Interaktive Erkundung**: Nutzen Sie die Jupyter-Notebooks, um fortgeschrittene Funktionen zu erkunden (45 Minuten)  
5. **Produktionsbereitstellung**: Implementieren Sie Monitoring- und Fehlerbehandlungs-Muster (30 Minuten)  
6. **Hybride Integration**: Konfigurieren Sie Azure AI Foundry Fallback-Szenarien (30 Minuten)  

## Zeitplan-Leitfaden  

Um das Beste aus dem erweiterten 30-Stunden-Kurs (inklusive Workshop) herauszuholen, hier ein Vorschlag zur Zeitaufteilung:  

| Aktivität | Zeitaufteilung | Beschreibung |  
|-----------|----------------|--------------|  
| Kernmaterialien lesen | 12 Stunden | Fokus auf die wesentlichen Konzepte in jedem Modul |  
| Praktische Übungen | 10 Stunden | Praktische Umsetzung der wichtigsten Techniken (inklusive Workshop) |  
| Selbstbewertung | 3 Stunden | Verständnis durch Fragen und Reflexion testen |  
| Mini-Projekt | 5 Stunden | Wissen in einer kleinen praktischen Umsetzung anwenden |  

### Wichtige Schwerpunkte nach Zeitvorgabe  

**Wenn Sie nur 10 Stunden haben:**  
- Modul 0 (Einführung) und Module 1, 2 und 3 (Kernkonzepte von EdgeAI) abschließen  
- Mindestens eine praktische Übung pro Modul durchführen  
- Fokus auf das Verständnis der Kernkonzepte statt auf Implementierungsdetails  

**Wenn Sie 20 Stunden investieren können:**  
- Alle acht Module (inklusive Einführung) abschließen  
- Wichtige praktische Übungen aus jedem Modul durchführen  
- Ein Mini-Projekt aus Modul 7 abschließen  
- Mindestens 2-3 ergänzende Ressourcen erkunden  

**Wenn Sie mehr als 20 Stunden haben:**  
- Alle Module (inklusive Einführung) mit detaillierten Übungen abschließen  
- Mehrere Mini-Projekte erstellen  
- Fortgeschrittene Optimierungstechniken in Modul 4 erkunden  
- Produktionsbereitstellung aus Modul 5 implementieren  

## Wichtige Ressourcen  

Diese sorgfältig ausgewählten Ressourcen bieten den größten Nutzen für Ihre begrenzte Lernzeit:  

### Unbedingt zu lesende Dokumentation  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) – Das effizienteste Werkzeug zur Modelloptimierung  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) – Schnellster Weg zur lokalen Bereitstellung von SLMs  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) – Referenz für ein führendes, edge-optimiertes Modell  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) – Intels umfassendes Optimierungs-Toolkit  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) – Integrierte EdgeAI-Entwicklungsumgebung  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) – Windows-spezifische EdgeAI-Entwicklungsplattform  

### Zeitersparende Tools  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) – Schneller Zugriff auf Modelle und Bereitstellung  
- [Gradio](https://www.gradio.app/docs/interface) – Schnelle UI-Entwicklung für KI-Demos  
- [Microsoft Olive](https://github.com/microsoft/Olive) – Vereinfachte Modelloptimierung  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) – Effiziente CPU-Inferenz  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) – Framework zur Kompression neuronaler Netze  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) – Toolkit zur Bereitstellung großer Sprachmodelle  

## Fortschritts-Tracking-Vorlage  

Nutzen Sie diese vereinfachte Vorlage, um Ihren Lernfortschritt im 20-Stunden-Kurs zu verfolgen:  

| Modul | Abschlussdatum | Stunden investiert | Wichtige Erkenntnisse |  
|-------|----------------|--------------------|-----------------------|  
| Modul 0: Einführung in EdgeAI | | | |  
| Modul 1: Grundlagen von EdgeAI | | | |  
| Modul 2: SLM-Grundlagen | | | |  
| Modul 3: SLM-Bereitstellung | | | |  
| Modul 4: Modelloptimierung | | | |  
| Modul 5: SLMOps | | | |  
| Modul 6: KI-Agenten | | | |  
| Modul 7: Entwickler-Tools | | | |  
| Workshop: Praktisches Lernen | | | |  
| Modul 8: Foundry Local Toolkit | | | |  
| Praktische Übungen | | | |  
| Mini-Projekt | | | |  

## Mini-Projekt-Ideen  

Erwägen Sie, eines dieser Projekte abzuschließen, um EdgeAI-Konzepte zu üben (jedes ist für 2-4 Stunden ausgelegt):  

### Anfängerprojekte (jeweils 2-3 Stunden)  
1. **Edge Text Assistant**: Erstellen Sie ein einfaches Offline-Textvervollständigungs-Tool mit einem kleinen Sprachmodell  
2. **Modellvergleichs-Dashboard**: Erstellen Sie eine grundlegende Visualisierung von Leistungsmetriken verschiedener SLMs  
3. **Optimierungsexperiment**: Messen Sie die Auswirkungen verschiedener Quantisierungsstufen auf dasselbe Basismodell  

### Mittelstufe-Projekte (jeweils 3-4 Stunden)  
4. **AI Toolkit Workflow**: Nutzen Sie das VS Code AI Toolkit, um ein Modell von Anfang bis Ende zu optimieren und bereitzustellen  
5. **Windows AI Foundry Anwendung**: Erstellen Sie eine Windows-App mit Phi Silica API und NPU-Optimierung  
6. **Plattformübergreifende Bereitstellung**: Stellen Sie dasselbe optimierte Modell auf Windows (OpenVINO) und Mobilgeräten (.NET MAUI) bereit  
7. **Funktionsaufruf-Agent**: Erstellen Sie einen KI-Agenten mit Funktionsaufruf-Fähigkeiten für Edge-Szenarien  

### Fortgeschrittene Integrationsprojekte (jeweils 4-5 Stunden)  
8. **OpenVINO-Optimierungspipeline**: Implementieren Sie eine vollständige Modelloptimierung mit NNCF und GenAI-Toolkit  
9. **SLMOps-Pipeline**: Implementieren Sie einen vollständigen Modell-Lebenszyklus von Training bis Edge-Bereitstellung  
10. **Multi-Modell-Edge-System**: Stellen Sie mehrere spezialisierte Modelle bereit, die auf Edge-Hardware zusammenarbeiten  
11. **MCP-Integrationssystem**: Erstellen Sie ein agentisches System mit Model Context Protocol für Tool-Interaktion  

## Referenzen  

- Microsoft Learn (Foundry Local)  
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Erste Schritte: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - CLI-Referenz: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integration mit Inferenz-SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Open WebUI Anleitung: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Hugging Face Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agenten (Übersicht): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Optimierungs- und Inferenz-Tools  
  - Microsoft Olive (Dokumentation): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (Erste Schritte): https://onnxruntime.ai/docs/get-started/with-python.html  
  - ONNX Runtime Olive-Integration: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (Dokumentation): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (Dokumentation): https://ml-explore.github.io/mlx/build/html/index.html  
- Bereitstellungs-Frameworks und Modelle  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (Dokumentation): https://docs.vllm.ai/  
  - Ollama (Erste Schritte): https://github.com/ollama/ollama#get-started  
- Entwickler-Tools (Windows und VS Code)  
  - AI Toolkit für VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (Übersicht): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Lern-Community  

Treten Sie der Diskussion bei und vernetzen Sie sich mit anderen Lernenden:  
- GitHub-Diskussionen im [EdgeAI for Beginners Repository](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Fazit  

EdgeAI repräsentiert die Zukunft der künstlichen Intelligenz, indem leistungsstarke Fähigkeiten direkt auf Geräte gebracht werden, während wichtige Anliegen wie Datenschutz, Latenz und Konnektivität berücksichtigt werden. Dieser 20-Stunden-Kurs vermittelt Ihnen das notwendige Wissen und die praktischen Fähigkeiten, um sofort mit EdgeAI-Technologien zu arbeiten.  

Der Kurs ist bewusst prägnant und konzentriert sich auf die wichtigsten Konzepte, sodass Sie schnell wertvolle Expertise gewinnen können, ohne eine überwältigende Zeitinvestition zu leisten. Denken Sie daran, dass praktische Übungen, selbst mit einfachen Beispielen, der Schlüssel zur Festigung des Gelernten sind.  

Viel Erfolg beim Lernen!  

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.