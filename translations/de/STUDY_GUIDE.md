<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3f8ec059920a41b354c806f312b6ee24",
  "translation_date": "2025-09-26T07:27:12+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "de"
}
-->
# EdgeAI für Anfänger: Lernpfade und Studienplan

### Konzentrierter Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 2 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 3 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 4-5 | Modul 4: Modelloptimierung (6 Frameworks) | 4 Stunden |
| Tag 6 | Modul 5: SLMOps | 3 Stunden |
| Tag 7 | Modul 6-7: KI-Agenten & Entwicklungstools | 4 Stunden |
| Tag 8 | Modul 8: Foundry Local Toolkit (Moderne Implementierung) | 1 Stunde |

### Konzentrierter Lernpfad (2 Wochen)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: KI-Agenten | 2 Stunden |
| Tag 13-14 | Modul 7: Entwicklungstools | 3 Stunden |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 3 Stunden |
| Tag 3-4 | Modul 2: Grundlagen von SLM | 3 Stunden |
| Tag 5-6 | Modul 3: SLM-Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: SLM-Agentensysteme | 2 Stunden |
| Tag 13-14 | Modul 7: EdgeAI-Implementierungsbeispiele | 2 Stunden |

| Modul | Abschlussdatum | Aufgewendete Stunden | Wichtige Erkenntnisse |
|-------|----------------|----------------------|-----------------------|
| Modul 0: Einführung in EdgeAI | | | |
| Modul 1: Grundlagen von EdgeAI | | | |
| Modul 2: SLM-Grundlagen | | | |
| Modul 3: SLM-Bereitstellung | | | |
| Modul 4: Modelloptimierung (6 Frameworks) | | | |
| Modul 5: SLMOps | | | |
| Modul 6: SLM-Agentensysteme | | | |
| Modul 7: EdgeAI-Implementierungsbeispiele | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM-Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

## Einführung

Willkommen beim Studienleitfaden für EdgeAI für Anfänger! Dieses Dokument soll Ihnen helfen, die Kursmaterialien effektiv zu nutzen und Ihr Lernerlebnis zu maximieren. Es bietet strukturierte Lernpfade, empfohlene Studienpläne, Zusammenfassungen wichtiger Konzepte und ergänzende Ressourcen, um Ihr Verständnis der EdgeAI-Technologien zu vertiefen.

Dies ist ein kompakter 20-Stunden-Kurs, der essenzielles Wissen über EdgeAI in einem zeiteffizienten Format vermittelt – ideal für vielbeschäftigte Fachleute und Studierende, die schnell praktische Fähigkeiten in diesem aufstrebenden Bereich erwerben möchten.

## Kursübersicht

Der Kurs ist in acht umfassende Module unterteilt:

0. **Einführung in EdgeAI** – Grundlagen und Kontext mit Branchenanwendungen und Lernzielen  
1. **Grundlagen und Transformation von EdgeAI** – Verständnis der Kernkonzepte und technologischen Veränderungen  
2. **Grundlagen von Small Language Models (SLM)** – Untersuchung verschiedener SLM-Familien und ihrer Architekturen  
3. **Bereitstellung von Small Language Models** – Praktische Strategien für die Implementierung  
4. **Modellformatkonvertierung und Quantisierung** – Fortgeschrittene Optimierung mit 6 Frameworks, einschließlich OpenVINO  
5. **SLMOps – Betrieb von Small Language Models** – Management des Produktionslebenszyklus und Bereitstellung  
6. **SLM-Agentensysteme** – KI-Agenten, Funktionsaufrufe und Model Context Protocol  
7. **EdgeAI-Implementierungsbeispiele** – KI-Toolkit, Windows-Entwicklung und plattformspezifische Implementierungen  
8. **Microsoft Foundry Local – Komplettes Entwickler-Toolkit** – Lokale Entwicklung mit hybrider Azure-Integration (Modul 08)

## So nutzen Sie diesen Studienleitfaden

- **Progressives Lernen**: Folgen Sie den Modulen der Reihe nach für ein kohärentes Lernerlebnis  
- **Wissens-Checkpoints**: Nutzen Sie die Selbstbewertungsfragen nach jedem Abschnitt  
- **Praktische Übungen**: Schließen Sie die vorgeschlagenen Übungen ab, um theoretische Konzepte zu festigen  
- **Ergänzende Ressourcen**: Erkunden Sie zusätzliche Materialien zu Themen, die Sie besonders interessieren  

## Empfehlungen für den Studienplan

### Konzentrierter Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 0 | Modul 0: Einführung in EdgeAI | 1-2 Stunden |
| Tag 1-2 | Modul 1: Grundlagen von EdgeAI | 6 Stunden |
| Tag 3-4 | Modul 2: SLM-Grundlagen | 8 Stunden |
| Tag 5 | Modul 3: SLM-Bereitstellung | 3 Stunden |
| Tag 6 | Modul 8: Foundry Local Toolkit | 3 Stunden |

### Teilzeitstudium (3 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|------------|--------------------|
| Woche 1 | Modul 0: Einführung + Modul 1: Grundlagen von EdgeAI | 7-9 Stunden |
| Woche 2 | Modul 2: SLM-Grundlagen | 7-8 Stunden |
| Woche 3 | Modul 3: SLM-Bereitstellung (3h) + Modul 8: Foundry Local Toolkit (2-3h) | 5-6 Stunden |

## Modul 0: Einführung in EdgeAI

### Wichtige Lernziele

- Verstehen, was EdgeAI ist und warum es in der heutigen Technologielandschaft wichtig ist  
- Wichtige Branchen identifizieren, die durch EdgeAI transformiert wurden, und deren spezifische Anwendungsfälle  
- Die Vorteile von Small Language Models (SLMs) für Edge-Bereitstellungen verstehen  
- Klare Lernziele und Ergebnisse für den gesamten Kurs festlegen  
- Karrierechancen und erforderliche Fähigkeiten im Bereich EdgeAI erkennen  

### Studien-Schwerpunkte

#### Abschnitt 1: EdgeAI-Paradigma und Definition
- **Priorisierte Konzepte**:  
  - EdgeAI vs. traditionelle Cloud-AI-Verarbeitung  
  - Die Konvergenz von Hardware, Modelloptimierung und geschäftlichen Anforderungen  
  - Echtzeit-, datenschutzfreundliche und kosteneffiziente KI-Bereitstellung  

#### Abschnitt 2: Branchenanwendungen
- **Priorisierte Konzepte**:  
  - Fertigung & Industrie 4.0: Prädiktive Wartung und Qualitätskontrolle  
  - Gesundheitswesen: Diagnostische Bildgebung und Patientenüberwachung  
  - Autonome Systeme: Selbstfahrende Fahrzeuge und Transportwesen  
  - Smart Cities: Verkehrsmanagement und öffentliche Sicherheit  
  - Verbrauchertechnologie: Smartphones, Wearables und Smart Homes  

#### Abschnitt 3: Grundlagen von Small Language Models
- **Priorisierte Konzepte**:  
  - Eigenschaften und Leistungsmerkmale von SLMs  
  - Trade-offs zwischen Parameter-Effizienz und Fähigkeiten  
  - Einschränkungen und Optimierungsstrategien für Edge-Bereitstellungen  

#### Abschnitt 4: Lernrahmen und Karriereweg
- **Priorisierte Konzepte**:  
  - Kursstruktur und progressiver Lernansatz  
  - Technische Fähigkeiten und praktische Implementierungsziele  
  - Karriereentwicklungsmöglichkeiten und Branchenanwendungen  

### Selbstbewertungsfragen

1. Welche drei technologischen Trends haben EdgeAI ermöglicht?  
2. Vergleichen Sie die Vorteile und Herausforderungen von EdgeAI gegenüber Cloud-basierter KI.  
3. Nennen Sie drei Branchen, in denen EdgeAI entscheidenden Geschäftswert bietet, und erklären Sie warum.  
4. Wie machen Small Language Models EdgeAI für reale Anwendungen praktikabel?  
5. Welche technischen Fähigkeiten werden Sie im Laufe dieses Kurses entwickeln?  
6. Beschreiben Sie den vierphasigen Lernansatz, der in diesem Kurs verwendet wird.  

### Praktische Übungen

1. **Branchenforschung**: Wählen Sie eine Branchenanwendung und recherchieren Sie eine reale EdgeAI-Implementierung (30 Minuten)  
2. **Modell-Erkundung**: Durchsuchen Sie verfügbare Small Language Models auf Hugging Face und vergleichen Sie deren Parameteranzahl und Fähigkeiten (30 Minuten)  
3. **Lernplanung**: Überprüfen Sie die gesamte Kursstruktur und erstellen Sie Ihren persönlichen Studienplan (15 Minuten)  

### Ergänzende Materialien

- [EdgeAI Marktübersicht - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)  
- [Small Language Models Übersicht - Hugging Face](https://huggingface.co/blog/small-language-models)  
- [Edge Computing Foundation](https://www.edgecomputing.org/)  

## Modul 1: Grundlagen und Transformation von EdgeAI

### Wichtige Lernziele

- Unterschiede zwischen Cloud-basierter und Edge-basierter KI verstehen  
- Kernoptimierungstechniken für ressourcenbeschränkte Umgebungen beherrschen  
- Analyse von realen Anwendungen der EdgeAI-Technologien  
- Entwicklungsumgebung für EdgeAI-Projekte einrichten  

### Studien-Schwerpunkte

#### Abschnitt 1: Grundlagen von EdgeAI
- **Priorisierte Konzepte**:  
  - Paradigmen von Edge- vs. Cloud-Computing  
  - Modell-Quantisierungstechniken  
  - Hardware-Beschleunigungsoptionen (NPUs, GPUs, CPUs)  
  - Datenschutz- und Sicherheitsvorteile  

- **Ergänzende Materialien**:  
  - [TensorFlow Lite Dokumentation](https://www.tensorflow.org/lite)  
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)  
  - [Edge Impulse Dokumentation](https://docs.edgeimpulse.com)  

#### Abschnitt 2: Fallstudien aus der Praxis
- **Priorisierte Konzepte**:  
  - Microsoft Phi & Mu Modell-Ökosystem  
  - Praktische Implementierungen in verschiedenen Branchen  
  - Bereitstellungsüberlegungen  

#### Abschnitt 3: Praktischer Implementierungsleitfaden
- **Priorisierte Konzepte**:  
  - Einrichtung der Entwicklungsumgebung  
  - Quantisierungs- und Optimierungstools  
  - Bewertungsmethoden für EdgeAI-Implementierungen  

#### Abschnitt 4: Hardware für Edge-Bereitstellungen
- **Priorisierte Konzepte**:  
  - Vergleich von Hardware-Plattformen  
  - Optimierungsstrategien für spezifische Hardware  
  - Bereitstellungsüberlegungen  

### Selbstbewertungsfragen

1. Vergleichen und kontrastieren Sie Cloud-basierte KI mit Edge-basierten KI-Implementierungen.  
2. Erklären Sie drei Schlüsseltechniken zur Optimierung von Modellen für Edge-Bereitstellungen.  
3. Was sind die Hauptvorteile der Ausführung von KI-Modellen am Edge?  
4. Beschreiben Sie den Prozess der Modell-Quantisierung und wie er die Leistung beeinflusst.  
5. Erklären Sie, wie verschiedene Hardware-Beschleuniger (NPUs, GPUs, CPUs) die EdgeAI-Bereitstellung beeinflussen.  

### Praktische Übungen

1. **Schnelle Einrichtung der Umgebung**: Konfigurieren Sie eine minimale Entwicklungsumgebung mit den wesentlichen Paketen (30 Minuten)  
2. **Modell-Erkundung**: Laden Sie ein vortrainiertes Small Language Model herunter und untersuchen Sie es (1 Stunde)  
3. **Grundlegende Quantisierung**: Probieren Sie einfache Quantisierung an einem kleinen Modell aus (1 Stunde)  

## Modul 2: Grundlagen von Small Language Models

### Wichtige Lernziele

- Architektonische Prinzipien verschiedener SLM-Familien verstehen  
- Modellfähigkeiten über verschiedene Parameter-Skalen hinweg vergleichen  
- Modelle basierend auf Effizienz, Fähigkeiten und Bereitstellungsanforderungen bewerten  
- Geeignete Anwendungsfälle für verschiedene Modellfamilien erkennen  

### Studien-Schwerpunkte

#### Abschnitt 1: Microsoft Phi Modellfamilie
- **Priorisierte Konzepte**:  
  - Entwicklung der Designphilosophie  
  - Effizienzorientierte Architektur  
  - Spezialisierte Fähigkeiten  

#### Abschnitt 2: Qwen Familie
- **Priorisierte Konzepte**:  
  - Beiträge aus der Open-Source-Community  
  - Skalierbare Bereitstellungsoptionen  
  - Architektur für fortgeschrittenes logisches Denken  

#### Abschnitt 3: Gemma Familie
- **Priorisierte Konzepte**:  
  - Innovationsgetriebene Forschung  
  - Multimodale Fähigkeiten  
  - Optimierung für mobile Geräte  

#### Abschnitt 4: BitNET Familie
- **Priorisierte Konzepte**:  
  - 1-Bit-Quantisierungstechnologie  
  - Framework für Inferenzoptimierung  
  - Nachhaltigkeitsüberlegungen  

#### Abschnitt 5: Microsoft Mu Modell
- **Priorisierte Konzepte**:  
  - Geräteorientierte Architektur  
  - Systemintegration mit Windows  
  - Datenschutzfreundlicher Betrieb  

#### Abschnitt 6: Phi-Silica
- **Priorisierte Konzepte**:  
  - NPU-optimierte Architektur  
  - Leistungskennzahlen  
  - Entwicklerintegration  

### Selbstbewertungsfragen

1. Vergleichen Sie die architektonischen Ansätze der Phi- und Qwen-Modellfamilien.  
2. Erklären Sie, wie sich die Quantisierungstechnologie von BitNET von traditioneller Quantisierung unterscheidet.  
3. Welche einzigartigen Vorteile bietet das Mu-Modell für die Integration in Windows?  
4. Beschreiben Sie, wie Phi-Silica NPU-Hardware zur Leistungsoptimierung nutzt.  
5. Für eine mobile Anwendung mit eingeschränkter Konnektivität: Welche Modellfamilie wäre am geeignetsten und warum?  

### Praktische Übungen  

1. **Modellvergleich**: Schneller Benchmark von zwei verschiedenen SLM-Modellen (1 Stunde)  
2. **Einfache Textgenerierung**: Grundlegende Implementierung der Textgenerierung mit einem kleinen Modell (1 Stunde)  
3. **Schnelle Optimierung**: Anwenden einer Optimierungstechnik zur Verbesserung der Inferenzgeschwindigkeit (1 Stunde)  

## Modul 3: Bereitstellung von Small Language Models  

### Wichtige Lernziele  

- Auswahl geeigneter Modelle basierend auf Bereitstellungsbeschränkungen  
- Beherrschen von Optimierungstechniken für verschiedene Bereitstellungsszenarien  
- Implementierung von SLMs in lokalen und Cloud-Umgebungen  
- Entwurf produktionsreifer Konfigurationen für EdgeAI-Anwendungen  

### Schwerpunktbereiche  

#### Abschnitt 1: Fortgeschrittenes Lernen mit SLM  
- **Prioritätskonzepte**:  
  - Parameterklassifikationsrahmen  
  - Fortgeschrittene Optimierungstechniken  
  - Strategien zur Modellauswahl  

#### Abschnitt 2: Bereitstellung in lokalen Umgebungen  
- **Prioritätskonzepte**:  
  - Ollama-Plattform-Bereitstellung  
  - Lokale Lösungen von Microsoft Foundry  
  - Vergleichende Analyse von Frameworks  

#### Abschnitt 3: Containerisierte Cloud-Bereitstellung  
- **Prioritätskonzepte**:  
  - vLLM Hochleistungsinferenz  
  - Container-Orchestrierung  
  - Implementierung von ONNX Runtime  

### Selbstbewertungsfragen  

1. Welche Faktoren sollten bei der Auswahl zwischen lokaler Bereitstellung und Cloud-Bereitstellung berücksichtigt werden?  
2. Vergleichen Sie Ollama und Microsoft Foundry Local als Bereitstellungsoptionen.  
3. Erklären Sie die Vorteile der Containerisierung für die Bereitstellung von SLMs.  
4. Welche Leistungskennzahlen sollten für ein Edge-bereitgestelltes SLM überwacht werden?  
5. Beschreiben Sie einen vollständigen Bereitstellungsworkflow von der Modellauswahl bis zur Produktionsimplementierung.  

### Praktische Übungen  

1. **Einfache lokale Bereitstellung**: Bereitstellung eines einfachen SLM mit Ollama (1 Stunde)  
2. **Leistungsprüfung**: Führen Sie einen schnellen Benchmark für Ihr bereitgestelltes Modell durch (30 Minuten)  
3. **Einfache Integration**: Erstellen Sie eine minimale Anwendung, die Ihr bereitgestelltes Modell verwendet (1 Stunde)  

## Modul 4: Modellformatkonvertierung und Quantisierung  

### Wichtige Lernziele  

- Beherrschen fortgeschrittener Quantisierungstechniken von 1-Bit bis 8-Bit Präzision  
- Verständnis von Strategien zur Formatkonvertierung (GGUF, ONNX)  
- Optimierung über sechs Frameworks (Llama.cpp, Olive, OpenVINO, MLX, Workflow-Synthese)  
- Bereitstellung optimierter Modelle für produktionsreife Edge-Umgebungen auf Intel-, Apple- und plattformübergreifender Hardware  

### Schwerpunktbereiche  

#### Abschnitt 1: Grundlagen der Quantisierung  
- **Prioritätskonzepte**:  
  - Präzisionsklassifikationsrahmen  
  - Abwägung zwischen Leistung und Genauigkeit  
  - Optimierung des Speicherbedarfs  

#### Abschnitt 2: Implementierung von Llama.cpp  
- **Prioritätskonzepte**:  
  - Plattformübergreifende Bereitstellung  
  - Optimierung des GGUF-Formats  
  - Hardware-Beschleunigungstechniken  

#### Abschnitt 3: Microsoft Olive Suite  
- **Prioritätskonzepte**:  
  - Hardware-basierte Optimierung  
  - Unternehmensgerechte Bereitstellung  
  - Automatisierte Optimierungs-Workflows  

#### Abschnitt 4: OpenVINO Toolkit  
- **Prioritätskonzepte**:  
  - Optimierung für Intel-Hardware  
  - Neural Network Compression Framework (NNCF)  
  - Plattformübergreifende Inferenzbereitstellung  
  - OpenVINO GenAI für LLM-Bereitstellung  

#### Abschnitt 5: Apple MLX Framework  
- **Prioritätskonzepte**:  
  - Optimierung für Apple Silicon  
  - Einheitliche Speicherarchitektur  
  - LoRA-Feinabstimmungsmöglichkeiten  

#### Abschnitt 6: Workflow-Synthese für Edge AI-Entwicklung  
- **Prioritätskonzepte**:  
  - Einheitliche Workflow-Architektur  
  - Entscheidungsbäume für die Auswahl von Frameworks  
  - Validierung der Produktionsreife  
  - Zukunftssichere Strategien  

### Selbstbewertungsfragen  

1. Vergleichen Sie Quantisierungsstrategien über verschiedene Präzisionsstufen (1-Bit bis 8-Bit).  
2. Erklären Sie die Vorteile des GGUF-Formats für die Edge-Bereitstellung.  
3. Wie verbessert hardware-basierte Optimierung in Microsoft Olive die Bereitstellungseffizienz?  
4. Was sind die Hauptvorteile von OpenVINOs NNCF für die Modellkompression?  
5. Beschreiben Sie, wie Apple MLX die einheitliche Speicherarchitektur für die Optimierung nutzt.  
6. Wie hilft die Workflow-Synthese bei der Auswahl optimaler Optimierungs-Frameworks?  

### Praktische Übungen  

1. **Modellquantisierung**: Wenden Sie verschiedene Quantisierungsstufen auf ein Modell an und vergleichen Sie die Ergebnisse (1 Stunde)  
2. **OpenVINO-Optimierung**: Verwenden Sie NNCF, um ein Modell für Intel-Hardware zu komprimieren (1 Stunde)  
3. **Framework-Vergleich**: Testen Sie dasselbe Modell in drei verschiedenen Optimierungs-Frameworks (1 Stunde)  
4. **Leistungsbenchmarking**: Messen Sie den Einfluss der Optimierung auf Inferenzgeschwindigkeit und Speicherverbrauch (1 Stunde)  

## Modul 5: SLMOps - Small Language Model Operations  

### Wichtige Lernziele  

- Verständnis der Prinzipien des Lebenszyklusmanagements von SLMOps  
- Beherrschen von Distillations- und Feinabstimmungstechniken für Edge-Bereitstellungen  
- Implementierung von Produktionsbereitstellungsstrategien mit Monitoring  
- Aufbau von unternehmensgerechten Workflows für SLM-Betrieb und Wartung  

### Schwerpunktbereiche  

#### Abschnitt 1: Einführung in SLMOps  
- **Prioritätskonzepte**:  
  - Paradigmenwechsel von SLMOps in der KI-Betriebsführung  
  - Kosten- und Datenschutzorientierte Architektur  
  - Strategische Geschäftsauswirkungen und Wettbewerbsvorteile  

#### Abschnitt 2: Modell-Distillation  
- **Prioritätskonzepte**:  
  - Techniken des Wissenstransfers  
  - Implementierung des zweistufigen Distillationsprozesses  
  - Distillations-Workflows mit Azure ML  

#### Abschnitt 3: Feinabstimmungsstrategien  
- **Prioritätskonzepte**:  
  - Parameter-effiziente Feinabstimmung (PEFT)  
  - Fortgeschrittene Methoden wie LoRA und QLoRA  
  - Multi-Adapter-Training und Hyperparameter-Optimierung  

#### Abschnitt 4: Produktionsbereitstellung  
- **Prioritätskonzepte**:  
  - Modellkonvertierung und Quantisierung für die Produktion  
  - Konfiguration der Foundry Local-Bereitstellung  
  - Leistungsbenchmarking und Qualitätsvalidierung  

### Selbstbewertungsfragen  

1. Wie unterscheidet sich SLMOps von traditionellem MLOps?  
2. Erklären Sie die Vorteile der Modell-Distillation für Edge-Bereitstellungen.  
3. Was sind die wichtigsten Überlegungen zur Feinabstimmung von SLMs in ressourcenbeschränkten Umgebungen?  
4. Beschreiben Sie eine vollständige Produktionsbereitstellungspipeline für Edge-AI-Anwendungen.  

### Praktische Übungen  

1. **Grundlegende Distillation**: Erstellen Sie ein kleineres Modell aus einem größeren Lehrermodell (1 Stunde)  
2. **Feinabstimmungsexperiment**: Feinabstimmung eines Modells für einen spezifischen Bereich (1 Stunde)  
3. **Bereitstellungspipeline**: Einrichten einer grundlegenden CI/CD-Pipeline für die Modellbereitstellung (1 Stunde)  

## Modul 6: SLM Agentensysteme - KI-Agenten und Funktionsaufrufe  

### Wichtige Lernziele  

- Aufbau intelligenter KI-Agenten für Edge-Umgebungen mit Small Language Models  
- Implementierung von Funktionsaufruf-Fähigkeiten mit systematischen Workflows  
- Beherrschen der Integration des Model Context Protocol (MCP) für standardisierte Werkzeuginteraktion  
- Erstellung anspruchsvoller Agentensysteme mit minimaler menschlicher Intervention  

### Schwerpunktbereiche  

#### Abschnitt 1: KI-Agenten und SLM-Grundlagen  
- **Prioritätskonzepte**:  
  - Klassifikationsrahmen für Agenten (Reflex-, modellbasierte, zielbasierte, lernende Agenten)  
  - Analyse der Abwägungen zwischen SLM und LLM  
  - Edge-spezifische Entwurfsmuster für Agenten  
  - Ressourcenoptimierung für Agenten  

#### Abschnitt 2: Funktionsaufrufe in Small Language Models  
- **Prioritätskonzepte**:  
  - Implementierung systematischer Workflows (Absichtserkennung, JSON-Ausgabe, externe Ausführung)  
  - Plattform-spezifische Implementierungen (Phi-4-mini, ausgewählte Qwen-Modelle, Microsoft Foundry Local)  
  - Fortgeschrittene Beispiele (Multi-Agenten-Kollaboration, dynamische Werkzeugauswahl)  
  - Produktionsüberlegungen (Rate-Limitierung, Audit-Logging, Sicherheitsmaßnahmen)  

#### Abschnitt 3: Integration des Model Context Protocol (MCP)  
- **Prioritätskonzepte**:  
  - Protokollarchitektur und schichtweises Systemdesign  
  - Unterstützung für mehrere Backends (Ollama für Entwicklung, vLLM für Produktion)  
  - Verbindungsprotokolle (STDIO- und SSE-Modi)  
  - Anwendungen in der Praxis (Web-Automatisierung, Datenverarbeitung, API-Integration)  

### Selbstbewertungsfragen  

1. Was sind die wichtigsten architektonischen Überlegungen für Edge-KI-Agenten?  
2. Wie verbessern Funktionsaufrufe die Fähigkeiten von Agenten?  
3. Erklären Sie die Rolle des Model Context Protocol in der Agentenkommunikation.  

### Praktische Übungen  

1. **Einfacher Agent**: Erstellen Sie einen grundlegenden KI-Agenten mit Funktionsaufrufen (1 Stunde)  
2. **MCP-Integration**: Implementieren Sie MCP in einer Agentenanwendung (30 Minuten)  

## Modul 7: EdgeAI Implementierungsbeispiele  

### Wichtige Lernziele  

- Beherrschen des AI Toolkits für Visual Studio Code für umfassende EdgeAI-Entwicklungs-Workflows  
- Expertise in der Windows AI Foundry-Plattform und NPU-Optimierungsstrategien  
- Implementierung von EdgeAI auf mehreren Hardwareplattformen und Bereitstellungsszenarien  
- Aufbau produktionsreifer EdgeAI-Anwendungen mit plattform-spezifischen Optimierungen  

### Schwerpunktbereiche  

#### Abschnitt 1: AI Toolkit für Visual Studio Code  
- **Prioritätskonzepte**:  
  - Umfassende Edge-AI-Entwicklungsumgebung innerhalb von VS Code  
  - Modellkatalog und -entdeckung für Edge-Bereitstellungen  
  - Lokale Tests, Optimierung und Agentenentwicklungs-Workflows  
  - Leistungsüberwachung und Bewertung für Edge-Szenarien  

#### Abschnitt 2: Windows EdgeAI Entwicklungsleitfaden  
- **Prioritätskonzepte**:  
  - Umfassender Überblick über die Windows AI Foundry-Plattform  
  - Phi Silica API für effiziente NPU-Inferenz  
  - Computer Vision APIs für Bildverarbeitung und OCR  
  - Foundry Local CLI für lokale Entwicklung und Tests  

#### Abschnitt 3: Plattform-spezifische Implementierungen  
- **Prioritätskonzepte**:  
  - NVIDIA Jetson Orin Nano-Bereitstellung (67 TOPS KI-Leistung)  
  - Mobile Anwendungen mit .NET MAUI und ONNX Runtime GenAI  
  - Azure EdgeAI-Lösungen mit Cloud-Edge-Hybridarchitektur  
  - Windows ML-Optimierung mit universeller Hardwareunterstützung  
  - Foundry Local-Anwendungen mit datenschutzorientierter RAG-Implementierung  

### Selbstbewertungsfragen  

1. Wie vereinfacht das AI Toolkit den EdgeAI-Entwicklungsworkflow?  
2. Vergleichen Sie Bereitstellungsstrategien auf verschiedenen Hardwareplattformen.  
3. Welche Vorteile bietet Windows AI Foundry für die Edge-Entwicklung?  
4. Erklären Sie die Rolle der NPU-Optimierung in modernen Edge-AI-Anwendungen.  
5. Wie nutzt die Phi Silica API NPU-Hardware zur Leistungsoptimierung?  
6. Vergleichen Sie die Vorteile von lokaler vs. Cloud-Bereitstellung für datenschutzsensible Anwendungen.  

### Praktische Übungen  

1. **AI Toolkit Einrichtung**: Konfigurieren Sie das AI Toolkit und optimieren Sie ein Modell (1 Stunde)  
2. **Windows AI Foundry**: Erstellen Sie eine einfache Windows-AI-Anwendung mit der Phi Silica API (1 Stunde)  
3. **Plattformübergreifende Bereitstellung**: Bereitstellung desselben Modells auf zwei verschiedenen Plattformen (1 Stunde)  
4. **NPU-Optimierung**: Testen Sie die NPU-Leistung mit den Tools von Windows AI Foundry (30 Minuten)  

## Modul 8: Microsoft Foundry Local – Komplettes Entwickler-Toolkit (Modernisiert)  

### Wichtige Lernziele  

- Installation und Konfiguration von Foundry Local mit moderner SDK-Integration  
- Implementierung fortgeschrittener Multi-Agenten-Systeme mit Koordinator-Mustern  
- Aufbau intelligenter Modellrouter mit automatischer aufgabenbasierter Auswahl  
- Bereitstellung produktionsreifer KI-Lösungen mit umfassendem Monitoring  
- Integration mit Azure AI Foundry für hybride Bereitstellungsszenarien  
- Beherrschen moderner SDK-Muster mit FoundryLocalManager und OpenAI-Client  

### Schwerpunktbereiche  

#### Abschnitt 1: Moderne Installation und Konfiguration  
- **Prioritätskonzepte**:  
  - Integration des FoundryLocalManager SDK  
  - Automatische Dienstentdeckung und Gesundheitsüberwachung  
  - Konfigurationsmuster basierend auf Umgebungen  
  - Überlegungen zur Produktionsbereitstellung  

#### Abschnitt 2: Fortgeschrittene Multi-Agenten-Systeme  
- **Prioritätskonzepte**:  
  - Koordinator-Muster mit spezialisierten Agenten  
  - Spezialisierung auf Abruf-, Argumentations- und Ausführungsagenten  
  - Feedback-Mechanismen zur Verfeinerung  
  - Leistungsüberwachung und Statistikverfolgung  

#### Abschnitt 3: Intelligente Modell-Routing  
- **Prioritätskonzepte**:  
  - Schlüsselwortbasierte Modell-Auswahlalgorithmen  
  - Unterstützung mehrerer Modelle (allgemein, Argumentation, Code, kreativ)  
  - Konfiguration von Umgebungsvariablen für Flexibilität  
  - Dienstgesundheitsprüfung und Fehlerbehandlung  

#### Abschnitt 4: Produktionsreife Implementierung  
- **Prioritätskonzepte**:  
  - Umfassende Fehlerbehandlung und Fallback-Mechanismen  
  - Anfragen-Monitoring und Leistungsüberwachung  
  - Interaktive Jupyter-Notebook-Beispiele mit Benchmarks  
  - Integrationsmuster mit bestehenden Anwendungen  

### Selbstbewertungsfragen  

1. Wie unterscheidet sich der moderne Ansatz des FoundryLocalManager von manuellen REST-Aufrufen?  
2. Erklären Sie das Koordinator-Muster und wie es spezialisierte Agenten orchestriert.  
3. Wie wählt der intelligente Router geeignete Modelle basierend auf dem Inhalt der Anfrage aus?  
4. Was sind die Hauptkomponenten eines produktionsreifen KI-Agentensystems?  
5. Wie implementieren Sie umfassende Gesundheitsüberwachung für Foundry Local-Dienste?  
6. Vergleichen Sie die Vorteile des modernisierten Ansatzes gegenüber traditionellen Implementierungsmustern.  

### Praktische Übungen  

1. **Moderne SDK-Einrichtung**: Konfigurieren Sie FoundryLocalManager mit automatischer Dienstentdeckung (30 Minuten)  
2. **Multi-Agenten-System**: Führen Sie den fortgeschrittenen Koordinator mit spezialisierten Agenten aus (30 Minuten)  
3. **Intelligentes Routing**: Testen Sie den Modellrouter mit verschiedenen Anfragearten (30 Minuten)  
4. **Interaktive Erkundung**: Verwenden Sie die Jupyter-Notebooks, um fortgeschrittene Funktionen zu erkunden (45 Minuten)  
5. **Produktionsbereitstellung**: Implementieren Sie Monitoring- und Fehlerbehandlungsmuster (30 Minuten)  
6. **Hybride Integration**: Konfigurieren Sie Azure AI Foundry-Fallback-Szenarien (30 Minuten)  

## Zeitaufteilung  

Um Ihnen zu helfen, das Beste aus dem 20-stündigen Kurszeitplan herauszuholen, finden Sie hier eine empfohlene Aufschlüsselung der Zeitzuweisung:  

| Aktivität | Zeitzuweisung | Beschreibung |  
|-----------|---------------|--------------|  
| Lesen der Kernmaterialien | 9 Stunden | Fokus auf die wesentlichen Konzepte in jedem Modul |  
| Praktische Übungen | 6 Stunden | Praktische Umsetzung der wichtigsten Techniken |
| Selbstbewertung | 2 Stunden | Testen des eigenen Verständnisses durch Fragen und Reflexion |
| Mini-Projekt | 3 Stunden | Anwendung des Wissens in einer kleinen praktischen Umsetzung |

### Wichtige Schwerpunktbereiche nach Zeitvorgabe

**Wenn Sie nur 10 Stunden haben:**
- Absolvieren Sie Modul 0 (Einführung) sowie Module 1, 2 und 3 (Kernkonzepte von EdgeAI)
- Machen Sie mindestens eine praktische Übung pro Modul
- Konzentrieren Sie sich auf das Verständnis der Kernkonzepte, statt auf Details der Implementierung

**Wenn Sie die vollen 20 Stunden investieren können:**
- Absolvieren Sie alle acht Module (einschließlich Einführung)
- Führen Sie die wichtigsten praktischen Übungen aus jedem Modul durch
- Schließen Sie ein Mini-Projekt aus Modul 7 ab
- Erkunden Sie mindestens 2-3 ergänzende Ressourcen

**Wenn Sie mehr als 20 Stunden zur Verfügung haben:**
- Absolvieren Sie alle Module (einschließlich Einführung) mit detaillierten Übungen
- Erstellen Sie mehrere Mini-Projekte
- Erkunden Sie fortgeschrittene Optimierungstechniken in Modul 4
- Implementieren Sie die Produktionsbereitstellung aus Modul 5

## Wesentliche Ressourcen

Diese sorgfältig ausgewählten Ressourcen bieten den größten Nutzen für Ihre begrenzte Lernzeit:

### Unverzichtbare Dokumentationen
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Das effizienteste Werkzeug zur Modelloptimierung
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Schnellster Weg zur lokalen Bereitstellung von SLMs
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referenz für ein führendes, edge-optimiertes Modell
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intels umfassendes Optimierungstoolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrierte EdgeAI-Entwicklungsumgebung
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-spezifische EdgeAI-Entwicklungsplattform

### Zeitersparende Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Schneller Zugriff auf Modelle und deren Bereitstellung
- [Gradio](https://www.gradio.app/docs/interface) - Schnelle UI-Entwicklung für AI-Demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Vereinfachte Modelloptimierung
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Effiziente CPU-Inferenz
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework zur Kompression neuronaler Netze
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit zur Bereitstellung großer Sprachmodelle

## Fortschritts-Tracking-Vorlage

Nutzen Sie diese vereinfachte Vorlage, um Ihren Lernfortschritt im 20-Stunden-Kurs zu verfolgen:

| Modul | Abschlussdatum | Aufgewendete Stunden | Wichtige Erkenntnisse |
|-------|----------------|----------------------|-----------------------|
| Modul 0: Einführung in EdgeAI | | | |
| Modul 1: Grundlagen von EdgeAI | | | |
| Modul 2: SLM-Grundlagen | | | |
| Modul 3: SLM-Bereitstellung | | | |
| Modul 4: Modelloptimierung | | | |
| Modul 5: SLMOps | | | |
| Modul 6: KI-Agenten | | | |
| Modul 7: Entwicklungstools | | | |
| Modul 8: Foundry Local Toolkit | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

## Mini-Projektideen

Erwägen Sie, eines dieser Projekte abzuschließen, um EdgeAI-Konzepte zu üben (jedes ist so konzipiert, dass es 2-4 Stunden dauert):

### Anfängerprojekte (jeweils 2-3 Stunden)
1. **Edge Text Assistant**: Erstellen Sie ein einfaches Offline-Textvervollständigungs-Tool mit einem kleinen Sprachmodell
2. **Modellvergleichs-Dashboard**: Erstellen Sie eine grundlegende Visualisierung von Leistungskennzahlen verschiedener SLMs
3. **Optimierungsexperiment**: Messen Sie die Auswirkungen verschiedener Quantisierungsstufen auf dasselbe Basismodell

### Projekte für Fortgeschrittene (jeweils 3-4 Stunden)
4. **AI Toolkit Workflow**: Nutzen Sie das VS Code AI Toolkit, um ein Modell von Anfang bis Ende zu optimieren und bereitzustellen
5. **Windows AI Foundry Anwendung**: Erstellen Sie eine Windows-App mit Phi Silica API und NPU-Optimierung
6. **Plattformübergreifende Bereitstellung**: Stellen Sie dasselbe optimierte Modell auf Windows (OpenVINO) und Mobilgeräten (.NET MAUI) bereit
7. **Funktionsaufruf-Agent**: Erstellen Sie einen KI-Agenten mit Funktionsaufruf-Fähigkeiten für Edge-Szenarien

### Projekte zur fortgeschrittenen Integration (jeweils 4-5 Stunden)
8. **OpenVINO-Optimierungspipeline**: Implementieren Sie eine vollständige Modelloptimierung mit NNCF und GenAI-Toolkit
9. **SLMOps-Pipeline**: Implementieren Sie einen vollständigen Modelllebenszyklus von Training bis zur Edge-Bereitstellung
10. **Multi-Modell-Edge-System**: Stellen Sie mehrere spezialisierte Modelle bereit, die zusammen auf Edge-Hardware arbeiten
11. **MCP-Integrationssystem**: Erstellen Sie ein agentenbasiertes System mit Model Context Protocol für Tool-Interaktionen

## Referenzen

- Microsoft Learn (Foundry Local)
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Erste Schritte: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - CLI-Referenz: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integration mit Inferenz-SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Open WebUI-Anleitung: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Hugging Face Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agenten (Übersicht): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Optimierungs- und Inferenz-Tools
  - Microsoft Olive (Dokumentation): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (Erste Schritte): https://onnxruntime.ai/docs/get-started/with-python.html
  - ONNX Runtime Olive-Integration: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (Dokumentation): https://docs.openvino.ai/2025/index.html
  - Apple MLX (Dokumentation): https://ml-explore.github.io/mlx/build/html/index.html
- Bereitstellungs-Frameworks und Modelle
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (Dokumentation): https://docs.vllm.ai/
  - Ollama (Erste Schritte): https://github.com/ollama/ollama#get-started
- Entwickler-Tools (Windows und VS Code)
  - AI Toolkit für VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (Übersicht): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Lern-Community

Treten Sie der Diskussion bei und vernetzen Sie sich mit anderen Lernenden:
- GitHub-Diskussionen im [EdgeAI for Beginners Repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Fazit

EdgeAI repräsentiert die Spitze der künstlichen Intelligenz, indem leistungsstarke Fähigkeiten direkt auf Geräte gebracht werden und gleichzeitig wichtige Aspekte wie Datenschutz, Latenz und Konnektivität berücksichtigt werden. Dieser 20-Stunden-Kurs vermittelt Ihnen das wesentliche Wissen und die praktischen Fähigkeiten, um sofort mit EdgeAI-Technologien zu arbeiten.

Der Kurs ist bewusst prägnant und konzentriert sich auf die wichtigsten Konzepte, sodass Sie schnell wertvolle Expertise gewinnen können, ohne sich überfordert zu fühlen. Denken Sie daran, dass praktische Übungen, selbst mit einfachen Beispielen, der Schlüssel zur Festigung des Gelernten sind.

Viel Erfolg beim Lernen!

---

