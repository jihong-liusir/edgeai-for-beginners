<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T12:46:41+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "de"
}
-->
# EdgeAI für Anfänger: Lernpfade und Studienplan

### Konzentrierter Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1 | Modul 1: EdgeAI Grundlagen | 3 Stunden |
| Tag 2 | Modul 2: SLM Grundlagen | 3 Stunden |
| Tag 3 | Modul 3: SLM Bereitstellung | 2 Stunden |
| Tag 4-5 | Modul 4: Modelloptimierung (6 Frameworks) | 4 Stunden |
| Tag 6 | Modul 5: SLMOps | 3 Stunden |
| Tag 7 | Modul 6-7: KI-Agenten & Entwicklungstools | 5 Stunden |

### Konzentrierter Lernpfad (2 Wochen)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: EdgeAI Grundlagen | 3 Stunden |
| Tag 3-4 | Modul 2: SLM Grundlagen | 3 Stunden |
| Tag 5-6 | Modul 3: SLM Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: KI-Agenten | 2 Stunden |
| Tag 13-14 | Modul 7: Entwicklungstools | 3 Stunden |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|-------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: EdgeAI Grundlagen | 3 Stunden |
| Tag 3-4 | Modul 2: SLM Grundlagen | 3 Stunden |
| Tag 5-6 | Modul 3: SLM Bereitstellung | 2 Stunden |
| Tag 7-8 | Modul 4: Modelloptimierung | 4 Stunden |
| Tag 9-10 | Modul 5: SLMOps | 3 Stunden |
| Tag 11-12 | Modul 6: SLM Agentensysteme | 2 Stunden |
| Tag 13-14 | Modul 7: EdgeAI Implementierungsbeispiele | 2 Stunden |

| Modul | Abschlussdatum | Aufgewendete Stunden | Wichtige Erkenntnisse |
|-------|----------------|----------------------|-----------------------|
| Modul 1: EdgeAI Grundlagen | | | |
| Modul 2: SLM Grundlagen | | | |
| Modul 3: SLM Bereitstellung | | | |
| Modul 4: Modelloptimierung (6 Frameworks) | | | |
| Modul 5: SLMOps | | | |
| Modul 6: SLM Agentensysteme | | | |
| Modul 7: EdgeAI Implementierungsbeispiele | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

### Teilzeitstudium (4 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|-------------|--------------------|
| Woche 1 | Modul 1-2: Grundlagen & SLM Grundlagen | 6 Stunden |
| Woche 2 | Modul 3-4: Bereitstellung & Optimierung | 6 Stunden |
| Woche 3 | Modul 5-6: SLMOps & KI-Agenten | 5 Stunden |
| Woche 4 | Modul 7: Entwicklungstools & Integration | 3 Stunden |

## Einführung

Willkommen zum Studienleitfaden für EdgeAI für Anfänger! Dieses Dokument soll Ihnen helfen, die Kursmaterialien effektiv zu nutzen und Ihr Lernerlebnis zu maximieren. Es bietet strukturierte Lernpfade, empfohlene Studienpläne, Zusammenfassungen wichtiger Konzepte und ergänzende Ressourcen, um Ihr Verständnis von EdgeAI-Technologien zu vertiefen.

Dies ist ein kompakter 20-Stunden-Kurs, der essenzielles Wissen über EdgeAI in einem zeiteffizienten Format vermittelt – ideal für vielbeschäftigte Fachleute und Studierende, die schnell praktische Fähigkeiten in diesem aufstrebenden Bereich erwerben möchten.

## Kursübersicht

Der Kurs ist in sieben umfassende Module unterteilt:

1. **EdgeAI Grundlagen und Transformation** – Verständnis der Kernkonzepte und des technologischen Wandels
2. **Small Language Model Grundlagen** – Erforschung verschiedener SLM-Familien und ihrer Architekturen
3. **Small Language Model Bereitstellung** – Praktische Strategien zur Implementierung
4. **Modellformatkonvertierung und Quantisierung** – Fortgeschrittene Optimierung mit 6 Frameworks, einschließlich OpenVINO
5. **SLMOps – Small Language Model Operations** – Produktionslebenszyklusmanagement und Bereitstellung
6. **SLM Agentensysteme** – KI-Agenten, Funktionsaufrufe und Model Context Protocol
7. **EdgeAI Implementierungsbeispiele** – KI-Toolkit, Windows-Entwicklung und plattformspezifische Implementierungen
8. **Microsoft Foundry Local – Komplettes Entwickler-Toolkit** – Lokale Entwicklung mit hybrider Azure-Integration (Modul 08)

## So nutzen Sie diesen Studienleitfaden

- **Progressives Lernen**: Folgen Sie den Modulen in der vorgegebenen Reihenfolge für ein kohärentes Lernerlebnis.
- **Wissenscheckpunkte**: Nutzen Sie die Selbstbewertungsfragen nach jedem Abschnitt.
- **Praktische Übungen**: Schließen Sie die vorgeschlagenen Übungen ab, um theoretische Konzepte zu festigen.
- **Ergänzende Ressourcen**: Erkunden Sie zusätzliche Materialien zu Themen, die Sie besonders interessieren.

## Empfehlungen für den Studienplan

### Konzentrierter Lernpfad (1 Woche)

| Tag | Schwerpunkt | Geschätzte Stunden |
|------|------------|--------------------|
| Tag 1-2 | Modul 1: EdgeAI Grundlagen | 6 Stunden |
| Tag 3-4 | Modul 2: SLM Grundlagen | 8 Stunden |
| Tag 5 | Modul 3: SLM Bereitstellung | 3 Stunden |
| Tag 6 | Modul 8: Foundry Local Toolkit | 3 Stunden |

### Teilzeitstudium (3 Wochen)

| Woche | Schwerpunkt | Geschätzte Stunden |
|-------|-------------|--------------------|
| Woche 1 | Modul 1: EdgeAI Grundlagen | 6-7 Stunden |
| Woche 2 | Modul 2: SLM Grundlagen | 7-8 Stunden |
| Woche 3 | Modul 3: SLM Bereitstellung (3h) + Modul 8: Foundry Local Toolkit (2-3h) | 5-6 Stunden |

## Modul 1: EdgeAI Grundlagen und Transformation

### Wichtige Lernziele

- Unterschiede zwischen cloudbasiertem und edgebasiertem KI verstehen
- Kernoptimierungstechniken für ressourcenbeschränkte Umgebungen beherrschen
- Analyse von realen Anwendungen von EdgeAI-Technologien
- Entwicklungsumgebung für EdgeAI-Projekte einrichten

### Studien-Schwerpunkte

#### Abschnitt 1: EdgeAI Grundlagen
- **Prioritätskonzepte**: 
  - Paradigmen von Edge- vs. Cloud-Computing
  - Modellquantisierungstechniken
  - Hardware-Beschleunigungsoptionen (NPUs, GPUs, CPUs)
  - Vorteile in Bezug auf Datenschutz und Sicherheit

- **Ergänzende Materialien**:
  - [TensorFlow Lite Dokumentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Dokumentation](https://docs.edgeimpulse.com)

#### Abschnitt 2: Fallstudien aus der Praxis
- **Prioritätskonzepte**: 
  - Microsoft Phi & Mu Modell-Ökosystem
  - Praktische Implementierungen in verschiedenen Branchen
  - Bereitstellungsüberlegungen

#### Abschnitt 3: Praktischer Implementierungsleitfaden
- **Prioritätskonzepte**: 
  - Einrichtung der Entwicklungsumgebung
  - Quantisierungs- und Optimierungstools
  - Bewertungsmethoden für EdgeAI-Implementierungen

#### Abschnitt 4: Edge-Bereitstellungshardware
- **Prioritätskonzepte**: 
  - Vergleich von Hardwareplattformen
  - Optimierungsstrategien für spezifische Hardware
  - Bereitstellungsüberlegungen

### Selbstbewertungsfragen

1. Vergleichen Sie cloudbasierte KI mit edgebasierter KI-Implementierung.
2. Erklären Sie drei Schlüsseltechniken zur Optimierung von Modellen für die Edge-Bereitstellung.
3. Was sind die Hauptvorteile der Ausführung von KI-Modellen am Edge?
4. Beschreiben Sie den Prozess der Modellquantisierung und wie er die Leistung beeinflusst.
5. Erklären Sie, wie verschiedene Hardware-Beschleuniger (NPUs, GPUs, CPUs) die EdgeAI-Bereitstellung beeinflussen.

### Praktische Übungen

1. **Schnelle Einrichtung der Umgebung**: Konfigurieren Sie eine minimale Entwicklungsumgebung mit den wesentlichen Paketen (30 Minuten)
2. **Modell-Erkundung**: Laden Sie ein vortrainiertes Small Language Model herunter und untersuchen Sie es (1 Stunde)
3. **Grundlegende Quantisierung**: Probieren Sie eine einfache Quantisierung an einem kleinen Modell aus (1 Stunde)

## Modul 2: Small Language Model Grundlagen

### Wichtige Lernziele

- Architektonische Prinzipien verschiedener SLM-Familien verstehen
- Modellfähigkeiten über verschiedene Parameter-Skalen hinweg vergleichen
- Modelle basierend auf Effizienz, Fähigkeiten und Bereitstellungsanforderungen bewerten
- Geeignete Anwendungsfälle für verschiedene Modellfamilien erkennen

### Studien-Schwerpunkte

#### Abschnitt 1: Microsoft Phi Modellfamilie
- **Prioritätskonzepte**: 
  - Entwicklung der Designphilosophie
  - Architektur mit Fokus auf Effizienz
  - Spezialisierte Fähigkeiten

#### Abschnitt 2: Qwen Familie
- **Prioritätskonzepte**: 
  - Beiträge aus der Open-Source-Community
  - Skalierbare Bereitstellungsoptionen
  - Architektur für fortgeschrittenes logisches Denken

#### Abschnitt 3: Gemma Familie
- **Prioritätskonzepte**: 
  - Innovationsgetriebene Forschung
  - Multimodale Fähigkeiten
  - Optimierung für mobile Geräte

#### Abschnitt 4: BitNET Familie
- **Prioritätskonzepte**: 
  - 1-Bit-Quantisierungstechnologie
  - Framework für Inferenzoptimierung
  - Nachhaltigkeitsüberlegungen

#### Abschnitt 5: Microsoft Mu Modell
- **Prioritätskonzepte**: 
  - Architektur mit Fokus auf Geräte
  - Systemintegration mit Windows
  - Datenschutzfreundlicher Betrieb

#### Abschnitt 6: Phi-Silica
- **Prioritätskonzepte**: 
  - NPU-optimierte Architektur
  - Leistungskennzahlen
  - Entwicklerintegration

### Selbstbewertungsfragen

1. Vergleichen Sie die architektonischen Ansätze der Phi- und Qwen-Modellfamilien.
2. Erklären Sie, wie sich die Quantisierungstechnologie von BitNET von herkömmlicher Quantisierung unterscheidet.
3. Was sind die einzigartigen Vorteile des Mu-Modells für die Windows-Integration?
4. Beschreiben Sie, wie Phi-Silica NPU-Hardware für Leistungsoptimierung nutzt.
5. Für eine mobile Anwendung mit eingeschränkter Konnektivität: Welche Modellfamilie wäre am besten geeignet und warum?

### Praktische Übungen

1. **Modellvergleich**: Schneller Benchmark von zwei verschiedenen SLM-Modellen (1 Stunde)
2. **Einfache Textgenerierung**: Grundlegende Implementierung der Textgenerierung mit einem kleinen Modell (1 Stunde)
3. **Schnelle Optimierung**: Wenden Sie eine Optimierungstechnik an, um die Inferenzgeschwindigkeit zu verbessern (1 Stunde)

## Modul 3: Small Language Model Bereitstellung

### Wichtige Lernziele

- Geeignete Modelle basierend auf Bereitstellungsbeschränkungen auswählen
- Optimierungstechniken für verschiedene Bereitstellungsszenarien beherrschen
- SLMs sowohl in lokalen als auch in Cloud-Umgebungen implementieren
- Produktionsreife Konfigurationen für EdgeAI-Anwendungen entwerfen

### Studien-Schwerpunkte

#### Abschnitt 1: Fortgeschrittenes Lernen mit SLM
- **Prioritätskonzepte**: 
  - Parameterklassifizierungs-Framework
  - Fortgeschrittene Optimierungstechniken
  - Strategien zur Modellbeschaffung

#### Abschnitt 2: Lokale Bereitstellung
- **Prioritätskonzepte**: 
  - Ollama Plattform-Bereitstellung
  - Microsoft Foundry lokale Lösungen
  - Vergleichende Analyse von Frameworks

#### Abschnitt 3: Containerisierte Cloud-Bereitstellung
- **Prioritätskonzepte**: 
  - vLLM Hochleistungsinferenz
  - Container-Orchestrierung
  - ONNX Runtime Implementierung

### Selbstbewertungsfragen

1. Welche Faktoren sollten bei der Auswahl zwischen lokaler Bereitstellung und Cloud-Bereitstellung berücksichtigt werden?
2. Vergleichen Sie Ollama und Microsoft Foundry Local als Bereitstellungsoptionen.
3. Erklären Sie die Vorteile der Containerisierung für die SLM-Bereitstellung.
4. Was sind die wichtigsten Leistungskennzahlen, die bei einem Edge-bereitgestellten SLM überwacht werden sollten?
5. Beschreiben Sie einen vollständigen Bereitstellungsworkflow von der Modellauswahl bis zur Produktionsimplementierung.

### Praktische Übungen

1. **Grundlegende lokale Bereitstellung**: Bereitstellung eines einfachen SLM mit Ollama (1 Stunde)
2. **Leistungsüberprüfung**: Führen Sie einen schnellen Benchmark für Ihr bereitgestelltes Modell durch (30 Minuten)
3. **Einfache Integration**: Erstellen Sie eine minimale Anwendung, die Ihr bereitgestelltes Modell verwendet (1 Stunde)

## Modul 4: Modellformatkonvertierung und Quantisierung

### Wichtige Lernziele

- Fortgeschrittene Quantisierungstechniken von 1-Bit bis 8-Bit Präzision beherrschen
- Strategien zur Formatkonvertierung verstehen (GGUF, ONNX)
- Optimierung über sechs Frameworks implementieren (Llama.cpp, Olive, OpenVINO, MLX, Workflow-Synthese)
- Optimierte Modelle für Produktionsumgebungen am Edge auf Intel-, Apple- und plattformübergreifender Hardware bereitstellen

### Studien-Schwerpunkte

#### Abschnitt 1: Grundlagen der Quantisierung
- **Prioritätskonzepte**: 
  - Präzisionsklassifizierungs-Framework
  - Abwägung zwischen Leistung und Genauigkeit
  - Optimierung des Speicherbedarfs

#### Abschnitt 2: Llama.cpp Implementierung
- **Prioritätskonzepte**: 
  - Plattformübergreifende Bereitstellung
  - GGUF Formatoptimierung
  - Hardware-Beschleunigungstechniken

#### Abschnitt 3: Microsoft Olive Suite
- **Prioritätskonzepte**: 
  - Hardwarebewusste Optimierung
  - Unternehmensgerechte Bereitstellung
  - Automatisierte Optimierungs-Workflows

#### Abschnitt 4: OpenVINO Toolkit
- **Prioritätskonzepte**: 
  - Intel Hardware-Optimierung
  - Neural Network Compression Framework (NNCF)
  - Plattformübergreifende Inferenzbereitstellung
  - OpenVINO GenAI für LLM-Bereitstellung

#### Abschnitt 5: Apple MLX Framework
- **Prioritätskonzepte**: 
  - Optimierung für Apple Silicon
  - Einheitliche Speicherarchitektur
  - LoRA-Fine-Tuning-Fähigkeiten

#### Abschnitt 6: Synthese des Edge-AI-Entwicklungsworkflows
- **Prioritätskonzepte**: 
  - Einheitliche Workflow-Architektur
  - Entscheidungsbäume für Framework-Auswahl
  - Validierung der Produktionsbereitschaft
  - Strategien zur Zukunftssicherung

### Selbstbewertungsfragen

1. Vergleichen Sie Quantisierungsstrategien über verschiedene Präzisionsstufen (1-Bit bis 8-Bit).
2. Erklären Sie die Vorteile des GGUF-Formats für Edge-Deployment.
3. Wie verbessert hardwarebewusste Optimierung in Microsoft Olive die Effizienz beim Deployment?
4. Was sind die Hauptvorteile von OpenVINOs NNCF für die Modellkompression?
5. Beschreiben Sie, wie Apple MLX die einheitliche Speicherarchitektur für Optimierungen nutzt.
6. Wie hilft die Synthese von Workflows bei der Auswahl optimaler Optimierungs-Frameworks?

### Praktische Übungen

1. **Modellquantisierung**: Wenden Sie verschiedene Quantisierungsstufen auf ein Modell an und vergleichen Sie die Ergebnisse (1 Stunde)
2. **OpenVINO-Optimierung**: Verwenden Sie NNCF, um ein Modell für Intel-Hardware zu komprimieren (1 Stunde)
3. **Framework-Vergleich**: Testen Sie dasselbe Modell in drei verschiedenen Optimierungs-Frameworks (1 Stunde)
4. **Leistungsbenchmarking**: Messen Sie den Einfluss der Optimierung auf die Inferenzgeschwindigkeit und den Speicherverbrauch (1 Stunde)

## Modul 5: SLMOps - Betrieb von kleinen Sprachmodellen

### Wichtige Lernziele

- Verstehen der Prinzipien des Lebenszyklusmanagements von SLMOps
- Beherrschen von Distillations- und Fine-Tuning-Techniken für Edge-Deployment
- Implementieren von Produktions-Deployment-Strategien mit Monitoring
- Aufbau von unternehmensgerechten Workflows für Betrieb und Wartung von SLMs

### Studienfokusbereiche

#### Abschnitt 1: Einführung in SLMOps
- **Prioritätskonzepte**: 
  - Paradigmenwechsel von SLMOps in der KI-Betriebsführung
  - Kosten- und Datenschutzorientierte Architektur
  - Strategische Geschäftsauswirkungen und Wettbewerbsvorteile

#### Abschnitt 2: Modell-Distillation
- **Prioritätskonzepte**: 
  - Techniken des Wissenstransfers
  - Implementierung des zweistufigen Distillationsprozesses
  - Distillations-Workflows mit Azure ML

#### Abschnitt 3: Fine-Tuning-Strategien
- **Prioritätskonzepte**: 
  - Parameter-effizientes Fine-Tuning (PEFT)
  - Fortgeschrittene Methoden wie LoRA und QLoRA
  - Multi-Adapter-Training und Hyperparameter-Optimierung

#### Abschnitt 4: Produktions-Deployment
- **Prioritätskonzepte**: 
  - Modellkonvertierung und Quantisierung für die Produktion
  - Konfiguration für Foundry Local Deployment
  - Leistungsbenchmarking und Qualitätsvalidierung

### Selbstbewertungsfragen

1. Wie unterscheidet sich SLMOps von traditionellem MLOps?
2. Erklären Sie die Vorteile der Modell-Distillation für Edge-Deployment.
3. Was sind die wichtigsten Überlegungen für das Fine-Tuning von SLMs in ressourcenbeschränkten Umgebungen?
4. Beschreiben Sie eine vollständige Produktions-Deployment-Pipeline für Edge-AI-Anwendungen.

### Praktische Übungen

1. **Grundlegende Distillation**: Erstellen Sie ein kleineres Modell aus einem größeren Lehrermodell (1 Stunde)
2. **Fine-Tuning-Experiment**: Fine-Tunen Sie ein Modell für einen spezifischen Anwendungsbereich (1 Stunde)
3. **Deployment-Pipeline**: Richten Sie eine grundlegende CI/CD-Pipeline für das Modell-Deployment ein (1 Stunde)

## Modul 6: SLM-Agentensysteme - KI-Agenten und Funktionsaufrufe

### Wichtige Lernziele

- Intelligente KI-Agenten für Edge-Umgebungen mit kleinen Sprachmodellen erstellen
- Funktionsaufruf-Fähigkeiten mit systematischen Workflows implementieren
- Model Context Protocol (MCP) für standardisierte Tool-Interaktion integrieren
- Komplexe Agentensysteme mit minimaler menschlicher Intervention entwickeln

### Studienfokusbereiche

#### Abschnitt 1: KI-Agenten und SLM-Grundlagen
- **Prioritätskonzepte**: 
  - Klassifikationsrahmen für Agenten (Reflex-, modellbasierte, zielbasierte, lernende Agenten)
  - Analyse der Trade-offs zwischen SLM und LLM
  - Edge-spezifische Designmuster für Agenten
  - Ressourcenoptimierung für Agenten

#### Abschnitt 2: Funktionsaufrufe in kleinen Sprachmodellen
- **Prioritätskonzepte**: 
  - Systematische Workflow-Implementierung (Intent-Erkennung, JSON-Ausgabe, externe Ausführung)
  - Plattform-spezifische Implementierungen (Phi-4-mini, ausgewählte Qwen-Modelle, Microsoft Foundry Local)
  - Fortgeschrittene Beispiele (Multi-Agenten-Kollaboration, dynamische Tool-Auswahl)
  - Produktionsüberlegungen (Rate-Limiting, Audit-Logging, Sicherheitsmaßnahmen)

#### Abschnitt 3: Integration des Model Context Protocol (MCP)
- **Prioritätskonzepte**: 
  - Protokollarchitektur und schichtbasiertes Systemdesign
  - Unterstützung für mehrere Backends (Ollama für Entwicklung, vLLM für Produktion)
  - Verbindungsprotokolle (STDIO- und SSE-Modi)
  - Anwendungen in der Praxis (Web-Automatisierung, Datenverarbeitung, API-Integration)

### Selbstbewertungsfragen

1. Was sind die wichtigsten architektonischen Überlegungen für Edge-KI-Agenten?
2. Wie verbessern Funktionsaufrufe die Fähigkeiten von Agenten?
3. Erklären Sie die Rolle des Model Context Protocol in der Agentenkommunikation.

### Praktische Übungen

1. **Einfacher Agent**: Erstellen Sie einen grundlegenden KI-Agenten mit Funktionsaufrufen (1 Stunde)
2. **MCP-Integration**: Implementieren Sie MCP in einer Agentenanwendung (30 Minuten)

## Modul 7: EdgeAI-Implementierungsbeispiele

### Wichtige Lernziele

- Beherrschen des AI Toolkit für Visual Studio Code für umfassende EdgeAI-Entwicklungsworkflows
- Expertise in der Windows AI Foundry-Plattform und NPU-Optimierungsstrategien gewinnen
- EdgeAI auf verschiedenen Hardwareplattformen und Deployment-Szenarien implementieren
- Produktionsreife EdgeAI-Anwendungen mit plattformspezifischen Optimierungen erstellen

### Studienfokusbereiche

#### Abschnitt 1: AI Toolkit für Visual Studio Code
- **Prioritätskonzepte**: 
  - Umfassende Edge-AI-Entwicklungsumgebung innerhalb von VS Code
  - Modellkatalog und -entdeckung für Edge-Deployment
  - Lokale Tests, Optimierung und Agentenentwicklungs-Workflows
  - Leistungsüberwachung und Bewertung für Edge-Szenarien

#### Abschnitt 2: Windows EdgeAI-Entwicklungsleitfaden
- **Prioritätskonzepte**: 
  - Umfassender Überblick über die Windows AI Foundry-Plattform
  - Phi Silica API für effiziente NPU-Inferenz
  - Computer Vision APIs für Bildverarbeitung und OCR
  - Foundry Local CLI für lokale Entwicklung und Tests

#### Abschnitt 3: Plattform-spezifische Implementierungen
- **Prioritätskonzepte**: 
  - NVIDIA Jetson Orin Nano Deployment (67 TOPS KI-Leistung)
  - Mobile Anwendungen mit .NET MAUI und ONNX Runtime GenAI
  - Azure EdgeAI-Lösungen mit Cloud-Edge-Hybridarchitektur
  - Windows ML-Optimierung mit universeller Hardwareunterstützung
  - Foundry Local-Anwendungen mit datenschutzorientierter RAG-Implementierung

### Selbstbewertungsfragen

1. Wie vereinfacht das AI Toolkit den EdgeAI-Entwicklungsworkflow?
2. Vergleichen Sie Deployment-Strategien auf verschiedenen Hardwareplattformen.
3. Was sind die Vorteile der Windows AI Foundry für die Edge-Entwicklung?
4. Erklären Sie die Rolle der NPU-Optimierung in modernen Edge-AI-Anwendungen.
5. Wie nutzt die Phi Silica API NPU-Hardware für Leistungsoptimierung?
6. Vergleichen Sie die Vorteile von lokalem vs. Cloud-Deployment für datenschutzsensible Anwendungen.

### Praktische Übungen

1. **AI Toolkit Setup**: Konfigurieren Sie das AI Toolkit und optimieren Sie ein Modell (1 Stunde)
2. **Windows AI Foundry**: Erstellen Sie eine einfache Windows-AI-Anwendung mit der Phi Silica API (1 Stunde)
3. **Cross-Plattform-Deployment**: Deployen Sie dasselbe Modell auf zwei verschiedenen Plattformen (1 Stunde)
4. **NPU-Optimierung**: Testen Sie die NPU-Leistung mit den Tools der Windows AI Foundry (30 Minuten)

## Modul 8: Microsoft Foundry Local – Komplettes Entwickler-Toolkit

### Wichtige Lernziele

- Foundry Local auf Windows installieren und konfigurieren
- Modelle lokal über die Foundry CLI ausführen, entdecken und verwalten
- Integration mit OpenAI-kompatiblen REST- und SDK-Clients
- Praktische Beispiele erstellen: Chainlit-Chat, Agenten und Modell-Router
- Hybridmuster mit Azure AI Foundry verstehen

### Studienfokusbereiche

- Installation und CLI-Grundlagen (Modell, Service, Cache)
- SDK-Integration (OpenAI-kompatible Clients und Azure OpenAI)
- Schnelle Validierung mit Open WebUI
- Muster für Agenten und Funktionsaufrufe
- Modelle-als-Tools (Router- und Registry-Design)

### Selbstbewertungsfragen

1. Wie entdecken Sie den lokalen Endpunkt und listen verfügbare Modelle auf?
2. Was sind die Unterschiede zwischen Foundry Local REST und Azure OpenAI?
3. Wie würden Sie einen einfachen Router entwerfen, um Modelle als Tools auszuwählen?
4. Welche CLI-Kategorien sind für die tägliche Entwicklung am relevantesten?
5. Wie validieren Sie die Einsatzbereitschaft von Foundry Local, bevor Sie Apps ausführen?

### Praktische Übungen

1. Installieren/aktualisieren Sie Foundry Local und führen Sie `phi-4-mini` lokal aus (30 Minuten)
2. Rufen Sie `/v1/models` auf und führen Sie einen einfachen Chat über REST aus (30 Minuten)
3. Starten Sie das Chainlit-App-Beispiel und chatten Sie lokal (30 Minuten)
4. Führen Sie den Multi-Agenten-Koordinator aus und inspizieren Sie die Ausgaben (30 Minuten)
5. Probieren Sie den Modelle-als-Tools-Router mit umgebungsbasierten Overrides aus (30 Minuten)

## Zeitplan-Leitfaden

Um das Beste aus den 20 Stunden Kurszeit herauszuholen, hier eine empfohlene Aufteilung:

| Aktivität | Zeitaufwand | Beschreibung |
|-----------|-------------|--------------|
| Lesen der Kernmaterialien | 9 Stunden | Fokus auf die wesentlichen Konzepte in jedem Modul |
| Praktische Übungen | 6 Stunden | Praktische Umsetzung der wichtigsten Techniken |
| Selbstbewertung | 2 Stunden | Testen des Verständnisses durch Fragen und Reflexion |
| Mini-Projekt | 3 Stunden | Anwendung des Wissens in einer kleinen praktischen Umsetzung |

### Schwerpunktbereiche nach Zeitvorgabe

**Wenn Sie nur 10 Stunden haben:**
- Abschließen der Module 1, 2 und 3 (Kernkonzepte von EdgeAI)
- Mindestens eine praktische Übung pro Modul durchführen
- Fokus auf das Verständnis der Kernkonzepte statt auf Implementierungsdetails

**Wenn Sie die vollen 20 Stunden investieren können:**
- Alle sieben Module abschließen
- Wichtige praktische Übungen aus jedem Modul durchführen
- Ein Mini-Projekt aus Modul 7 abschließen
- Mindestens 2-3 ergänzende Ressourcen erkunden

**Wenn Sie mehr als 20 Stunden haben:**
- Alle Module mit detaillierten Übungen abschließen
- Mehrere Mini-Projekte erstellen
- Fortgeschrittene Optimierungstechniken in Modul 4 erkunden
- Produktions-Deployment aus Modul 5 implementieren

## Wichtige Ressourcen

Diese sorgfältig ausgewählten Ressourcen bieten den größten Nutzen für Ihre begrenzte Lernzeit:

### Unbedingt zu lesende Dokumentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Das effizienteste Tool zur Modelloptimierung
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Schnellster Weg, SLMs lokal zu deployen
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referenz für ein führendes Edge-optimiertes Modell
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intels umfassendes Optimierungstoolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrierte EdgeAI-Entwicklungsumgebung
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows-spezifische EdgeAI-Entwicklungsplattform

### Zeitersparende Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Schneller Zugriff auf Modelle und Deployment
- [Gradio](https://www.gradio.app/docs/interface) - Schnelle UI-Entwicklung für KI-Demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Vereinfachte Modelloptimierung
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Effiziente CPU-Inferenz
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework zur Kompression neuronaler Netze
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit für den Einsatz großer Sprachmodelle

## Fortschritts-Tracking-Vorlage

Verwenden Sie diese vereinfachte Vorlage, um Ihren Lernfortschritt im 20-Stunden-Kurs zu verfolgen:

| Modul | Abschlussdatum | Zeitaufwand | Wichtige Erkenntnisse |
|-------|----------------|-------------|-----------------------|
| Modul 1: EdgeAI-Grundlagen | | | |
| Modul 2: SLM-Grundlagen | | | |
| Modul 3: SLM-Deployment | | | |
| Modul 4: Modelloptimierung | | | |
| Modul 5: SLMOps | | | |
| Modul 6: KI-Agenten | | | |
| Modul 7: Entwicklungstools | | | |
| Modul 8: Foundry Local Toolkit | | | |
| Praktische Übungen | | | |
| Mini-Projekt | | | |

## Mini-Projektideen

Erwägen Sie, eines dieser Projekte abzuschließen, um EdgeAI-Konzepte zu üben (jedes ist so konzipiert, dass es 2-4 Stunden dauert):

### Anfängerprojekte (jeweils 2-3 Stunden)
1. **Edge-Text-Assistent**: Erstellen Sie ein einfaches Offline-Textvervollständigungstool mit einem kleinen Sprachmodell
2. **Modellvergleichs-Dashboard**: Erstellen Sie eine grundlegende Visualisierung von Leistungsmetriken verschiedener SLMs
3. **Optimierungsexperiment**: Messen Sie die Auswirkungen verschiedener Quantisierungsstufen auf dasselbe Basismodell

### Mittelstufenprojekte (jeweils 3-4 Stunden)
4. **AI Toolkit Workflow**: Verwenden Sie das VS Code AI Toolkit, um ein Modell von Anfang bis Ende zu optimieren und zu deployen
5. **Windows AI Foundry-Anwendung**: Erstellen Sie eine Windows-App mit der Phi Silica API und NPU-Optimierung
6. **Cross-Plattform-Deployment**: Deployen Sie dasselbe optimierte Modell auf Windows (OpenVINO) und Mobilgeräten (.NET MAUI)
7. **Funktionsaufruf-Agent**: Erstellen Sie einen KI-Agenten mit Funktionsaufruf-Fähigkeiten für Edge-Szenarien

### Fortgeschrittene Integrationsprojekte (jeweils 4-5 Stunden)
8. **OpenVINO-Optimierungspipeline**: Implementieren Sie eine vollständige Modelloptimierung mit NNCF und dem GenAI-Toolkit  
9. **SLMOps-Pipeline**: Implementieren Sie einen vollständigen Modelllebenszyklus von der Schulung bis zur Bereitstellung am Edge  
10. **Multi-Modell-Edge-System**: Bereitstellung mehrerer spezialisierter Modelle, die gemeinsam auf Edge-Hardware arbeiten  
11. **MCP-Integrationssystem**: Aufbau eines agentenbasierten Systems mit Model Context Protocol für die Werkzeuginteraktion  

## Referenzen

- Microsoft Learn (Foundry Local)  
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Erste Schritte: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - CLI-Referenz: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integration mit Inferenz-SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Open WebUI-Anleitung: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Hugging Face-Modelle kompilieren: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Übersicht: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agents (Übersicht): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Optimierungs- und Inferenz-Tools  
  - Microsoft Olive (Dokumentation): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (Erste Schritte): https://onnxruntime.ai/docs/get-started/with-python.html  
  - ONNX Runtime Olive-Integration: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (Dokumentation): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (Dokumentation): https://ml-explore.github.io/mlx/build/html/index.html  
- Bereitstellungs-Frameworks und Modelle  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (Dokumentation): https://docs.vllm.ai/  
  - Ollama (Schnellstart): https://github.com/ollama/ollama#get-started  
- Entwickler-Tools (Windows und VS Code)  
  - AI Toolkit für VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (Übersicht): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Lern-Community

Treten Sie der Diskussion bei und vernetzen Sie sich mit anderen Lernenden:  
- GitHub-Diskussionen im [EdgeAI for Beginners Repository](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Fazit

EdgeAI repräsentiert die Spitze der künstlichen Intelligenz, indem leistungsstarke Fähigkeiten direkt auf Geräte gebracht werden und gleichzeitig wichtige Aspekte wie Datenschutz, Latenz und Konnektivität berücksichtigt werden. Dieser 20-stündige Kurs vermittelt Ihnen das grundlegende Wissen und die praktischen Fähigkeiten, um sofort mit EdgeAI-Technologien zu arbeiten.

Der Kurs ist bewusst kompakt und konzentriert sich auf die wichtigsten Konzepte, sodass Sie schnell wertvolle Expertise gewinnen können, ohne eine überwältigende Zeitinvestition. Denken Sie daran, dass praktische Übungen, selbst mit einfachen Beispielen, der Schlüssel sind, um das Gelernte zu festigen.

Viel Erfolg beim Lernen!

---

