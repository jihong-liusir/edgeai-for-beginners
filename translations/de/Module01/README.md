<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddfe62b8e130979b7034bc6fbb7d510c",
  "translation_date": "2025-09-17T13:12:53+00:00",
  "source_file": "Module01/README.md",
  "language_code": "de"
}
-->
# Kapitel 01: Transformation der KI-Bereitstellung für Edge-Geräte

EdgeAI stellt einen Paradigmenwechsel in der Bereitstellung von künstlicher Intelligenz dar, bei dem KI-Funktionen von cloudbasiertem Processing auf lokale Edge-Geräte verlagert werden. Dieses Kapitel untersucht die grundlegenden Konzepte, Schlüsseltechnologien und praktischen Anwendungen, die diesen transformativen Ansatz der KI-Implementierung definieren.

## Modulstruktur

### [Abschnitt 1: Grundlagen von EdgeAI](./01.EdgeAIFundamentals.md)
Dieser Abschnitt legt die Basis, indem traditionelle cloudbasierte KI mit EdgeAI-Bereitstellungsmodellen verglichen wird. Wir analysieren entscheidende Technologien wie Modellquantisierung, Kompressionsoptimierung und Small Language Models (SLMs), die die rechnerischen Einschränkungen von Edge-Geräten überwinden. Die Diskussion hebt hervor, wie diese Innovationen verbesserte Datenschutzmaßnahmen, extrem niedrige Latenzzeiten und robuste Offline-Verarbeitungsfähigkeiten ermöglichen.

### [Abschnitt 2: Fallstudien aus der Praxis](./02.RealWorldCaseStudies.md)
Anhand konkreter Beispiele wie den Phi- und Mu-Modellökosystemen von Microsoft und dem KI-Berichtssystem von Japan Airlines zeigt dieser Abschnitt erfolgreiche EdgeAI-Implementierungen in verschiedenen Branchen. Diese Fallstudien bestätigen die außergewöhnliche Leistung von SLMs bei spezialisierten Aufgaben und veranschaulichen die praktischen Vorteile von Edge-Bereitstellungsstrategien.

### [Abschnitt 3: Leitfaden zur praktischen Umsetzung](./03.PracticalImplementationGuide.md)
Dieser Abschnitt bietet umfassende Richtlinien zur Vorbereitung der Umgebung für praxisorientiertes Lernen, einschließlich der wesentlichen Entwicklungstools, Hardwareanforderungen, Kernmodellressourcen und Optimierungsframeworks. Er schafft die technische Grundlage, die Lernende benötigen, um eigene EdgeAI-Lösungen zu entwickeln und bereitzustellen.

### [Abschnitt 4: Hardwareplattformen für die Bereitstellung von EdgeAI](./04.EdgeDeployment.md)
Dieser Abschnitt untersucht das Hardware-Ökosystem, das die Bereitstellung von EdgeAI ermöglicht, und behandelt Plattformen von Intel, Qualcomm, NVIDIA und Windows AI PCs. Es werden detaillierte Vergleiche der Hardwarefähigkeiten, plattformspezifische Optimierungstechniken und praktische Überlegungen zur Bereitstellung in verschiedenen Edge-Computing-Szenarien vorgestellt.

## Wichtige Lernergebnisse

Am Ende dieses Kapitels werden die Leser verstehen:
- Die grundlegenden Unterschiede zwischen Cloud- und Edge-KI-Architekturen
- Kernoptimierungstechniken für die Edge-Bereitstellung
- Anwendungen und Erfolgsgeschichten aus der Praxis
- Praktische Fähigkeiten zur Implementierung von EdgeAI-Lösungen
- Auswahl von Hardwareplattformen und plattformspezifische Optimierungsansätze
- Leistungsbewertung und Best Practices für die Bereitstellung

## Zukünftige Auswirkungen

EdgeAI entwickelt sich zu einem entscheidenden Trend, der die Zukunft der KI-Bereitstellung prägt. Es ebnet den Weg für verteilte, effiziente und datenschutzfreundliche KI-Systeme, die unabhängig von der Cloud-Konnektivität arbeiten können und gleichzeitig hohe Leistungsstandards aufrechterhalten.

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.