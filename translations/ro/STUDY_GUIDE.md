<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e94a6b6e8c8f3f9c881b7d6222cd9c6b",
  "translation_date": "2025-09-22T23:42:54+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "ro"
}
-->
# EdgeAI pentru Începători: Căi de Învățare și Program de Studiu

### Cale de Învățare Concentrată (1 săptămână)

| Zi | Focus | Ore Estimative |
|------|-------|------------------|
| Ziua 1 | Modulul 1: Fundamentele EdgeAI | 3 ore |
| Ziua 2 | Modulul 2: Fundamentele SLM | 3 ore |
| Ziua 3 | Modulul 3: Implementarea SLM | 2 ore |
| Zilele 4-5 | Modulul 4: Optimizarea Modelului (6 cadre) | 4 ore |
| Ziua 6 | Modulul 5: SLMOps | 3 ore |
| Ziua 7 | Modulele 6-7: Agenți AI & Instrumente de Dezvoltare | 5 ore |

### Cale de Învățare Concentrată (2 săptămâni)

| Zi | Focus | Ore Estimative |
|------|-------|------------------|
| Zilele 1-2 | Modulul 1: Fundamentele EdgeAI | 3 ore |
| Zilele 3-4 | Modulul 2: Fundamentele SLM | 3 ore |
| Zilele 5-6 | Modulul 3: Implementarea SLM | 2 ore |
| Zilele 7-8 | Modulul 4: Optimizarea Modelului | 4 ore |
| Zilele 9-10 | Modulul 5: SLMOps | 3 ore |
| Zilele 11-12 | Modulul 6: Agenți AI | 2 ore |
| Zilele 13-14 | Modulul 7: Instrumente de Dezvoltare | 3 ore |

### Studiu Part-Time (4 săptămâni)

| Săptămână | Focus | Ore Estimative |
|------|-------|------------------|
| Săptămâna 1 | Modulele 1-2: Fundamente & Fundamentele SLM | 6 ore |
| Săptămâna 2 | Modulele 3-4: Implementare & Optimizare | 6 ore |
| Săptămâna 3 | Modulele 5-6: SLMOps & Agenți AI | 5 ore |
| Săptămâna 4 | Modulul 7: Instrumente de Dezvoltare & Integrare | 3 ore |

| Zi | Focus | Ore Estimative |
|------|-------|------------------|
| Zilele 1-2 | Modulul 1: Fundamentele EdgeAI | 3 ore |
| Zilele 3-4 | Modulul 2: Fundamentele SLM | 3 ore |
| Zilele 5-6 | Modulul 3: Implementarea SLM | 2 ore |
| Zilele 7-8 | Modulul 4: Optimizarea Modelului | 4 ore |
| Zilele 9-10 | Modulul 5: SLMOps | 3 ore |
| Zilele 11-12 | Modulul 6: Sisteme Agentice SLM | 2 ore |
| Zilele 13-14 | Modulul 7: Exemple de Implementare EdgeAI | 2 ore |

| Modul | Data Finalizării | Ore Petrecute | Concluzii Cheie |
|--------|----------------|-------------|--------------|
| Modulul 1: Fundamentele EdgeAI | | | |
| Modulul 2: Fundamentele SLM | | | |
| Modulul 3: Implementarea SLM | | | |
| Modulul 4: Optimizarea Modelului (6 cadre) | | | |
| Modulul 5: SLMOps | | | |
| Modulul 6: Sisteme Agentice SLM | | | |
| Modulul 7: Exemple de Implementare EdgeAI | | | |
| Exerciții Practice | | | |
| Mini-Proiect | | | |

### Studiu Part-Time (4 săptămâni)

| Săptămână | Focus | Ore Estimative |
|------|-------|------------------|
| Săptămâna 1 | Modulele 1-2: Fundamente & Fundamentele SLM | 6 ore |
| Săptămâna 2 | Modulele 3-4: Implementare & Optimizare | 6 ore |
| Săptămâna 3 | Modulele 5-6: SLMOps & Agenți AI | 5 ore |
| Săptămâna 4 | Modulul 7: Instrumente de Dezvoltare & Integrare | 3 ore |

## Introducere

Bine ați venit la ghidul de studiu EdgeAI pentru Începători! Acest document este conceput pentru a vă ajuta să navigați eficient materialele cursului și să maximizați experiența de învățare. Oferă căi de învățare structurate, programe de studiu sugerate, rezumate ale conceptelor cheie și resurse suplimentare pentru a aprofunda înțelegerea tehnologiilor EdgeAI.

Acesta este un curs concis de 20 de ore care oferă cunoștințe esențiale despre EdgeAI într-un format eficient din punct de vedere al timpului, fiind ideal pentru profesioniști ocupați și studenți care doresc să dobândească rapid abilități practice în acest domeniu emergent.

## Prezentare Generală a Cursului

Cursul este organizat în șapte module cuprinzătoare:

1. **Fundamentele și Transformarea EdgeAI** - Înțelegerea conceptelor de bază și a schimbării tehnologice
2. **Fundamentele Modelului de Limbaj Mic (SLM)** - Explorarea diferitelor familii SLM și a arhitecturilor acestora
3. **Implementarea Modelului de Limbaj Mic** - Strategii practice de implementare
4. **Conversia Formatului Modelului și Cuantizarea** - Optimizare avansată cu 6 cadre, inclusiv OpenVINO
5. **SLMOps - Operațiuni pentru Modele de Limbaj Mic** - Managementul ciclului de viață al producției și implementării
6. **Sisteme Agentice SLM** - Agenți AI, apelarea funcțiilor și Protocolul Contextului Modelului
7. **Exemple de Implementare EdgeAI** - Instrumente AI, dezvoltare pe Windows și implementări specifice platformei
8. **Microsoft Foundry Local – Kit Complet pentru Dezvoltatori** - Dezvoltare locală cu integrare hibridă Azure (Modulul 08)

## Cum să Utilizați Acest Ghid de Studiu

- **Învățare Progresivă**: Urmați modulele în ordine pentru o experiență de învățare coerentă
- **Puncte de Verificare a Cunoștințelor**: Folosiți întrebările de autoevaluare după fiecare secțiune
- **Practica Practică**: Finalizați exercițiile sugerate pentru a consolida conceptele teoretice
- **Resurse Suplimentare**: Explorați materiale suplimentare pentru subiectele care vă interesează cel mai mult

## Recomandări pentru Programul de Studiu

### Cale de Învățare Concentrată (1 săptămână)

| Zi | Focus | Ore Estimative |
|------|-------|-----------------|
| Zilele 1-2 | Modulul 1: Fundamentele EdgeAI | 6 ore |
| Zilele 3-4 | Modulul 2: Fundamentele SLM | 8 ore |
| Ziua 5 | Modulul 3: Implementarea SLM | 3 ore |
| Ziua 6 | Modulul 8: Kitul Foundry Local | 3 ore |

### Studiu Part-Time (3 săptămâni)

| Săptămână | Focus | Ore Estimative |
|------|-------|-----------------|
| Săptămâna 1 | Modulul 1: Fundamentele EdgeAI | 6-7 ore |
| Săptămâna 2 | Modulul 2: Fundamentele SLM | 7-8 ore |
| Săptămâna 3 | Modulul 3: Implementarea SLM (3h) + Modulul 8: Kitul Foundry Local (2-3h) | 5-6 ore |

## Modulul 1: Fundamentele și Transformarea EdgeAI

### Obiective Cheie de Învățare

- Înțelegeți diferențele dintre AI bazat pe cloud și AI bazat pe edge
- Stăpâniți tehnicile de optimizare de bază pentru medii cu resurse limitate
- Analizați aplicațiile reale ale tehnologiilor EdgeAI
- Configurați un mediu de dezvoltare pentru proiecte EdgeAI

### Zone de Focus pentru Studiu

#### Secțiunea 1: Fundamentele EdgeAI
- **Concepte Prioritare**: 
  - Paradigmele de calcul Edge vs. Cloud
  - Tehnici de cuantizare a modelului
  - Opțiuni de accelerare hardware (NPU-uri, GPU-uri, CPU-uri)
  - Avantajele privind confidențialitatea și securitatea

- **Materiale Suplimentare**:
  - [Documentația TensorFlow Lite](https://www.tensorflow.org/lite)
  - [GitHub ONNX Runtime](https://github.com/microsoft/onnxruntime)
  - [Documentația Edge Impulse](https://docs.edgeimpulse.com)

#### Secțiunea 2: Studii de Caz Reale
- **Concepte Prioritare**: 
  - Ecosistemul de modele Microsoft Phi & Mu
  - Implementări practice în diverse industrii
  - Considerații pentru implementare

#### Secțiunea 3: Ghid Practic de Implementare
- **Concepte Prioritare**: 
  - Configurarea mediului de dezvoltare
  - Instrumente de cuantizare și optimizare
  - Metode de evaluare pentru implementările EdgeAI

#### Secțiunea 4: Hardware pentru Implementare Edge
- **Concepte Prioritare**: 
  - Compararea platformelor hardware
  - Strategii de optimizare pentru hardware specific
  - Considerații pentru implementare

### Întrebări de Autoevaluare

1. Comparați și contrastați AI bazat pe cloud cu implementările AI bazate pe edge.
2. Explicați trei tehnici cheie pentru optimizarea modelelor pentru implementarea pe edge.
3. Care sunt principalele avantaje ale rulării modelelor AI pe edge?
4. Descrieți procesul de cuantizare a unui model și cum afectează performanța.
5. Explicați cum diferiți acceleratori hardware (NPU-uri, GPU-uri, CPU-uri) influențează implementarea EdgeAI.

### Exerciții Practice

1. **Configurare Rapidă a Mediului**: Configurați un mediu de dezvoltare minimal cu pachetele esențiale (30 minute)
2. **Explorarea Modelului**: Descărcați și examinați un model de limbaj mic pre-antrenat (1 oră)
3. **Cuantizare de Bază**: Încercați o cuantizare simplă pe un model mic (1 oră)

## Modulul 2: Fundamentele Modelului de Limbaj Mic

### Obiective Cheie de Învățare

- Înțelegeți principiile arhitecturale ale diferitelor familii SLM
- Comparați capacitățile modelelor la diferite scări de parametri
- Evaluați modelele pe baza eficienței, capacității și cerințelor de implementare
- Recunoașteți cazurile de utilizare adecvate pentru diferite familii de modele

### Zone de Focus pentru Studiu

#### Secțiunea 1: Familia de Modele Microsoft Phi
- **Concepte Prioritare**: 
  - Evoluția filozofiei de design
  - Arhitectură orientată spre eficiență
  - Capacități specializate

#### Secțiunea 2: Familia Qwen
- **Concepte Prioritare**: 
  - Contribuții open source
  - Opțiuni scalabile de implementare
  - Arhitectură avansată de raționament

#### Secțiunea 3: Familia Gemma
- **Concepte Prioritare**: 
  - Inovație bazată pe cercetare
  - Capacități multimodale
  - Optimizare pentru mobil

#### Secțiunea 4: Familia BitNET
- **Concepte Prioritare**: 
  - Tehnologie de cuantizare la 1-bit
  - Cadru de optimizare pentru inferență
  - Considerații de sustenabilitate

#### Secțiunea 5: Modelul Microsoft Mu
- **Concepte Prioritare**: 
  - Arhitectură orientată spre dispozitive
  - Integrare sistemică cu Windows
  - Operare care protejează confidențialitatea

#### Secțiunea 6: Phi-Silica
- **Concepte Prioritare**: 
  - Arhitectură optimizată pentru NPU
  - Metrice de performanță
  - Integrare pentru dezvoltatori

### Întrebări de Autoevaluare

1. Comparați abordările arhitecturale ale familiilor de modele Phi și Qwen.
2. Explicați cum tehnologia de cuantizare a BitNET diferă de cuantizarea tradițională.
3. Care sunt avantajele unice ale modelului Mu pentru integrarea cu Windows?
4. Descrieți cum Phi-Silica utilizează hardware-ul NPU pentru optimizarea performanței.
5. Pentru o aplicație mobilă cu conectivitate limitată, care familie de modele ar fi cea mai potrivită și de ce?

### Exerciții Practice

1. **Compararea Modelelor**: Benchmark rapid al două modele SLM diferite (1 oră)
2. **Generare Simplă de Text**: Implementare de bază a generării de text cu un model mic (1 oră)
3. **Optimizare Rapidă**: Aplicați o tehnică de optimizare pentru a îmbunătăți viteza de inferență (1 oră)

## Modulul 3: Implementarea Modelului de Limbaj Mic

### Obiective Cheie de Învățare

- Selectați modele adecvate pe baza constrângerilor de implementare
- Stăpâniți tehnicile de optimizare pentru diverse scenarii de implementare
- Implementați SLM-uri atât în medii locale, cât și în cloud
- Proiectați configurații pregătite pentru producție pentru aplicații EdgeAI

### Zone de Focus pentru Studiu

#### Secțiunea 1: Învățare Avansată SLM
- **Concepte Prioritare**: 
  - Cadru de clasificare a parametrilor
  - Tehnici avansate de optimizare
  - Strategii de achiziție a modelelor

#### Secțiunea 2: Implementare în Mediu Local
- **Concepte Prioritare**: 
  - Implementare pe platforma Ollama
  - Soluții locale Microsoft Foundry
  - Analiza comparativă a cadrelor

#### Secțiunea 3: Implementare în Cloud Containerizat
- **Concepte Prioritare**: 
  - Inferență performantă vLLM
  - Orchestrarea containerelor
  - Implementare ONNX Runtime

### Întrebări de Autoevaluare

1. Ce factori ar trebui luați în considerare atunci când se selectează între implementarea locală și cea în cloud?
2. Comparați Ollama și Microsoft Foundry Local ca opțiuni de implementare.
3. Explicați beneficiile containerizării pentru implementarea SLM.
4. Care sunt principalele metrici de performanță de monitorizat pentru un SLM implementat pe edge?
5. Descrieți un flux complet de implementare, de la selecția modelului la implementarea în producție.

### Exerciții Practice

1. **Implementare Locală de Bază**: Implementați un SLM simplu folosind Ollama (1 oră)
2. **Verificare Performanță**: Rulați un benchmark rapid pe modelul implementat (30 minute)
3. **Integrare Simplă**: Creați o aplicație minimală care utilizează modelul implementat (1 oră)

## Modulul 4: Conversia Formatului Modelului și Cuantizarea

### Obiective Cheie de Învățare

- Stăpâniți tehnici avansate de cuantizare de la precizie de 1-bit la 8-bit
- Înțelegeți strategiile de conversie a formatului (GGUF, ONNX)
- Implementați optimizarea în șase cadre (Llama.cpp, Olive, OpenVINO, MLX, sinteza fluxului de lucru)
- Implementați modele optimizate pentru medii de producție edge pe hardware Intel, Apple și cross-platform

### Zone de Focus pentru Studiu

#### Secțiunea 1: Fundamentele Cuantizării
- **Concepte Prioritare**: 
  - Cadru de clasificare a preciziei
  - Compromisuri între performanță și acuratețe
  - Optimizarea amprentei de memorie

#### Secțiunea 2: Implementarea Llama.cpp
- **Concepte Prioritare**: 
  - Implementare cross-platform
  - Optimizarea formatului GGUF
  - Tehnici de accelerare hardware

#### Secțiunea 3: Suita Microsoft Olive
- **Concepte Prioritare**: 
  - Optimizare orientată pe hardware
  - Implementare de nivel enterprise
  - Fluxuri de lucru automate pentru optimizare

#### Secțiunea 4: Toolkit OpenVINO
- **Concepte Prioritare**: 
  - Optimizare pentru hardware Intel
  - Cadru de compresie a rețelelor neuronale (NNCF)
  - Implementare cross-platform pentru inferență
  - OpenVINO GenAI pentru implementarea LLM

#### Secțiunea 5: Cadru Apple MLX
- **Concepte Prioritare**:  
  - Optimizare pentru Apple Silicon  
  - Arhitectură de memorie unificată  
  - Capacități de fine-tuning LoRA  

#### Secțiunea 6: Sinteza Fluxului de Lucru pentru Dezvoltarea Edge AI  
- **Concepte Prioritare**:  
  - Arhitectură unificată a fluxului de lucru  
  - Arborele decizional pentru selecția framework-urilor  
  - Validarea pregătirii pentru producție  
  - Strategii pentru viitor  

### Întrebări de Autoevaluare  

1. Compară strategiile de cuantizare la diferite niveluri de precizie (1-bit până la 8-bit).  
2. Explică avantajele formatului GGUF pentru implementarea la margine.  
3. Cum îmbunătățește optimizarea hardware-aware din Microsoft Olive eficiența implementării?  
4. Care sunt beneficiile cheie ale NNCF din OpenVINO pentru comprimarea modelelor?  
5. Descrie cum Apple MLX utilizează arhitectura de memorie unificată pentru optimizare.  
6. Cum ajută sinteza fluxului de lucru în selectarea framework-urilor optime de optimizare?  

### Exerciții Practice  

1. **Cuantizarea Modelului**: Aplică diferite niveluri de cuantizare unui model și compară rezultatele (1 oră).  
2. **Optimizare OpenVINO**: Utilizează NNCF pentru a comprima un model pentru hardware Intel (1 oră).  
3. **Compararea Framework-urilor**: Testează același model pe trei framework-uri diferite de optimizare (1 oră).  
4. **Benchmarking Performanță**: Măsoară impactul optimizării asupra vitezei de inferență și utilizării memoriei (1 oră).  

## Modulul 5: SLMOps - Operarea Modelelor de Limbaj Mic  

### Obiective Cheie de Învățare  

- Înțelegerea principiilor de gestionare a ciclului de viață SLMOps  
- Stăpânirea tehnicilor de distilare și fine-tuning pentru implementarea la margine  
- Implementarea strategiilor de implementare în producție cu monitorizare  
- Construirea fluxurilor de lucru pentru operarea și întreținerea SLM la nivel enterprise  

### Zone de Studiu  

#### Secțiunea 1: Introducere în SLMOps  
- **Concepte Prioritare**:  
  - Schimbarea paradigmei SLMOps în operațiunile AI  
  - Arhitectură eficientă din punct de vedere al costurilor și orientată spre confidențialitate  
  - Impact strategic asupra afacerilor și avantaje competitive  

#### Secțiunea 2: Distilarea Modelului  
- **Concepte Prioritare**:  
  - Tehnici de transfer de cunoștințe  
  - Implementarea procesului de distilare în două etape  
  - Fluxuri de lucru pentru distilare în Azure ML  

#### Secțiunea 3: Strategii de Fine-Tuning  
- **Concepte Prioritare**:  
  - Fine-tuning eficient din punct de vedere al parametrilor (PEFT)  
  - Metode avansate LoRA și QLoRA  
  - Antrenament multi-adaptor și optimizarea hiperparametrilor  

#### Secțiunea 4: Implementare în Producție  
- **Concepte Prioritare**:  
  - Conversia și cuantizarea modelelor pentru producție  
  - Configurarea implementării locale Foundry  
  - Benchmarking performanță și validarea calității  

### Întrebări de Autoevaluare  

1. Cum diferă SLMOps de MLOps tradițional?  
2. Explică beneficiile distilării modelelor pentru implementarea la margine.  
3. Care sunt considerațiile cheie pentru fine-tuning-ul SLM-urilor în medii cu resurse limitate?  
4. Descrie un flux complet de implementare în producție pentru aplicații AI la margine.  

### Exerciții Practice  

1. **Distilare de Bază**: Creează un model mai mic dintr-un model profesor mai mare (1 oră).  
2. **Experiment de Fine-Tuning**: Fine-tunează un model pentru un domeniu specific (1 oră).  
3. **Pipeline de Implementare**: Configurează un pipeline CI/CD de bază pentru implementarea modelului (1 oră).  

## Modulul 6: Sisteme Agentice SLM - Agenți AI și Apelarea Funcțiilor  

### Obiective Cheie de Învățare  

- Construirea agenților AI inteligenți pentru medii la margine utilizând modele de limbaj mic  
- Implementarea capacităților de apelare a funcțiilor cu fluxuri de lucru sistematice  
- Stăpânirea integrării Model Context Protocol (MCP) pentru interacțiunea standardizată cu instrumentele  
- Crearea sistemelor agentice sofisticate cu intervenție umană minimă  

### Zone de Studiu  

#### Secțiunea 1: Agenți AI și Fundamentele SLM  
- **Concepte Prioritare**:  
  - Cadru de clasificare a agenților (reflex, bazat pe model, bazat pe obiective, agenți de învățare)  
  - Analiza compromisurilor SLM vs LLM  
  - Modele de design pentru agenți specifici marginii  
  - Optimizarea resurselor pentru agenți  

#### Secțiunea 2: Apelarea Funcțiilor în Modelele de Limbaj Mic  
- **Concepte Prioritare**:  
  - Implementarea fluxurilor de lucru sistematice (detectarea intenției, output JSON, execuție externă)  
  - Implementări specifice platformei (Phi-4-mini, modele Qwen selectate, Microsoft Foundry Local)  
  - Exemple avansate (colaborare multi-agent, selecție dinamică de instrumente)  
  - Considerații pentru producție (limitarea ratei, jurnalizare audit, măsuri de securitate)  

#### Secțiunea 3: Integrarea Model Context Protocol (MCP)  
- **Concepte Prioritare**:  
  - Arhitectura protocolului și designul sistemului stratificat  
  - Suport multi-backend (Ollama pentru dezvoltare, vLLM pentru producție)  
  - Protocoale de conexiune (moduri STDIO și SSE)  
  - Aplicații reale (automatizare web, procesare de date, integrare API)  

### Întrebări de Autoevaluare  

1. Care sunt considerațiile arhitecturale cheie pentru agenții AI la margine?  
2. Cum îmbunătățește apelarea funcțiilor capacitățile agenților?  
3. Explică rolul Model Context Protocol în comunicarea agenților.  

### Exerciții Practice  

1. **Agent Simplu**: Construiește un agent AI de bază cu apelarea funcțiilor (1 oră).  
2. **Integrarea MCP**: Implementează MCP într-o aplicație de agent (30 minute).  

## Modulul 7: Exemple de Implementare EdgeAI  

### Obiective Cheie de Învățare  

- Stăpânirea AI Toolkit pentru Visual Studio Code pentru fluxuri de lucru complete de dezvoltare EdgeAI  
- Dobândirea expertizei în platforma Windows AI Foundry și strategiile de optimizare NPU  
- Implementarea EdgeAI pe mai multe platforme hardware și scenarii de implementare  
- Construirea aplicațiilor EdgeAI pregătite pentru producție cu optimizări specifice platformei  

### Zone de Studiu  

#### Secțiunea 1: AI Toolkit pentru Visual Studio Code  
- **Concepte Prioritare**:  
  - Mediu complet de dezvoltare Edge AI în cadrul VS Code  
  - Catalog de modele și descoperire pentru implementarea la margine  
  - Testare locală, optimizare și fluxuri de lucru pentru dezvoltarea agenților  
  - Monitorizarea performanței și evaluarea pentru scenarii la margine  

#### Secțiunea 2: Ghid de Dezvoltare Windows EdgeAI  
- **Concepte Prioritare**:  
  - Prezentare generală a platformei Windows AI Foundry  
  - API-ul Phi Silica pentru inferență eficientă NPU  
  - API-uri pentru Computer Vision pentru procesarea imaginilor și OCR  
  - CLI Foundry Local pentru dezvoltare și testare locală  

#### Secțiunea 3: Implementări Specifice Platformei  
- **Concepte Prioritare**:  
  - Implementare NVIDIA Jetson Orin Nano (performanță AI de 67 TOPS)  
  - Aplicații mobile cu .NET MAUI și ONNX Runtime GenAI  
  - Soluții Azure EdgeAI cu arhitectură hibridă cloud-margine  
  - Optimizare Windows ML cu suport hardware universal  
  - Aplicații Foundry Local cu implementare RAG orientată spre confidențialitate  

### Întrebări de Autoevaluare  

1. Cum simplifică AI Toolkit fluxul de lucru pentru dezvoltarea EdgeAI?  
2. Compară strategiile de implementare pe diferite platforme hardware.  
3. Care sunt avantajele Windows AI Foundry pentru dezvoltarea la margine?  
4. Explică rolul optimizării NPU în aplicațiile moderne Edge AI.  
5. Cum utilizează API-ul Phi Silica hardware-ul NPU pentru optimizarea performanței?  
6. Compară beneficiile implementării locale vs. în cloud pentru aplicațiile sensibile la confidențialitate.  

### Exerciții Practice  

1. **Configurare AI Toolkit**: Configurează AI Toolkit și optimizează un model (1 oră).  
2. **Windows AI Foundry**: Construiește o aplicație Windows AI simplă utilizând API-ul Phi Silica (1 oră).  
3. **Implementare Cross-Platform**: Implementează același model pe două platforme diferite (1 oră).  
4. **Optimizare NPU**: Testează performanța NPU cu instrumentele Windows AI Foundry (30 minute).  

## Modulul 8: Microsoft Foundry Local – Toolkit-ul Complet pentru Dezvoltatori  

### Obiective Cheie de Învățare  

- Instalează și configurează Foundry Local pe Windows  
- Rulează, descoperă și gestionează modele local prin CLI Foundry  
- Integrează cu clienți REST și SDK compatibili OpenAI  
- Construiește exemple practice: Chainlit chat, agenți și router de modele  
- Înțelege modelele hibride cu Azure AI Foundry  

### Zone de Studiu  

- Instalare și elemente esențiale CLI (model, serviciu, cache)  
- Integrare SDK (clienți compatibili OpenAI și Azure OpenAI)  
- Validare rapidă Open WebUI  
- Modele de apelare funcții și agenți  
- Modele ca instrumente (design router și registru)  

### Întrebări de Autoevaluare  

1. Cum descoperi endpoint-ul local și listezi modelele disponibile?  
2. Care sunt diferențele între utilizarea REST Foundry Local și Azure OpenAI?  
3. Cum ai proiecta un router simplu pentru a selecta modele ca instrumente?  
4. Care categorii CLI sunt cele mai relevante pentru dezvoltarea zilnică?  
5. Cum validezi pregătirea Foundry Local înainte de a rula aplicații?  

### Exerciții Practice  

1. Instalează/actualizează Foundry Local și rulează `phi-4-mini` local (30 minute).  
2. Apelează `/v1/models` și rulează un chat simplu prin REST (30 minute).  
3. Lansează exemplul aplicației Chainlit și chat-ul local (30 minute).  
4. Rulează coordonatorul multi-agent și inspectează output-urile (30 minute).  
5. Încearcă router-ul modele ca instrumente cu suprascrieri bazate pe mediu (30 minute).  

## Ghid de Alocare a Timpului  

Pentru a profita la maximum de cele 20 de ore ale cursului, iată o sugestie de împărțire a timpului:  

| Activitate | Alocare Timp | Descriere |  
|------------|--------------|-----------|  
| Citirea Materialelor de Bază | 9 ore | Concentrare pe conceptele esențiale din fiecare modul |  
| Exerciții Practice | 6 ore | Implementarea practică a tehnicilor cheie |  
| Autoevaluare | 2 ore | Testarea înțelegerii prin întrebări și reflecție |  
| Mini-Proiect | 3 ore | Aplicarea cunoștințelor într-o implementare practică mică |  

### Zone Cheie de Focalizare în Funcție de Timp  

**Dacă ai doar 10 ore:**  
- Completează Modulele 1, 2 și 3 (concepte de bază EdgeAI).  
- Fă cel puțin un exercițiu practic pe modul.  
- Concentrează-te pe înțelegerea conceptelor de bază, mai degrabă decât pe detalii de implementare.  

**Dacă poți dedica toate cele 20 de ore:**  
- Completează toate cele șapte module.  
- Realizează exercițiile practice cheie din fiecare modul.  
- Finalizează un mini-proiect din Modulul 7.  
- Explorează cel puțin 2-3 resurse suplimentare.  

**Dacă ai mai mult de 20 de ore:**  
- Completează toate modulele cu exerciții detaliate.  
- Construiește mai multe mini-proiecte.  
- Explorează tehnici avansate de optimizare din Modulul 4.  
- Implementează implementarea în producție din Modulul 5.  

## Resurse Esențiale  

Aceste resurse atent selectate oferă cea mai mare valoare pentru timpul tău limitat de studiu:  

### Documentație de Citit Obligatoriu  
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Cel mai eficient instrument de optimizare a modelelor  
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Cea mai rapidă metodă de a implementa SLM-uri local  
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Referință pentru un model optimizat pentru margine  
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Toolkit-ul cuprinzător de optimizare Intel  
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Mediu integrat de dezvoltare EdgeAI  
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Platforma specifică Windows pentru dezvoltare EdgeAI  

### Instrumente Economisitoare de Timp  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Acces rapid la modele și implementare  
- [Gradio](https://www.gradio.app/docs/interface) - Dezvoltare rapidă de UI pentru demonstrații AI  
- [Microsoft Olive](https://github.com/microsoft/Olive) - Optimizare simplificată a modelelor  
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Inferență eficientă pe CPU  
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Framework pentru comprimarea rețelelor neuronale  
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit pentru implementarea modelelor de limbaj mare  

## Șablon de Urmărire a Progresului  

Folosește acest șablon simplificat pentru a urmări progresul în cursul de 20 de ore:  

| Modul | Data Finalizării | Ore Petrecute | Concluzii Cheie |  
|-------|------------------|---------------|-----------------|  
| Modulul 1: Fundamente EdgeAI | | | |  
| Modulul 2: Fundamente SLM | | | |  
| Modulul 3: Implementare SLM | | | |  
| Modulul 4: Optimizare Model | | | |  
| Modulul 5: SLMOps | | | |  
| Modulul 6: Agenți AI | | | |  
| Modulul 7: Instrumente de Dezvoltare | | | |  
| Modulul 8: Toolkit Foundry Local | | | |  
| Exerciții Practice | | | |  
| Mini-Proiect | | | |  

## Idei de Mini-Proiecte  

Ia în considerare finalizarea unuia dintre aceste proiecte pentru a exersa conceptele EdgeAI (fiecare proiect este conceput să dureze 2-4 ore):  

### Proiecte pentru Începători (2-3 ore fiecare)  
1. **Asistent Text la Margine**: Creează un instrument simplu de completare text offline utilizând un model de limbaj mic.  
2. **Dashboard de Comparare a Modelelor**: Construiește o vizualizare de bază a metricilor de performanță pentru diferite SLM-uri.  
3. **Experiment de Optimizare**: Măsoară impactul diferitelor niveluri de cuantizare asupra aceluiași model de bază.  

### Proiecte Intermediare (3-4 ore fiecare)  
4. **Flux de Lucru AI Toolkit**: Utilizează AI Toolkit din VS Code pentru a optimiza și implementa un model de la început până la sfârșit.  
5. **Aplicație Windows AI Foundry**: Creează o aplicație Windows utilizând API-ul Phi Silica și optimizarea NPU.  
6. **Implementare Cross-Platform**: Implementează același model optimizat pe Windows (OpenVINO) și mobil (.NET MAUI).  
7. **Agent cu Apelare Funcții**: Construiește un agent AI cu capacități de apelare funcții pentru scenarii la margine.  

### Proiecte de Integrare Avansată (4-5 ore
8. **Pipeline de Optimizare OpenVINO**: Implementați optimizarea completă a modelului folosind NNCF și GenAI toolkit  
9. **Pipeline SLMOps**: Implementați un ciclu complet de viață al modelului, de la antrenare până la implementare pe dispozitive edge  
10. **Sistem Edge Multi-Model**: Implementați mai multe modele specializate care lucrează împreună pe hardware edge  
11. **Sistem de Integrare MCP**: Construiți un sistem agentic folosind Model Context Protocol pentru interacțiunea cu instrumente  

## Referințe

- Microsoft Learn (Foundry Local)  
  - Prezentare generală: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/  
  - Începeți: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started  
  - Referință CLI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli  
  - Integrare cu SDK-uri de inferență: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks  
  - Cum să deschideți WebUI: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui  
  - Compilare modele Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models  
- Azure AI Foundry  
  - Prezentare generală: https://learn.microsoft.com/en-us/azure/ai-foundry/  
  - Agenți (prezentare generală): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview  
- Instrumente de Optimizare și Inferență  
  - Microsoft Olive (documentație): https://microsoft.github.io/Olive/  
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive  
  - ONNX Runtime (începeți): https://onnxruntime.ai/docs/get-started/with-python.html  
  - Integrare ONNX Runtime Olive: https://onnxruntime.ai/docs/performance/olive.html  
  - OpenVINO (documentație): https://docs.openvino.ai/2025/index.html  
  - Apple MLX (documentație): https://ml-explore.github.io/mlx/build/html/index.html  
- Framework-uri de Implementare și Modele  
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp  
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index  
  - vLLM (documentație): https://docs.vllm.ai/  
  - Ollama (începeți rapid): https://github.com/ollama/ollama#get-started  
- Instrumente pentru Dezvoltatori (Windows și VS Code)  
  - AI Toolkit pentru VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview  
  - Windows ML (prezentare generală): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview  

## Comunitatea de Învățare

Alăturați-vă discuțiilor și conectați-vă cu alți cursanți:  
- Discuții pe GitHub în [repository-ul EdgeAI for Beginners](https://github.com/microsoft/edgeai-for-beginners/discussions)  
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)  
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)  

## Concluzie

EdgeAI reprezintă frontiera implementării inteligenței artificiale, aducând capabilități puternice direct pe dispozitive, în timp ce abordează preocupări critice legate de confidențialitate, latență și conectivitate. Acest curs de 20 de ore vă oferă cunoștințele esențiale și abilitățile practice pentru a începe să lucrați imediat cu tehnologiile EdgeAI.  

Cursul este conceput să fie concis și concentrat pe cele mai importante concepte, permițându-vă să dobândiți rapid expertiză valoroasă fără un angajament de timp copleșitor. Amintiți-vă că practica hands-on, chiar și cu exemple simple, este cheia consolidării a ceea ce ați învățat.  

Învățare plăcută!  

---

