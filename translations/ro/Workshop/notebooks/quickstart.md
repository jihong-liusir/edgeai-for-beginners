<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-08T15:33:32+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "ro"
}
-->
# Ghid Rapid pentru Workshop Notebooks

## Cuprins

- [Prerechizite](../../../../Workshop/notebooks)
- [Configurare IniÈ›ialÄƒ](../../../../Workshop/notebooks)
- [Sesiunea 04: Compararea Modelelor](../../../../Workshop/notebooks)
- [Sesiunea 05: Orchestrator Multi-Agent](../../../../Workshop/notebooks)
- [Sesiunea 06: Rutare pe BazÄƒ de IntenÈ›ie](../../../../Workshop/notebooks)
- [Variabile de Mediu](../../../../Workshop/notebooks)
- [Comenzi Comune](../../../../Workshop/notebooks)

---

## Prerechizite

### 1. InstaleazÄƒ Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**VerificÄƒ Instalarea:**
```bash
foundry --version
```

### 2. InstaleazÄƒ DependenÈ›ele Python

```bash
cd Workshop
pip install -r requirements.txt
```

Sau instaleazÄƒ individual:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Configurare IniÈ›ialÄƒ

### PorneÈ™te Serviciul Foundry Local

**Necesar Ã®nainte de a rula orice notebook:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

Rezultatul aÈ™teptat:
```
âœ… Service started successfully
Endpoint: http://localhost:59959
```

### DescarcÄƒ È™i ÃŽncarcÄƒ Modelele

Notebook-urile folosesc aceste modele implicit:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### VerificÄƒ Configurarea

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Sesiunea 04: Compararea Modelelor

### Scop
ComparÄƒ performanÈ›a Ã®ntre Modele de Limbaj Mici (SLM) È™i Modele de Limbaj Mari (LLM).

### Configurare RapidÄƒ

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Rularea Notebook-ului

1. **Deschide** `session04_model_compare.ipynb` Ã®n VS Code sau Jupyter
2. **ReporneÈ™te kernel-ul** (Kernel â†’ Restart Kernel)
3. **RuleazÄƒ toate celulele** Ã®n ordine

### Configurare Cheie

**Modele Implicite:**
- **SLM:** `phi-4-mini` (~4GB RAM, mai rapid)
- **LLM:** `qwen2.5-3b` (~3GB RAM, optimizat pentru memorie)

**Variabile de Mediu (opÈ›ional):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### Rezultatul AÈ™teptat

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

ðŸ’¡ SLM is 1.99x faster than LLM for this prompt
```

### Personalizare

**FoloseÈ™te modele diferite:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Prompt personalizat:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### ListÄƒ de Verificare pentru Validare

- [ ] Celula 12 afiÈ™eazÄƒ modelele corecte (phi-4-mini, qwen2.5-3b)
- [ ] Celula 12 afiÈ™eazÄƒ endpoint-ul corect (port 59959)
- [ ] Diagnosticul din celula 16 trece (âœ… Serviciul este activ)
- [ ] Verificarea prealabilÄƒ din celula 20 trece (ambele modele sunt ok)
- [ ] Compararea din celula 22 se finalizeazÄƒ cu valori de latenÈ›Äƒ
- [ ] Validarea din celula 24 afiÈ™eazÄƒ ðŸŽ‰ TOATE VERIFICÄ‚RILE AU TRECUT!

### Estimare Timp
- **Prima rulare:** 5-10 minute (inclusiv descÄƒrcarea modelelor)
- **RulÄƒri ulterioare:** 1-2 minute

---

## Sesiunea 05: Orchestrator Multi-Agent

### Scop
DemonstreazÄƒ colaborarea multi-agent folosind Foundry Local SDK - agenÈ›ii lucreazÄƒ Ã®mpreunÄƒ pentru a produce rezultate rafinate.

### Configurare RapidÄƒ

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Rularea Notebook-ului

1. **Deschide** `session05_agents_orchestrator.ipynb`
2. **ReporneÈ™te kernel-ul**
3. **RuleazÄƒ toate celulele** Ã®n ordine

### Configurare Cheie

**ConfiguraÈ›ie ImplicitÄƒ (AcelaÈ™i Model pentru Ambii AgenÈ›i):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**ConfiguraÈ›ie AvansatÄƒ (Modele Diferite):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### ArhitecturÄƒ

```
User Question
    â†“
Researcher Agent (phi-4-mini)
  â†’ Gathers bullet points
    â†“
Editor Agent (phi-4-mini or qwen2.5-7b)
  â†’ Refines into executive summary
    â†“
Final Output
```

### Rezultatul AÈ™teptat

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: â€¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Extensii

**AdaugÄƒ mai mulÈ›i agenÈ›i:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Testare Ã®n loturi:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Estimare Timp
- **Prima rulare:** 3-5 minute
- **RulÄƒri ulterioare:** 1-2 minute per Ã®ntrebare

---

## Sesiunea 06: Rutare pe BazÄƒ de IntenÈ›ie

### Scop
RuteazÄƒ inteligent prompturile cÄƒtre modele specializate pe baza intenÈ›iei detectate.

### Configurare RapidÄƒ

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**NotÄƒ:** Sesiunea 06 foloseÈ™te modele CPU implicit pentru compatibilitate maximÄƒ.

### Rularea Notebook-ului

1. **Deschide** `session06_models_router.ipynb`
2. **ReporneÈ™te kernel-ul**
3. **RuleazÄƒ toate celulele** Ã®n ordine

### Configurare Cheie

**Catalog Implicit (Modele CPU):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**AlternativÄƒ (Modele GPU):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Detectarea IntenÈ›iei

Router-ul foloseÈ™te expresii regex pentru a detecta intenÈ›ia:

| IntenÈ›ie | Exemple de Pattern | Rutat CÄƒtre |
|----------|--------------------|-------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | Orice altceva | phi-4-mini-cpu |

### Rezultatul AÈ™teptat

```
âœ“ Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

âœ“ Success! All prompts routed correctly.
```

### Personalizare

**AdaugÄƒ intenÈ›ii personalizate:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ç¿»è¯‘', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**ActiveazÄƒ urmÄƒrirea token-urilor:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### Trecerea la Modele GPU

DacÄƒ ai 8GB+ VRAM:

1. ÃŽn **Celula #6**, comenteazÄƒ catalogul CPU
2. DecomenteazÄƒ catalogul GPU
3. ÃŽncarcÄƒ modelele GPU:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. ReporneÈ™te kernel-ul È™i ruleazÄƒ din nou notebook-ul

### Estimare Timp
- **Prima rulare:** 5-10 minute (Ã®ncÄƒrcarea modelelor)
- **RulÄƒri ulterioare:** 30-60 secunde per test

---

## Variabile de Mediu

### Configurare GlobalÄƒ

SeteazÄƒ Ã®nainte de a porni Jupyter/VS Code:

**Windows (Command Prompt):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Configurare Ã®n Notebook

SeteazÄƒ la Ã®nceputul oricÄƒrui notebook:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Comenzi Comune

### Managementul Serviciului

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Managementul Modelelor

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Testarea Endpoint-urilor

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Comenzi de Diagnostic

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Cele Mai Bune Practici

### ÃŽnainte de a Porni Orice Notebook

1. **VerificÄƒ dacÄƒ serviciul este activ:**
   ```bash
   foundry service status
   ```

2. **AsigurÄƒ-te cÄƒ modelele sunt Ã®ncÄƒrcate:**
   ```bash
   foundry model ls
   ```

3. **ReporneÈ™te kernel-ul notebook-ului** dacÄƒ rulezi din nou

4. **È˜terge toate rezultatele** pentru o rulare curatÄƒ

### Managementul Resurselor

1. **FoloseÈ™te modele CPU implicit** pentru compatibilitate
2. **Treci la modele GPU** doar dacÄƒ ai 8GB+ VRAM
3. **ÃŽnchide alte aplicaÈ›ii GPU** Ã®nainte de rulare
4. **MenÈ›ine serviciul activ** Ã®ntre sesiunile notebook
5. **MonitorizeazÄƒ utilizarea resurselor** cu Task Manager / nvidia-smi

### Depanare

1. **VerificÄƒ Ã®ntotdeauna serviciul mai Ã®ntÃ¢i** Ã®nainte de a depana codul
2. **ReporneÈ™te kernel-ul** dacÄƒ vezi configuraÈ›ii vechi
3. **RuleazÄƒ din nou celulele de diagnostic** dupÄƒ orice modificare
4. **VerificÄƒ numele modelelor** sÄƒ corespundÄƒ cu cele Ã®ncÄƒrcate
5. **AsigurÄƒ-te cÄƒ portul endpoint** corespunde cu starea serviciului

---

## ReferinÈ›Äƒ RapidÄƒ: Aliasuri Modele

### Modele Comune

| Alias | Dimensiune | Cel Mai Bun Pentru | RAM/VRAM | Variante |
|-------|------------|--------------------|----------|----------|
| `phi-4-mini` | ~4B | Chat general, sumarizare | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Generare cod, refactorizare | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Sarcini generale, eficient | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Rapid, resurse reduse | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Clasificare, resurse minime | 1-2GB | `-cpu`, `-cuda-gpu` |

### Denumirea Variantelor

- **Nume de bazÄƒ** (ex.: `phi-4-mini`): SelecteazÄƒ automat cea mai bunÄƒ variantÄƒ pentru hardware-ul tÄƒu
- **`-cpu`**: Optimizat pentru CPU, funcÈ›ioneazÄƒ peste tot
- **`-cuda-gpu`**: Optimizat pentru GPU NVIDIA, necesitÄƒ 8GB+ VRAM
- **`-npu`**: Optimizat pentru NPU Qualcomm, necesitÄƒ drivere NPU

**Recomandare:** FoloseÈ™te numele de bazÄƒ (fÄƒrÄƒ sufix) È™i lasÄƒ Foundry Local sÄƒ selecteze automat cea mai bunÄƒ variantÄƒ.

---

## Indicatori de Succes

EÈ™ti pregÄƒtit cÃ¢nd vezi:

âœ… `foundry service status` afiÈ™eazÄƒ "running"  
âœ… `foundry model ls` afiÈ™eazÄƒ modelele necesare  
âœ… Serviciul este accesibil la endpoint-ul corect  
âœ… Verificarea de sÄƒnÄƒtate returneazÄƒ 200 OK  
âœ… Celulele de diagnostic din notebook trec  
âœ… Nu existÄƒ erori de conexiune Ã®n rezultate  

---

## ObÈ›inerea Ajutorului

### DocumentaÈ›ie
- **Repository Principal:** https://github.com/microsoft/Foundry-Local
- **Python SDK:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **ReferinÈ›Äƒ CLI:** https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **Depanare:** Vezi `troubleshooting.md` Ã®n acest director

### Probleme pe GitHub
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Ultima Actualizare:** 8 Octombrie 2025  
**Versiune:** Workshop Notebooks 2.0

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.