<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-23T01:12:36+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ro"
}
-->
# Sesiunea 4: Modele de Ultimă Generație – LLM-uri, SLM-uri și Inferență pe Dispozitiv

## Prezentare Generală

Comparați LLM-urile și SLM-urile, evaluați compromisurile între inferența locală și cea în cloud și implementați demonstrații care evidențiază scenarii EdgeAI folosind Phi și ONNX Runtime. Vom evidenția, de asemenea, Chainlit RAG, opțiunile de inferență WebGPU și integrarea Open WebUI.

Referințe:
- Documentația Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Ghid Open WebUI (aplicație de chat cu Open WebUI): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## Obiective de Învățare
- Înțelegeți compromisurile LLM vs SLM în ceea ce privește costul, latența și acuratețea
- Alegeți între inferența locală și cea în cloud pentru nevoi specifice de afaceri
- Implementați o demonstrație simplă RAG cu Chainlit
- Explorați WebGPU pentru accelerare în browser
- Conectați Open WebUI la Foundry Local

## Partea 1: LLM vs SLM – Matricea Decizională

Luați în considerare:
- Latența: SLM-urile pe dispozitiv oferă adesea răspunsuri sub o secundă
- Costul: inferența locală reduce costurile cloud
- Confidențialitatea: datele sensibile rămân pe dispozitiv
- Capacitatea: LLM-urile pot depăși SLM-urile în sarcini complexe
- Fiabilitatea: strategiile hibride reduc riscul de întreruperi

## Partea 2: Local vs Cloud – Modele Hibride

- Local-primar cu fallback în cloud pentru prompturi mari/complexe
- Cloud-primar cu local pentru scenarii sensibile la confidențialitate sau offline
- Rutare în funcție de tipul sarcinii (generare de cod către DeepSeek, chat general către Phi/Qwen)

## Partea 3: Aplicație de Chat RAG cu Chainlit (Minimal)

Instalați dependențele:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Rulați:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

Extindeți: adăugați un simplu recuperator (fișiere locale) și prependați contextul recuperat la promptul utilizatorului.

## Partea 4: Inferență WebGPU (Atenție)

Rulați modele mici direct în browser folosind WebGPU. Acesta este ideal pentru demonstrații orientate spre confidențialitate și experiențe fără instalare. Mai jos este un exemplu minimal, pas cu pas, folosind ONNX Runtime Web cu furnizorul de execuție WebGPU.

1) Verificați suportul WebGPU
- Browsere Chromium: chrome://gpu → confirmați că “WebGPU” este activat
- Verificare programatică (vom verifica și în cod): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) Creați un proiect minimal
Creați un folder și două fișiere: `index.html` și `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) Serviți local (Windows cmd.exe)
Folosiți un server static simplu astfel încât browserul să poată accesa modelul.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Deschideți http://localhost:5173 în browserul dvs. Ar trebui să vedeți jurnale de inițializare, crearea sesiunii cu WebGPU și o predicție argmax.

4) Depanare
- Dacă WebGPU nu este disponibil: actualizați Chrome/Edge și asigurați-vă că driverele GPU sunt la zi, apoi verificați chrome://flags pentru “Enable WebGPU”.
- Dacă apar erori CORS sau fetch: asigurați-vă că serviți fișierele prin http:// (nu file://) și că URL-ul modelului permite cereri cross-origin.
- Fallback la CPU: schimbați `executionProviders: ['wasm']` pentru a verifica comportamentul de bază.

5) Pași următori
- Înlocuiți cu un model ONNX specific domeniului (de exemplu, clasificare de imagini sau un model text mic).
- Adăugați logică de preprocesare/postprocesare pentru intrări reale.
- Pentru modele mai mari sau latență în producție, preferați Foundry Local sau ONNX Runtime Server.

## Partea 5: Open WebUI + Foundry Local (Pas cu pas)

Aceasta conectează Open WebUI la endpoint-ul compatibil OpenAI al Foundry Local pentru o interfață de chat locală.

1) Cerințe preliminare
- Foundry Local instalat și funcțional (`foundry --version`)
- Un model gata de rulare locală (de exemplu, `phi-4-mini`)
- Docker Desktop instalat (recomandat pentru Open WebUI)

2) Porniți un model cu Foundry Local
```powershell
foundry model run phi-4-mini
```
Acesta expune un API compatibil OpenAI la `http://localhost:8000`.

3) Porniți Open WebUI (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
Note:
- Pe Windows, `host.docker.internal` permite containerului să acceseze gazda la `localhost`.
- Setăm `OPENAI_API_BASE_URL` la endpoint-ul Foundry Local și un `OPENAI_API_KEY` dummy.

4) Configurați din interfața Open WebUI (alternativ)
- Accesați http://localhost:3000
- Completați configurarea inițială (utilizator admin)
- Mergeți la Settings → Models/Providers
- Set Base URL: `http://host.docker.internal:8000/v1`
- Set API Key: `local-key` (placeholder)
- Salvați

5) Rulați un prompt de test
- În chat-ul Open WebUI, selectați sau introduceți numele modelului `phi-4-mini`
- Prompt: “Listează cinci beneficii ale inferenței AI pe dispozitiv.”
- Ar trebui să vedeți un răspuns transmis de modelul local

6) Depanare
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) Opțional: Persistați datele Open WebUI
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## Lista de Verificare Practică
- [ ] Comparați răspunsurile/latența între SLM și LLM local
- [ ] Rulați demonstrația Chainlit pe cel puțin două modele
- [ ] Conectați Open WebUI la endpoint-ul local și testați

## Pași Următori
- Pregătiți-vă pentru fluxurile de lucru ale agenților în Sesiunea 5
- Identificați scenarii în care modelul hibrid local/cloud îmbunătățește ROI-ul

---

