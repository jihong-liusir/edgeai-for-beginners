<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T01:36:37+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "ro"
}
-->
# Sesiunea 4: Construirea Aplicațiilor de Chat pentru Producție cu Chainlit

## Prezentare Generală

Această sesiune se concentrează pe crearea aplicațiilor de chat pregătite pentru producție utilizând Chainlit și Microsoft Foundry Local. Vei învăța să creezi interfețe web moderne pentru conversații AI, să implementezi răspunsuri în flux și să distribui aplicații de chat robuste cu gestionarea corectă a erorilor și design pentru experiența utilizatorului.

**Ce vei construi:**
- **Aplicație de Chat Chainlit**: Interfață web modernă cu răspunsuri în flux
- **Demo WebGPU**: Inferență în browser pentru aplicații care prioritizează confidențialitatea  
- **Integrare Open WebUI**: Interfață de chat profesională cu Foundry Local
- **Modele de Producție**: Gestionarea erorilor, monitorizare și strategii de distribuție

## Obiective de Învățare

- Construirea aplicațiilor de chat pregătite pentru producție cu Chainlit
- Implementarea răspunsurilor în flux pentru o experiență îmbunătățită a utilizatorului
- Stăpânirea modelelor de integrare SDK Foundry Local
- Aplicarea gestionării corecte a erorilor și degradării grațioase
- Distribuirea și configurarea aplicațiilor de chat pentru diferite medii
- Înțelegerea modelelor moderne de interfață web pentru AI conversațional

## Cerințe Prealabile

- **Foundry Local**: Instalată și funcțională ([Ghid de Instalare](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: Versiunea 3.10 sau mai recentă, cu capacitate de mediu virtual
- **Model**: Cel puțin un model încărcat (`foundry model run phi-4-mini`)
- **Browser**: Browser web modern cu suport WebGPU (Chrome/Edge)
- **Docker**: Pentru integrarea Open WebUI (opțional)

## Partea 1: Înțelegerea Aplicațiilor Moderne de Chat

### Prezentare Generală a Arhitecturii

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### Tehnologii Cheie

**Modele SDK Foundry Local:**
- `FoundryLocalManager(alias)`: Gestionarea automată a serviciilor
- `manager.endpoint` și `manager.api_key`: Detalii de conectare
- `manager.get_model_info(alias).id`: Identificarea modelului

**Framework Chainlit:**
- `@cl.on_chat_start`: Inițializarea sesiunilor de chat
- `@cl.on_message`: Gestionarea mesajelor utilizatorului  
- `cl.Message().stream_token()`: Streaming în timp real
- Generarea automată a interfeței și gestionarea WebSocket

## Partea 2: Matricea de Decizie Local vs Cloud

### Caracteristici de Performanță

| Aspect | Local (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latență** | 🚀 50-200ms (fără rețea) | ⏱️ 200-2000ms (dependent de rețea) |
| **Confidențialitate** | 🔒 Datele nu părăsesc dispozitivul | ⚠️ Datele sunt trimise în cloud |
| **Cost** | 💰 Gratuit după hardware | 💸 Plată per token |
| **Offline** | ✅ Funcționează fără internet | ❌ Necesită internet |
| **Dimensiunea Modelului** | ⚠️ Limitată de hardware | ✅ Acces la cele mai mari modele |
| **Scalabilitate** | ⚠️ Dependentă de hardware | ✅ Scalabilitate nelimitată |

### Modele de Strategie Hibridă

**Local-First cu Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Rutare Bazată pe Sarcini:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## Partea 3: Exemplul 04 - Aplicație de Chat Chainlit

### Start Rapid

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Aplicația se deschide automat la `http://localhost:8080` cu o interfață de chat modernă.

### Implementare de Bază

Aplicația Exemplu 04 demonstrează modele pregătite pentru producție:

**Descoperirea Automată a Serviciilor:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Gestionarea Chatului în Flux:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Opțiuni de Configurare

**Variabile de Mediu:**

| Variabilă | Descriere | Implicit | Exemplu |
|----------|-------------|---------|----------|
| `MODEL` | Alias-ul modelului utilizat | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Endpoint Foundry Local | Detectat automat | `http://localhost:51211` |
| `API_KEY` | Cheie API (opțional pentru local) | `""` | `your-api-key` |

**Utilizare Avansată:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## Partea 4: Crearea și Utilizarea Notebook-urilor Jupyter

### Prezentare Generală a Suportului pentru Notebook-uri

Exemplul 04 include un notebook Jupyter cuprinzător (`chainlit_app.ipynb`) care oferă:

- **📚 Conținut Educațional**: Materiale de învățare pas cu pas
- **🔬 Explorare Interactivă**: Rulează și experimentează cu celule de cod
- **📊 Demonstrații Vizuale**: Grafice, diagrame și vizualizarea rezultatelor
- **🛠️ Instrumente de Dezvoltare**: Testare și depanare

### Crearea Propriilor Notebook-uri

#### Pasul 1: Configurarea Mediului Jupyter

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### Pasul 2: Crearea unui Nou Notebook

**Utilizând VS Code:**
1. Deschide VS Code în directorul Module08
2. Creează un fișier nou cu extensia `.ipynb`
3. Selectează kernel-ul "Foundry Local" când este solicitat
4. Începe să adaugi celule cu conținutul tău

**Utilizând Jupyter Lab:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Cele Mai Bune Practici pentru Structura Notebook-urilor

#### Organizarea Celulelor

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### Exemple Interactive și Exerciții

#### Exercițiul 1: Testarea Configurării Clientului

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Exercițiul 2: Simularea Răspunsurilor în Flux

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## Partea 5: Demo de Inferență WebGPU în Browser

### Prezentare Generală

WebGPU permite rularea modelelor AI direct în browser pentru confidențialitate maximă și experiențe fără instalare. Acest exemplu demonstrează ONNX Runtime Web cu execuție WebGPU.

### Pasul 1: Verificarea Suportului WebGPU

**Cerințe pentru Browser:**
- Chrome/Edge 113+ cu WebGPU activat
- Verificare: `chrome://gpu` → confirmă starea "WebGPU"
- Verificare programatică: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### Pasul 2: Crearea Demo-ului WebGPU

Creează directorul: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### Pasul 3: Rulează Demo-ul

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## Partea 6: Integrarea Open WebUI

### Prezentare Generală

Open WebUI oferă o interfață profesională asemănătoare ChatGPT care se conectează la API-ul compatibil OpenAI al Foundry Local.

### Pasul 1: Cerințe Prealabile

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### Pasul 2: Configurarea Docker (Recomandat)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**Notă:** `host.docker.internal` permite containerelor Docker să acceseze mașina gazdă pe Windows.

### Pasul 3: Configurare

1. **Deschide Browserul:** Navighează la `http://localhost:3000`
2. **Configurare Inițială:** Creează un cont de administrator
3. **Configurarea Modelului:**
   - Setări → Modele → API OpenAI  
   - URL de bază: `http://host.docker.internal:51211/v1`
   - Cheie API: `foundry-local-key` (orice valoare funcționează)
4. **Testează Conexiunea:** Modelele ar trebui să apară în lista derulantă

### Depanare

**Probleme Comune:**

1. **Conexiune Refuzată:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Modelele Nu Apar:**
   - Verifică dacă modelul este încărcat: `foundry model list`
   - Verifică răspunsul API: `curl http://localhost:51211/v1/models`
   - Repornește containerul Open WebUI

## Partea 7: Considerații pentru Distribuția în Producție

### Configurarea Mediului

**Setare pentru Dezvoltare:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Distribuție în Producție:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Probleme Comune cu Porturile și Soluții

**Prevenirea Conflictului pe Portul 51211:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Monitorizarea Performanței

**Implementarea Verificării Stării:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## Rezumat

Sesiunea 4 a acoperit construirea aplicațiilor Chainlit pregătite pentru producție pentru AI conversațional. Ai învățat despre:

- ✅ **Framework Chainlit**: Interfață modernă și suport pentru streaming în aplicațiile de chat
- ✅ **Integrarea Foundry Local**: Utilizarea SDK și modelele de configurare  
- ✅ **Inferență WebGPU**: AI în browser pentru confidențialitate maximă
- ✅ **Configurarea Open WebUI**: Distribuirea interfeței de chat profesionale
- ✅ **Modele de Producție**: Gestionarea erorilor, monitorizare și scalabilitate

Aplicația Exemplu 04 demonstrează cele mai bune practici pentru construirea interfețelor de chat robuste care utilizează modele AI locale prin Microsoft Foundry Local, oferind în același timp experiențe excelente utilizatorilor.

## Referințe

- **[Exemplul 04: Aplicație Chainlit](samples/04/README.md)**: Aplicație completă cu documentație
- **[Notebook Educațional Chainlit](samples/04/chainlit_app.ipynb)**: Materiale interactive de învățare
- **[Documentația Foundry Local](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Documentație completă a platformei
- **[Documentația Chainlit](https://docs.chainlit.io/)**: Documentația oficială a framework-ului
- **[Ghid de Integrare Open WebUI](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: Tutorial oficial

---

