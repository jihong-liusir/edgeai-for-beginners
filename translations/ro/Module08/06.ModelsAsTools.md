<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7256301d9d690c2054eabbf2bc5b10bf",
  "translation_date": "2025-09-23T01:08:09+00:00",
  "source_file": "Module08/06.ModelsAsTools.md",
  "language_code": "ro"
}
-->
# Sesiunea 6: Foundry Local – Modele ca Instrumente

## Prezentare Generală

Tratează modelele AI ca instrumente modulare, personalizabile, care rulează direct pe dispozitiv cu Foundry Local. Această sesiune pune accent pe fluxuri de lucru practice pentru inferență cu protecția confidențialității și latență redusă, precum și pe integrarea acestor instrumente prin SDK-uri, API-uri sau CLI. Vei învăța, de asemenea, cum să scalezi la Azure AI Foundry atunci când este necesar.

Referințe:
- Documentația Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Integrare cu SDK-uri de inferență: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- Compilarea modelelor Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models

## Obiective de Învățare
- Proiectarea modelelor ca instrumente pe dispozitiv
- Integrare prin API REST compatibil cu OpenAI sau SDK-uri
- Personalizarea modelelor pentru cazuri de utilizare specifice domeniului
- Planificarea scalării hibride către Azure AI Foundry

## Partea 1: Abstracții ale Instrumentelor (Pas cu pas)

Obiectiv: Reprezentarea modelelor ca instrumente cu contracte clare și un router simplu.

Pasul 1) Definirea interfeței și registrului instrumentului  
```python
# tools/registry.py
from dataclasses import dataclass
from typing import Callable, Dict

@dataclass
class Tool:
    name: str
    description: str
    input_schema: Dict
    output_schema: Dict
    handler: Callable[[Dict], Dict]

REGISTRY: Dict[str, Tool] = {}

def register(tool: Tool):
    REGISTRY[tool.name] = tool

def get_tool(name: str) -> Tool:
    return REGISTRY[name]
```
  
Pasul 2) Implementarea a două instrumente bazate pe Foundry Local  
```python
# tools/impl.py
import requests, os
from tools.registry import Tool, register

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}

# Chat tool (general assistant)

def chat_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    messages = payload.get("messages", [{"role":"user","content":"Hello"}])
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": payload.get("max_tokens", 300),
        "temperature": payload.get("temperature", 0.6)
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    msg = r.json()["choices"][0]["message"]["content"]
    return {"content": msg}

register(Tool(
    name="chat.assistant",
    description="General chat assistant",
    input_schema={"type":"object","properties":{"messages":{"type":"array"}}},
    output_schema={"type":"object","properties":{"content":{"type":"string"}}},
    handler=chat_handler
))

# Summarizer tool

def summarize_handler(payload: dict) -> dict:
    model = payload.get("model", "phi-4-mini")
    text = payload.get("text", "")
    messages = [
        {"role":"system","content":"You summarize text into 3 concise bullet points."},
        {"role":"user","content": f"Summarize:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
        "model": model,
        "messages": messages,
        "max_tokens": 200,
        "temperature": 0.2
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    return {"summary": r.json()["choices"][0]["message"]["content"]}

register(Tool(
    name="text.summarize",
    description="Summarize text into bullets",
    input_schema={"type":"object","properties":{"text":{"type":"string"}}},
    output_schema={"type":"object","properties":{"summary":{"type":"string"}}},
    handler=summarize_handler
))
```
  
Pasul 3) Router pe baza sarcinii  
```python
# tools/router.py
from tools.registry import get_tool

def route(task: str, payload: dict):
    mapping = {
        "general": "chat.assistant",
        "summarize": "text.summarize"
    }
    tool = get_tool(mapping[task])
    return tool.handler(payload)

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    print(route("general", {"messages":[{"role":"user","content":"Hi!"}]}))
    print(route("summarize", {"text":"Edge AI brings models to devices for privacy and low latency."}))
```
  

## Partea 2: Integrarea SDK și API (Pas cu pas)

Obiectiv: Utilizarea SDK-ului Python OpenAI împotriva endpoint-ului Foundry Local.

Pasul 1) Instalare  
```cmd
cd Module08
.\.venv\Scripts\activate
pip install openai
```
  
Pasul 2) Configurarea variabilelor de mediu  
```cmd
setx OPENAI_BASE_URL http://localhost:8000/v1
setx OPENAI_API_KEY local-key
```
  
Pasul 3) Apelarea API-ului de chat  
```python
# sdk_demo.py
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "local-key")
)

resp = client.chat.completions.create(
    model="phi-4-mini",
    messages=[{"role": "user", "content": "Summarize edge AI in one sentence."}],
    max_tokens=64
)
print(resp.choices[0].message.content)
```
  

## Partea 3: Personalizare pentru Domeniu (Pas cu pas)

Obiectiv: Adaptarea rezultatelor pentru un domeniu folosind șabloane de prompt și schema JSON.

Pasul 1) Crearea unui șablon de prompt pentru domeniu  
```python
# domain/templates.py
BUSINESS_ANALYST_SYSTEM = """
You are a senior business analyst. Provide:
1) Key insights
2) Risks
3) Next steps
Respond in valid JSON with fields: insights, risks, next_steps.
"""
```
  
Pasul 2) Impunerea unui output JSON  
```python
# domain/analyst.py
import requests, os, json

BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
HEADERS = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}

from domain.templates import BUSINESS_ANALYST_SYSTEM

def analyze(text: str) -> dict:
    messages = [
        {"role":"system","content": BUSINESS_ANALYST_SYSTEM},
        {"role":"user","content": f"Analyze this business text:\n{text}"}
    ]
    r = requests.post(f"{BASE_URL}/chat/completions", json={
    "model":"phi-4-mini",
        "messages": messages,
        "response_format": {"type":"json_object"},
        "temperature": 0.3
    }, headers=HEADERS, timeout=60)
    r.raise_for_status()
    # Parse JSON content
    content = r.json()["choices"][0]["message"]["content"]
    return json.loads(content)

if __name__ == "__main__":
    print(analyze("Sales dipped 12% in Q3 due to supply constraints and marketing cuts."))
```
  

## Partea 4: Funcționare Offline și Postură de Securitate (Pas cu pas)

Obiectiv: Asigurarea confidențialității și rezilienței atunci când modelele rulează local ca instrumente.

Pasul 1) Preîncălzirea și validarea endpoint-ului local  
```cmd
foundry model run phi-4-mini
curl http://localhost:8000/v1/models
```
  
Pasul 2) Sanitizarea inputurilor  
```python
# security/sanitize.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  
Pasul 3) Activarea flag-ului local-only și logarea  
```python
# security/local_only.py
import os, json, time
LOG = os.getenv("MODELS_AS_TOOLS_LOG", "./tools_logs.jsonl")

def record(event: dict):
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(event) + "\n")

# Usage before each call
def before_call(tool_name, payload):
    record({"ts": time.time(), "tool": tool_name, "event": "before_call"})

# After each call
def after_call(tool_name, result):
    record({"ts": time.time(), "tool": tool_name, "event": "after_call"})
```
  

## Partea 5: Scalarea către Azure AI Foundry (Pas cu pas)

Obiectiv: Reflectarea modelelor locale cu endpoint-uri Azure pentru capacitate suplimentară.

Pasul 1) Deciderea strategiei de rutare  
- Local-primar pentru confidențialitate/latență, fallback Azure în caz de erori sau prompturi mari  

Pasul 2) Implementarea unui stub simplu de router  
```python
# hybrid/router.py
import os, requests

LOCAL_BASE = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
AZURE_BASE = os.getenv("AZURE_FOUNDRY_BASE_URL", "")  # set to your project endpoint
API_KEY = os.getenv("OPENAI_API_KEY", "local-key")
AZURE_KEY = os.getenv("AZURE_FOUNDRY_API_KEY", "")

HEADERS_LOCAL = {"Content-Type":"application/json","Authorization":f"Bearer {API_KEY}"}
HEADERS_AZURE = {"Content-Type":"application/json","Authorization":f"Bearer {AZURE_KEY}"}

def chat_local(payload: dict):
    r = requests.post(f"{LOCAL_BASE}/chat/completions", json=payload, headers=HEADERS_LOCAL, timeout=60)
    r.raise_for_status()
    return r.json()

def chat_azure(payload: dict):
    if not AZURE_BASE:
        raise RuntimeError("Azure base URL not configured")
    r = requests.post(f"{AZURE_BASE}/chat/completions", json=payload, headers=HEADERS_AZURE, timeout=60)
    r.raise_for_status()
    return r.json()

def hybrid_chat(messages, prefer_local=True):
    payload = {"model":"phi-4-mini", "messages": messages, "max_tokens": 256}
    if prefer_local:
        try:
            return chat_local(payload)
        except Exception:
            return chat_azure(payload)
    else:
        try:
            return chat_azure(payload)
        except Exception:
            return chat_local(payload)

if __name__ == "__main__":
    # Ensure local model is running
    print(hybrid_chat([{"role":"user","content":"Hello from hybrid router!"}]))
```
  

## Checklist Practic
- [ ] Înregistrarea a cel puțin două instrumente și rutarea cererilor
- [ ] Apelarea Foundry Local prin SDK OpenAI și REST brut
- [ ] Impunerea outputurilor JSON pentru un șablon de domeniu
- [ ] Sanitizarea și logarea apelurilor local
- [ ] Implementarea unui router hibrid simplu cu fallback Azure

## Încheiere

Foundry Local permite AI robust pe dispozitiv, unde modelele devin instrumente compozabile. Cu interfețe clare, guvernanță și scalare hibridă, echipele pot livra aplicații AI în timp real, sigure, care respectă confidențialitatea utilizatorilor, fiind în același timp pregătite pentru mediul enterprise.

---

