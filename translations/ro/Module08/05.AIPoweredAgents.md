<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "382a763fcea7087e68a94c26216e5e70",
  "translation_date": "2025-09-23T00:56:02+00:00",
  "source_file": "Module08/05.AIPoweredAgents.md",
  "language_code": "ro"
}
-->
# Sesiunea 5: Construiește rapid agenți alimentați de AI cu Foundry Local

Notă: Capacitățile agenților în Foundry Local evoluează—confirmă suportul în cele mai recente note de lansare înainte de a implementa modele avansate.

## Prezentare generală

Folosește Foundry Local pentru a prototipa rapid aplicații agentice: prompturi de sistem, fundamentare și modele de orchestrare. Când suportul pentru agenți este disponibil, poți standardiza pe apeluri de funcții compatibile cu OpenAI sau poți utiliza Azure AI Agents pe partea de cloud în designuri hibride.

Referințe:
- Documentația Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Azure AI Foundry Agents: https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Exemplu de apelare funcții (exemple Foundry Local): https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## Obiective de învățare
- Proiectează prompturi de sistem și strategii de fundamentare pentru un comportament fiabil
- Implementează modele de apelare funcții (utilizare de instrumente)
- Orchestrare fluxuri de lucru multi-agent (local și hibrid)
- Planifică pentru observabilitate și siguranță

## Partea 1: Prompturi de sistem și fundamentare

- Definește roluri stricte, constrângeri și scheme de ieșire
- Fundamentează răspunsurile cu date locale sau de întreprindere
- Impune ieșiri JSON pentru automatizare ulterioară

## Partea 2: Apelare funcții (compatibil cu OpenAI)

```python
# tools.py
import json

def get_weather(city: str) -> str:
    return f"Weather in {city}: Sunny, 25C"

FUNCTIONS = [
    {
        "name": "get_weather",
        "description": "Get current weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "City name"}
            },
            "required": ["city"]
        }
    }
]
```

```python
# agent.py
import requests
import json
from tools import FUNCTIONS, get_weather

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

SYSTEM_PROMPT = "You are a helpful assistant. Use tools when needed."

def call_model(messages, functions=None):
    payload = {
        "model": MODEL,
        "messages": messages,
        "functions": functions,
        "function_call": "auto"
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    return r.json()

messages = [{"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": "What's the weather in Paris?"}]

resp = call_model(messages, functions=FUNCTIONS)
choice = resp["choices"][0]["message"]

if "function_call" in choice:
    fc = choice["function_call"]
    if fc["name"] == "get_weather":
        args = json.loads(fc["arguments"])
        result = get_weather(args["city"])
        messages.append(choice)
        messages.append({"role": "function", "name": "get_weather", "content": result})
        final = call_model(messages)
        print(final["choices"][0]["message"]["content"]) 
else:
    print(choice.get("content"))
```

Execută:
```powershell
# Ensure a model is running
foundry model run phi-4-mini
python agent.py
```


## Partea 3: Orchestrare multi-agent (model)

Proiectează un coordonator care direcționează sarcinile către agenți specializați (recuperare, raționament, execuție) folosind endpoint-ul compatibil cu OpenAI din Foundry Local.

Pasul 1) Definește agenți specializați  
```python
# agents/specialists.py
import requests
BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

headers = {"Content-Type": "application/json", "Authorization": "Bearer local-key"}

def chat(messages, max_tokens=300, temperature=0.4):
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json={
        "model": MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature
    }, headers=headers, timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

class RetrievalAgent:
    SYSTEM = "You retrieve relevant snippets from knowledge sources based on a query."
    def run(self, query: str) -> str:
        # Placeholder: in real use, fetch from local files or vector DB
        messages = [{"role": "system", "content": self.SYSTEM},
                    {"role": "user", "content": f"Retrieve key facts for: {query}"}]
        return chat(messages)

class ReasoningAgent:
    SYSTEM = "You analyze inputs step by step and produce structured conclusions."
    def run(self, context: str, question: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}\nThink step-by-step and produce a concise answer."}
        ]
        return chat(messages)

class ExecutionAgent:
    SYSTEM = "You transform decisions into actionable steps (JSON with actions)."
    def run(self, decision: str) -> str:
        messages = [
            {"role": "system", "content": self.SYSTEM},
            {"role": "user", "content": f"Turn this decision into 3 executable steps as JSON:\n{decision}"}
        ]
        return chat(messages)
```
  
Pasul 2) Construiește coordonatorul  
```python
# agents/coordinator.py
from agents.specialists import RetrievalAgent, ReasoningAgent, ExecutionAgent

class Coordinator:
    def __init__(self):
        self.retrieval = RetrievalAgent()
        self.reasoning = ReasoningAgent()
        self.execution = ExecutionAgent()

    def handle(self, user_goal: str) -> dict:
        # 1. Retrieve context
        context = self.retrieval.run(user_goal)
        # 2. Reason on context
        decision = self.reasoning.run(context, user_goal)
        # 3. Produce actionable steps
        actions = self.execution.run(decision)
        return {
            "goal": user_goal,
            "context": context,
            "decision": decision,
            "actions": actions
        }

if __name__ == "__main__":
    # Ensure: foundry model run phi-4-mini
    coord = Coordinator()
    result = coord.handle("Create a plan to onboard 5 new customers this month")
    print(result)
```
  
Pasul 3) Validează împotriva Foundry Local  
```powershell
REM Confirm the local endpoint and model are available
foundry model list
foundry model run phi-4-mini
curl http://localhost:8000/v1/models

REM Run the coordinator
python -m samples.05.agents.coordinator
```
  

Ghiduri:
- Implementează reîncercări și limite de timp între agenți
- Adaugă un mic depozit în memorie (dict) pentru starea conversației/firului
- Introdu rate-limiting când lansezi mai multe apeluri în lanț

## Partea 4: Observabilitate și siguranță

Monitorizează prompturile, răspunsurile și erorile local, în timp ce aplici igiena datelor în stiva ta de agenți.

Pasul 1) Jurnalizare ușoară a cererilor (opțional)

Notă: Următorul ajutor nu este inclus implicit. Creează `infra/obs.py` dacă dorești jurnalizare JSON locală pentru experimente.  
```python
# infra/obs.py
import time, json, os
from datetime import datetime

LOG_DIR = os.getenv("FOUNDRY_AGENT_LOG_DIR", "./agent_logs")
os.makedirs(LOG_DIR, exist_ok=True)

def log_event(kind: str, payload: dict):
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(LOG_DIR, f"{ts}_{kind}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
```
  

Integrează jurnalizarea în agenți (opțional):  
```python
# in agents/specialists.py after receiving content
from infra.obs import log_event
# ... inside chat(...)
resp = r.json()
log_event("chat_request", {"endpoint": f"{BASE_URL}/v1/chat/completions"})
log_event("chat_response", resp)
return resp["choices"][0]["message"]["content"]
```
  

Pasul 2) Validează disponibilitatea și sănătatea de bază prin CLI  
```powershell
REM Ensure Foundry Local is running a model
foundry model list
foundry model run phi-4-mini

REM Validate the OpenAI-compatible endpoint
curl http://localhost:8000/v1/models
```
  

Pasul 3) Redactare și igiena PII  
- Înainte de a trimite mesaje către model, elimină sau hash-uiește câmpurile sensibile (emailuri, numere de telefon, ID-uri)  
- Păstrează datele sursă brute pe dispozitiv, transmite doar șirurile de context necesare  

Exemplu de ajutor pentru redactare:  
```python
# infra/redact.py
import re
EMAIL_RE = re.compile(r"[\w\.-]+@[\w\.-]+")
PHONE_RE = re.compile(r"\+?\d[\d\s\-]{7,}\d")

def sanitize(text: str) -> str:
    text = EMAIL_RE.sub("[REDACTED_EMAIL]", text)
    text = PHONE_RE.sub("[REDACTED_PHONE]", text)
    return text
```
  

Utilizare în agenți:  
```python
from infra.redact import sanitize
# user_goal = sanitize(user_goal)
# context = sanitize(context)
```
  

Pasul 4) Circuit breakers și gestionarea erorilor  
- Învelește fiecare apel de agent cu try/except și backoff exponențial  
- Oprește pipeline-ul în cazul eșecurilor repetate  

```python
import time

def with_retry(func, retries=3, base_delay=0.5):
    for i in range(retries):
        try:
            return func()
        except Exception as e:
            if i == retries - 1:
                raise
            time.sleep(base_delay * (2 ** i))
```
  

Pasul 5) Urmărire locală și export  
- Stochează jurnalele JSON sub `./agent_logs`  
- Comprimă și rotește periodic jurnalele  
- Exportă rezumate pentru revizuiri (numărări, latență medie, rate de eroare)  

Pasul 6) Verificare încrucișată cu documentația Microsoft Learn  
- Foundry Local oferă o API compatibilă cu OpenAI (validată cu `curl /v1/models`)  
- Folosește `foundry model run <name>` pentru a confirma disponibilitatea modelului  
- Urmează ghidurile oficiale pentru integrarea clientului și aplicațiile exemplu (Open WebUI/how-tos)  

Referințe:
- Foundry Local (Learn): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Open WebUI how-to: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
- Exemplu de apelare funcții: https://github.com/microsoft/Foundry-Local/tree/main/samples/python/functioncalling

## Pași următori
- Explorează Azure AI Agents pentru orchestrare găzduită în cloud
- Adaugă conectori de întreprindere (Microsoft Graph, Search, baze de date)

---

