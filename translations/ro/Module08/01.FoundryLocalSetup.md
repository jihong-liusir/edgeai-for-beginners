<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6a574846c3919c56f1d02bf1de2003ca",
  "translation_date": "2025-10-01T01:27:20+00:00",
  "source_file": "Module08/01.FoundryLocalSetup.md",
  "language_code": "ro"
}
-->
# Sesiunea 1: Introducere în Foundry Local

## Prezentare generală

Microsoft Foundry Local aduce capabilitățile Azure AI Foundry direct în mediul de dezvoltare Windows 11, permițând dezvoltarea AI cu latență redusă și protecția confidențialității, utilizând instrumente de nivel enterprise. Această sesiune acoperă instalarea completă, configurarea și implementarea practică a unor modele populare, inclusiv phi, qwen, deepseek și GPT-OSS-20B.

## Obiective de învățare

Până la finalul acestei sesiuni, vei putea:
- Instala și configura Foundry Local pe Windows 11
- Stăpâni comenzile CLI și opțiunile de configurare
- Înțelege strategiile de caching ale modelelor pentru performanță optimă
- Rula cu succes modelele phi, qwen, deepseek și GPT-OSS-20B
- Crea prima ta aplicație AI utilizând Foundry Local

## Cerințe preliminare

### Cerințe de sistem
- **Windows 11**: Versiunea 22H2 sau mai recentă
- **RAM**: Minimum 16GB, recomandat 32GB
- **Spațiu de stocare**: 50GB liberi pentru modele și cache
- **Hardware**: Dispozitiv cu NPU sau GPU preferat (PC Copilot+ sau GPU NVIDIA)
- **Rețea**: Internet de mare viteză pentru descărcarea modelelor

### Mediu de dezvoltare
```powershell
# Verify Windows version
winver

# Check available memory
Get-ComputerInfo | Select-Object TotalPhysicalMemory

# Verify PowerShell version (5.1+ required)
$PSVersionTable.PSVersion

# Set up Python environment (recommended)
py -m venv .venv
.venv\Scripts\activate

# Install required dependencies
pip install openai foundry-local-sdk
```

## Partea 1: Instalare și configurare

### Pasul 1: Instalarea Foundry Local

Instalează Foundry Local utilizând Winget sau descarcă instalatorul de pe GitHub:

```powershell
# Winget (Windows)
winget install --id Microsoft.FoundryLocal --source winget

# Alternatively: download installer from the official repo
# https://aka.ms/foundry-local-installer
```

### Pasul 2: Verificarea instalării

```powershell
# Check Foundry Local version
foundry --version

# Verify CLI accessibility and categories
foundry --help
foundry model --help
foundry cache --help
foundry service --help
```

## Partea 2: Înțelegerea CLI

### Structura comenzilor de bază

```powershell
# General command structure
foundry [category] [command] [options]

# Main categories
foundry model   # manage and run models
foundry service # manage the local service
foundry cache   # manage local model cache

# Common commands
foundry model list              # list available models
foundry model run phi-4-mini  # run a model (downloads as needed)
foundry cache ls                # list cached models
```


## Partea 3: Gestionarea și caching-ul modelelor

Foundry Local implementează caching inteligent al modelelor pentru optimizarea performanței și stocării:

```powershell
# Show cache contents
foundry cache ls

# Optional: change cache directory (advanced)
foundry cache cd "C:\\FoundryLocal\\Cache"
foundry cache ls
```

## Partea 4: Implementarea practică a modelelor

### Rularea modelelor Microsoft Phi

```powershell
# List catalog and run Phi (auto-downloads best variant for your hardware)
foundry model list
foundry model run phi-4-mini
```

### Lucrul cu modelele Qwen

```powershell
# Run Qwen2.5 models (downloads on first run)
foundry model run qwen2.5-7b
foundry model run qwen2.5-14b
```

### Rularea modelelor DeepSeek

```powershell
# Run DeepSeek model
foundry model run deepseek-r1-7b
```

### Rularea GPT-OSS-20B

```powershell
# Run the latest OpenAI open-source model (requires recent Foundry Local and sufficient GPU VRAM)
foundry model run gpt-oss-20b

# Check version if you encounter errors (requires 0.6.87+ per docs)
foundry --version
```

## Partea 5: Crearea primei aplicații

### Aplicație modernă de chat (OpenAI SDK + Foundry Local)

Creează o aplicație de chat pregătită pentru producție utilizând OpenAI SDK cu integrarea Foundry Local, urmând modelele din Sample 01.

```python
# chat_quickstart.py (Sample 01 pattern)
import os
import sys
from openai import OpenAI

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False
    print("⚠️ Install foundry-local-sdk: pip install foundry-local-sdk")

def create_client():
    """Create OpenAI client with Foundry Local or Azure OpenAI."""
    # Check for Azure OpenAI configuration
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        # Azure OpenAI path
        model = os.environ.get("MODEL", "your-deployment-name")
        client = OpenAI(
            base_url=f"{azure_endpoint}/openai",
            api_key=azure_api_key,
            default_query={"api-version": "2024-08-01-preview"},
        )
        print(f"🌐 Using Azure OpenAI with model: {model}")
        return client, model
    
    # Foundry Local path with SDK management
    alias = os.environ.get("MODEL", "phi-4-mini")
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # Use FoundryLocalManager for proper service management
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            model = model_info.id
            print(f"🏠 Using Foundry Local SDK with model: {model}")
            return client, model
        except Exception as e:
            print(f"⚠️ Foundry SDK failed ({e}), using manual configuration")
    
    # Fallback to manual configuration
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    model = alias
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    print(f"🔧 Manual configuration with model: {model}")
    return client, model

def main():
    """Main chat function."""
    client, model = create_client()
    
    print("Foundry Local Chat Interface (type 'quit' to exit)\n")
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            break
        
        try:
            # Add user message to history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=conversation_history,
                max_tokens=500,
                temperature=0.7
            )
            
            assistant_message = response.choices[0].message.content
            conversation_history.append({"role": "assistant", "content": assistant_message})
            
            print(f"Assistant: {assistant_message}\n")
            
        except Exception as e:
            print(f"Error: {e}\n")

if __name__ == "__main__":
    main()
```

### Rularea aplicației de chat

```powershell
# Ensure the model is running in another terminal
foundry model run phi-4-mini

# Option 1: Using FoundryLocalManager (recommended)
python chat_quickstart.py "Explain what Foundry Local is"

# Option 2: Manual configuration with environment variables
set BASE_URL=http://localhost:8000
set MODEL=phi-4-mini
set API_KEY=
python chat_quickstart.py "Write a welcome message"

# Option 3: Azure OpenAI configuration
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
python chat_quickstart.py "Hello from Azure OpenAI"
```

## Partea 6: Depanare și bune practici

### Probleme comune și soluții

```powershell
# Issue: "Could not use Foundry SDK" warning
pip install foundry-local-sdk
# Or set environment variables for manual configuration

# Issue: Connection refused
foundry service status
foundry service ps  # Check loaded models

# Issue: Model not found
foundry model list
foundry model run phi-4-mini

# Issue: Cache problems or low disk space
foundry cache ls
foundry cache clean

# Issue: GPT-OSS-20B not supported on your version
foundry --version
winget upgrade --id Microsoft.FoundryLocal

# Test API endpoint
curl http://localhost:8000/v1/models
```

### Monitorizarea resurselor sistemului (Windows)

```powershell
# Quick CPU and process view
Get-Process | Sort-Object -Property CPU -Descending | Select-Object -First 10
Get-Counter '\\Processor(_Total)\\% Processor Time' -SampleInterval 1 -MaxSamples 10
```

### Variabile de mediu

| Variabilă | Descriere | Implicit | Necesară |
|-----------|-----------|----------|----------|
| `MODEL` | Alias sau nume model | `phi-4-mini` | Nu |
| `BASE_URL` | URL de bază Foundry Local | `http://localhost:8000` | Nu |
| `API_KEY` | Cheie API (de obicei nu este necesară local) | `""` | Nu |
| `AZURE_OPENAI_ENDPOINT` | Endpoint Azure OpenAI | - | Pentru Azure |
| `AZURE_OPENAI_API_KEY` | Cheie API Azure OpenAI | - | Pentru Azure |
| `AZURE_OPENAI_API_VERSION` | Versiune API Azure | `2024-08-01-preview` | Nu |

### Bune practici

- **Utilizează OpenAI SDK**: Preferă OpenAI SDK în locul cererilor HTTP brute pentru o mai bună întreținere
- **FoundryLocalManager**: Utilizează SDK-ul oficial pentru gestionarea serviciilor, dacă este disponibil
- **Gestionarea erorilor**: Implementează strategii de fallback adecvate pentru aplicațiile de producție
- **Actualizează regulat**: Menține Foundry Local actualizat pentru acces la modele noi și corecții
- **Începe cu modele mici**: Începe cu modele mai mici (Phi mini, Qwen 7B) și extinde treptat
- **Monitorizează resursele**: Urmărește CPU/GPU/memoria în timp ce ajustezi prompturile și setările

## Partea 7: Exerciții practice

### Exercițiul 1: Test rapid cu mai multe modele

```powershell
# deploy-models.ps1
$models = @(
    "phi-4-mini",
    "qwen2.5-7b"
)
foreach ($model in $models) {
    Write-Host "Running $model..."
    foundry model run $model --verbose
}
```

### Exercițiul 2: Test de integrare OpenAI SDK

```python
# sdk_integration_test.py (matching Sample 01 pattern)
import os
from openai import OpenAI
from foundry_local import FoundryLocalManager

def test_model_integration(model_alias):
    """Test OpenAI SDK integration with different models."""
    try:
        # Use FoundryLocalManager for proper setup
        manager = FoundryLocalManager(model_alias)
        model_info = manager.get_model_info(model_alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # Test basic completion
        response = client.chat.completions.create(
            model=model_info.id,
            messages=[{"role": "user", "content": "Say hello and state your model name."}],
            max_tokens=50
        )
        
        print(f"✅ {model_alias}: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"❌ {model_alias}: {e}")
        return False

# Test multiple models
models_to_test = ["phi-4-mini", "qwen2.5-7b"]
for model in models_to_test:
    test_model_integration(model)
```

### Exercițiul 3: Verificare completă a sănătății serviciului

```python
# health_check.py
from openai import OpenAI
from foundry_local import FoundryLocalManager

def comprehensive_health_check():
    """Perform comprehensive health check of Foundry Local service."""
    try:
        # Initialize with a common model
        manager = FoundryLocalManager("phi-4-mini")
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key
        )
        
        # 1. Check service connectivity
        models_response = client.models.list()
        available_models = [model.id for model in models_response.data]
        print(f"✅ Service healthy - {len(available_models)} models available")
        
        # 2. Test each available model
        for model_id in available_models:
            try:
                response = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=10
                )
                print(f"✅ {model_id}: Working")
            except Exception as e:
                print(f"❌ {model_id}: {e}")
        
        return True
    except Exception as e:
        print(f"❌ Service check failed: {e}")
        return False

comprehensive_health_check()
```

## Referințe

- **Introducere în Foundry Local**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
- **Referință CLI și prezentare generală a comenzilor**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
- **Integrarea OpenAI SDK**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
- **Compilarea modelelor Hugging Face**: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- **Microsoft Foundry Local GitHub**: https://github.com/microsoft/Foundry-Local
- **OpenAI Python SDK**: https://github.com/openai/openai-python
- **Sample 01: Chat rapid prin OpenAI SDK**: samples/01/README.md
- **Sample 02: Integrare avansată SDK**: samples/02/README.md

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.