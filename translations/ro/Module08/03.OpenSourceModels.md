<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:01:21+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "ro"
}
-->
# Sesiunea 3: Modele Open-Source cu Foundry Local

## Prezentare Generală

Această sesiune explorează cum să aduci modele open-source în Foundry Local: selectarea modelelor comunitare, integrarea conținutului Hugging Face și adoptarea strategiilor „aduci propriul model” (BYOM). Vei descoperi, de asemenea, seria Model Mondays pentru învățare continuă și descoperirea de modele.

Referințe:
- Documentația Foundry Local: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Compilarea modelelor Hugging Face: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Obiective de Învățare
- Descoperirea și evaluarea modelelor open-source pentru inferență locală
- Compilarea și rularea modelelor selectate Hugging Face în Foundry Local
- Aplicarea strategiilor de selecție a modelelor pentru acuratețe, latență și necesități de resurse
- Gestionarea modelelor local cu cache și versiuni

## Partea 1: Descoperirea și Selectarea Modelelor (Pas cu pas)

Pasul 1) Listează modelele disponibile în catalogul local  
```cmd
foundry model list
```
  
Pasul 2) Test rapid al două modele candidate (descărcare automată la prima rulare)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Pasul 3) Notează metricile de bază  
- Observă latența (subiectivă) și calitatea pentru un prompt fix  
- Monitorizează utilizarea memoriei prin Task Manager în timp ce fiecare model rulează  

## Partea 2: Rularea Modelelor din Catalog prin CLI (Pas cu pas)

Pasul 1) Pornește un model  
```cmd
foundry model run llama-3.2
```
  
Pasul 2) Trimite un prompt de test prin endpoint-ul compatibil OpenAI  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Partea 3: BYOM – Compilarea Modelelor Hugging Face (Pas cu pas)

Urmează ghidul oficial pentru compilarea modelelor. Fluxul general este prezentat mai jos—vezi articolul Microsoft Learn pentru comenzi exacte și configurații suportate.

Pasul 1) Pregătește un director de lucru  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Pasul 2) Compilează un model HF suportat  
- Folosește pașii din documentația Learn pentru a converti și plasa modelul ONNX compilat în directorul `models`  
- Confirmă cu:  
```cmd
foundry cache ls
```
  
Ar trebui să vezi numele modelului compilat (de exemplu, `llama-3.2`).  

Pasul 3) Rulează modelul compilat  
```cmd
foundry model run llama-3.2 --verbose
```
  
Note:  
- Asigură-te că ai suficient spațiu pe disc și RAM pentru compilare și rulare  
- Începe cu modele mai mici pentru a valida fluxul, apoi extinde  

## Partea 4: Curarea Practică a Modelelor (Pas cu pas)

Pasul 1) Creează un registru `models.json`  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Pasul 2) Script mic de selecție  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Partea 5: Benchmarks Practice (Pas cu pas)

Pasul 1) Benchmark simplu de latență  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Pasul 2) Verificare rapidă a calității  
- Folosește un set fix de prompturi, capturează rezultatele într-un CSV/JSON  
- Evaluează manual fluența, relevanța și corectitudinea (1–5)  

## Partea 6: Pași Următori
- Abonează-te la Model Mondays pentru modele noi și sfaturi: https://aka.ms/model-mondays  
- Contribuie cu descoperirile tale la `models.json` al echipei  
- Pregătește-te pentru Sesiunea 4: compararea LLM-urilor vs SLM-urilor, inferența locală vs cloud și demonstrații practice  

---

