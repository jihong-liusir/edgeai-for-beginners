<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T23:44:30+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ro"
}
-->
# Secțiunea 3: Microsoft Olive Optimization Suite

## Cuprins
1. [Introducere](../../../Module04)
2. [Ce este Microsoft Olive?](../../../Module04)
3. [Instalare](../../../Module04)
4. [Ghid de început rapid](../../../Module04)
5. [Exemplu: Conversia Qwen3 la ONNX INT4](../../../Module04)
6. [Utilizare avansată](../../../Module04)
7. [Cele mai bune practici](../../../Module04)
8. [Depanare](../../../Module04)
9. [Resurse suplimentare](../../../Module04)

## Introducere

Microsoft Olive este un toolkit puternic și ușor de utilizat pentru optimizarea modelelor, orientat pe hardware, care simplifică procesul de optimizare a modelelor de învățare automată pentru implementare pe diverse platforme hardware. Indiferent dacă vizați CPU-uri, GPU-uri sau acceleratoare AI specializate, Olive vă ajută să obțineți performanțe optime, menținând în același timp acuratețea modelului.

## Ce este Microsoft Olive?

Olive este un instrument de optimizare a modelelor, orientat pe hardware, care integrează tehnici de vârf din industrie pentru compresia, optimizarea și compilarea modelelor. Funcționează împreună cu ONNX Runtime ca o soluție completă de optimizare pentru inferență.

### Caracteristici principale

- **Optimizare orientată pe hardware**: Selectează automat cele mai bune tehnici de optimizare pentru hardware-ul țintă
- **Peste 40 de componente de optimizare integrate**: Include compresia modelelor, cuantificarea, optimizarea graficului și altele
- **Interfață CLI ușor de utilizat**: Comenzi simple pentru sarcini comune de optimizare
- **Suport multi-cadru**: Funcționează cu PyTorch, modele Hugging Face și ONNX
- **Suport pentru modele populare**: Olive poate optimiza automat arhitecturi de modele populare precum Llama, Phi, Qwen, Gemma etc., direct din cutie

### Beneficii

- **Reducerea timpului de dezvoltare**: Nu mai este nevoie să experimentați manual cu diferite tehnici de optimizare
- **Creșteri de performanță**: Îmbunătățiri semnificative ale vitezei (până la 6x în unele cazuri)
- **Implementare pe mai multe platforme**: Modelele optimizate funcționează pe diferite hardware-uri și sisteme de operare
- **Menținerea acurateței**: Optimizările păstrează calitatea modelului, îmbunătățind în același timp performanța

## Instalare

### Cerințe preliminare

- Python 3.8 sau mai recent
- Manager de pachete pip
- Mediu virtual (recomandat)

### Instalare de bază

Creați și activați un mediu virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instalați Olive cu funcții de auto-optimizare:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependențe opționale

Olive oferă diverse dependențe opționale pentru funcții suplimentare:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificarea instalării

```bash
olive --help
```

Dacă instalarea este reușită, ar trebui să vedeți mesajul de ajutor al CLI Olive.

## Ghid de început rapid

### Prima optimizare

Să optimizăm un model de limbaj mic folosind funcția de auto-optimizare a Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ce face această comandă

Procesul de optimizare implică: preluarea modelului din cache-ul local, capturarea graficului ONNX și stocarea greutăților într-un fișier de date ONNX, optimizarea graficului ONNX și cuantificarea modelului la int4 folosind metoda RTN.

### Explicația parametrilor comenzii

- `--model_name_or_path`: Identificatorul modelului Hugging Face sau calea locală
- `--output_path`: Directorul în care va fi salvat modelul optimizat
- `--device`: Dispozitivul țintă (cpu, gpu)
- `--provider`: Furnizorul de execuție (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Utilizați ONNX Runtime Generate AI pentru inferență
- `--precision`: Precizia cuantificării (int4, int8, fp16)
- `--log_level`: Nivelul de detaliere al jurnalului (0=minimal, 1=detaliat)

## Exemplu: Conversia Qwen3 la ONNX INT4

Bazat pe exemplul oferit de Hugging Face la [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), iată cum să optimizați un model Qwen3:

### Pasul 1: Descărcarea modelului (opțional)

Pentru a minimiza timpul de descărcare, cache-uiți doar fișierele esențiale:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Pasul 2: Optimizarea modelului Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Pasul 3: Testarea modelului optimizat

Creați un script Python simplu pentru a testa modelul optimizat:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Structura rezultatelor

După optimizare, directorul de ieșire va conține:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Utilizare avansată

### Fișiere de configurare

Pentru fluxuri de lucru mai complexe de optimizare, puteți utiliza fișiere de configurare JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Rulați cu configurare:

```bash
olive run --config config.json
```

### Optimizare GPU

Pentru optimizarea GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pentru DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ajustare fină cu Olive

Olive suportă și ajustarea fină a modelelor:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Cele mai bune practici

### 1. Selectarea modelului
- Începeți cu modele mai mici pentru testare (de exemplu, 0.5B-7B parametri)
- Asigurați-vă că arhitectura modelului țintă este suportată de Olive

### 2. Considerații hardware
- Potriviți optimizarea țintă cu hardware-ul de implementare
- Utilizați optimizarea GPU dacă aveți hardware compatibil CUDA
- Luați în considerare DirectML pentru mașinile Windows cu grafică integrată

### 3. Selectarea preciziei
- **INT4**: Compresie maximă, pierdere ușoară de acuratețe
- **INT8**: Echilibru bun între dimensiune și acuratețe
- **FP16**: Pierdere minimă de acuratețe, reducere moderată a dimensiunii

### 4. Testare și validare
- Testați întotdeauna modelele optimizate cu cazurile dvs. specifice de utilizare
- Comparați metricile de performanță (latență, throughput, acuratețe)
- Utilizați date de intrare reprezentative pentru evaluare

### 5. Optimizare iterativă
- Începeți cu auto-optimizarea pentru rezultate rapide
- Utilizați fișiere de configurare pentru control detaliat
- Experimentați cu diferite etape de optimizare

## Depanare

### Probleme comune

#### 1. Probleme de instalare
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Probleme CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Probleme de memorie
- Utilizați dimensiuni mai mici ale batch-ului în timpul optimizării
- Încercați cuantificarea cu o precizie mai mare mai întâi (int8 în loc de int4)
- Asigurați-vă că aveți suficient spațiu pe disc pentru cache-ul modelului

#### 4. Erori la încărcarea modelului
- Verificați calea modelului și permisiunile de acces
- Verificați dacă modelul necesită `trust_remote_code=True`
- Asigurați-vă că toate fișierele necesare ale modelului sunt descărcate

### Obținerea ajutorului

- **Documentație**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Probleme GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemple**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Resurse suplimentare

### Linkuri oficiale
- **Repository GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Documentație ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemplu Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemple comunitare
- **Jupyter Notebooks**: Disponibile în repository-ul GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Extensie VS Code**: Prezentare generală AI Toolkit pentru VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Postări pe blog**: Blogul Microsoft Open Source — https://opensource.microsoft.com/blog/

### Instrumente conexe
- **ONNX Runtime**: Motor de inferență de înaltă performanță — https://onnxruntime.ai/
- **Hugging Face Transformers**: Sursă pentru multe modele compatibile — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Fluxuri de lucru de optimizare bazate pe cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Ce urmează

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

