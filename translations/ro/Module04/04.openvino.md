<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T18:49:57+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ro"
}
-->
# Secțiunea 4: Suita de Optimizare OpenVINO Toolkit

## Cuprins
1. [Introducere](../../../Module04)
2. [Ce este OpenVINO?](../../../Module04)
3. [Instalare](../../../Module04)
4. [Ghid de Start Rapid](../../../Module04)
5. [Exemplu: Conversia și Optimizarea Modelelor cu OpenVINO](../../../Module04)
6. [Utilizare Avansată](../../../Module04)
7. [Cele Mai Bune Practici](../../../Module04)
8. [Depanare](../../../Module04)
9. [Resurse Suplimentare](../../../Module04)

## Introducere

OpenVINO (Open Visual Inference and Neural Network Optimization) este un toolkit open-source dezvoltat de Intel pentru implementarea soluțiilor AI performante în medii cloud, on-premises și edge. Indiferent dacă vizați CPU-uri, GPU-uri, VPU-uri sau acceleratoare AI specializate, OpenVINO oferă capabilități cuprinzătoare de optimizare, menținând acuratețea modelelor și facilitând implementarea pe mai multe platforme.

## Ce este OpenVINO?

OpenVINO este un toolkit open-source care permite dezvoltatorilor să optimizeze, să convertească și să implementeze modele AI eficient pe diverse platforme hardware. Acesta include trei componente principale: OpenVINO Runtime pentru inferență, Neural Network Compression Framework (NNCF) pentru optimizarea modelelor și OpenVINO Model Server pentru implementare scalabilă.

### Caracteristici Cheie

- **Implementare Multi-Platformă**: Suportă Linux, Windows și macOS cu API-uri Python, C++ și C
- **Accelerare Hardware**: Descoperire automată a dispozitivelor și optimizare pentru CPU, GPU, VPU și acceleratoare AI
- **Framework de Compresie a Rețelelor Neurale**: Tehnici avansate de cuantizare, reducere și optimizare prin NNCF
- **Compatibilitate cu Framework-uri**: Suport direct pentru modele TensorFlow, ONNX, PaddlePaddle și PyTorch
- **Suport pentru AI Generativ**: OpenVINO GenAI specializat pentru implementarea modelelor de limbaj mari și aplicații AI generative

### Beneficii

- **Optimizare a Performanței**: Îmbunătățiri semnificative ale vitezei cu pierderi minime de acuratețe
- **Amprentă de Implementare Redusă**: Dependențe externe minime simplifică instalarea și implementarea
- **Timp de Pornire Îmbunătățit**: Încărcare și caching optimizate ale modelelor pentru inițializare mai rapidă a aplicațiilor
- **Implementare Scalabilă**: De la dispozitive edge la infrastructură cloud cu API-uri consistente
- **Pregătit pentru Producție**: Fiabilitate la nivel enterprise cu documentație cuprinzătoare și suport comunitar

## Instalare

### Cerințe Prealabile

- Python 3.8 sau mai recent
- Manager de pachete pip
- Mediu virtual (recomandat)
- Hardware compatibil (CPU-uri Intel recomandate, dar suportă diverse arhitecturi)

### Instalare de Bază

Creați și activați un mediu virtual:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Instalați OpenVINO Runtime:

```bash
pip install openvino
```

Instalați NNCF pentru optimizarea modelelor:

```bash
pip install nncf
```

### Instalare OpenVINO GenAI

Pentru aplicații AI generative:

```bash
pip install openvino-genai
```

### Dependențe Opționale

Pachete suplimentare pentru cazuri de utilizare specifice:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Verificarea Instalării

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Dacă instalarea este reușită, ar trebui să vedeți informațiile despre versiunea OpenVINO.

## Ghid de Start Rapid

### Prima Optimizare a Modelului

Să convertim și să optimizăm un model Hugging Face folosind OpenVINO:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Ce Face Acest Proces

Fluxul de lucru de optimizare implică: încărcarea modelului original de la Hugging Face, conversia în formatul Intermediate Representation (IR) OpenVINO, aplicarea optimizărilor implicite și compilarea pentru hardware-ul țintă.

### Parametrii Cheie Explicați

- `export=True`: Convertește modelul în formatul IR OpenVINO
- `compile=False`: Amână compilarea până la rulare pentru flexibilitate
- `device`: Hardware-ul țintă ("CPU", "GPU", "AUTO" pentru selecție automată)
- `save_pretrained()`: Salvează modelul optimizat pentru reutilizare

## Exemplu: Conversia și Optimizarea Modelelor cu OpenVINO

### Pasul 1: Conversia Modelului cu Cuantizare NNCF

Iată cum să aplicați cuantizarea post-antrenament folosind NNCF:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Pasul 2: Optimizare Avansată cu Compresia Greutăților

Pentru modele bazate pe transformatoare, aplicați compresia greutăților:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Pasul 3: Inferență cu Modelul Optimizat

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Structura Output-ului

După optimizare, directorul modelului va conține:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Utilizare Avansată

### Configurare cu NNCF YAML

Pentru fluxuri de lucru complexe de optimizare, utilizați fișiere de configurare NNCF:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Aplicați configurația:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### Optimizare GPU

Pentru accelerare GPU:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Optimizare Procesare în Loturi

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Implementare Server de Modele

Implementați modele optimizate cu OpenVINO Model Server:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Cod client pentru serverul de modele:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Cele Mai Bune Practici

### 1. Selectarea și Pregătirea Modelului
- Utilizați modele din framework-uri suportate (PyTorch, TensorFlow, ONNX)
- Asigurați-vă că intrările modelului au forme fixe sau dinamice cunoscute
- Testați cu seturi de date reprezentative pentru calibrare

### 2. Selectarea Strategiei de Optimizare
- **Cuantizare Post-Antrenament**: Începeți aici pentru optimizare rapidă
- **Compresia Greutăților**: Ideal pentru modele de limbaj mari și transformatoare
- **Antrenament Conștient de Cuantizare**: Utilizați când acuratețea este critică

### 3. Optimizare Specifică Hardware-ului
- **CPU**: Utilizați cuantizarea INT8 pentru performanță echilibrată
- **GPU**: Profitați de precizia FP16 și procesarea în loturi
- **VPU**: Concentrați-vă pe simplificarea modelului și fuziunea straturilor

### 4. Reglarea Performanței
- **Modul Throughput**: Pentru procesarea în loturi de volum mare
- **Modul Latency**: Pentru aplicații interactive în timp real
- **Dispozitiv AUTO**: Lăsați OpenVINO să selecteze hardware-ul optim

### 5. Gestionarea Memoriei
- Utilizați forme dinamice cu prudență pentru a evita supraîncărcarea memoriei
- Implementați caching-ul modelului pentru încărcări ulterioare mai rapide
- Monitorizați utilizarea memoriei în timpul optimizării

### 6. Validarea Acurateței
- Validați întotdeauna modelele optimizate față de performanța originală
- Utilizați seturi de date de test reprezentative pentru evaluare
- Luați în considerare optimizarea graduală (începeți cu setări conservatoare)

## Depanare

### Probleme Comune

#### 1. Probleme de Instalare
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Erori de Conversie a Modelului
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Probleme de Performanță
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Probleme de Memorie
- Reduceți dimensiunea lotului modelului în timpul optimizării
- Utilizați streaming pentru seturi de date mari
- Activați caching-ul modelului: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Degradarea Acurateței
- Utilizați o precizie mai mare (INT8 în loc de INT4)
- Creșteți dimensiunea setului de date de calibrare
- Aplicați optimizarea cu precizie mixtă

### Monitorizarea Performanței

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Obținerea Ajutorului

- **Documentație**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **Probleme GitHub**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Forum Comunitar**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Resurse Suplimentare

### Link-uri Oficiale
- **Pagina Principală OpenVINO**: [openvino.ai](https://openvino.ai/)
- **Repository GitHub**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **Repository NNCF**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Resurse de Învățare
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Ghid de Start Rapid**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Ghid de Optimizare**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Instrumente de Integrare
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Benchmarks de Performanță
- **Benchmarks Oficiale**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Exemple Comunitare
- **Jupyter Notebooks**: [Repository OpenVINO Notebooks](https://github.com/openvinotoolkit/openvino_notebooks) - Tutoriale cuprinzătoare disponibile în repository-ul OpenVINO notebooks
- **Aplicații de Exemplu**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Exemple din lumea reală pentru diverse domenii (computer vision, NLP, audio)
- **Postări pe Blog**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Postări pe blogul Intel AI și comunitar cu cazuri de utilizare detaliate

### Instrumente Asemănătoare
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Tehnici suplimentare de optimizare pentru hardware-ul Intel
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Pentru comparații de implementare pe mobil și edge
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Alternative pentru motorul de inferență cross-platform

## ➡️ Ce urmează

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.