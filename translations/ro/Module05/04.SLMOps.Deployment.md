<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-18T19:03:00+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "ro"
}
-->
# Secțiunea 4: Implementarea modelului pregătit pentru producție

## Prezentare generală

Acest tutorial cuprinzător te va ghida prin procesul complet de implementare a modelelor cuantificate ajustate folosind Foundry Local. Vom acoperi conversia modelului, optimizarea prin cuantificare și configurarea implementării de la început până la sfârșit.

## Cerințe preliminare

Înainte de a începe, asigură-te că ai următoarele:

- ✅ Un model onnx ajustat, pregătit pentru implementare
- ✅ Computer Windows sau Mac
- ✅ Python 3.10 sau o versiune mai nouă
- ✅ Cel puțin 8GB RAM disponibil
- ✅ Foundry Local instalat pe sistemul tău

## Partea 1: Configurarea mediului

### Instalarea instrumentelor necesare

Deschide terminalul (Command Prompt pe Windows, Terminal pe Mac) și rulează următoarele comenzi în ordine:

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **Notă importantă**: Vei avea nevoie și de CMake versiunea 3.31 sau mai nouă, care poate fi descărcat de la [cmake.org](https://cmake.org/download/).

## Partea 2: Conversia și cuantificarea modelului

### Alegerea formatului potrivit

Pentru modelele mici de limbaj ajustate, recomandăm utilizarea formatului **ONNX**, deoarece oferă:

- 🚀 Optimizare mai bună a performanței
- 🔧 Implementare independentă de hardware
- 🏭 Capacități pregătite pentru producție
- 📱 Compatibilitate cross-platform

### Metoda 1: Conversie cu o singură comandă (Recomandată)

Folosește următoarea comandă pentru a converti direct modelul ajustat:

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Explicația parametrilor:**
- `--model_name_or_path`: Calea către modelul ajustat
- `--device cpu`: Utilizează CPU pentru optimizare
- `--precision int4`: Utilizează cuantificarea INT4 (reducere de aproximativ 75% a dimensiunii)
- `--output_path`: Calea de ieșire pentru modelul convertit

### Metoda 2: Abordare prin fișier de configurare (Utilizatori avansați)

Creează un fișier de configurare numit `finetuned_conversion_config.json`:

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

Apoi rulează:

```bash
olive run --config ./finetuned_conversion_config.json
```

### Compararea opțiunilor de cuantificare

| Precizie | Dimensiunea fișierului | Viteza de inferență | Calitatea modelului | Utilizare recomandată |
|----------|-------------------------|---------------------|---------------------|-----------------------|
| FP16     | Bază × 0.5             | Rapid               | Cea mai bună       | Hardware de înaltă performanță |
| INT8     | Bază × 0.25            | Foarte rapid        | Bună               | Alegere echilibrată |
| INT4     | Bază × 0.125           | Cea mai rapidă      | Acceptabilă        | Resurse limitate |

💡 **Recomandare**: Începe cu cuantificarea INT4 pentru prima implementare. Dacă calitatea nu este satisfăcătoare, încearcă INT8 sau FP16.

## Partea 3: Configurarea implementării Foundry Local

### Crearea configurației modelului

Navighează la directorul de modele Foundry Local:

```bash
foundry cache cd ./models/
```

Creează structura directorului modelului:

```bash
mkdir -p ./models/custom/your-finetuned-model
```

Creează fișierul de configurare `inference_model.json` în directorul modelului:

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### Configurații șablon specifice modelului

#### Pentru modelele din seria Qwen:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## Partea 4: Testarea și optimizarea modelului

### Verificarea instalării modelului

Verifică dacă Foundry Local poate recunoaște modelul tău:

```bash
foundry cache ls
```

Ar trebui să vezi `your-finetuned-model-int4` în listă.

### Începerea testării modelului

```bash
foundry model run your-finetuned-model-int4
```

### Evaluarea performanței

Monitorizează metricile cheie în timpul testării:

1. **Timp de răspuns**: Măsoară timpul mediu per răspuns
2. **Utilizarea memoriei**: Monitorizează consumul de RAM
3. **Utilizarea CPU**: Verifică încărcarea procesorului
4. **Calitatea ieșirii**: Evaluează relevanța și coerența răspunsurilor

### Lista de verificare pentru validarea calității

- ✅ Modelul răspunde corespunzător la interogările din domeniul ajustat
- ✅ Formatul răspunsului se potrivește cu structura de ieșire așteptată
- ✅ Nu există scurgeri de memorie în timpul utilizării prelungite
- ✅ Performanță consistentă pentru diferite lungimi de intrare
- ✅ Gestionare corespunzătoare a cazurilor limită și a intrărilor invalide

## Rezumat

Felicitări! Ai finalizat cu succes:

- ✅ Conversia formatului modelului ajustat
- ✅ Optimizarea prin cuantificare a modelului
- ✅ Configurarea implementării Foundry Local
- ✅ Reglarea performanței și depanarea

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.