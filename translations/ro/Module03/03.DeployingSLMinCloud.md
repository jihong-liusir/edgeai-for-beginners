<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-09-18T19:09:01+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "ro"
}
-->
# Implementarea în Cloud Containerizat - Soluții la Scară de Producție

Acest tutorial cuprinzător acoperă trei abordări majore pentru implementarea modelului Phi-4-mini-instruct de la Microsoft în medii containerizate: vLLM, Ollama și SLM Engine cu ONNX Runtime. Acest model cu 3.8 miliarde de parametri reprezintă o alegere optimă pentru sarcini de raționament, menținând eficiența pentru implementarea la marginea rețelei.

## Cuprins

1. [Introducere în Implementarea Containerizată a Phi-4-mini](../../../Module03)
2. [Obiective de Învățare](../../../Module03)
3. [Înțelegerea Clasificării Phi-4-mini](../../../Module03)
4. [Implementarea Containerului vLLM](../../../Module03)
5. [Implementarea Containerului Ollama](../../../Module03)
6. [SLM Engine cu ONNX Runtime](../../../Module03)
7. [Cadru de Comparare](../../../Module03)
8. [Cele Mai Bune Practici](../../../Module03)

## Introducere în Implementarea Containerizată a Phi-4-mini

Modelele de Limbaj Mic (SLM) reprezintă un avans crucial în EdgeAI, oferind capabilități sofisticate de procesare a limbajului natural pe dispozitive cu resurse limitate. Acest tutorial se concentrează pe strategiile de implementare containerizată pentru Phi-4-mini-instruct de la Microsoft, un model de raționament de ultimă generație care echilibrează capabilitatea cu eficiența.

### Modelul Prezentat: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8 miliarde de parametri)**: Cel mai recent model ușor ajustat pentru instrucțiuni de la Microsoft, conceput pentru medii cu memorie/calcul limitat, cu capabilități excepționale în:
- **Raționament matematic și calcule complexe**
- **Generare de cod, depanare și analiză**
- **Rezolvarea logică a problemelor și raționament pas cu pas**
- **Aplicații educaționale care necesită explicații detaliate**
- **Apelarea funcțiilor și integrarea instrumentelor**

Parte din categoria "SLM-uri mici" (1.5B - 13.9B parametri), Phi-4-mini oferă un echilibru optim între capabilitatea de raționament și eficiența resurselor.

### Beneficii ale Implementării Containerizate a Phi-4-mini

- **Eficiență Operațională**: Inferență rapidă pentru sarcini de raționament cu cerințe computaționale reduse
- **Flexibilitate în Implementare**: Capabilități AI pe dispozitiv cu confidențialitate sporită prin procesare locală
- **Eficiență Costuri**: Costuri operaționale reduse comparativ cu modelele mai mari, menținând calitatea
- **Izolare**: Separare curată între instanțele modelului și medii de execuție securizate
- **Scalabilitate**: Scalare orizontală ușoară pentru creșterea capacității de raționament

## Obiective de Învățare

Până la finalul acestui tutorial, veți putea:

- Implementa și optimiza Phi-4-mini-instruct în diverse medii containerizate
- Aplica strategii avansate de cuantizare și compresie pentru diferite scenarii de implementare
- Configura orchestrarea containerelor pregătită pentru producție pentru sarcini de raționament
- Evalua și selecta cadrele de implementare adecvate în funcție de cerințele cazului de utilizare
- Aplica cele mai bune practici de securitate, monitorizare și scalare pentru implementările containerizate SLM

## Înțelegerea Clasificării Phi-4-mini

### Specificațiile Modelului

**Detalii Tehnice:**
- **Parametri**: 3.8 miliarde (categoria SLM mic)
- **Arhitectură**: Transformer dens, doar decodor, cu atenție pe grupuri de interogări
- **Lungimea Contextului**: 128K token-uri (32K recomandate pentru performanță optimă)
- **Vocabular**: 200K token-uri cu suport multilingv
- **Date de Antrenament**: 5T token-uri de conținut dens în raționament de înaltă calitate

### Cerințe de Resurse

| Tip de Implementare | RAM Min | RAM Recomandată | VRAM (GPU) | Stocare | Cazuri de Utilizare Tipice |
|---------------------|---------|-----------------|------------|---------|---------------------------|
| **Dezvoltare** | 6GB | 8GB | - | 8GB | Testare locală, prototipare |
| **Producție CPU** | 8GB | 12GB | - | 10GB | Servere edge, implementare optimizată pentru costuri |
| **Producție GPU** | 6GB | 8GB | 4-6GB | 8GB | Servicii de raționament cu debit mare |
| **Optimizat pentru Edge** | 4GB | 6GB | - | 6GB | Implementare cuantizată, gateway-uri IoT |

### Capabilitățile Phi-4-mini

- **Excelență Matematică**: Rezolvarea avansată a problemelor de aritmetică, algebră și calcul
- **Inteligență în Cod**: Generare de cod în Python, JavaScript și alte limbaje, cu depanare
- **Raționament Logic**: Decompoziția pas cu pas a problemelor și construirea soluțiilor
- **Suport Educațional**: Explicații detaliate potrivite pentru învățare și predare
- **Apelarea Funcțiilor**: Suport nativ pentru integrarea instrumentelor și interacțiuni API

## Implementarea Containerului vLLM

vLLM oferă suport excelent pentru Phi-4-mini-instruct, cu performanță optimizată de inferență și API-uri compatibile cu OpenAI, fiind ideal pentru servicii de raționament în producție.

### Exemple de Pornire Rapidă

#### Implementare de Bază pe CPU (Dezvoltare)
```bash
# CPU-optimized deployment for development and testing
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### Implementare Accelerată pe GPU pentru Producție
```bash
# GPU deployment for high-performance reasoning
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### Configurare pentru Producție

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testarea Capabilităților de Raționament ale Phi-4-mini

```bash
# Test mathematical reasoning
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# Test code generation
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# Test function calling capability
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Implementarea Containerului Ollama

Ollama oferă suport excelent pentru Phi-4-mini-instruct, cu implementare și gestionare simplificată, fiind ideal pentru dezvoltare și implementări echilibrate în producție.

### Configurare Rapidă

```bash
# Deploy Ollama container with GPU support
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Pull Phi-4-mini-instruct model
docker exec ollama-phi4 ollama pull phi4-mini

# Test mathematical reasoning
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# Test code generation
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### Configurare pentru Producție

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### Optimizarea Modelului și Variante

```bash
# Create reasoning-optimized variant
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# Create code-focused variant
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### Exemple de Utilizare API

```bash
# Mathematical reasoning via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# Code generation via API
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM Engine cu ONNX Runtime

ONNX Runtime oferă performanță optimă pentru implementarea la marginea rețelei a Phi-4-mini-instruct, cu optimizare avansată și compatibilitate cross-platform.

### Configurare de Bază

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### Implementare Simplificată a Serverului

```python
# app/server.py - Optimized for Phi-4-mini reasoning tasks
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # reasoning, coding, math
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # Optimized providers for reasoning tasks
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # Task-specific prompting for better reasoning
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # Tokenize and run inference
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # Decode response
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# Initialize engine
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### Script de Conversie a Modelului

```python
# convert_phi4_mini.py - Convert Phi-4-mini to optimized ONNX
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Convert to ONNX
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # Step 2: Apply optimizations for reasoning tasks
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # Step 3: Apply quantization for edge deployment
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # Step 4: Save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # Step 5: Create final optimized model
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### Configurare pentru Producție

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Testarea Implementării ONNX

```bash
# Test mathematical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# Test code generation
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# Test logical reasoning
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## Cadru de Comparare

### Compararea Cadrelor pentru Phi-4-mini

| Caracteristică | vLLM | Ollama | ONNX Runtime |
|----------------|------|--------|--------------|
| **Complexitatea Configurării** | Moderată | Ușoară | Complexă |
| **Performanță (GPU)** | Excelentă (~25 tok/s) | Foarte Bună (~20 tok/s) | Bună (~15 tok/s) |
| **Performanță (CPU)** | Bună (~8 tok/s) | Foarte Bună (~12 tok/s) | Excelentă (~15 tok/s) |
| **Utilizare Memorie** | 8-12GB | 6-10GB | 4-8GB |
| **Compatibilitate API** | Compatibil OpenAI | REST Personalizat | FastAPI Personalizat |
| **Apelarea Funcțiilor** | ✅ Nativ | ✅ Suportat | ⚠️ Implementare Personalizată |
| **Suport Cuantizare** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | Cuantizare ONNX |
| **Pregătit pentru Producție** | ✅ Excelent | ✅ Foarte Bun | ✅ Bun |
| **Implementare la Marginea Rețelei** | Bună | Excelentă | Remarcabilă |

## Resurse Suplimentare

### Documentație Oficială
- **Microsoft Phi-4 Model Card**: Specificații detaliate și ghiduri de utilizare
- **Documentație vLLM**: Opțiuni avansate de configurare și optimizare
- **Biblioteca de Modele Ollama**: Modele comunitare și exemple de personalizare
- **Ghiduri ONNX Runtime**: Strategii de optimizare a performanței și implementare

### Instrumente de Dezvoltare
- **Hugging Face Transformers**: Pentru interacțiunea și personalizarea modelului
- **Specificație API OpenAI**: Pentru testarea compatibilității vLLM
- **Cele Mai Bune Practici Docker**: Securitate și optimizare a containerelor
- **Implementare Kubernetes**: Modele de orchestrare pentru scalarea producției

### Resurse de Învățare
- **Benchmarking Performanță SLM**: Metodologii de analiză comparativă
- **Implementare AI la Marginea Rețelei**: Cele mai bune practici pentru medii cu resurse limitate
- **Optimizarea Sarcinilor de Raționament**: Strategii de prompting pentru probleme matematice și logice
- **Securitatea Containerelor**: Practici de întărire pentru implementările de modele AI

## Rezultate de Învățare

După finalizarea acestui modul, veți putea:

1. Implementa modelul Phi-4-mini-instruct în medii containerizate utilizând diverse cadre
2. Configura și optimiza implementările SLM pentru diferite medii hardware
3. Aplica cele mai bune practici de securitate pentru implementările AI containerizate
4. Compara și selecta cadrele de implementare adecvate în funcție de cerințele cazului de utilizare
5. Aplica strategii de monitorizare și scalare pentru servicii SLM de nivel producție

## Ce urmează

- Revenire la [Modulul 1](../Module01/README.md)
- Revenire la [Modulul 2](../Module02/README.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.