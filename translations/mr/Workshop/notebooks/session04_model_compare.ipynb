{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ba9a1",
   "metadata": {},
   "source": [
    "# рд╕рддреНрд░ рек тАУ SLM рд╡рд┐. LLM рддреБрд▓рдирд╛\n",
    "\n",
    "рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдЖрдгрд┐ рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓рдордзреАрд▓ рд╡рд┐рд▓рдВрдмрддрд╛ рдЖрдгрд┐ рдирдореБрдирд╛ рдкреНрд░рддрд┐рд╕рд╛рдж рдЧреБрдгрд╡рддреНрддреЗрдЪреА рддреБрд▓рдирд╛ Foundry Local рджреНрд╡рд╛рд░реЗ рдЪрд╛рд▓рд╡рддрд╛рдирд╛ рдХрд░рд╛.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9330f",
   "metadata": {},
   "source": [
    "## тЪб рдЬрд▓рдж рдкреНрд░рд╛рд░рдВрдн\n",
    "\n",
    "**рдореЗрдорд░реА-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реА рд╕реЗрдЯрдЕрдк (рдЕрдкрдбреЗрдЯреЗрдб):**\n",
    "1. рдореЙрдбреЗрд▓реНрд╕ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ CPU рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрддрд╛рдд (рдХреЛрдгрддреНрдпрд╛рд╣реА рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╡рд░ рдХрд╛рд░реНрдп рдХрд░рддреЗ)\n",
    "2. `qwen2.5-3b` рдЪрд╛ рд╡рд╛рдкрд░ 7B рдРрд╡рдЬреА (рд╕реБрдорд╛рд░реЗ ~4GB RAM рд╡рд╛рдЪрддреЗ)\n",
    "3. рдкреЛрд░реНрдЯ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╢реЛрдз (рдореЕрдиреНрдпреБрдЕрд▓ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдирдЪреА рдЧрд░рдЬ рдирд╛рд╣реА)\n",
    "4. рдЖрд╡рд╢реНрдпрдХ рдПрдХреВрдг RAM: ~8GB рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реЗрд▓реА (рдореЙрдбреЗрд▓реНрд╕ + OS)\n",
    "\n",
    "**рдЯрд░реНрдорд┐рдирд▓ рд╕реЗрдЯрдЕрдк (30 рд╕реЗрдХрдВрдж):**\n",
    "```bash\n",
    "foundry service start\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "рдордЧ рд╣реЗ рдиреЛрдЯрдмреБрдХ рдЪрд╛рд▓рд╡рд╛! ЁЯЪА\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941ea8c",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рдЕрд╡рд▓рдВрдмрд┐рддреНрд╡ рд╕реНрдерд╛рдкрдирд╛\n",
    "рд╡реЗрд│ рдЖрдгрд┐ рдЪреЕрдЯ рд╡рд┐рдирдВрддреНрдпрд╛рдВрд╕рд╛рдареА рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдХрд┐рдорд╛рди рдкреЕрдХреЗрдЬреЗрд╕ (`foundry-local-sdk`, `openai`, `numpy`) рд╕реНрдерд╛рдкрд┐рдд рдХрд░рддреЗ. рд╕реБрд░рдХреНрд╖рд┐рддрдкрдгреЗ рдкреБрдиреНрд╣рд╛ рдЪрд╛рд▓рд╡рддрд╛ рдпреЗрддреЗ, рдЖрдгрд┐ рдпрд╛рдореБрд│реЗ рдХреЛрдгрддреЗрд╣реА рджреБрдкреНрдкрдЯ рдкрд░рд┐рдгрд╛рдо рд╣реЛрдд рдирд╛рд╣реАрдд.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c7c7",
   "metadata": {},
   "source": [
    "# рдкрд░рд┐рд╕реНрдерд┐рддреА\n",
    "рдПрдХрд╛ рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ (SLM) рдЖрдгрд┐ рдПрдХрд╛ рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓рдЪреА рдПрдХрд╛ рд╕рд┐рдВрдЧрд▓ рдкреНрд░реЙрдореНрдкреНрдЯрд╡рд░ рддреБрд▓рдирд╛ рдХрд░рд╛, рдЬреЗрдгреЗрдХрд░реВрди рдЦрд╛рд▓реАрд▓ рдЧреЛрд╖реНрдЯреАрдВрдЪреЗ рдлрд╛рдпрджреЗ-рддреЛрдЯреЗ рд╕реНрдкрд╖реНрдЯ рд╣реЛрддреАрд▓:\n",
    "- **рдкреНрд░рддреАрдХреНрд╖рд╛ рд╡реЗрд│реЗрддреАрд▓ рдлрд░рдХ** (рднрд┐рдВрддреАрд╡рд░реАрд▓ рдШрдбреНрдпрд╛рд│рд╛рдЪреНрдпрд╛ рд╕реЗрдХрдВрджрд╛рдВрдордзреНрдпреЗ)\n",
    "- **рдЯреЛрдХрди рд╡рд╛рдкрд░** (рдЬрд░ рдЙрдкрд▓рдмреНрдз рдЕрд╕реЗрд▓) рдереНрд░реВрдкреБрдЯрд╕рд╛рдареА рдПрдХ рдкреНрд░реЙрдХреНрд╕реА рдореНрд╣рдгреВрди\n",
    "- **рдирдореБрдиреНрдпрд╛рдЪрд╛ рдЧреБрдгрд╛рддреНрдордХ рдЖрдЙрдЯрдкреБрдЯ** рдЬрд▓рдж рддрдкрд╛рд╕рдгреАрд╕рд╛рдареА\n",
    "- **рдЧрддреА рд╡рд╛рдвреАрдЪреА рдЧрдгрдирд╛** рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрддреАрд▓ рд╕реБрдзрд╛рд░рдгрд╛ рдореЛрдЬрдгреНрдпрд╛рд╕рд╛рдареА\n",
    "\n",
    "**рдкрд░реНрдпрд╛рд╡рд░рдгреАрдп рдШрдЯрдХ:**\n",
    "- `SLM_ALIAS` - рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ (рдбреАрдлреЙрд▓реНрдЯ: phi-4-mini, ~4GB RAM)\n",
    "- `LLM_ALIAS` - рдореЛрдареЗ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ (рдбреАрдлреЙрд▓реНрдЯ: qwen2.5-7b, ~7GB RAM)\n",
    "- `COMPARE_PROMPT` - рддреБрд▓рдирд╛ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдЪрд╛рдЪрдгреА рдкреНрд░реЙрдореНрдкреНрдЯ\n",
    "- `COMPARE_RETRIES` - рдЯрд┐рдХрд╛рд╡рд╛рд╕рд╛рдареА рдкреБрдиреНрд╣рд╛ рдкреНрд░рдпрддреНрди (рдбреАрдлреЙрд▓реНрдЯ: 2)\n",
    "- `FOUNDRY_LOCAL_ENDPOINT` - рд╕реЗрд╡рд╛ рдПрдВрдбрдкреЙрдЗрдВрдЯ рдУрд╡реНрд╣рд░рд░рд╛рдЗрдб рдХрд░рд╛ (рд╕реЗрдЯ рдХреЗрд▓реЗ рдирд╕рд▓реНрдпрд╛рд╕ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд╢реЛрдзрд▓реЗ рдЬрд╛рддреЗ)\n",
    "\n",
    "**рд╣реЗ рдХрд╕реЗ рдХрд╛рд░реНрдп рдХрд░рддреЗ (рдЕрдзрд┐рдХреГрдд SDK рдкреЕрдЯрд░реНрди):**\n",
    "1. **FoundryLocalManager** Foundry Local рд╕реЗрд╡реЗрд▓рд╛ рдкреНрд░рд╛рд░рдВрдн рдХрд░рддреЛ рдЖрдгрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрд┐рдд рдХрд░рддреЛ\n",
    "2. рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╕реЗрд▓ рддрд░ рддреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд╕реБрд░реВ рд╣реЛрддреЗ (рдореЕрдиреНрдпреБрдЕрд▓ рд╕реЗрдЯрдЕрдкрдЪреА рдЧрд░рдЬ рдирд╛рд╣реА)\n",
    "3. рдореЙрдбреЗрд▓реНрд╕ рдЕреЕрд▓рд┐рдпрд╛рд╕рдордзреВрди рдареЛрд╕ рдЖрдпрдбреАрдордзреНрдпреЗ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд░реВрдкрд╛рдВрддрд░рд┐рдд рд╣реЛрддрд╛рдд\n",
    "4. рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭреНрдб рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрд▓реЗ рдЬрд╛рддрд╛рдд (CUDA, NPU, рдХрд┐рдВрд╡рд╛ CPU)\n",
    "5. OpenAI-рд╕реБрд╕рдВрдЧрдд рдХреНрд▓рд╛рдпрдВрдЯ рдЪреЕрдЯ рдкреВрд░реНрдгрддрд╛ рдХрд░рддреЛ\n",
    "6. рдореЗрдЯреНрд░рд┐рдХреНрд╕ рдХреЕрдкреНрдЪрд░ рдХреЗрд▓реЗ рдЬрд╛рддрд╛рдд: рдкреНрд░рддреАрдХреНрд╖рд╛ рд╡реЗрд│, рдЯреЛрдХрдиреНрд╕, рдЖрдЙрдЯрдкреБрдЯ рдЧреБрдгрд╡рддреНрддрд╛\n",
    "7. рдирд┐рдХрд╛рд▓рд╛рдВрдЪреА рддреБрд▓рдирд╛ рдХрд░реВрди рдЧрддреА рд╡рд╛рдвреАрдЪрд╛ рдЧреБрдгреЛрддреНрддрд░ рдХрд╛рдврд▓рд╛ рдЬрд╛рддреЛ\n",
    "\n",
    "рд╣реА рд╕реВрдХреНрд╖реНрдо рддреБрд▓рдирд╛ рддреБрдордЪреНрдпрд╛ рд╡рд╛рдкрд░рд╛рдЪреНрдпрд╛ рдкреНрд░рдХрд░рдгрд╛рд╕рд╛рдареА рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓рдХрдбреЗ рд╡рд│рдгреЗ рдХрдзреА рдпреЛрдЧреНрдп рдЖрд╣реЗ рд╣реЗ рдард░рд╡рд┐рдгреНрдпрд╛рд╕ рдорджрдд рдХрд░рддреЗ.\n",
    "\n",
    "**SDK рд╕рдВрджрд░реНрдн:** \n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- рд╡рд░реНрдХрд╢реЙрдк рдпреБрдЯрд┐рд▓реНрд╕: ../samples/workshop_utils.py рдордзреАрд▓ рдЕрдзрд┐рдХреГрдд рдкреЕрдЯрд░реНрди рд╡рд╛рдкрд░рддреЛ\n",
    "\n",
    "**рдореБрдЦреНрдп рдлрд╛рдпрджреЗ:**\n",
    "- тЬЕ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╕реЗрд╡рд╛ рд╢реЛрдз рдЖрдгрд┐ рдкреНрд░рд╛рд░рдВрдн\n",
    "- тЬЕ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╕реЗрд▓ рддрд░ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд╕реБрд░реВ рд╣реЛрддреЗ\n",
    "- тЬЕ рдЕрдВрдЧрднреВрдд рдореЙрдбреЗрд▓ рд░рд┐рдЭреЛрд▓реНрдпреВрд╢рди рдЖрдгрд┐ рдХреЕрд╢рд┐рдВрдЧ\n",
    "- тЬЕ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди (CUDA/NPU/CPU)\n",
    "- тЬЕ OpenAI SDK рд╕реБрд╕рдВрдЧрддрддрд╛\n",
    "- тЬЕ рдкреБрдиреНрд╣рд╛ рдкреНрд░рдпрддреНрдирд╛рдВрд╕рд╣ рдордЬрдмреВрдд рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА\n",
    "- тЬЕ рд╕реНрдерд╛рдирд┐рдХ рдЕрдиреБрдорд╛рди (рдХреНрд▓рд╛рдЙрдб API рдЪреА рдЧрд░рдЬ рдирд╛рд╣реА)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4813a6",
   "metadata": {},
   "source": [
    "## ЁЯЪи рдкреВрд░реНрд╡рдЕрдЯ: Foundry Local рдЪрд╛рд▓реВ рдЕрд╕рдгреЗ рдЖрд╡рд╢реНрдпрдХ рдЖрд╣реЗ!\n",
    "\n",
    "**рд╣рд╛ рдиреЛрдЯрдмреБрдХ рдЪрд╛рд▓рд╡рдгреНрдпрд╛рдкреВрд░реНрд╡реА**, Foundry Local рд╕реЗрд╡рд╛ рд╕реЗрдЯрдЕрдк рдХреЗрд▓реЗрд▓реА рдЕрд╕рд╛рд╡реА:\n",
    "\n",
    "### рдЬрд▓рдж рдкреНрд░рд╛рд░рдВрдн рдЖрджреЗрд╢ (рдЯрд░реНрдорд┐рдирд▓рдордзреНрдпреЗ рдЪрд╛рд▓рд╡рд╛):\n",
    "\n",
    "```bash\n",
    "# 1. Start the Foundry Local service\n",
    "foundry service start\n",
    "\n",
    "# 2. Load the default models used in this comparison (CPU-optimized)\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-3b\n",
    "\n",
    "# 3. Verify models are loaded\n",
    "foundry model ls\n",
    "\n",
    "# 4. Check service health\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "### рдкрд░реНрдпрд╛рдпреА рдореЙрдбреЗрд▓реНрд╕ (рдЬрд░ рдбреАрдлреЙрд▓реНрдЯ рдЙрдкрд▓рдмреНрдз рдирд╕рддреАрд▓):\n",
    "\n",
    "```bash\n",
    "# Even smaller alternatives (if memory is very limited)\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-0.5b\n",
    "\n",
    "# Or update the environment variables in this notebook:\n",
    "# SLM_ALIAS = 'phi-3.5-mini'\n",
    "# LLM_ALIAS = 'qwen2.5-1.5b'  # Or qwen2.5-0.5b for minimum memory\n",
    "```\n",
    "\n",
    "тЪая╕П **рдЬрд░ рддреБрдореНрд╣реА рд╣реЗ рдкрд╛рдпрд▒реНрдпрд╛ рд╡рдЧрд│рд▓реНрдпрд╛**, рддрд░ рдЦрд╛рд▓реАрд▓ рдиреЛрдЯрдмреБрдХ рд╕реЗрд▓реНрд╕ рдЪрд╛рд▓рд╡рддрд╛рдирд╛ рддреБрдореНрд╣рд╛рд▓рд╛ `APIConnectionError` рджрд┐рд╕реЗрд▓.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d2ae1",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рдореБрдЦреНрдп рдЖрдпрд╛рдд\n",
    "рд╡реЗрд│ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рд╕рд╛рдзрдиреЗ рдЖрдгрд┐ Foundry Local / OpenAI рдХреНрд▓рд╛рдпрдВрдЯреНрд╕ рдЖрдгрддреЗ, рдЬреЗ рдореЙрдбреЗрд▓ рдорд╛рд╣рд┐рддреА рдорд┐рд│рд╡рдгреНрдпрд╛рд╕рд╛рдареА рдЖрдгрд┐ рдЪреЕрдЯ рдкреВрд░реНрдгрддрд╛ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╡рд╛рдкрд░рд▓реЗ рдЬрд╛рддрд╛рдд.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1233c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "sys.path.append('../samples')\n",
    "from workshop_utils import get_client, chat_once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6aa79",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рдЙрдкрдирд╛рдореЗ рдЖрдгрд┐ рдкреНрд░реЙрдореНрдкреНрдЯ рд╕реЗрдЯрдЕрдк  \n",
    "рд▓рд╣рд╛рди рдЖрдгрд┐ рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓рд╕рд╛рдареА рдкрд░реНрдпрд╛рд╡рд░рдгрд╛рдиреБрд╕рд╛рд░ рд╕рд╛рдиреБрдХреВрд▓рд┐рдд рд╣реЛрдгрд╛рд░реЗ рдЙрдкрдирд╛рдо рдкрд░рд┐рднрд╛рд╖рд┐рдд рдХрд░рддреЛ рддрд╕реЗрдЪ рддреБрд▓рдирд╛ рдкреНрд░реЙрдореНрдкреНрдЯ рджреЗрдЦреАрд▓. рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдореЙрдбреЗрд▓ рдХреБрдЯреБрдВрдмреЗ рдХрд┐рдВрд╡рд╛ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдкреНрд░рдпреЛрдЧ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдкрд░реНрдпрд╛рд╡рд░рдгреАрдп рдЪрд▓ (env vars) рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░рд╛.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU models for better memory efficiency\n",
    "SLM = os.getenv('SLM_ALIAS', 'phi-4-mini')  # Auto-selects CPU variant\n",
    "LLM = os.getenv('LLM_ALIAS', 'qwen2.5-3b')  # Smaller LLM, more memory-friendly\n",
    "PROMPT = os.getenv('COMPARE_PROMPT', 'List 5 benefits of local AI inference.')\n",
    "# Endpoint is now managed by FoundryLocalManager - it auto-detects or can be overridden\n",
    "ENDPOINT = os.getenv('FOUNDRY_LOCAL_ENDPOINT', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29375d5",
   "metadata": {},
   "source": [
    "### ЁЯТб рдореЗрдорд░реА-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭреНрдб рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди\n",
    "\n",
    "**рд╣реЗ рдиреЛрдЯрдмреБрдХ рдбрд┐рдлреЙрд▓реНрдЯрдиреЗ рдореЗрдорд░реА-рдЗрдлрд┐рд╢рд┐рдпрдВрдЯ рдореЙрдбреЗрд▓реНрд╕ рд╡рд╛рдкрд░рддреЗ:**\n",
    "- `phi-4-mini` тЖТ ~4GB RAM (Foundry Local рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ CPU рд╡реНрд╣реЗрд░рд┐рдПрдВрдЯ рдирд┐рд╡рдбрддреЗ)\n",
    "- `qwen2.5-3b` тЖТ ~3GB RAM (7B рдРрд╡рдЬреА рдЬреНрдпрд╛рд▓рд╛ ~7GB+ рд▓рд╛рдЧрддрд╛рдд)\n",
    "\n",
    "**рдкреЛрд░реНрдЯ рдСрдЯреЛ-рдбрд┐рдЯреЗрдХреНрд╢рди:**\n",
    "- Foundry Local рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдкреЛрд░реНрдЯреНрд╕ рд╡рд╛рдкрд░реВ рд╢рдХрддреЗ (рд╕рд╛рдорд╛рдиреНрдпрддрдГ 55769 рдХрд┐рдВрд╡рд╛ 59959)\n",
    "- рдЦрд╛рд▓реАрд▓ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓ рдпреЛрдЧреНрдп рдкреЛрд░реНрдЯ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд╢реЛрдзрддреЛ\n",
    "- рдХреЛрдгрддреНрдпрд╛рд╣реА рдкреНрд░рдХрд╛рд░рдЪреА рдореЕрдиреНрдпреБрдЕрд▓ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдЖрд╡рд╢реНрдпрдХ рдирд╛рд╣реА!\n",
    "\n",
    "**рдЬрд░ рддреБрдордЪреНрдпрд╛рдХрдбреЗ рдорд░реНрдпрд╛рджрд┐рдд RAM (<8GB) рдЕрд╕реЗрд▓, рддрд░ рдЖрдгрдЦреА рдЫреЛрдЯреЗ рдореЙрдбреЗрд▓реНрд╕ рд╡рд╛рдкрд░рд╛:**\n",
    "```python\n",
    "SLM = 'phi-3.5-mini'      # ~2GB\n",
    "LLM = 'qwen2.5-0.5b'      # ~500MB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c17fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CURRENT CONFIGURATION\n",
      "============================================================\n",
      "SLM Model:     phi-4-mini\n",
      "LLM Model:     qwen2.5-7b\n",
      "SDK Pattern:   FoundryLocalManager (official)\n",
      "Endpoint:      Auto-detect\n",
      "Test Prompt:   List 5 benefits of local AI inference....\n",
      "Retry Count:   2\n",
      "============================================================\n",
      "\n",
      "ЁЯТб Using official Foundry SDK pattern from workshop_utils\n",
      "   тЖТ FoundryLocalManager handles service lifecycle\n",
      "   тЖТ Automatic model resolution and hardware optimization\n",
      "   тЖТ OpenAI-compatible API for inference\n"
     ]
    }
   ],
   "source": [
    "# Display current configuration\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SLM Model:     {SLM}\")\n",
    "print(f\"LLM Model:     {LLM}\")\n",
    "print(f\"SDK Pattern:   FoundryLocalManager (official)\")\n",
    "print(f\"Endpoint:      {ENDPOINT or 'Auto-detect'}\")\n",
    "print(f\"Test Prompt:   {PROMPT[:50]}...\")\n",
    "print(f\"Retry Count:   2\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nЁЯТб Using official Foundry SDK pattern from workshop_utils\")\n",
    "print(\"   тЖТ FoundryLocalManager handles service lifecycle\")\n",
    "print(\"   тЖТ Automatic model resolution and hardware optimization\")\n",
    "print(\"   тЖТ OpenAI-compatible API for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df58",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рдХрд╛рд░реНрдпрд╛рдиреНрд╡рдпрди рд╕рд╣рд╛рдпреНрдпрдХ (Foundry SDK рдкреЕрдЯрд░реНрди)\n",
    "рд╡рд░реНрдХрд╢реЙрдк рдирдореБрдиреНрдпрд╛рдВрдордзреНрдпреЗ рджрд╕реНрддрдРрд╡рдЬреАрдХрд░рдг рдХреЗрд▓реЗрд▓реНрдпрд╛ рдЕрдзрд┐рдХреГрдд Foundry Local SDK рдкреЕрдЯрд░реНрдирдЪрд╛ рд╡рд╛рдкрд░:\n",
    "\n",
    "**рдкрджреНрдзрдд:**\n",
    "- **FoundryLocalManager** - Foundry Local рд╕реЗрд╡рд╛ рд╕реБрд░реВ рдХрд░рддреЗ рдЖрдгрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрд┐рдд рдХрд░рддреЗ\n",
    "- **рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╢реЛрдз** - рдПрдВрдбрдкреЙрдЗрдВрдЯ рдЖрдкреЛрдЖрдк рд╢реЛрдзрддреЛ рдЖрдгрд┐ рд╕реЗрд╡рд╛ рдЬреАрд╡рдирдЪрдХреНрд░ рд╣рд╛рддрд╛рд│рддреЛ\n",
    "- **рдореЙрдбреЗрд▓ рд░рд┐рдЭреЛрд▓реНрдпреВрд╢рди** - рдЙрдкрдирд╛рдорд╛рдВрдирд╛ рдкреВрд░реНрдг рдореЙрдбреЗрд▓ рдЖрдпрдбреАрдордзреНрдпреЗ рд░реВрдкрд╛рдВрддрд░рд┐рдд рдХрд░рддреЗ (рдЙрджрд╛., phi-4-mini тЖТ phi-4-mini-instruct-cpu)\n",
    "- **рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди** - рдЙрдкрд▓рдмреНрдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╕рд╛рдареА рд╕рд░реНрд╡реЛрддреНрддрдо рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрддреЛ (CUDA, NPU, рдХрд┐рдВрд╡рд╛ CPU)\n",
    "- **OpenAI рдХреНрд▓рд╛рдпрдВрдЯ** - OpenAI-рд╕реБрд╕рдВрдЧрдд API рдкреНрд░рд╡реЗрд╢рд╛рд╕рд╛рдареА рд╡реНрдпрд╡рд╕реНрдерд╛рдкрдХрд╛рдЪреНрдпрд╛ рдПрдВрдбрдкреЙрдЗрдВрдЯрд╕рд╣ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХреЗрд▓реЗрд▓реЗ\n",
    "\n",
    "**рд╕рд╣рдирд╢реАрд▓рддрд╛ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ:**\n",
    "- рдПрдХреНрд╕реНрдкреЛрдиреЗрдВрд╢рд┐рдпрд▓ рдмреЕрдХрдСрдл рд░реАрдЯреНрд░рд╛рдп рд▓реЙрдЬрд┐рдХ (рдкрд░реНрдпрд╛рд╡рд░рдгрд╛рджреНрд╡рд╛рд░реЗ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рддрд╛ рдпреЗрдгреНрдпрд╛рдЬреЛрдЧреЗ)\n",
    "- рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╕рд▓реНрдпрд╛рд╕ рдЖрдкреЛрдЖрдк рд╕реБрд░реВ рд╣реЛрддреЗ\n",
    "- рдкреНрд░рд╛рд░рдВрднрд╛рдирдВрддрд░ рдХрдиреЗрдХреНрд╢рди рд╕рддреНрдпрд╛рдкрди\n",
    "- рддрдкрд╢реАрд▓рд╡рд╛рд░ рддреНрд░реБрдЯреА рдЕрд╣рд╡рд╛рд▓рд╛рд╕рд╣ рд╕реМрдореНрдп рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА\n",
    "- рд╡рд╛рд░рдВрд╡рд╛рд░ рдкреНрд░рд╛рд░рдВрдн рдЯрд╛рд│рдгреНрдпрд╛рд╕рд╛рдареА рдореЙрдбреЗрд▓ рдХреЕрд╢рд┐рдВрдЧ\n",
    "\n",
    "**рдкрд░рд┐рдгрд╛рдо рд╕рдВрд░рдЪрдирд╛:**\n",
    "- рд╡рд┐рд▓рдВрдм рдореЛрдЬрдорд╛рдк (рд╡реЙрд▓ рдХреНрд▓реЙрдХ рд╡реЗрд│)\n",
    "- рдЯреЛрдХрди рд╡рд╛рдкрд░ рдЯреНрд░реЕрдХрд┐рдВрдЧ (рдЬрд░ рдЙрдкрд▓рдмреНрдз рдЕрд╕реЗрд▓)\n",
    "- рдирдореБрдирд╛ рдЖрдЙрдЯрдкреБрдЯ (рд╡рд╛рдЪрдиреАрдпрддреЗрд╕рд╛рдареА рд╕рдВрдХреНрд╖рд┐рдкреНрдд)\n",
    "- рдЕрдкрдпрд╢реА рд╡рд┐рдирдВрддреНрдпрд╛рдВрд╕рд╛рдареА рддреНрд░реБрдЯреА рддрдкрд╢реАрд▓\n",
    "\n",
    "рдпрд╛ рдкреЕрдЯрд░реНрдирдордзреНрдпреЗ workshop_utils рдореЙрдбреНрдпреВрд▓рдЪрд╛ рдЙрдкрдпреЛрдЧ рдХреЗрд▓рд╛ рдЬрд╛рддреЛ, рдЬреЛ рдЕрдзрд┐рдХреГрдд SDK рдкреЕрдЯрд░реНрдирдЪреЗ рдЕрдиреБрд╕рд░рдг рдХрд░рддреЛ.\n",
    "\n",
    "**SDK рд╕рдВрджрд░реНрдн:**\n",
    "- рдореБрдЦреНрдп рд░реЗрдкреЛ: https://github.com/microsoft/Foundry-Local\n",
    "- Python SDK: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "- рд╡рд░реНрдХрд╢реЙрдк рдпреБрдЯрд┐рд▓реНрд╕: ../samples/workshop_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e646fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Execution helpers defined: setup(), run()\n",
      "   тЖТ Uses workshop_utils for proper SDK integration\n",
      "   тЖТ setup() initializes with FoundryLocalManager\n",
      "   тЖТ run() executes inference via OpenAI-compatible API\n",
      "   тЖТ Token counting: Uses API data or estimates if unavailable\n"
     ]
    }
   ],
   "source": [
    "def setup(alias: str, endpoint: str = None, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Initialize a Foundry Local model connection using official SDK pattern.\n",
    "    \n",
    "    This follows the workshop_utils pattern which uses FoundryLocalManager\n",
    "    to properly initialize the Foundry Local service and resolve models.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias (e.g., 'phi-4-mini', 'qwen2.5-3b')\n",
    "        endpoint: Optional endpoint override (usually auto-detected)\n",
    "        retries: Number of connection attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (manager, client, model_id, metadata) or (None, None, alias, error_metadata) if failed\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    last_err = None\n",
    "    current_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Connecting to '{alias}' (attempt {attempt}/{retries})...\")\n",
    "            \n",
    "            # Use the workshop utility which follows the official SDK pattern\n",
    "            manager, client, model_id = get_client(alias, endpoint=endpoint)\n",
    "            \n",
    "            print(f\"[OK] Connected to '{alias}' -> {model_id}\")\n",
    "            print(f\"     Endpoint: {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, {\n",
    "                'endpoint': manager.endpoint,\n",
    "                'resolved': model_id,\n",
    "                'attempts': attempt,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Provide helpful error messages\n",
    "            if \"Connection error\" in error_msg or \"connection refused\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Cannot connect to Foundry Local service\")\n",
    "                print(f\"        тЖТ Is the service running? Try: foundry service start\")\n",
    "                print(f\"        тЖТ Is the model loaded? Try: foundry model run {alias}\")\n",
    "            elif \"not found\" in error_msg.lower():\n",
    "                print(f\"[ERROR] Model '{alias}' not found in catalog\")\n",
    "                print(f\"        тЖТ Available models: Run 'foundry model ls' in terminal\")\n",
    "                print(f\"        тЖТ Download model: Run 'foundry model download {alias}'\")\n",
    "            else:\n",
    "                print(f\"[ERROR] Setup failed: {type(e).__name__}: {error_msg}\")\n",
    "            \n",
    "            if attempt < retries:\n",
    "                print(f\"[Retry] Waiting {current_delay:.1f}s before retry...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    # All retries failed - provide actionable guidance\n",
    "    print(f\"\\nтЭМ Failed to initialize '{alias}' after {retries} attempts\")\n",
    "    print(f\"   Last error: {type(last_err).__name__}: {str(last_err)}\")\n",
    "    print(f\"\\nЁЯТб Troubleshooting steps:\")\n",
    "    print(f\"   1. Ensure Foundry Local service is running:\")\n",
    "    print(f\"      тЖТ foundry service status\")\n",
    "    print(f\"      тЖТ foundry service start (if not running)\")\n",
    "    print(f\"   2. Ensure model is loaded:\")\n",
    "    print(f\"      тЖТ foundry model run {alias}\")\n",
    "    print(f\"   3. Check available models:\")\n",
    "    print(f\"      тЖТ foundry model ls\")\n",
    "    print(f\"   4. Try alternative models if '{alias}' isn't available\")\n",
    "    \n",
    "    return None, None, alias, {\n",
    "        'error': f\"{type(last_err).__name__}: {str(last_err)}\",\n",
    "        'endpoint': endpoint or 'auto-detect',\n",
    "        'attempts': retries,\n",
    "        'status': 'failed'\n",
    "    }\n",
    "\n",
    "\n",
    "def run(client, model_id: str, prompt: str, max_tokens: int = 180, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Run inference with the configured model using OpenAI SDK.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance (configured for Foundry Local)\n",
    "        model_id: Model identifier (resolved from alias)\n",
    "        prompt: Input prompt\n",
    "        max_tokens: Maximum response tokens (default: 180)\n",
    "        temperature: Sampling temperature (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response with timing, tokens, and content\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Extract response details\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract token usage from multiple possible locations\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            usage_info['prompt_tokens'] = getattr(response.usage, 'prompt_tokens', None)\n",
    "            usage_info['completion_tokens'] = getattr(response.usage, 'completion_tokens', None)\n",
    "            usage_info['total_tokens'] = getattr(response.usage, 'total_tokens', None)\n",
    "        \n",
    "        # Calculate approximate token count if API doesn't provide it\n",
    "        # Rough estimate: ~4 characters per token for English text\n",
    "        if not usage_info.get('total_tokens'):\n",
    "            estimated_prompt_tokens = len(prompt) // 4\n",
    "            estimated_completion_tokens = len(content) // 4\n",
    "            estimated_total = estimated_prompt_tokens + estimated_completion_tokens\n",
    "            usage_info['estimated_tokens'] = estimated_total\n",
    "            usage_info['estimated_prompt_tokens'] = estimated_prompt_tokens\n",
    "            usage_info['estimated_completion_tokens'] = estimated_completion_tokens\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'content': content,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'tokens': usage_info.get('total_tokens') or usage_info.get('estimated_tokens'),\n",
    "            'usage': usage_info,\n",
    "            'model': model_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f\"{type(e).__name__}: {str(e)}\",\n",
    "            'elapsed_sec': elapsed,\n",
    "            'model': model_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"тЬЕ Execution helpers defined: setup(), run()\")\n",
    "print(\"   тЖТ Uses workshop_utils for proper SDK integration\")\n",
    "print(\"   тЖТ setup() initializes with FoundryLocalManager\")\n",
    "print(\"   тЖТ run() executes inference via OpenAI-compatible API\")\n",
    "print(\"   тЖТ Token counting: Uses API data or estimates if unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba23b5",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рдкреНрд░реА-рдлреНрд▓рд╛рдЗрдЯ рд╕реЗрд▓реНрдл-рдЯреЗрд╕реНрдЯ\n",
    "FoundryLocalManager рд╡рд╛рдкрд░реВрди рджреЛрдиреНрд╣реА рдореЙрдбреЗрд▓реНрд╕рд╕рд╛рдареА рд╣рд▓рдХрд╛рд╕рд╛ рдХрдиреЗрдХреНрдЯрд┐рд╡реНрд╣рд┐рдЯреА рдЪреЗрдХ рдЪрд╛рд▓рд╡рддреЛ. рдпрд╛рдордзреНрдпреЗ рдЦрд╛рд▓реАрд▓ рдЧреЛрд╖реНрдЯреАрдВрдЪреА рдЦрд╛рддреНрд░реА рдХреЗрд▓реА рдЬрд╛рддреЗ:\n",
    "- рд╕реЗрд╡рд╛ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп рдЖрд╣реЗ\n",
    "- рдореЙрдбреЗрд▓реНрд╕ рд╕реБрд░реВ рд╣реЛрдК рд╢рдХрддрд╛рдд\n",
    "- рдЙрдкрдирд╛рдо рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдореЙрдбреЗрд▓ рдЖрдпрдбреАрдВрд╢реА рдЬреБрд│рддрд╛рдд\n",
    "- рддреБрд▓рдирд╛ рдЪрд╛рд▓рд╡рдгреНрдпрд╛рдкреВрд░реНрд╡реА рдХрдиреЗрдХреНрд╢рди рд╕реНрдерд┐рд░ рдЖрд╣реЗ\n",
    "\n",
    "setup() рдлрдВрдХреНрд╢рди workshop_utils рдордзреАрд▓ рдЕрдзрд┐рдХреГрдд SDK рдкреЕрдЯрд░реНрди рд╡рд╛рдкрд░рддреЗ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e251bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diagnostic] Checking Foundry Local service...\n",
      "\n",
      "тЭМ Foundry Local service not found!\n",
      "\n",
      "ЁЯТб To fix this:\n",
      "   1. Open a terminal\n",
      "   2. Run: foundry service start\n",
      "   3. Run: foundry model run phi-4-mini\n",
      "   4. Run: foundry model run qwen2.5-3b\n",
      "   5. Re-run this notebook\n",
      "\n",
      "тЪая╕П  No service detected - FoundryLocalManager will attempt to start it\n"
     ]
    }
   ],
   "source": [
    "# Simplified diagnostic: Just verify service is accessible\n",
    "import requests\n",
    "\n",
    "def check_foundry_service():\n",
    "    \"\"\"Quick diagnostic to verify Foundry Local is running.\"\"\"\n",
    "    # Try common ports\n",
    "    endpoints_to_try = [\n",
    "        \"http://localhost:59959\",\n",
    "        \"http://127.0.0.1:59959\", \n",
    "        \"http://localhost:55769\",\n",
    "        \"http://127.0.0.1:55769\",\n",
    "    ]\n",
    "    \n",
    "    print(\"[Diagnostic] Checking Foundry Local service...\")\n",
    "    \n",
    "    for endpoint in endpoints_to_try:\n",
    "        try:\n",
    "            response = requests.get(f\"{endpoint}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"тЬЕ Service is running at {endpoint}\")\n",
    "                \n",
    "                # Try to list models\n",
    "                try:\n",
    "                    models_response = requests.get(f\"{endpoint}/v1/models\", timeout=2)\n",
    "                    if models_response.status_code == 200:\n",
    "                        models_data = models_response.json()\n",
    "                        model_count = len(models_data.get('data', []))\n",
    "                        print(f\"тЬЕ Found {model_count} models available\")\n",
    "                        if model_count > 0:\n",
    "                            print(\"   Models:\", [m.get('id', 'unknown') for m in models_data.get('data', [])[:5]])\n",
    "                except Exception as e:\n",
    "                    print(f\"тЪая╕П  Could not list models: {e}\")\n",
    "                \n",
    "                return endpoint\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"тЪая╕П  Error checking {endpoint}: {e}\")\n",
    "    \n",
    "    print(\"\\nтЭМ Foundry Local service not found!\")\n",
    "    print(\"\\nЁЯТб To fix this:\")\n",
    "    print(\"   1. Open a terminal\")\n",
    "    print(\"   2. Run: foundry service start\")\n",
    "    print(\"   3. Run: foundry model run phi-4-mini\")\n",
    "    print(\"   4. Run: foundry model run qwen2.5-3b\")\n",
    "    print(\"   5. Re-run this notebook\")\n",
    "    return None\n",
    "\n",
    "# Run diagnostic\n",
    "discovered_endpoint = check_foundry_service()\n",
    "\n",
    "if discovered_endpoint:\n",
    "    print(f\"\\nтЬЕ Service detected (will be managed by FoundryLocalManager)\")\n",
    "else:\n",
    "    print(f\"\\nтЪая╕П  No service detected - FoundryLocalManager will attempt to start it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35317769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЪая╕П  The commands above are commented out.\n",
      "Uncomment them if you want to start the service from the notebook.\n",
      "\n",
      "ЁЯТб Recommended: Run these commands in a separate terminal instead:\n",
      "   foundry service start\n",
      "   foundry model run phi-4-mini\n",
      "   foundry model run qwen2.5-3b\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Start service and load models from notebook\n",
    "# Uncomment the commands you need:\n",
    "\n",
    "# !foundry service start\n",
    "# !foundry model run phi-4-mini\n",
    "# !foundry model run qwen2.5-3b\n",
    "# !foundry model ls\n",
    "\n",
    "print(\"тЪая╕П  The commands above are commented out.\")\n",
    "print(\"Uncomment them if you want to start the service from the notebook.\")\n",
    "print(\"\")\n",
    "print(\"ЁЯТб Recommended: Run these commands in a separate terminal instead:\")\n",
    "print(\"   foundry service start\")\n",
    "print(\"   foundry model run phi-4-mini\")\n",
    "print(\"   foundry model run qwen2.5-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2d0a5",
   "metadata": {},
   "source": [
    "### ЁЯЫая╕П рдЬрд▓рдж рдЙрдкрд╛рдп: рдиреЛрдЯрдмреБрдХрдордзреВрди рд╕реНрдерд╛рдирд┐рдХ Foundry рд╕реБрд░реВ рдХрд░рд╛ (рдРрдЪреНрдЫрд┐рдХ)\n",
    "\n",
    "рд╡рд░реАрд▓ рдирд┐рджрд╛рдирд╛рдиреБрд╕рд╛рд░ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╕рд▓реНрдпрд╛рдЪреЗ рджрд┐рд╕рд▓реНрдпрд╛рд╕, рддреБрдореНрд╣реА рддреА рдЗрдереВрди рд╕реБрд░реВ рдХрд░рдгреНрдпрд╛рдЪрд╛ рдкреНрд░рдпрддреНрди рдХрд░реВ рд╢рдХрддрд╛:\n",
    "\n",
    "**рдЯреАрдк:** рд╣реЗ Windows рд╡рд░ рдЙрддреНрддрдо рдкреНрд░рдХрд╛рд░реЗ рдХрд╛рд░реНрдп рдХрд░рддреЗ. рдЗрддрд░ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдорд╡рд░, рдЯрд░реНрдорд┐рдирд▓ рдХрдорд╛рдВрдб рд╡рд╛рдкрд░рд╛.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0bd2",
   "metadata": {},
   "source": [
    "### тЪая╕П рдХрдиреЗрдХреНрд╢рди рддреНрд░реБрдЯреАрдВрдЪреЗ рдирд┐рд░рд╛рдХрд░рдг\n",
    "\n",
    "рдЬрд░ рддреБрдореНрд╣рд╛рд▓рд╛ `APIConnectionError` рджрд┐рд╕рдд рдЕрд╕реЗрд▓, рддрд░ Foundry Local рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╕рд╛рд╡реА рдХрд┐рдВрд╡рд╛ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХреЗрд▓реЗрд▓реЗ рдирд╕рд╛рд╡реЗрдд. рдЦрд╛рд▓реАрд▓ рдЪрд░рдгрд╛рдВрдЪрд╛ рдкреНрд░рдпрддреНрди рдХрд░рд╛:\n",
    "\n",
    "**1. рд╕реЗрд╡рд╛ рд╕реНрдерд┐рддреА рддрдкрд╛рд╕рд╛:**\n",
    "```bash\n",
    "# In a terminal (not in notebook):\n",
    "foundry service status\n",
    "```\n",
    "\n",
    "**2. рд╕реЗрд╡рд╛ рд╕реБрд░реВ рдХрд░рд╛ (рдЬрд░ рдЪрд╛рд▓реВ рдирд╕реЗрд▓):**\n",
    "```bash\n",
    "foundry service start\n",
    "```\n",
    "\n",
    "**3. рдЖрд╡рд╢реНрдпрдХ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХрд░рд╛:**\n",
    "```bash\n",
    "# Load the models needed for comparison\n",
    "foundry model run phi-4-mini\n",
    "foundry model run qwen2.5-7b\n",
    "\n",
    "# Or use alternative models:\n",
    "foundry model run phi-3.5-mini\n",
    "foundry model run qwen2.5-3b\n",
    "```\n",
    "\n",
    "**4. рдореЙрдбреЗрд▓реНрд╕ рдЙрдкрд▓рдмреНрдз рдЖрд╣реЗрдд рдХрд╛ рддреЗ рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░рд╛:**\n",
    "```bash\n",
    "foundry model ls\n",
    "```\n",
    "\n",
    "**рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛:**\n",
    "- тЭМ рд╕реЗрд╡рд╛ рдЪрд╛рд▓реВ рдирд╛рд╣реА тЖТ `foundry service start` рдЪрд╛рд▓рд╡рд╛\n",
    "- тЭМ рдореЙрдбреЗрд▓реНрд╕ рд▓реЛрдб рдХреЗрд▓реЗрд▓реЗ рдирд╛рд╣реАрдд тЖТ `foundry model run <model-name>` рдЪрд╛рд▓рд╡рд╛\n",
    "- тЭМ рдкреЛрд░реНрдЯ рд╕рдВрдШрд░реНрд╖ тЖТ рддрдкрд╛рд╕рд╛ рдХреА рджреБрд╕рд░реА рд╕реЗрд╡рд╛ рдкреЛрд░реНрдЯ рд╡рд╛рдкрд░рдд рдЖрд╣реЗ рдХрд╛\n",
    "- тЭМ рдлрд╛рдпрд░рд╡реЙрд▓ рдЕрдбрдерд│рд╛ тЖТ рд╕реНрдерд╛рдирд┐рдХ рдХрдиреЗрдХреНрд╢рди рдкрд░рд╡рд╛рдирдЧреА рджрд┐рд▓реА рдЖрд╣реЗ рдпрд╛рдЪреА рдЦрд╛рддреНрд░реА рдХрд░рд╛\n",
    "\n",
    "**рдЬрд▓рдж рдЙрдкрд╛рдп:** рдкреНрд░реА-рдлреНрд▓рд╛рдЗрдЯ рддрдкрд╛рд╕рдгреАрдкреВрд░реНрд╡реА рдЦрд╛рд▓реАрд▓ рдбрд╛рдпрдЧреНрдиреЛрд╕реНрдЯрд┐рдХ рд╕реЗрд▓ рдЪрд╛рд▓рд╡рд╛.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a607078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "[Pre-flight Check]\n",
      "  тЬЕ phi-4-mini: success - Phi-4-mini-instruct-cuda-gpu:4\n",
      "  тЬЕ qwen2.5-7b: success - qwen2.5-7b-instruct-cuda-gpu:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phi-4-mini': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'Phi-4-mini-instruct-cuda-gpu:4',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'},\n",
       " 'qwen2.5-7b': {'endpoint': 'http://127.0.0.1:59959/v1',\n",
       "  'resolved': 'qwen2.5-7b-instruct-cuda-gpu:3',\n",
       "  'attempts': 1,\n",
       "  'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preflight = {}\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for a in (SLM, LLM):\n",
    "    mgr, c, mid, info = setup(a, endpoint=ENDPOINT, retries=retries)\n",
    "    # Keep the original status from info (either 'success' or 'failed')\n",
    "    preflight[a] = info\n",
    "\n",
    "print('\\n[Pre-flight Check]')\n",
    "for alias, details in preflight.items():\n",
    "    status_icon = 'тЬЕ' if details['status'] == 'success' else 'тЭМ'\n",
    "    print(f\"  {status_icon} {alias}: {details['status']} - {details.get('resolved', details.get('error', 'unknown'))}\")\n",
    "\n",
    "preflight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76741a",
   "metadata": {},
   "source": [
    "### тЬЕ рдкреВрд░реНрд╡рддрдпрд╛рд░реА рддрдкрд╛рд╕рдгреА: рдореЙрдбреЗрд▓ рдЙрдкрд▓рдмреНрдзрддрд╛\n",
    "\n",
    "рд╣реА рд╕реЗрд▓ рддреБрд▓рдирд╛ рдЪрд╛рд▓рд╡рдгреНрдпрд╛рдкреВрд░реНрд╡реА рдХреЙрдиреНрдлрд┐рдЧрд░ рдХреЗрд▓реЗрд▓реНрдпрд╛ рдПрдВрдбрдкреЙрдЗрдВрдЯрд╡рд░ рджреЛрдиреНрд╣реА рдореЙрдбреЗрд▓реНрд╕ рдЙрдкрд▓рдмреНрдз рдЖрд╣реЗрдд рдХрд╛ рддреЗ рддрдкрд╛рд╕рддреЗ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22961df",
   "metadata": {},
   "source": [
    "### рд╕реНрдкрд╖реНрдЯреАрдХрд░рдг: рд░рди рддреБрд▓рдирд╛ рдЖрдгрд┐ рдирд┐рдХрд╛рд▓ рдЧреЛрд│рд╛ рдХрд░рд╛\n",
    "рджреЛрдиреНрд╣реА рдЙрдкрдирд╛рдорд╛рдВрд╡рд░ рдЕрдзрд┐рдХреГрдд Foundry SDK рдкреЕрдЯрд░реНрди рд╡рд╛рдкрд░реВрди рдкреБрдирд░рд╛рд╡реГрддреНрддреА рдХрд░рддреЗ:\n",
    "1. рдкреНрд░рддреНрдпреЗрдХ рдореЙрдбреЗрд▓рд▓рд╛ setup() рд╕рд╣ рдкреНрд░рд╛рд░рдВрдн рдХрд░рд╛ (FoundryLocalManager рд╡рд╛рдкрд░рддреЗ)\n",
    "2. OpenAI-рд╕реБрд╕рдВрдЧрдд API рд╕рд╣ рдЕрдиреБрдорд╛рди рдЪрд╛рд▓рд╡рд╛\n",
    "3. рд╡рд┐рд▓рдВрдм, рдЯреЛрдХрди рдЖрдгрд┐ рдирдореБрдирд╛ рдЖрдЙрдЯрдкреБрдЯ рдХреЕрдкреНрдЪрд░ рдХрд░рд╛\n",
    "4. рддреБрд▓рдирд╛рддреНрдордХ рд╡рд┐рд╢реНрд▓реЗрд╖рдгрд╛рд╕рд╣ JSON рд╕рд╛рд░рд╛рдВрд╢ рддрдпрд╛рд░ рдХрд░рд╛\n",
    "\n",
    "рд╣реЗ session04/model_compare.py рдордзреАрд▓ рд╡рд░реНрдХрд╢реЙрдк рдирдореБрдиреНрдпрд╛рдВрдкреНрд░рдорд╛рдгреЗрдЪ рдкреЕрдЯрд░реНрди рдЕрдиреБрд╕рд░рддреЗ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f12b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Connecting to 'phi-4-mini' (attempt 1/2)...\n",
      "[OK] Connected to 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[Init] Connecting to 'qwen2.5-7b' (attempt 1/2)...\n",
      "[OK] Connected to 'qwen2.5-7b' -> qwen2.5-7b-instruct-cuda-gpu:3\n",
      "     Endpoint: http://127.0.0.1:59959/v1\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"phi-4-mini\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.\",\n",
      "    \"elapsed_sec\": 51.97519612312317,\n",
      "    \"tokens\": 247,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 247,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 238\n",
      "    },\n",
      "    \"model\": \"Phi-4-mini-instruct-cuda-gpu:4\"\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"qwen2.5-7b\",\n",
      "    \"status\": \"success\",\n",
      "    \"content\": \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
      "    \"elapsed_sec\": 329.4796121120453,\n",
      "    \"tokens\": 264,\n",
      "    \"usage\": {\n",
      "      \"estimated_tokens\": 264,\n",
      "      \"estimated_prompt_tokens\": 9,\n",
      "      \"estimated_completion_tokens\": 255\n",
      "    },\n",
      "    \"model\": \"qwen2.5-7b-instruct-cuda-gpu:3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "Alias                Status          Latency(s)      Tokens         \n",
      "--------------------------------------------------------------------------------\n",
      "тЬЕ phi-4-mini         success         51.975          ~247 (est.)    \n",
      "тЬЕ qwen2.5-7b         success         329.480         ~264 (est.)    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Token Usage:\n",
      "\n",
      "  phi-4-mini:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 238\n",
      "    Estimated total:      247\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "  qwen2.5-7b:\n",
      "    Estimated prompt:     9\n",
      "    Estimated completion: 255\n",
      "    Estimated total:      264\n",
      "    (API did not provide token counts - using ~4 chars/token estimate)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ЁЯТб SLM is 6.34x faster than LLM for this prompt\n",
      "   SLM throughput: 4.8 tokens/sec\n",
      "   LLM throughput: 0.8 tokens/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alias': 'phi-4-mini',\n",
       "  'status': 'success',\n",
       "  'content': '1. Reduced Latency: Local AI inference can significantly reduce latency by processing data closer to the source, which is particularly beneficial for real-time applications such as autonomous vehicles or augmented reality.\\n\\n2. Enhanced Privacy: By keeping data processing local, sensitive information is less likely to be exposed to external networks, thereby enhancing privacy and security.\\n\\n3. Lower Bandwidth Usage: Local AI inference reduces the need for data transmission over the network, which can save bandwidth and reduce the risk of network congestion.\\n\\n4. Improved Reliability: Local processing can be more reliable, as it is less dependent on network connectivity. This is particularly important in scenarios where network connectivity is unreliable or intermittent.\\n\\n5. Scalability: Local AI inference can be easily scaled by adding more local processing units, making it easier to handle increasing data volumes or more complex AI models.',\n",
       "  'elapsed_sec': 51.97519612312317,\n",
       "  'tokens': 247,\n",
       "  'usage': {'estimated_tokens': 247,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 238},\n",
       "  'model': 'Phi-4-mini-instruct-cuda-gpu:4'},\n",
       " {'alias': 'qwen2.5-7b',\n",
       "  'status': 'success',\n",
       "  'content': \"Local AI inference offers several advantages over cloud-based or remote inference solutions. Here are five key benefits:\\n\\n1. **Latency Reduction**: Local AI inference reduces latency because the data does not need to be sent to a remote server for processing. This is particularly important in applications where real-time response is critical, such as autonomous vehicles, medical imaging, and real-time analytics.\\n\\n2. **Data Privacy and Security**: Processing data locally can enhance privacy and security by keeping sensitive information within the user's control. This is especially important in industries like healthcare, finance, and government where data breaches can have severe consequences.\\n\\n3. **Cost Efficiency**: For certain applications, local inference can be more cost-effective than cloud inference. While initial setup costs for hardware might be higher, ongoing costs for cloud services can add up over time, especially for high-frequency or large-scale operations.\\n\\n4. **Offline Capabilities**: Devices\",\n",
       "  'elapsed_sec': 329.4796121120453,\n",
       "  'tokens': 264,\n",
       "  'usage': {'estimated_tokens': 264,\n",
       "   'estimated_prompt_tokens': 9,\n",
       "   'estimated_completion_tokens': 255},\n",
       "  'model': 'qwen2.5-7b-instruct-cuda-gpu:3'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "retries = 2  # Number of retry attempts\n",
    "\n",
    "for alias in (SLM, LLM):\n",
    "    mgr, client, mid, info = setup(alias, endpoint=ENDPOINT, retries=retries)\n",
    "    if client:\n",
    "        r = run(client, mid, PROMPT)\n",
    "        results.append({'alias': alias, **r})\n",
    "    else:\n",
    "        # If setup failed, record error\n",
    "        results.append({\n",
    "            'alias': alias,\n",
    "            'status': 'error',\n",
    "            'error': info.get('error', 'Setup failed'),\n",
    "            'elapsed_sec': 0,\n",
    "            'tokens': None,\n",
    "            'model': alias\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Quick comparative view\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(f\"{'Alias':<20} {'Status':<15} {'Latency(s)':<15} {'Tokens':<15}\")\n",
    "print('-'*80)\n",
    "\n",
    "for row in results:\n",
    "    status = row.get('status', 'unknown')\n",
    "    status_icon = 'тЬЕ' if status == 'success' else 'тЭМ'\n",
    "    latency_str = f\"{row.get('elapsed_sec', 0):.3f}\" if row.get('elapsed_sec') else 'N/A'\n",
    "    \n",
    "    # Handle token display - show if available or indicate estimated\n",
    "    tokens = row.get('tokens')\n",
    "    usage = row.get('usage', {})\n",
    "    if tokens:\n",
    "        if 'estimated_tokens' in usage:\n",
    "            tokens_str = f\"~{tokens} (est.)\"\n",
    "        else:\n",
    "            tokens_str = str(tokens)\n",
    "    else:\n",
    "        tokens_str = 'N/A'\n",
    "    \n",
    "    print(f\"{status_icon} {row['alias']:<18} {status:<15} {latency_str:<15} {tokens_str:<15}\")\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# Show detailed token breakdown if available\n",
    "print(\"\\nDetailed Token Usage:\")\n",
    "for row in results:\n",
    "    if row.get('status') == 'success' and row.get('usage'):\n",
    "        usage = row['usage']\n",
    "        print(f\"\\n  {row['alias']}:\")\n",
    "        if 'prompt_tokens' in usage and usage['prompt_tokens']:\n",
    "            print(f\"    Prompt tokens:     {usage['prompt_tokens']}\")\n",
    "            print(f\"    Completion tokens: {usage['completion_tokens']}\")\n",
    "            print(f\"    Total tokens:      {usage['total_tokens']}\")\n",
    "        elif 'estimated_tokens' in usage:\n",
    "            print(f\"    Estimated prompt:     {usage['estimated_prompt_tokens']}\")\n",
    "            print(f\"    Estimated completion: {usage['estimated_completion_tokens']}\")\n",
    "            print(f\"    Estimated total:      {usage['estimated_tokens']}\")\n",
    "            print(f\"    (API did not provide token counts - using ~4 chars/token estimate)\")\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Calculate speedup if both succeeded\n",
    "if len(results) == 2 and all(r.get('status') == 'success' and r.get('elapsed_sec') for r in results):\n",
    "    speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec']\n",
    "    print(f\"\\nЁЯТб SLM is {speedup:.2f}x faster than LLM for this prompt\")\n",
    "    \n",
    "    # Compare token throughput if available\n",
    "    slm_tokens = results[0].get('tokens', 0)\n",
    "    llm_tokens = results[1].get('tokens', 0)\n",
    "    if slm_tokens and llm_tokens:\n",
    "        slm_tps = slm_tokens / results[0]['elapsed_sec']\n",
    "        llm_tps = llm_tokens / results[1]['elapsed_sec']\n",
    "        print(f\"   SLM throughput: {slm_tps:.1f} tokens/sec\")\n",
    "        print(f\"   LLM throughput: {llm_tps:.1f} tokens/sec\")\n",
    "        \n",
    "elif any(r.get('status') == 'error' for r in results):\n",
    "    print(f\"\\nтЪая╕П  Some models failed - check errors above\")\n",
    "    print(\"   Ensure Foundry Local is running: foundry service start\")\n",
    "    print(\"   Ensure models are loaded: foundry model run <model-name>\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d052c",
   "metadata": {},
   "source": [
    "### рдирд┐рдХрд╛рд▓рд╛рдВрдЪреЗ рд╡рд┐рд╢реНрд▓реЗрд╖рдг\n",
    "\n",
    "**рдорд╣рддреНрддреНрд╡рд╛рдЪреЗ рдореЗрдЯреНрд░рд┐рдХреНрд╕:**\n",
    "- **рдкреНрд░рддрд┐рдХреНрд░рд┐рдпреЗрдЪрд╛ рд╡реЗрд│ (Latency)**: рдХрдореА рдЕрд╕рдгреЗ рдЪрд╛рдВрдЧрд▓реЗ - рдЬрд▓рдж рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│ рд╕реВрдЪрд┐рдд рдХрд░рддреЗ\n",
    "- **рдЯреЛрдХрдиреНрд╕ (Tokens)**: рдЬрд╛рд╕реНрдд throughput = рдЕрдзрд┐рдХ рдЯреЛрдХрдиреНрд╕ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХреЗрд▓реА рдЬрд╛рддрд╛рдд\n",
    "- **рдорд╛рд░реНрдЧ (Route)**: рдХреЛрдгрддрд╛ API endpoint рд╡рд╛рдкрд░рд▓рд╛ рдЧреЗрд▓рд╛ рд╣реЗ рдкреБрд╖реНрдЯреА рдХрд░рддреЗ\n",
    "\n",
    "**SLM рдЖрдгрд┐ LLM рдХрдзреА рд╡рд╛рдкрд░рд╛рд╡реЗ:**\n",
    "- **SLM (рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓)**: рдЬрд▓рдж рдкреНрд░рддрд┐рд╕рд╛рдж, рдХрдореА рд╕рдВрд╕рд╛рдзрди рд╡рд╛рдкрд░, рд╕рд╛рдзреНрдпрд╛ рдХрд╛рдорд╛рдВрд╕рд╛рдареА рдЪрд╛рдВрдЧрд▓реЗ\n",
    "- **LLM (рдореЛрдареЗ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓)**: рдЙрдЪреНрдЪ рдЧреБрдгрд╡рддреНрддрд╛, рдЪрд╛рдВрдЧрд▓реЗ рддрд░реНрдХрд╢рдХреНрддреА, рдЧреБрдгрд╡рддреНрддрд╛ рдорд╣рддреНрддреНрд╡рд╛рдЪреА рдЕрд╕реЗрд▓ рддреЗрд╡реНрд╣рд╛ рд╡рд╛рдкрд░рд╛\n",
    "\n",
    "**рдкреБрдвреАрд▓ рдкрд╛рд╡рд▓реЗ:**\n",
    "1. рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдкреНрд░реЙрдореНрдкреНрдЯреНрд╕ рд╡рд╛рдкрд░реВрди рдкрд╛рд╣рд╛ рдЖрдгрд┐ рддреБрд▓рдирд╛ рдХрд╢реА рдмрджрд▓рддреЗ рддреЗ рдкрд╣рд╛\n",
    "2. рдЗрддрд░ рдореЙрдбреЗрд▓ рдЬреЛрдбреНрдпрд╛ рд╡рд╛рдкрд░реВрди рдкреНрд░рдпреЛрдЧ рдХрд░рд╛\n",
    "3. Workshop router samples (Session 06) рд╡рд╛рдкрд░реВрди рдХрд╛рд░реНрдпрд╛рдЪреНрдпрд╛ рдЬрдЯрд┐рд▓рддреЗрдиреБрд╕рд╛рд░ рд╣реБрд╢рд╛рд░реАрдиреЗ рдорд╛рд░реНрдЧ рдирд┐рд╡рдбрд╛\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8caed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "тЬЕ SLM Model: phi-4-mini\n",
      "тЬЕ LLM Model: qwen2.5-7b\n",
      "тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\n",
      "тЬЕ Pre-flight passed: True\n",
      "тЬЕ Comparison completed: True\n",
      "тЬЕ Both models responded: True\n",
      "======================================================================\n",
      "ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\n",
      "   SLM (phi-4-mini) vs LLM (qwen2.5-7b) comparison completed.\n",
      "   Performance: SLM is 5.14x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Validation Check\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"тЬЕ SLM Model: {SLM}\")\n",
    "print(f\"тЬЕ LLM Model: {LLM}\")\n",
    "print(f\"тЬЕ Using Foundry SDK Pattern: workshop_utils with FoundryLocalManager\")\n",
    "print(f\"тЬЕ Pre-flight passed: {all(v['status'] == 'success' for v in preflight.values()) if 'preflight' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Comparison completed: {len(results) == 2 if 'results' in dir() else 'Not run yet'}\")\n",
    "print(f\"тЬЕ Both models responded: {all(r.get('status') == 'success' for r in results) if 'results' in dir() and results else 'Not run yet'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common configuration issues\n",
    "issues = []\n",
    "if 'LLM' in dir() and LLM not in ['qwen2.5-3b', 'qwen2.5-0.5b', 'qwen2.5-1.5b', 'qwen2.5-7b', 'phi-3.5-mini']:\n",
    "    issues.append(f\"тЪая╕П  LLM is '{LLM}' - expected qwen2.5-3b for memory efficiency\")\n",
    "if 'preflight' in dir() and not all(v['status'] == 'success' for v in preflight.values()):\n",
    "    issues.append(\"тЪая╕П  Pre-flight check failed - models not accessible\")\n",
    "if 'results' in dir() and results and not all(r.get('status') == 'success' for r in results):\n",
    "    issues.append(\"тЪая╕П  Comparison incomplete - check for errors above\")\n",
    "\n",
    "if not issues and 'results' in dir() and results and all(r.get('status') == 'success' for r in results):\n",
    "    print(\"ЁЯОЙ ALL CHECKS PASSED! Notebook completed successfully.\")\n",
    "    print(f\"   SLM ({SLM}) vs LLM ({LLM}) comparison completed.\")\n",
    "    if len(results) == 2:\n",
    "        speedup = results[1]['elapsed_sec'] / results[0]['elapsed_sec'] if results[0]['elapsed_sec'] > 0 else 0\n",
    "        print(f\"   Performance: SLM is {speedup:.2f}x faster\")\n",
    "elif issues:\n",
    "    print(\"\\nтЪая╕П  Issues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "    print(\"\\nЁЯТб Troubleshooting:\")\n",
    "    print(\"   1. Ensure service is running: foundry service start\")\n",
    "    print(\"   2. Load models: foundry model run phi-4-mini && foundry model run qwen2.5-7b\")\n",
    "    print(\"   3. Check model list: foundry model ls\")\n",
    "else:\n",
    "    print(\"\\nЁЯТб Run all cells above first, then re-run this validation.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**рдЕрд╕реНрд╡реАрдХрд░рдг**:  \nрд╣рд╛ рджрд╕реНрддрдРрд╡рдЬ [Co-op Translator](https://github.com/Azure/co-op-translator) рдпрд╛ рдПрдЖрдп рднрд╛рд╖рд╛рдВрддрд░ рд╕реЗрд╡реЗрдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рднрд╛рд╖рд╛рдВрддрд░рд┐рдд рдХрд░рдгреНрдпрд╛рдд рдЖрд▓рд╛ рдЖрд╣реЗ. рдЖрдореНрд╣реА рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА рдкреНрд░рдпрддреНрдирд╢реАрд▓ рдЕрд╕рд▓реЛ рддрд░реА, рдХреГрдкрдпрд╛ рд▓рдХреНрд╖рд╛рдд рдШреНрдпрд╛ рдХреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд╛рд╖рд╛рдВрддрд░рд╛рдВрдордзреНрдпреЗ рддреНрд░реБрдЯреА рдХрд┐рдВрд╡рд╛ рдЕрдЪреВрдХрддреЗрдЪрд╛ рдЕрднрд╛рд╡ рдЕрд╕реВ рд╢рдХрддреЛ. рдореВрд│ рднрд╛рд╖реЗрддреАрд▓ рджрд╕реНрддрдРрд╡рдЬ рд╣рд╛ рдЕрдзрд┐рдХреГрдд рд╕реНрд░реЛрдд рдорд╛рдирд▓рд╛ рдЬрд╛рд╡рд╛. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рдорд╛рд╣рд┐рддреАрд╕рд╛рдареА рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡реА рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ. рдпрд╛ рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХреЗрд▓реНрдпрд╛рдореБрд│реЗ рдЙрджреНрднрд╡рдгрд╛рд▒реНрдпрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рдЧреИрд░рд╕рдордЬрд╛рдВрдХрд░рд┐рддрд╛ рдХрд┐рдВрд╡рд╛ рдЪреБрдХреАрдЪреНрдпрд╛ рдЕрд░реНрде рд▓рд╛рд╡рдгреНрдпрд╛рдХрд░рд┐рддрд╛ рдЖрдореНрд╣реА рдЬрдмрд╛рдмрджрд╛рд░ рд░рд╛рд╣рдгрд╛рд░ рдирд╛рд╣реА.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "8220e33feaffbd35ece0f93e8214c24f",
   "translation_date": "2025-10-09T10:04:09+00:00",
   "source_file": "Workshop/notebooks/session04_model_compare.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}