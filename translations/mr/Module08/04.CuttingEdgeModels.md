<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8ccc4f76611daedf918e34460128fc21",
  "translation_date": "2025-09-30T23:51:30+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "mr"
}
-->
# рд╕рддреНрд░ рек: Chainlit рд╕рд╣ рдЙрддреНрдкрд╛рджрди-рд╕реНрддрд░реАрдп рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддрдпрд╛рд░ рдХрд░рдгреЗ

## рдЖрдврд╛рд╡рд╛

рдпрд╛ рд╕рддреНрд░рд╛рдд Chainlit рдЖрдгрд┐ Microsoft Foundry Local рд╡рд╛рдкрд░реВрди рдЙрддреНрдкрд╛рджрди-рд╕реНрддрд░реАрдп рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╡рд░ рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд рдХреЗрд▓реЗ рдЖрд╣реЗ. рддреБрдореНрд╣реА AI рд╕рдВрд╡рд╛рджрд╛рдВрд╕рд╛рдареА рдЖрдзреБрдирд┐рдХ рд╡реЗрдм рдЗрдВрдЯрд░рдлреЗрд╕ рддрдпрд╛рд░ рдХрд░рдгреЗ, рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рдкреНрд░рддрд┐рд╕рд╛рдж рдЕрдВрдорд▓рд╛рдд рдЖрдгрдгреЗ рдЖрдгрд┐ рдпреЛрдЧреНрдп рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА рд╡ рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рдбрд┐рдЭрд╛рдЗрдирд╕рд╣ рдордЬрдмреВрдд рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддреИрдирд╛рдд рдХрд░рдгреЗ рд╢рд┐рдХрд╛рд▓.

**рддреБрдореНрд╣реА рдХрд╛рдп рддрдпрд╛рд░ рдХрд░рд╛рд▓:**
- **Chainlit Chat App**: рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рдкреНрд░рддрд┐рд╕рд╛рджрд╛рдВрд╕рд╣ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм UI
- **WebGPU рдбреЗрдореЛ**: рдЧреЛрдкрдиреАрдпрддрд╛-рдкреНрд░рдердо рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдмреНрд░рд╛рдЙрдЭрд░-рдЖрдзрд╛рд░рд┐рдд рдЕрдиреБрдорд╛рди  
- **Open WebUI рдПрдХрддреНрд░реАрдХрд░рдг**: Foundry Local рд╕рд╣ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЪреЕрдЯ рдЗрдВрдЯрд░рдлреЗрд╕
- **рдЙрддреНрдкрд╛рджрди рдирдореБрдиреЗ**: рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА, рдирд┐рд░реАрдХреНрд╖рдг рдЖрдгрд┐ рддреИрдирд╛рддреА рдзреЛрд░рдгреЗ

## рд╢рд┐рдХрдгреНрдпрд╛рдЪреА рдЙрджреНрджрд┐рд╖реНрдЯреЗ

- Chainlit рд╕рд╣ рдЙрддреНрдкрд╛рджрди-рд╕реНрддрд░реАрдп рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддрдпрд╛рд░ рдХрд░рдгреЗ
- рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рд╕реБрдзрд╛рд░рдгреНрдпрд╛рд╕рд╛рдареА рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рдкреНрд░рддрд┐рд╕рд╛рдж рдЕрдВрдорд▓рд╛рдд рдЖрдгрдгреЗ
- Foundry Local SDK рдПрдХрддреНрд░реАрдХрд░рдг рдирдореБрдиреЗ рдЖрддреНрдорд╕рд╛рдд рдХрд░рдгреЗ
- рдпреЛрдЧреНрдп рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА рдЖрдгрд┐ рд╕реМрдореНрдп рдЕрдкрдпрд╢ рдЕрдВрдорд▓рд╛рдд рдЖрдгрдгреЗ
- рд╡рд┐рд╡рд┐рдз рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдВрд╕рд╛рдареА рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддреИрдирд╛рдд рдЖрдгрд┐ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рдгреЗ
- рд╕рдВрд╡рд╛рджрд╛рддреНрдордХ AI рд╕рд╛рдареА рдЖрдзреБрдирд┐рдХ рд╡реЗрдм UI рдирдореБрдиреЗ рд╕рдордЬреВрди рдШреЗрдгреЗ

## рдкреВрд░реНрд╡рддрдпрд╛рд░реА

- **Foundry Local**: рд╕реНрдерд╛рдкрд┐рдд рдЖрдгрд┐ рдЪрд╛рд▓реВ ([рд╕реНрдерд╛рдкрдирд╛ рдорд╛рд░реНрдЧрджрд░реНрд╢рдХ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 рдХрд┐рдВрд╡рд╛ рдирдВрддрд░рдЪреЗ, рд╡рд░реНрдЪреНрдпреБрдЕрд▓ рд╡рд╛рддрд╛рд╡рд░рдг рдХреНрд╖рдорддреЗрд╕рд╣
- **рдореЙрдбреЗрд▓**: рдХрд┐рдорд╛рди рдПрдХ рдореЙрдбреЗрд▓ рд▓реЛрдб рдХреЗрд▓реЗрд▓реЗ (`foundry model run phi-4-mini`)
- **рдмреНрд░рд╛рдЙрдЭрд░**: WebGPU рд╕рдорд░реНрдердирд╛рд╕рд╣ рдЖрдзреБрдирд┐рдХ рд╡реЗрдм рдмреНрд░рд╛рдЙрдЭрд░ (Chrome/Edge)
- **Docker**: Open WebUI рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╛рдареА (рдкрд░реНрдпрд╛рдпреА)

## рднрд╛рдЧ рез: рдЖрдзреБрдирд┐рдХ рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ рд╕рдордЬреВрди рдШреЗрдгреЗ

### рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЖрдврд╛рд╡рд╛

```
User Browser тЖРтЖТ Chainlit UI тЖРтЖТ Python Backend тЖРтЖТ Foundry Local тЖРтЖТ AI Model
      тЖУ              тЖУ              тЖУ              тЖУ            тЖУ
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### рдореБрдЦреНрдп рддрдВрддреНрд░рдЬреНрдЮрд╛рди

**Foundry Local SDK рдирдореБрдиреЗ:**
- `FoundryLocalManager(alias)`: рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╕реЗрд╡рд╛ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди
- `manager.endpoint` рдЖрдгрд┐ `manager.api_key`: рдХрдиреЗрдХреНрд╢рди рддрдкрд╢реАрд▓
- `manager.get_model_info(alias).id`: рдореЙрдбреЗрд▓ рдУрд│рдЦ

**Chainlit рдлреНрд░реЗрдорд╡рд░реНрдХ:**
- `@cl.on_chat_start`: рдЪреЕрдЯ рд╕рддреНрд░реЗ рд╕реБрд░реВ рдХрд░рд╛
- `@cl.on_message`: рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдЪреЗ рд╕рдВрджреЗрд╢ рд╣рд╛рддрд╛рд│рд╛  
- `cl.Message().stream_token()`: рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ
- рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд UI рдирд┐рд░реНрдорд┐рддреА рдЖрдгрд┐ WebSocket рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди

## рднрд╛рдЧ реи: рд╕реНрдерд╛рдирд┐рдХ рд╡рд┐рд░реБрджреНрдз рдХреНрд▓рд╛рдЙрдб рдирд┐рд░реНрдгрдп рдореЕрдЯреНрд░рд┐рдХреНрд╕

### рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ

| рдкреИрд▓реВ | рд╕реНрдерд╛рдирд┐рдХ (Foundry) | рдХреНрд▓рд╛рдЙрдб (Azure OpenAI) |
|------|-------------------|-----------------------|
| **рд╡рд┐рд▓рдВрдм** | ЁЯЪА 50-200ms (рдиреЗрдЯрд╡рд░реНрдХ рдирд╛рд╣реА) | тП▒я╕П 200-2000ms (рдиреЗрдЯрд╡рд░реНрдХ рдЕрд╡рд▓рдВрдмреВрди) |
| **рдЧреЛрдкрдиреАрдпрддрд╛** | ЁЯФТ рдбреЗрдЯрд╛ рдХрдзреАрд╣реА рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рд╕реЛрдбрдд рдирд╛рд╣реА | тЪая╕П рдбреЗрдЯрд╛ рдХреНрд▓рд╛рдЙрдбрд╡рд░ рдкрд╛рдард╡рд▓рд╛ рдЬрд╛рддреЛ |
| **рдЦрд░реНрдЪ** | ЁЯТ░ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдирдВрддрд░ рдореЛрдлрдд | ЁЯТ╕ рдкреНрд░рддрд┐ рдЯреЛрдХрди рдкреИрд╕реЗ |
| **рдСрдлрд▓рд╛рдЗрди** | тЬЕ рдЗрдВрдЯрд░рдиреЗрдЯрд╢рд┐рд╡рд╛рдп рдХрд╛рд░реНрдп рдХрд░рддреЗ | тЭМ рдЗрдВрдЯрд░рдиреЗрдЯ рдЖрд╡рд╢реНрдпрдХ |
| **рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░** | тЪая╕П рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рдиреЗ рдорд░реНрдпрд╛рджрд┐рдд | тЬЕ рд╕рд░реНрд╡рд╛рдд рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рдкреНрд░рд╡реЗрд╢ |
| **рд╕реНрдХреЗрд▓рд┐рдВрдЧ** | тЪая╕П рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдЕрд╡рд▓рдВрдмреВрди | тЬЕ рдЕрдорд░реНрдпрд╛рджрд┐рдд рд╕реНрдХреЗрд▓рд┐рдВрдЧ |

### рд╣рд╛рдпрдмреНрд░рд┐рдб рдзреЛрд░рдг рдирдореБрдиреЗ

**рд╕реНрдерд╛рдирд┐рдХ-рдкреНрд░рдердо рдлреЙрд▓рдмреЕрдХрд╕рд╣:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**рдХрд╛рд░реНрдп-рдЖрдзрд╛рд░рд┐рдд рд░реВрдЯрд┐рдВрдЧ:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## рднрд╛рдЧ рей: рдирдореБрдирд╛ режрек - Chainlit рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

### рдЬрд▓рдж рдкреНрд░рд╛рд░рдВрдн

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

рдЕрдиреБрдкреНрд░рдпреЛрдЧ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ `http://localhost:8080` рдпреЗрдереЗ рдЖрдзреБрдирд┐рдХ рдЪреЕрдЯ рдЗрдВрдЯрд░рдлреЗрд╕рд╕рд╣ рдЙрдШрдбрддреЛ.

### рдореБрдЦреНрдп рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА

рдирдореБрдирд╛ режрек рдЕрдиреБрдкреНрд░рдпреЛрдЧ рдЙрддреНрдкрд╛рджрди-рд╕реНрддрд░реАрдп рдирдореБрдиреЗ рдкреНрд░рджрд░реНрд╢рд┐рдд рдХрд░рддреЛ:

**рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╕реЗрд╡рд╛ рд╢реЛрдз:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рдЪреЕрдЯ рд╣рдБрдбрд▓рд░:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдкрд░реНрдпрд╛рдп

**рдкрд░реНрдпрд╛рд╡рд░рдгреАрдп рдЪрд▓ (Environment Variables):**

| рдЪрд▓ | рд╡рд░реНрдгрди | рдбреАрдлреЙрд▓реНрдЯ | рдЙрджрд╛рд╣рд░рдг |
|----|-------|---------|--------|
| `MODEL` | рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕рд╛рдареА рдореЙрдбреЗрд▓ рдЙрдкрдирд╛рдо | `phi-4-mini` | `qwen2.5-7b` |
| `BASE_URL` | Foundry Local рдПрдВрдбрдкреЙрдЗрдВрдЯ | рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд╢реЛрдзрд▓реЗрд▓реЗ | `http://localhost:51211` |
| `API_KEY` | API рдХреА (рд╕реНрдерд╛рдирд┐рдХрд╕рд╛рдареА рдкрд░реНрдпрд╛рдпреА) | `""` | `your-api-key` |

**рдкреНрд░рдЧрдд рд╡рд╛рдкрд░:**
```cmd
# Use different model
set MODEL=qwen2.5-7b
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## рднрд╛рдЧ рек: Jupyter рдиреЛрдЯрдмреБрдХ рддрдпрд╛рд░ рдХрд░рдгреЗ рдЖрдгрд┐ рд╡рд╛рдкрд░рдгреЗ

### рдиреЛрдЯрдмреБрдХ рд╕рдорд░реНрдердирд╛рдЪрд╛ рдЖрдврд╛рд╡рд╛

рдирдореБрдирд╛ режрек рдордзреНрдпреЗ рдПрдХ рд╡реНрдпрд╛рдкрдХ Jupyter рдиреЛрдЯрдмреБрдХ (`chainlit_app.ipynb`) рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЖрд╣реЗ рдЬреНрдпрд╛рдордзреНрдпреЗ:

- **ЁЯУЪ рд╢реИрдХреНрд╖рдгрд┐рдХ рд╕рд╛рдордЧреНрд░реА**: рдЪрд░рдг-рджрд░-рдЪрд░рдг рд╢рд┐рдХрдгреНрдпрд╛рдЪреА рд╕рд╛рдордЧреНрд░реА
- **ЁЯФм рдкрд░рд╕реНрдкрд░ рдЕрдиреНрд╡реЗрд╖рдг**: рдХреЛрдб рд╕реЗрд▓ рдЪрд╛рд▓рд╡рд╛ рдЖрдгрд┐ рдкреНрд░рдпреЛрдЧ рдХрд░рд╛
- **ЁЯУК рджреГрд╢реНрдп рдкреНрд░рджрд░реНрд╢рди**: рдЪрд╛рд░реНрдЯ, рдЖрдХреГрддреНрдпрд╛ рдЖрдгрд┐ рдЖрдЙрдЯрдкреБрдЯ рд╡реНрд╣рд┐рдЬреНрдпреБрдЕрд▓рд╛рдпрдЭреЗрд╢рди
- **ЁЯЫая╕П рд╡рд┐рдХрд╛рд╕ рд╕рд╛рдзрдиреЗ**: рдЪрд╛рдЪрдгреА рдЖрдгрд┐ рдбреАрдмрдЧрд┐рдВрдЧ рдХреНрд╖рдорддрд╛

### рд╕реНрд╡рддрдГрдЪреЗ рдиреЛрдЯрдмреБрдХ рддрдпрд╛рд░ рдХрд░рдгреЗ

#### рдЪрд░рдг рез: Jupyter рд╡рд╛рддрд╛рд╡рд░рдг рд╕реЗрдЯ рдХрд░рд╛

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### рдЪрд░рдг реи: рдирд╡реАрди рдиреЛрдЯрдмреБрдХ рддрдпрд╛рд░ рдХрд░рд╛

**VS Code рд╡рд╛рдкрд░реВрди:**
1. Module08 рдбрд┐рд░реЗрдХреНрдЯрд░реАрдордзреНрдпреЗ VS Code рдЙрдШрдбрд╛
2. `.ipynb` рд╡рд┐рд╕реНрддрд╛рд░рд╛рд╕рд╣ рдирд╡реАрди рдлрд╛рдЗрд▓ рддрдпрд╛рд░ рдХрд░рд╛
3. "Foundry Local" рдХрд░реНрдирд▓ рдирд┐рд╡рдбрд╛
4. рддреБрдордЪреА рд╕рд╛рдордЧреНрд░реА рдЕрд╕рд▓реЗрд▓реЗ рд╕реЗрд▓ рдЬреЛрдбрд╛рдпрд▓рд╛ рд╕реБрд░реБрд╡рд╛рдд рдХрд░рд╛

**Jupyter Lab рд╡рд╛рдкрд░реВрди:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### рдиреЛрдЯрдмреБрдХ рд╕рдВрд░рдЪрдирд╛ рд╕рд░реНрд╡реЛрддреНрддрдо рдкрджреНрдзрддреА

#### рд╕реЗрд▓ рд╕рдВрдШрдЯрди

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("тЬЕ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### рдкрд░рд╕реНрдкрд░ рдЙрджрд╛рд╣рд░рдгреЗ рдЖрдгрд┐ рд╕рд░рд╛рд╡

#### рд╕рд░рд╛рд╡ рез: рдХреНрд▓рд╛рдпрдВрдЯ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдЪрд╛рдЪрдгреА

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b"},
]

for config in configurations:
    print(f"\nЁЯзк Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'тЬЕ Success' if result['status'] == 'ok' else 'тЭМ Failed'}")
```

#### рд╕рд░рд╛рд╡ реи: рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рдкреНрд░рддрд┐рд╕рд╛рдж рдЕрдиреБрдХрд░рдг

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("ЁЯМК Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\nтЬЕ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## рднрд╛рдЧ рел: WebGPU рдмреНрд░рд╛рдЙрдЭрд░ рдЕрдиреБрдорд╛рди рдбреЗрдореЛ

### рдЖрдврд╛рд╡рд╛

WebGPU AI рдореЙрдбреЗрд▓реНрд╕ рдереЗрдЯ рдмреНрд░рд╛рдЙрдЭрд░рдордзреНрдпреЗ рдЪрд╛рд▓рд╡рд┐рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддреЗ, рдЬреНрдпрд╛рдореБрд│реЗ рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдЧреЛрдкрдиреАрдпрддрд╛ рдЖрдгрд┐ рд╢реВрдиреНрдп-рд╕реНрдерд╛рдкрдирд╛ рдЕрдиреБрднрд╡ рдорд┐рд│рддреЛ. рд╣рд╛ рдирдореБрдирд╛ ONNX Runtime Web рд╕рд╣ WebGPU рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдкреНрд░рджрд░реНрд╢рд┐рдд рдХрд░рддреЛ.

### рдЪрд░рдг рез: WebGPU рд╕рдорд░реНрдерди рддрдкрд╛рд╕рд╛

**рдмреНрд░рд╛рдЙрдЭрд░ рдЖрд╡рд╢реНрдпрдХрддрд╛:**
- Chrome/Edge 113+ WebGPU рд╕рдХреНрд╖рдо рдЕрд╕рд▓реЗрд▓реЗ
- рддрдкрд╛рд╕рд╛: `chrome://gpu` тЖТ "WebGPU" рд╕реНрдерд┐рддреАрдЪреА рдкреБрд╖реНрдЯреА рдХрд░рд╛
- рдкреНрд░реЛрдЧреНрд░рд╛рдореЗрдЯрд┐рдХ рддрдкрд╛рд╕рдгреА: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### рдЪрд░рдг реи: WebGPU рдбреЗрдореЛ рддрдпрд╛рд░ рдХрд░рд╛

рдбрд┐рд░реЗрдХреНрдЯрд░реА рддрдпрд╛рд░ рдХрд░рд╛: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>ЁЯЪА WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = 'тЭМ WebGPU not available';
            return;
        }
        
        statusEl.textContent = 'ЁЯФН WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('тЬЕ ONNX Runtime session created with WebGPU');
        log(`ЁЯУК Input names: ${session.inputNames.join(', ')}`);
        log(`ЁЯУК Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = 'тЬЕ WebGPU inference complete!';
        log(`ЁЯОп Predicted class: ${maxIdx}`);
        log(`ЁЯУИ Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `тЭМ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### рдЪрд░рдг рей: рдбреЗрдореЛ рдЪрд╛рд▓рд╡рд╛

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## рднрд╛рдЧ рем: Open WebUI рдПрдХрддреНрд░реАрдХрд░рдг

### рдЖрдврд╛рд╡рд╛

Open WebUI Foundry Local рдЪреНрдпрд╛ OpenAI-рд╕реБрд╕рдВрдЧрдд API рд╢реА рдЬреЛрдбрдгрд╛рд░рд╛ рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ ChatGPT-рд╕рд╛рд░рдЦрд╛ рдЗрдВрдЯрд░рдлреЗрд╕ рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

### рдЪрд░рдг рез: рдкреВрд░реНрд╡рддрдпрд╛рд░реА

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### рдЪрд░рдг реи: Docker рд╕реЗрдЯрдЕрдк (рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реЗрд▓реЗ)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**рдЯреАрдк:** `host.docker.internal` Windows рд╡рд░ Docker рдХрдВрдЯреЗрдирд░рдирд╛ рд╣реЛрд╕реНрдЯ рдорд╢реАрдирд╡рд░ рдкреНрд░рд╡реЗрд╢ рдХрд░рдгреНрдпрд╛рд╕ рдЕрдиреБрдорддреА рджреЗрддреЗ.

### рдЪрд░рдг рей: рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди

1. **рдмреНрд░рд╛рдЙрдЭрд░ рдЙрдШрдбрд╛:** `http://localhost:3000` рдпреЗрдереЗ рдЬрд╛
2. **рдкреНрд░рд╛рд░рдВрднрд┐рдХ рд╕реЗрдЯрдЕрдк:** рдкреНрд░рд╢рд╛рд╕рдХ рдЦрд╛рддреЗ рддрдпрд╛рд░ рдХрд░рд╛
3. **рдореЙрдбреЗрд▓ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди:**
   - рд╕реЗрдЯрд┐рдВрдЧреНрдЬ тЖТ рдореЙрдбреЗрд▓реНрд╕ тЖТ OpenAI API  
   - рдмреЗрд╕ URL: `http://host.docker.internal:51211/v1`
   - API рдХреА: `foundry-local-key` (рдХреЛрдгрддреАрд╣реА рдХрд┐рдВрдордд рдЪрд╛рд▓рддреЗ)
4. **рдХрдиреЗрдХреНрд╢рди рдЪрд╛рдЪрдгреА:** рдореЙрдбреЗрд▓реНрд╕ рдбреНрд░реЙрдкрдбрд╛рдЙрдирдордзреНрдпреЗ рджрд┐рд╕рд╛рдпрд▓рд╛ рд╣рд╡реЗ

### рд╕рдорд╕реНрдпрд╛ рдирд┐рд╡рд╛рд░рдг

**рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╕реНрдпрд╛:**

1. **рдХрдиреЗрдХреНрд╢рди рдирд╛рдХрд╛рд░рд▓реЗ:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **рдореЙрдбреЗрд▓реНрд╕ рджрд┐рд╕рдд рдирд╛рд╣реАрдд:**
   - рдореЙрдбреЗрд▓ рд▓реЛрдб рдХреЗрд▓реЗ рдЖрд╣реЗ рдпрд╛рдЪреА рдкреБрд╖реНрдЯреА рдХрд░рд╛: `foundry model list`
   - API рдкреНрд░рддрд┐рд╕рд╛рдж рддрдкрд╛рд╕рд╛: `curl http://localhost:51211/v1/models`
   - Open WebUI рдХрдВрдЯреЗрдирд░ рдкреБрдиреНрд╣рд╛ рд╕реБрд░реВ рдХрд░рд╛

## рднрд╛рдЧ рен: рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА рд╡рд┐рдЪрд╛рд░

### рдкрд░реНрдпрд╛рд╡рд░рдгреАрдп рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди

**рд╡рд┐рдХрд╕рди рд╕реЗрдЯрдЕрдк:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### рд╕рд╛рдорд╛рдиреНрдп рдкреЛрд░реНрдЯ рд╕рдорд╕реНрдпрд╛ рдЖрдгрд┐ рдЙрдкрд╛рдп

**рдкреЛрд░реНрдЯ 51211 рд╕рдВрдШрд░реНрд╖ рдкреНрд░рддрд┐рдмрдВрдз:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдирд┐рд░реАрдХреНрд╖рдг

**рд╣реЗрд▓реНрде рдЪреЗрдХ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## рд╕рд╛рд░рд╛рдВрд╢

рд╕рддреНрд░ рек рдордзреНрдпреЗ рд╕рдВрд╡рд╛рджрд╛рддреНрдордХ AI рд╕рд╛рдареА рдЙрддреНрдкрд╛рджрди-рд╕реНрддрд░реАрдп Chainlit рдЕрдиреБрдкреНрд░рдпреЛрдЧ рддрдпрд╛рд░ рдХрд░рдгреЗ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рд╣реЛрддреЗ. рддреБрдореНрд╣реА рд╢рд┐рдХрд▓рд╛:

- тЬЕ **Chainlit рдлреНрд░реЗрдорд╡рд░реНрдХ**: рдЪреЕрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЖрдзреБрдирд┐рдХ UI рдЖрдгрд┐ рд╕реНрдЯреНрд░реАрдорд┐рдВрдЧ рд╕рдорд░реНрдерди
- тЬЕ **Foundry Local рдПрдХрддреНрд░реАрдХрд░рдг**: SDK рд╡рд╛рдкрд░ рдЖрдгрд┐ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдирдореБрдиреЗ  
- тЬЕ **WebGPU рдЕрдиреБрдорд╛рди**: рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдЧреЛрдкрдиреАрдпрддреЗрд╕рд╛рдареА рдмреНрд░рд╛рдЙрдЭрд░-рдЖрдзрд╛рд░рд┐рдд AI
- тЬЕ **Open WebUI рд╕реЗрдЯрдЕрдк**: рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдЪреЕрдЯ рдЗрдВрдЯрд░рдлреЗрд╕ рддреИрдирд╛рддреА
- тЬЕ **рдЙрддреНрдкрд╛рджрди рдирдореБрдиреЗ**: рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреА, рдирд┐рд░реАрдХреНрд╖рдг рдЖрдгрд┐ рд╕реНрдХреЗрд▓рд┐рдВрдЧ

рдирдореБрдирд╛ режрек рдЕрдиреБрдкреНрд░рдпреЛрдЧ Microsoft Foundry Local рдЪреНрдпрд╛ рд╕реНрдерд╛рдирд┐рдХ AI рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рд▓рд╛рдн рдШреЗрдд рдЙрддреНрдХреГрд╖реНрдЯ рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рдкреНрд░рджрд╛рди рдХрд░рдд рдордЬрдмреВрдд рдЪреЕрдЯ рдЗрдВрдЯрд░рдлреЗрд╕ рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╕рд░реНрд╡реЛрддреНрддрдо рдкрджреНрдзрддреА рдкреНрд░рджрд░реНрд╢рд┐рдд рдХрд░рддреЛ.

## рд╕рдВрджрд░реНрдн

- **[рдирдореБрдирд╛ режрек: Chainlit рдЕрдиреБрдкреНрд░рдпреЛрдЧ](samples/04/README.md)**: рджрд╕реНрддрдРрд╡рдЬрд╛рд╕рд╣ рд╕рдВрдкреВрд░реНрдг рдЕрдиреБрдкреНрд░рдпреЛрдЧ
- **[Chainlit рд╢реИрдХреНрд╖рдгрд┐рдХ рдиреЛрдЯрдмреБрдХ](samples/04/chainlit_app.ipynb)**: рдкрд░рд╕реНрдкрд░ рд╢рд┐рдХрдгреНрдпрд╛рдЪреА рд╕рд╛рдордЧреНрд░реА
- **[Foundry Local рджрд╕реНрддрдРрд╡рдЬ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: рд╕рдВрдкреВрд░реНрдг рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рджрд╕реНрддрдРрд╡рдЬ
- **[Chainlit рджрд╕реНрддрдРрд╡рдЬ](https://docs.chainlit.io/)**: рдЕрдзрд┐рдХреГрдд рдлреНрд░реЗрдорд╡рд░реНрдХ рджрд╕реНрддрдРрд╡рдЬ
- **[Open WebUI рдПрдХрддреНрд░реАрдХрд░рдг рдорд╛рд░реНрдЧрджрд░реНрд╢рдХ](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: рдЕрдзрд┐рдХреГрдд рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓

---

**рдЕрд╕реНрд╡реАрдХрд░рдг**:  
рд╣рд╛ рджрд╕реНрддрдРрд╡рдЬ AI рднрд╛рд╖рд╛рдВрддрд░ рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рднрд╛рд╖рд╛рдВрддрд░рд┐рдд рдХрд░рдгреНрдпрд╛рдд рдЖрд▓рд╛ рдЖрд╣реЗ. рдЖрдореНрд╣реА рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА рдкреНрд░рдпрддреНрдирд╢реАрд▓ рдЕрд╕рд▓реЛ рддрд░реА рдХреГрдкрдпрд╛ рд▓рдХреНрд╖рд╛рдд рдареЗрд╡рд╛ рдХреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд╛рд╖рд╛рдВрддрд░рд╛рдВрдордзреНрдпреЗ рддреНрд░реБрдЯреА рдХрд┐рдВрд╡рд╛ рдЕрдЪреВрдХрддреЗрдЪрд╛ рдЕрднрд╛рд╡ рдЕрд╕реВ рд╢рдХрддреЛ. рдореВрд│ рднрд╛рд╖реЗрддреАрд▓ рджрд╕реНрддрдРрд╡рдЬ рд╣рд╛ рдЕрдзрд┐рдХреГрдд рд╕реНрд░реЛрдд рдорд╛рдирд▓рд╛ рдЬрд╛рд╡рд╛. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рдорд╛рд╣рд┐рддреАрд╕рд╛рдареА рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡реА рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ. рдпрд╛ рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рдирд┐рд░реНрдорд╛рдг рд╣реЛрдгрд╛рд▒реНрдпрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рдЧреИрд░рд╕рдордЬ рдХрд┐рдВрд╡рд╛ рдЪреБрдХреАрдЪреНрдпрд╛ рдЕрд░реНрдерд╛рд╕рд╛рдареА рдЖрдореНрд╣реА рдЬрдмрд╛рдмрджрд╛рд░ рд░рд╛рд╣рдгрд╛рд░ рдирд╛рд╣реА.