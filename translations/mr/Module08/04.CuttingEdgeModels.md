<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-22T17:46:02+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "mr"
}
-->
# सत्र ४: अत्याधुनिक मॉडेल्स – LLMs, SLMs, आणि ऑन-डिव्हाइस इनफरन्स

## आढावा

LLMs आणि SLMs ची तुलना करा, स्थानिक आणि क्लाउड इनफरन्समधील फायदे-तोटे समजून घ्या, आणि Phi आणि ONNX Runtime वापरून EdgeAI परिदृश्ये दाखवणारे डेमो तयार करा. आम्ही Chainlit RAG, WebGPU इनफरन्स पर्याय, आणि Open WebUI एकत्रीकरण यावरही प्रकाश टाकू.

संदर्भ:
- Foundry Local दस्तऐवज: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI कसे करावे (Open WebUI सह चॅट अॅप): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## शिकण्याची उद्दिष्टे
- LLM आणि SLM मधील खर्च, विलंबता, आणि अचूकतेसाठी फायदे-तोटे समजून घ्या
- विशिष्ट व्यवसायाच्या गरजांसाठी स्थानिक आणि क्लाउड इनफरन्समधून योग्य पर्याय निवडा
- Chainlit सह एक छोटासा RAG डेमो तयार करा
- ब्राउझर-साइड गतीसाठी WebGPU एक्सप्लोर करा
- Open WebUI ला Foundry Local शी जोडा

## भाग १: LLM vs SLM – निर्णय मॅट्रिक्स

विचार करा:
- विलंबता: SLMs ऑन-डिव्हाइस सहसा उप-सेकंद प्रतिसाद देतात
- खर्च: स्थानिक इनफरन्स क्लाउड खर्च कमी करतो
- गोपनीयता: संवेदनशील डेटा ऑन-डिव्हाइस राहतो
- क्षमता: LLMs जटिल कार्यांमध्ये SLMs पेक्षा चांगले काम करू शकतात
- विश्वासार्हता: हायब्रिड रणनीती डाउनटाइमचा धोका कमी करतात

## भाग २: स्थानिक vs क्लाउड – हायब्रिड पॅटर्न

- मोठ्या/जटिल प्रॉम्प्टसाठी क्लाउड फॉलबॅकसह स्थानिक-प्रथम
- गोपनीयता-संवेदनशील किंवा ऑफलाइन परिस्थितीसाठी स्थानिकसह क्लाउड-प्रथम
- कार्य प्रकारानुसार रूट करा (कोड-जनरेशनसाठी DeepSeek, सामान्य चॅटसाठी Phi/Qwen)

## भाग ३: Chainlit सह RAG चॅट अॅप (मिनिमल)

आवश्यकता स्थापित करा:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

चालवा:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

वाढवा: एक साधा रिट्रीव्हर (स्थानिक फायली) जोडा आणि वापरकर्त्याच्या प्रॉम्प्टला प्राप्त केलेल्या संदर्भासह प्रीपेंड करा.

## भाग ४: WebGPU इनफरन्स (Heads-up)

WebGPU वापरून ब्राउझरमध्ये छोटे मॉडेल्स थेट चालवा. हे गोपनीयता-प्रथम डेमो आणि झिरो-इंस्टॉल अनुभवांसाठी आदर्श आहे. खाली ONNX Runtime Web सह WebGPU कार्यकारी प्रदात्याचा वापर करून एक मिनिमल, चरण-दर-चरण उदाहरण दिले आहे.

1) WebGPU समर्थन तपासा
- Chromium ब्राउझर: chrome://gpu → “WebGPU” सक्षम असल्याची पुष्टी करा
- प्रोग्रामॅटिक तपासणी (आम्ही कोडमध्येही तपासू): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

2) एक मिनिमल प्रोजेक्ट तयार करा
एक फोल्डर आणि दोन फायली तयार करा: `index.html` आणि `main.js`.

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

3) स्थानिकपणे सर्व्ह करा (Windows cmd.exe)
एक साधा स्थिर सर्व्हर वापरा जेणेकरून ब्राउझर मॉडेल मिळवू शकेल.

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

आपल्या ब्राउझरमध्ये http://localhost:5173 उघडा. आपल्याला इनिशियलायझेशन लॉग्स, WebGPU सह सत्र निर्मिती, आणि argmax प्रेडिक्शन दिसेल.

4) समस्या सोडवणे
- WebGPU अनुपलब्ध असल्यास: Chrome/Edge अपडेट करा आणि GPU ड्रायव्हर्स अद्ययावत ठेवा, नंतर chrome://flags मध्ये “Enable WebGPU” तपासा.
- CORS किंवा fetch त्रुटी असल्यास: सुनिश्चित करा की आपण फायली http:// (file:// नाही) वर सर्व्ह करत आहात आणि मॉडेल URL क्रॉस-ओरिजिन विनंत्यांना अनुमती देते.
- CPU वर फॉलबॅक: `executionProviders: ['wasm']` बदलून बेसलाइन वर्तन सत्यापित करा.

5) पुढील पावले
- डोमेन-विशिष्ट ONNX मॉडेल (उदा. इमेज क्लासिफिकेशन किंवा एक छोटासा टेक्स्ट मॉडेल) बदला.
- वास्तविक इनपुटसाठी प्रीप्रोसेसिंग/पोस्टप्रोसेसिंग लॉजिक जोडा.
- मोठ्या मॉडेल्स किंवा उत्पादन विलंबासाठी, Foundry Local किंवा ONNX Runtime Server प्राधान्य द्या.

## भाग ५: Open WebUI + Foundry Local (चरण-दर-चरण)

हे Open WebUI ला Foundry Local च्या OpenAI-सुसंग एन्डपॉइंटशी जोडते जे स्थानिक चॅट UI साठी आहे.

1) पूर्वतयारी
- Foundry Local स्थापित आणि कार्यरत (`foundry --version`)
- एक मॉडेल स्थानिकपणे चालवण्यासाठी तयार (उदा. `phi-4-mini`)
- Docker Desktop स्थापित (Open WebUI साठी शिफारस केलेले)

2) Foundry Local सह मॉडेल सुरू करा
```powershell
foundry model run phi-4-mini
```
हे `http://localhost:8000` वर OpenAI-सुसंग API उघडते.

3) Open WebUI सुरू करा (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
टीप:
- Windows वर, `host.docker.internal` कंटेनरला आपल्या होस्टला `localhost` वर पोहोचण्याची परवानगी देते.
- आम्ही `OPENAI_API_BASE_URL` Foundry Local च्या एन्डपॉइंटवर सेट करतो आणि एक डमी `OPENAI_API_KEY`.

4) Open WebUI UI मधून कॉन्फिगर करा (पर्याय)
- http://localhost:3000 वर ब्राउझ करा
- प्रारंभिक सेटअप पूर्ण करा (प्रशासक वापरकर्ता)
- Settings → Models/Providers वर जा
- Base URL सेट करा: `http://host.docker.internal:8000/v1`
- API Key सेट करा: `local-key` (प्लेसहोल्डर)
- सेव्ह करा

5) चाचणी प्रॉम्प्ट चालवा
- Open WebUI चॅटमध्ये, मॉडेलचे नाव निवडा किंवा प्रविष्ट करा `phi-4-mini`
- प्रॉम्प्ट: “List five benefits of on-device AI inference.”
- आपल्याला स्थानिक मॉडेलमधून प्रतिसाद प्रवाहित होताना दिसेल

6) समस्या सोडवणे
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

7) पर्यायी: Open WebUI डेटा कायम ठेवा
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```


## हाताळणी चेकलिस्ट
- [ ] SLM आणि LLM स्थानिकपणे प्रतिसाद/विलंबता तुलना करा
- [ ] Chainlit डेमो किमान दोन मॉडेल्सवर चालवा
- [ ] Open WebUI ला आपल्या स्थानिक एन्डपॉइंटशी जोडा आणि चाचणी करा

## पुढील पावले
- सत्र ५ मध्ये एजंट वर्कफ्लो तयार करण्यासाठी तयारी करा
- हायब्रिड स्थानिक/क्लाउड ROI सुधारण्यासाठी उपयुक्त परिस्थिती ओळखा

---

