<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T17:43:13+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "mr"
}
-->
# सत्र ३: फाउंड्री लोकलसह ओपन-सोर्स मॉडेल्स

## आढावा

या सत्रात फाउंड्री लोकलमध्ये ओपन-सोर्स मॉडेल्स कसे आणायचे यावर चर्चा केली आहे: समुदाय मॉडेल्स निवडणे, Hugging Face सामग्री समाकलित करणे, आणि "आपले स्वतःचे मॉडेल आणा" (BYOM) धोरण स्वीकारणे. तुम्ही सतत शिकण्यासाठी आणि मॉडेल शोधण्यासाठी Model Mondays मालिकाही शोधाल.

संदर्भ:
- फाउंड्री लोकल दस्तऐवज: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face मॉडेल्स संकलित करा: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- फाउंड्री लोकल GitHub: https://github.com/microsoft/Foundry-Local

## शिकण्याची उद्दिष्टे
- स्थानिक अनुमानासाठी ओपन-सोर्स मॉडेल्स शोधा आणि त्यांचे मूल्यमापन करा
- फाउंड्री लोकलमध्ये निवडक Hugging Face मॉडेल्स संकलित करा आणि चालवा
- अचूकता, विलंबता, आणि संसाधन गरजांसाठी मॉडेल निवड धोरणे लागू करा
- कॅश आणि आवृत्ती व्यवस्थापनासह मॉडेल्स स्थानिकपणे व्यवस्थापित करा

## भाग १: मॉडेल शोध आणि निवड (पायरी-पायरीने)

पायरी १) स्थानिक कॅटलॉगमध्ये उपलब्ध मॉडेल्सची यादी करा  
```cmd
foundry model list
```
  
पायरी २) दोन उमेदवार मॉडेल्स जलद चाचणी करा (पहिल्या रनवर स्वयंचलित डाउनलोड)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
पायरी ३) मूलभूत मेट्रिक्स नोंदवा  
- निश्चित प्रॉम्प्टसाठी विलंबता (सवंग) आणि गुणवत्ता निरीक्षण करा  
- प्रत्येक मॉडेल चालू असताना Task Manager वापरून मेमरी वापर पहा  

## भाग २: कॅटलॉग मॉडेल्स CLI द्वारे चालवणे (पायरी-पायरीने)

पायरी १) मॉडेल सुरू करा  
```cmd
foundry model run llama-3.2
```
  
पायरी २) OpenAI-सुसंगत एन्डपॉइंटद्वारे चाचणी प्रॉम्प्ट पाठवा  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## भाग ३: BYOM – Hugging Face मॉडेल्स संकलित करा (पायरी-पायरीने)

मॉडेल्स संकलित करण्यासाठी अधिकृत कसे करावे ते अनुसरा. खाली उच्च-स्तरीय प्रवाह दिला आहे—अचूक आदेश आणि समर्थित कॉन्फिगरेशनसाठी Microsoft Learn लेख पहा.

पायरी १) कार्यरत निर्देशिका तयार करा  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
पायरी २) समर्थित HF मॉडेल संकलित करा  
- Learn दस्तऐवजातील चरणांचा वापर करून ONNX मॉडेलमध्ये रूपांतर करा आणि ते `models` निर्देशिकेत ठेवा  
- पुष्टी करा:  
```cmd
foundry cache ls
```
  
तुम्हाला तुमच्या संकलित मॉडेलचे नाव दिसले पाहिजे (उदाहरणार्थ, `llama-3.2`).  

पायरी ३) संकलित मॉडेल चालवा  
```cmd
foundry model run llama-3.2 --verbose
```
  
टीप:  
- संकलन आणि चालवण्यासाठी पुरेशी डिस्क आणि RAM सुनिश्चित करा  
- प्रवाह सत्यापित करण्यासाठी लहान मॉडेल्सपासून सुरुवात करा, नंतर स्केल करा  

## भाग ४: व्यावहारिक मॉडेल क्युरेशन (पायरी-पायरीने)

पायरी १) `models.json` रजिस्ट्रि तयार करा  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
पायरी २) लहान निवडकर्ता स्क्रिप्ट  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## भाग ५: हँड्स-ऑन बेंचमार्क्स (पायरी-पायरीने)

पायरी १) साधा विलंबता बेंचमार्क  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
पायरी २) गुणवत्ता स्पॉट-चेक  
- निश्चित प्रॉम्प्ट सेट वापरा, आउटपुट CSV/JSON मध्ये कॅप्चर करा  
- फ्लुएन्सी, संबंधितता, आणि अचूकता (१–५) मॅन्युअली रेट करा  

## भाग ६: पुढील पावले
- नवीन मॉडेल्स आणि टिप्ससाठी Model Mondays सबस्क्राइब करा: https://aka.ms/model-mondays  
- तुमच्या टीमच्या `models.json` मध्ये निष्कर्ष योगदान द्या  
- सत्र ४ साठी तयारी करा: LLMs vs SLMs, स्थानिक vs क्लाउड अनुमान, आणि हँड्स-ऑन डेमो तुलना

---

