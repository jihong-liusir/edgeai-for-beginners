<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-17T21:07:04+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "mr"
}
-->
# विभाग 2 : Llama.cpp अंमलबजावणी मार्गदर्शक

## विषय सूची
1. [परिचय](../../../Module04)
2. [Llama.cpp म्हणजे काय?](../../../Module04)
3. [स्थापना](../../../Module04)
4. [स्रोत कोडमधून बांधणी](../../../Module04)
5. [मॉडेल क्वांटायझेशन](../../../Module04)
6. [मूलभूत वापर](../../../Module04)
7. [प्रगत वैशिष्ट्ये](../../../Module04)
8. [Python एकत्रीकरण](../../../Module04)
9. [समस्या निवारण](../../../Module04)
10. [सर्वोत्तम पद्धती](../../../Module04)

## परिचय

ही सविस्तर ट्यूटोरियल तुम्हाला Llama.cpp बद्दल सर्व काही शिकवेल, मूलभूत स्थापनेपासून प्रगत वापराच्या परिस्थितींपर्यंत. Llama.cpp ही एक शक्तिशाली C++ अंमलबजावणी आहे जी मोठ्या भाषा मॉडेल्स (LLMs) च्या कार्यक्षम अंदाजासाठी किमान सेटअपसह आणि विविध हार्डवेअर कॉन्फिगरेशनमध्ये उत्कृष्ट कार्यक्षमता सक्षम करते.

## Llama.cpp म्हणजे काय?

Llama.cpp हे C/C++ मध्ये लिहिलेले LLM अंदाज फ्रेमवर्क आहे, जे मोठ्या भाषा मॉडेल्सना स्थानिक पातळीवर चालविण्यासाठी किमान सेटअपसह आणि अत्याधुनिक कार्यक्षमतेसह सक्षम करते. याची मुख्य वैशिष्ट्ये खालीलप्रमाणे आहेत:

### मुख्य वैशिष्ट्ये
- **साधी C/C++ अंमलबजावणी** कोणत्याही अतिरिक्त अवलंबित्वांशिवाय
- **क्रॉस-प्लॅटफॉर्म सुसंगतता** (Windows, macOS, Linux)
- **हार्डवेअर ऑप्टिमायझेशन** विविध आर्किटेक्चर्ससाठी
- **क्वांटायझेशन समर्थन** (1.5-बिट ते 8-बिट पूर्णांक क्वांटायझेशन)
- **CPU आणि GPU प्रवेग** समर्थन
- **स्मृती कार्यक्षमता** मर्यादित संसाधनांसाठी

### फायदे
- CPU वर कार्यक्षमतेने चालते, विशेष हार्डवेअरची आवश्यकता नाही
- GPU बॅकएंड्ससाठी समर्थन (CUDA, Metal, OpenCL, Vulkan)
- हलके आणि पोर्टेबल
- Apple silicon साठी प्रथम श्रेणीचे समर्थन - ARM NEON, Accelerate आणि Metal फ्रेमवर्कद्वारे ऑप्टिमाइझ केलेले
- कमी स्मृती वापरासाठी विविध क्वांटायझेशन स्तरांना समर्थन

## स्थापना

### पद्धत 1: पूर्व-निर्मित बायनरीज (नवशिक्यांसाठी शिफारस केलेली)

#### GitHub Releases वरून डाउनलोड करा
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) ला भेट द्या
2. तुमच्या सिस्टमसाठी योग्य बायनरी डाउनलोड करा:
   - Windows साठी `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS साठी `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux साठी `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. आर्काइव्ह अनझिप करा आणि डायरेक्टरीला तुमच्या सिस्टमच्या PATH मध्ये जोडा

#### पॅकेज मॅनेजर्सचा वापर

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (विविध वितरणे):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### पद्धत 2: Python पॅकेज (llama-cpp-python)

#### मूलभूत स्थापना
```bash
pip install llama-cpp-python
```

#### हार्डवेअर प्रवेगासह
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## स्रोत कोडमधून बांधणी

### पूर्वतयारी

**सिस्टम आवश्यकता:**
- C++ कंपायलर (GCC, Clang, किंवा MSVC)
- CMake (आवृत्ती 3.14 किंवा त्याहून अधिक)
- Git
- तुमच्या प्लॅटफॉर्मसाठी बिल्ड टूल्स

**पूर्वतयारी कशी स्थापित करावी:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Visual Studio 2022 C++ विकास साधनांसह स्थापित करा
- अधिकृत वेबसाइटवरून CMake स्थापित करा
- Git स्थापित करा

### मूलभूत बांधणी प्रक्रिया

1. **रेपॉजिटरी क्लोन करा:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **बांधणी कॉन्फिगर करा:**
```bash
cmake -B build
```

3. **प्रकल्प तयार करा:**
```bash
cmake --build build --config Release
```

जलद संकलनासाठी, समांतर जॉब्स वापरा:
```bash
cmake --build build --config Release -j 8
```

### हार्डवेअर-विशिष्ट बांधणी

#### CUDA समर्थन (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal समर्थन (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS समर्थन (CPU ऑप्टिमायझेशन)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan समर्थन
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### प्रगत बांधणी पर्याय

#### डीबग बिल्ड
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### अतिरिक्त वैशिष्ट्यांसह
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## मॉडेल क्वांटायझेशन

### GGUF स्वरूप समजून घेणे

GGUF (Generalized GGML Unified Format) हे एक ऑप्टिमाइझ केलेले फाइल स्वरूप आहे, जे Llama.cpp आणि इतर फ्रेमवर्क्ससह मोठ्या भाषा मॉडेल्स कार्यक्षमतेने चालविण्यासाठी डिझाइन केले आहे. यामध्ये समाविष्ट आहे:

- मॉडेल वजन संचयासाठी मानकीकृत स्वरूप
- प्लॅटफॉर्म्समध्ये सुधारित सुसंगतता
- कार्यक्षमतेत वाढ
- मेटाडेटा हाताळणीसाठी कार्यक्षमता

### क्वांटायझेशन प्रकार

Llama.cpp विविध क्वांटायझेशन स्तरांना समर्थन देते:

| प्रकार | बिट्स | वर्णन | वापर प्रकरण |
|-------|-------|--------|-------------|
| F16 | 16 | हाफ प्रिसिजन | उच्च गुणवत्ता, मोठी स्मृती |
| Q8_0 | 8 | 8-बिट क्वांटायझेशन | चांगला समतोल |
| Q4_0 | 4 | 4-बिट क्वांटायझेशन | मध्यम गुणवत्ता, लहान आकार |
| Q2_K | 2 | 2-बिट क्वांटायझेशन | सर्वात लहान आकार, कमी गुणवत्ता |

### मॉडेल्स रूपांतरित करणे

#### PyTorch वरून GGUF मध्ये
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face वरून थेट डाउनलोड
Hugging Face वर GGUF स्वरूपात अनेक मॉडेल्स उपलब्ध आहेत:
- "GGUF" नाव असलेल्या मॉडेल्स शोधा
- योग्य क्वांटायझेशन स्तर डाउनलोड करा
- Llama.cpp सह थेट वापरा

## मूलभूत वापर

### कमांड लाइन इंटरफेस

#### साधे मजकूर निर्माण
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Hugging Face मधील मॉडेल्स वापरणे
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### सर्व्हर मोड
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### सामान्य पॅरामीटर्स

| पॅरामीटर | वर्णन | उदाहरण |
|----------|-------|---------|
| `-m` | मॉडेल फाइलचा पथ | `-m model.gguf` |
| `-p` | प्रॉम्प्ट मजकूर | `-p "Hello world"` |
| `-n` | निर्माण करण्यासाठी टोकन्सची संख्या | `-n 100` |
| `-c` | संदर्भ आकार | `-c 4096` |
| `-t` | थ्रेड्सची संख्या | `-t 8` |
| `-ngl` | GPU स्तर | `-ngl 32` |
| `-temp` | तापमान | `-temp 0.7` |

### परस्परसंवादी मोड

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## प्रगत वैशिष्ट्ये

### सर्व्हर API

#### सर्व्हर सुरू करणे
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API वापर
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### कार्यक्षमता ऑप्टिमायझेशन

#### स्मृती व्यवस्थापन
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### मल्टी-थ्रेडिंग
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU प्रवेग
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python एकत्रीकरण

### llama-cpp-python सह मूलभूत वापर

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### चॅट इंटरफेस

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### प्रवाहित प्रतिसाद

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain सह एकत्रीकरण

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## समस्या निवारण

### सामान्य समस्या आणि उपाय

#### बांधणी त्रुटी

**समस्या: CMake सापडत नाही**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**समस्या: कंपायलर सापडत नाही**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### रनटाइम समस्या

**समस्या: मॉडेल लोडिंग अयशस्वी**
- मॉडेल फाइलचा पथ सत्यापित करा
- फाइल परवानग्या तपासा
- पुरेशी RAM सुनिश्चित करा
- वेगवेगळ्या क्वांटायझेशन स्तरांचा प्रयत्न करा

**समस्या: खराब कार्यक्षमता**
- हार्डवेअर प्रवेग सक्षम करा
- थ्रेड्सची संख्या वाढवा
- योग्य क्वांटायझेशन वापरा
- GPU स्मृती वापर तपासा

#### स्मृती समस्या

**समस्या: स्मृती संपली**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### प्लॅटफॉर्म-विशिष्ट समस्या

#### Windows
- MinGW किंवा Visual Studio कंपायलर वापरा
- योग्य PATH कॉन्फिगरेशन सुनिश्चित करा
- अँटीव्हायरस हस्तक्षेप तपासा

#### macOS
- Apple Silicon साठी Metal सक्षम करा
- सुसंगततेसाठी Rosetta 2 वापरा (आवश्यक असल्यास)
- Xcode कमांड लाइन साधने तपासा

#### Linux
- विकास पॅकेजेस स्थापित करा
- GPU ड्रायव्हर आवृत्त्या तपासा
- CUDA टूलकिट स्थापना सत्यापित करा

## सर्वोत्तम पद्धती

### मॉडेल निवड
1. **तुमच्या हार्डवेअरवर आधारित योग्य क्वांटायझेशन निवडा**
2. **मॉडेलचा आकार आणि गुणवत्ता यामध्ये संतुलन साधा**
3. **तुमच्या विशिष्ट वापर प्रकरणासाठी वेगवेगळ्या मॉडेल्सची चाचणी करा**

### कार्यक्षमता ऑप्टिमायझेशन
1. **GPU प्रवेग वापरा** (उपलब्ध असल्यास)
2. **तुमच्या CPU साठी थ्रेड्सची संख्या ऑप्टिमाइझ करा**
3. **तुमच्या वापर प्रकरणासाठी योग्य संदर्भ आकार सेट करा**
4. **मोठ्या मॉडेल्ससाठी मेमरी मॅपिंग सक्षम करा**

### उत्पादन तैनाती
1. **API प्रवेशासाठी सर्व्हर मोड वापरा**
2. **योग्य त्रुटी हाताळणी अंमलात आणा**
3. **संसाधन वापराचे निरीक्षण करा**
4. **लॉगिंग आणि मॉनिटरिंग सेट करा**

### विकास कार्यप्रवाह
1. **चाचणीसाठी लहान मॉडेल्सपासून सुरुवात करा**
2. **मॉडेल कॉन्फिगरेशनसाठी आवृत्ती नियंत्रण वापरा**
3. **तुमच्या कॉन्फिगरेशन्सचे दस्तऐवजीकरण करा**
4. **वेगवेगळ्या प्लॅटफॉर्मवर चाचणी करा**

### सुरक्षा विचार
1. **इनपुट प्रॉम्प्ट्सची पडताळणी करा**
2. **रेट लिमिटिंग अंमलात आणा**
3. **API एंडपॉइंट्स सुरक्षित करा**
4. **दुरुपयोग नमुन्यांसाठी निरीक्षण करा**

## निष्कर्ष

Llama.cpp विविध हार्डवेअर कॉन्फिगरेशनमध्ये स्थानिक पातळीवर मोठ्या भाषा मॉडेल्स चालविण्यासाठी एक शक्तिशाली आणि कार्यक्षम मार्ग प्रदान करते. तुम्ही AI अनुप्रयोग विकसित करत असाल, संशोधन करत असाल किंवा LLMs सह प्रयोग करत असाल, हे फ्रेमवर्क विविध वापर प्रकरणांसाठी आवश्यक लवचिकता आणि कार्यक्षमता प्रदान करते.

महत्त्वाचे मुद्दे:
- तुमच्या गरजेनुसार योग्य स्थापना पद्धत निवडा
- तुमच्या हार्डवेअर कॉन्फिगरेशनसाठी ऑप्टिमाइझ करा
- मूलभूत वापराने सुरुवात करा आणि हळूहळू प्रगत वैशिष्ट्ये एक्सप्लोर करा
- सुलभ एकत्रीकरणासाठी Python बाइंडिंग्सचा विचार करा
- उत्पादन तैनातीसाठी सर्वोत्तम पद्धतींचे अनुसरण करा

अधिक माहितीसाठी आणि अद्यतनांसाठी, [अधिकृत Llama.cpp रेपॉजिटरी](https://github.com/ggml-org/llama.cpp) ला भेट द्या आणि उपलब्ध सविस्तर दस्तऐवजीकरण आणि समुदाय संसाधनांचा संदर्भ घ्या.

## ➡️ पुढे काय

- [03: Microsoft Olive ऑप्टिमायझेशन सूट](./03.MicrosoftOlive.md)

---

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.