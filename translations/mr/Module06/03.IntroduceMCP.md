<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-17T20:33:00+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "mr"
}
-->
# विभाग 03 - मॉडेल कॉन्टेक्स्ट प्रोटोकॉल (MCP) एकत्रीकरण

## MCP (मॉडेल कॉन्टेक्स्ट प्रोटोकॉल) ची ओळख

मॉडेल कॉन्टेक्स्ट प्रोटोकॉल (MCP) हे एक क्रांतिकारी फ्रेमवर्क आहे जे भाषा मॉडेल्सना बाह्य साधने आणि प्रणालींसोबत मानक पद्धतीने संवाद साधण्याची परवानगी देते. पारंपरिक पद्धतींमध्ये जिथे मॉडेल्स वेगळे असतात, तिथे MCP एक स्पष्ट प्रोटोकॉलद्वारे AI मॉडेल्स आणि वास्तविक जगामध्ये पूल तयार करते.

### MCP म्हणजे काय?

MCP एक संवाद प्रोटोकॉल म्हणून कार्य करते ज्यामुळे भाषा मॉडेल्सना खालील गोष्टी करता येतात:
- बाह्य डेटा स्रोतांशी कनेक्ट होणे
- साधने आणि फंक्शन्स चालवणे
- APIs आणि सेवांसोबत संवाद साधणे
- रिअल-टाइम माहिती मिळवणे
- जटिल मल्टी-स्टेप ऑपरेशन्स करणे

हा प्रोटोकॉल स्थिर भाषा मॉडेल्सना गतिशील एजंट्समध्ये रूपांतरित करतो, जे केवळ टेक्स्ट जनरेशनच्या पलीकडे व्यावहारिक कार्ये करू शकतात.

## लहान भाषा मॉडेल्स (SLMs) MCP मध्ये

लहान भाषा मॉडेल्स AI तैनातीसाठी एक कार्यक्षम दृष्टिकोन सादर करतात, ज्यामुळे अनेक फायदे मिळतात:

### SLMs चे फायदे
- **संसाधन कार्यक्षमता**: कमी संगणकीय आवश्यकता
- **वेगवान प्रतिसाद वेळा**: रिअल-टाइम अनुप्रयोगांसाठी कमी विलंब  
- **खर्च कार्यक्षमता**: किमान इन्फ्रास्ट्रक्चर गरजा
- **गोपनीयता**: डेटा ट्रान्समिशनशिवाय स्थानिक पातळीवर चालवता येते
- **सानुकूलन**: विशिष्ट डोमेनसाठी सुलभ ट्यूनिंग

### MCP सोबत SLMs का चांगले कार्य करतात

SLMs MCP सोबत जोडल्यावर एक शक्तिशाली संयोजन तयार होते जिथे मॉडेलच्या विचार करण्याच्या क्षमतेला बाह्य साधनांनी वाढवले जाते, ज्यामुळे त्यांच्या कमी पॅरामीटर काउंटची भरपाई होते.

## Python MCP SDK ची ओळख

Python MCP SDK MCP-सक्षम अनुप्रयोग तयार करण्यासाठी पाया प्रदान करते. SDK मध्ये समाविष्ट आहे:

- **क्लायंट लायब्ररी**: MCP सर्व्हर्सशी कनेक्ट होण्यासाठी
- **सर्व्हर फ्रेमवर्क**: सानुकूल MCP सर्व्हर्स तयार करण्यासाठी
- **प्रोटोकॉल हँडलर्स**: संवाद व्यवस्थापनासाठी
- **साधन एकत्रीकरण**: बाह्य फंक्शन्स चालवण्यासाठी

## व्यावहारिक अंमलबजावणी: Phi-4 MCP क्लायंट

Microsoft च्या Phi-4 मिनी मॉडेलचा MCP क्षमतांसह एकत्रित वापर करून एक वास्तविक-जगातील अंमलबजावणी पाहूया.

### प्रणाली आर्किटेक्चर

अंमलबजावणी स्तरित आर्किटेक्चरचे अनुसरण करते:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### मुख्य घटक

#### 1. MCP क्लायंट क्लासेस

**BaseMCPClient**: सामान्य कार्यक्षमता प्रदान करणारे अब्स्ट्रॅक्ट फाउंडेशन
- Async कॉन्टेक्स्ट मॅनेजर प्रोटोकॉल
- मानक इंटरफेस परिभाषा
- संसाधन व्यवस्थापन

**Phi4MiniMCPClient**: STDIO-आधारित अंमलबजावणी
- स्थानिक प्रक्रिया संवाद
- मानक इनपुट/आउटपुट हाताळणी
- सबप्रोसेस व्यवस्थापन

**Phi4MiniSSEMCPClient**: सर्व्हर-सेंट इव्हेंट्स अंमलबजावणी
- HTTP स्ट्रीमिंग संवाद
- रिअल-टाइम इव्हेंट हाताळणी
- वेब-आधारित सर्व्हर कनेक्टिव्हिटी

#### 2. LLM एकत्रीकरण

**OllamaClient**: स्थानिक मॉडेल होस्टिंग
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: उच्च-प्रदर्शन सर्व्हिंग
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. साधन प्रक्रिया पाइपलाइन

साधन प्रक्रिया पाइपलाइन MCP साधनांना भाषा मॉडेल्ससाठी सुसंगत स्वरूपात रूपांतरित करते:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## सुरुवात करणे: चरण-दर-चरण मार्गदर्शक

### चरण 1: पर्यावरण सेटअप

आवश्यक डिपेंडन्सी इंस्टॉल करा:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### चरण 2: मूलभूत कॉन्फिगरेशन

आपले पर्यावरण व्हेरिएबल्स सेट करा:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### चरण 3: आपला पहिला MCP क्लायंट चालवणे

**मूलभूत Ollama सेटअप:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM बॅकएंड वापरणे:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**सर्व्हर-सेंट इव्हेंट्स कनेक्शन:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**सानुकूल MCP सर्व्हर:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### चरण 4: प्रोग्रामॅटिक वापर

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## प्रगत वैशिष्ट्ये

### मल्टी-बॅकएंड समर्थन

अंमलबजावणी Ollama आणि vLLM बॅकएंड्स दोन्हीला समर्थन देते, ज्यामुळे आपल्याला आपल्या गरजांनुसार निवड करता येते:

- **Ollama**: स्थानिक विकास आणि चाचणीसाठी चांगले
- **vLLM**: उत्पादन आणि उच्च-थ्रूपुट परिस्थितीसाठी अनुकूल

### लवचिक कनेक्शन प्रोटोकॉल्स

दोन कनेक्शन मोड्स समर्थित आहेत:

**STDIO मोड**: थेट प्रक्रिया संवाद
- कमी विलंब
- स्थानिक साधनांसाठी योग्य
- सोपी सेटअप

**SSE मोड**: HTTP-आधारित स्ट्रीमिंग
- नेटवर्क-सक्षम
- वितरित प्रणालींसाठी चांगले
- रिअल-टाइम अपडेट्स

### साधन एकत्रीकरण क्षमता

सिस्टम विविध साधनांसोबत एकत्रित होऊ शकते:
- वेब ऑटोमेशन (Playwright)
- फाइल ऑपरेशन्स
- API संवाद
- सिस्टम कमांड्स
- सानुकूल फंक्शन्स

## त्रुटी हाताळणी आणि सर्वोत्तम पद्धती

### व्यापक त्रुटी व्यवस्थापन

अंमलबजावणी robust त्रुटी हाताळणी समाविष्ट करते:

**कनेक्शन त्रुटी:**
- MCP सर्व्हर अपयश
- नेटवर्क टाइमआउट्स
- कनेक्टिव्हिटी समस्या

**साधन अंमलबजावणी त्रुटी:**
- साधने अनुपस्थित
- पॅरामीटर सत्यापन
- अंमलबजावणी अपयश

**प्रतिक्रिया प्रक्रिया त्रुटी:**
- JSON पार्सिंग समस्या
- स्वरूप विसंगती
- LLM प्रतिसाद विसंगती

### सर्वोत्तम पद्धती

1. **संसाधन व्यवस्थापन**: Async कॉन्टेक्स्ट मॅनेजर्स वापरा
2. **त्रुटी हाताळणी**: व्यापक try-catch ब्लॉक्स अंमलात आणा
3. **लॉगिंग**: योग्य लॉगिंग स्तर सक्षम करा
4. **सुरक्षा**: इनपुट्स सत्यापित करा आणि आउटपुट्स स्वच्छ करा
5. **प्रदर्शन**: कनेक्शन पूलिंग आणि कॅशिंग वापरा

## वास्तविक-जगातील अनुप्रयोग

### वेब ऑटोमेशन
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### डेटा प्रक्रिया
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API एकत्रीकरण
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## कार्यक्षमता ऑप्टिमायझेशन

### मेमरी व्यवस्थापन
- कार्यक्षम संदेश इतिहास हाताळणी
- योग्य संसाधन साफसफाई
- कनेक्शन पूलिंग

### नेटवर्क ऑप्टिमायझेशन
- Async HTTP ऑपरेशन्स
- कॉन्फिगरेबल टाइमआउट्स
- ग्रेसफुल त्रुटी पुनर्प्राप्ती

### समांतर प्रक्रिया
- नॉन-ब्लॉकिंग I/O
- साधनांचे समांतर अंमलबजावणी
- कार्यक्षम async पॅटर्न्स

## सुरक्षा विचार

### डेटा संरक्षण
- सुरक्षित API की व्यवस्थापन
- इनपुट सत्यापन
- आउटपुट स्वच्छता

### नेटवर्क सुरक्षा
- HTTPS समर्थन
- स्थानिक एंडपॉइंट डीफॉल्ट्स
- सुरक्षित टोकन हाताळणी

### अंमलबजावणी सुरक्षा
- साधन फिल्टरिंग
- सॅंडबॉक्स केलेली वातावरणे
- ऑडिट लॉगिंग

## निष्कर्ष

MCP सह एकत्रित SLMs AI अनुप्रयोग विकासामध्ये एक पद्धतीत बदल दर्शवतात. लहान मॉडेल्सची कार्यक्षमता बाह्य साधनांच्या सामर्थ्यासह एकत्र करून, विकसक संसाधन-कार्यक्षम आणि अत्यंत सक्षम बुद्धिमान प्रणाली तयार करू शकतात.

Phi-4 MCP क्लायंट अंमलबजावणी हे दाखवते की ही एकत्रीकरण प्रत्यक्षात कसे साध्य करता येते, सुसंस्कृत AI-सक्षम अनुप्रयोग तयार करण्यासाठी एक मजबूत पाया प्रदान करते.

महत्त्वाचे मुद्दे:
- MCP भाषा मॉडेल्स आणि बाह्य प्रणालींमधील अंतर भरते
- साधनांनी वाढवलेले SLMs कार्यक्षमता न गमावता कार्यक्षमता प्रदान करतात
- मॉड्यूलर आर्किटेक्चर सुलभ विस्तार आणि सानुकूलन सक्षम करते
- उत्पादन वापरासाठी योग्य त्रुटी हाताळणी आणि सुरक्षा उपाय आवश्यक आहेत

ही ट्यूटोरियल आपले स्वतःचे SLM-सक्षम MCP अनुप्रयोग तयार करण्यासाठी पाया प्रदान करते, ऑटोमेशन, डेटा प्रक्रिया आणि बुद्धिमान प्रणाली एकत्रीकरणासाठी शक्यता उघडते.

---

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.