<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-09-18T16:17:21+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "cs"
}
-->
# Sekce 3: Základy rodiny Gemma

Modelová rodina Gemma představuje komplexní přístup Googlu k open-source velkým jazykovým modelům a multimodální AI, ukazující, že dostupné modely mohou dosáhnout pozoruhodného výkonu a zároveň být nasaditelné v různých scénářích, od mobilních zařízení po podnikové pracovní stanice. Je důležité pochopit, jak rodina Gemma umožňuje výkonné AI schopnosti s flexibilními možnostmi nasazení, přičemž si zachovává konkurenceschopný výkon a odpovědné praktiky AI.

## Úvod

V tomto tutoriálu prozkoumáme modelovou rodinu Gemma od Googlu a její základní koncepty. Pokryjeme evoluci rodiny Gemma, inovativní metodiky tréninku, které činí modely Gemma efektivními, klíčové varianty v rodině a praktické aplikace v různých scénářích nasazení.

## Cíle učení

Na konci tohoto tutoriálu budete schopni:

- Porozumět filozofii designu a evoluci modelové rodiny Gemma od Googlu
- Identifikovat klíčové inovace, které umožňují modelům Gemma dosáhnout vysokého výkonu napříč různými velikostmi parametrů
- Rozpoznat výhody a omezení různých variant modelů Gemma
- Aplikovat znalosti o modelech Gemma k výběru vhodných variant pro reálné scénáře

## Porozumění modernímu prostředí AI modelů

Prostředí AI se výrazně vyvinulo, přičemž různé organizace sledují různé přístupy k vývoji jazykových modelů. Zatímco některé se zaměřují na proprietární uzavřené modely dostupné pouze prostřednictvím API, jiné kladou důraz na open-source přístupnost a transparentnost. Tradiční přístup zahrnuje buď masivní proprietární modely s průběžnými náklady, nebo open-source modely, které mohou vyžadovat značné technické znalosti pro nasazení.

Tento paradigma vytváří výzvy pro organizace, které hledají výkonné AI schopnosti a zároveň si chtějí zachovat kontrolu nad svými daty, náklady a flexibilitou nasazení. Konvenční přístup často vyžaduje volbu mezi špičkovým výkonem a praktickými úvahami o nasazení.

## Výzva dostupné AI excelence

Potřeba vysoce kvalitní, dostupné AI se stává stále důležitější v různých scénářích. Zvažte aplikace vyžadující flexibilní možnosti nasazení pro různé organizační potřeby, nákladově efektivní implementace, kde náklady na API mohou být významné, multimodální schopnosti pro komplexní porozumění nebo specializované nasazení na mobilních a edge zařízeních.

### Klíčové požadavky na nasazení

Moderní nasazení AI čelí několika základním požadavkům, které omezují praktickou použitelnost:

- **Přístupnost**: Dostupnost open-source pro transparentnost a přizpůsobení
- **Nákladová efektivita**: Rozumné požadavky na výpočetní výkon pro různé rozpočty
- **Flexibilita**: Různé velikosti modelů pro různé scénáře nasazení
- **Multimodální porozumění**: Schopnosti zpracování obrazu, textu a zvuku
- **Nasazení na edge zařízeních**: Optimalizovaný výkon na mobilních a omezených zařízeních

## Filozofie modelů Gemma

Rodina modelů Gemma představuje komplexní přístup Googlu k vývoji AI modelů, který klade důraz na open-source přístupnost, multimodální schopnosti a praktické nasazení, přičemž si zachovává konkurenceschopné charakteristiky výkonu. Modely Gemma toho dosahují prostřednictvím různých velikostí modelů, vysoce kvalitních metodik tréninku odvozených z výzkumu Gemini a specializovaných variant pro různé domény a scénáře nasazení.

Rodina Gemma zahrnuje různé přístupy navržené tak, aby poskytovaly možnosti napříč spektrem výkonu a efektivity, umožňující nasazení od mobilních zařízení po podnikové servery a zároveň poskytující smysluplné AI schopnosti. Cílem je demokratizovat přístup k vysoce kvalitní AI technologii a zároveň poskytovat flexibilitu v možnostech nasazení.

### Základní principy designu Gemma

Modely Gemma jsou postaveny na několika základních principech, které je odlišují od jiných rodin jazykových modelů:

- **Open Source na prvním místě**: Kompletní transparentnost a přístupnost pro výzkum i komerční použití
- **Vývoj založený na výzkumu**: Postaveno na stejném výzkumu a technologii, která pohání modely Gemini
- **Škálovatelná architektura**: Různé velikosti modelů odpovídající různým výpočetním požadavkům
- **Odpovědná AI**: Integrovaná bezpečnostní opatření a odpovědné praktiky vývoje

## Klíčové technologie umožňující rodinu Gemma

### Pokročilé metodiky tréninku

Jedním z definujících aspektů rodiny Gemma je sofistikovaný přístup k tréninku odvozený z výzkumu Gemini od Googlu. Modely Gemma využívají destilaci z větších modelů, učení posilováním na základě zpětné vazby od lidí (RLHF) a techniky slučování modelů k dosažení vylepšeného výkonu v matematice, kódování a následování instrukcí.

Proces tréninku zahrnuje destilaci z větších modelů pro instrukce, učení posilováním na základě zpětné vazby od lidí (RLHF) pro sladění s lidskými preferencemi, učení posilováním na základě zpětné vazby od strojů (RLMF) pro matematické uvažování a učení posilováním na základě zpětné vazby z provádění (RLEF) pro schopnosti kódování.

### Multimodální integrace a porozumění

Nedávné modely Gemma zahrnují sofistikované multimodální schopnosti, které umožňují komplexní porozumění napříč různými typy vstupů:

**Integrace vize a jazyka (Gemma 3)**: Gemma 3 dokáže zpracovávat text i obrázky současně, což jí umožňuje analyzovat obrázky, odpovídat na otázky o vizuálním obsahu, extrahovat text z obrázků a rozumět složitým vizuálním datům.

**Zpracování zvuku (Gemma 3n)**: Gemma 3n obsahuje pokročilé zvukové schopnosti, včetně automatického rozpoznávání řeči (ASR) a automatického překladu řeči (AST), s obzvláště silným výkonem při překladu mezi angličtinou a španělštinou, francouzštinou, italštinou a portugalštinou.

**Zpracování prokládaných vstupů**: Modely Gemma podporují prokládané vstupy napříč modalitami, což umožňuje porozumění složitým multimodálním interakcím, kde text, obrázky a zvuk mohou být zpracovány společně.

### Architektonické inovace

Rodina Gemma zahrnuje několik architektonických optimalizací navržených pro výkon i efektivitu:

**Rozšíření kontextového okna**: Modely Gemma 3 mají kontextové okno o velikosti 128K tokenů, což je 16x více než u předchozích modelů Gemma, umožňující zpracování obrovského množství informací, včetně více dokumentů nebo stovek obrázků.

**Architektura zaměřená na mobilní zařízení (Gemma 3n)**: Gemma 3n využívá technologii Per-Layer Embeddings (PLE) a architekturu MatFormer, což umožňuje větším modelům běžet s paměťovými nároky srovnatelnými s menšími tradičními modely.

**Schopnosti volání funkcí**: Gemma 3 podporuje volání funkcí, což umožňuje vývojářům vytvářet rozhraní pro programování na základě přirozeného jazyka a vytvářet inteligentní automatizační systémy.
- Gemma 3 přináší výkonné schopnosti pro vývojáře s pokročilými textovými a vizuálními schopnostmi, podporující vstupy z obrázků a textu pro multimodální porozumění  
- Gemma 3n se řadí mezi nejlepší jak mezi populárními proprietárními, tak otevřenými modely podle Elo skóre v Chatbot Areně, což naznačuje silnou uživatelskou preferenci  

**Úspěchy v efektivitě:**  
- Modely Gemma 3 dokážou zpracovat vstupy až do 128K tokenů, což je 16x větší kontextové okno než u předchozích modelů Gemma  
- Gemma 3n využívá Per-Layer Embeddings (PLE), které výrazně snižují využití RAM při zachování schopností větších modelů  

**Optimalizace pro mobilní zařízení:**  
- Gemma 3n E2B funguje s pouhými 2GB paměti, zatímco E4B vyžaduje pouze 3GB, přestože má počet parametrů 5B a 8B  
- AI schopnosti v reálném čase přímo na mobilních zařízeních s důrazem na soukromí a offline provoz  

**Rozsah trénování:**  
- Gemma 3 byla trénována na 2T tokenů pro 1B, 4T pro 4B, 12T pro 12B a 14T tokenů pro modely 27B pomocí Google TPU a JAX Framework  

### Porovnávací tabulka modelů  

| Řada modelů | Rozsah parametrů | Délka kontextu | Klíčové přednosti | Nejlepší využití |  
|--------------|------------------|----------------|-------------------|------------------|  
| **Gemma 3** | 1B-27B | 128K | Multimodální porozumění, volání funkcí | Obecné aplikace, úkoly spojené s vizí a jazykem |  
| **Gemma 3n** | E2B (5B), E4B (8B) | Proměnlivá | Optimalizace pro mobilní zařízení, zpracování zvuku | Mobilní aplikace, edge computing, AI v reálném čase |  
| **Gemma 2.5** | 0.5B-72B | 32K-128K | Vyvážený výkon, vícejazyčnost | Nasazení v produkci, stávající pracovní postupy |  
| **Gemma-VL** | Různé | Proměnlivá | Specializace na vizi a jazyk | Analýza obrázků, vizuální odpovědi na otázky |  

## Průvodce výběrem modelu  

### Pro základní aplikace  
- **Gemma 3-1B**: Jednoduché textové úkoly, základní mobilní aplikace  
- **Gemma 3-4B**: Vyvážený výkon s multimodální podporou pro obecné použití  

### Pro multimodální aplikace  
- **Gemma 3-4B/12B**: Porozumění obrázkům, vizuální odpovědi na otázky  
- **Gemma 3n**: Multimodální mobilní aplikace se schopnostmi zpracování zvuku  

### Pro mobilní a edge nasazení  
- **Gemma 3n E2B**: Zařízení s omezenými zdroji, mobilní AI v reálném čase  
- **Gemma 3n E4B**: Vylepšený mobilní výkon se schopnostmi zpracování zvuku  

### Pro podnikové nasazení  
- **Gemma 3-12B/27B**: Vysoce výkonné porozumění jazyku a vizím  
- **Schopnosti volání funkcí**: Budování inteligentních automatizačních systémů  

### Pro globální aplikace  
- **Jakýkoli Gemma 3 variant**: Podpora 140+ jazyků s kulturním porozuměním  
- **Gemma 3n**: Mobilní aplikace zaměřené na globální použití s překlady zvuku  

## Platformy pro nasazení a dostupnost  

### Cloudové platformy  
- **Vertex AI**: Kompletní MLOps schopnosti s bezserverovým zážitkem  
- **Google Kubernetes Engine (GKE)**: Škálovatelné nasazení kontejnerů pro složité pracovní zátěže  
- **Google GenAI API**: Přímý přístup k API pro rychlé prototypování  
- **NVIDIA API Catalog**: Optimalizovaný výkon na GPU NVIDIA  

### Lokální vývojové frameworky  
- **Hugging Face Transformers**: Standardní integrace pro vývoj  
- **Ollama**: Zjednodušené lokální nasazení a správa  
- **vLLM**: Vysoce výkonné nasazení pro produkci  
- **Gemma.cpp**: Optimalizované provádění na CPU  
- **Google AI Edge**: Optimalizace pro mobilní a edge nasazení  

### Výukové zdroje  
- **Google AI Studio**: Vyzkoušejte modely Gemma během několika kliknutí  
- **Kaggle a Hugging Face**: Stáhněte si váhy modelů a příklady od komunity  
- **Technické zprávy**: Komplexní dokumentace a výzkumné práce  
- **Fóra komunity**: Aktivní podpora a diskuse  

### Začínáme s modely Gemma  

#### Vývojové platformy  
1. **Google AI Studio**: Začněte s experimentováním na webu  
2. **Hugging Face Hub**: Prozkoumejte modely a implementace od komunity  
3. **Lokální nasazení**: Použijte Ollama nebo Transformers pro vývoj  

#### Výuková cesta  
1. **Porozumění základním konceptům**: Studujte multimodální schopnosti a možnosti nasazení  
2. **Experimentování s variantami**: Vyzkoušejte různé velikosti modelů a specializované verze  
3. **Praxe implementace**: Nasazujte modely v prostředích pro vývoj  
4. **Optimalizace pro produkci**: Doladění pro specifické případy použití a platformy  

#### Nejlepší postupy  
- **Začněte s menším modelem**: Začněte s Gemma 3-4B pro počáteční vývoj a testování  
- **Používejte oficiální šablony**: Aplikujte správné šablony pro chat pro optimální výsledky  
- **Sledujte zdroje**: Sledujte využití paměti a výkon při inferenci  
- **Zvažte specializaci**: Vyberte vhodné varianty pro multimodální nebo mobilní potřeby  

## Pokročilé vzory použití  

### Příklady doladění  

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# Load Gemma model for fine-tuning
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA for efficient fine-tuning
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# Apply LoRA to model
model = get_peft_model(model, peft_config)

# Training configuration optimized for Gemma
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# Load and prepare dataset
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# Start fine-tuning
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```  

### Specializované inženýrství promptů  

**Pro multimodální úkoly:**  
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# Example usage for image analysis
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```  

**Pro volání funkcí s kontextem:**  
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# Example function definitions
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# Create function calling prompt
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```  

### Vícejazyčné aplikace s kulturním kontextem  

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# Example culturally-aware usage
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```  

### Vzory nasazení v produkci  

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # Load multimodal variant
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load text-only model
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Optimize for inference
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # Text-only processing
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # Process images
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # Process audio (Gemma 3n specific)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # Multimodal processing
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # Use processor for multimodal input
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # Fallback to text-only
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # Text-only processing
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # Extract generated text
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # Check if model is loaded
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # Test basic functionality
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # Synchronous test for health check
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # Check memory usage
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                health_status["memory_usage"] = memory_used
            
            # Evaluate response time
            if response_time > 5.0:  # seconds
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# Example production usage
async def production_example():
    # Initialize production service
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # Health check
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # Text-only generation
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # Multimodal generation (if supported)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # Example with image
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# Run production example
# asyncio.run(production_example())
```  

## Strategie optimalizace výkonu  

### Optimalizace paměti  

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Memory-efficient loading strategies for Gemma models
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # 4-bit quantization for maximum memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # 8-bit quantization for balanced performance
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # Full precision for maximum performance
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# Example usage
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```  

### Optimalizace inferencí  

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # Enable optimized attention mechanisms
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # Set optimal threading for CPU operations
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # Enable JIT compilation for repeated patterns
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # Optimize model for inference
        self.model.eval()
        
        # Enable torch.compile if available (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # Format input
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Use optimized attention backend
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # Extract response
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # Format all inputs
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # Tokenize batch with padding
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # Extract all responses
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# Example usage
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# Single optimized generation
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```  

## Nejlepší postupy a pokyny  

### Bezpečnost a soukromí  

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # Only enable for trusted models
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # Remove potentially harmful patterns
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # Limit length to prevent resource exhaustion
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 hour in seconds
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # Clean old requests outside the time window
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # Check if limit exceeded
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # Log current request
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # Reasonable conversation limit
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # Validate role
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # Sanitize content
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # This is a simplified example - in production, use more sophisticated filtering
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # Add more as needed for your use case
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # Rate limiting
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # Input validation
            validated_messages = self._validate_messages(messages)
            
            # Content filtering
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # Log request (with hashed content for privacy)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # Validate token limit
            max_allowed_tokens = min(max_tokens, 1024)
            
            # Generate response
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # Filter response content
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # Log successful generation
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 hour
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # Count recent requests
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # Calculate reset time (when oldest request expires)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# Example secure usage
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# Safe generation
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# Check usage statistics
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```  

### Monitorování a hodnocení  

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # seconds
            "max_memory_usage": 8192,   # MB
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # GPU metrics
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # Format input
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # Generate response
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # Calculate final metrics
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # Simple quality assessment (in production, use more sophisticated methods)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # Scale appropriately
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # Short text, assume reasonably coherent
        
        # Simple heuristic: check for reasonable sentence length
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # Without reference, use simple heuristics
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # With reference, calculate similarity (simplified)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # Last 10 requests
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # Check response time
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check memory usage
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # Check tokens per second
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # Check success rate
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# Example monitoring usage
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # Initialize monitoring
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # Load model for testing
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # Test various scenarios
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # Measure performance
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # Generate actual response for quality evaluation
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # Evaluate quality
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # Get comprehensive summary
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # Export metrics
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# Run monitoring example
# monitoring_example()
```  

## Závěr  

Rodina modelů Gemma představuje komplexní přístup Googlu k demokratizaci AI technologie při zachování konkurenceschopného výkonu napříč různými aplikacemi a scénáři nasazení. Díky závazku k otevřenému přístupu, multimodálním schopnostem a inovativním architektonickým návrhům umožňuje Gemma organizacím a vývojářům využívat výkonné AI schopnosti bez ohledu na jejich zdroje nebo specifické požadavky.  

### Klíčové poznatky  

**Excelence v open-source:** Gemma ukazuje, že open-source modely mohou dosáhnout výkonu konkurujícího proprietárním alternativám, přičemž poskytují transparentnost, přizpůsobení a kontrolu nad nasazením AI.  

**Multimodální inovace:** Integrace textových, vizuálních a zvukových schopností v Gemma 3 a Gemma 3n představuje významný pokrok v dostupné multimodální AI, umožňující komplexní porozumění různým typům vstupů.  

**Architektura zaměřená na mobilní zařízení:** Průlomová technologie Per-Layer Embeddings (PLE) a optimalizace pro mobilní zařízení u Gemma 3n ukazuje, že výkonná AI může efektivně fungovat na zařízeních s omezenými zdroji bez ztráty schopností.  

**Škálovatelné nasazení:** Rozsah od 1B do 27B parametrů, se specializovanými mobilními variantami, umožňuje nasazení napříč celým spektrem výpočetních prostředí při zachování konzistentní kvality a výkonu.  

**Odpovědná integrace AI:** Vestavěná bezpečnostní opatření prostřednictvím ShieldGemma 2 a odpovědné vývojové postupy zajišťují, že výkonné AI schopnosti mohou být nasazeny bezpečně a eticky.  

### Výhled do budoucna  

Jak se rodina Gemma bude dále vyvíjet, můžeme očekávat:  

**Vylepšené mobilní schopnosti:** Další optimalizace pro mobilní a edge nasazení s integrací architektury Gemma 3n do hlavních platforem jako Android a Chrome.  

**Rozšířené multimodální porozumění:** Pokračující pokrok v integraci vizí, jazyka a zvuku pro komplexnější AI zážitky.  

**Zlepšená efektivita:** Neustálé architektonické inovace pro lepší poměr výkonu na parametr a snížené výpočetní požadavky.  

**Širší integrace ekosystému:** Rozšířená podpora napříč vývojovými frameworky, cloudovými platformami a nástroji pro nasazení pro bezproblémovou integraci do stávajících pracovních postupů.  

**Růst komunity:** Pokračující expanze Gemmaverse s modely, nástroji a aplikacemi vytvořenými komunitou, které rozšiřují základní schopnosti.  

### Další kroky  

Ať už vytváříte mobilní aplikace s AI schopnostmi v reálném čase, vyvíjíte multimodální vzdělávací nástroje, budujete inteligentní automatizační systémy nebo pracujete na globálních aplikacích vyžadujících vícejazyčnou podporu, rodina Gemma poskytuje škálovatelné řešení se silnou podporou komunity a komplexní dokumentací.  

**Doporučení pro začátek:**  
1. **Experimentujte s Google AI Studio** pro okamžitou praktickou zkušenost  
2. **Stáhněte si modely z Hugging Face** pro lokální vývoj a přizpůsobení  
3. **Prozkoumejte specializované varianty** jako Gemma 3n pro mobilní aplikace  
4. **Implementujte multimodální schopnosti** pro komplexní AI zážitky  
5. **Dodržujte bezpečnostní postupy** pro nasazení v produkci  

**Pro mobilní vývoj:** Začněte s Gemma 3n E2B pro efektivní nasazení se schopnostmi zpracování zvuku a vizí.  

**Pro podnikové aplikace:** Zvažte modely Gemma 3-12B nebo 27B pro maximální schopnosti s voláním funkcí a pokročilým porozuměním.  

**Pro globální aplikace:** Využijte podporu 140+ jazyků Gemma s kulturně citlivým inženýrstvím promptů.  

**Pro specializované případy použití:** Prozkoumejte přístupy doladění a techniky optimalizace pro konkrétní domény.  

### 🔮 Demokratizace AI  

Rodina Gemma představuje budoucnost vývoje AI, kde výkonné, schopné modely jsou dostupné všem, od jednotlivých vývojářů po velké podniky. Kombinací špičkového výzkumu s otevřeným přístupem vytvořil Google základ, který umožňuje inovace napříč všemi sektory a měřítky.  

Úspěch Gemma s více než 100 miliony stažení a 60 000+ komunitními variantami ukazuje sílu otevřené spolupráce při pokroku AI technologie. Jak postupujeme vpřed, rodina Gemma bude i nadále sloužit jako katalyzátor inovací v AI, umožňující vývoj aplikací, které byly dříve možné pouze s proprietárními, drahými modely.  

Budoucnost AI je otevřená, dostupná a výkonná – a rodina Gemma vede cestu k realizaci této vize.  

## Další zdroje  

**Oficiální dokumentace a modely:**  
- **Google AI Studio**: [Vyzkoušejte modely Gemma přímo](https://aistudio.google.com)  
- **Hugging Face Collections:**  
  - [Gemma 3 Release](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)  
  - [Gemma 3n Preview](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)  
- **Google AI Developer Documentation**: [Komplexní průvodce Gemma](https://ai.google.dev/gemma)  
- **Vertex AI Documentation**: [Průvodce nasazením v podniku](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)  

**Technické zdroje:**  
- **Výzkumné práce a technické zprávy**: [Publikace Google DeepMind](https://deepmind.google/models/gemma/)  
- **Blogy pro vývojáře**: [Nejnovější oznámení a návody](https://developers.googleblog.com)  
- **Modelové karty**: Podrobné technické specifikace a výkonnostní benchmarky  

**Komunita a podpora:**  
- **Hugging Face Community**: Aktivní diskuse a příklady od komunity  
- **GitHub Repositories**: Open-source implementace a nástroje  
- **Fóra pro vývojáře**: Podpora komunity Google AI Developer  
- **Stack Overflow**: Otázky s tagy a řešení od komunity  

**Vývojové nástroje:**  
- **Ollama**: [Jednoduché lokální nasazení](https://ollama.ai)  
- **vLLM**: [Vysoce výkonné nasazení](https://github.com/vllm-project/vllm)  
- **Transformers Library**: [Integrace Hugging Face](https://huggingface.co/docs/transformers)  
- **Google AI Edge**: Optimalizace pro mobilní a edge nasazení  

**Výukové cesty:**  
- **Začátečník**: Začněte s Google AI Studio → Příklady Hugging Face → Lokální nasazení  
- **Vývojář**: Integrace Transformers → Vlastní aplikace → Nasazení v produkci  
- **Výzkumník**: Technické práce → Doladění → Nové aplikace  
- **Podnik**: Nasazení Vertex AI → Implementace bezpečnosti → Optimalizace škálování  

Rodina modelů Gemma nepředstavuje jen sbírku AI modelů, ale kompletní ekosystém pro budování budoucnosti dostupných, výkonných a odpovědných AI aplikací. Začněte objevovat ještě dnes a připojte se k rostoucí komunitě vývojářů a výzkumníků, kteří posouvají hranice toho, co je možné s open-source AI.  

## Další zdroje  

### Oficiální dokumentace  
- Technická dokumentace Google Gemma  
- Modelové karty a pokyny k použití  
- Průvodce implementací odpovědné AI  
- Průvodce integrací Google Vertex AI  

### Vývojové nástroje  
- Google AI Studio pro nasazení v cloudu  
- Hugging Face Transformers pro integraci modelů  
- vLLM pro vysoce výkonné nasazení  
- Gemma.cpp pro optimalizované inferenční výpočty na CPU  

### Výukové zdroje  
- Technické práce Gemma 3 a Gemma 3n  
- Blog Google AI a návody  
- Průvodce optimalizací a kvantizací modelů  
- Fóra komunity a diskusní skupiny  

## Výukové výsledky  

Po dokončení tohoto modulu budete schopni:  

1. Vysvětlit architektonické výhody rodiny modelů Gemma a její open-source přístup  
2. Vybrat vhodnou variantu Gemma na základě specifických požadavků aplikace a hardwarových omezení  
3. Implementovat modely Gemma v různých scénářích nasazení od mobilních zařízení po cloud s optimalizovanými konfiguracemi  
4. Aplikovat techniky kvantizace a optimalizace ke zlepšení výkonu model

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace doporučujeme profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.