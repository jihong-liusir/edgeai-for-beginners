<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a35d3b47e6ae98ad9b3e89fb73917e90",
  "translation_date": "2025-09-18T16:47:07+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "cs"
}
-->
# Sekce 1: Základy EdgeAI

EdgeAI představuje zásadní změnu v nasazení umělé inteligence, přinášející schopnosti AI přímo na koncová zařízení, místo spoléhání se výhradně na zpracování v cloudu. Je důležité pochopit, jak EdgeAI umožňuje lokální zpracování AI na zařízeních s omezenými zdroji, přičemž zachovává přiměřený výkon a řeší výzvy jako soukromí, latence a offline schopnosti.

## Úvod

V této lekci prozkoumáme EdgeAI a její základní koncepty. Pokryjeme tradiční paradigma výpočetní AI, výzvy spojené s edge computingem, klíčové technologie umožňující EdgeAI a praktické aplikace napříč různými odvětvími.

## Cíle učení

Na konci této lekce budete schopni:

- Porozumět rozdílu mezi tradičním přístupem AI založeným na cloudu a přístupem EdgeAI.
- Identifikovat klíčové technologie umožňující zpracování AI na koncových zařízeních.
- Rozpoznat výhody a omezení implementací EdgeAI.
- Aplikovat znalosti EdgeAI na reálné scénáře a případy použití.

## Porozumění tradičnímu paradigmatu výpočetní AI

Tradičně se generativní AI aplikace spoléhají na vysoce výkonnou výpočetní infrastrukturu pro efektivní provoz velkých jazykových modelů (LLMs). Organizace obvykle nasazují tyto modely na GPU clusterech v cloudových prostředích, přičemž jejich schopnosti jsou přístupné prostřednictvím API rozhraní.

Tento centralizovaný model funguje dobře pro mnoho aplikací, ale má inherentní omezení v kontextu edge computingu. Tradiční přístup zahrnuje odesílání uživatelských dotazů na vzdálené servery, jejich zpracování pomocí výkonného hardwaru a vrácení výsledků přes internet. Zatímco tato metoda poskytuje přístup k nejmodernějším modelům, vytváří závislosti na internetovém připojení, přináší problémy s latencí a vyvolává otázky ohledně soukromí při přenosu citlivých dat na externí servery.

Existují některé základní koncepty, které je třeba pochopit při práci s tradičními paradigmaty výpočetní AI, konkrétně:

- **☁️ Zpracování v cloudu**: AI modely běží na výkonné serverové infrastruktuře s vysokými výpočetními zdroji.
- **🔌 Přístup přes API**: Aplikace přistupují k schopnostem AI prostřednictvím vzdálených API volání místo lokálního zpracování.
- **🎛️ Centralizovaná správa modelů**: Modely jsou udržovány a aktualizovány centrálně, což zajišťuje konzistenci, ale vyžaduje síťové připojení.
- **📈 Škálovatelnost zdrojů**: Cloudová infrastruktura se může dynamicky škálovat, aby zvládla různé výpočetní požadavky.

## Výzvy edge computingu

Koncová zařízení, jako jsou notebooky, mobilní telefony a zařízení Internetu věcí (IoT), například Raspberry Pi a NVIDIA Orin Nano, představují jedinečná omezení v oblasti výpočetní kapacity. Tato zařízení mají obvykle omezený výpočetní výkon, paměť a energetické zdroje ve srovnání s datovými centry.

Provoz tradičních LLMs na těchto zařízeních byl historicky náročný kvůli těmto hardwarovým omezením. Nicméně potřeba zpracování AI na koncových zařízeních se stává stále důležitější v různých scénářích. Zvažte situace, kdy je internetové připojení nespolehlivé nebo nedostupné, například na vzdálených průmyslových lokalitách, v dopravních prostředcích nebo v oblastech se špatným pokrytím sítě. Navíc aplikace vyžadující vysoké bezpečnostní standardy, jako jsou zdravotnické přístroje, finanční systémy nebo vládní aplikace, mohou potřebovat zpracovávat citlivá data lokálně, aby zachovaly soukromí a splnily požadavky na dodržování předpisů.

### Klíčová omezení edge computingu

Prostředí edge computingu čelí několika základním omezením, která tradiční cloudová řešení AI neřeší:

- **Omezený výpočetní výkon**: Koncová zařízení mají obvykle méně CPU jader a nižší taktovací frekvence ve srovnání s hardwarem serverové třídy.
- **Paměťová omezení**: Dostupná RAM a kapacita úložiště jsou na koncových zařízeních výrazně sníženy.
- **Energetická omezení**: Zařízení napájená bateriemi musí vyvážit výkon s energetickou spotřebou pro dlouhodobý provoz.
- **Tepelné řízení**: Kompaktní formáty omezují možnosti chlazení, což ovlivňuje udržitelný výkon při zátěži.

## Co je EdgeAI?

### Koncept: Definice EdgeAI

EdgeAI označuje nasazení a provádění algoritmů umělé inteligence přímo na koncových zařízeních—fyzickém hardwaru, který existuje na "okraji" sítě, blízko místa, kde jsou data generována a sbírána. Tato zařízení zahrnují chytré telefony, IoT senzory, chytré kamery, autonomní vozidla, nositelná zařízení a průmyslové vybavení. Na rozdíl od tradičních AI systémů, které se spoléhají na cloudové servery pro zpracování, EdgeAI přináší inteligenci přímo ke zdroji dat.

V jádru jde u EdgeAI o decentralizaci zpracování AI, přesun od centralizovaných datových center k distribuci napříč rozsáhlou sítí zařízení, která tvoří náš digitální ekosystém. To představuje zásadní architektonický posun v tom, jak jsou AI systémy navrhovány a nasazovány.

Klíčové konceptuální pilíře EdgeAI zahrnují:

- **Lokální zpracování**: Výpočty probíhají fyzicky blízko místa, kde data vznikají.
- **Decentralizovaná inteligence**: Schopnosti rozhodování jsou rozděleny mezi více zařízení.
- **Suverenita dat**: Informace zůstávají pod lokální kontrolou, často nikdy neopouštějí zařízení.
- **Autonomní provoz**: Zařízení mohou fungovat inteligentně bez nutnosti neustálého připojení.
- **Vestavěná AI**: Inteligence se stává nedílnou součástí běžných zařízení.

### Vizualizace architektury EdgeAI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                  │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                      │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────────────────────────────────────────┐   Direct Response   ┌───────────┐
│              Edge Devices with Embedded AI        │───────────────────>│ End Users │
│  ┌─────────┐  ┌──────────────┐  ┌──────────────┐ │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │ │
│  └─────────┘  └──────────────┘  └──────────────┘ │
└──────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI představuje zásadní změnu v nasazení umělé inteligence, přinášející schopnosti AI přímo na koncová zařízení místo spoléhání se výhradně na zpracování v cloudu. Tento přístup umožňuje provoz AI modelů lokálně na zařízeních s omezenými výpočetními zdroji, poskytující schopnosti inferencí v reálném čase bez nutnosti neustálého připojení k internetu.

EdgeAI zahrnuje různé technologie a techniky navržené tak, aby AI modely byly efektivnější a vhodné pro nasazení na zařízeních s omezenými zdroji. Cílem je zachovat přiměřený výkon při výrazném snížení výpočetních a paměťových požadavků AI modelů.

Podívejme se na základní přístupy, které umožňují implementace EdgeAI napříč různými typy zařízení a případy použití.

### Základní principy EdgeAI

EdgeAI je postaveno na několika základních principech, které jej odlišují od tradiční AI založené na cloudu:

- **Lokální zpracování**: Inferenční AI probíhá přímo na koncovém zařízení bez nutnosti externího připojení.
- **Optimalizace zdrojů**: Modely jsou optimalizovány specificky pro hardwarová omezení cílových zařízení.
- **Výkon v reálném čase**: Zpracování probíhá s minimální latencí pro časově citlivé aplikace.
- **Soukromí jako základ**: Citlivá data zůstávají na zařízení, což zvyšuje bezpečnost a dodržování předpisů.

## Klíčové technologie umožňující EdgeAI

### Kvantizace modelů

Jednou z nejdůležitějších technik v EdgeAI je kvantizace modelů. Tento proces zahrnuje snížení přesnosti parametrů modelu, obvykle z 32bitových čísel s plovoucí desetinnou čárkou na 8bitová celá čísla nebo dokonce formáty s nižší přesností. Přestože toto snížení přesnosti může působit znepokojivě, výzkum ukázal, že mnoho AI modelů si dokáže zachovat svůj výkon i při výrazně snížené přesnosti.

Kvantizace funguje tak, že mapuje rozsah hodnot s plovoucí desetinnou čárkou na menší sadu diskrétních hodnot. Například místo použití 32 bitů k reprezentaci každého parametru může kvantizace použít pouze 8 bitů, což vede ke čtyřnásobnému snížení paměťových požadavků a často k rychlejším časům inferencí.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Různé techniky kvantizace zahrnují:

- **Post-Training Quantization (PTQ)**: Aplikováno po trénování modelu bez nutnosti opětovného trénování.
- **Quantization-Aware Training (QAT)**: Zahrnuje efekty kvantizace během trénování pro lepší přesnost.
- **Dynamická kvantizace**: Kvantizuje váhy na int8, ale aktivace počítá dynamicky.
- **Statická kvantizace**: Předem vypočítává všechny parametry kvantizace pro váhy i aktivace.

Pro nasazení EdgeAI je výběr vhodné strategie kvantizace závislý na konkrétní architektuře modelu, požadavcích na výkon a hardwarových schopnostech cílového zařízení.

### Komprese a optimalizace modelů

Kromě kvantizace pomáhají různé techniky komprese snížit velikost modelu a výpočetní požadavky. Patří sem:

**Pruning**: Tato technika odstraňuje nepotřebné spojení nebo neurony z neuronových sítí. Identifikací a eliminací parametrů, které málo přispívají k výkonu modelu, může pruning výrazně snížit velikost modelu při zachování přesnosti.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Knowledge Distillation**: Tento přístup zahrnuje trénování menšího "studentského" modelu, aby napodoboval chování většího "učitelského" modelu. Studentský model se učí přibližovat výstupy učitele, často dosahuje podobného výkonu s výrazně menším počtem parametrů.

**Optimalizace architektury modelu**: Výzkumníci vyvinuli specializované architektury navržené specificky pro nasazení na koncových zařízeních, jako jsou MobileNets, EfficientNets a další lehké architektury, které vyvažují výkon s výpočetní efektivitou.

### Malé jazykové modely (SLMs)

Vznikajícím trendem v EdgeAI je vývoj malých jazykových modelů (SLMs). Tyto modely jsou navrženy od základu tak, aby byly kompaktní a efektivní, přičemž stále poskytují smysluplné schopnosti přirozeného jazyka. SLMs toho dosahují prostřednictvím pečlivých architektonických voleb, efektivních tréninkových technik a zaměřeného tréninku na specifické domény nebo úkoly.

Na rozdíl od tradičních přístupů, které zahrnují kompresi velkých modelů, jsou SLMs často trénovány na menších datových sadách a optimalizovaných architekturách navržených specificky pro nasazení na koncových zařízeních. Tento přístup může vést k modelům, které jsou nejen menší, ale také efektivnější pro specifické případy použití.

## Hardwarová akcelerace pro EdgeAI

Moderní koncová zařízení stále častěji zahrnují specializovaný hardware navržený k akceleraci AI úloh:

### Neuronové procesory (NPUs)

NPUs jsou specializované procesory navržené specificky pro výpočty neuronových sítí. Tyto čipy mohou provádět úlohy inferencí AI mnohem efektivněji než tradiční CPU, často s nižší spotřebou energie. Mnoho moderních chytrých telefonů, notebooků a IoT zařízení nyní zahrnuje NPUs, aby umožnily zpracování AI přímo na zařízení.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Zařízení s NPUs zahrnují:

- **Apple**: Čipy řady A a M s Neural Engine
- **Qualcomm**: Procesory Snapdragon s Hexagon DSP/NPU
- **Samsung**: Procesory Exynos s NPU
- **Intel**: Movidius VPUs a akcelerátory Habana Labs
- **Microsoft**: Windows Copilot+ PC s NPUs

### 🎮 Akcelerace pomocí GPU

Zatímco koncová zařízení nemusí mít výkonné GPU jako datová centra, mnoho z nich stále zahrnuje integrované nebo diskrétní GPU, které mohou akcelerovat AI úlohy. Moderní mobilní GPU a integrované grafické procesory mohou poskytnout významné zlepšení výkonu pro úlohy inferencí AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optimalizace CPU

Dokonce i zařízení pouze s CPU mohou těžit z EdgeAI díky optimalizovaným implementacím. Moderní CPU zahrnují specializované instrukce pro AI úlohy a byly vyvinuty softwarové frameworky, které maximalizují výkon CPU pro inferenční AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pro softwarové inženýry pracující s EdgeAI je klíčové pochopit, jak využít tyto možnosti hardwarové akcelerace pro optimalizaci výkonu inferencí a energetické efektivity na cílových zařízeních.

## Výhody EdgeAI

### Soukromí a bezpečnost

Jednou z nejvýznamnějších výhod EdgeAI je zvýšené soukromí a bezpečnost. Zpracováním dat lokálně na zařízení citlivé informace nikdy neopustí kontrolu uživatele. To je obzvláště důležité pro aplikace, které pracují s osobními údaji, zdravotnickými informacemi nebo důvěrnými obchodními daty.

### Snížená latence

EdgeAI eliminuje potřebu odesílat data na vzdálené servery ke zpracování, což výrazně snižuje latenci. To je klíčové pro aplikace v reálném čase, jako jsou autonomní vozidla, průmyslová automatizace nebo interaktivní aplikace, kde jsou vyžadovány okamžité reakce.

### Offline schopnosti

EdgeAI umožňuje funkčnost AI i v situacích, kdy není dostupné internetové připojení. To je cenné pro aplikace na vzdálených místech, během cestování nebo v situacích, kdy je spolehlivost sítě problémem.

### Nákladová efektivita

Snížením závislosti na cloudových AI službách může EdgeAI pomoci snížit provozní náklady, zejména u aplikací s vysokým objemem použití. Organizace mohou vyhnout se průběžným nákladům na API a snížit požadavky na šířku pásma.

### Škálovatelnost

EdgeAI rozděluje výpočetní zátěž mezi koncová zařízení místo její centralizace v datových centrech. To může pomoci snížit náklady na infrastrukturu a zlepšit celkovou škálovatelnost systému.

## Aplikace EdgeAI

### Chytrá zařízení a IoT

EdgeAI pohání mnoho funkcí chytrých zařízení, od hlasových asistentů, které mohou zpracovávat příkazy lokálně, po chytré kamery, které mohou identifikovat objekty a osoby bez odesílání videa do cloudu. IoT zařízení využívají EdgeAI pro prediktivní údržbu, monitorování prostředí a automatizované rozhodování.

### Mobilní aplikace

Chytré telefony a tablety využívají EdgeAI pro různé funkce, včetně vylepšení fotografií, překladů v reálném čase, rozšířené reality a personalizovaných doporučení. Tyto aplikace těží z nízké latence a výhod soukromí lokálního zpracování.

###
## ➡️ Co dál

- [02: EdgeAI Aplikace](02.RealWorldCaseStudies.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby AI pro překlady [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatizované překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádné nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.