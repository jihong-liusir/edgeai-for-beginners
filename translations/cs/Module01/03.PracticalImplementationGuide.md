<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cf6b1cba2ead9fb7fdc55f77232db067",
  "translation_date": "2025-09-18T16:41:12+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "cs"
}
-->
# Sekce 3: Praktický průvodce implementací

## Přehled

Tento komplexní průvodce vám pomůže připravit se na kurz EdgeAI, který se zaměřuje na vytváření praktických AI řešení, která efektivně fungují na edge zařízeních. Kurz klade důraz na praktický vývoj s využitím moderních frameworků a špičkových modelů optimalizovaných pro nasazení na edge.

## 1. Nastavení vývojového prostředí

### Programovací jazyky a frameworky

**Python prostředí**
- **Verze**: Python 3.10 nebo vyšší (doporučeno: Python 3.11)
- **Správce balíčků**: pip nebo conda
- **Virtuální prostředí**: Používejte venv nebo conda prostředí pro izolaci
- **Klíčové knihovny**: Specifické EdgeAI knihovny nainstalujeme během kurzu

**Microsoft .NET prostředí**
- **Verze**: .NET 8 nebo vyšší
- **IDE**: Visual Studio 2022, Visual Studio Code nebo JetBrains Rider
- **SDK**: Ujistěte se, že máte nainstalovaný .NET SDK pro multiplatformní vývoj

### Vývojové nástroje

**Editory kódu a IDE**
- Visual Studio Code (doporučeno pro multiplatformní vývoj)
- PyCharm nebo Visual Studio (pro jazykově specifický vývoj)
- Jupyter Notebooks pro interaktivní vývoj a prototypování

**Verzovací systém**
- Git (nejnovější verze)
- GitHub účet pro přístup k repozitářům a spolupráci

## 2. Požadavky na hardware a doporučení

### Minimální systémové požadavky
- **CPU**: Vícejádrový procesor (Intel i5/AMD Ryzen 5 nebo ekvivalent)
- **RAM**: Minimálně 8GB, doporučeno 16GB
- **Úložiště**: 50GB volného místa pro modely a vývojové nástroje
- **OS**: Windows 10/11, macOS 10.15+ nebo Linux (Ubuntu 20.04+)

### Strategie výpočetních zdrojů
Kurz je navržen tak, aby byl dostupný na různých hardwarových konfiguracích:

**Lokální vývoj (zaměření na CPU/NPU)**
- Primární vývoj bude využívat akceleraci CPU a NPU
- Vhodné pro většinu moderních notebooků a stolních počítačů
- Důraz na efektivitu a praktické scénáře nasazení

**Cloudové GPU zdroje (volitelné)**
- **Azure Machine Learning**: Pro intenzivní trénink a experimentování
- **Google Colab**: Bezplatná verze dostupná pro vzdělávací účely
- **Kaggle Notebooks**: Alternativní cloudová platforma pro výpočty

### Úvahy o edge zařízeních
- Znalost procesorů založených na ARM
- Porozumění omezením mobilního a IoT hardwaru
- Znalost optimalizace spotřeby energie

## 3. Hlavní modelové rodiny a zdroje

### Primární modelové rodiny

**Microsoft Phi-4 Family**
- **Popis**: Kompaktní, efektivní modely navržené pro nasazení na edge
- **Silné stránky**: Vynikající poměr výkonu k velikosti, optimalizované pro úlohy uvažování
- **Zdroj**: [Phi-4 Collection na Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Použití**: Generování kódu, matematické uvažování, obecná konverzace

**Qwen-3 Family**
- **Popis**: Nejnovější generace vícejazyčných modelů od Alibaba
- **Silné stránky**: Silné vícejazyčné schopnosti, efektivní architektura
- **Zdroj**: [Qwen-3 Collection na Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Použití**: Vícejazyčné aplikace, AI řešení pro různé kultury

**Google Gemma-3n Family**
- **Popis**: Lehký model od Google optimalizovaný pro nasazení na edge
- **Silné stránky**: Rychlá inference, architektura přátelská k mobilním zařízením
- **Zdroj**: [Gemma-3n Collection na Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Použití**: Mobilní aplikace, zpracování v reálném čase

### Kritéria výběru modelů
- **Kompromis mezi výkonem a velikostí**: Porozumění, kdy zvolit menší nebo větší modely
- **Optimalizace pro konkrétní úlohy**: Přizpůsobení modelů specifickým použitím
- **Omezení nasazení**: Paměť, latence a spotřeba energie

## 4. Nástroje pro kvantizaci a optimalizaci

### Llama.cpp Framework
- **Repozitář**: [Llama.cpp na GitHub](https://github.com/ggml-org/llama.cpp)
- **Účel**: Vysoce výkonný inference engine pro LLMs
- **Klíčové vlastnosti**:
  - Optimalizovaná inference na CPU
  - Podpora více kvantizačních formátů (Q4, Q5, Q8)
  - Multiplatformní kompatibilita
  - Paměťově efektivní provádění
- **Instalace a základní použití**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repozitář**: [Microsoft Olive na GitHub](https://github.com/microsoft/olive)
- **Účel**: Nástroj pro optimalizaci modelů pro nasazení na edge
- **Klíčové vlastnosti**:
  - Automatizované workflow pro optimalizaci modelů
  - Optimalizace přizpůsobená hardwaru
  - Integrace s ONNX Runtime
  - Nástroje pro benchmarking výkonu
- **Instalace a základní použití**:
  ```bash
  # Install Olive
  pip install olive-ai
  
  # Example Python script for model optimization
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Definice modelu a konfigurace optimalizace
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Spuštění workflow optimalizace
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Uložení optimalizovaného modelu
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS Users)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Purpose**: Machine learning framework for Apple Silicon
- **Key Features**:
  - Native Apple Silicon optimization
  - Memory-efficient operations
  - PyTorch-like API
  - Unified memory architecture support
- **Installation and Basic Usage**:
  ```bash
  # Instalace MLX
  pip install mlx
  
  # Ukázkový Python skript pro načtení a optimalizaci modelu
  ```python
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repozitář**: [ONNX Runtime na GitHub](https://github.com/microsoft/onnxruntime)
- **Účel**: Multiplatformní akcelerace inference pro ONNX modely
- **Klíčové vlastnosti**:
  - Optimalizace specifické pro hardware (CPU, GPU, NPU)
  - Grafové optimalizace pro inference
  - Podpora kvantizace
  - Podpora více jazyků (Python, C++, C#, JavaScript)
- **Instalace a základní použití**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```
## 5. Doporučené čtení a zdroje

### Základní dokumentace
- **ONNX Runtime Dokumentace**: Porozumění multiplatformní inference 
- **Hugging Face Transformers Průvodce**: Načítání modelů a inference
- **Edge AI Design Patterns**: Nejlepší postupy pro nasazení na edge

### Technické články
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Komunitní zdroje
- **EdgeAI Slack/Discord Komunity**: Podpora od kolegů a diskuse
- **GitHub Repozitáře**: Ukázkové implementace a tutoriály
- **YouTube Kanály**: Technické rozbory a tutoriály

## 6. Hodnocení a ověření

### Předkurzovní kontrolní seznam
- [ ] Nainstalován a ověřen Python 3.10+
- [ ] Nainstalován a ověřen .NET 8+
- [ ] Konfigurováno vývojové prostředí
- [ ] Vytvořen účet na Hugging Face
- [ ] Základní znalost cílových modelových rodin
- [ ] Nainstalovány a otestovány nástroje pro kvantizaci
- [ ] Splněny požadavky na hardware
- [ ] Nastaveny účty pro cloud computing (pokud je potřeba)

## Klíčové vzdělávací cíle

Na konci tohoto průvodce budete schopni:

1. Nastavit kompletní vývojové prostředí pro vývoj EdgeAI aplikací
2. Nainstalovat a konfigurovat potřebné nástroje a frameworky pro optimalizaci modelů
3. Vybrat vhodné hardwarové a softwarové konfigurace pro vaše EdgeAI projekty
4. Porozumět klíčovým aspektům nasazení AI modelů na edge zařízeních
5. Připravit svůj systém na praktická cvičení v kurzu

## Další zdroje

### Oficiální dokumentace
- **Python Dokumentace**: Oficiální dokumentace jazyka Python
- **Microsoft .NET Dokumentace**: Oficiální zdroje pro vývoj v .NET
- **ONNX Runtime Dokumentace**: Komplexní průvodce ONNX Runtime
- **TensorFlow Lite Dokumentace**: Oficiální dokumentace TensorFlow Lite

### Vývojové nástroje
- **Visual Studio Code**: Lehký editor kódu s rozšířeními pro AI vývoj
- **Jupyter Notebooks**: Interaktivní prostředí pro experimentování s ML
- **Docker**: Platforma pro kontejnerizaci konzistentních vývojových prostředí
- **Git**: Systém pro správu verzí kódu

### Vzdělávací zdroje
- **EdgeAI Výzkumné články**: Nejnovější akademický výzkum o efektivních modelech
- **Online kurzy**: Doplňkové vzdělávací materiály o optimalizaci AI
- **Komunitní fóra**: Platformy pro otázky a odpovědi ohledně výzev EdgeAI vývoje
- **Benchmarkové datové sady**: Standardní datové sady pro hodnocení výkonu modelů

## Výsledky vzdělávání

Po dokončení tohoto přípravného průvodce budete:

1. Mít plně konfigurované vývojové prostředí připravené pro vývoj EdgeAI
2. Rozumět požadavkům na hardware a software pro různé scénáře nasazení
3. Být obeznámeni s klíčovými frameworky a nástroji používanými v průběhu kurzu
4. Schopni vybrat vhodné modely na základě omezení zařízení a požadavků
5. Mít základní znalosti o optimalizačních technikách pro nasazení na edge

## ➡️ Co dál

- [04: EdgeAI Hardware a nasazení](04.EdgeDeployment.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). I když se snažíme o přesnost, mějte na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.