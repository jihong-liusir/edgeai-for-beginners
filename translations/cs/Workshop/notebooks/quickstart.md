<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ddaad917d0c16fc3d498a6b4eabc8088",
  "translation_date": "2025-10-09T21:53:27+00:00",
  "source_file": "Workshop/notebooks/quickstart.md",
  "language_code": "cs"
}
-->
# Workshop Notebooks - Rychl√Ω pr≈Øvodce

## Obsah

- [P≈ôedpoklady](../../../../Workshop/notebooks)
- [Poƒç√°teƒçn√≠ nastaven√≠](../../../../Workshop/notebooks)
- [Sezen√≠ 04: Porovn√°n√≠ model≈Ø](../../../../Workshop/notebooks)
- [Sezen√≠ 05: Orchestr√°tor pro v√≠ce agent≈Ø](../../../../Workshop/notebooks)
- [Sezen√≠ 06: Smƒõrov√°n√≠ model≈Ø na z√°kladƒõ z√°mƒõru](../../../../Workshop/notebooks)
- [Promƒõnn√© prost≈ôed√≠](../../../../Workshop/notebooks)
- [Bƒõ≈æn√© p≈ô√≠kazy](../../../../Workshop/notebooks)

---

## P≈ôedpoklady

### 1. Instalace Foundry Local

**Windows:**
```bash
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Ovƒõ≈ôen√≠ instalace:**
```bash
foundry --version
```

### 2. Instalace Python z√°vislost√≠

```bash
cd Workshop
pip install -r requirements.txt
```

Nebo instalace jednotlivƒõ:
```bash
pip install foundry-local-sdk openai numpy requests
```

---

## Poƒç√°teƒçn√≠ nastaven√≠

### Spu≈°tƒõn√≠ slu≈æby Foundry Local

**Nutn√© p≈ôed spu≈°tƒõn√≠m jak√©hokoliv notebooku:**

```bash
# Start the service
foundry service start

# Verify it's running
foundry service status
```

Oƒçek√°van√Ω v√Ωstup:
```
‚úÖ Service started successfully
Endpoint: http://localhost:59959
```

### Sta≈æen√≠ a naƒçten√≠ model≈Ø

Notebooky pou≈æ√≠vaj√≠ tyto modely jako v√Ωchoz√≠:

```bash
# Download models (first time only - may take several minutes)
foundry model download phi-4-mini
foundry model download qwen2.5-3b
foundry model download phi-3.5-mini
foundry model download qwen2.5-0.5b

# Load models into memory
foundry model run phi-4-mini
foundry model run qwen2.5-3b
foundry model run phi-3.5-mini
```

### Ovƒõ≈ôen√≠ nastaven√≠

```bash
# List loaded models
foundry model ls

# Check service health
curl http://localhost:59959/v1/models
```

---

## Sezen√≠ 04: Porovn√°n√≠ model≈Ø

### √öƒçel
Porovnat v√Ωkon mezi mal√Ωmi jazykov√Ωmi modely (SLM) a velk√Ωmi jazykov√Ωmi modely (LLM).

### Rychl√© nastaven√≠

```bash
# Start service (if not already running)
foundry service start

# Load required models
foundry model run phi-4-mini
foundry model run qwen2.5-3b
```

### Spu≈°tƒõn√≠ notebooku

1. **Otev≈ôete** `session04_model_compare.ipynb` ve VS Code nebo Jupyteru
2. **Restartujte kernel** (Kernel ‚Üí Restart Kernel)
3. **Spus≈•te v≈°echny bu≈àky** v po≈ôad√≠

### Kl√≠ƒçov√° konfigurace

**V√Ωchoz√≠ modely:**
- **SLM:** `phi-4-mini` (~4GB RAM, rychlej≈°√≠)
- **LLM:** `qwen2.5-3b` (~3GB RAM, optimalizov√°no pro pamƒõ≈•)

**Promƒõnn√© prost≈ôed√≠ (voliteln√©):**
```python
import os
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'
```

### Oƒçek√°van√Ω v√Ωstup

```
================================================================================
COMPARISON SUMMARY
================================================================================
Alias                Latency(s)      Tokens     Route               
--------------------------------------------------------------------------------
phi-4-mini           1.234           150        chat.completions    
qwen2.5-3b           2.456           180        chat.completions    
================================================================================

üí° SLM is 1.99x faster than LLM for this prompt
```

### P≈ôizp≈Øsoben√≠

**Pou≈æit√≠ jin√Ωch model≈Ø:**
```python
os.environ['SLM_ALIAS'] = 'phi-3.5-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-1.5b'
```

**Vlastn√≠ prompt:**
```python
os.environ['COMPARE_PROMPT'] = 'Explain quantum computing in simple terms'
```

### Kontroln√≠ seznam validace

- [ ] Bu≈àka 12 ukazuje spr√°vn√© modely (phi-4-mini, qwen2.5-3b)
- [ ] Bu≈àka 12 ukazuje spr√°vn√Ω endpoint (port 59959)
- [ ] Diagnostika bu≈àky 16 √∫spƒõ≈°n√° (‚úÖ Slu≈æba bƒõ≈æ√≠)
- [ ] Kontrola p≈ôed spu≈°tƒõn√≠m v bu≈àce 20 √∫spƒõ≈°n√° (oba modely v po≈ô√°dku)
- [ ] Porovn√°n√≠ v bu≈àce 22 dokonƒçeno s hodnotami latence
- [ ] Validace v bu≈àce 24 ukazuje üéâ V≈†ECHNY KONTROLY √öSPƒö≈†N√â!

### Odhad ƒçasu
- **Prvn√≠ spu≈°tƒõn√≠:** 5-10 minut (vƒçetnƒõ stahov√°n√≠ model≈Ø)
- **Dal≈°√≠ spu≈°tƒõn√≠:** 1-2 minuty

---

## Sezen√≠ 05: Orchestr√°tor pro v√≠ce agent≈Ø

### √öƒçel
Uk√°zat spolupr√°ci v√≠ce agent≈Ø pomoc√≠ Foundry Local SDK - agenti spolupracuj√≠ na vytvo≈ôen√≠ vylep≈°en√Ωch v√Ωstup≈Ø.

### Rychl√© nastaven√≠

```bash
# Start service
foundry service start

# Load models
foundry model run phi-4-mini  # Primary model
foundry model run qwen2.5-7b  # Optional: higher quality editor
```

### Spu≈°tƒõn√≠ notebooku

1. **Otev≈ôete** `session05_agents_orchestrator.ipynb`
2. **Restartujte kernel**
3. **Spus≈•te v≈°echny bu≈àky** v po≈ôad√≠

### Kl√≠ƒçov√° konfigurace

**V√Ωchoz√≠ nastaven√≠ (stejn√Ω model pro oba agenty):**
```python
PRIMARY_ALIAS = 'phi-4-mini'
EDITOR_ALIAS = 'phi-4-mini'  # Uses same model
```

**Pokroƒçil√© nastaven√≠ (r≈Øzn√© modely):**
```python
import os
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'     # Fast for research
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'      # High quality for editing
```

### Architektura

```
User Question
    ‚Üì
Researcher Agent (phi-4-mini)
  ‚Üí Gathers bullet points
    ‚Üì
Editor Agent (phi-4-mini or qwen2.5-7b)
  ‚Üí Refines into executive summary
    ‚Üì
Final Output
```

### Oƒçek√°van√Ω v√Ωstup

```
================================================================================
[Pipeline] Question: Explain why edge AI matters for compliance.
================================================================================

[Stage 1: Research]
Output: ‚Ä¢ Edge AI processes data locally, reducing transmission...

[Stage 2: Editorial Refinement]
Output: Executive Summary: Edge AI enhances compliance by keeping data...

[FINAL OUTPUT]
Executive Summary: Edge AI enhances compliance by keeping sensitive data 
on-premises and reduces latency through local processing.

[METADATA]
Models used: {'researcher': 'phi-4-mini', 'editor': 'phi-4-mini'}
```

### Roz≈°√≠≈ôen√≠

**P≈ôid√°n√≠ dal≈°√≠ch agent≈Ø:**
```python
critic = Agent(
    name='Critic',
    system='Review content for accuracy',
    client=client,
    model_id=model_id
)
```

**Batch testov√°n√≠:**
```python
test_questions = [
    "What are benefits of local AI?",
    "How does RAG improve accuracy?",
]

for q in test_questions:
    result = pipeline(q, verbose=False)
    print(result['final'])
```

### Odhad ƒçasu
- **Prvn√≠ spu≈°tƒõn√≠:** 3-5 minut
- **Dal≈°√≠ spu≈°tƒõn√≠:** 1-2 minuty na ot√°zku

---

## Sezen√≠ 06: Smƒõrov√°n√≠ model≈Ø na z√°kladƒõ z√°mƒõru

### √öƒçel
Inteligentnƒõ smƒõrovat prompty na specializovan√© modely na z√°kladƒõ detekovan√©ho z√°mƒõru.

### Rychl√© nastaven√≠

```bash
# Start service
foundry service start

# Load all routing models (CPU variants recommended)
foundry model run phi-4-mini-cpu
foundry model run qwen2.5-0.5b-cpu
foundry model run phi-3.5-mini-cpu
```

**Pozn√°mka:** Sezen√≠ 06 pou≈æ√≠v√° v√Ωchoz√≠ modely na CPU pro maxim√°ln√≠ kompatibilitu.

### Spu≈°tƒõn√≠ notebooku

1. **Otev≈ôete** `session06_models_router.ipynb`
2. **Restartujte kernel**
3. **Spus≈•te v≈°echny bu≈àky** v po≈ôad√≠

### Kl√≠ƒçov√° konfigurace

**V√Ωchoz√≠ katalog (CPU modely):**
```python
CATALOG = {
    'phi-4-mini-cpu': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b-cpu': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini-cpu': {'capabilities':['code','refactor'],'priority':3},
}
```

**Alternativa (GPU modely):**
```python
# Uncomment GPU catalog in Cell #6 if you have sufficient VRAM (8GB+)
CATALOG = {
    'phi-4-mini': {'capabilities':['general','summarize'],'priority':2},
    'qwen2.5-0.5b': {'capabilities':['classification','fast'],'priority':1},
    'phi-3.5-mini': {'capabilities':['code','refactor'],'priority':3},
}
```

### Detekce z√°mƒõru

Router pou≈æ√≠v√° regex vzory k detekci z√°mƒõru:

| Z√°mƒõr | P≈ô√≠klady vzor≈Ø | Smƒõrov√°no na |
|-------|----------------|-------------|
| `code` | "refactor", "implement function" | phi-3.5-mini-cpu |
| `classification` | "categorize", "classify this" | qwen2.5-0.5b-cpu |
| `summarize` | "summarize", "tl;dr" | phi-4-mini-cpu |
| `general` | V≈°e ostatn√≠ | phi-4-mini-cpu |

### Oƒçek√°van√Ω v√Ωstup

```
‚úì Using CPU-optimized models (default configuration)
  Models: phi-4-mini-cpu, qwen2.5-0.5b-cpu, phi-3.5-mini-cpu

Routing prompts to specialized models...
============================================================

Prompt: Refactor this Python function for readability
  Intent: code           | Model: phi-3.5-mini-cpu
  Output: Here's a refactored version...
  Tokens: 156

Prompt: Categorize this email as urgent or normal
  Intent: classification | Model: qwen2.5-0.5b-cpu
  Output: Category: Normal
  Tokens: 45

‚úì Success! All prompts routed correctly.
```

### P≈ôizp≈Øsoben√≠

**P≈ôid√°n√≠ vlastn√≠ho z√°mƒõru:**
```python
import re

# Add to RULES
RULES.append((re.compile('translate|ÁøªËØë', re.I), 'translation'))

# Add capability to catalog
CATALOG['phi-4-mini-cpu']['capabilities'].append('translation')
```

**Povolen√≠ sledov√°n√≠ token≈Ø:**
```python
import os
os.environ['SHOW_USAGE'] = '1'
```

### P≈ôepnut√≠ na GPU modely

Pokud m√°te 8GB+ VRAM:

1. V **bu≈àce #6** zakomentujte CPU katalog
2. Odkomentujte GPU katalog
3. Naƒçtƒõte GPU modely:
   ```bash
   foundry model run phi-4-mini
   foundry model run qwen2.5-0.5b
   foundry model run phi-3.5-mini
   ```
4. Restartujte kernel a znovu spus≈•te notebook

### Odhad ƒçasu
- **Prvn√≠ spu≈°tƒõn√≠:** 5-10 minut (naƒç√≠t√°n√≠ model≈Ø)
- **Dal≈°√≠ spu≈°tƒõn√≠:** 30-60 sekund na test

---

## Promƒõnn√© prost≈ôed√≠

### Glob√°ln√≠ konfigurace

Nastavte p≈ôed spu≈°tƒõn√≠m Jupyter/VS Code:

**Windows (P≈ô√≠kazov√Ω ≈ô√°dek):**
```cmd
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
set SHOW_USAGE=1
set RETRY_ON_FAIL=1
```

**Windows (PowerShell):**
```powershell
$env:FOUNDRY_LOCAL_ENDPOINT="http://localhost:59959/v1"
$env:SHOW_USAGE="1"
$env:RETRY_ON_FAIL="1"
```

**macOS/Linux:**
```bash
export FOUNDRY_LOCAL_ENDPOINT=http://localhost:59959/v1
export SHOW_USAGE=1
export RETRY_ON_FAIL=1
```

### Konfigurace v notebooku

Nastavte na zaƒç√°tku jak√©hokoliv notebooku:

```python
import os

# Foundry Local configuration
os.environ['FOUNDRY_LOCAL_ENDPOINT'] = 'http://localhost:59959/v1'

# Model selection
os.environ['SLM_ALIAS'] = 'phi-4-mini'
os.environ['LLM_ALIAS'] = 'qwen2.5-3b'

# Agent models
os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'
os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'

# Debugging
os.environ['SHOW_USAGE'] = '1'       # Show token usage
os.environ['RETRY_ON_FAIL'] = '1'    # Enable retries
os.environ['RETRY_BACKOFF'] = '2.0'  # Retry delay
```

---

## Bƒõ≈æn√© p≈ô√≠kazy

### Spr√°va slu≈æby

```bash
# Start service
foundry service start

# Check status
foundry service status

# Stop service
foundry service stop

# View logs
foundry service logs
```

### Spr√°va model≈Ø

```bash
# List all available models in catalog
foundry model catalog

# List loaded models
foundry model ls

# Download a model
foundry model download phi-4-mini

# Load a model
foundry model run phi-4-mini

# Unload a model
foundry model unload phi-4-mini

# Remove a model
foundry model remove phi-4-mini

# Get model info
foundry model info phi-4-mini
```

### Testov√°n√≠ endpoint≈Ø

```bash
# Check service health
curl http://localhost:59959/health

# List available models via API
curl http://localhost:59959/v1/models

# Test model completion
curl http://localhost:59959/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi-4-mini",
    "messages": [{"role":"user","content":"Hello"}],
    "max_tokens": 50
  }'
```

### Diagnostick√© p≈ô√≠kazy

```bash
# Check everything
foundry --version
foundry service status
foundry model ls
foundry device info

# GPU status (NVIDIA)
nvidia-smi

# NPU status (Qualcomm)
foundry device info
```

---

## Nejlep≈°√≠ postupy

### P≈ôed spu≈°tƒõn√≠m jak√©hokoliv notebooku

1. **Zkontrolujte, zda slu≈æba bƒõ≈æ√≠:**
   ```bash
   foundry service status
   ```

2. **Ovƒõ≈ôte, zda jsou modely naƒçteny:**
   ```bash
   foundry model ls
   ```

3. **Restartujte kernel notebooku** p≈ôi opakovan√©m spu≈°tƒõn√≠

4. **Vyma≈æte v≈°echny v√Ωstupy** pro ƒçist√© spu≈°tƒõn√≠

### Spr√°va zdroj≈Ø

1. **Pou≈æ√≠vejte CPU modely jako v√Ωchoz√≠** pro kompatibilitu
2. **P≈ôepnƒõte na GPU modely** pouze pokud m√°te 8GB+ VRAM
3. **Zav≈ôete ostatn√≠ GPU aplikace** p≈ôed spu≈°tƒõn√≠m
4. **Udr≈æujte slu≈æbu bƒõ≈æ√≠c√≠** mezi sezen√≠mi notebooku
5. **Sledujte vyu≈æit√≠ zdroj≈Ø** pomoc√≠ Spr√°vce √∫loh / nvidia-smi

### ≈òe≈°en√≠ probl√©m≈Ø

1. **V≈ædy nejprve zkontrolujte slu≈æbu** p≈ôed ladƒõn√≠m k√≥du
2. **Restartujte kernel** pokud vid√≠te zastaralou konfiguraci
3. **Znovu spus≈•te diagnostick√© bu≈àky** po jak√Ωchkoliv zmƒõn√°ch
4. **Ovƒõ≈ôte n√°zvy model≈Ø** odpov√≠daj√≠ naƒçten√Ωm model≈Øm
5. **Ovƒõ≈ôte port endpointu** odpov√≠d√° stavu slu≈æby

---

## Rychl√° reference: Alias model≈Ø

### Bƒõ≈æn√© modely

| Alias | Velikost | Nejlep≈°√≠ pro | RAM/VRAM | Varianty |
|-------|----------|--------------|----------|----------|
| `phi-4-mini` | ~4B | Obecn√Ω chat, sumarizace | 4-6GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `phi-3.5-mini` | ~3.5B | Generov√°n√≠ k√≥du, refaktoring | 3-5GB | `-cpu`, `-cuda-gpu`, `-npu` |
| `qwen2.5-3b` | ~3B | Obecn√© √∫koly, efektivn√≠ | 3-4GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-1.5b` | ~1.5B | Rychl√©, n√≠zk√© n√°roky | 2-3GB | `-cpu`, `-cuda-gpu` |
| `qwen2.5-0.5b` | ~0.5B | Klasifikace, minim√°ln√≠ n√°roky | 1-2GB | `-cpu`, `-cuda-gpu` |

### N√°zvy variant

- **Z√°kladn√≠ n√°zev** (nap≈ô. `phi-4-mini`): Automaticky vybere nejlep≈°√≠ variantu pro va≈°e hardware
- **`-cpu`**: Optimalizov√°no pro CPU, funguje v≈°ude
- **`-cuda-gpu`**: Optimalizov√°no pro NVIDIA GPU, vy≈æaduje 8GB+ VRAM
- **`-npu`**: Optimalizov√°no pro Qualcomm NPU, vy≈æaduje NPU ovladaƒçe

**Doporuƒçen√≠:** Pou≈æ√≠vejte z√°kladn√≠ n√°zvy (bez p≈ô√≠pony) a nechte Foundry Local automaticky vybrat nejlep≈°√≠ variantu.

---

## Indik√°tory √∫spƒõchu

Jste p≈ôipraveni, kdy≈æ vid√≠te:

‚úÖ `foundry service status` ukazuje "running"
‚úÖ `foundry model ls` ukazuje po≈æadovan√© modely
‚úÖ Slu≈æba je dostupn√° na spr√°vn√©m endpointu
‚úÖ Kontrola stavu vrac√≠ 200 OK
‚úÖ Diagnostick√© bu≈àky notebooku √∫spƒõ≈°n√©
‚úÖ ≈Ω√°dn√© chyby p≈ôipojen√≠ ve v√Ωstupu

---

## Z√≠sk√°n√≠ pomoci

### Dokumentace
- **Hlavn√≠ repozit√°≈ô**: https://github.com/microsoft/Foundry-Local
- **Python SDK**: https://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **CLI Reference**: https://github.com/microsoft/Foundry-Local/blob/main/docs/reference/reference-cli.md
- **≈òe≈°en√≠ probl√©m≈Ø**: Viz `troubleshooting.md` v tomto adres√°≈ôi

### GitHub Issues
- https://github.com/microsoft/Foundry-Local/issues
- https://github.com/microsoft/edgeai-for-beginners/issues

---

**Posledn√≠ aktualizace:** 8. ≈ô√≠jna 2025  
**Verze:** Workshop Notebooks 2.0

---

**Prohl√°≈°en√≠**:  
Tento dokument byl p≈ôelo≈æen pomoc√≠ slu≈æby AI pro p≈ôeklad [Co-op Translator](https://github.com/Azure/co-op-translator). I kdy≈æ se sna≈æ√≠me o p≈ôesnost, mƒõjte pros√≠m na pamƒõti, ≈æe automatizovan√© p≈ôeklady mohou obsahovat chyby nebo nep≈ôesnosti. P≈Øvodn√≠ dokument v jeho rodn√©m jazyce by mƒõl b√Ωt pova≈æov√°n za autoritativn√≠ zdroj. Pro d≈Øle≈æit√© informace se doporuƒçuje profesion√°ln√≠ lidsk√Ω p≈ôeklad. Neodpov√≠d√°me za ≈æ√°dn√° nedorozumƒõn√≠ nebo nespr√°vn√© interpretace vypl√Ωvaj√≠c√≠ z pou≈æit√≠ tohoto p≈ôekladu.