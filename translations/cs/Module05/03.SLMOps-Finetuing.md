<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-09-18T17:12:58+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "cs"
}
-->
# Sekce 3: Doladění - Přizpůsobení modelů pro specifické úkoly

## Obsah
1. [Úvod do doladění](../../../Module05)
2. [Proč je doladění důležité](../../../Module05)
3. [Typy doladění](../../../Module05)
4. [Doladění s Microsoft Olive](../../../Module05)
5. [Praktické příklady](../../../Module05)
6. [Osvědčené postupy a doporučení](../../../Module05)
7. [Pokročilé techniky](../../../Module05)
8. [Hodnocení a monitorování](../../../Module05)
9. [Běžné výzvy a řešení](../../../Module05)
10. [Závěr](../../../Module05)

## Úvod do doladění

**Doladění** je výkonná technika strojového učení, která spočívá v přizpůsobení předem naučeného modelu tak, aby vykonával specifické úkoly nebo pracoval s specializovanými datovými sadami. Místo trénování modelu od začátku využívá doladění znalosti již naučené předem naučeným modelem a upravuje je pro váš konkrétní případ použití.

### Co je doladění?

Doladění je forma **transferového učení**, při které:
- Začínáte s předem naučeným modelem, který se naučil obecné vzory z velkých datových sad
- Upravujete interní parametry modelu pomocí vaší specifické datové sady
- Zachováváte cenné znalosti a zároveň specializujete model pro váš úkol

Představte si to jako učení zkušeného kuchaře novou kuchyni – už rozumí základům vaření, ale potřebuje se naučit specifické techniky a chutě pro nový styl.

### Klíčové výhody

- **Úspora času**: Výrazně rychlejší než trénování od začátku
- **Efektivita dat**: Vyžaduje menší datové sady pro dosažení dobrého výkonu
- **Nákladová efektivita**: Nižší požadavky na výpočetní výkon
- **Lepší výkon**: Často dosahuje lepších výsledků než trénování od začátku
- **Optimalizace zdrojů**: Zpřístupňuje výkonnou AI menším týmům a organizacím

## Proč je doladění důležité

### Aplikace v reálném světě

Doladění je zásadní v mnoha scénářích:

**1. Přizpůsobení doméně**
- Medicínská AI: Přizpůsobení obecných jazykových modelů pro lékařskou terminologii a klinické poznámky
- Právní technologie: Specializace modelů pro analýzu právních dokumentů a revizi smluv
- Finanční služby: Přizpůsobení modelů pro analýzu finančních zpráv a hodnocení rizik

**2. Specializace úkolů**
- Generování obsahu: Doladění pro specifické styly psaní nebo tón
- Generování kódu: Přizpůsobení modelů pro konkrétní programovací jazyky nebo frameworky
- Překlad: Zlepšení výkonu pro specifické jazykové páry nebo technické domény

**3. Firemní aplikace**
- Zákaznický servis: Vytváření chatbotů, které rozumí specifické terminologii společnosti
- Interní dokumentace: Budování AI asistentů obeznámených s procesy organizace
- Řešení specifická pro odvětví: Vývoj modelů, které rozumí žargonu a pracovním postupům daného sektoru

## Typy doladění

### 1. Plné doladění (Instruction Fine-Tuning)

Při plném doladění se během trénování aktualizují všechny parametry modelu. Tento přístup:
- Poskytuje maximální flexibilitu a potenciál výkonu
- Vyžaduje značné výpočetní zdroje
- Výsledkem je zcela nová verze modelu
- Nejlepší pro scénáře, kdy máte dostatek trénovacích dat a výpočetních zdrojů

### 2. Efektivní doladění parametrů (PEFT)

Metody PEFT aktualizují pouze malou podmnožinu parametrů, což proces činí efektivnějším:

#### Low-Rank Adaptation (LoRA)
- Přidává malé trénovatelné matice rozkladu k existujícím váhám
- Výrazně snižuje počet trénovatelných parametrů
- Udržuje výkon blízký plnému doladění
- Umožňuje snadné přepínání mezi různými přizpůsobeními

#### QLoRA (Quantized LoRA)
- Kombinuje LoRA s technikami kvantizace
- Dále snižuje požadavky na paměť
- Umožňuje doladění větších modelů na běžném hardwaru
- Vyvažuje efektivitu s výkonem

#### Adapters
- Vkládají malé neuronové sítě mezi existující vrstvy
- Umožňují cílené doladění při zachování základního modelu
- Umožňují modulární přístup k přizpůsobení modelu

### 3. Doladění specifické pro úkol

Zaměřuje se na přizpůsobení modelů pro konkrétní následné úkoly:
- **Klasifikace**: Úprava modelů pro úkoly kategorizace
- **Generování**: Optimalizace pro tvorbu obsahu a generování textu
- **Extrahování**: Doladění pro extrakci informací a rozpoznávání pojmenovaných entit
- **Shrnutí**: Specializace modelů pro shrnutí dokumentů

## Doladění s Microsoft Olive

Microsoft Olive je komplexní nástroj pro optimalizaci modelů, který zjednodušuje proces doladění a poskytuje funkce na úrovni podniků.

### Co je Microsoft Olive?

Microsoft Olive je open-source nástroj pro optimalizaci modelů, který:
- Zjednodušuje pracovní postupy doladění pro různé hardwarové cíle
- Poskytuje vestavěnou podporu pro populární architektury modelů (Llama, Phi, Qwen, Gemma)
- Nabízí možnosti nasazení v cloudu i lokálně
- Bezproblémově se integruje s Azure ML a dalšími službami Microsoft AI
- Podporuje automatickou optimalizaci a kvantizaci

### Klíčové funkce

- **Optimalizace podle hardwaru**: Automaticky optimalizuje modely pro konkrétní hardware (CPU, GPU, NPU)
- **Podpora více formátů**: Funguje s modely PyTorch, Hugging Face a ONNX
- **Automatizované pracovní postupy**: Snižuje manuální konfiguraci a pokusy-omyl
- **Integrace do podniků**: Vestavěná podpora pro Azure ML a nasazení v cloudu
- **Rozšiřitelná architektura**: Umožňuje vlastní optimalizační techniky

### Instalace a nastavení

#### Základní instalace

```bash
# Create a virtual environment
python -m venv olive-env
source olive-env/bin/activate  # On Windows: olive-env\Scripts\activate

# Install Olive with auto-optimization features
pip install olive-ai[auto-opt]

# Install additional dependencies
pip install transformers onnxruntime-genai
```

#### Volitelné závislosti

```bash
# For CPU optimization
pip install olive-ai[cpu]

# For GPU optimization
pip install olive-ai[gpu]

# For DirectML (Windows)
pip install olive-ai[directml]

# For Azure ML integration
pip install olive-ai[azureml]
```

#### Ověření instalace

```bash
# Check Olive CLI is available
olive --help

# Verify installation
python -c "import olive; print('Olive installed successfully')"
```

## Praktické příklady

### Příklad 1: Základní doladění pomocí Olive CLI

Tento příklad ukazuje doladění malého jazykového modelu pro klasifikaci frází:

#### Krok 1: Příprava prostředí

```bash
# Set up the environment
mkdir fine-tuning-project
cd fine-tuning-project

# Download sample data (optional - Olive can fetch data automatically)
huggingface-cli login  # If using private datasets
```

#### Krok 2: Doladění modelu

```bash
# Basic fine-tuning command
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### Krok 3: Optimalizace pro nasazení

```bash
# Convert to ONNX format for optimized inference
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### Příklad 2: Pokročilá konfigurace s vlastním datovým souborem

#### Krok 1: Příprava vlastního datového souboru

Vytvořte JSON soubor s vašimi trénovacími daty:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### Krok 2: Vytvoření konfiguračního souboru

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### Krok 3: Provedení doladění

```bash
# Run with custom configuration
olive run --config olive-config.yaml --setup
```

### Příklad 3: Doladění QLoRA pro efektivitu paměti

```bash
# Fine-tune with QLoRA for better memory efficiency
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## Osvědčené postupy a doporučení

### Příprava dat

**1. Kvalita dat nad kvantitou**
- Upřednostněte vysoce kvalitní, rozmanité příklady před velkými objemy nekvalitních dat
- Zajistěte, aby data reprezentovala váš cílový případ použití
- Data důsledně čistěte a předzpracovávejte

**2. Formát dat a šablony**
- Používejte konzistentní formátování u všech trénovacích příkladů
- Vytvářejte jasné šablony vstupů a výstupů, které odpovídají vašemu případu použití
- Zahrňte vhodné formátování instrukcí pro modely doladěné na instrukce

**3. Rozdělení datové sady**
- Vyhraďte 10–20 % dat pro validaci
- Udržujte podobné rozložení mezi trénovacími a validačními daty
- Zvažte stratifikované vzorkování pro úkoly klasifikace

### Konfigurace trénování

**1. Výběr učícího tempa**
- Začněte s menšími učícími tempy (1e-5 až 1e-4) pro doladění
- Používejte plánování učícího tempa pro lepší konvergenci
- Sledujte křivky ztrát pro úpravu tempa

**2. Optimalizace velikosti dávky**
- Vyvažte velikost dávky s dostupnou pamětí
- Používejte akumulaci gradientů pro větší efektivní velikosti dávek
- Zvažte vztah mezi velikostí dávky a učícím tempem

**3. Délka trénování**
- Sledujte validační metriky, abyste se vyhnuli přeučení
- Používejte předčasné zastavení, když se výkon validace ustálí
- Pravidelně ukládejte kontrolní body pro obnovu a analýzu

### Výběr modelu

**1. Volba základního modelu**
- Vyberte modely předem naučené na podobných doménách, pokud je to možné
- Zvažte velikost modelu vzhledem k vašim výpočetním omezením
- Vyhodnoťte licenční požadavky pro komerční použití

**2. Výběr metody doladění**
- Používejte LoRA/QLoRA pro prostředí s omezenými zdroji
- Zvolte plné doladění, když je kritický maximální výkon
- Zvažte přístupy založené na adaptérech pro scénáře s více úkoly

### Správa zdrojů

**1. Optimalizace hardwaru**
- Vyberte vhodný hardware pro velikost modelu a metodu
- Efektivně využívejte paměť GPU pomocí kontrolování gradientů
- Zvažte cloudová řešení pro větší modely

**2. Správa paměti**
- Používejte trénování s mixovanou přesností, pokud je dostupné
- Implementujte akumulaci gradientů pro omezení paměti
- Sledujte využití paměti GPU během trénování

## Pokročilé techniky

### Trénování více adaptérů

Trénujte více adaptérů pro různé úkoly při sdílení základního modelu:

```bash
# Train multiple LoRA adapters
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# Generate multi-adapter ONNX model
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### Optimalizace hyperparametrů

Implementujte systematické ladění hyperparametrů:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### Vlastní ztrátové funkce

Implementujte ztrátové funkce specifické pro doménu:

```python
# custom_loss.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## Hodnocení a monitorování

### Metriky a hodnocení

**1. Standardní metriky**
- **Přesnost**: Celková správnost pro úkoly klasifikace
- **Perplexita**: Míra kvality jazykového modelování
- **BLEU/ROUGE**: Kvalita generování textu a shrnutí
- **F1 skóre**: Vyvážená přesnost a odvolání pro klasifikaci

**2. Metriky specifické pro doménu**
- **Benchmarky specifické pro úkol**: Používejte zavedené benchmarky pro vaši doménu
- **Hodnocení lidmi**: Zahrňte lidské hodnocení pro subjektivní úkoly
- **Obchodní metriky**: Slaďte s reálnými obchodními cíli

**3. Nastavení hodnocení**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # Evaluation logic here
    results = {}
    
    for example in test_dataset:
        # Process example and calculate metrics
        pass
    
    return results
```

### Monitorování průběhu trénování

**1. Sledování ztráty**
```bash
# Enable detailed logging
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. Monitorování validace**
- Sledujte validační ztrátu vedle trénovací ztráty
- Sledujte známky přeučení (validační ztráta roste, zatímco trénovací ztráta klesá)
- Používejte předčasné zastavení na základě validačních metrik

**3. Monitorování zdrojů**
- Sledujte využití GPU/CPU
- Sledujte vzorce využití paměti
- Sledujte rychlost trénování a propustnost

## Běžné výzvy a řešení

### Výzva 1: Přeučení

**Příznaky:**
- Trénovací ztráta stále klesá, zatímco validační ztráta roste
- Velký rozdíl mezi trénovacím a validačním výkonem
- Špatná generalizace na nová data

**Řešení:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### Výzva 2: Omezení paměti

**Řešení:**
- Používejte kontrolování gradientů
- Implementujte akumulaci gradientů
- Zvolte metody efektivní na parametry (LoRA, QLoRA)
- Využívejte paralelizaci modelu pro velké modely

```bash
# Memory-efficient training
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### Výzva 3: Pomalé trénování

**Řešení:**
- Optimalizujte datové načítací pipeline
- Používejte trénování s mixovanou přesností
- Implementujte efektivní strategie dávkování
- Zvažte distribuované trénování pro velké datové sady

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### Výzva 4: Špatný výkon

**Kroky diagnostiky:**
1. Ověřte kvalitu a formát dat
2. Zkontrolujte učící tempo a délku trénování
3. Vyhodnoťte volbu základního modelu
4. Zkontrolujte předzpracování a tokenizaci

**Řešení:**
- Zvyšte rozmanitost trénovacích dat
- Upravte plán učícího tempa
- Vyzkoušejte různé základní modely
- Implementujte techniky augmentace dat

## Závěr

Doladění je výkonná technika, která demokratizuje přístup k nejmodernějším schopnostem AI. Využitím nástrojů jako Microsoft Olive mohou organizace efektivně přizpůsobit předem naučené modely svým specifickým potřebám a zároveň optimalizovat výkon a omezení zdrojů.

### Klíčové poznatky

1. **Vyberte správný přístup**: Zvolte metody doladění na základě vašich výpočetních zdrojů a požadavků na výkon
2. **Kvalita dat je důležitá**: Investujte do vysoce kvalitních, reprezentativních trénovacích dat
3. **Sledujte a iterujte**: Nepřetržitě vyhodnocujte a zlepšujte své modely
4. **Využívejte nástroje**: Používejte frameworky jako Olive pro zjednodušení a optimalizaci procesu
5. **Zvažte nasazení**: Plánujte optimalizaci a nasazení modelu od začátku

## ➡️ Co dál

- [04: Nasazení - Implementace modelu připraveného pro produkci](./04.SLMOps.Deployment.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace doporučujeme profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.