<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bd5b920b610665fd0462f6b5c2e134",
  "translation_date": "2025-09-18T16:38:50+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "cs"
}
-->
# Sekce 03 - Integrace Model Context Protocol (MCP)

## Úvod do MCP (Model Context Protocol)

Model Context Protocol (MCP) je revoluční rámec, který umožňuje jazykovým modelům komunikovat s externími nástroji a systémy standardizovaným způsobem. Na rozdíl od tradičních přístupů, kde jsou modely izolované, MCP vytváří most mezi AI modely a reálným světem prostřednictvím dobře definovaného protokolu.

### Co je MCP?

MCP slouží jako komunikační protokol, který umožňuje jazykovým modelům:
- Připojit se k externím datovým zdrojům
- Spouštět nástroje a funkce
- Interagovat s API a službami
- Přistupovat k informacím v reálném čase
- Provádět složité vícekrokové operace

Tento protokol proměňuje statické jazykové modely na dynamické agenty schopné vykonávat praktické úkoly nad rámec generování textu.

## Malé jazykové modely (SLMs) v MCP

Malé jazykové modely představují efektivní přístup k nasazení AI a nabízejí několik výhod:

### Výhody SLMs
- **Efektivita zdrojů**: Nižší požadavky na výpočetní výkon
- **Rychlejší odezva**: Snížená latence pro aplikace v reálném čase  
- **Nákladová efektivita**: Minimální potřeba infrastruktury
- **Soukromí**: Možnost provozu lokálně bez přenosu dat
- **Přizpůsobení**: Snadnější doladění pro specifické oblasti

### Proč SLMs dobře fungují s MCP

SLMs ve spojení s MCP vytvářejí silnou kombinaci, kde schopnosti modelu v oblasti uvažování jsou rozšířeny o externí nástroje, což kompenzuje jejich menší počet parametrů prostřednictvím rozšířené funkčnosti.

## Přehled Python MCP SDK

Python MCP SDK poskytuje základ pro vytváření aplikací podporujících MCP. SDK zahrnuje:

- **Klientské knihovny**: Pro připojení k MCP serverům
- **Serverový rámec**: Pro vytváření vlastních MCP serverů
- **Protokolové obslužné moduly**: Pro správu komunikace
- **Integraci nástrojů**: Pro spouštění externích funkcí

## Praktická implementace: Phi-4 MCP klient

Podívejme se na implementaci v reálném světě pomocí mini modelu Phi-4 od Microsoftu integrovaného s MCP schopnostmi.

### Architektura systému

Implementace následuje vrstvenou architekturu:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Klíčové komponenty

#### 1. MCP klientské třídy

**BaseMCPClient**: Abstraktní základ poskytující společné funkce
- Asynchronní protokol správce kontextu
- Standardní definice rozhraní
- Správa zdrojů

**Phi4MiniMCPClient**: Implementace založená na STDIO
- Komunikace lokálního procesu
- Zpracování standardního vstupu/výstupu
- Správa podprocesů

**Phi4MiniSSEMCPClient**: Implementace Server-Sent Events
- HTTP streamovací komunikace
- Zpracování událostí v reálném čase
- Připojení k webovým serverům

#### 2. Integrace LLM

**OllamaClient**: Lokální hosting modelu
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: Vysoce výkonné podávání
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Pipeline pro zpracování nástrojů

Pipeline pro zpracování nástrojů transformuje MCP nástroje do formátů kompatibilních s jazykovými modely:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Začínáme: Průvodce krok za krokem

### Krok 1: Nastavení prostředí

Nainstalujte požadované závislosti:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Krok 2: Základní konfigurace

Nastavte proměnné prostředí:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Krok 3: Spuštění prvního MCP klienta

**Základní nastavení Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Použití backendu vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Připojení Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Vlastní MCP server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Krok 4: Programové použití

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Pokročilé funkce

### Podpora více backendů

Implementace podporuje backendy Ollama i vLLM, což vám umožňuje volbu podle vašich požadavků:

- **Ollama**: Vhodné pro lokální vývoj a testování
- **vLLM**: Optimalizováno pro produkci a scénáře s vysokou propustností

### Flexibilní komunikační protokoly

Podporovány jsou dva režimy připojení:

**STDIO režim**: Přímá komunikace procesu
- Nižší latence
- Vhodné pro lokální nástroje
- Jednoduché nastavení

**SSE režim**: Streamování přes HTTP
- Schopnost práce v síti
- Vhodné pro distribuované systémy
- Aktualizace v reálném čase

### Schopnosti integrace nástrojů

Systém může integrovat různé nástroje:
- Webová automatizace (Playwright)
- Operace se soubory
- Interakce s API
- Systémové příkazy
- Vlastní funkce

## Zpracování chyb a osvědčené postupy

### Komplexní správa chyb

Implementace zahrnuje robustní zpracování chyb pro:

**Chyby připojení:**
- Selhání MCP serveru
- Časové limity sítě
- Problémy s konektivitou

**Chyby při spouštění nástrojů:**
- Chybějící nástroje
- Validace parametrů
- Selhání při spuštění

**Chyby při zpracování odpovědí:**
- Problémy s parsováním JSON
- Nekonzistence formátu
- Anomálie v odpovědích LLM

### Osvědčené postupy

1. **Správa zdrojů**: Používejte asynchronní správce kontextu
2. **Zpracování chyb**: Implementujte komplexní bloky try-catch
3. **Logování**: Aktivujte vhodné úrovně logování
4. **Bezpečnost**: Validujte vstupy a čistěte výstupy
5. **Výkon**: Používejte pooling připojení a caching

## Aplikace v reálném světě

### Webová automatizace
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Zpracování dat
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrace API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimalizace výkonu

### Správa paměti
- Efektivní zpracování historie zpráv
- Správné čištění zdrojů
- Pooling připojení

### Optimalizace sítě
- Asynchronní HTTP operace
- Konfigurovatelné časové limity
- Plynulé zotavení z chyb

### Paralelní zpracování
- Neblokující I/O
- Paralelní spouštění nástrojů
- Efektivní asynchronní vzory

## Bezpečnostní aspekty

### Ochrana dat
- Bezpečná správa API klíčů
- Validace vstupů
- Čištění výstupů

### Síťová bezpečnost
- Podpora HTTPS
- Výchozí lokální endpointy
- Bezpečné zpracování tokenů

### Bezpečnost při spuštění
- Filtrování nástrojů
- Sandboxované prostředí
- Auditní logování

## Závěr

SLMs integrované s MCP představují zásadní změnu v oblasti vývoje AI aplikací. Kombinací efektivity malých modelů s výkonem externích nástrojů mohou vývojáři vytvářet inteligentní systémy, které jsou zároveň efektivní z hlediska zdrojů a vysoce schopné.

Implementace Phi-4 MCP klienta ukazuje, jak lze tuto integraci prakticky dosáhnout, a poskytuje pevný základ pro vytváření sofistikovaných aplikací poháněných AI.

Klíčové poznatky:
- MCP překonává propast mezi jazykovými modely a externími systémy
- SLMs nabízejí efektivitu bez ztráty schopností díky rozšíření o nástroje
- Modulární architektura umožňuje snadné rozšíření a přizpůsobení
- Správné zpracování chyb a bezpečnostní opatření jsou nezbytné pro produkční použití

Tento návod poskytuje základ pro vytváření vlastních aplikací podporujících MCP, čímž otevírá možnosti pro automatizaci, zpracování dat a integraci inteligentních systémů.

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). I když se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádné nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.