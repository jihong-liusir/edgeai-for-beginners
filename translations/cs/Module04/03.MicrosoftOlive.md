<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T23:31:18+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "cs"
}
-->
# Sekce 3: Microsoft Olive Optimization Suite

## Obsah
1. [Úvod](../../../Module04)
2. [Co je Microsoft Olive?](../../../Module04)
3. [Instalace](../../../Module04)
4. [Rychlý průvodce](../../../Module04)
5. [Příklad: Konverze Qwen3 na ONNX INT4](../../../Module04)
6. [Pokročilé použití](../../../Module04)
7. [Osvědčené postupy](../../../Module04)
8. [Řešení problémů](../../../Module04)
9. [Další zdroje](../../../Module04)

## Úvod

Microsoft Olive je výkonný a snadno použitelný nástroj pro optimalizaci modelů s ohledem na hardware, který zjednodušuje proces optimalizace strojového učení pro nasazení na různých hardwarových platformách. Ať už cílíte na CPU, GPU nebo specializované AI akcelerátory, Olive vám pomůže dosáhnout optimálního výkonu při zachování přesnosti modelu.

## Co je Microsoft Olive?

Olive je snadno použitelný nástroj pro optimalizaci modelů s ohledem na hardware, který kombinuje špičkové techniky v oblasti komprese, optimalizace a kompilace modelů. Funguje s ONNX Runtime jako komplexní řešení pro optimalizaci inferencí.

### Klíčové vlastnosti

- **Optimalizace s ohledem na hardware**: Automaticky vybírá nejlepší optimalizační techniky pro cílový hardware
- **Více než 40 vestavěných optimalizačních komponent**: Zahrnuje kompresi modelů, kvantizaci, optimalizaci grafů a další
- **Jednoduché CLI rozhraní**: Snadné příkazy pro běžné úkoly optimalizace
- **Podpora více frameworků**: Funguje s PyTorch, modely Hugging Face a ONNX
- **Podpora populárních modelů**: Olive dokáže automaticky optimalizovat populární architektury modelů jako Llama, Phi, Qwen, Gemma a další

### Výhody

- **Zkrácení času vývoje**: Není třeba ručně experimentovat s různými optimalizačními technikami
- **Zvýšení výkonu**: Významné zrychlení (až 6x v některých případech)
- **Nasazení napříč platformami**: Optimalizované modely fungují na různém hardwaru a operačních systémech
- **Zachování přesnosti**: Optimalizace zachovávají kvalitu modelu při zlepšení výkonu

## Instalace

### Předpoklady

- Python 3.8 nebo vyšší
- Správce balíčků pip
- Virtuální prostředí (doporučeno)

### Základní instalace

Vytvořte a aktivujte virtuální prostředí:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Nainstalujte Olive s funkcemi automatické optimalizace:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Volitelné závislosti

Olive nabízí různé volitelné závislosti pro další funkce:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Ověření instalace

```bash
olive --help
```

Pokud je instalace úspěšná, zobrazí se nápověda Olive CLI.

## Rychlý průvodce

### Vaše první optimalizace

Optimalizujme malý jazykový model pomocí funkce automatické optimalizace Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Co tento příkaz dělá

Proces optimalizace zahrnuje: získání modelu z lokálního úložiště, zachycení ONNX grafu a uložení vah do ONNX datového souboru, optimalizaci ONNX grafu a kvantizaci modelu na int4 pomocí metody RTN.

### Vysvětlení parametrů příkazu

- `--model_name_or_path`: Identifikátor modelu Hugging Face nebo lokální cesta
- `--output_path`: Adresář, kam bude uložen optimalizovaný model
- `--device`: Cílové zařízení (cpu, gpu)
- `--provider`: Poskytovatel inferencí (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Použití ONNX Runtime Generate AI pro inference
- `--precision`: Kvantizační přesnost (int4, int8, fp16)
- `--log_level`: Úroveň podrobnosti logování (0=minimální, 1=podrobné)

## Příklad: Konverze Qwen3 na ONNX INT4

Na základě poskytnutého příkladu Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) zde uvádíme, jak optimalizovat model Qwen3:

### Krok 1: Stažení modelu (volitelné)

Pro minimalizaci času stahování uložte pouze nezbytné soubory:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Optimalizace modelu Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Testování optimalizovaného modelu

Vytvořte jednoduchý Python skript pro testování vašeho optimalizovaného modelu:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura výstupu

Po optimalizaci bude váš výstupní adresář obsahovat:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Pokročilé použití

### Konfigurační soubory

Pro složitější pracovní postupy optimalizace můžete použít JSON konfigurační soubory:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Spuštění s konfigurací:

```bash
olive run --config config.json
```

### Optimalizace pro GPU

Pro optimalizaci CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pro DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Doladění s Olive

Olive podporuje také doladění modelů:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Osvědčené postupy

### 1. Výběr modelu
- Začněte s menšími modely pro testování (např. 0.5B-7B parametrů)
- Ujistěte se, že cílová architektura modelu je podporována Olive

### 2. Hardwarové úvahy
- Přizpůsobte optimalizaci cílovému hardwaru pro nasazení
- Použijte optimalizaci pro GPU, pokud máte hardware kompatibilní s CUDA
- Zvažte DirectML pro Windows zařízení s integrovanou grafikou

### 3. Výběr přesnosti
- **INT4**: Maximální komprese, mírná ztráta přesnosti
- **INT8**: Dobrá rovnováha mezi velikostí a přesností
- **FP16**: Minimální ztráta přesnosti, střední redukce velikosti

### 4. Testování a validace
- Vždy testujte optimalizované modely na vašich konkrétních případech použití
- Porovnávejte výkonnostní metriky (latence, propustnost, přesnost)
- Používejte reprezentativní vstupní data pro hodnocení

### 5. Iterativní optimalizace
- Začněte s automatickou optimalizací pro rychlé výsledky
- Používejte konfigurační soubory pro jemné doladění
- Experimentujte s různými optimalizačními kroky

## Řešení problémů

### Běžné problémy

#### 1. Problémy s instalací
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problémy s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problémy s pamětí
- Používejte menší velikosti batchů během optimalizace
- Zkuste kvantizaci s vyšší přesností nejdříve (int8 místo int4)
- Ujistěte se, že máte dostatek místa na disku pro ukládání modelů

#### 4. Chyby při načítání modelu
- Ověřte cestu k modelu a přístupová oprávnění
- Zkontrolujte, zda model vyžaduje `trust_remote_code=True`
- Ujistěte se, že všechny požadované soubory modelu jsou staženy

### Získání pomoci

- **Dokumentace**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Příklady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Další zdroje

### Oficiální odkazy
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokumentace**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Příklad**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Příklady z komunity
- **Jupyter Notebooks**: Dostupné v Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: Přehled AI Toolkit pro VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogové příspěvky**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Související nástroje
- **ONNX Runtime**: Vysoce výkonný inferenční engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Zdroj mnoha kompatibilních modelů — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Optimalizační pracovní postupy v cloudu — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Co dál

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

