<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T16:59:30+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "cs"
}
-->
# Sekce 3: Microsoft Olive Optimization Suite

## Obsah
1. [Úvod](../../../Module04)
2. [Co je Microsoft Olive?](../../../Module04)
3. [Instalace](../../../Module04)
4. [Rychlý průvodce](../../../Module04)
5. [Příklad: Konverze Qwen3 na ONNX INT4](../../../Module04)
6. [Pokročilé použití](../../../Module04)
7. [Nejlepší postupy](../../../Module04)
8. [Řešení problémů](../../../Module04)
9. [Další zdroje](../../../Module04)

## Úvod

Microsoft Olive je výkonný a snadno použitelný nástroj pro optimalizaci modelů, který zohledňuje hardware a zjednodušuje proces optimalizace modelů strojového učení pro nasazení na různých hardwarových platformách. Ať už cílíte na CPU, GPU nebo specializované AI akcelerátory, Olive vám pomůže dosáhnout optimálního výkonu při zachování přesnosti modelu.

## Co je Microsoft Olive?

Olive je snadno použitelný nástroj pro optimalizaci modelů, který zohledňuje hardware a kombinuje špičkové techniky v oblasti komprese, optimalizace a kompilace modelů. Funguje s ONNX Runtime jako komplexní řešení pro optimalizaci inferencí.

### Klíčové vlastnosti

- **Optimalizace zohledňující hardware**: Automaticky vybírá nejlepší optimalizační techniky pro cílový hardware
- **Více než 40 vestavěných optimalizačních komponent**: Zahrnuje kompresi modelů, kvantizaci, optimalizaci grafů a další
- **Jednoduché CLI rozhraní**: Snadné příkazy pro běžné úkoly optimalizace
- **Podpora více frameworků**: Funguje s PyTorch, modely Hugging Face a ONNX
- **Podpora populárních modelů**: Olive dokáže automaticky optimalizovat populární architektury modelů jako Llama, Phi, Qwen, Gemma a další

### Výhody

- **Zkrácení doby vývoje**: Není třeba ručně experimentovat s různými optimalizačními technikami
- **Zvýšení výkonu**: Významné zrychlení (až 6x v některých případech)
- **Nasazení napříč platformami**: Optimalizované modely fungují na různém hardwaru a operačních systémech
- **Zachování přesnosti**: Optimalizace zachovávají kvalitu modelu při zlepšení výkonu

## Instalace

### Předpoklady

- Python 3.8 nebo vyšší
- Správce balíčků pip
- Virtuální prostředí (doporučeno)

### Základní instalace

Vytvořte a aktivujte virtuální prostředí:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Nainstalujte Olive s funkcemi automatické optimalizace:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Volitelné závislosti

Olive nabízí různé volitelné závislosti pro další funkce:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Ověření instalace

```bash
olive --help
```

Pokud instalace proběhla úspěšně, zobrazí se nápověda Olive CLI.

## Rychlý průvodce

### Vaše první optimalizace

Optimalizujme malý jazykový model pomocí funkce automatické optimalizace Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Co tento příkaz dělá

Proces optimalizace zahrnuje: získání modelu z lokálního úložiště, zachycení ONNX grafu a uložení vah do ONNX datového souboru, optimalizaci ONNX grafu a kvantizaci modelu na int4 pomocí metody RTN.

### Vysvětlení parametrů příkazu

- `--model_name_or_path`: Identifikátor modelu Hugging Face nebo lokální cesta
- `--output_path`: Adresář, kam bude uložen optimalizovaný model
- `--device`: Cílové zařízení (cpu, gpu)
- `--provider`: Poskytovatel inferencí (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Použití ONNX Runtime Generate AI pro inferenci
- `--precision`: Kvantizační přesnost (int4, int8, fp16)
- `--log_level`: Úroveň podrobnosti logování (0=minimální, 1=podrobné)

## Příklad: Konverze Qwen3 na ONNX INT4

Na základě příkladu Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) zde uvádíme, jak optimalizovat model Qwen3:

### Krok 1: Stažení modelu (volitelné)

Pro minimalizaci doby stahování uložte pouze nezbytné soubory:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Optimalizace modelu Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Testování optimalizovaného modelu

Vytvořte jednoduchý Python skript pro testování optimalizovaného modelu:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura výstupu

Po optimalizaci bude váš výstupní adresář obsahovat:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Pokročilé použití

### Konfigurační soubory

Pro složitější optimalizační workflow můžete použít JSON konfigurační soubory:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Spuštění s konfigurací:

```bash
olive run --config config.json
```

### Optimalizace pro GPU

Pro optimalizaci CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pro DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Doladění modelů pomocí Olive

Olive podporuje také doladění modelů:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Nejlepší postupy

### 1. Výběr modelu
- Začněte s menšími modely pro testování (např. 0.5B-7B parametrů)
- Ujistěte se, že cílová architektura modelu je podporována Olive

### 2. Hardwarové úvahy
- Přizpůsobte optimalizaci cílovému hardwaru
- Použijte optimalizaci pro GPU, pokud máte hardware kompatibilní s CUDA
- Zvažte DirectML pro Windows zařízení s integrovanou grafikou

### 3. Výběr přesnosti
- **INT4**: Maximální komprese, mírná ztráta přesnosti
- **INT8**: Dobrá rovnováha mezi velikostí a přesností
- **FP16**: Minimální ztráta přesnosti, střední redukce velikosti

### 4. Testování a validace
- Vždy testujte optimalizované modely na vašich konkrétních případech použití
- Porovnávejte výkonnostní metriky (latence, propustnost, přesnost)
- Používejte reprezentativní vstupní data pro hodnocení

### 5. Iterativní optimalizace
- Začněte s automatickou optimalizací pro rychlé výsledky
- Použijte konfigurační soubory pro jemné doladění
- Experimentujte s různými optimalizačními kroky

## Řešení problémů

### Běžné problémy

#### 1. Problémy s instalací
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problémy s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problémy s pamětí
- Použijte menší velikosti batchů během optimalizace
- Nejprve zkuste kvantizaci s vyšší přesností (int8 místo int4)
- Ujistěte se, že máte dostatek místa na disku pro ukládání modelů

#### 4. Chyby při načítání modelu
- Ověřte cestu k modelu a přístupová oprávnění
- Zkontrolujte, zda model vyžaduje `trust_remote_code=True`
- Ujistěte se, že všechny potřebné soubory modelu jsou staženy

### Získání pomoci

- **Dokumentace**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Příklady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Další zdroje

### Oficiální odkazy
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Dokumentace**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Příklad**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Příklady z komunity
- **Jupyter Notebooks**: Dostupné v GitHub repozitáři Olive
- **VS Code Extension**: Rozšíření AI Toolkit používá Olive pro optimalizaci modelů
- **Blogové příspěvky**: Microsoft Open Source Blog obsahuje podrobné tutoriály Olive

### Související nástroje
- **ONNX Runtime**: Vysoce výkonný inferenční engine
- **Hugging Face Transformers**: Zdroj mnoha kompatibilních modelů
- **Azure Machine Learning**: Cloudové workflow pro optimalizaci

## ➡️ Co dál

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). I když se snažíme o přesnost, mějte na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.