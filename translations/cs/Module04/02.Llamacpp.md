<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T17:03:02+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "cs"
}
-->
# Sekce 2: Průvodce implementací Llama.cpp

## Obsah
1. [Úvod](../../../Module04)
2. [Co je Llama.cpp?](../../../Module04)
3. [Instalace](../../../Module04)
4. [Sestavení ze zdroje](../../../Module04)
5. [Kvantizace modelu](../../../Module04)
6. [Základní použití](../../../Module04)
7. [Pokročilé funkce](../../../Module04)
8. [Integrace s Pythonem](../../../Module04)
9. [Řešení problémů](../../../Module04)
10. [Nejlepší postupy](../../../Module04)

## Úvod

Tento komplexní návod vás provede vším, co potřebujete vědět o Llama.cpp, od základní instalace až po pokročilé scénáře použití. Llama.cpp je výkonná implementace v C++, která umožňuje efektivní inferenci velkých jazykových modelů (LLM) s minimálním nastavením a vynikajícím výkonem na různých hardwarových konfiguracích.

## Co je Llama.cpp?

Llama.cpp je framework pro inferenci LLM napsaný v C/C++, který umožňuje provozování velkých jazykových modelů lokálně s minimálním nastavením a špičkovým výkonem na široké škále hardwaru. Klíčové vlastnosti zahrnují:

### Hlavní funkce
- **Čistá implementace v C/C++** bez závislostí
- **Kompatibilita napříč platformami** (Windows, macOS, Linux)
- **Optimalizace hardwaru** pro různé architektury
- **Podpora kvantizace** (1,5-bitová až 8-bitová kvantizace)
- **Podpora akcelerace na CPU a GPU**
- **Efektivní využití paměti** pro omezené prostředí

### Výhody
- Efektivní provoz na CPU bez nutnosti specializovaného hardwaru
- Podpora více GPU backendů (CUDA, Metal, OpenCL, Vulkan)
- Lehký a přenosný
- Apple Silicon je prioritní - optimalizace prostřednictvím ARM NEON, Accelerate a Metal frameworků
- Podpora různých úrovní kvantizace pro snížení využití paměti

## Instalace

### Metoda 1: Předem sestavené binární soubory (doporučeno pro začátečníky)

#### Stažení z GitHub Releases
1. Navštivte [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Stáhněte si odpovídající binární soubor pro váš systém:
   - `llama-<verze>-bin-win-<funkce>-<arch>.zip` pro Windows
   - `llama-<verze>-bin-macos-<funkce>-<arch>.zip` pro macOS
   - `llama-<verze>-bin-linux-<funkce>-<arch>.zip` pro Linux

3. Rozbalte archiv a přidejte adresář do PATH vašeho systému.

#### Použití správců balíčků

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (různé distribuce):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Metoda 2: Python balíček (llama-cpp-python)

#### Základní instalace
```bash
pip install llama-cpp-python
```

#### S hardwarovou akcelerací
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Sestavení ze zdroje

### Předpoklady

**Systémové požadavky:**
- Kompilátor C++ (GCC, Clang nebo MSVC)
- CMake (verze 3.14 nebo vyšší)
- Git
- Nástroje pro sestavení pro vaši platformu

**Instalace předpokladů:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Nainstalujte Visual Studio 2022 s nástroji pro vývoj v C++
- Nainstalujte CMake z oficiálních stránek
- Nainstalujte Git

### Základní proces sestavení

1. **Naklonujte repozitář:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Konfigurujte sestavení:**
```bash
cmake -B build
```

3. **Sestavte projekt:**
```bash
cmake --build build --config Release
```

Pro rychlejší kompilaci použijte paralelní úlohy:
```bash
cmake --build build --config Release -j 8
```

### Sestavení specifické pro hardware

#### Podpora CUDA (NVIDIA GPU)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Podpora Metal (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### Podpora OpenBLAS (optimalizace CPU)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Podpora Vulkan
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Pokročilé možnosti sestavení

#### Debug sestavení
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### S dalšími funkcemi
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Kvantizace modelu

### Porozumění formátu GGUF

GGUF (Generalized GGML Unified Format) je optimalizovaný formát souborů navržený pro efektivní provozování velkých jazykových modelů pomocí Llama.cpp a dalších frameworků. Nabízí:

- Standardizované ukládání vah modelu
- Zlepšenou kompatibilitu napříč platformami
- Zvýšený výkon
- Efektivní zpracování metadat

### Typy kvantizace

Llama.cpp podporuje různé úrovně kvantizace:

| Typ | Bity | Popis | Použití |
|-----|------|-------|---------|
| F16 | 16 | Poloviční přesnost | Vysoká kvalita, velká paměť |
| Q8_0 | 8 | 8-bitová kvantizace | Dobrá rovnováha |
| Q4_0 | 4 | 4-bitová kvantizace | Střední kvalita, menší velikost |
| Q2_K | 2 | 2-bitová kvantizace | Nejmenší velikost, nižší kvalita |

### Konverze modelů

#### Z PyTorch na GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Přímé stažení z Hugging Face
Mnoho modelů je dostupných ve formátu GGUF na Hugging Face:
- Vyhledejte modely s "GGUF" v názvu
- Stáhněte si odpovídající úroveň kvantizace
- Použijte přímo s Llama.cpp

## Základní použití

### Rozhraní příkazového řádku

#### Jednoduchá generace textu
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Použití modelů z Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Serverový režim
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Běžné parametry

| Parametr | Popis | Příklad |
|----------|-------|---------|
| `-m` | Cesta k souboru modelu | `-m model.gguf` |
| `-p` | Text promptu | `-p "Hello world"` |
| `-n` | Počet generovaných tokenů | `-n 100` |
| `-c` | Velikost kontextu | `-c 4096` |
| `-t` | Počet vláken | `-t 8` |
| `-ngl` | GPU vrstvy | `-ngl 32` |
| `-temp` | Teplota | `-temp 0.7` |

### Interaktivní režim

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Pokročilé funkce

### Serverové API

#### Spuštění serveru
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### Použití API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Optimalizace výkonu

#### Správa paměti
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multithreading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### Akcelerace na GPU
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Integrace s Pythonem

### Základní použití s llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chatovací rozhraní

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streamování odpovědí

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integrace s LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Řešení problémů

### Běžné problémy a jejich řešení

#### Chyby při sestavení

**Problém: CMake nebyl nalezen**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problém: Kompilátor nebyl nalezen**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Problémy za běhu

**Problém: Načítání modelu selhalo**
- Ověřte cestu k souboru modelu
- Zkontrolujte oprávnění k souboru
- Ujistěte se, že máte dostatek RAM
- Vyzkoušejte různé úrovně kvantizace

**Problém: Nízký výkon**
- Aktivujte hardwarovou akceleraci
- Zvyšte počet vláken
- Použijte odpovídající kvantizaci
- Zkontrolujte využití paměti GPU

#### Problémy s pamětí

**Problém: Nedostatek paměti**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problémy specifické pro platformu

#### Windows
- Použijte kompilátor MinGW nebo Visual Studio
- Ujistěte se, že je správně nakonfigurován PATH
- Zkontrolujte, zda antivirus nezasahuje

#### macOS
- Aktivujte Metal pro Apple Silicon
- Použijte Rosetta 2 pro kompatibilitu, pokud je potřeba
- Zkontrolujte nástroje příkazového řádku Xcode

#### Linux
- Nainstalujte vývojové balíčky
- Zkontrolujte verze ovladačů GPU
- Ověřte instalaci CUDA toolkitu

## Nejlepší postupy

### Výběr modelu
1. **Vyberte odpovídající kvantizaci** podle vašeho hardwaru
2. **Zvažte velikost modelu** vs. kompromis v kvalitě
3. **Testujte různé modely** pro váš konkrétní případ použití

### Optimalizace výkonu
1. **Použijte akceleraci na GPU**, pokud je dostupná
2. **Optimalizujte počet vláken** pro váš CPU
3. **Nastavte odpovídající velikost kontextu** pro váš případ použití
4. **Aktivujte mapování paměti** pro velké modely

### Nasazení do produkce
1. **Použijte serverový režim** pro přístup k API
2. **Implementujte správné zpracování chyb**
3. **Monitorujte využití zdrojů**
4. **Nastavte logování a monitoring**

### Vývojový workflow
1. **Začněte s menšími modely** pro testování
2. **Používejte verzování** pro konfigurace modelů
3. **Dokumentujte své konfigurace**
4. **Testujte na různých platformách**

### Bezpečnostní aspekty
1. **Validujte vstupní prompty**
2. **Implementujte omezení rychlosti**
3. **Zabezpečte API endpointy**
4. **Monitorujte vzory zneužití**

## Závěr

Llama.cpp poskytuje výkonný a efektivní způsob, jak provozovat velké jazykové modely lokálně na různých hardwarových konfiguracích. Ať už vyvíjíte AI aplikace, provádíte výzkum nebo jen experimentujete s LLM, tento framework nabízí flexibilitu a výkon potřebný pro širokou škálu případů použití.

Klíčové poznatky:
- Vyberte metodu instalace, která nejlépe vyhovuje vašim potřebám
- Optimalizujte pro vaši konkrétní hardwarovou konfiguraci
- Začněte se základním použitím a postupně objevujte pokročilé funkce
- Zvažte použití Python bindingů pro snadnější integraci
- Dodržujte nejlepší postupy pro nasazení do produkce

Pro více informací a aktualizace navštivte [oficiální repozitář Llama.cpp](https://github.com/ggml-org/llama.cpp) a odkazujte se na komplexní dokumentaci a dostupné komunitní zdroje.

## ➡️ Co dál

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby pro automatický překlad [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádné nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.