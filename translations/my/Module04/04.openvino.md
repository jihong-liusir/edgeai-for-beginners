<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-19T00:29:10+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "my"
}
-->
# အပိုင်း ၄ : OpenVINO Toolkit Optimization Suite

## အကြောင်းအရာများ
1. [မိတ်ဆက်](../../../Module04)
2. [OpenVINO ဆိုတာဘာလဲ?](../../../Module04)
3. [တပ်ဆင်ခြင်း](../../../Module04)
4. [အမြန်စတင်လမ်းညွှန်](../../../Module04)
5. [ဥပမာ - OpenVINO ဖြင့် မော်ဒယ်များကို ပြောင်းလဲခြင်းနှင့် အကောင်းဆုံးဖြစ်အောင်လုပ်ခြင်း](../../../Module04)
6. [အဆင့်မြင့်အသုံးပြုမှု](../../../Module04)
7. [အကောင်းဆုံးလေ့ကျင့်မှုများ](../../../Module04)
8. [ပြဿနာများကို ဖြေရှင်းခြင်း](../../../Module04)
9. [အပိုဆောင်းအရင်းအမြစ်များ](../../../Module04)

## မိတ်ဆက်

OpenVINO (Open Visual Inference and Neural Network Optimization) သည် Intel မှ ဖွင့်လှစ်အရင်းအမြစ် toolkit ဖြစ်ပြီး cloud, on-premises, နှင့် edge ပတ်ဝန်းကျင်များတွင် AI ဖြေရှင်းနည်းများကို အမြန်နှင့် ထိရောက်စွာ အသုံးပြုနိုင်ရန် ရည်ရွယ်ထားသည်။ CPU, GPU, VPU, သို့မဟုတ် အထူး AI accelerators များကို ရည်ရွယ်ထားစေကာ OpenVINO သည် မော်ဒယ်တိကျမှုကို ထိန်းသိမ်းထားပြီး cross-platform deployment ကို အလွယ်တကူလုပ်ဆောင်နိုင်စေသည်။

## OpenVINO ဆိုတာဘာလဲ?

OpenVINO သည် ဖွင့်လှစ်အရင်းအမြစ် toolkit ဖြစ်ပြီး developer များအတွက် AI မော်ဒယ်များကို အကောင်းဆုံးဖြစ်အောင်လုပ်ခြင်း၊ ပြောင်းလဲခြင်း၊ နှင့် အမျိုးမျိုးသော hardware platform များတွင် ထိရောက်စွာ အသုံးပြုနိုင်ရန် ရည်ရွယ်ထားသည်။ ၎င်းတွင် အဓိက component သုံးခုပါဝင်သည် - OpenVINO Runtime (inference အတွက်), Neural Network Compression Framework (NNCF) (မော်ဒယ် optimization အတွက်), နှင့် OpenVINO Model Server (scalable deployment အတွက်)။

### အဓိကအင်္ဂါရပ်များ

- **Cross-Platform Deployment**: Linux, Windows, နှင့် macOS ကို Python, C++, နှင့် C API များဖြင့် ပံ့ပိုးသည်။
- **Hardware Acceleration**: CPU, GPU, VPU, နှင့် AI accelerators များအတွက် အလိုအလျောက် device ရွေးချယ်ခြင်းနှင့် optimization
- **Model Compression Framework**: NNCF မှတဆင့် quantization, pruning, နှင့် optimization နည်းလမ်းများ
- **Framework Compatibility**: TensorFlow, ONNX, PaddlePaddle, နှင့် PyTorch မော်ဒယ်များကို တိုက်ရိုက်ပံ့ပိုးသည်။
- **Generative AI Support**: OpenVINO GenAI ကို အသုံးပြု၍ large language models နှင့် generative AI application များကို deploy လုပ်နိုင်သည်။

### အကျိုးကျေးဇူးများ

- **Performance Optimization**: တိကျမှုကို အနည်းငယ်ပျောက်ဆုံးစေပြီး အမြန်နှုန်းကို တိုးမြှင့်နိုင်သည်။
- **Reduced Deployment Footprint**: အပြင်ပံ့ပိုးမှုများကို လျှော့ချပြီး installation နှင့် deployment ကို လွယ်ကူစေသည်။
- **Enhanced Start-up Time**: မော်ဒယ် loading နှင့် caching ကို optimize လုပ်ပြီး application initialization ကို မြန်ဆန်စေသည်။
- **Scalable Deployment**: edge device များမှ cloud infrastructure အထိ consistent API များဖြင့်
- **Production Ready**: လုပ်ငန်းအဆင့်အတန်းရှိသော ယုံကြည်စိတ်ချရမှုနှင့် အပြည့်အစုံသော documentation နှင့် community support

## တပ်ဆင်ခြင်း

### လိုအပ်ချက်များ

- Python 3.8 သို့မဟုတ် အထက်
- pip package manager
- Virtual environment (အကြံပြုသည်)
- သင့် hardware (Intel CPUs ကို အကြံပြုသည်၊ သို့သော် အမျိုးမျိုးသော architecture များကို ပံ့ပိုးသည်)

### အခြေခံတပ်ဆင်ခြင်း

Virtual environment တစ်ခုကို ဖန်တီးပြီး active လုပ်ပါ:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

OpenVINO Runtime ကို install လုပ်ပါ:

```bash
pip install openvino
```

မော်ဒယ် optimization အတွက် NNCF ကို install လုပ်ပါ:

```bash
pip install nncf
```

### OpenVINO GenAI တပ်ဆင်ခြင်း

Generative AI application များအတွက်:

```bash
pip install openvino-genai
```

### အပိုလိုအပ်ချက်များ

အထူးအသုံးပြုမှုများအတွက် အပို package များ:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### တပ်ဆင်မှုကို စစ်ဆေးပါ

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

အောင်မြင်ပါက OpenVINO version အချက်အလက်ကို တွေ့ရမည်။

## အမြန်စတင်လမ်းညွှန်

### သင့်ပထမဆုံး မော်ဒယ် optimization

OpenVINO ကို အသုံးပြု၍ Hugging Face မော်ဒယ်ကို ပြောင်းလဲပြီး optimize လုပ်ပါ:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### ဒီလုပ်ငန်းစဉ်က ဘာလုပ်သလဲ

Optimization workflow တွင် - Hugging Face မှ မူရင်းမော်ဒယ်ကို load လုပ်ခြင်း၊ OpenVINO Intermediate Representation (IR) format သို့ ပြောင်းလဲခြင်း၊ default optimization များကို အသုံးပြုခြင်း၊ နှင့် ရည်ရွယ်ထားသော hardware အတွက် compile လုပ်ခြင်းတို့ ပါဝင်သည်။

### အဓိက parameter များကို ရှင်းပြခြင်း

- `export=True`: မော်ဒယ်ကို OpenVINO IR format သို့ ပြောင်းလဲသည်။
- `compile=False`: runtime အထိ compilation ကို ရှောင်ရှားပြီး flexibility ရရှိစေသည်။
- `device`: ရည်ရွယ်ထားသော hardware ("CPU", "GPU", "AUTO" အလိုအလျောက်ရွေးချယ်မှု)
- `save_pretrained()`: optimized မော်ဒယ်ကို ပြန်လည်အသုံးပြုနိုင်ရန် save လုပ်သည်။

## ဥပမာ - OpenVINO ဖြင့် မော်ဒယ်များကို ပြောင်းလဲခြင်းနှင့် အကောင်းဆုံးဖြစ်အောင်လုပ်ခြင်း

### အဆင့် ၁: NNCF Quantization ဖြင့် မော်ဒယ်ပြောင်းလဲခြင်း

Post-training quantization ကို NNCF ဖြင့် အသုံးပြုရန်:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### အဆင့် ၂: Weight Compression ဖြင့် အဆင့်မြင့် optimization

Transformer-based မော်ဒယ်များအတွက် weight compression ကို အသုံးပြုပါ:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### အဆင့် ၃: Optimized မော်ဒယ်ဖြင့် Inference

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Output Structure

Optimization ပြီးနောက် သင့်မော်ဒယ် directory တွင် ပါဝင်မည်:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## အဆင့်မြင့်အသုံးပြုမှု

### NNCF YAML ဖြင့် Configuration

အဆင့်မြင့် optimization workflow များအတွက် NNCF configuration ဖိုင်များကို အသုံးပြုပါ:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Configuration ကို အသုံးပြုပါ:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU Optimization

GPU acceleration အတွက်:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Batch Processing Optimization

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Model Server Deployment

Optimized မော်ဒယ်များကို OpenVINO Model Server ဖြင့် deploy လုပ်ပါ:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Model server အတွက် client code:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## အကောင်းဆုံးလေ့ကျင့်မှုများ

### ၁. မော်ဒယ်ရွေးချယ်ခြင်းနှင့် ပြင်ဆင်ခြင်း
- Supported framework (PyTorch, TensorFlow, ONNX) မော်ဒယ်များကို အသုံးပြုပါ။
- မော်ဒယ် input များတွင် fixed shape သို့မဟုတ် known dynamic shape ရှိရန် သေချာပါ။
- Calibration အတွက် representative dataset များဖြင့် စမ်းသပ်ပါ။

### ၂. Optimization Strategy ရွေးချယ်ခြင်း
- **Post-training Quantization**: အမြန် optimization အတွက် စတင်ပါ။
- **Weight Compression**: Large language models နှင့် transformers အတွက် အကောင်းဆုံး။
- **Quantization-aware Training**: တိကျမှုအရေးကြီးသောအခါ အသုံးပြုပါ။

### ၃. Hardware-Specific Optimization
- **CPU**: INT8 quantization ကို အသုံးပြုပါ။
- **GPU**: FP16 precision နှင့် batch processing ကို အသုံးပြုပါ။
- **VPU**: မော်ဒယ် simplification နှင့် layer fusion ကို အာရုံစိုက်ပါ။

### ၄. Performance Tuning
- **Throughput Mode**: Batch processing အတွက်
- **Latency Mode**: Real-time application များအတွက်
- **AUTO Device**: OpenVINO ကို hardware ရွေးချယ်စေပါ။

### ၅. Memory Management
- Dynamic shape များကို memory overhead ရှောင်ရှားရန် အသုံးပြုပါ။
- Model caching ကို အသုံးပြု၍ အမြန် load လုပ်နိုင်စေပါ။
- Optimization အတွင်း memory usage ကို စောင့်ကြည့်ပါ။

### ၆. Accuracy Validation
- Optimized မော်ဒယ်များကို မူရင်း performance နှင့် နှိုင်းယှဉ်စစ်ဆေးပါ။
- Representative test dataset များကို အသုံးပြုပါ။
- Gradual optimization ကို စတင်ပါ (conservative setting များဖြင့် စတင်ပါ)။

## ပြဿနာများကို ဖြေရှင်းခြင်း

### အများဆုံးတွေ့ရသော ပြဿနာများ

#### ၁. Installation ပြဿနာများ
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### ၂. Model Conversion အမှားများ
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### ၃. Performance ပြဿနာများ
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### ၄. Memory ပြဿနာများ
- Optimization အတွင်း model batch size ကို လျှော့ချပါ။
- Large dataset များအတွက် streaming ကို အသုံးပြုပါ။
- Model caching ကို enable လုပ်ပါ: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### ၅. Accuracy ပျောက်ဆုံးမှု
- အမြင့် precision (INT8 အစား INT4) ကို အသုံးပြုပါ။
- Calibration dataset size ကို တိုးမြှင့်ပါ။
- Mixed precision optimization ကို အသုံးပြုပါ။

### Performance Monitoring

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### အကူအညီရယူခြင်း

- **Documentation**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Community Forum**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## အပိုဆောင်းအရင်းအမြစ်များ

### တရားဝင်လင့်များ
- **OpenVINO Homepage**: [openvino.ai](https://openvino.ai/)
- **GitHub Repository**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF Repository**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Model Zoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### သင်ကြားရေးအရင်းအမြစ်များ
- **OpenVINO Notebooks**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **အမြန်စတင်လမ်းညွှန်**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimization Guide**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integration Tools
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Performance Benchmarks
- **တရားဝင် Benchmarks**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF Model Zoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Community Examples
- **Jupyter Notebooks**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO notebooks repository တွင် အပြည့်အစုံသော tutorial များ
- **Sample Applications**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - computer vision, NLP, audio စသည်တို့အတွက် အမှန်တကယ်အသုံးပြုမှုများ
- **Blog Posts**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI နှင့် community blog post များ

### ဆက်စပ် Tools
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intel hardware အတွက် အပို optimization နည်းလမ်းများ
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - mobile နှင့် edge deployment နှိုင်းယှဉ်မှုများ
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - cross-platform inference engine အခြားရွေးချယ်မှုများ

## ➡️ နောက်တစ်ခု

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်ခြင်းတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါရှိနိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတရားရှိသော ရင်းမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။