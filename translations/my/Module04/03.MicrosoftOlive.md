<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-19T00:43:01+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "my"
}
-->
# အပိုင်း ၃ : Microsoft Olive Optimization Suite

## အကြောင်းအရာများ
1. [မိတ်ဆက်](../../../Module04)
2. [Microsoft Olive ဆိုတာဘာလဲ?](../../../Module04)
3. [တပ်ဆင်ခြင်း](../../../Module04)
4. [လွယ်ကူသော စတင်အသုံးပြုမှု လမ်းညွှန်](../../../Module04)
5. [ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းခြင်း](../../../Module04)
6. [အဆင့်မြင့် အသုံးပြုမှု](../../../Module04)
7. [အကောင်းဆုံး လုပ်ဆောင်နည်းများ](../../../Module04)
8. [ပြဿနာများ ဖြေရှင်းခြင်း](../../../Module04)
9. [အပိုဆောင်း အရင်းအမြစ်များ](../../../Module04)

## မိတ်ဆက်

Microsoft Olive သည် hardware-aware model optimization toolkit တစ်ခုဖြစ်ပြီး machine learning model များကို အမျိုးမျိုးသော hardware platform များတွင် အသုံးပြုရန် အဆင်ပြေစေသော optimization လုပ်ငန်းစဉ်ကို လွယ်ကူစွာ ပြုလုပ်နိုင်စေသည်။ CPU, GPU, သို့မဟုတ် AI accelerators အထူးပြု hardware များကို ရည်ရွယ်ထားပါက Olive သည် model accuracy ကို ထိန်းသိမ်းထားပြီး အကောင်းဆုံး performance ကို ရရှိစေပါသည်။

## Microsoft Olive ဆိုတာဘာလဲ?

Olive သည် hardware-aware model optimization tool တစ်ခုဖြစ်ပြီး model compression, optimization, နှင့် compilation အတွက် စက်မှုလုပ်ငန်းအဆင့် techniques များကို ပေါင်းစပ်အသုံးပြုထားသည်။ ONNX Runtime နှင့်အတူ E2E inference optimization solution အဖြစ် လုပ်ဆောင်ပါသည်။

### အဓိက အင်္ဂါရပ်များ

- **Hardware-Aware Optimization**: သင့်ရည်ရွယ်ထားသော hardware အတွက် အကောင်းဆုံး optimization techniques ကို အလိုအလျောက် ရွေးချယ်ပေးသည်။
- **40+ Built-in Optimization Components**: Model compression, quantization, graph optimization စသည်တို့ကို အကျုံးဝင်သည်။
- **လွယ်ကူသော CLI Interface**: Optimization လုပ်ငန်းစဉ်များအတွက် ရိုးရှင်းသော command များ။
- **Multi-Framework Support**: PyTorch, Hugging Face models, နှင့် ONNX တို့နှင့် လိုက်ဖက်သည်။
- **Popular Model Support**: Llama, Phi, Qwen, Gemma စသည်တို့ကို Olive မှ အလိုအလျောက် optimize လုပ်နိုင်သည်။

### အကျိုးကျေးဇူးများ

- **ဖွံ့ဖြိုးတိုးတက်မှု အချိန်လျော့ချခြင်း**: အမျိုးမျိုးသော optimization techniques များကို လက်တွေ့စမ်းသပ်ရန် မလိုအပ်တော့ပါ။
- **Performance တိုးတက်မှု**: အချို့သော အခြေအနေများတွင် ၆ ဆအထိ မြန်ဆန်မှု တိုးတက်မှု။
- **Cross-Platform Deployment**: Optimized models များကို အမျိုးမျိုးသော hardware နှင့် operating systems များတွင် အသုံးပြုနိုင်သည်။
- **Accuracy ထိန်းသိမ်းထားခြင်း**: Optimization များသည် model quality ကို ထိန်းသိမ်းထားပြီး performance ကို တိုးတက်စေသည်။

## တပ်ဆင်ခြင်း

### လိုအပ်ချက်များ

- Python 3.8 သို့မဟုတ် အထက်
- pip package manager
- Virtual environment (အကြံပြုသည်)

### အခြေခံ တပ်ဆင်ခြင်း

Virtual environment တစ်ခု ဖန်တီးပြီး အလုပ်လုပ်စေပါ:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Auto-optimization အင်္ဂါရပ်များပါရှိသော Olive ကို တပ်ဆင်ပါ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### အပိုဆောင်း လိုအပ်ချက်များ

Olive သည် အပိုဆောင်း features များအတွက် optional dependencies များကို ပေးထားသည်:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### တပ်ဆင်မှု အတည်ပြုခြင်း

```bash
olive --help
```

အောင်မြင်ပါက Olive CLI help message ကို တွေ့ရမည်။

## လွယ်ကူသော စတင်အသုံးပြုမှု လမ်းညွှန်

### သင့်ပထမဆုံး Optimization

Olive ၏ auto-optimization အင်္ဂါရပ်ကို အသုံးပြု၍ language model သေးငယ်တစ်ခုကို optimize လုပ်ပါ:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Command ၏ လုပ်ဆောင်မှု

Optimization လုပ်ငန်းစဉ်တွင် model ကို local cache မှ ရယူခြင်း၊ ONNX Graph ကို capture လုပ်ပြီး ONNX data file တွင် weight များကို သိမ်းဆည်းခြင်း၊ ONNX Graph ကို optimize လုပ်ခြင်း၊ RTN နည်းလမ်းဖြင့် model ကို int4 သို့ quantize လုပ်ခြင်းတို့ ပါဝင်သည်။

### Command Parameters ရှင်းလင်းချက်

- `--model_name_or_path`: Hugging Face model identifier သို့မဟုတ် local path
- `--output_path`: Optimized model ကို သိမ်းဆည်းမည့် directory
- `--device`: ရည်ရွယ်ထားသော device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ကို inference အတွက် အသုံးပြုရန်
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းခြင်း

Hugging Face ၏ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) ဥပမာအရ Qwen3 model ကို optimize လုပ်နည်း:

### အဆင့် ၁: Model ကို Download လုပ်ခြင်း (Optional)

Download အချိန်ကို လျော့ချရန် အရေးကြီးသော file များကိုသာ cache လုပ်ပါ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### အဆင့် ၂: Qwen3 Model ကို Optimize လုပ်ခြင်း

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### အဆင့် ၃: Optimized Model ကို စမ်းသပ်ခြင်း

Optimized model ကို စမ်းသပ်ရန် ရိုးရှင်းသော Python script တစ်ခု ဖန်တီးပါ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output Structure

Optimization ပြီးဆုံးပါက output directory တွင် အောက်ပါအရာများ ပါဝင်မည်:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## အဆင့်မြင့် အသုံးပြုမှု

### Configuration Files

ပိုမို ရှုပ်ထွေးသော optimization လုပ်ငန်းစဉ်များအတွက် JSON configuration files ကို အသုံးပြုနိုင်သည်:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Configuration ဖြင့် run လုပ်ပါ:

```bash
olive run --config config.json
```

### GPU Optimization

CUDA GPU optimization အတွက်:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Windows အတွက် DirectML:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive ဖြင့် Fine-tuning

Olive သည် model များကို fine-tuning လုပ်နိုင်သည်:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## အကောင်းဆုံး လုပ်ဆောင်နည်းများ

### ၁. Model ရွေးချယ်ခြင်း
- စမ်းသပ်ရန် model သေးငယ်များ (ဥပမာ 0.5B-7B parameters) ဖြင့် စတင်ပါ။
- Olive မှ support လုပ်သော model architecture ဖြစ်ကြောင်း သေချာပါစေ။

### ၂. Hardware ကို စဉ်းစားခြင်း
- Optimization ရည်ရွယ်ချက်ကို သင့် deployment hardware နှင့် ကိုက်ညီစေရန် သေချာပါ။
- CUDA-compatible hardware ရှိပါက GPU optimization ကို အသုံးပြုပါ။
- Windows များတွင် integrated graphics ရှိပါက DirectML ကို စဉ်းစားပါ။

### ၃. Precision ရွေးချယ်ခြင်း
- **INT4**: အများဆုံး compression, accuracy အနည်းငယ် လျော့နည်းမှု
- **INT8**: အရွယ်အစားနှင့် accuracy အချိုးကျမှု
- **FP16**: Accuracy အနည်းငယ် လျော့နည်းမှု၊ အရွယ်အစား လျော့နည်းမှု အလယ်အလတ်

### ၄. စမ်းသပ်ခြင်းနှင့် အတည်ပြုခြင်း
- Optimized models များကို သင့်ရည်ရွယ်ထားသော use cases များနှင့် စမ်းသပ်ပါ။
- Performance metrics (latency, throughput, accuracy) ကို နှိုင်းယှဉ်ပါ။
- အကဲဖြတ်ရန် သက်ဆိုင်သော input data ကို အသုံးပြုပါ။

### ၅. Iterative Optimization
- အမြန်ရလဒ်များအတွက် auto-optimization ဖြင့် စတင်ပါ။
- Fine-grained control အတွက် configuration files ကို အသုံးပြုပါ။
- Optimization passes များကို အမျိုးမျိုး စမ်းသပ်ပါ။

## ပြဿနာများ ဖြေရှင်းခြင်း

### အများဆုံး တွေ့ရသော ပြဿနာများ

#### ၁. Installation ပြဿနာများ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ၂. CUDA/GPU ပြဿနာများ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ၃. Memory ပြဿနာများ
- Optimization လုပ်စဉ်တွင် batch size သေးငယ်များကို အသုံးပြုပါ။
- int4 အစား int8 ဖြင့် precision မြင့်မားသော quantization ကို စမ်းသပ်ပါ။
- Model caching အတွက် disk space လုံလောက်စွာ ရှိကြောင်း သေချာပါ။

#### ၄. Model Loading Errors
- Model path နှင့် access permissions ကို စစ်ဆေးပါ။
- Model သည် `trust_remote_code=True` လိုအပ်ကြောင်း စစ်ဆေးပါ။
- Model files အားလုံးကို download လုပ်ပြီးကြောင်း သေချာပါ။

### အကူအညီ ရယူခြင်း

- **Documentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Examples**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## အပိုဆောင်း အရင်းအမြစ်များ

### တရားဝင် Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Examples
- **Jupyter Notebooks**: Olive GitHub repository တွင် ရရှိနိုင်သည်။
- **VS Code Extension**: AI Toolkit extension သည် Olive ကို အသုံးပြု၍ model optimization လုပ်ဆောင်သည်။
- **Blog Posts**: Microsoft Open Source Blog တွင် Olive tutorials အကြောင်း အသေးစိတ် ရှင်းလင်းထားသည်။

### ဆက်စပ် Tools
- **ONNX Runtime**: High-performance inference engine
- **Hugging Face Transformers**: အများဆုံး လိုက်ဖက်သော models ရင်းမြစ်
- **Azure Machine Learning**: Cloud-based optimization workflows

## ➡️ အခုနောက်မှာ ဘာလုပ်မလဲ

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါရှိနိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတရ အရင်းအမြစ်အဖြစ် ရှုလို့ရပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်မှုကို အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွဲအချော်များ သို့မဟုတ် အနားယူမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။