<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-23T00:24:27+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "my"
}
-->
# အပိုင်း ၃ : Microsoft Olive Optimization Suite

## အကြောင်းအရာများ
1. [မိတ်ဆက်](../../../Module04)
2. [Microsoft Olive ဆိုတာဘာလဲ?](../../../Module04)
3. [တပ်ဆင်ခြင်း](../../../Module04)
4. [အမြန်စတင်လမ်းညွှန်](../../../Module04)
5. [ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းခြင်း](../../../Module04)
6. [အဆင့်မြင့်အသုံးပြုမှု](../../../Module04)
7. [အကောင်းဆုံးအလေ့အကျင့်များ](../../../Module04)
8. [ပြဿနာများကို ဖြေရှင်းခြင်း](../../../Module04)
9. [အပိုဆောင်းအရင်းအမြစ်များ](../../../Module04)

## မိတ်ဆက်

Microsoft Olive သည် hardware-aware model optimization toolkit အားလုံးကို လွယ်ကူစွာ အသုံးပြုနိုင်သော အင်အားကြီး tool တစ်ခုဖြစ်ပြီး machine learning models များကို အမျိုးမျိုးသော hardware platforms များတွင် အသုံးပြုရန်အတွက် optimization လုပ်ခြင်းကို လွယ်ကူစေသည်။ CPU, GPU, သို့မဟုတ် AI accelerators အထူးပြု hardware များကို ရည်ရွယ်ထားပါက Olive သည် model accuracy ကို ထိန်းသိမ်းထားပြီး အကောင်းဆုံး performance ကို ရရှိစေပါသည်။

## Microsoft Olive ဆိုတာဘာလဲ?

Olive သည် hardware-aware model optimization tool တစ်ခုဖြစ်ပြီး model compression, optimization, နှင့် compilation အတွက် စက်မှုလုပ်ငန်းအဆင့် techniques များကို ပေါင်းစပ်ထားသည်။ ONNX Runtime နှင့်အတူ E2E inference optimization solution အဖြစ် အလုပ်လုပ်သည်။

### အဓိကအင်္ဂါရပ်များ

- **Hardware-Aware Optimization**: သင့်ရည်ရွယ်ထားသော hardware အတွက် အကောင်းဆုံး optimization techniques ကို အလိုအလျောက် ရွေးချယ်ပေးသည်
- **40+ Built-in Optimization Components**: Model compression, quantization, graph optimization စသည်တို့ကို အကျုံးဝင်သည်
- **Easy CLI Interface**: Optimization tasks များအတွက် လွယ်ကူသော command များ
- **Multi-Framework Support**: PyTorch, Hugging Face models, နှင့် ONNX တို့နှင့် အလုပ်လုပ်နိုင်သည်
- **Popular Model Support**: Olive သည် Llama, Phi, Qwen, Gemma စသည်တို့ကဲ့သို့သော model architectures များကို အလိုအလျောက် optimize လုပ်နိုင်သည်

### အကျိုးကျေးဇူးများ

- **Development Time လျှော့ချခြင်း**: အမျိုးမျိုးသော optimization techniques များကို လက်တွေ့စမ်းသပ်ရန် မလိုအပ်တော့ပါ
- **Performance တိုးတက်မှု**: အချို့သောအခြေအနေများတွင် 6x အထိ အမြန်နှုန်းတိုးတက်မှု
- **Cross-Platform Deployment**: Optimized models များကို hardware နှင့် operating systems များအမျိုးမျိုးတွင် အသုံးပြုနိုင်သည်
- **Accuracy ထိန်းသိမ်းထားခြင်း**: Optimization များသည် model quality ကို ထိန်းသိမ်းထားပြီး performance ကို တိုးတက်စေသည်

## တပ်ဆင်ခြင်း

### လိုအပ်ချက်များ

- Python 3.8 သို့မဟုတ် အထက်
- pip package manager
- Virtual environment (အကြံပြုသည်)

### အခြေခံတပ်ဆင်ခြင်း

Virtual environment တစ်ခုကို ဖန်တီးပြီး အလုပ်လုပ်စေပါ:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
Auto-optimization features ဖြင့် Olive ကို တပ်ဆင်ပါ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  

### အပိုလိုအပ်ချက်များ

Olive သည် အပို features များအတွက် အမျိုးမျိုးသော optional dependencies များကို ပေးသည်:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  

### တပ်ဆင်မှုကို အတည်ပြုပါ

```bash
olive --help
```
  
အောင်မြင်ပါက Olive CLI help message ကို တွေ့ရမည်။

## အမြန်စတင်လမ်းညွှန်

### သင့်ပထမဆုံး Optimization

Olive ၏ auto-optimization feature ကို အသုံးပြု၍ language model သေးငယ်တစ်ခုကို optimize လုပ်ကြပါစို့:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### Command ၏လုပ်ဆောင်မှု

Optimization လုပ်ငန်းစဉ်တွင် model ကို local cache မှရယူခြင်း၊ ONNX Graph ကို capture လုပ်ပြီး ONNX data file တွင် weight များကို သိမ်းဆည်းခြင်း၊ ONNX Graph ကို optimize လုပ်ခြင်း၊ RTN နည်းလမ်းဖြင့် model ကို int4 သို့ quantize လုပ်ခြင်းတို့ ပါဝင်သည်။

### Command Parameters ရှင်းလင်းချက်

- `--model_name_or_path`: Hugging Face model identifier သို့မဟုတ် local path
- `--output_path`: Optimized model ကို သိမ်းဆည်းမည့် directory
- `--device`: ရည်ရွယ်ထားသော device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ကို inference အတွက် အသုံးပြုပါ
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းခြင်း

Hugging Face ၏ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) မှပေးထားသော ဥပမာအရ Qwen3 model ကို optimize လုပ်နည်း:

### အဆင့် ၁: Model ကို Download လုပ်ပါ (Optional)

Download time ကို လျှော့ချရန် အရေးကြီးသော file များကိုသာ cache လုပ်ပါ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  

### အဆင့် ၂: Qwen3 Model ကို Optimize လုပ်ပါ

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### အဆင့် ၃: Optimized Model ကို စမ်းသပ်ပါ

Optimized model ကို စမ်းသပ်ရန် Python script ရေးပါ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  

### Output Structure

Optimization ပြီးဆုံးပါက output directory တွင် အောက်ပါအရာများပါဝင်မည်:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## အဆင့်မြင့်အသုံးပြုမှု

### Configuration Files

ပိုမိုရှုပ်ထွေးသော optimization workflows များအတွက် JSON configuration files ကို အသုံးပြုနိုင်သည်:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
Configuration ဖြင့် run လုပ်ပါ:

```bash
olive run --config config.json
```
  

### GPU Optimization

CUDA GPU optimization အတွက်:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
Windows အတွက် DirectML:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### Olive ဖြင့် Fine-tuning

Olive သည် models များကို fine-tuning လုပ်နိုင်သည်:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## အကောင်းဆုံးအလေ့အကျင့်များ

### ၁. Model ရွေးချယ်ခြင်း
- စမ်းသပ်ရန် model သေးငယ်များ (ဥပမာ 0.5B-7B parameters) ဖြင့် စတင်ပါ
- Olive မှ support လုပ်သော model architecture ဖြစ်ကြောင်း သေချာပါ

### ၂. Hardware ကို စဉ်းစားပါ
- Optimization target ကို deployment hardware နှင့် ကိုက်ညီစေပါ
- CUDA-compatible hardware ရှိပါက GPU optimization ကို အသုံးပြုပါ
- Windows များတွင် integrated graphics ရှိပါက DirectML ကို စဉ်းစားပါ

### ၃. Precision ရွေးချယ်ခြင်း
- **INT4**: Compression အများဆုံး၊ accuracy အနည်းငယ်ဆုံး
- **INT8**: Size နှင့် accuracy အချိုးကျ
- **FP16**: Accuracy အနည်းငယ်ဆုံးဆုံးရှုံးမှု၊ size လျှော့ချမှု အလယ်အလတ်

### ၄. စမ်းသပ်ခြင်းနှင့် အတည်ပြုခြင်း
- Optimized models များကို သင့်ရည်ရွယ်ထားသော use cases များနှင့် စမ်းသပ်ပါ
- Performance metrics (latency, throughput, accuracy) ကို နှိုင်းယှဉ်ပါ
- အကဲဖြတ်ရန် representative input data ကို အသုံးပြုပါ

### ၅. Iterative Optimization
- အမြန်ရလဒ်များအတွက် auto-optimization ဖြင့် စတင်ပါ
- Fine-grained control အတွက် configuration files ကို အသုံးပြုပါ
- Optimization passes များကို စမ်းသပ်ပါ

## ပြဿနာများကို ဖြေရှင်းခြင်း

### အများဆုံးတွေ့ရသော ပြဿနာများ

#### ၁. Installation ပြဿနာများ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  

#### ၂. CUDA/GPU ပြဿနာများ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  

#### ၃. Memory ပြဿနာများ
- Optimization လုပ်စဉ်တွင် batch size သေးငယ်များကို အသုံးပြုပါ
- int4 အစား int8 ဖြင့် precision မြင့်မားသော quantization ကို စမ်းသပ်ပါ
- Model caching အတွက် disk space လုံလောက်စေရန် သေချာပါ

#### ၄. Model Loading Errors
- Model path နှင့် access permissions ကို စစ်ဆေးပါ
- Model သည် `trust_remote_code=True` လိုအပ်ကြောင်း စစ်ဆေးပါ
- Model files အားလုံးကို download လုပ်ထားကြောင်း သေချာပါ

### အကူအညီရယူခြင်း

- **Documentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Examples**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## အပိုဆောင်းအရင်းအမြစ်များ

### တရားဝင်လင့်များ
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Examples
- **Jupyter Notebooks**: Olive GitHub repository တွင် ရရှိနိုင်သည် — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit for VS Code overview — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Posts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### ဆက်စပ် Tools များ
- **ONNX Runtime**: High-performance inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: အများဆုံး support လုပ်သော models များရရှိနိုင်သောအရင်းအမြစ် — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-based optimization workflows — https://learn.microsoft.com/azure/machine-learning/

## ➡️ အခုနောက်တစ်ခု

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

