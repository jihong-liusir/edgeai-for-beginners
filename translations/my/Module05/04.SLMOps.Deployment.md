<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ad0c054ebd3de404d997d1707a513c70",
  "translation_date": "2025-09-19T01:20:33+00:00",
  "source_file": "Module05/04.SLMOps.Deployment.md",
  "language_code": "my"
}
-->
# အပိုင်း ၄: Deployment - ထုတ်လုပ်မှုအဆင့်သင့်မော်ဒယ်အကောင်အထည်ဖော်ခြင်း

## အကျဉ်းချုပ်

ဒီလမ်းညွှန်ချက်က Foundry Local ကို အသုံးပြုပြီး Fine-tuned Quantized Models တွေကို Deploy လုပ်တဲ့ အပြည့်အစုံလုပ်ငန်းစဉ်ကို လမ်းညွှန်ပေးမှာပါ။ မော်ဒယ်ပြောင်းလဲခြင်း၊ Quantization Optimization နဲ့ Deployment Configuration ကို အစအဆုံး လေ့လာနိုင်ပါမယ်။

## မလိုအပ်မဖြစ်လိုအပ်ချက်များ

စတင်မလုပ်ခင်မှာ အောက်ပါအရာတွေရှိကြောင်း သေချာပါစေ-

- ✅ Deployment အတွက် အသင့်ဖြစ်တဲ့ Fine-tuned ONNX Model
- ✅ Windows သို့မဟုတ် Mac ကွန်ပျူတာ
- ✅ Python 3.10 သို့မဟုတ် အထက်
- ✅ အနည်းဆုံး 8GB RAM ရှိရမည်
- ✅ Foundry Local ကို သင့်စနစ်မှာ Install လုပ်ထားရမည်

## အပိုင်း ၁: ပတ်ဝန်းကျင် Setup

### လိုအပ်တဲ့ Tools တွေ Install လုပ်ခြင်း

Terminal (Windows မှာ Command Prompt, Mac မှာ Terminal) ကို ဖွင့်ပြီး အောက်ပါ Command တွေကို အစဉ်လိုက် Run လုပ်ပါ-

```bash
# Update transformers library to the latest version
pip install transformers -U

# Install Microsoft Olive (model conversion tool)
pip install git+https://github.com/microsoft/Olive.git

# Install ONNX Runtime GenAI
git clone https://github.com/microsoft/onnxruntime-genai
cd onnxruntime-genai && python build.py --config Release
pip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl

# Install additional dependencies
pip install onnx onnxruntime numpy
```

⚠️ **အရေးကြီးမှတ်ချက်**: CMake version 3.31 သို့မဟုတ် အထက်ကိုလည်း လိုအပ်ပါမယ်။ [cmake.org](https://cmake.org/download/) မှာ Download လုပ်နိုင်ပါတယ်။

## အပိုင်း ၂: မော်ဒယ်ပြောင်းလဲခြင်းနှင့် Quantization

### Format ရွေးချယ်ခြင်း

Fine-tuned Small Language Models အတွက် **ONNX Format** ကို အသုံးပြုရန် အကြံပြုပါတယ်၊ အကြောင်းက-

- 🚀 Performance Optimization ပိုကောင်းစေခြင်း
- 🔧 Hardware-agnostic Deployment
- 🏭 ထုတ်လုပ်မှုအဆင့်သင့်စွမ်းရည်
- 📱 Cross-platform Compatibility

### နည်းလမ်း ၁: Command တစ်ခုတည်းဖြင့် ပြောင်းလဲခြင်း (အကြံပြု)

Fine-tuned Model ကို Direct ပြောင်းလဲဖို့ အောက်ပါ Command ကို အသုံးပြုပါ-

```bash
olive auto-opt \
    --model_name_or_path /path/to/your/finetuned/model \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/your-model-name/onnx \
    --log_level 1
```

**Parameter ရှင်းလင်းချက်:**
- `--model_name_or_path`: Fine-tuned Model ရဲ့ Path
- `--device cpu`: Optimization အတွက် CPU ကို အသုံးပြု
- `--precision int4`: INT4 Quantization (File Size ကို 75% လျှော့ချနိုင်)
- `--output_path`: ပြောင်းလဲပြီး Model ရဲ့ Output Path

### နည်းလမ်း ၂: Configuration File Approach (အဆင့်မြင့်အသုံးပြုသူများ)

`finetuned_conversion_config.json` ဆိုတဲ့ Configuration File တစ်ခု Create လုပ်ပါ-

```json
{
    "input_model": {
        "type": "HfModel",
        "model_path": "/path/to/your/finetuned/model",
        "task": "text-generation"
    },
    "systems": {
        "local_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "execution_providers": [
                        "CPUExecutionProvider"
                    ]
                }
            ]
        }
    },
    "passes": {
        "builder": {
            "type": "ModelBuilder",
            "config": {
                "precision": "int4",
                "quantization_config": {
                    "block_size": 128,
                    "is_symmetric": false
                }
            }
        }
    },
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/output/your-finetuned-model-onnx"
}
```

ပြီးရင် အောက်ပါ Command ကို Run လုပ်ပါ-

```bash
olive run --config ./finetuned_conversion_config.json
```

### Quantization Options နှိုင်းယှဉ်ခြင်း

| Precision | File Size | Inference Speed | Model Quality | Recommended Use |
|-----------|-----------|-----------------|---------------|-----------------|
| FP16      | Baseline × 0.5 | အမြန် | အကောင်းဆုံး | High-end Hardware |
| INT8      | Baseline × 0.25 | အလွန်မြန် | ကောင်း | Balanced Choice |
| INT4      | Baseline × 0.125 | အမြန်ဆုံး | လက်ခံနိုင် | Resource-limited |

💡 **အကြံပြုချက်**: Deployment ပထမဆုံးအကြိမ်အတွက် INT4 Quantization ကို စတင်အသုံးပြုပါ။ Quality မကျေနပ်ရင် INT8 သို့မဟုတ် FP16 ကို စမ်းကြည့်ပါ။

## အပိုင်း ၃: Foundry Local Deployment Configuration

### မော်ဒယ် Configuration ဖန်တီးခြင်း

Foundry Local Models Directory ကို သွားပါ-

```bash
foundry cache cd ./models/
```

မော်ဒယ် Directory Structure ကို ဖန်တီးပါ-

```bash
mkdir -p ./models/custom/your-finetuned-model
```

မော်ဒယ် Directory ထဲမှာ `inference_model.json` Configuration File ကို ဖန်တီးပါ-

```json
{
  "Name": "your-finetuned-model-int4",
  "Description": "Fine-tuned quantized model",
  "Version": "1.0",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  },
  "ModelConfig": {
    "max_length": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

### မော်ဒယ်အလိုက် Template Configurations

#### Qwen Series Models အတွက်:

```json
{
  "Name": "qwen-finetuned-int4",
  "PromptTemplate": {
    "system": "<|im_start|>system\n{Content}<|im_end|>",
    "user": "<|im_start|>user\n{Content}<|im_end|>",
    "assistant": "<|im_start|>assistant\n{Content}<|im_end|>",
    "prompt": "<|im_start|>user\n{Content}<|im_end|>\n<|im_start|>assistant"
  }
}
```

## အပိုင်း ၄: မော်ဒယ်စမ်းသပ်ခြင်းနှင့် Optimization

### မော်ဒယ် Installation ကို Verify လုပ်ခြင်း

Foundry Local က သင့်မော်ဒယ်ကို အသိအမှတ်ပြုနိုင်ကြောင်း စစ်ဆေးပါ-

```bash
foundry cache ls
```

`your-finetuned-model-int4` ကို List ထဲမှာ တွေ့ရပါမယ်။

### မော်ဒယ်စမ်းသပ်ခြင်း စတင်ခြင်း

```bash
foundry model run your-finetuned-model-int4
```

### Performance Benchmarking

စမ်းသပ်စဉ် Key Metrics တွေကို Monitor လုပ်ပါ-

1. **Response Time**: Response တစ်ခုစီအတွက် အချိန်ပျမ်းမျှကို တိုင်းတာပါ
2. **Memory Usage**: RAM အသုံးပြုမှုကို စောင့်ကြည့်ပါ
3. **CPU Utilization**: Processor Load ကို စစ်ဆေးပါ
4. **Output Quality**: Response ရဲ့ သက်ဆိုင်မှုနဲ့ Coherence ကို အကဲဖြတ်ပါ

### Quality Validation Checklist

- ✅ Fine-tuned Domain Queries တွေကို Model က သင့်တော်စွာ ပြန်လည်ဖြေရှင်းပေးခြင်း
- ✅ Response Format က မျှော်မှန်းထားတဲ့ Output Structure နဲ့ ကိုက်ညီခြင်း
- ✅ အချိန်ကြာရှည်အသုံးပြုမှုမှာ Memory Leak မရှိခြင်း
- ✅ Input Length မတူညီမှုများအတွက် တိုးတက်မှု တစ်မျိုးတည်းရှိခြင်း
- ✅ Edge Cases နဲ့ Invalid Inputs တွေကို သင့်တော်စွာ ကိုင်တွယ်နိုင်ခြင်း

## အကျဉ်းချုပ်

ဂုဏ်ယူပါတယ်! သင်အောင်မြင်စွာ ပြီးမြောက်ခဲ့ပါပြီ-

- ✅ Fine-tuned Model Format ပြောင်းလဲခြင်း
- ✅ Model Quantization Optimization
- ✅ Foundry Local Deployment Configuration
- ✅ Performance Tuning နဲ့ Troubleshooting

---

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်ခြင်းတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူလဘာသာစကားဖြင့် အာဏာတရားရှိသော ရင်းမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူက ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲသုံးစားမှု သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။