{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392a8a8",
   "metadata": {},
   "source": [
    "# အစည်းအဝေး ၂ – RAG အကဲဖြတ်ခြင်း ragas ဖြင့်\n",
    "\n",
    "ragas အတိုင်းအတာများဖြင့် အနည်းဆုံး RAG ပိုင်းလိုင်းကို အကဲဖြတ်ပါ။ answer_relevancy, faithfulness, context_precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34473b",
   "metadata": {},
   "source": [
    "# အခြေအနေ\n",
    "ဤအခြေအနေသည် Retrieval Augmented Generation (RAG) pipeline အနည်းငယ်ကို ဒေသတွင်းတွင် အကဲဖြတ်သည်။ ကျွန်ုပ်တို့:\n",
    "- သေးငယ်သော စက်မှုဇာတိစာရွက်စာတမ်းများကို သတ်မှတ်သည်။\n",
    "- စာရွက်စာတမ်းများကို embed လုပ်ပြီး ရိုးရှင်းသော ဆင်တူမှု ရှာဖွေသူကို အကောင်အထည်ဖော်သည်။\n",
    "- ဒေသတွင်းမော်ဒယ် (Foundry Local / OpenAI-compatible) ကို အသုံးပြု၍ အခြေခံထားသော အဖြေများကို ဖန်တီးသည်။\n",
    "- ragas metrics (`answer_relevancy`, `faithfulness`, `context_precision`) ကိုတွက်ချက်သည်။\n",
    "- အမြန် iteration အတွက် အဖြေသက်ဆိုင်မှုကိုသာတွက်ချက်ရန် FAST mode (env `RAG_FAST=1`) ကို ပံ့ပိုးသည်။\n",
    "\n",
    "ဤ notebook ကို အသုံးပြု၍ သင်၏ ဒေသတွင်းမော်ဒယ် + embeddings stack သည် အချက်အလက်အခြေခံထားသော အဖြေများကို ဖန်တီးနိုင်ကြောင်း အကြီးမားသော စာရွက်စာတမ်းများကို အတိုင်းအတာချဲ့မီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမီမ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb1aa2",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက်: အချိုးအစားလိုအပ်ချက်များ ထည့်သွင်းခြင်း\n",
    "လိုအပ်သော စာကြောင်းများကို ထည့်သွင်းသည်။\n",
    "- `foundry-local-sdk` သည် ဒေသတွင်း မော်ဒယ်များကို စီမံခန့်ခွဲရန်။\n",
    "- `openai` သည် client interface အတွက်။\n",
    "- `sentence-transformers` သည် အထူအကျယ် embeddings အတွက်။\n",
    "- `ragas` + `datasets` သည် အကဲဖြတ်ခြင်းနှင့် metric တွက်ချက်မှုအတွက်။\n",
    "- `langchain-openai` adapter သည် ragas LLM interface အတွက်။\n",
    "\n",
    "ပြန်လည်လုပ်ဆောင်ရန် အန္တရာယ်ကင်းသည်။ ပတ်ဝန်းကျင်ကို အဆင်သင့်ပြင်ဆင်ပြီးသားဖြစ်ပါက ကျော်သွားနိုင်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff641221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (ragas pulls datasets, evaluate, etc.)\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers ragas datasets numpy langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e82678",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက်: Core Imports & Metrics\n",
    "အဓိက libraries နှင့် ragas metrics များကို load လုပ်ခြင်း။ အရေးပါသော အစိတ်အပိုင်းများမှာ:\n",
    "- Embeddings အတွက် SentenceTransformer။\n",
    "- `evaluate` + ရွေးချယ်ထားသော ragas metrics များ။\n",
    "- အကဲဖြတ်မှု corpus တည်ဆောက်ရန် `Dataset`။\n",
    "ဤ imports များသည် remote calls မဖြစ်စေပါ (embeddings အတွက် model cache load ဖြစ်နိုင်ခြင်းကို မျှလွှဲ၍)။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01938",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက် - Toy Corpus နှင့် QA Ground Truth\n",
    "အမှတ်တရ corpus (`DOCS`) တစ်ခု၊ အသုံးပြုသူမေးခွန်းများနှင့် မျှော်မှန်းထားသော ground truth အဖြေများကို သတ်မှတ်ထားသည်။ ဤအရာများသည် အပြင်မှဒေတာများကို ရယူရန်မလိုဘဲ မြန်ဆန်ပြီး အတိအကျသော metric တွက်ချက်မှုကို ခွင့်ပြုသည်။ အမှန်တကယ်အခြေအနေများတွင် သင်သည် ထုတ်လုပ်မှုမေးခွန်းများနှင့် စနစ်တကျရွေးချယ်ထားသော အဖြေများကို ရွေးချယ်ရမည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27307d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    " 'Foundry Local exposes a local OpenAI-compatible endpoint.',\n",
    " 'RAG retrieves relevant context snippets before generation.',\n",
    " 'Local inference improves privacy and reduces latency.',\n",
    "]\n",
    "QUESTIONS = [\n",
    " 'What advantage does local inference offer?',\n",
    " 'How does RAG improve grounding?',\n",
    "]\n",
    "GROUND_TRUTH = [\n",
    " 'It reduces latency and preserves privacy.',\n",
    " 'It adds retrieved context snippets for factual grounding.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b2ec",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက်: Service Init, Embeddings & Safety Patch\n",
    "Foundry Local manager ကို စတင်လုပ်ဆောင်ပြီး `promptTemplate` အတွက် schema-drift safety patch ကို အသုံးပြုသည်။ model id ကို ဖြေရှင်းပြီး OpenAI-compatible client ကို ဖန်တီးကာ စာရွက်စာတမ်းများ၏ corpus အတွက် dense embeddings များကို ကြိုတင်တွက်ချက်သည်။ ဒါကတော့ retrieval + generation အတွက် အသုံးပြုနိုင်သော အခြေအနေကို ပြင်ဆင်ပေးခြင်းဖြစ်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service running: True | Endpoint: http://127.0.0.1:57127/v1\n",
      "Cached models: [FoundryModelInfo(alias=gpt-oss-20b, id=gpt-oss-20b-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=9882 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu:4, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0)]\n",
      "Using model id: Phi-4-mini-instruct-cuda-gpu:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leestott\\AppData\\Local\\miniforge\\envs\\demo\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from foundry_local.models import FoundryModelInfo\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Safe monkeypatch for potential null promptTemplate field (schema drift guard) ---\n",
    "_original_from_list_response = FoundryModelInfo.from_list_response\n",
    "\n",
    "def _safe_from_list_response(response):  # type: ignore\n",
    "    try:\n",
    "        if isinstance(response, dict) and response.get(\"promptTemplate\") is None:\n",
    "            response[\"promptTemplate\"] = {}\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        print(f\"Warning normalizing promptTemplate: {e}\")\n",
    "    return _original_from_list_response(response)\n",
    "\n",
    "if getattr(FoundryModelInfo.from_list_response, \"__name__\", \"\") != \"_safe_from_list_response\":\n",
    "    FoundryModelInfo.from_list_response = staticmethod(_safe_from_list_response)  # type: ignore\n",
    "# --- End monkeypatch ---\n",
    "\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS','phi-3.5-mini')\n",
    "manager = FoundryLocalManager(alias)\n",
    "print(f\"Service running: {manager.is_service_running()} | Endpoint: {manager.endpoint}\")\n",
    "print('Cached models:', manager.list_cached_models())\n",
    "model_info = manager.get_model_info(alias)\n",
    "model_id = model_info.id\n",
    "print(f\"Using model id: {model_id}\")\n",
    "\n",
    "# OpenAI-compatible client\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import numpy as np\n",
    "doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d24a8",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက်: Retriever Function\n",
    "နိမ့်ကျသော embedding များပေါ်တွင် dot product ကို အသုံးပြုသော ရိုးရိုး vector similarity retriever ကို သတ်မှတ်သည်။ အမြင့်ဆုံး k (default k=2) စာရွက်များကို ပြန်ပေးသည်။ အလုပ်လုပ်နေသော စနစ်တွင် အတိုင်းအတာနှင့် latency အတွက် ANN index (FAISS, Chroma, Milvus) ဖြင့် အစားထိုးပါ။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af32d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return [DOCS[i] for i in sims.argsort()[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f284e",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက် - Generation Function\n",
    "`generate` သည် ကန့်သတ်ထားသော prompt (context ကိုသာ အသုံးပြုရန် စနစ်က ညွှန်ကြားသည်) ကို ဖန်တီးပြီး local model ကို ခေါ်သုံးသည်။ အနိမ့်ဆုံး temperature (0.1) သည် ဖန်တီးမှုထက် သက်သေပြနိုင်သော အချက်အလက်များကို ပိုမိုကောင်းမွန်စွာ ထုတ်ယူရန် အားပေးသည်။ ပြန်လည်ရရှိသော အဖြေစာသားကို ဖြတ်တောက်ပြီး ပြန်ပေးသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7798ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, contexts):\n",
    "    ctx = \"\\n\".join(contexts)\n",
    "    messages = [\n",
    "        {'role':'system','content':'Answer using ONLY the provided context.'},\n",
    "        {'role':'user','content':f\"Context:\\n{ctx}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=model_id, messages=messages, max_tokens=120, temperature=0.1)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde788",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက် - Fallback Client Initialization\n",
    "မူလ `client` ကို initialize လုပ်ခြင်းကို ကျော်သွားသည့်အခါ သို့မဟုတ် မအောင်မြင်ခဲ့သည့်အခါတွင် NameError ဖြစ်ပေါ်မှုကို ကာကွယ်ရန် `client` ရှိနေသည်ကို သေချာစေပါသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback client initialization (added after patch failure)\n",
    "try:\n",
    "    client  # type: ignore\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "    print('Initialized OpenAI-compatible client (late init).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17386ee",
   "metadata": {},
   "source": [
    "### ရှင်းလင်းချက်: အကဲဖြတ်မှု Loop နှင့် Metrics\n",
    "အကဲဖြတ်မှုအတွက် dataset ကို တည်ဆောက်ပါ (လိုအပ်သောကော်လံများ: question, answer, contexts, ground_truths, reference)၊ ထို့နောက် ရွေးချယ်ထားသော ragas metrics များကို iteration လုပ်ပါ။\n",
    "\n",
    "အကောင်းမြှင့်တင်မှု:\n",
    "- FAST_MODE သည် အမြန်စမ်းသပ်မှုများအတွက် အဖြေသက်ဆိုင်မှုကိုသာ ကန့်သတ်သည်။\n",
    "- Per-metric loop သည် metric တစ်ခုမအောင်မြင်ပါက ပြန်လည်တွက်ချက်မှုအပြည့်အစုံကို ရှောင်ရှားနိုင်သည်။\n",
    "\n",
    "Output သည် metric -> score (မအောင်မြင်ပါက NaN) အဖြစ် dict တစ်ခုကို ထုတ်ပေးသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset columns: ['question', 'answer', 'contexts', 'ground_truths', 'reference']\n",
      "Metrics to compute: ['answer_relevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy finished in 78.1s -> 0.6975427764759168\n",
      "RAG evaluation results: {'answer_relevancy': 0.6975427764759168}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6975427764759168}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build evaluation dataset with required columns (including 'reference' for context_precision)\n",
    "records = []\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTH):\n",
    "    ctxs = retrieve(q)\n",
    "    ans = generate(q, ctxs)\n",
    "    records.append({\n",
    "        'question': q,\n",
    "        'answer': ans,\n",
    "        'contexts': ctxs,\n",
    "        'ground_truths': [gt],\n",
    "        'reference': gt\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.run_config import RunConfig\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=model_id, base_url=manager.endpoint, api_key=manager.api_key or 'not-needed', temperature=0.0, timeout=60)\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "# Fast mode: only answer_relevancy unless RAG_FAST=0\n",
    "FAST_MODE = os.getenv('RAG_FAST','1') == '1'\n",
    "metrics = [answer_relevancy] if FAST_MODE else [answer_relevancy, faithfulness, context_precision]\n",
    "\n",
    "base_timeout = 45 if FAST_MODE else 120\n",
    "\n",
    "ds = Dataset.from_list(records)\n",
    "print('Evaluation dataset columns:', ds.column_names)\n",
    "print('Metrics to compute:', [m.name for m in metrics])\n",
    "\n",
    "results_dict = {}\n",
    "for metric in metrics:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        cfg = RunConfig(timeout=base_timeout, max_workers=1)\n",
    "        partial = evaluate(ds, metrics=[metric], llm=ragas_llm, embeddings=LocalEmbeddings(), run_config=cfg, show_progress=False)\n",
    "        raw_val = partial[metric.name]\n",
    "        if isinstance(raw_val, list):\n",
    "            numeric = [v for v in raw_val if isinstance(v, (int, float))]\n",
    "            score = float(np.nanmean(numeric)) if numeric else math.nan\n",
    "        else:\n",
    "            score = float(raw_val)\n",
    "        results_dict[metric.name] = score\n",
    "    except Exception as e:\n",
    "        results_dict[metric.name] = math.nan\n",
    "        print(f\"Metric {metric.name} failed: {e}\")\n",
    "    finally:\n",
    "        print(f\"{metric.name} finished in {time.time()-t0:.1f}s -> {results_dict[metric.name]}\")\n",
    "\n",
    "print('RAG evaluation results:', results_dict)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ဝန်ခံချက်**:  \nဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက်ဘာသာပြန်ဆိုမှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူလဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတည်သော ရင်းမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအမှားများ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့ တာဝန်မယူပါ။\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "4895a2e01d85b98643a177c89ff7757f",
   "translation_date": "2025-10-08T12:37:11+00:00",
   "source_file": "Workshop/notebooks/session02_rag_eval_ragas.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}