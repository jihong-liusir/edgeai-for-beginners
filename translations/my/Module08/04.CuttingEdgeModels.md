<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "66aa3b998a0c5c69721c661bdaaf6830",
  "translation_date": "2025-09-23T01:14:04+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "my"
}
-->
# အစည်းအဝေး ၄: နောက်ဆုံးပေါ် မော်ဒယ်များ – LLMs, SLMs, နှင့် On-Device Inference

## အကျဉ်းချုပ်

LLMs နှင့် SLMs ကိုနှိုင်းယှဉ်ပါ၊ ဒေသတွင်းနှင့် cloud inference အားသာချက်များနှင့် အားနည်းချက်များကို သုံးသပ်ပါ၊ နှင့် Phi နှင့် ONNX Runtime ကို အသုံးပြု၍ EdgeAI ရှုခင်းများကို ပြသသည့် နမူနာများကို အကောင်အထည်ဖော်ပါ။ Chainlit RAG, WebGPU inference ရွေးချယ်မှုများနှင့် Open WebUI ပေါင်းစည်းမှုကိုလည်း အထူးပြောပါမည်။

ရင်းမြစ်များ:
- Foundry Local စာရွက်စာတမ်းများ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- ONNX Runtime GenAI: https://onnxruntime.ai/
- Open WebUI အသုံးပြုနည်း (Open WebUI ဖြင့် chat app): https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui

## သင်ယူရမည့် ရည်ရွယ်ချက်များ
- LLM နှင့် SLM အားသာချက်/အားနည်းချက်များကို ကုန်ကျစရိတ်၊ latency နှင့် တိကျမှုအရ နားလည်ပါ
- အထူးလုပ်ငန်းလိုအပ်ချက်များအတွက် ဒေသတွင်းနှင့် cloud inference ကို ရွေးချယ်ပါ
- Chainlit ဖြင့် သေးငယ်သော RAG နမူနာကို အကောင်အထည်ဖော်ပါ
- Browser-side acceleration အတွက် WebGPU ကို စူးစမ်းပါ
- Open WebUI ကို Foundry Local နှင့် ချိတ်ဆက်ပါ

## အပိုင်း ၁: LLM နှင့် SLM – ဆုံးဖြတ်ချက် Matrix

စဉ်းစားရန်:
- Latency: SLMs ကို device ပေါ်တွင် အသုံးပြုပါက sub-second အဖြေများကို ပေးနိုင်သည်
- ကုန်ကျစရိတ်: ဒေသတွင်း inference သည် cloud ကုန်ကျစရိတ်ကို လျှော့ချသည်
- Privacy: အရေးကြီးသော ဒေတာများကို device ပေါ်တွင်ထားရှိနိုင်သည်
- Capability: LLMs သည် ရှုပ်ထွေးသော အလုပ်များတွင် SLMs ထက် ပိုမိုကောင်းမွန်နိုင်သည်
- Reliability: hybrid မဟာဗျူဟာများသည် downtime အန္တရာယ်ကို လျှော့ချနိုင်သည်

## အပိုင်း ၂: ဒေသတွင်းနှင့် Cloud – Hybrid ပုံစံများ

- ဒေသတွင်းကို ဦးစားပေးပြီး cloud fallback ကို ကြီးမား/ရှုပ်ထွေးသော prompts များအတွက် အသုံးပြုပါ
- Cloud ကို ဦးစားပေးပြီး privacy-sensitive သို့မဟုတ် offline ရှုခင်းများအတွက် ဒေသတွင်းကို အသုံးပြုပါ
- Task အမျိုးအစားအလိုက် ခွဲခြားပေးပါ (code-gen ကို DeepSeek, general chat ကို Phi/Qwen)

## အပိုင်း ၃: RAG Chat App with Chainlit (သေးငယ်သော)

Dependencies များကို install လုပ်ပါ:
```powershell
python -m venv .venv
.\.venv\Scripts\activate
pip install chainlit requests
```

`app.py`:
```python
import chainlit as cl
import requests

BASE_URL = "http://localhost:8000"
MODEL = "phi-4-mini"

@cl.on_chat_start
async def start():
    await cl.Message(content="Welcome to Local RAG Chat!").send()

@cl.on_message
async def main(message: cl.Message):
    # Basic passthrough to Foundry Local (extend with retrieval)
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": message.content}],
        "max_tokens": 300
    }
    r = requests.post(f"{BASE_URL}/v1/chat/completions", json=payload, timeout=60)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    await cl.Message(content=content).send()
```

Run:
```powershell
# Ensure a model is running
foundry model run phi-4-mini

# Launch Chainlit
chainlit run app.py -w
```

တိုးချဲ့ပါ: retriever (ဒေသတွင်းဖိုင်များ) တစ်ခုကို ထည့်သွင်းပြီး retrieved context ကို user prompt အရှေ့တွင် ထည့်ပါ။

## အပိုင်း ၄: WebGPU Inference (Heads-up)

WebGPU ကို အသုံးပြု၍ browser တွင် သေးငယ်သော မော်ဒယ်များကို တိုက်ရိုက် run လုပ်ပါ။ ဤနည်းလမ်းသည် privacy-first နမူနာများနှင့် zero-install အတွေ့အကြုံများအတွက် အကောင်းဆုံးဖြစ်သည်။ ONNX Runtime Web နှင့် WebGPU execution provider ကို အသုံးပြု၍ အနည်းဆုံး၊ အဆင့်ဆင့် နမူနာကို အောက်တွင် ဖော်ပြထားသည်။

၁) WebGPU support ကို စစ်ဆေးပါ
- Chromium browsers: chrome://gpu → “WebGPU” enabled ဖြစ်ကြောင်း အတည်ပြုပါ
- Programmatic check (code တွင်လည်း စစ်ဆေးပါမည်): `if (!('gpu' in navigator)) { /* no WebGPU */ }`

၂) အနည်းဆုံး project တစ်ခုကို ဖန်တီးပါ
Folder တစ်ခုနှင့် ဖိုင်နှစ်ခုကို ဖန်တီးပါ: `index.html` နှင့် `main.js`။

`index.html`
```html
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU + ONNX Runtime Web (Demo)</title>
    <!-- Load ONNX Runtime Web (WebGPU build) from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
      body { font-family: system-ui, sans-serif; margin: 2rem; }
      pre { background: #f5f5f5; padding: 1rem; overflow:auto; }
    </style>
  </head>
  <body>
    <h1>WebGPU + ONNX Runtime Web</h1>
    <p id="status">Initializing…</p>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
  </body>
</html>
```

`main.js`
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
  outputEl.textContent += `${msg}\n`;
}

(async () => {
  try {
    if (!('gpu' in navigator)) {
      statusEl.textContent = 'WebGPU not available in this browser.';
      return;
    }
    statusEl.textContent = 'WebGPU detected. Loading model…';

    // Use a tiny identity model (or any small ONNX) hosted remotely.
    // For demo, we synthesize a trivial input and expect the same output shape.
    // Replace with your own model URL if desired.
    const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx?download=true';

    // Create a WebGPU session
    const session = await ort.InferenceSession.create(modelUrl, {
      executionProviders: ['webgpu']
    });
    log('Session created with WebGPU.');

    // Create a dummy input of the correct shape for MNIST (1x1x28x28)
    const inputData = new Float32Array(1 * 1 * 28 * 28);
    // Minimal non-zero to avoid NaNs in some models
    inputData[0] = 1.0;
    const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);

    // The MNIST model input name is typically 'Input3' or similar; we can inspect session.inputNames
    const feeds = {};
    const inputName = session.inputNames[0];
    feeds[inputName] = input;

    const results = await session.run(feeds);
    const outputName = session.outputNames[0];
    const output = results[outputName]; // Float32Array logits

    statusEl.textContent = 'Inference complete.';
    log(`Input name: ${inputName}`);
    log(`Output name: ${outputName}`);
    log(`Output length: ${output.data.length}`);

    // Argmax to get predicted digit
    let bestIdx = 0; let bestVal = -Infinity;
    for (let i = 0; i < output.data.length; i++) {
      if (output.data[i] > bestVal) { bestVal = output.data[i]; bestIdx = i; }
    }
    log(`Predicted class (argmax): ${bestIdx}`);
  } catch (e) {
    statusEl.textContent = 'Error during inference.';
    log(e?.stack || String(e));
  }
})();
```

၃) ဒေသတွင်းတွင် serve လုပ်ပါ (Windows cmd.exe)
Browser သည် မော်ဒယ်ကို fetch လုပ်နိုင်ရန် simple static server ကို အသုံးပြုပါ။

```cmd
REM In cmd.exe
mkdir webgpu-demo
cd webgpu-demo
REM Save index.html and main.js here

REM Option A: Python (if installed)
py -m http.server 5173

REM Option B: Node serve (requires Node.js)
REM npx serve -p 5173 .
```

Browser တွင် http://localhost:5173 ကို ဖွင့်ပါ။ Initialization logs, WebGPU ဖြင့် session creation, နှင့် argmax prediction ကို တွေ့ရပါမည်။

၄) Troubleshooting
- WebGPU မရရှိပါက: Chrome/Edge ကို update လုပ်ပြီး GPU drivers ကို အဆင့်မြှင့်ပါ၊ chrome://flags တွင် “Enable WebGPU” ကို စစ်ဆေးပါ။
- CORS သို့မဟုတ် fetch errors ဖြစ်ပါက: ဖိုင်များကို http:// (file:// မဟုတ်) ဖြင့် serve လုပ်ပြီး model URL သည် cross-origin requests ကို ခွင့်ပြုကြောင်း အတည်ပြုပါ။
- CPU ကို fallback လုပ်ပါ: `executionProviders: ['wasm']` ကို ပြောင်းပြီး baseline behavior ကို စစ်ဆေးပါ။

၅) နောက်တစ်ဆင့်များ
- Domain-specific ONNX model (ဥပမာ: image classification သို့မဟုတ် သေးငယ်သော text model) ကို ထည့်သွင်းပါ။
- အမှန်တကယ် input များအတွက် preprocessing/postprocessing logic ကို ထည့်ပါ။
- ကြီးမားသော မော်ဒယ်များ သို့မဟုတ် production latency အတွက် Foundry Local သို့မဟုတ် ONNX Runtime Server ကို ဦးစားပေးပါ။

## အပိုင်း ၅: Open WebUI + Foundry Local (အဆင့်ဆင့်)

ဤနည်းလမ်းသည် Open WebUI ကို Foundry Local ၏ OpenAI-compatible endpoint နှင့် ချိတ်ဆက်ပြီး ဒေသတွင်း chat UI ကို ဖန်တီးသည်။

၁) လိုအပ်ချက်များ
- Foundry Local install လုပ်ပြီး အလုပ်လုပ်နေသည် (`foundry --version`)
- ဒေသတွင်းတွင် run လုပ်ရန် model တစ်ခု (ဥပမာ: `phi-4-mini`)
- Docker Desktop install လုပ်ထားသည် (Open WebUI အတွက် အကြံပြုသည်)

၂) Foundry Local ဖြင့် model တစ်ခုကို စတင်ပါ
```powershell
foundry model run phi-4-mini
```
ဤနည်းလမ်းသည် OpenAI-compatible API ကို `http://localhost:8000` တွင် expose လုပ်သည်။

၃) Open WebUI ကို စတင်ပါ (Docker)
```powershell
# Pull and run Open WebUI
docker pull ghcr.io/open-webui/open-webui:main

docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```
မှတ်ချက်များ:
- Windows တွင် `host.docker.internal` သည် container ကို host တွင် `localhost` သို့ ရောက်နိုင်စေသည်။
- `OPENAI_API_BASE_URL` ကို Foundry Local ၏ endpoint သို့နှင့် dummy `OPENAI_API_KEY` ကို သတ်မှတ်ထားသည်။

၄) Open WebUI UI မှတစ်ဆင့် configure လုပ်ပါ (အခြားနည်းလမ်း)
- http://localhost:3000 သို့ browse လုပ်ပါ
- Initial setup (admin user) ကို ပြီးစီးပါ
- Settings → Models/Providers သို့ သွားပါ
- Base URL ကို သတ်မှတ်ပါ: `http://host.docker.internal:8000/v1`
- API Key ကို သတ်မှတ်ပါ: `local-key` (placeholder)
- Save လုပ်ပါ

၅) Test prompt တစ်ခုကို run လုပ်ပါ
- Open WebUI chat တွင် model name `phi-4-mini` ကို ရွေးချယ်ပါ သို့မဟုတ် ထည့်သွင်းပါ
- Prompt: “List five benefits of on-device AI inference.”
- ဒေသတွင်း model မှ streamed response ကို တွေ့ရပါမည်

၆) Troubleshooting
```powershell
# Verify Foundry Local is serving and responding
curl http://localhost:8000/v1/models

# Check Docker logs for Open WebUI
docker logs -f open-webui

# If you get ECONNREFUSED, ensure the model is running and port 8000 is open
# If the model name doesn't appear, try typing it manually in Open WebUI
```

၇) Optional: Open WebUI data ကို ထိန်းသိမ်းပါ
```powershell
# Stop and remove the container
docker stop open-webui
docker rm open-webui

# Re-run with a volume to persist data
mkdir %USERPROFILE%\openwebui-data

docker run -d --name open-webui -p 3000:8080 ^
  -v %USERPROFILE%\openwebui-data:/app/backend/data ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 ^
  -e OPENAI_API_KEY=local-key ^
  ghcr.io/open-webui/open-webui:main
```

## လက်တွေ့လုပ်ဆောင်ရန် စစ်ဆေးစာရင်း
- [ ] SLM နှင့် LLM တို့၏ ဒေသတွင်း response/latency ကို နှိုင်းယှဉ်ပါ
- [ ] Chainlit demo ကို model နှစ်ခုအနည်းဆုံးတွင် run လုပ်ပါ
- [ ] Open WebUI ကို ဒေသတွင်း endpoint နှင့် ချိတ်ဆက်ပြီး စမ်းသပ်ပါ

## နောက်တစ်ဆင့်များ
- အစည်းအဝေး ၅ တွင် agent workflows အတွက် ပြင်ဆင်ပါ
- Hybrid local/cloud သည် ROI ကို တိုးတက်စေသည့် ရှုခင်းများကို ဖော်ထုတ်ပါ

---

