<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "949045c54206448d55d987e8b23a82cf",
  "translation_date": "2025-09-25T02:21:07+00:00",
  "source_file": "Module08/04.CuttingEdgeModels.md",
  "language_code": "my"
}
-->
# အခန်း ၄: Chainlit ဖြင့် ထုတ်လုပ်မှုအဆင့် Chat Applications တည်ဆောက်ခြင်း

## အကျဉ်းချုပ်

ဒီအခန်းမှာ Chainlit နဲ့ Microsoft Foundry Local ကို အသုံးပြုပြီး ထုတ်လုပ်မှုအဆင့် chat applications တည်ဆောက်ပုံကို လေ့လာပါမယ်။ AI စကားဝိုင်းများအတွက် ခေတ်မီ web interface များဖန်တီးခြင်း၊ streaming responses ကို အကောင်အထည်ဖော်ခြင်း၊ error handling နဲ့ user experience design ကို ထိရောက်စွာ အသုံးပြုထားတဲ့ chat applications များကို တည်ဆောက်ခြင်းတို့ကို သင်ယူနိုင်ပါမယ်။

**သင်တည်ဆောက်မယ့်အရာများ:**
- **Chainlit Chat App**: Streaming responses ပါဝင်တဲ့ ခေတ်မီ web UI
- **WebGPU Demo**: Privacy-first applications အတွက် browser-based inference  
- **Open WebUI Integration**: Foundry Local ဖြင့် ပရော်ဖက်ရှင်နယ် chat interface
- **Production Patterns**: Error handling, monitoring, နဲ့ deployment strategies

## သင်ယူရမယ့်အရာများ

- Chainlit ဖြင့် ထုတ်လုပ်မှုအဆင့် chat applications တည်ဆောက်ခြင်း
- User experience ကို မြှင့်တင်ပေးတဲ့ streaming responses ကို အကောင်အထည်ဖော်ခြင်း
- Foundry Local SDK integration patterns ကို ကျွမ်းကျင်စွာ အသုံးပြုခြင်း
- Error handling နဲ့ graceful degradation ကို အကောင်းဆုံး အသုံးပြုခြင်း
- Chat applications များကို အခြေအနေအမျိုးမျိုးအတွက် deploy နဲ့ configure လုပ်ခြင်း
- Conversational AI အတွက် ခေတ်မီ web UI patterns ကို နားလည်ခြင်း

## ကြိုတင်လိုအပ်ချက်များ

- **Foundry Local**: Install လုပ်ပြီး run လုပ်ထား ([Installation Guide](https://learn.microsoft.com/azure/ai-foundry/foundry-local/))
- **Python**: 3.10 သို့မဟုတ် အထက် version နဲ့ virtual environment capability
- **Model**: အနည်းဆုံး model တစ်ခု load လုပ်ထား (`foundry model run phi-4-mini`)
- **Browser**: WebGPU support ရှိတဲ့ ခေတ်မီ web browser (Chrome/Edge)
- **Docker**: Open WebUI integration အတွက် (optional)

## အပိုင်း ၁: ခေတ်မီ Chat Applications ကို နားလည်ခြင်း

### Architecture အကျဉ်းချုပ်

```
User Browser ←→ Chainlit UI ←→ Python Backend ←→ Foundry Local ←→ AI Model
      ↓              ↓              ↓              ↓            ↓
   Web UI      Event Handlers   OpenAI Client   HTTP API    Local GPU
```

### အဓိက နည်းပညာများ

**Foundry Local SDK Patterns:**
- `FoundryLocalManager(alias)`: Service ကို အလိုအလျောက် စီမံခန့်ခွဲခြင်း
- `manager.endpoint` နဲ့ `manager.api_key`: Connection details
- `manager.get_model_info(alias).id`: Model identification

**Chainlit Framework:**
- `@cl.on_chat_start`: Chat session များကို initialize လုပ်ခြင်း
- `@cl.on_message`: User message များကို handle လုပ်ခြင်း  
- `cl.Message().stream_token()`: Real-time streaming
- UI ကို အလိုအလျောက် ဖန်တီးခြင်း နဲ့ WebSocket စီမံခန့်ခွဲမှု

## အပိုင်း ၂: Local နဲ့ Cloud Decision Matrix

### Performance Characteristics

| Aspect | Local (Foundry) | Cloud (Azure OpenAI) |
|--------|-----------------|---------------------|
| **Latency** | 🚀 50-200ms (network မလိုအပ်) | ⏱️ 200-2000ms (network အပေါ် မူတည်) |
| **Privacy** | 🔒 Data က device ထဲမှာပဲ ရှိ | ⚠️ Data ကို cloud သို့ ပို့ |
| **Cost** | 💰 Hardware ရှိပြီးရင် အခမဲ့ | 💸 Token အလိုက် ပေးရ |
| **Offline** | ✅ Internet မလိုအပ် | ❌ Internet လိုအပ် |
| **Model Size** | ⚠️ Hardware အပေါ် မူတည် | ✅ အကြီးဆုံး models ကို အသုံးပြုနိုင် |
| **Scaling** | ⚠️ Hardware အပေါ် မူတည် | ✅ အကန့်အသတ်မရှိ scaling |

### Hybrid Strategy Patterns

**Local-First with Fallback:**
```python
async def hybrid_completion(prompt: str, complexity_threshold: int = 100):
    if len(prompt.split()) < complexity_threshold:
        return await local_completion(prompt)  # Fast, private
    else:
        return await cloud_completion(prompt)   # Complex reasoning
```

**Task-Based Routing:**
```python
async def smart_routing(prompt: str, task_type: str):
    routing_rules = {
        "code_generation": "local",     # Privacy-sensitive
        "creative_writing": "cloud",    # Benefits from larger models
        "data_analysis": "local",       # Fast iteration needed
        "research": "cloud"             # Requires broad knowledge
    }
    
    if routing_rules.get(task_type) == "local":
        return await foundry_completion(prompt)
    else:
        return await azure_completion(prompt)
```

## အပိုင်း ၃: Sample 04 - Chainlit Chat Application

### Quick Start

```cmd
# Navigate to Module08 directory  
cd Module08

# Start your preferred model
foundry model run phi-4-mini

# Run the Chainlit application (avoiding port conflicts)
chainlit run samples\04\app.py -w --port 8080
```

Application က `http://localhost:8080` မှာ ခေတ်မီ chat interface နဲ့ အလိုအလျောက် ဖွင့်ပါမယ်။

### Core Implementation

Sample 04 application က ထုတ်လုပ်မှုအဆင့် patterns များကို ပြသထားပါတယ်:

**Automatic Service Discovery:**
```python
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

# Global variables for client and model
client = None
model_name = None

async def initialize_client():
    global client, model_name
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    try:
        # Use FoundryLocalManager for proper service management
        manager = FoundryLocalManager(alias)
        model_info = manager.get_model_info(alias)
        
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-required"
        )
        model_name = model_info.id if model_info else alias
        return True
    except Exception as e:
        # Fallback to manual configuration
        base_url = os.environ.get("BASE_URL", "http://localhost:51211")
        client = OpenAI(base_url=f"{base_url}/v1", api_key="not-required")
        model_name = alias
        return True
```

**Streaming Chat Handler:**
```python
@cl.on_message
async def main(message: cl.Message):
    # Create streaming response
    msg = cl.Message(content="")
    await msg.send()
    
    stream = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": message.content}
        ],
        stream=True
    )
    
    # Stream tokens in real-time
    for chunk in stream:
        if chunk.choices[0].delta.content:
            await msg.stream_token(chunk.choices[0].delta.content)
    
    await msg.update()
```

### Configuration Options

**Environment Variables:**

| Variable | Description | Default | Example |
|----------|-------------|---------|----------|
| `MODEL` | အသုံးပြုမယ့် model alias | `phi-4-mini` | `qwen2.5-7b-instruct` |
| `BASE_URL` | Foundry Local endpoint | Auto-detected | `http://localhost:51211` |
| `API_KEY` | API key (optional for local) | `""` | `your-api-key` |

**Advanced Usage:**
```cmd
# Use different model
set MODEL=qwen2.5-7b-instruct
chainlit run samples\04\app.py -w --port 8080

# Use different ports (avoid 51211 which is used by Foundry Local)
chainlit run samples\04\app.py -w --port 3000
chainlit run samples\04\app.py -w --port 5000
```

## အပိုင်း ၄: Jupyter Notebooks ဖန်တီးခြင်း နဲ့ အသုံးပြုခြင်း

### Notebook Support အကျဉ်းချုပ်

Sample 04 မှာ comprehensive Jupyter notebook (`chainlit_app.ipynb`) ပါဝင်ပြီး:

- **📚 ပညာရေးအကြောင်းအရာများ**: အဆင့်ဆင့် လေ့လာရေးအကြောင်းအရာများ
- **🔬 အပြန်အလှန် စမ်းသပ်မှု**: Code cells များကို run နဲ့ စမ်းသပ်ခြင်း
- **📊 အမြင်အာရုံ ပြသမှုများ**: Charts, diagrams, နဲ့ output visualization
- **🛠️ ဖွံ့ဖြိုးရေး Tools**: Testing နဲ့ debugging အခွင့်အရေးများ

### သင့်ကိုယ်ပိုင် Notebooks ဖန်တီးခြင်း

#### အဆင့် ၁: Jupyter Environment ကို Set Up လုပ်ပါ

```cmd
# Ensure you're in the Module08 directory
cd Module08

# Activate your virtual environment
.venv\Scripts\activate

# Install Jupyter and dependencies
pip install jupyter notebook jupyterlab ipykernel
pip install -r requirements.txt

# Register the kernel for VS Code
python -m ipykernel install --user --name=foundry-local --display-name="Foundry Local"
```

#### အဆင့် ၂: Notebook အသစ် တစ်ခု ဖန်တီးပါ

**VS Code အသုံးပြုခြင်း:**
1. Module08 directory ကို VS Code မှာ ဖွင့်ပါ
2. `.ipynb` extension နဲ့ ဖိုင်အသစ် တစ်ခု ဖန်တီးပါ
3. Prompt လုပ်တဲ့အခါ "Foundry Local" kernel ကို ရွေးပါ
4. သင့်အကြောင်းအရာများနဲ့ cells များကို စတင်ထည့်ပါ

**Jupyter Lab အသုံးပြုခြင်း:**
```cmd
# Start Jupyter Lab
jupyter lab

# Navigate to samples/04/ and create new notebook
# Choose Python 3 kernel
```

### Notebook Structure Best Practices

#### Cell Organization

```python
# Cell 1: Imports and Setup
import os
import sys
import chainlit as cl
from openai import OpenAI
from foundry_local import FoundryLocalManager

print("✅ Libraries imported successfully")
```

```python
# Cell 2: Configuration and Client Setup
class FoundryClientManager:
    def __init__(self, model_name="phi-4-mini"):
        self.model_name = model_name
        self.client = None
        
    def initialize_client(self):
        # Client initialization logic
        pass

# Initialize and test
client_manager = FoundryClientManager()
result = client_manager.initialize_client()
print(f"Client initialized: {result}")
```

### အပြန်အလှန် နမူနာများ နဲ့ လေ့ကျင့်ခန်းများ

#### Exercise 1: Client Configuration Testing

```python
# Test different configuration methods
configurations = [
    {"method": "foundry_sdk", "model": "phi-4-mini"},
    {"method": "manual", "base_url": "http://localhost:51211", "model": "qwen2.5-7b-instruct"},
]

for config in configurations:
    print(f"\n🧪 Testing {config['method']} configuration...")
    # Implementation here
    result = test_configuration(config)
    print(f"Result: {'✅ Success' if result['status'] == 'ok' else '❌ Failed'}")
```

#### Exercise 2: Streaming Response Simulation

```python
import asyncio

async def simulate_streaming_response(text, delay=0.1):
    """Simulate how streaming works in Chainlit."""
    print("🌊 Simulating streaming response...")
    
    for char in text:
        print(char, end='', flush=True)
        await asyncio.sleep(delay)
    
    print("\n✅ Streaming complete!")

# Test the simulation
sample_text = "This is how streaming responses work in Chainlit applications!"
await simulate_streaming_response(sample_text)
```

## အပိုင်း ၅: WebGPU Browser Inference Demo

### အကျဉ်းချုပ်

WebGPU က AI models ကို browser ထဲမှာ run လုပ်နိုင်အောင် လုပ်ပေးပြီး privacy နဲ့ zero-install experiences အတွက် အကောင်းဆုံးဖြစ်စေပါတယ်။ ဒီ sample က ONNX Runtime Web နဲ့ WebGPU execution ကို ပြသထားပါတယ်။

### အဆင့် ၁: WebGPU Support ကို စစ်ဆေးပါ

**Browser Requirements:**
- Chrome/Edge 113+ WebGPU enabled
- စစ်ဆေးရန်: `chrome://gpu` → "WebGPU" status ကို အတည်ပြုပါ
- Programmatic check: `if (!('gpu' in navigator)) { /* no WebGPU */ }`

### အဆင့် ၂: WebGPU Demo တစ်ခု ဖန်တီးပါ

Directory ဖန်တီးပါ: `samples/04/webgpu-demo/`

**index.html:**
```html
<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU + ONNX Runtime Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <style>
        body { font-family: system-ui, sans-serif; margin: 2rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow: auto; }
        .status { padding: 1rem; background: #e3f2fd; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🚀 WebGPU + Foundry Local Integration</h1>
    <div id="status" class="status">Initializing...</div>
    <pre id="output"></pre>
    <script type="module" src="./main.js"></script>
</body>
</html>
```

**main.js:**
```javascript
const statusEl = document.getElementById('status');
const outputEl = document.getElementById('output');

function log(msg) {
    outputEl.textContent += `${msg}\n`;
    console.log(msg);
}

(async () => {
    try {
        if (!('gpu' in navigator)) {
            statusEl.textContent = '❌ WebGPU not available';
            return;
        }
        
        statusEl.textContent = '🔍 WebGPU detected. Loading model...';
        
        // Use a small ONNX model for demo
        const modelUrl = 'https://huggingface.co/onnx/models/resolve/main/vision/classification/mnist-12/mnist-12.onnx';
        
        const session = await ort.InferenceSession.create(modelUrl, {
            executionProviders: ['webgpu']
        });
        
        log('✅ ONNX Runtime session created with WebGPU');
        log(`📊 Input names: ${session.inputNames.join(', ')}`);
        log(`📊 Output names: ${session.outputNames.join(', ')}`);
        
        // Create dummy input (MNIST expects 1x1x28x28)
        const inputData = new Float32Array(1 * 1 * 28 * 28).fill(0.1);
        const input = new ort.Tensor('float32', inputData, [1, 1, 28, 28]);
        
        const feeds = {};
        feeds[session.inputNames[0]] = input;
        
        const results = await session.run(feeds);
        const output = results[session.outputNames[0]];
        
        // Find prediction (argmax)
        let maxIdx = 0;
        for (let i = 1; i < output.data.length; i++) {
            if (output.data[i] > output.data[maxIdx]) maxIdx = i;
        }
        
        statusEl.textContent = '✅ WebGPU inference complete!';
        log(`🎯 Predicted class: ${maxIdx}`);
        log(`📈 Confidence scores: [${Array.from(output.data).map(x => x.toFixed(3)).join(', ')}]`);
        
    } catch (error) {
        statusEl.textContent = `❌ Error: ${error.message}`;
        log(`Error: ${error.message}`);
        console.error(error);
    }
})();
```

### အဆင့် ၃: Demo ကို Run လုပ်ပါ

```cmd
# Create demo directory
mkdir samples\04\webgpu-demo
cd samples\04\webgpu-demo

# Save HTML and JS files, then serve
python -m http.server 5173

# Open browser to http://localhost:5173
```

## အပိုင်း ၆: Open WebUI Integration

### အကျဉ်းချုပ်

Open WebUI က Foundry Local ရဲ့ OpenAI-compatible API ကို ချိတ်ဆက်ထားတဲ့ ပရော်ဖက်ရှင်နယ် ChatGPT-like interface ကို ပေးစွမ်းပါတယ်။

### အဆင့် ၁: Prerequisites

```cmd
# Verify Foundry Local is running
foundry service status

# Start a model
foundry model run phi-4-mini

# Confirm API endpoint is accessible
curl http://localhost:51211/v1/models
```

### အဆင့် ၂: Docker Setup (အကြံပြုထား)

```cmd
# Pull Open WebUI image
docker pull ghcr.io/open-webui/open-webui:main

# Run with Foundry Local connection
docker run -d --name open-webui -p 3000:8080 ^
  -e OPENAI_API_BASE_URL=http://host.docker.internal:51211/v1 ^
  -e OPENAI_API_KEY=foundry-local-key ^
  -v open-webui-data:/app/backend/data ^
  ghcr.io/open-webui/open-webui:main
```

**မှတ်ချက်:** `host.docker.internal` က Windows မှာ Docker containers တွေကို host machine ကို ချိတ်ဆက်နိုင်အောင် လုပ်ပေးပါတယ်။

### အဆင့် ၃: Configuration

1. **Browser ကို ဖွင့်ပါ:** `http://localhost:3000` သို့ သွားပါ
2. **Initial Setup:** Admin account တစ်ခု ဖန်တီးပါ
3. **Model Configuration:**
   - Settings → Models → OpenAI API  
   - Base URL: `http://host.docker.internal:51211/v1`
   - API Key: `foundry-local-key` (မည်သည့် value မဆို အသုံးပြုနိုင်)
4. **Test Connection:** Models တွေ dropdown မှာ ပေါ်လာရမယ်

### Troubleshooting

**အများဆုံး ကြုံရတဲ့ ပြဿနာများ:**

1. **Connection Refused:**
   ```cmd
   # Check Foundry Local status
   foundry service ps
   netstat -ano | findstr :51211
   ```

2. **Models မပေါ်လာခြင်း:**
   - Model load လုပ်ထားမှုကို စစ်ဆေးပါ: `foundry model list`
   - API response ကို စစ်ဆေးပါ: `curl http://localhost:51211/v1/models`
   - Open WebUI container ကို ပြန်စတင်ပါ

## အပိုင်း ၇: Production Deployment အတွက် စဉ်းစားစရာများ

### Environment Configuration

**Development Setup:**
```cmd
# Development with auto-reload and debugging
chainlit run samples\04\app.py -w --port 8080 --debug
```

**Production Deployment:**
```cmd
# Production mode with optimizations
chainlit run samples\04\app.py --host 0.0.0.0 --port 8080 --no-cache
```

### Common Port Issues နဲ့ Solutions

**Port 51211 Conflict ကို ကာကွယ်ခြင်း:**
```cmd
# Check what's using Foundry Local port
netstat -ano | findstr :51211

# Use different port for Chainlit
chainlit run samples\04\app.py -w --port 8080
```

### Performance Monitoring

**Health Check Implementation:**
```python
@cl.on_chat_start
async def health_check():
    try:
        # Test model availability
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=1
        )
        return {"status": "healthy", "model": model_name}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

## အကျဉ်းချုပ်

Session 4 မှာ conversational AI အတွက် production-ready Chainlit applications တည်ဆောက်ခြင်းကို လေ့လာခဲ့ပါတယ်။ သင်လေ့လာခဲ့တာတွေက:

- ✅ **Chainlit Framework**: Chat applications အတွက် ခေတ်မီ UI နဲ့ streaming support
- ✅ **Foundry Local Integration**: SDK အသုံးပြုမှု နဲ့ configuration patterns  
- ✅ **WebGPU Inference**: Privacy အတွက် browser-based AI
- ✅ **Open WebUI Setup**: ပရော်ဖက်ရှင်နယ် chat interface deployment
- ✅ **Production Patterns**: Error handling, monitoring, နဲ့ scaling

Sample 04 application က Microsoft Foundry Local ကို အသုံးပြုပြီး local AI models ကို leverage လုပ်ထားတဲ့ ခိုင်ခံ့တဲ့ chat interfaces တည်ဆောက်ပုံအတွက် အကောင်းဆုံးနမူနာများကို ပြသထားပါတယ်။

## References

- **[Sample 04: Chainlit Application](samples/04/README.md)**: Documentation ပါဝင်တဲ့ အပြည့်အစုံ application
- **[Chainlit Educational Notebook](samples/04/chainlit_app.ipynb)**: အပြန်အလှန် လေ့လာရေးအကြောင်းအရာများ
- **[Foundry Local Documentation](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Platform documentation အပြည့်အစုံ
- **[Chainlit Documentation](https://docs.chainlit.io/)**: Framework documentation
- **[Open WebUI Integration Guide](https://github.com/microsoft/foundry-local/blob/main/docs/tutorials/chat-application-with-open-web-ui.md)**: တရားဝင် tutorial

---

