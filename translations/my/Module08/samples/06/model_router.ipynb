{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b0bff0",
   "metadata": {},
   "source": [
    "# ·Äî·Äô·Ä∞·Äî·Ä¨ 06: ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ Model Router - Tools ·Ä°·Äñ·Äº·ÄÖ·Ä∫ Models\n",
    "\n",
    "·Äí·ÄÆ notebook ·ÄÄ Microsoft Foundry Local ·Äî·Ä≤·Ä∑ \"Models as Tools\" ·ÄÖ·Äî·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äï·Äº·ÄÆ·Ä∏ ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Äô·Äª·Ä≠·ÄØ·Ä∏·Äû·Ä±·Ä¨·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏ AI ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äï·Ä±·Ä∏·Äê·Ä≤·Ä∑ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ routing ·ÄÖ·Äî·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äû·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "\n",
    "## ·Ä°·ÄÄ·Äª·Äâ·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫\n",
    "\n",
    "Model Router ·ÄÄ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä°·Äë·Ää·Ä∫·Äñ·Ä±·Ä¨·Ä∫·Äï·Ä±·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "\n",
    "- üéØ **·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ**: ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äû·Ä∞·Äõ·Ä≤·Ä∑ ·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "- üîß **·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äô·Äæ·ÄØ**: ·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·ÄÄ·Ä≠·ÄØ·ÄÄ·Ä∫·Ää·ÄÆ·ÄÖ·Ä±·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "- ‚öôÔ∏è **Dynamic Configuration**: ·Äï·Äê·Ä∫·Äù·Äî·Ä∫·Ä∏·ÄÄ·Äª·ÄÑ·Ä∫·Ä°·ÄÅ·Äº·Ä±·Äï·Äº·ÄØ tool registry\n",
    "- üìä **Health Monitoring**: ·Äù·Äî·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äô·Äæ·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äæ·ÄØ\n",
    "- üöÄ **Production Ready**: ·ÄÖ·ÄÆ·Ä∏·Äï·ÄΩ·Ä¨·Ä∏·Äõ·Ä±·Ä∏·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äû·Ä±·Ä¨ routing ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ error handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00acb1",
   "metadata": {},
   "source": [
    "## ·Ä°·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·Ä°·Ä°·ÄØ·Ä∂·Ä°·ÄÄ·Äª·Äâ·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫\n",
    "\n",
    "```\n",
    "User Query ‚Üí Task Classifier ‚Üí Model Selector ‚Üí Specialized Model ‚Üí Response\n",
    "     ‚Üì              ‚Üì               ‚Üì              ‚Üì\n",
    "\"Fix this code\" ‚Üí Code Task ‚Üí qwen2.5-7b ‚Üí Code Model ‚Üí Fixed Code\n",
    "\"Why is...?\"   ‚Üí Reasoning  ‚Üí deepseek-r1 ‚Üí Reasoning ‚Üí Analysis\n",
    "\"Write story\"  ‚Üí Creative   ‚Üí phi-4-mini  ‚Üí Creative ‚Üí Story\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b102f",
   "metadata": {},
   "source": [
    "## ·Ä°·ÄÄ·Äº·Ä≠·ÄØ·Äê·ÄÑ·Ä∫·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Setup\n",
    "\n",
    "·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·Äô·Äæ·Äê·Ä∫·Äô·Äæ·ÄØ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äñ·Äº·ÄÖ·Ä∫·ÄÖ·Ä±·Äõ·Äî·Ä∫ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÖ·ÄΩ·Ä¨·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·ÄØ·Äï·Ä∫·Äú·ÄØ·Äï·Ä∫·Äî·Ä±·Äñ·Ä≠·ÄØ·Ä∑·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã ·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂·Äï·Äê·Ä∫·Äù·Äî·Ä∫·Ä∏·ÄÄ·Äª·ÄÑ·Ä∫·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·ÄÄ·Äº·Äô·Äö·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai foundry-local-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00eba5",
   "metadata": {},
   "source": [
    "## ·ÄÖ·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äñ·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äê·ÄÑ·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äñ·Ä±·Ä¨·Ä∫·Äï·Äº·ÄÅ·Äª·ÄÄ·Ä∫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e68b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "from openai import OpenAI\n",
    "\n",
    "try:\n",
    "    from foundry_local import FoundryLocalManager\n",
    "    FOUNDRY_SDK_AVAILABLE = True\n",
    "    print(\"‚úÖ Foundry Local SDK is available\")\n",
    "except ImportError:\n",
    "    FOUNDRY_SDK_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Foundry Local SDK not available, will use manual configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c2b3",
   "metadata": {},
   "source": [
    "## ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫ Router Class\n",
    "\n",
    "·Ä°·Äì·Ä≠·ÄÄ router ·Äû·Ää·Ä∫ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·Äï·Ä±·Ä∏·Äû·Ää·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRouter:\n",
    "    \"\"\"Intelligent model router that selects appropriate models for different task types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.base_url = None\n",
    "        self.tools = self._load_tool_registry()\n",
    "        self._initialize_client()\n",
    "    \n",
    "    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Load tool registry from environment or use defaults.\"\"\"\n",
    "        default_tools = {\n",
    "            \"general\": {\n",
    "                \"model\": \"phi-4-mini\",\n",
    "                \"notes\": \"Fast general-purpose chat and Q&A\",\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"model\": \"deepseek-r1-7b\",\n",
    "                \"notes\": \"Step-by-step analysis and logical reasoning\",\n",
    "                \"temperature\": 0.3\n",
    "            },\n",
    "            \"code\": {\n",
    "                \"model\": \"qwen2.5-7b\",\n",
    "                \"notes\": \"Code generation, debugging, and technical tasks\",\n",
    "                \"temperature\": 0.2\n",
    "            },\n",
    "            \"creative\": {\n",
    "                \"model\": \"phi-4-mini\",\n",
    "                \"notes\": \"Creative writing and storytelling\",\n",
    "                \"temperature\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Override with environment variables\n",
    "        if os.environ.get(\"GENERAL_MODEL\"):\n",
    "            default_tools[\"general\"][\"model\"] = os.environ[\"GENERAL_MODEL\"]\n",
    "        if os.environ.get(\"REASONING_MODEL\"):\n",
    "            default_tools[\"reasoning\"][\"model\"] = os.environ[\"REASONING_MODEL\"]\n",
    "        if os.environ.get(\"CODE_MODEL\"):\n",
    "            default_tools[\"code\"][\"model\"] = os.environ[\"CODE_MODEL\"]\n",
    "        if os.environ.get(\"CREATIVE_MODEL\"):\n",
    "            default_tools[\"creative\"][\"model\"] = os.environ[\"CREATIVE_MODEL\"]\n",
    "        \n",
    "        # Check for complete JSON override\n",
    "        tools_env = os.environ.get(\"TOOL_REGISTRY\")\n",
    "        if tools_env:\n",
    "            try:\n",
    "                custom_tools = json.loads(tools_env)\n",
    "                return custom_tools\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"‚ö†Ô∏è Invalid TOOL_REGISTRY JSON, using defaults\")\n",
    "        \n",
    "        return default_tools\n",
    "    \n",
    "    def _discover_base_url(self, default: str = \"http://localhost:8000\") -> str:\n",
    "        \"\"\"Discover Foundry Local service URL.\"\"\"\n",
    "        env_url = os.environ.get(\"BASE_URL\")\n",
    "        if env_url:\n",
    "            return env_url\n",
    "        \n",
    "        try:\n",
    "            # Try to get URL from Foundry CLI\n",
    "            result = subprocess.run(\n",
    "                [\"foundry\", \"service\", \"status\"],\n",
    "                capture_output=True, text=True, timeout=3\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                match = re.search(r\"https?://[\\w\\.-]+(?::\\d+)?\", result.stdout)\n",
    "                if match:\n",
    "                    return match.group(0)\n",
    "        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):\n",
    "            pass\n",
    "        \n",
    "        return default\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize OpenAI client with Foundry Local or fallback configuration.\"\"\"\n",
    "        if FOUNDRY_SDK_AVAILABLE:\n",
    "            try:\n",
    "                # Try to use any available model for client initialization\n",
    "                first_model = next(iter(self.tools.values()))[\"model\"]\n",
    "                print(f\"üîÑ Initializing Foundry Local SDK with model: {first_model}...\")\n",
    "                manager = FoundryLocalManager(first_model)\n",
    "                \n",
    "                self.client = OpenAI(\n",
    "                    base_url=manager.endpoint,\n",
    "                    api_key=manager.api_key\n",
    "                )\n",
    "                self.base_url = manager.endpoint\n",
    "                print(f\"‚úÖ Foundry Local SDK initialized\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not use Foundry SDK ({e}), falling back to manual configuration\")\n",
    "        \n",
    "        # Fallback to manual configuration\n",
    "        self.base_url = self._discover_base_url()\n",
    "        api_key = os.environ.get(\"API_KEY\", \"\")\n",
    "        \n",
    "        self.client = OpenAI(\n",
    "            base_url=f\"{self.base_url}/v1\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "        print(f\"üîß Manual configuration initialized at {self.base_url}\")\n",
    "    \n",
    "    def check_service_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check Foundry Local service health and available models.\"\"\"\n",
    "        try:\n",
    "            # Try to list models\n",
    "            models_response = self.client.models.list()\n",
    "            available_models = [model.id for model in models_response.data]\n",
    "            \n",
    "            # Check which configured models are available\n",
    "            configured_models = [tool[\"model\"] for tool in self.tools.values()]\n",
    "            available_configured = [m for m in configured_models if m in available_models]\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"base_url\": self.base_url,\n",
    "                \"available_models\": available_models,\n",
    "                \"configured_models\": configured_models,\n",
    "                \"available_configured\": available_configured,\n",
    "                \"tools_configured\": list(self.tools.keys())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"base_url\": self.base_url,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def select_tool(self, user_query: str) -> str:\n",
    "        \"\"\"Select the most appropriate tool based on the user query.\"\"\"\n",
    "        query_lower = user_query.lower()\n",
    "        \n",
    "        # Code-related keywords\n",
    "        code_keywords = [\n",
    "            \"code\", \"python\", \"function\", \"class\", \"method\", \"bug\", \"debug\", \n",
    "            \"programming\", \"script\", \"algorithm\", \"implementation\", \"refactor\",\n",
    "            \"syntax\", \"compile\", \"error\", \"exception\", \"variable\", \"loop\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in code_keywords):\n",
    "            return \"code\"\n",
    "        \n",
    "        # Reasoning keywords\n",
    "        reasoning_keywords = [\n",
    "            \"why\", \"how\", \"explain\", \"step-by-step\", \"reason\", \"analyze\", \n",
    "            \"think\", \"logic\", \"because\", \"cause\", \"compare\", \"evaluate\",\n",
    "            \"pros and cons\", \"advantage\", \"disadvantage\", \"benefit\", \"drawback\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in reasoning_keywords):\n",
    "            return \"reasoning\"\n",
    "        \n",
    "        # Creative keywords\n",
    "        creative_keywords = [\n",
    "            \"story\", \"poem\", \"creative\", \"imagine\", \"write\", \"tale\", \n",
    "            \"narrative\", \"fiction\", \"character\", \"plot\", \"novel\", \"poetry\",\n",
    "            \"song\", \"lyrics\", \"dialogue\", \"script\"\n",
    "        ]\n",
    "        if any(keyword in query_lower for keyword in creative_keywords):\n",
    "            return \"creative\"\n",
    "        \n",
    "        # Default to general\n",
    "        return \"general\"\n",
    "    \n",
    "    def chat(self, model: str, content: str, max_tokens: int = 300, temperature: Optional[float] = None) -> str:\n",
    "        \"\"\"Send chat completion request to the specified model.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "            \n",
    "            if temperature is not None:\n",
    "                params[\"temperature\"] = temperature\n",
    "            \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response with model {model}: {str(e)}\"\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route the prompt to the appropriate model and generate response.\"\"\"\n",
    "        tool_key = self.select_tool(prompt)\n",
    "        tool_config = self.tools[tool_key]\n",
    "        model = tool_config[\"model\"]\n",
    "        temperature = tool_config.get(\"temperature\", 0.7)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        answer = self.chat(\n",
    "            model=model, \n",
    "            content=prompt, \n",
    "            max_tokens=400, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            \"tool\": tool_key,\n",
    "            \"model\": model,\n",
    "            \"tool_description\": tool_config[\"notes\"],\n",
    "            \"temperature\": temperature,\n",
    "            \"processing_time\": end_time - start_time,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "# Initialize the router\n",
    "print(\"Initializing Model Router...\")\n",
    "print(\"=\" * 50)\n",
    "router = ModelRouter()\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Model Router initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7dfc2",
   "metadata": {},
   "source": [
    "## ·Äù·Äî·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äô·Äæ·ÄØ ·ÄÄ·Äª·Äî·Ä∫·Ä∏·Äô·Ä¨·Äõ·Ä±·Ä∏ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "·ÄÄ·Äª·ÄΩ·Äî·Ä∫·ÄØ·Äï·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Åè Foundry Local ·Äù·Äî·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äô·Äæ·ÄØ·Åè ·ÄÄ·Äª·Äî·Ä∫·Ä∏·Äô·Ä¨·Äõ·Ä±·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏ ·Äõ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÄ·Äº·Äô·Äö·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de120b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service health and available models\n",
    "health = router.check_service_health()\n",
    "\n",
    "print(\"üè• **Service Health Check**\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üö¶ Status: {health['status']}\")\n",
    "print(f\"üîó Base URL: {health['base_url']}\")\n",
    "\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"\\nüìã **Available Models:**\")\n",
    "    for i, model in enumerate(health['available_models'], 1):\n",
    "        print(f\"   {i}. {model}\")\n",
    "    \n",
    "    print(f\"\\nüéØ **Configured Models:**\")\n",
    "    for tool, config in router.tools.items():\n",
    "        model = config['model']\n",
    "        status = \"‚úÖ Available\" if model in health['available_models'] else \"‚ùå Not Available\"\n",
    "        print(f\"   {tool.title()}: {model} - {status}\")\n",
    "    \n",
    "    available_tools = len(health['available_configured'])\n",
    "    total_tools = len(health['configured_models'])\n",
    "    print(f\"\\nüìä **Tools Ready:** {available_tools}/{total_tools}\")\n",
    "    \n",
    "    if available_tools < total_tools:\n",
    "        missing_models = set(health['configured_models']) - set(health['available_configured'])\n",
    "        print(f\"‚ö†Ô∏è **Missing Models:** {', '.join(missing_models)}\")\n",
    "        print(\"üí° To start missing models, run: foundry model run <model-name>\")\n",
    "else:\n",
    "    print(f\"‚ùå **Error:** {health.get('error', 'Unknown error')}\")\n",
    "    print(\"\\nüîß **Troubleshooting:**\")\n",
    "    print(\"1. Ensure Foundry Local is running: foundry service status\")\n",
    "    print(\"2. Start a model: foundry model run phi-4-mini\")\n",
    "    print(\"3. Check the endpoint URL is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d58e64",
   "metadata": {},
   "source": [
    "## ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ ·ÄÖ·Äô·Ä∫·Ä∏·Äû·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "Router ·Äõ·Ä≤·Ä∑ ·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äô·Ä∫·Ä∏·Äû·Äï·Ä∫·ÄÄ·Äº·Äô·Äö·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(test_queries: List[str]):\n",
    "    \"\"\"Test the classification system with various queries.\"\"\"\n",
    "    print(\"üîç **Classification Testing**\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        tool = router.select_tool(query)\n",
    "        tool_config = router.tools[tool]\n",
    "        print(f\"\\n{i}. **Query:** {query}\")\n",
    "        print(f\"   üéØ **Classified as:** {tool.title()}\")\n",
    "        print(f\"   ü§ñ **Model:** {tool_config['model']}\")\n",
    "        print(f\"   üå°Ô∏è **Temperature:** {tool_config['temperature']}\")\n",
    "        print(f\"   üìù **Purpose:** {tool_config['notes']}\")\n",
    "\n",
    "# Test classification with diverse queries\n",
    "test_queries = [\n",
    "    # General queries\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Tell me about Microsoft Foundry Local\",\n",
    "    \n",
    "    # Code queries\n",
    "    \"Write a Python function to sort a list\",\n",
    "    \"Fix this bug in my JavaScript code\",\n",
    "    \"How do I implement a binary search algorithm?\",\n",
    "    \n",
    "    # Reasoning queries\n",
    "    \"Why is edge AI becoming important?\",\n",
    "    \"Explain step-by-step how neural networks work\",\n",
    "    \"Compare the pros and cons of local vs cloud inference\",\n",
    "    \n",
    "    # Creative queries\n",
    "    \"Write a short story about robots\",\n",
    "    \"Create a poem about technology\",\n",
    "    \"Write dialogue for a sci-fi movie\"\n",
    "]\n",
    "\n",
    "test_classification(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a911381",
   "metadata": {},
   "source": [
    "## ·Äê·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä≠·ÄØ·ÄÄ·Ä∫·Äú·Äô·Ä∫·Ä∏·Ää·ÄΩ·Äæ·Äî·Ä∫·Äî·Äô·Ä∞·Äî·Ä¨·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "·Ä°·ÄÅ·ÄØ·Äê·Ä±·Ä¨·Ä∑ ·Äê·ÄÄ·Äö·Ä∑·Ä∫·Äê·ÄØ·Ä∂·Ä∑·Äï·Äº·Äî·Ä∫·Äô·Äæ·ÄØ·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫·Ä°·Äê·Ä∞ ·Äú·Äô·Ä∫·Ä∏·Ää·ÄΩ·Äæ·Äî·Ä∫·ÄÄ·Ä≠·ÄØ ·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äî·Ä±·Äû·Ää·Ä∫·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äú·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä°·Ä±·Ä¨·ÄÑ·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb51e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_routing(example_queries: List[str]):\n",
    "    \"\"\"Demonstrate live routing with actual model responses.\"\"\"\n",
    "    print(\"üöÄ **Live Routing Demonstration**\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, query in enumerate(example_queries, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Example {i}: {query}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Route and get response\n",
    "        result = router.route_and_run(query)\n",
    "        \n",
    "        # Display routing information\n",
    "        print(f\"üéØ **Tool Selected:** {result['tool'].title()}\")\n",
    "        print(f\"ü§ñ **Model Used:** {result['model']}\")\n",
    "        print(f\"üå°Ô∏è **Temperature:** {result['temperature']}\")\n",
    "        print(f\"‚è±Ô∏è **Processing Time:** {result['processing_time']:.2f}s\")\n",
    "        print(f\"üìù **Tool Purpose:** {result['tool_description']}\")\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"\\nüí¨ **Response:**\")\n",
    "        print(result['answer'])\n",
    "\n",
    "# Example queries for live demonstration\n",
    "demo_queries = [\n",
    "    \"What are the main benefits of running AI models locally?\",\n",
    "    \"Write a Python function to calculate fibonacci numbers\",\n",
    "    \"Explain step-by-step why local AI inference is faster than cloud\",\n",
    "    \"Write a creative story about an AI assistant living on an edge device\"\n",
    "]\n",
    "\n",
    "demonstrate_routing(demo_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562549ee",
   "metadata": {},
   "source": [
    "## ·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Ää·Ä∫·Äî·Äæ·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äö·Äæ·Äâ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "·Äô·Äê·Ä∞·ÄÄ·ÄΩ·Ä≤·Äï·Äº·Ä¨·Ä∏·Äû·Ä±·Ä¨·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·Äû·Ää·Ä∫ ·Äê·ÄÖ·Ä∫·ÄÅ·Äª·Ä≠·Äî·Ä∫·Äê·Ää·Ä∫·Ä∏·Äû·Ä±·Ä¨·Ä°·Äú·ÄØ·Äï·Ä∫·ÄÄ·Ä≠·ÄØ ·Äò·Äö·Ä∫·Äú·Ä≠·ÄØ·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·ÄΩ·ÄÄ·Ä∫·Äû·Äú·Ä≤·ÄÜ·Ä≠·ÄØ·Äê·Ä¨·ÄÄ·Ä≠·ÄØ ·Äî·Äæ·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äö·Äæ·Äâ·Ä∫·ÄÄ·Äº·Äô·Äö·Ä∫:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_performance(test_prompt: str, max_tokens: int = 150):\n",
    "    \"\"\"Compare how different models handle the same prompt.\"\"\"\n",
    "    print(f\"‚öñÔ∏è **Model Performance Comparison**\")\n",
    "    print(f\"üìù **Test Prompt:** {test_prompt}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test with each configured model\n",
    "    for tool_name, tool_config in router.tools.items():\n",
    "        model = tool_config['model']\n",
    "        temperature = tool_config['temperature']\n",
    "        \n",
    "        print(f\"\\nüîß **{tool_name.title()} Tool ({model})**\")\n",
    "        print(f\"   üå°Ô∏è Temperature: {temperature}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = router.chat(\n",
    "            model=model, \n",
    "            content=test_prompt, \n",
    "            max_tokens=max_tokens, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"‚è±Ô∏è **Time:** {processing_time:.2f}s\")\n",
    "        print(f\"üí¨ **Response:** {response}\")\n",
    "        \n",
    "        # Calculate response length\n",
    "        response_length = len(response.split())\n",
    "        print(f\"üìè **Length:** {response_length} words\")\n",
    "        \n",
    "        if \"Error\" in response:\n",
    "            print(\"‚ùå **Status:** Error\")\n",
    "        else:\n",
    "            print(\"‚úÖ **Status:** Success\")\n",
    "\n",
    "# Test with a neutral prompt that could work for any model\n",
    "comparison_prompt = \"Explain the concept of edge computing in simple terms.\"\n",
    "compare_model_performance(comparison_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5bb7c",
   "metadata": {},
   "source": [
    "## ·Ä°·Äï·Äº·Äî·Ä∫·Ä°·Äú·Äæ·Äî·Ä∫ Router ·ÄÖ·Äô·Ä∫·Ä∏·Äû·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "·Äû·ÄÑ·Ä∑·Ä∫·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äï·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÖ·Ä≠·Äê·Ä∫·ÄÄ·Äº·Ä≠·ÄØ·ÄÄ·Ä∫ query ·Äô·Äª·Ä¨·Ä∏·Äñ·Äº·ÄÑ·Ä∑·Ä∫ router ·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äô·Ä∫·Ä∏·Äû·Äï·Ä∫·Äï·Ä´:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_test(custom_query: str, force_tool: str = None):\n",
    "    \"\"\"Test the router with a custom query, optionally forcing a specific tool.\"\"\"\n",
    "    print(f\"üé™ **Interactive Router Test**\")\n",
    "    print(f\"üí≠ **Your Query:** {custom_query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if force_tool:\n",
    "        if force_tool not in router.tools:\n",
    "            print(f\"‚ùå Invalid tool: {force_tool}. Available: {list(router.tools.keys())}\")\n",
    "            return\n",
    "        selected_tool = force_tool\n",
    "        print(f\"üîß **Forced Tool:** {selected_tool.title()}\")\n",
    "    else:\n",
    "        selected_tool = router.select_tool(custom_query)\n",
    "        print(f\"üéØ **Auto-Selected Tool:** {selected_tool.title()}\")\n",
    "    \n",
    "    # Get full result\n",
    "    if force_tool:\n",
    "        # Manual routing\n",
    "        tool_config = router.tools[force_tool]\n",
    "        model = tool_config['model']\n",
    "        temperature = tool_config['temperature']\n",
    "        \n",
    "        start_time = time.time()\n",
    "        answer = router.chat(model, custom_query, temperature=temperature)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'tool': force_tool,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'processing_time': end_time - start_time,\n",
    "            'answer': answer\n",
    "        }\n",
    "    else:\n",
    "        # Automatic routing\n",
    "        result = router.route_and_run(custom_query)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä **Routing Details:**\")\n",
    "    print(f\"   üîß Tool: {result['tool'].title()}\")\n",
    "    print(f\"   ü§ñ Model: {result['model']}\")\n",
    "    print(f\"   üå°Ô∏è Temperature: {result['temperature']}\")\n",
    "    print(f\"   ‚è±Ô∏è Time: {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüí¨ **Response:**\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with your custom query\n",
    "custom_query = \"How can I optimize my Python code for better performance?\"\n",
    "# Uncomment the next line to force a specific tool\n",
    "# force_tool = \"code\"  # Options: \"general\", \"reasoning\", \"code\", \"creative\"\n",
    "force_tool = None\n",
    "\n",
    "result = interactive_test(custom_query, force_tool=force_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24aa28",
   "metadata": {},
   "source": [
    "## ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·ÄÑ·Ä∑·Ä∫·Äñ·ÄΩ·Ä≤·Ä∑·ÄÖ·Ää·Ä∫·Ä∏·Äô·Äæ·ÄØ\n",
    "\n",
    "Router ·Äñ·ÄΩ·Ä≤·Ä∑·ÄÖ·Ää·Ä∫·Ä∏·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Äô·Ä≠·Äô·Ä≠·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Äú·Ä≠·ÄØ ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·Äï·Äº·Äû·Äï·ÄØ·Ä∂:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelRouter(ModelRouter):\n",
    "    \"\"\"Extended router with custom configuration and additional features.\"\"\"\n",
    "    \n",
    "    def __init__(self, custom_tools: Dict[str, Dict[str, Any]] = None):\n",
    "        self.custom_tools = custom_tools\n",
    "        super().__init__()\n",
    "    \n",
    "    def _load_tool_registry(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Load custom tool registry if provided.\"\"\"\n",
    "        if self.custom_tools:\n",
    "            return self.custom_tools\n",
    "        return super()._load_tool_registry()\n",
    "    \n",
    "    def add_custom_classifier(self, tool_name: str, keywords: List[str]):\n",
    "        \"\"\"Add a custom classification rule.\"\"\"\n",
    "        self.custom_keywords = getattr(self, 'custom_keywords', {})\n",
    "        self.custom_keywords[tool_name] = keywords\n",
    "    \n",
    "    def select_tool(self, user_query: str) -> str:\n",
    "        \"\"\"Enhanced tool selection with custom rules.\"\"\"\n",
    "        # Check custom keywords first\n",
    "        if hasattr(self, 'custom_keywords'):\n",
    "            query_lower = user_query.lower()\n",
    "            for tool_name, keywords in self.custom_keywords.items():\n",
    "                if any(keyword in query_lower for keyword in keywords):\n",
    "                    return tool_name\n",
    "        \n",
    "        # Fall back to default classification\n",
    "        return super().select_tool(user_query)\n",
    "    \n",
    "    def get_tool_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about tool usage.\"\"\"\n",
    "        if not hasattr(self, 'usage_stats'):\n",
    "            self.usage_stats = {tool: 0 for tool in self.tools.keys()}\n",
    "            self.total_requests = 0\n",
    "        \n",
    "        return {\n",
    "            'usage_by_tool': self.usage_stats.copy(),\n",
    "            'total_requests': self.total_requests,\n",
    "            'most_used_tool': max(self.usage_stats, key=self.usage_stats.get) if self.usage_stats else None\n",
    "        }\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced routing with usage tracking.\"\"\"\n",
    "        result = super().route_and_run(prompt)\n",
    "        \n",
    "        # Track usage\n",
    "        if not hasattr(self, 'usage_stats'):\n",
    "            self.usage_stats = {tool: 0 for tool in self.tools.keys()}\n",
    "            self.total_requests = 0\n",
    "        \n",
    "        self.usage_stats[result['tool']] += 1\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example: Create a router with custom configuration\n",
    "custom_config = {\n",
    "    \"general\": {\n",
    "        \"model\": \"phi-4-mini\",\n",
    "        \"notes\": \"General purpose model\",\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    \"technical\": {\n",
    "        \"model\": \"qwen2.5-7b\",\n",
    "        \"notes\": \"Technical documentation and analysis\",\n",
    "        \"temperature\": 0.3\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"model\": \"phi-4-mini\",\n",
    "        \"notes\": \"Creative writing and storytelling\",\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß **Creating Custom Router Configuration**\")\n",
    "custom_router = CustomModelRouter(custom_tools=custom_config)\n",
    "\n",
    "# Add custom classification rules\n",
    "custom_router.add_custom_classifier(\"technical\", [\"documentation\", \"specification\", \"architecture\", \"design\"])\n",
    "\n",
    "print(\"‚úÖ Custom router created with enhanced features\")\n",
    "print(f\"üéØ Available tools: {list(custom_router.tools.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a692983",
   "metadata": {},
   "source": [
    "## ·Ä°·ÄÖ·ÄØ·Äú·Ä≠·ÄØ·ÄÄ·Ä∫ ·Ä°·Äú·ÄØ·Äï·Ä∫·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ ·Ä•·Äï·Äô·Ä¨\n",
    "\n",
    "·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÖ·ÄΩ·Ä¨·ÄÄ·Ä≠·ÄØ ·Äë·Ä≠·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·ÄÖ·ÄΩ·Ä¨ ·Ä°·Äú·ÄØ·Äï·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_queries(queries: List[str], router_instance: ModelRouter = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple queries in batch and analyze results.\"\"\"\n",
    "    if router_instance is None:\n",
    "        router_instance = router\n",
    "    \n",
    "    print(f\"üì¶ **Batch Processing {len(queries)} Queries**\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    tool_usage = {}\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n{i}. Processing: {query[:50]}{'...' if len(query) > 50 else ''}\")\n",
    "        \n",
    "        result = router_instance.route_and_run(query)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Track statistics\n",
    "        total_time += result['processing_time']\n",
    "        tool = result['tool']\n",
    "        tool_usage[tool] = tool_usage.get(tool, 0) + 1\n",
    "        \n",
    "        print(f\"   üéØ Tool: {tool.title()} | ‚è±Ô∏è {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä **Batch Processing Summary**\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üìù Total Queries: {len(queries)}\")\n",
    "    print(f\"‚è±Ô∏è Total Time: {total_time:.2f}s\")\n",
    "    print(f\"üöÄ Average Time: {total_time/len(queries):.2f}s per query\")\n",
    "    \n",
    "    print(f\"\\nüéØ **Tool Usage:**\")\n",
    "    for tool, count in sorted(tool_usage.items()):\n",
    "        percentage = (count / len(queries)) * 100\n",
    "        print(f\"   {tool.title()}: {count} queries ({percentage:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Batch test queries\n",
    "batch_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a Python function for quicksort\",\n",
    "    \"Why is data privacy important in AI?\",\n",
    "    \"Create a haiku about artificial intelligence\",\n",
    "    \"How do I debug a memory leak in my application?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning\",\n",
    "    \"Write a short story about a robot chef\",\n",
    "    \"What are the advantages of edge computing?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_process_queries(batch_queries)\n",
    "\n",
    "# Show detailed results for first few queries\n",
    "print(f\"\\nüìã **Sample Detailed Results:**\")\n",
    "for i, result in enumerate(batch_results[:3], 1):\n",
    "    print(f\"\\n{i}. **Tool:** {result['tool'].title()} | **Model:** {result['model']}\")\n",
    "    print(f\"   **Response:** {result['answer'][:100]}{'...' if len(result['answer']) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5d741",
   "metadata": {},
   "source": [
    "## ·Äë·ÄØ·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "·Äë·ÄØ·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äô·Äæ·ÄØ·Ä°·ÄÜ·ÄÑ·Ä∫·Äû·ÄÑ·Ä∑·Ä∫·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åè ·Äî·Äô·Ä∞·Äî·Ä¨:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eace241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionModelRouter(ModelRouter):\n",
    "    \"\"\"Production-ready router with comprehensive monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'total_processing_time': 0,\n",
    "            'tool_usage': {},\n",
    "            'model_performance': {},\n",
    "            'error_log': []\n",
    "        }\n",
    "    \n",
    "    def route_and_run(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced routing with comprehensive monitoring.\"\"\"\n",
    "        self.metrics['total_requests'] += 1\n",
    "        request_id = self.metrics['total_requests']\n",
    "        \n",
    "        try:\n",
    "            result = super().route_and_run(prompt)\n",
    "            \n",
    "            # Track success metrics\n",
    "            self.metrics['successful_requests'] += 1\n",
    "            self.metrics['total_processing_time'] += result['processing_time']\n",
    "            \n",
    "            # Track tool usage\n",
    "            tool = result['tool']\n",
    "            if tool not in self.metrics['tool_usage']:\n",
    "                self.metrics['tool_usage'][tool] = {'count': 0, 'total_time': 0}\n",
    "            self.metrics['tool_usage'][tool]['count'] += 1\n",
    "            self.metrics['tool_usage'][tool]['total_time'] += result['processing_time']\n",
    "            \n",
    "            # Track model performance\n",
    "            model = result['model']\n",
    "            if model not in self.metrics['model_performance']:\n",
    "                self.metrics['model_performance'][model] = {'requests': 0, 'total_time': 0, 'avg_time': 0}\n",
    "            self.metrics['model_performance'][model]['requests'] += 1\n",
    "            self.metrics['model_performance'][model]['total_time'] += result['processing_time']\n",
    "            self.metrics['model_performance'][model]['avg_time'] = (\n",
    "                self.metrics['model_performance'][model]['total_time'] / \n",
    "                self.metrics['model_performance'][model]['requests']\n",
    "            )\n",
    "            \n",
    "            # Add monitoring metadata\n",
    "            result['request_id'] = request_id\n",
    "            result['status'] = 'success'\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Track failure metrics\n",
    "            self.metrics['failed_requests'] += 1\n",
    "            error_entry = {\n",
    "                'request_id': request_id,\n",
    "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'prompt': prompt[:100],  # Truncate for privacy\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.metrics['error_log'].append(error_entry)\n",
    "            \n",
    "            return {\n",
    "                'request_id': request_id,\n",
    "                'status': 'error',\n",
    "                'error': str(e),\n",
    "                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "    \n",
    "    def get_performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        total_requests = self.metrics['total_requests']\n",
    "        if total_requests == 0:\n",
    "            return {'message': 'No requests processed yet'}\n",
    "        \n",
    "        success_rate = (self.metrics['successful_requests'] / total_requests) * 100\n",
    "        avg_processing_time = (\n",
    "            self.metrics['total_processing_time'] / max(1, self.metrics['successful_requests'])\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overview': {\n",
    "                'total_requests': total_requests,\n",
    "                'successful_requests': self.metrics['successful_requests'],\n",
    "                'failed_requests': self.metrics['failed_requests'],\n",
    "                'success_rate': f\"{success_rate:.1f}%\",\n",
    "                'average_processing_time': f\"{avg_processing_time:.2f}s\"\n",
    "            },\n",
    "            'tool_usage': self.metrics['tool_usage'],\n",
    "            'model_performance': self.metrics['model_performance'],\n",
    "            'recent_errors': self.metrics['error_log'][-5:] if self.metrics['error_log'] else []\n",
    "        }\n",
    "\n",
    "# Create production router and test\n",
    "print(\"üè≠ **Production Router Testing**\")\n",
    "prod_router = ProductionModelRouter()\n",
    "\n",
    "# Process several requests\n",
    "test_requests = [\n",
    "    \"Explain quantum computing\",\n",
    "    \"Write a function to reverse a string\",\n",
    "    \"Why is cybersecurity important?\",\n",
    "    \"Create a poem about the future\"\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(test_requests)} test requests...\")\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    result = prod_router.route_and_run(request)\n",
    "    status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
    "    print(f\"{i}. {status} Request {result['request_id']}: {request[:30]}...\")\n",
    "\n",
    "# Generate performance report\n",
    "report = prod_router.get_performance_report()\n",
    "\n",
    "print(f\"\\nüìä **Performance Report**\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìà **Overview:**\")\n",
    "for key, value in report['overview'].items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ **Tool Usage:**\")\n",
    "for tool, stats in report['tool_usage'].items():\n",
    "    avg_time = stats['total_time'] / stats['count']\n",
    "    print(f\"   {tool.title()}: {stats['count']} requests (avg: {avg_time:.2f}s)\")\n",
    "\n",
    "print(f\"\\nü§ñ **Model Performance:**\")\n",
    "for model, stats in report['model_performance'].items():\n",
    "    print(f\"   {model}: {stats['requests']} requests (avg: {stats['avg_time']:.2f}s)\")\n",
    "\n",
    "if report['recent_errors']:\n",
    "    print(f\"\\n‚ùå **Recent Errors:**\")\n",
    "    for error in report['recent_errors']:\n",
    "        print(f\"   Request {error['request_id']}: {error['error']}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ **No Recent Errors**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894acf23",
   "metadata": {},
   "source": [
    "## ·Ä°·ÄÄ·Äª·Äâ·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Äî·Ä∫·Äî·Ää·Ä∫·Ä∏·Äú·Äô·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "·Äí·ÄÆ notebook ·Äô·Äæ·Ä¨ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·ÄÑ·Ä∑·Ä∫·Äû·Ä±·Ä¨ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äê·Ä≤·Ä∑ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÖ·Äî·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äû·Äë·Ä¨·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "\n",
    "### ‚úÖ ·Äï·Äº·Äû·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ ·Ä°·Äì·Ä≠·ÄÄ·Ä°·ÄÑ·Ä∫·Äπ·ÄÇ·Ä´·Äõ·Äï·Ä∫·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "1. **üéØ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ**: keyword ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂·Äï·Äº·ÄÆ·Ä∏ ·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "2. **üîß ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äô·Äæ·ÄØ ·Ä°·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äú·Ä≤**: ·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÜ·ÄÆ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "3. **‚öôÔ∏è ·Ä°·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äú·Ä≤·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Äñ·ÄΩ·Ä≤·Ä∑·ÄÖ·Ää·Ä∫·Ä∏·Äô·Äæ·ÄØ**: ·Äï·Äê·Ä∫·Äù·Äî·Ä∫·Ä∏·ÄÄ·Äª·ÄÑ·Ä∫·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂·Äï·Äº·ÄÆ·Ä∏ ·Ä°·Äë·Ä∞·Ä∏·ÄÄ·Ä≠·Äõ·Ä≠·Äö·Ä¨·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äï·ÄØ·Ä∂·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "4. **üìä ·ÄÄ·Äª·Äî·Ä∫·Ä∏·Äô·Ä¨·Äõ·Ä±·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: ·Äù·Äî·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äô·Äæ·ÄØ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äõ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Äæ·ÄØ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "5. **‚ö° ·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Ää·Ä∫·ÄÄ·Ä≠·ÄØ ·Äú·Ä≠·ÄØ·ÄÄ·Ä∫·Äú·Ä∂·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: ·Äê·ÄØ·Ä∂·Ä∑·Äï·Äº·Äî·Ä∫·ÄÅ·Äª·Ä≠·Äî·Ä∫·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äô·Äæ·ÄØ ·Ä°·ÄÅ·Äª·ÄÄ·Ä∫·Ä°·Äú·ÄÄ·Ä∫·Äô·Äª·Ä¨·Ä∏  \n",
    "6. **üè≠ ·Äë·ÄØ·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äô·Äæ·ÄØ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫**: ·ÄÖ·ÄØ·Ä∂·Äú·ÄÑ·Ä∫·Äû·Ä±·Ä¨ ·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·Äô·Äæ·ÄØ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Ä°·Äô·Äæ·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "\n",
    "### üé® ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏ ·Ä°·ÄÄ·Äª·Äâ·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫\n",
    "\n",
    "| ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏ | ·Äï·ÄØ·Ä∂·Äô·Äæ·Äî·Ä∫·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫ | Temperature | ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äô·Äæ·ÄØ |\n",
    "|-------------|----------------|-------------|--------------------------|\n",
    "| **üåê ·Ä°·Äë·ÄΩ·Ä±·Äë·ÄΩ·Ä±** | phi-4-mini | 0.7 | ·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫·Ä°·Äñ·Äº·Ä±, ·ÄÖ·ÄÄ·Ä¨·Ä∏·Äï·Äº·Ä±·Ä¨, ·Ä°·Äë·ÄΩ·Ä±·Äë·ÄΩ·Ä±·Ä°·Äú·ÄØ·Äï·Ä∫·Äô·Äª·Ä¨·Ä∏ |\n",
    "| **üß† ·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äõ·ÄÑ·Ä∫·Ä∏·Äõ·Äæ·Ä¨·Äñ·ÄΩ·Ä±·Äô·Äæ·ÄØ** | deepseek-r1-7b | 0.3 | ·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Äô·Ä∫·Ä∏·ÄÖ·Ä≠·Äê·Ä∫·Äñ·Äº·Ä¨·Äô·Äæ·ÄØ, ·Äõ·Äæ·ÄÑ·Ä∫·Ä∏·Äú·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÄ·Ä∫·Äô·Äª·Ä¨·Ä∏ |\n",
    "| **üíª ·ÄÄ·ÄØ·Äí·Ä∫** | qwen2.5-7b | 0.2 | programming, debugging |\n",
    "| **üé® ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äô·Äæ·ÄØ** | phi-4-mini | 0.9 | ·Äï·ÄØ·Ä∂·Äï·Äº·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏, ·ÄÄ·Äó·Äª·Ä¨·Äô·Äª·Ä¨·Ä∏, ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äõ·Ä±·Ä∏·Äû·Ä¨·Ä∏·Äô·Äæ·ÄØ |\n",
    "\n",
    "### üîç ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ ·ÄÖ·Ää·Ä∫·Ä∏·Äô·Äª·Äâ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "- **·ÄÄ·ÄØ·Äí·Ä∫·ÄÄ·Ä≠·ÄØ ·Äõ·Äæ·Ä¨·Äñ·ÄΩ·Ä±·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: `code`, `python`, `function`, `class`, `bug`, `debug`, `programming`  \n",
    "- **·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äõ·ÄÑ·Ä∫·Ä∏·Äõ·Äæ·Ä¨·Äñ·ÄΩ·Ä±·Äô·Äæ·ÄØ**: `why`, `how`, `explain`, `step-by-step`, `reason`, `analyze`  \n",
    "- **·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Äõ·Äæ·Ä¨·Äñ·ÄΩ·Ä±·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: `story`, `poem`, `creative`, `imagine`, `write`, `tale`  \n",
    "- **·Äï·ÄØ·Ä∂·Äô·Äæ·Äî·Ä∫**: ·Ä°·ÄÅ·Äº·Ä¨·Ä∏·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äë·ÄΩ·Ä±·Äë·ÄΩ·Ä±·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äï·Ä´  \n",
    "\n",
    "### üí° ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Äî·Ä∫·Äî·Ää·Ä∫·Ä∏·Äú·Äô·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "1. **üéØ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ·Äô·Äæ·ÄØ**: ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÖ·ÄÆ·Åè ·Ä°·Ä¨·Ä∏·Äû·Ä¨·ÄÅ·Äª·ÄÄ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äï·Ä´  \n",
    "2. **üå°Ô∏è Temperature ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Äª·Ä≠·Äî·Ä∫·Ää·Äæ·Ä≠·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: ·Äê·Ä≠·ÄÄ·Äª·Äô·Äæ·ÄØ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äî·Ä≠·Äô·Ä∑·Ä∫·Äï·Äº·ÄÆ·Ä∏ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äô·Äæ·ÄØ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äô·Äº·ÄÑ·Ä∑·Ä∫  \n",
    "3. **üìä ·ÄÜ·ÄÄ·Ä∫·Äú·ÄÄ·Ä∫·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: ·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Ää·Ä∫·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äô·Äæ·ÄØ ·Äï·ÄØ·Ä∂·ÄÖ·Ä∂·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äú·Ä≠·ÄØ·ÄÄ·Ä∫·Äú·Ä∂·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äõ·Äæ·ÄØ·Äï·Ä´  \n",
    "4. **üîÑ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äî·Ää·Ä∫·Ä∏·Äú·Äô·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏**: ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äõ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨·Ä°·ÄÅ·Ä´ ·Ä°·ÄÜ·ÄÑ·Ä∫·Äï·Äº·Ä±·ÄÖ·ÄΩ·Ä¨ ·Äú·Äª·Ä±·Ä¨·Ä∑·Äî·Ää·Ä∫·Ä∏·ÄÖ·Ä±·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "5. **‚öñÔ∏è Load Balancing**: ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·Ä°·ÄÄ·Äº·Ä¨·Ä∏ ·Ä°·Äú·ÄØ·Äï·Ä∫·Äï·Äô·Ä¨·Äè·ÄÄ·Ä≠·ÄØ ·Äñ·Äº·Äî·Ä∑·Ä∫·Äù·Ä±·Äï·Ä´  \n",
    "6. **üîß ·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ**: ·Äî·Äö·Ä∫·Äï·Äö·Ä∫·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÖ·Ää·Ä∫·Ä∏·Äô·Äª·Äâ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·Äï·Ä´  \n",
    "\n",
    "### üöÄ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äô·Äæ·ÄØ·Äô·Äª·Ä¨·Ä∏\n",
    "\n",
    "- **üë®‚Äçüíª ·Äñ·ÄΩ·Ä∂·Ä∑·Äñ·Äº·Ä≠·ÄØ·Ä∏·Äõ·Ä±·Ä∏·ÄÄ·Ä≠·Äõ·Ä≠·Äö·Ä¨·Äô·Äª·Ä¨·Ä∏**: ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ IDE plugins  \n",
    "- **üéß ·Äñ·Ä±·Ä¨·ÄÄ·Ä∫·Äû·Ää·Ä∫·Äï·Ä∂·Ä∑·Äï·Ä≠·ÄØ·Ä∏·Äô·Äæ·ÄØ**: ·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ·Äï·Ä∂·Ä∑·Äï·Ä≠·ÄØ·Ä∏·Äô·Äæ·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÜ·ÄÆ ·Äô·Ä±·Ä∏·ÄÅ·ÄΩ·Äî·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üìù ·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äõ·Ä¨·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äô·Äæ·ÄØ**: ·Ä°·Äú·ÄØ·Äï·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·ÄÑ·Ä∑·Ä∫·Äú·Äª·Ä±·Ä¨·Ä∫·Äû·Ä±·Ä¨ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äô·Äæ·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äê·ÄΩ·Ä≤·Äñ·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üìö Documentation**: ·Äî·Ää·Ä∫·Ä∏·Äï·Ää·Ä¨·Äõ·Ä±·Ä∏·Äû·Ä¨·Ä∏·Äô·Äæ·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÜ·ÄÆ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üéì ·Äï·Ää·Ä¨·Äõ·Ä±·Ä∏·Äï·Äú·ÄÄ·Ä∫·Äñ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏**: ·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Ä°·Äõ·Ä¨·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "\n",
    "### üîÆ ·Äî·Ä±·Ä¨·ÄÄ·Ä∫·Äê·ÄÖ·Ä∫·ÄÜ·ÄÑ·Ä∑·Ä∫·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Äî·Ä∫\n",
    "\n",
    "- **ü§ñ Machine Learning**: ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏·ÄÅ·ÄΩ·Ä≤·ÄÅ·Äº·Ä¨·Ä∏·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·Äô·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·ÄΩ·Äî·Ä∫·ÄÖ·Ä±·Äõ·Äî·Ä∫ ML ·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äï·Ä´  \n",
    "- **üîß Function Calling**: ·Ä°·Äë·Ä∞·Ä∏·ÄÄ·Ä≠·Äõ·Ä≠·Äö·Ä¨·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·Äõ·Ää·Ä∫·Äô·Äª·Ä¨·Ä∏·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÜ·ÄÆ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üì∏ Multi-Modal**: ·Ä°·Äë·ÄΩ·Ä±·Äë·ÄΩ·Ä± input ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Ä°·ÄÖ·Ä¨·Ä∏ (·ÄÖ·Ä¨·Äû·Ä¨·Ä∏, ·Äï·ÄØ·Ä∂, ·Ä°·Äû·Ä∂) ·Ä°·Äï·Ä±·Ä´·Ä∫ ·Äô·Ä∞·Äê·Ää·Ä∫·Äï·Äº·ÄÆ·Ä∏ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üåê Federated Routing**: Foundry Local instances ·Äô·Äª·Ä¨·Ä∏·Ä°·ÄÄ·Äº·Ä¨·Ä∏ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "- **üí∞ ·ÄÄ·ÄØ·Äî·Ä∫·ÄÄ·Äª·ÄÖ·Äõ·Ä≠·Äê·Ä∫·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏**: ·ÄÄ·ÄØ·Äî·Ä∫·ÄÄ·Äª·ÄÖ·Äõ·Ä≠·Äê·Ä∫·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Ää·Ä∫ metrics ·Ä°·Äï·Ä±·Ä´·Ä∫ ·Äô·Ä∞·Äê·Ää·Ä∫·Äï·Äº·ÄÆ·Ä∏ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äª·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏  \n",
    "\n",
    "·Ä§ ·Äâ·Ä¨·Äè·Ä∫·Äõ·Ää·Ä∫·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÖ·Äî·ÄÖ·Ä∫·Äû·Ää·Ä∫ Microsoft Foundry Local ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ inference ·Äú·ÄØ·Äï·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Äô·Äæ privacy ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·ÄÖ·ÄΩ·Äô·Ä∫·Ä∏·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Ää·Ä∫ ·Ä°·ÄÄ·Äª·Ä≠·ÄØ·Ä∏·ÄÄ·Äª·Ä±·Ä∏·Äá·Ä∞·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äë·Ä≠·Äî·Ä∫·Ä∏·Äû·Ä≠·Äô·Ä∫·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·Ä°·Äë·Ä∞·Ä∏·Äï·Äº·ÄØ·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·Äô·Äª·Ä¨·Ä∏·Åè ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Äô·Äª·Ä¨·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äõ·Äõ·Äæ·Ä≠·ÄÖ·Ä±·Äõ·Äî·Ä∫ ·Äï·Äº·Äû·Äë·Ä¨·Ä∏·Äû·Ää·Ä∫·Åã \"Models as Tools\" paradigm ·Äû·Ää·Ä∫ ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Åè ·Ä°·Äú·ÄØ·Äï·Ä∫·Ä°·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∏·ÄÅ·Äª·Äö·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·ÄÑ·Ä∑·Ä∫ AI ·Ä°·Äï·Äú·ÄÆ·ÄÄ·Ä±·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äê·Ää·Ä∫·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·Äî·Ä∫ ·ÄÅ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äï·Äº·ÄØ·Äû·Ää·Ä∫·Åã\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ä¨·Ä∏·ÄÅ·Äª·ÄÄ·Ä∫**:  \n·Ä§·ÄÖ·Ä¨·Äõ·ÄΩ·ÄÄ·Ä∫·ÄÖ·Ä¨·Äê·Äô·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ AI ·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äù·Äî·Ä∫·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫·Äô·Äæ·ÄØ [Co-op Translator](https://github.com/Azure/co-op-translator) ·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Åç ·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äë·Ä¨·Ä∏·Äï·Ä´·Äû·Ää·Ä∫·Åã ·ÄÄ·Äª·ÄΩ·Äî·Ä∫·ÄØ·Äï·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Äû·Ää·Ä∫ ·Äê·Ä≠·ÄÄ·Äª·Äô·Äæ·ÄØ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÄ·Äº·Ä≠·ÄØ·Ä∏·ÄÖ·Ä¨·Ä∏·Äî·Ä±·Äû·Ä±·Ä¨·Ä∫·Äú·Ää·Ä∫·Ä∏ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äô·Äæ·ÄØ·Äô·Äª·Ä¨·Ä∏·Äê·ÄΩ·ÄÑ·Ä∫ ·Ä°·Äô·Äæ·Ä¨·Ä∏·Äô·Äª·Ä¨·Ä∏ ·Äû·Ä≠·ÄØ·Ä∑·Äô·Äü·ÄØ·Äê·Ä∫ ·Äô·Äô·Äæ·Äî·Ä∫·ÄÄ·Äî·Ä∫·Äô·Äæ·ÄØ·Äô·Äª·Ä¨·Ä∏ ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä≠·Äï·Äº·ÄØ·Äï·Ä´·Åã ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏·Äò·Ä¨·Äû·Ä¨·ÄÖ·ÄÄ·Ä¨·Ä∏·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äõ·Ä±·Ä∏·Äû·Ä¨·Ä∏·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ ·ÄÖ·Ä¨·Äõ·ÄΩ·ÄÄ·Ä∫·ÄÖ·Ä¨·Äê·Äô·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Ä°·Ä¨·Äè·Ä¨·Äê·Äõ·Ä¨·Ä∏·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ ·Ä°·Äõ·ÄÑ·Ä∫·Ä∏·Ä°·Äô·Äº·ÄÖ·Ä∫·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·ÄÑ·Ä∑·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã ·Ä°·Äõ·Ä±·Ä∏·ÄÄ·Äº·ÄÆ·Ä∏·Äû·Ä±·Ä¨ ·Ä°·ÄÅ·Äª·ÄÄ·Ä∫·Ä°·Äú·ÄÄ·Ä∫·Äô·Äª·Ä¨·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äú·Ä∞·Ä∑·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äï·Ää·Ä¨·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·Äô·Äæ ·Äï·Äõ·Ä±·Ä¨·Ä∫·Äñ·ÄÄ·Ä∫·Äõ·Äæ·ÄÑ·Ä∫·Äî·Äö·Ä∫ ·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÄ·Äº·Ä∂·Äï·Äº·ÄØ·Äï·Ä´·Äû·Ää·Ä∫·Åã ·Ä§·Äò·Ä¨·Äû·Ä¨·Äï·Äº·Äî·Ä∫·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Äô·Äæ ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä±·Ä´·Ä∫·Äú·Ä¨·Äû·Ä±·Ä¨ ·Ä°·Äú·ÄΩ·Ä≤·Ä°·Äú·ÄΩ·Äê·Ä∫·Äô·Äª·Ä¨·Ä∏ ·Äû·Ä≠·ÄØ·Ä∑·Äô·Äü·ÄØ·Äê·Ä∫ ·Ä°·Äî·Ä¨·Ä∏·Äö·Ä∞·Äô·Äæ·ÄØ·Äô·Äæ·Ä¨·Ä∏·Äô·Äª·Ä¨·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÄ·Äª·ÄΩ·Äî·Ä∫·ÄØ·Äï·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Äû·Ää·Ä∫ ·Äê·Ä¨·Äù·Äî·Ä∫·Äô·Äö·Ä∞·Äï·Ä´·Åã\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "244af75414f6f2705d84855b890df6e9",
   "translation_date": "2025-09-30T22:48:54+00:00",
   "source_file": "Module08/samples/06/model_router.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}