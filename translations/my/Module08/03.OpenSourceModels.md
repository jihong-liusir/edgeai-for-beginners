<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-23T01:03:04+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "my"
}
-->
# အခန်း ၃: Foundry Local နှင့် Open-Source မော်ဒယ်များ

## အကျဉ်းချုပ်

ဒီအခန်းမှာ Foundry Local တွင် Open-Source မော်ဒယ်များကို အသုံးပြုနည်းကို လေ့လာပါမည်။ အများပြည်သူ မော်ဒယ်များကို ရွေးချယ်ခြင်း၊ Hugging Face အကြောင်းအရာများကို ပေါင်းစည်းခြင်း၊ "သင့်မော်ဒယ်ကို သင့်ကိုယ်တိုင် ယူလာပါ" (BYOM) မဟာဗျူဟာများကို လက်ခံခြင်းတို့ကို လေ့လာပါမည်။ Model Mondays စီးရီးကိုလည်း ရှာဖွေခြင်းနှင့် မော်ဒယ်များကို ဆက်လက်လေ့လာနိုင်ရန် ရှင်းလင်းပြသပါမည်။

ရင်းမြစ်များ:
- Foundry Local စာရွက်စာတမ်းများ: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face မော်ဒယ်များကို Compile လုပ်ခြင်း: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## သင်ယူရမည့်အရာများ
- ဒေသတွင်း inference အတွက် Open-Source မော်ဒယ်များကို ရှာဖွေပြီး အကဲဖြတ်နိုင်ရန်
- Foundry Local တွင် Hugging Face မော်ဒယ်များကို Compile လုပ်ပြီး အလုပ်လုပ်စေခြင်း
- တိကျမှု၊ latency နှင့် resource လိုအပ်ချက်များအတွက် မော်ဒယ်ရွေးချယ်မှု မဟာဗျူဟာများကို အသုံးပြုခြင်း
- Cache နှင့် versioning ဖြင့် ဒေသတွင်းတွင် မော်ဒယ်များကို စီမံခန့်ခွဲခြင်း

## အပိုင်း ၁: မော်ဒယ်ရှာဖွေခြင်းနှင့် ရွေးချယ်ခြင်း (အဆင့်ဆင့်)

အဆင့် ၁) ဒေသတွင်း catalog တွင် ရရှိနိုင်သော မော်ဒယ်များကို စာရင်းပြုစုပါ
```cmd
foundry model list
```

အဆင့် ၂) မော်ဒယ်နှစ်ခုကို အမြန်စမ်းသပ်ပါ (ပထမဆုံးအကြိမ်တွင် auto-download ဖြစ်ပါမည်)
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```

အဆင့် ၃) အခြေခံ metrics ကို မှတ်သားပါ
- တစ်ခုတည်းသော prompt အတွက် latency (အတွေ့အကြုံအရ) နှင့် အရည်အသွေးကို ကြည့်ရှုပါ
- မော်ဒယ်များအလုပ်လုပ်နေစဉ် Task Manager မှ memory အသုံးပြုမှုကို ကြည့်ပါ

## အပိုင်း ၂: CLI မှတစ်ဆင့် Catalog မော်ဒယ်များကို အလုပ်လုပ်စေခြင်း (အဆင့်ဆင့်)

အဆင့် ၁) မော်ဒယ်တစ်ခုကို စတင်ပါ
```cmd
foundry model run llama-3.2
```

အဆင့် ၂) OpenAI-compatible endpoint မှတစ်ဆင့် စမ်းသပ် prompt တစ်ခုကို ပို့ပါ
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```


## အပိုင်း ၃: BYOM – Hugging Face မော်ဒယ်များကို Compile လုပ်ခြင်း (အဆင့်ဆင့်)

မော်ဒယ်များကို Compile လုပ်ရန် အတိအကျသော command များနှင့် configuration များကို Microsoft Learn ဆောင်းပါးတွင် ကြည့်ပါ။ အထက်လမ်းကြောင်းကို အကျဉ်းချုပ်ထားပါသည်။

အဆင့် ၁) အလုပ်လုပ်ရန် directory တစ်ခုကို ပြင်ဆင်ပါ
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```

အဆင့် ၂) HF မော်ဒယ်တစ်ခုကို Compile လုပ်ပါ
- Learn စာရွက်စာတမ်းမှ အဆင့်များကို အသုံးပြု၍ ONNX မော်ဒယ်ကို ပြောင်းလဲပြီး `models` directory တွင်ထားပါ
- အတည်ပြုရန်:
```cmd
foundry cache ls
```
သင့် compiled မော်ဒယ်နာမည်ကို (ဥပမာ `llama-3.2`) တွေ့ရပါမည်။

အဆင့် ၃) Compiled မော်ဒယ်ကို အလုပ်လုပ်စေပါ
```cmd
foundry model run llama-3.2 --verbose
```

မှတ်ချက်များ:
- Compile နှင့် run လုပ်ရန် disk နှင့် RAM လုံလောက်မှုရှိရန် သေချာပါ
- လမ်းကြောင်းကို အတည်ပြုရန် မော်ဒယ်အသေးစားများကို စတင်ပြီး အဆင့်မြှင့်ပါ

## အပိုင်း ၄: မော်ဒယ်များကို လက်တွေ့စီမံခန့်ခွဲခြင်း (အဆင့်ဆင့်)

အဆင့် ၁) `models.json` registry တစ်ခုကို ဖန်တီးပါ
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```

အဆင့် ၂) မော်ဒယ်ရွေးချယ် script အသေးစားတစ်ခု
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```


## အပိုင်း ၅: လက်တွေ့ Benchmark များ (အဆင့်ဆင့်)

အဆင့် ၁) latency benchmark ရိုးရှင်းသောတစ်ခု
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```

အဆင့် ၂) အရည်အသွေးကို စမ်းသပ်ခြင်း
- Prompt set တစ်ခုကို အသုံးပြုပြီး output များကို CSV/JSON တွင် သိမ်းဆည်းပါ
- Fluency, relevance, correctness (၁–၅) ကို လက်တွေ့အဆင့်သတ်မှတ်ပါ

## အပိုင်း ၆: နောက်တစ်ဆင့်များ
- Model Mondays subscription ရယူပြီး မော်ဒယ်အသစ်များနှင့် အကြံဉာဏ်များကို ရယူပါ: https://aka.ms/model-mondays
- သင့်အဖွဲ့၏ `models.json` တွင် ရှာဖွေတွေ့ရှိမှုများကို ထည့်သွင်းပါ
- အခန်း ၄ အတွက် ပြင်ဆင်ပါ: LLMs နှင့် SLMs ကို နှိုင်းယှဉ်ခြင်း၊ ဒေသတွင်းနှင့် cloud inference နှိုင်းယှဉ်ခြင်း၊ လက်တွေ့စမ်းသပ်မှုများ

---

