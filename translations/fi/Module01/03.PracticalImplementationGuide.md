<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c37dfe660161e652077f6b7b23bb2167",
  "translation_date": "2025-10-11T14:34:12+00:00",
  "source_file": "Module01/03.PracticalImplementationGuide.md",
  "language_code": "fi"
}
-->
# Osa 3: Käytännön toteutusopas

## Yleiskatsaus

Tämä kattava opas auttaa sinua valmistautumaan EdgeAI-kurssiin, joka keskittyy käytännön tekoälyratkaisujen rakentamiseen, jotka toimivat tehokkaasti reunalaitteilla. Kurssi painottaa käytännön kehitystä nykyaikaisilla kehyksillä ja huippuluokan malleilla, jotka on optimoitu reunasovelluksiin.

## 1. Kehitysympäristön asennus

### Ohjelmointikielet ja kehykset

**Python-ympäristö**
- **Versio**: Python 3.10 tai uudempi (suositus: Python 3.11)
- **Paketinhallinta**: pip tai conda
- **Virtuaaliympäristö**: Käytä venv- tai conda-ympäristöjä eristämiseen
- **Keskeiset kirjastot**: Asennamme kurssin aikana erityisiä EdgeAI-kirjastoja

**Microsoft .NET -ympäristö**
- **Versio**: .NET 8 tai uudempi
- **IDE**: Visual Studio 2022, Visual Studio Code tai JetBrains Rider
- **SDK**: Varmista, että .NET SDK on asennettu alustojen väliseen kehitykseen

### Kehitystyökalut

**Koodieditorit ja IDE:t**
- Visual Studio Code (suositeltu alustojen väliseen kehitykseen)
- PyCharm tai Visual Studio (kielikohtaiseen kehitykseen)
- Jupyter Notebooks interaktiiviseen kehitykseen ja prototyyppien luomiseen

**Versionhallinta**
- Git (uusin versio)
- GitHub-tili repositorioiden käyttöä ja yhteistyötä varten

## 2. Laitteistovaatimukset ja suositukset

### Vähimmäisjärjestelmävaatimukset
- **CPU**: Moniytiminen prosessori (Intel i5/AMD Ryzen 5 tai vastaava)
- **RAM**: Vähintään 8GB, suositus 16GB
- **Tallennustila**: 50GB vapaata tilaa malleille ja kehitystyökaluille
- **Käyttöjärjestelmä**: Windows 10/11, macOS 10.15+ tai Linux (Ubuntu 20.04+)

### Laskentaresurssien strategia
Kurssi on suunniteltu toimimaan eri laitteistokokoonpanoilla:

**Paikallinen kehitys (CPU/NPU-painotus)**
- Ensisijainen kehitys tapahtuu CPU:n ja NPU:n kiihdytyksellä
- Sopii useimmille nykyaikaisille kannettaville ja pöytätietokoneille
- Keskittyy tehokkuuteen ja käytännön sovelluksiin

**Pilvi-GPU-resurssit (valinnainen)**
- **Azure Machine Learning**: Raskaaseen koulutukseen ja kokeiluun
- **Google Colab**: Ilmainen taso opetuskäyttöön
- **Kaggle Notebooks**: Vaihtoehtoinen pilvilaskenta-alusta

### Reunalaitteiden huomioiminen
- ARM-pohjaisten prosessorien ymmärtäminen
- Tietoisuus mobiili- ja IoT-laitteiden rajoituksista
- Virrankulutuksen optimoinnin tuntemus

## 3. Keskeiset malliperheet ja resurssit

### Ensisijaiset malliperheet

**Microsoft Phi-4 -perhe**
- **Kuvaus**: Kompaktit, tehokkaat mallit, jotka on suunniteltu reunasovelluksiin
- **Vahvuudet**: Erinomainen suorituskyvyn ja koon suhde, optimoitu päättelytehtäviin
- **Resurssi**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **Käyttötapaukset**: Koodin generointi, matemaattinen päättely, yleinen keskustelu

**Qwen-3 -perhe**
- **Kuvaus**: Alibaban uusin sukupolvi monikielisiä malleja
- **Vahvuudet**: Vahvat monikieliset ominaisuudet, tehokas arkkitehtuuri
- **Resurssi**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **Käyttötapaukset**: Monikieliset sovellukset, kulttuurienväliset tekoälyratkaisut

**Google Gemma-3n -perhe**
- **Kuvaus**: Googlen kevyet mallit, jotka on optimoitu reunasovelluksiin
- **Vahvuudet**: Nopea päättely, mobiiliystävällinen arkkitehtuuri
- **Resurssi**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Käyttötapaukset**: Mobiilisovellukset, reaaliaikainen käsittely

### Mallin valintakriteerit
- **Suorituskyvyn ja koon kompromissit**: Milloin valita pienempiä vs. suurempia malleja
- **Tehtäväkohtainen optimointi**: Mallien sovittaminen tiettyihin käyttötapauksiin
- **Käyttöönoton rajoitukset**: Muisti, viive ja virrankulutus

## 4. Kvantisointi- ja optimointityökalut

### Llama.cpp-kehys
- **Repository**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **Tarkoitus**: Suorituskykyinen päättelymoottori LLM:ille
- **Keskeiset ominaisuudet**:
  - CPU-optimoitu päättely
  - Useita kvantisointimuotoja (Q4, Q5, Q8)
  - Alustojen välinen yhteensopivuus
  - Muistitehokas suoritus
- **Asennus ja peruskäyttö**:
  ```bash
  # Clone the repository
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # Build the project with optimizations
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # Quantize a model (from GGUF format to 4-bit quantization)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # Run inference with the quantized model
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **Repository**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **Tarkoitus**: Mallien optimointityökalu reunasovelluksiin
- **Keskeiset ominaisuudet**:
  - Automatisoidut mallien optimointityönkulut
  - Laitteistotietoinen optimointi
  - Integraatio ONNX Runtimeen
  - Suorituskyvyn vertailutyökalut
- **Asennus ja peruskäyttö**:
  ```bash
  # Install Olive
  pip install olive-ai
  ```
  
  # Esimerkkipython-skripti mallin optimointiin
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # Define model and optimization config
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # Run optimization workflow
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # Save optimized model
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS-käyttäjille)
- **Repository**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **Tarkoitus**: Koneoppimiskehys Apple Siliconille
- **Keskeiset ominaisuudet**:
  - Natiivisti optimoitu Apple Siliconille
  - Muistitehokkaat toiminnot
  - PyTorch-tyylinen API
  - Yhtenäisen muistiarkkitehtuurin tuki
- **Asennus ja peruskäyttö**:
  ```bash
  # Install MLX
  pip install mlx
  ```
  
  ```python
  # Example Python script for loading and optimizing a model
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # Load pre-trained weights (example with a simple MLP)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # Create model and load weights
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # Quantize the model weights to FP16
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # Save quantized model
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # Run inference
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **Repository**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **Tarkoitus**: Alustojen välinen päättelykiihdytys ONNX-malleille
- **Keskeiset ominaisuudet**:
  - Laitteistokohtaiset optimoinnit (CPU, GPU, NPU)
  - Graafioptimoinnit päättelyyn
  - Kvantisointituki
  - Kielen tuki (Python, C++, C#, JavaScript)
- **Asennus ja peruskäyttö**:
  ```bash
  # Install ONNX Runtime
  pip install onnxruntime
  
  # For GPU support
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # Create inference session with optimizations
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # Enable performance profiling
  
  # Create session with provider selection for hardware acceleration
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # Use GPU if available
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # Prepare input data
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # Run inference
  outputs = session.run(None, {input_name: input_data})
  
  # Get profiling data
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```


## 5. Suositeltu lukeminen ja resurssit

### Keskeinen dokumentaatio
- **ONNX Runtime Documentation**: Alustojen välinen päättely
- **Hugging Face Transformers Guide**: Mallien lataus ja päättely
- **Edge AI Design Patterns**: Parhaat käytännöt reunasovelluksiin

### Teknisiä artikkeleita
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### Yhteisöresurssit
- **EdgeAI Slack/Discord-yhteisöt**: Vertaisapu ja keskustelu
- **GitHub-repositoriot**: Esimerkkitoteutukset ja tutoriaalit
- **YouTube-kanavat**: Teknisiä syväluotauksia ja tutoriaaleja

## 6. Arviointi ja varmistus

### Ennen kurssia -tarkistuslista
- [ ] Python 3.10+ asennettu ja tarkistettu
- [ ] .NET 8+ asennettu ja tarkistettu
- [ ] Kehitysympäristö konfiguroitu
- [ ] Hugging Face -tili luotu
- [ ] Perustuntemus kohdemalliperheistä
- [ ] Kvantisointityökalut asennettu ja testattu
- [ ] Laitteistovaatimukset täytetty
- [ ] Pilvilaskentatilit luotu (tarvittaessa)

## Keskeiset oppimistavoitteet

Oppaan lopussa pystyt:

1. Konfiguroimaan täydellisen kehitysympäristön EdgeAI-sovellusten kehitykseen
2. Asentamaan ja konfiguroimaan tarvittavat työkalut ja kehykset mallien optimointiin
3. Valitsemaan sopivat laitteisto- ja ohjelmistokokoonpanot EdgeAI-projekteihisi
4. Ymmärtämään keskeiset näkökohdat tekoälymallien käyttöönotossa reunalaitteilla
5. Valmistamaan järjestelmäsi kurssin käytännön harjoituksia varten

## Lisäresurssit

### Virallinen dokumentaatio
- **Python Documentation**: Python-kielen virallinen dokumentaatio
- **Microsoft .NET Documentation**: .NET-kehityksen viralliset resurssit
- **ONNX Runtime Documentation**: Kattava opas ONNX Runtimeen
- **TensorFlow Lite Documentation**: TensorFlow Liten virallinen dokumentaatio

### Kehitystyökalut
- **Visual Studio Code**: Kevyt koodieditori tekoälyn kehityslaajennuksilla
- **Jupyter Notebooks**: Interaktiivinen laskentaympäristö ML-kokeiluihin
- **Docker**: Konttialusta yhtenäisten kehitysympäristöjen luomiseen
- **Git**: Versionhallintajärjestelmä koodin hallintaan

### Oppimisresurssit
- **EdgeAI-tutkimusartikkelit**: Viimeisimmät akateemiset tutkimukset tehokkaista malleista
- **Verkkokurssit**: Lisämateriaaleja tekoälyn optimoinnista
- **Yhteisöfoorumit**: Kysymys-vastaus-alustat EdgeAI-kehityshaasteisiin
- **Vertailudatasetit**: Standardoidut datasetit mallien suorituskyvyn arviointiin

## Oppimistulokset

Oppaan suorittamisen jälkeen:

1. Sinulla on täysin konfiguroitu kehitysympäristö EdgeAI-kehitystä varten
2. Ymmärrät laitteisto- ja ohjelmistovaatimukset eri käyttöönottojen skenaarioissa
3. Tunnet kurssilla käytettävät keskeiset kehykset ja työkalut
4. Osaat valita sopivat mallit laitteiden rajoitusten ja vaatimusten perusteella
5. Hallitset olennaiset optimointitekniikat reunasovelluksiin

## ➡️ Mitä seuraavaksi

- [04: EdgeAI-laitteisto ja käyttöönotto](04.EdgeDeployment.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.