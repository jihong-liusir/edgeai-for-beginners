<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-09-18T10:33:51+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "fi"
}
-->
# Osa 2: Mallin tislaus - teoriasta käytäntöön

## Sisällysluettelo
1. [Johdatus mallin tislaamiseen](../../../Module05)
2. [Miksi tislaus on tärkeää](../../../Module05)
3. [Tislausprosessi](../../../Module05)
4. [Käytännön toteutus](../../../Module05)
5. [Azure ML -tislausesimerkki](../../../Module05)
6. [Parhaat käytännöt ja optimointi](../../../Module05)
7. [Käyttökohteet tosielämässä](../../../Module05)
8. [Yhteenveto](../../../Module05)

## Johdatus mallin tislaamiseen {#introduction}

Mallin tislaus on tehokas menetelmä, jonka avulla voidaan luoda pienempiä ja tehokkaampia malleja säilyttäen samalla suurten ja monimutkaisten mallien suorituskyky. Prosessi sisältää kompaktin "oppilasmallin" kouluttamisen jäljittelemään suuremman "opettajamallin" toimintaa.

**Keskeiset hyödyt:**
- **Vähentyneet laskentavaatimukset** ennustamisessa
- **Pienempi muistin käyttö** ja tallennustarve
- **Nopeammat ennustamisajat** säilyttäen kohtuullisen tarkkuuden
- **Kustannustehokas käyttöönotto** resurssirajoitteisissa ympäristöissä

## Miksi tislaus on tärkeää {#why-distillation-matters}

Suuret kielimallit (LLM:t) ovat yhä tehokkaampia, mutta samalla yhä vaativampia resurssien suhteen. Vaikka miljardien parametrien malli voi tuottaa erinomaisia tuloksia, se ei välttämättä ole käytännöllinen monissa tosielämän sovelluksissa seuraavista syistä:

### Resurssirajoitteet
- **Laskentakuorma**: Suuret mallit vaativat merkittävästi GPU-muistia ja prosessointitehoa
- **Ennustamisen viive**: Monimutkaiset mallit vievät enemmän aikaa vastauksien tuottamiseen
- **Energian kulutus**: Suuret mallit kuluttavat enemmän energiaa, mikä lisää käyttökustannuksia
- **Infrastruktuurikustannukset**: Suurten mallien ylläpito vaatii kallista laitteistoa

### Käytännön rajoitukset
- **Mobiilikäyttö**: Suuret mallit eivät toimi tehokkaasti mobiililaitteilla
- **Reaaliaikaiset sovellukset**: Sovellukset, jotka vaativat matalaa viivettä, eivät voi käyttää hitaita ennustuksia
- **Reunalaskenta**: IoT- ja reunalaitteilla on rajalliset laskentaresurssit
- **Kustannusnäkökohdat**: Monilla organisaatioilla ei ole varaa suurten mallien infrastruktuuriin

## Tislausprosessi {#the-distillation-process}

Mallin tislaus noudattaa kaksivaiheista prosessia, jossa opettajamallin tieto siirretään oppilasmallille:

### Vaihe 1: Synteettisen datan luominen

Opettajamalli tuottaa vastauksia koulutusdatallesi, luoden korkealaatuista synteettistä dataa, joka heijastaa opettajan tietoa ja päättelymalleja.

```python
# Conceptual example of synthetic data generation
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**Tämän vaiheen keskeiset näkökohdat:**
- Opettajamalli käsittelee jokaisen koulutusesimerkin
- Tuotetut vastaukset toimivat "totuudenmukaisena datana" oppilasmallin koulutuksessa
- Prosessi tallentaa opettajan päätöksentekotavat
- Synteettisen datan laatu vaikuttaa suoraan oppilasmallin suorituskykyyn

### Vaihe 2: Oppilasmallin hienosäätö

Oppilasmalli koulutetaan synteettisellä datalla, oppien jäljittelemään opettajan toimintaa ja vastauksia.

```python
# Conceptual example of student training
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**Koulutuksen tavoitteet:**
- Minimoi ero oppilas- ja opettajamallin tulosten välillä
- Säilytä opettajan tieto pienemmässä parametrimäärässä
- Säilytä suorituskyky samalla kun mallin monimutkaisuus vähenee

## Käytännön toteutus {#practical-implementation}

### Opettaja- ja oppilasmallien valinta

**Opettajamallin valinta:**
- Valitse laajamittaiset LLM:t (100B+ parametriä), jotka ovat osoittaneet suorituskykyä tehtävässäsi
- Suosittuja opettajamalleja ovat:
  - **DeepSeek V3** (671B parametriä) - erinomainen päättelyyn ja koodin generointiin
  - **Meta Llama 3.1 405B Instruct** - monipuoliset yleiskäyttöiset ominaisuudet
  - **GPT-4** - vahva suorituskyky monenlaisissa tehtävissä
  - **Claude 3.5 Sonnet** - erinomainen monimutkaisten päättelytehtävien ratkaisuun
- Varmista, että opettajamalli toimii hyvin alakohtaisella datallasi

**Oppilasmallin valinta:**
- Tasapainota mallin koko ja suorituskykyvaatimukset
- Keskity tehokkaisiin, pienempiin malleihin, kuten:
  - **Microsoft Phi-4-mini** - uusin tehokas malli, jossa vahvat päättelyominaisuudet
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K ja 128K versiot)
  - Microsoft Phi-3.5 Mini Instruct

### Toteutusvaiheet

1. **Datan valmistelu**
   ```python
   # Prepare your training dataset
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **Opettajamallin asennus**
   ```python
   # Initialize large-scale teacher model (100B+ parameters)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # Alternative: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **Synteettisen datan luominen**
   ```python
   # Generate responses from teacher model
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **Oppilasmallin koulutus**
   ```python
   # Fine-tune Phi-4-mini as student model
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Azure ML -tislausesimerkki {#azure-ml-example}

Azure Machine Learning tarjoaa kattavan alustan mallin tislaamisen toteuttamiseen. Näin voit hyödyntää Azure ML:ää tislausprosessissasi:

### Esivaatimukset

1. **Azure ML -työtila**: Määritä työtilasi sopivalla alueella
   - Varmista pääsy laajamittaisiin opettajamalleihin (DeepSeek V3, Llama 405B)
   - Konfiguroi alueet mallien saatavuuden mukaan

2. **Laskentaresurssit**: Määritä sopivat laskentayksiköt koulutusta varten
   - Suurimuistiset yksiköt opettajamallin ennustamiseen
   - GPU-tuetut yksiköt oppilasmallin hienosäätöön

### Tuetut tehtävätyypit

Azure ML tukee tislausta eri tehtäviin:

- **Luonnollisen kielen tulkinta (NLI)**
- **Keskustelu-AI**
- **Kysymysten ja vastausten tuottaminen (QA)**
- **Matemaattinen päättely**
- **Tekstin tiivistäminen**

### Esimerkkitoteutus

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Initialize Azure ML client
ml_client = MLClient.from_config()

# Define distillation job with DeepSeek V3 as teacher and Phi-4-mini as student
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # Large-scale teacher model (671B parameters)
    student_model="phi-4-mini",   # Efficient student model
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # Lower learning rate for fine-tuning
        "batch_size": 2,          # Smaller batch size for memory efficiency
        "num_epochs": 3,
        "temperature": 0.7        # Teacher output softness
    }
)

# Submit distillation job
job = ml_client.jobs.create_or_update(distillation_job)
```

### Seuranta ja arviointi

```python
# Monitor training progress
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# Evaluate distilled Phi-4-mini model
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# Compare with original Phi-4-mini baseline
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## Parhaat käytännöt ja optimointi {#best-practices}

### Datan laatu

**Korkealaatuinen koulutusdata on ratkaisevaa:**
- Varmista monipuoliset ja edustavat koulutusesimerkit
- Käytä alakohtaista dataa, kun mahdollista
- Vahvista opettajamallin tuotokset ennen niiden käyttöä oppilasmallin koulutuksessa
- Tasapainota datasetti välttääksesi oppilasmallin oppimisharhat

### Hyperparametrien säätö

**Keskeiset parametrit optimoitavaksi:**
- **Oppimisnopeus**: Aloita pienillä nopeuksilla (1e-5 - 5e-5) hienosäätöä varten
- **Eräkoko**: Tasapainota muistin rajoitteet ja koulutuksen vakaus
- **Epochien määrä**: Seuraa ylikoulutusta; yleensä 2-5 epochia riittää
- **Lämpötilan säätö**: Säädä opettajan tuotosten pehmeyttä paremman tiedonsiirron saavuttamiseksi

### Mallin arkkitehtuurin huomioiminen

**Opettaja-oppilas yhteensopivuus:**
- Varmista arkkitehtuurinen yhteensopivuus opettaja- ja oppilasmallien välillä
- Harkitse välivaiheiden kerrosten yhteensovittamista paremman tiedonsiirron saavuttamiseksi
- Käytä huomion siirtotekniikoita, kun mahdollista

### Arviointistrategiat

**Kattava arviointilähestymistapa:**
```python
# Multi-metric evaluation
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## Käyttökohteet tosielämässä {#real-world-applications}

### Mobiili- ja reunalaitteiden käyttö

Tislattujen mallien avulla AI-ominaisuudet voidaan tuoda resurssirajoitteisiin laitteisiin:
- **Älypuhelinsovellukset**, joissa reaaliaikainen tekstinkäsittely
- **IoT-laitteet**, jotka suorittavat paikallista ennustamista
- **Sulautetut järjestelmät**, joissa rajalliset laskentaresurssit

### Kustannustehokkaat tuotantojärjestelmät

Organisaatiot käyttävät tislausta vähentääkseen käyttökustannuksia:
- **Asiakaspalveluchatbotit**, joissa nopeammat vastausajat
- **Sisällön moderointijärjestelmät**, jotka käsittelevät suuria määriä tehokkaasti
- **Reaaliaikaiset käännöspalvelut**, joissa matalammat viivevaatimukset

### Alakohtaiset sovellukset

Tislaus auttaa luomaan erikoistuneita malleja:
- **Lääketieteellinen diagnoosiapu**, jossa yksityisyyttä säilyttävä paikallinen ennustaminen
- **Oikeudellisten asiakirjojen analyysi**, optimoitu tiettyihin oikeudellisiin aloihin
- **Taloudellinen riskianalyysi**, jossa nopea päätöksenteko

### Case Study: Asiakastuki DeepSeek V3 → Phi-4-mini

Teknologiayritys toteutti tislausta asiakastukijärjestelmäänsä:

**Toteutuksen yksityiskohdat:**
- **Opettajamalli**: DeepSeek V3 (671B parametriä) - erinomainen päättelyyn monimutkaisissa asiakaskyselyissä
- **Oppilasmalli**: Phi-4-mini - optimoitu nopeaan ennustamiseen ja käyttöönottoon
- **Koulutusdata**: 50 000 asiakastukikeskustelua
- **Tehtävä**: Monivaiheinen keskustelutuki teknisten ongelmien ratkaisuun

**Saavutetut tulokset:**
- **85 % vähennys** ennustusaikaan (3,2s → 0,48s per vastaus)
- **95 % väheneminen** muistivaatimuksissa (1,2TB → 60GB)
- **92 % alkuperäisen mallin tarkkuuden säilyttäminen** tukitehtävissä
- **60 % kustannusten vähennys**
- **Parantunut skaalautuvuus** - pystyy nyt käsittelemään 10x enemmän samanaikaisia käyttäjiä

**Suorituskyvyn erittely:**
```python
# Comparison metrics
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## Yhteenveto {#conclusion}

Mallin tislaus edustaa keskeistä tekniikkaa edistyneiden AI-ominaisuuksien saatavuuden demokratisoimiseksi. Tislaus mahdollistaa pienempien, tehokkaampien mallien luomisen, jotka säilyttävät suurten mallien suorituskyvyn ja vastaavat kasvavaan tarpeeseen käytännölliselle AI-käyttöönotolle.

### Keskeiset opit

1. **Tislaus yhdistää kuilun** mallin suorituskyvyn ja käytännön rajoitteiden välillä
2. **Kaksivaiheinen prosessi** varmistaa tehokkaan tiedonsiirron opettajalta oppilaalle
3. **Azure ML tarjoaa vahvan infrastruktuurin** tislausprosessien toteuttamiseen
4. **Asianmukainen arviointi ja optimointi** ovat välttämättömiä onnistuneelle tislaamiselle
5. **Tosielämän sovellukset** osoittavat merkittäviä hyötyjä kustannuksissa, nopeudessa ja saavutettavuudessa

### Tulevaisuuden suuntaukset

Alan kehittyessä voimme odottaa:
- **Edistyneitä tislaustekniikoita**, joissa paremmat tiedonsiirtomenetelmät
- **Moniopettajatislausta**, joka parantaa oppilasmallin ominaisuuksia
- **Automatisoitua optimointia** tislausprosessille
- **Laajempaa mallitukea** eri arkkitehtuureille ja aloille

Mallin tislaus antaa organisaatioille mahdollisuuden hyödyntää huippuluokan AI-ominaisuuksia samalla kun säilytetään käytännön käyttöönoton rajoitteet, tehden edistyneistä kielimalleista saavutettavia monenlaisissa sovelluksissa ja ympäristöissä.

## ➡️ Mitä seuraavaksi

- [03: Hienosäätö - Mallien mukauttaminen erityistehtäviin](./03.SLMOps-Finetuing.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulee pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskääntämistä. Emme ole vastuussa tämän käännöksen käytöstä aiheutuvista väärinkäsityksistä tai virhetulkinnoista.