<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b72406fecfa88eca821ee16264194172",
  "translation_date": "2025-09-22T20:15:55+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fi"
}
-->
# Osa 3: Microsoft Olive Optimization Suite

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on Microsoft Olive?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Pikaopas](../../../Module04)
5. [Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon](../../../Module04)
6. [Edistyneet käyttömahdollisuudet](../../../Module04)
7. [Parhaat käytännöt](../../../Module04)
8. [Vianmääritys](../../../Module04)
9. [Lisäresurssit](../../../Module04)

## Johdanto

Microsoft Olive on tehokas ja helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yksinkertaistaa koneoppimismallien optimointia eri laitteistoalustoille. Olipa kohteena CPU, GPU tai erikoistuneet AI-kiihdyttimet, Olive auttaa saavuttamaan optimaalisen suorituskyvyn säilyttäen samalla mallin tarkkuuden.

## Mikä on Microsoft Olive?

Olive on helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yhdistää alan johtavia tekniikoita mallin pakkaamiseen, optimointiin ja kääntämiseen. Se toimii ONNX Runtime -ympäristön kanssa tarjoten kokonaisvaltaisen optimointiratkaisun mallien suorituskyvyn parantamiseen.

### Keskeiset ominaisuudet

- **Laitteistotietoinen optimointi**: Valitsee automaattisesti parhaat optimointitekniikat kohdelaitteistolle
- **Yli 40 sisäänrakennettua optimointikomponenttia**: Sisältää mallin pakkaamisen, kvantisoinnin, graafin optimoinnin ja paljon muuta
- **Helppo CLI-käyttöliittymä**: Yksinkertaiset komennot yleisiin optimointitehtäviin
- **Monikehys-tuki**: Yhteensopiva PyTorchin, Hugging Facen mallien ja ONNX:n kanssa
- **Suosittujen mallien tuki**: Olive voi automaattisesti optimoida suosittuja mallirakenteita, kuten Llama, Phi, Qwen, Gemma jne.

### Hyödyt

- **Lyhentynyt kehitysaika**: Ei tarvetta kokeilla manuaalisesti eri optimointitekniikoita
- **Suorituskyvyn parannukset**: Merkittäviä nopeusparannuksia (jopa 6x joissakin tapauksissa)
- **Monialustainen käyttöönotto**: Optimoidut mallit toimivat eri laitteistoilla ja käyttöjärjestelmillä
- **Säilytetty tarkkuus**: Optimoinnit parantavat suorituskykyä säilyttäen mallin laadun

## Asennus

### Esivaatimukset

- Python 3.8 tai uudempi
- pip-pakettien hallinta
- Virtuaalinen ympäristö (suositeltu)

### Perusasennus

Luo ja aktivoi virtuaalinen ympäristö:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Asenna Olive automaattisten optimointiominaisuuksien kanssa:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valinnaiset riippuvuudet

Olive tarjoaa erilaisia valinnaisia riippuvuuksia lisäominaisuuksille:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Asennuksen tarkistus

```bash
olive --help
```

Jos asennus onnistui, näet Olive CLI:n ohjeviestin.

## Pikaopas

### Ensimmäinen optimointi

Optimoidaan pieni kielimalli Oliven automaattisen optimointiominaisuuden avulla:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mitä tämä komento tekee

Optimointiprosessi sisältää: mallin hakemisen paikallisesta välimuistista, ONNX-graafin tallentamisen ja painojen säilyttämisen ONNX-tiedostossa, ONNX-graafin optimoinnin ja mallin kvantisoinnin int4-muotoon RTN-menetelmällä.

### Komentoparametrien selitys

- `--model_name_or_path`: Hugging Facen mallin tunniste tai paikallinen polku
- `--output_path`: Hakemisto, johon optimoitu malli tallennetaan
- `--device`: Kohdelaitteisto (cpu, gpu)
- `--provider`: Suoritusalusta (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Käytä ONNX Runtime Generate AI -ominaisuutta inferenssiin
- `--precision`: Kvantisoinnin tarkkuus (int4, int8, fp16)
- `--log_level`: Lokiviestien yksityiskohtaisuus (0=minimaalinen, 1=laaja)

## Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon

Hugging Facen esimerkin [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) perusteella tässä on ohjeet Qwen3-mallin optimointiin:

### Vaihe 1: Lataa malli (valinnainen)

Vähennä latausaikaa välimuistittamalla vain olennaiset tiedostot:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Vaihe 2: Optimoi Qwen3-malli

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Vaihe 3: Testaa optimoitu malli

Luo yksinkertainen Python-skripti optimoidun mallin testaamiseen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Tulostusrakenne

Optimoinnin jälkeen tulostushakemisto sisältää:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Edistyneet käyttömahdollisuudet

### Konfiguraatiotiedostot

Monimutkaisempia optimointityönkulkuja varten voit käyttää JSON-konfiguraatiotiedostoja:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Suorita konfiguraatiolla:

```bash
olive run --config config.json
```

### GPU-optimointi

CUDA GPU -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mallin hienosäätö Olive-työkalulla

Olive tukee myös mallien hienosäätöä:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Parhaat käytännöt

### 1. Mallin valinta
- Aloita pienemmistä malleista testaukseen (esim. 0.5B-7B parametrit)
- Varmista, että kohdemallin rakenne on yhteensopiva Oliven kanssa

### 2. Laitteistoharkinnat
- Sovita optimointitavoite käyttöönoton laitteistoon
- Käytä GPU-optimointia, jos sinulla on CUDA-yhteensopiva laitteisto
- Harkitse DirectML:ää Windows-koneille, joissa on integroitu grafiikka

### 3. Tarkkuuden valinta
- **INT4**: Maksimaalinen pakkaus, pieni tarkkuuden menetys
- **INT8**: Hyvä tasapaino koon ja tarkkuuden välillä
- **FP16**: Vähäinen tarkkuuden menetys, kohtuullinen koon pienennys

### 4. Testaus ja validointi
- Testaa optimoidut mallit aina omien käyttötapauksiesi kanssa
- Vertaa suorituskykymittareita (viive, läpimeno, tarkkuus)
- Käytä edustavaa syöttödataa arviointiin

### 5. Iteratiivinen optimointi
- Aloita automaattisella optimoinnilla nopeiden tulosten saamiseksi
- Käytä konfiguraatiotiedostoja tarkempaan hallintaan
- Kokeile eri optimointivaiheita

## Vianmääritys

### Yleiset ongelmat

#### 1. Asennusongelmat
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-ongelmat
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Muistiongelmat
- Käytä pienempiä eräkokoja optimoinnin aikana
- Kokeile kvantisointia korkeammalla tarkkuudella ensin (int8 int4:n sijaan)
- Varmista riittävä levytila mallin välimuistille

#### 4. Mallin latausvirheet
- Varmista mallin polku ja käyttöoikeudet
- Tarkista, vaatiiko malli `trust_remote_code=True`
- Varmista, että kaikki tarvittavat mallin tiedostot on ladattu

### Apua ongelmatilanteissa

- **Dokumentaatio**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub-ongelmat**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Esimerkit**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Lisäresurssit

### Viralliset linkit
- **GitHub-repositorio**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime -dokumentaatio**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face -esimerkki**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Yhteisön esimerkit
- **Jupyter-muistikirjat**: Saatavilla Oliven GitHub-repositoriossa — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code -laajennus**: AI Toolkit for VS Code -yleiskatsaus — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogikirjoitukset**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Liittyvät työkalut
- **ONNX Runtime**: Suorituskykyinen inferenssimoottori — https://onnxruntime.ai/
- **Hugging Face Transformers**: Lähde monille yhteensopiville malleille — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Pilvipohjaiset optimointityönkulut — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Mitä seuraavaksi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

