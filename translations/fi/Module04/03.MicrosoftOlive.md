<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "875ceb3a2e681fba8d521e86e8cf7286",
  "translation_date": "2025-09-18T10:22:36+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fi"
}
-->
# Osa 3: Microsoft Olive Optimization Suite

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on Microsoft Olive?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Pikaopas](../../../Module04)
5. [Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon](../../../Module04)
6. [Edistynyt käyttö](../../../Module04)
7. [Parhaat käytännöt](../../../Module04)
8. [Vianmääritys](../../../Module04)
9. [Lisäresurssit](../../../Module04)

## Johdanto

Microsoft Olive on tehokas ja helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yksinkertaistaa koneoppimismallien optimointia eri laitteistoalustoille. Olitpa kohdistamassa CPU:ta, GPU:ta tai erikoistuneita AI-kiihdyttimiä, Olive auttaa saavuttamaan optimaalisen suorituskyvyn säilyttäen samalla mallin tarkkuuden.

## Mikä on Microsoft Olive?

Olive on helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yhdistää alan johtavia tekniikoita mallin pakkaamiseen, optimointiin ja kääntämiseen. Se toimii ONNX Runtime -alustan kanssa tarjoten kokonaisvaltaisen optimointiratkaisun mallien inferenssille.

### Keskeiset ominaisuudet

- **Laitteistotietoinen optimointi**: Valitsee automaattisesti parhaat optimointitekniikat kohdelaitteistolle
- **40+ sisäänrakennettua optimointikomponenttia**: Sisältää mallin pakkaamisen, kvantisoinnin, graafin optimoinnin ja paljon muuta
- **Helppo CLI-käyttöliittymä**: Yksinkertaiset komennot yleisiin optimointitehtäviin
- **Monikehys-tuki**: Yhteensopiva PyTorchin, Hugging Facen mallien ja ONNX:n kanssa
- **Suosittujen mallien tuki**: Olive voi automaattisesti optimoida suosittuja mallirakenteita, kuten Llama, Phi, Qwen, Gemma jne.

### Hyödyt

- **Lyhyempi kehitysaika**: Ei tarvetta kokeilla manuaalisesti eri optimointitekniikoita
- **Suorituskyvyn parannukset**: Merkittäviä nopeusparannuksia (jopa 6x joissakin tapauksissa)
- **Monialustainen käyttöönotto**: Optimoidut mallit toimivat eri laitteistoilla ja käyttöjärjestelmillä
- **Säilytetty tarkkuus**: Optimoinnit säilyttävät mallin laadun parantaen samalla suorituskykyä

## Asennus

### Esivaatimukset

- Python 3.8 tai uudempi
- pip-pakettienhallinta
- Virtuaalinen ympäristö (suositeltu)

### Perusasennus

Luo ja aktivoi virtuaalinen ympäristö:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Asenna Olive automaattisten optimointiominaisuuksien kanssa:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valinnaiset riippuvuudet

Olive tarjoaa erilaisia valinnaisia riippuvuuksia lisäominaisuuksille:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Asennuksen tarkistus

```bash
olive --help
```

Jos asennus onnistuu, näet Olive CLI:n ohjeviestin.

## Pikaopas

### Ensimmäinen optimointisi

Optimoidaan pieni kielimalli Oliven automaattisen optimointiominaisuuden avulla:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mitä tämä komento tekee

Optimointiprosessi sisältää: mallin hankkimisen paikallisesta välimuistista, ONNX-graafin tallentamisen ja painojen säilömisen ONNX-tiedostoon, graafin optimoinnin ja mallin kvantisoinnin int4-muotoon RTN-menetelmällä.

### Komentoparametrien selitys

- `--model_name_or_path`: Hugging Facen mallin tunniste tai paikallinen polku
- `--output_path`: Hakemisto, johon optimoitu malli tallennetaan
- `--device`: Kohdelaite (cpu, gpu)
- `--provider`: Suoritusalusta (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Käytä ONNX Runtime Generate AI -ominaisuutta inferenssissä
- `--precision`: Kvantisoinnin tarkkuus (int4, int8, fp16)
- `--log_level`: Lokien yksityiskohtaisuus (0=minimaalinen, 1=laaja)

## Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon

Hugging Facen esimerkin [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) perusteella optimoidaan Qwen3-malli seuraavasti:

### Vaihe 1: Lataa malli (valinnainen)

Vähennä latausaikaa välimuistittamalla vain olennaiset tiedostot:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Vaihe 2: Optimoi Qwen3-malli

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Vaihe 3: Testaa optimoitu malli

Luo yksinkertainen Python-skripti optimoidun mallin testaamiseksi:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Tulostusrakenne

Optimoinnin jälkeen tuloshakemisto sisältää:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Edistynyt käyttö

### Konfiguraatiotiedostot

Monimutkaisempia optimointityönkulkuja varten voit käyttää JSON-konfiguraatiotiedostoja:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Suorita konfiguraation avulla:

```bash
olive run --config config.json
```

### GPU-optimointi

CUDA GPU -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mallien hienosäätö Olivea käyttäen

Olive tukee myös mallien hienosäätöä:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Parhaat käytännöt

### 1. Mallin valinta
- Aloita pienemmillä malleilla testaukseen (esim. 0.5B-7B parametrit)
- Varmista, että kohdemallin rakenne on Oliven tukema

### 2. Laitteistoharkinnat
- Kohdista optimointi käyttöönoton laitteistoon
- Käytä GPU-optimointia, jos sinulla on CUDA-yhteensopiva laitteisto
- Harkitse DirectML:ää Windows-koneille, joissa on integroitu grafiikka

### 3. Tarkkuuden valinta
- **INT4**: Maksimaalinen pakkaus, pieni tarkkuuden menetys
- **INT8**: Hyvä tasapaino koon ja tarkkuuden välillä
- **FP16**: Vähäinen tarkkuuden menetys, kohtalainen koon pienennys

### 4. Testaus ja validointi
- Testaa optimoidut mallit aina omien käyttötapauksiesi kanssa
- Vertaa suorituskykymittareita (viive, läpimeno, tarkkuus)
- Käytä edustavaa syöttödataa arviointiin

### 5. Iteratiivinen optimointi
- Aloita automaattisella optimoinnilla nopeiden tulosten saamiseksi
- Käytä konfiguraatiotiedostoja tarkempaan hallintaan
- Kokeile eri optimointivaiheita

## Vianmääritys

### Yleiset ongelmat

#### 1. Asennusongelmat
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-ongelmat
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Muistiongelmat
- Käytä pienempiä eräkokoja optimoinnin aikana
- Kokeile kvantisointia korkeammalla tarkkuudella ensin (int8 int4:n sijaan)
- Varmista riittävä levytila mallien välimuistille

#### 4. Mallin latausvirheet
- Varmista mallin polku ja käyttöoikeudet
- Tarkista, vaatiiko malli `trust_remote_code=True`
- Varmista, että kaikki tarvittavat mallin tiedostot on ladattu

### Apua ongelmiin

- **Dokumentaatio**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub-ongelmat**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Esimerkit**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Lisäresurssit

### Viralliset linkit
- **GitHub-repositorio**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ONNX Runtime -dokumentaatio**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face -esimerkki**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Yhteisön esimerkit
- **Jupyter-muistikirjat**: Saatavilla Oliven GitHub-repositoriossa
- **VS Code -laajennus**: AI Toolkit -laajennus käyttää Olivea mallien optimointiin
- **Blogikirjoitukset**: Microsoft Open Source Blog sisältää yksityiskohtaisia Olive-opetusohjelmia

### Liittyvät työkalut
- **ONNX Runtime**: Suorituskykyinen inferenssimoottori
- **Hugging Face Transformers**: Lähde monille yhteensopiville malleille
- **Azure Machine Learning**: Pilvipohjaiset optimointityönkulut

## ➡️ Mitä seuraavaksi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.