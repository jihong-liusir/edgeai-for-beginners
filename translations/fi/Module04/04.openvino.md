<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-18T10:16:58+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "fi"
}
-->
# Osa 4: OpenVINO Toolkit Optimization Suite

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on OpenVINO?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Pikaopas](../../../Module04)
5. [Esimerkki: Mallien muuntaminen ja optimointi OpenVINOn avulla](../../../Module04)
6. [Edistynyt käyttö](../../../Module04)
7. [Parhaat käytännöt](../../../Module04)
8. [Vianmääritys](../../../Module04)
9. [Lisäresurssit](../../../Module04)

## Johdanto

OpenVINO (Open Visual Inference and Neural Network Optimization) on Intelin avoimen lähdekoodin työkalu, joka mahdollistaa suorituskykyisten tekoälyratkaisujen käyttöönoton pilvessä, paikallisissa ympäristöissä ja reunalaitteissa. Olipa kohteena CPU, GPU, VPU tai erikoistuneet tekoälykiihdyttimet, OpenVINO tarjoaa kattavat optimointimahdollisuudet säilyttäen mallin tarkkuuden ja mahdollistamalla alustojen välisen käyttöönoton.

## Mikä on OpenVINO?

OpenVINO on avoimen lähdekoodin työkalu, joka auttaa kehittäjiä optimoimaan, muuntamaan ja ottamaan tekoälymalleja tehokkaasti käyttöön monipuolisilla laitealustoilla. Se koostuu kolmesta pääkomponentista: OpenVINO Runtime suorituskykyä varten, Neural Network Compression Framework (NNCF) mallien optimointiin ja OpenVINO Model Server skaalautuvaan käyttöönottoon.

### Keskeiset ominaisuudet

- **Alustojen välinen käyttöönotto**: Tukee Linuxia, Windowsia ja macOSia Python-, C++- ja C-rajapinnoilla
- **Laitteistokiihdytys**: Automaattinen laitteiden tunnistus ja optimointi CPU-, GPU-, VPU- ja tekoälykiihdyttimille
- **Mallien kompressiokehys**: Kehittyneet kvantisointi-, karsinta- ja optimointitekniikat NNCF:n avulla
- **Yhteensopivuus kehysten kanssa**: Suora tuki TensorFlow-, ONNX-, PaddlePaddle- ja PyTorch-malleille
- **Generatiivinen tekoäly**: Erikoistunut OpenVINO GenAI suurten kielimallien ja generatiivisten tekoälysovellusten käyttöönottoon

### Hyödyt

- **Suorituskyvyn optimointi**: Merkittäviä nopeusparannuksia vähäisellä tarkkuuden menetyksellä
- **Pienempi käyttöönottojälki**: Vähäiset ulkoiset riippuvuudet yksinkertaistavat asennusta ja käyttöönottoa
- **Parannettu käynnistysaika**: Optimoitu mallin lataus ja välimuisti nopeampaan sovelluksen käynnistykseen
- **Skaalautuva käyttöönotto**: Reunalaitteista pilvi-infrastruktuuriin yhtenäisillä rajapinnoilla
- **Valmis tuotantoon**: Yritystason luotettavuus kattavalla dokumentaatiolla ja yhteisön tuella

## Asennus

### Esivaatimukset

- Python 3.8 tai uudempi
- pip-pakettien hallinta
- Virtuaalinen ympäristö (suositeltu)
- Yhteensopiva laitteisto (Intel CPU:t suositeltuja, mutta tukee useita arkkitehtuureja)

### Perusasennus

Luo ja aktivoi virtuaalinen ympäristö:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

Asenna OpenVINO Runtime:

```bash
pip install openvino
```

Asenna NNCF mallien optimointiin:

```bash
pip install nncf
```

### OpenVINO GenAI -asennus

Generatiivisia tekoälysovelluksia varten:

```bash
pip install openvino-genai
```

### Valinnaiset riippuvuudet

Lisäpaketteja erityisiin käyttötapauksiin:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### Asennuksen tarkistus

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

Jos asennus onnistui, näet OpenVINOn version tiedot.

## Pikaopas

### Ensimmäinen mallin optimointi

Muunnetaan ja optimoidaan Hugging Face -malli OpenVINOn avulla:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### Mitä tämä prosessi tekee

Optimointityönkulku sisältää: alkuperäisen mallin lataamisen Hugging Facesta, muuntamisen OpenVINOn Intermediate Representation (IR) -muotoon, oletusoptimointien soveltamisen ja kääntämisen kohdelaitteistolle.

### Keskeisten parametrien selitys

- `export=True`: Muuntaa mallin OpenVINOn IR-muotoon
- `compile=False`: Viivästyttää kääntämistä joustavuuden vuoksi
- `device`: Kohdelaitteisto ("CPU", "GPU", "AUTO" automaattista valintaa varten)
- `save_pretrained()`: Tallentaa optimoidun mallin uudelleenkäyttöä varten

## Esimerkki: Mallien muuntaminen ja optimointi OpenVINOn avulla

### Vaihe 1: Mallin muuntaminen NNCF-kvantisoinnilla

Näin sovelletaan jälkikoulutuksen kvantisointia NNCF:n avulla:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### Vaihe 2: Edistynyt optimointi painojen kompressiolla

Transformer-pohjaisille malleille sovelletaan painojen kompressiota:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### Vaihe 3: Inferenssi optimoidulla mallilla

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### Lopputuloksen rakenne

Optimoinnin jälkeen mallihakemisto sisältää:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## Edistynyt käyttö

### NNCF YAML -konfiguraatio

Monimutkaisiin optimointityönkulkuihin käytä NNCF-konfiguraatiotiedostoja:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

Sovella konfiguraatio:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU-optimointi

GPU-kiihdytykseen:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### Eräkäsittelyn optimointi

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### Mallipalvelimen käyttöönotto

Ota optimoidut mallit käyttöön OpenVINO Model Serverin avulla:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

Asiakaskoodi mallipalvelimelle:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## Parhaat käytännöt

### 1. Mallin valinta ja valmistelu
- Käytä malleja tuetuista kehyksistä (PyTorch, TensorFlow, ONNX)
- Varmista, että mallin syötteillä on kiinteät tai tunnetut dynaamiset muodot
- Testaa edustavilla datasarjoilla kalibrointia varten

### 2. Optimointistrategian valinta
- **Jälkikoulutuksen kvantisointi**: Aloita tästä nopeaan optimointiin
- **Painojen kompressio**: Sopii suurille kielimalleille ja transformereille
- **Kvantisointiin liittyvä koulutus**: Käytä, kun tarkkuus on kriittistä

### 3. Laitteistokohtainen optimointi
- **CPU**: Käytä INT8-kvantisointia tasapainoiseen suorituskykyyn
- **GPU**: Hyödynnä FP16-tarkkuutta ja eräkäsittelyä
- **VPU**: Keskity mallin yksinkertaistamiseen ja kerrosten yhdistämiseen

### 4. Suorituskyvyn hienosäätö
- **Läpimenoaikatila**: Suurivolyymiseen eräkäsittelyyn
- **Viiveaikatila**: Reaaliaikaisiin interaktiivisiin sovelluksiin
- **AUTO-laite**: Anna OpenVINOn valita optimaalinen laitteisto

### 5. Muistin hallinta
- Käytä dynaamisia muotoja harkiten välttääksesi muistin ylikuormituksen
- Toteuta mallin välimuisti nopeampia latauksia varten
- Seuraa muistin käyttöä optimoinnin aikana

### 6. Tarkkuuden validointi
- Varmista aina optimoitujen mallien suorituskyky alkuperäiseen verrattuna
- Käytä edustavia testidatasarjoja arviointiin
- Harkitse asteittaista optimointia (aloita varovaisilla asetuksilla)

## Vianmääritys

### Yleiset ongelmat

#### 1. Asennusongelmat
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. Mallin muunnosvirheet
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. Suorituskykyongelmat
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. Muistiongelmat
- Pienennä mallin eräkokoa optimoinnin aikana
- Käytä suoratoistoa suurille datasarjoille
- Ota mallin välimuisti käyttöön: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. Tarkkuuden heikkeneminen
- Käytä korkeampaa tarkkuutta (INT8 INT4:n sijaan)
- Lisää kalibrointidatasarjan kokoa
- Sovella sekatarkkuuden optimointia

### Suorituskyvyn seuranta

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### Apua ongelmatilanteisiin

- **Dokumentaatio**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub-ongelmat**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **Yhteisöfoorumi**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## Lisäresurssit

### Viralliset linkit
- **OpenVINO-etusivu**: [openvino.ai](https://openvino.ai/)
- **GitHub-repositorio**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF-repositorio**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **Mallizoo**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### Oppimisresurssit
- **OpenVINO-notebookit**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **Pikaopas**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **Optimointiopas**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### Integraatiotyökalut
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### Suorituskykyvertailut
- **Viralliset vertailut**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF-mallizoo**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### Yhteisön esimerkit
- **Jupyter-notebookit**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - Kattavat tutoriaalit OpenVINO-notebookien repositoriossa
- **Esimerkkisovellukset**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - Käytännön esimerkkejä eri aloilta (tietokonenäkö, NLP, ääni)
- **Blogikirjoitukset**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI:n ja yhteisön blogikirjoituksia yksityiskohtaisilla käyttötapauksilla

### Liittyvät työkalut
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Lisäoptimointitekniikoita Intelin laitteistolle
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Vertailuja mobiili- ja reunakäyttöönottoa varten
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Vaihtoehtoinen alustojen välinen inferenssimoottori

## ➡️ Mitä seuraavaksi

- [05: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.