<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-09-18T10:26:50+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "fi"
}
-->
# Osa 2: Llama.cpp Toteutusopas

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on Llama.cpp?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Rakentaminen lähdekoodista](../../../Module04)
5. [Mallin kvantisointi](../../../Module04)
6. [Peruskäyttö](../../../Module04)
7. [Edistyneet ominaisuudet](../../../Module04)
8. [Python-integraatio](../../../Module04)
9. [Vianmääritys](../../../Module04)
10. [Parhaat käytännöt](../../../Module04)

## Johdanto

Tämä kattava opas opastaa sinut Llama.cpp:n käytön perusteista aina edistyneisiin käyttötapauksiin. Llama.cpp on tehokas C++-toteutus, joka mahdollistaa suurten kielimallien (LLM) tehokkaan käytön minimaalisella asennuksella ja erinomaisella suorituskyvyllä eri laitteistokokoonpanoissa.

## Mikä on Llama.cpp?

Llama.cpp on C/C++-kielellä kirjoitettu LLM-päättelykehys, joka mahdollistaa suurten kielimallien ajamisen paikallisesti minimaalisella asennuksella ja huippuluokan suorituskyvyllä monenlaisilla laitteistoilla. Keskeisiä ominaisuuksia ovat:

### Keskeiset ominaisuudet
- **Pelkkä C/C++-toteutus** ilman riippuvuuksia
- **Monialustainen yhteensopivuus** (Windows, macOS, Linux)
- **Laitteistojen optimointi** eri arkkitehtuureille
- **Kvantisointituki** (1,5-bittisestä 8-bittiseen kokonaislukukvantisointiin)
- **CPU- ja GPU-kiihdytys** tuki
- **Muistitehokkuus** rajallisissa ympäristöissä

### Edut
- Toimii tehokkaasti CPU:lla ilman erikoislaitteistoa
- Tukee useita GPU-taustajärjestelmiä (CUDA, Metal, OpenCL, Vulkan)
- Kevyt ja helposti siirrettävä
- Apple Silicon on ensiluokkainen - optimoitu ARM NEON-, Accelerate- ja Metal-kehysten avulla
- Tukee erilaisia kvantisointitasoja pienemmän muistinkäytön saavuttamiseksi

## Asennus

### Menetelmä 1: Esirakennetut binääritiedostot (suositeltu aloittelijoille)

#### Lataa GitHub Releases -sivulta
1. Siirry [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) -sivulle
2. Lataa järjestelmällesi sopiva binääritiedosto:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` Windowsille
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` macOS:lle
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` Linuxille

3. Pura arkisto ja lisää hakemisto järjestelmän PATH-muuttujaan

#### Pakettienhallinnan käyttö

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (eri jakelut):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Menetelmä 2: Python-paketti (llama-cpp-python)

#### Perusasennus
```bash
pip install llama-cpp-python
```

#### Laitteistokiihdytyksellä
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## Rakentaminen lähdekoodista

### Esivaatimukset

**Järjestelmävaatimukset:**
- C++-kääntäjä (GCC, Clang tai MSVC)
- CMake (versio 3.14 tai uudempi)
- Git
- Rakennustyökalut alustallesi

**Esivaatimusten asennus:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Asenna Visual Studio 2022 C++-kehitystyökaluilla
- Asenna CMake viralliselta verkkosivustolta
- Asenna Git

### Perusrakennusprosessi

1. **Kloonaa arkisto:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Määritä rakennus:**
```bash
cmake -B build
```

3. **Rakenna projekti:**
```bash
cmake --build build --config Release
```

Nopeampaa kääntämistä varten käytä rinnakkaisia tehtäviä:
```bash
cmake --build build --config Release -j 8
```

### Laitteistokohtaiset rakennukset

#### CUDA-tuki (NVIDIA GPU:t)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal-tuki (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS-tuki (CPU-optimointi)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan-tuki
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Edistyneet rakennusvaihtoehdot

#### Debug-rakennus
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### Lisäominaisuuksilla
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Mallin kvantisointi

### GGUF-muodon ymmärtäminen

GGUF (Generalized GGML Unified Format) on optimoitu tiedostomuoto, joka on suunniteltu suurten kielimallien tehokkaaseen käyttöön Llama.cpp:n ja muiden kehysten avulla. Se tarjoaa:

- Vakioidun mallipainojen tallennuksen
- Parannetun yhteensopivuuden eri alustojen välillä
- Tehostetun suorituskyvyn
- Tehokkaan metadatan käsittelyn

### Kvantisointityypit

Llama.cpp tukee useita kvantisointitasoja:

| Tyyppi | Bitit | Kuvaus | Käyttötapaus |
|--------|-------|--------|--------------|
| F16 | 16 | Puolitarkkuus | Korkea laatu, suuri muisti |
| Q8_0 | 8 | 8-bittinen kvantisointi | Hyvä tasapaino |
| Q4_0 | 4 | 4-bittinen kvantisointi | Kohtalainen laatu, pienempi koko |
| Q2_K | 2 | 2-bittinen kvantisointi | Pienin koko, heikompi laatu |

### Mallien muuntaminen

#### PyTorchista GGUF:ksi
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Suora lataus Hugging Facesta
Monet mallit ovat saatavilla GGUF-muodossa Hugging Facessa:
- Etsi malleja, joiden nimessä on "GGUF"
- Lataa sopiva kvantisointitaso
- Käytä suoraan Llama.cpp:n kanssa

## Peruskäyttö

### Komentorivikäyttö

#### Yksinkertainen tekstin generointi
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Mallien käyttö Hugging Facesta
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Palvelintila
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Yleiset parametrit

| Parametri | Kuvaus | Esimerkki |
|-----------|--------|-----------|
| `-m` | Mallitiedoston polku | `-m model.gguf` |
| `-p` | Kehoteteksti | `-p "Hello world"` |
| `-n` | Generoitavien tokenien määrä | `-n 100` |
| `-c` | Kontekstin koko | `-c 4096` |
| `-t` | Säikeiden määrä | `-t 8` |
| `-ngl` | GPU-kerrokset | `-ngl 32` |
| `-temp` | Lämpötila | `-temp 0.7` |

### Vuorovaikutteinen tila

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Edistyneet ominaisuudet

### Palvelimen API

#### Palvelimen käynnistäminen
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API:n käyttö
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### Suorituskyvyn optimointi

#### Muistinhallinta
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Monisäikeisyys
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU-kiihdytys
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python-integraatio

### Peruskäyttö llama-cpp-pythonin kanssa

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Keskustelukäyttöliittymä

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Vastausten suoratoisto

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integraatio LangChainin kanssa

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Vianmääritys

### Yleiset ongelmat ja ratkaisut

#### Rakennusvirheet

**Ongelma: CMake ei löydy**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Ongelma: Kääntäjä ei löydy**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Käyttövirheet

**Ongelma: Mallin lataus epäonnistuu**
- Tarkista mallin tiedostopolku
- Varmista tiedoston käyttöoikeudet
- Varmista riittävä RAM-muisti
- Kokeile eri kvantisointitasoja

**Ongelma: Heikko suorituskyky**
- Ota laitteistokiihdytys käyttöön
- Lisää säikeiden määrää
- Käytä sopivaa kvantisointia
- Tarkista GPU-muistin käyttö

#### Muistiongelmat

**Ongelma: Muisti loppuu**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Alustakohtaiset ongelmat

#### Windows
- Käytä MinGW- tai Visual Studio -kääntäjää
- Varmista oikea PATH-asetus
- Tarkista virustorjunnan häiriöt

#### macOS
- Ota Metal käyttöön Apple Siliconille
- Käytä Rosetta 2:ta yhteensopivuuden varmistamiseksi tarvittaessa
- Tarkista Xcode-komentorivityökalut

#### Linux
- Asenna kehityspaketit
- Tarkista GPU-ajurien versiot
- Varmista CUDA-työkalupaketin asennus

## Parhaat käytännöt

### Mallin valinta
1. **Valitse sopiva kvantisointi** laitteistosi perusteella
2. **Harkitse mallin kokoa** ja laadun kompromisseja
3. **Testaa eri malleja** käyttötapauksesi mukaan

### Suorituskyvyn optimointi
1. **Käytä GPU-kiihdytystä**, jos mahdollista
2. **Optimoi säikeiden määrä** CPU:llesi
3. **Aseta sopiva kontekstin koko** käyttötapauksesi mukaan
4. **Ota muistimappaus käyttöön** suurille malleille

### Tuotantokäyttö
1. **Käytä palvelintilaa** API:n käyttöön
2. **Toteuta asianmukainen virheenkäsittely**
3. **Seuraa resurssien käyttöä**
4. **Ota käyttöön lokitus ja valvonta**

### Kehitystyönkulku
1. **Aloita pienemmillä malleilla** testaukseen
2. **Käytä versionhallintaa** mallikonfiguraatioille
3. **Dokumentoi konfiguraatiosi**
4. **Testaa eri alustoilla**

### Tietoturva
1. **Vahvista syötekäskyt**
2. **Toteuta rajoitukset**
3. **Suojaa API-päätepisteet**
4. **Seuraa väärinkäytöksen malleja**

## Yhteenveto

Llama.cpp tarjoaa tehokkaan ja joustavan tavan käyttää suuria kielimalleja paikallisesti eri laitteistokokoonpanoilla. Olitpa kehittämässä tekoälysovelluksia, tekemässä tutkimusta tai kokeilemassa LLM:ien käyttöä, tämä kehys tarjoaa tarvittavan joustavuuden ja suorituskyvyn monenlaisiin käyttötapauksiin.

Keskeiset asiat:
- Valitse asennusmenetelmä tarpeidesi mukaan
- Optimoi laitteistosi mukaisesti
- Aloita peruskäytöstä ja tutustu vähitellen edistyneisiin ominaisuuksiin
- Harkitse Python-sidosten käyttöä helpompaan integraatioon
- Noudata parhaita käytäntöjä tuotantokäyttöön

Lisätietoja ja päivityksiä löydät [virallisesta Llama.cpp-arkistosta](https://github.com/ggml-org/llama.cpp) sekä kattavasta dokumentaatiosta ja yhteisön resursseista.

## ➡️ Mitä seuraavaksi

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.