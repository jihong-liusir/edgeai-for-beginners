<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "38b68a204a9621126d056b0e5b51ab7c",
  "translation_date": "2025-10-09T14:21:33+00:00",
  "source_file": "Module03/01.SLMAdvancedLearning.md",
  "language_code": "fi"
}
-->
# Osa 1: SLM-tekniikan perusteet ja optimointi

Pienet kielimallit (Small Language Models, SLMs) ovat merkitt√§v√§ edistysaskel EdgeAI:ssa, mahdollistaen kehittyneet luonnollisen kielen k√§sittelyominaisuudet resurssirajoitteisilla laitteilla. SLM-mallien tehokas k√§ytt√∂√∂notto, optimointi ja hy√∂dynt√§minen ovat olennaisia k√§yt√§nn√∂nl√§heisten edge-pohjaisten teko√§lyratkaisujen rakentamisessa.

## Johdanto

T√§ss√§ oppitunnissa tutustumme pieniin kielimalleihin (SLMs) ja niiden edistyneisiin toteutusstrategioihin. K√§ymme l√§pi SLM-mallien perusk√§sitteet, niiden parametrit ja luokittelut, optimointitekniikat sek√§ k√§yt√§nn√∂n k√§ytt√∂√∂noton strategiat edge-laskentaymp√§rist√∂iss√§.

## Oppimistavoitteet

Oppitunnin lopussa osaat:

- üî¢ Ymm√§rt√§√§ pienten kielimallien parametrit ja luokittelut.
- üõ†Ô∏è Tunnistaa keskeiset optimointitekniikat SLM-mallien k√§ytt√∂√∂nottoon edge-laitteilla.
- üöÄ Oppia toteuttamaan edistyneit√§ kvantisointi- ja pakkausstrategioita SLM-malleille.

## SLM-mallien parametrien rajat ja luokittelut

Pienet kielimallit (SLMs) ovat teko√§lymalleja, jotka on suunniteltu k√§sittelem√§√§n, ymm√§rt√§m√§√§n ja tuottamaan luonnollista kielt√§ huomattavasti pienemm√§ll√§ parametrim√§√§r√§ll√§ kuin suuret kielimallit (LLMs). Suurilla kielimalleilla on satoja miljardeja tai jopa biljoonia parametreja, kun taas SLM-mallit on suunniteltu tehokkuutta ja edge-k√§ytt√∂√§ silm√§ll√§ pit√§en.

Parametriluokittelun avulla voidaan ymm√§rt√§√§ SLM-mallien eri kategoriat ja niiden sopivat k√§ytt√∂tapaukset. T√§m√§ luokittelu on t√§rke√§ oikean mallin valitsemiseksi tiettyihin edge-laskentaskenaarioihin.

### Parametriluokittelun viitekehys

Parametrirajojen ymm√§rt√§minen auttaa valitsemaan sopivat mallit eri edge-laskentaskenaarioihin:

- **üî¨ Mikro-SLM:t**: 100M - 1,4B parametri√§ (eritt√§in kevyet mobiililaitteille)
- **üì± Pienet SLM:t**: 1,5B - 13,9B parametri√§ (tasapainoinen suorituskyky ja tehokkuus)
- **‚öñÔ∏è Keskikokoiset SLM:t**: 14B - 30B parametri√§ (l√§hell√§ LLM-ominaisuuksia s√§ilytt√§en tehokkuuden)

Tarkka raja on tutkimusyhteis√∂ss√§ joustava, mutta useimmat asiantuntijat pit√§v√§t alle 30 miljardin parametrin malleja "pienin√§", ja jotkut l√§hteet asettavat rajan jopa 10 miljardiin parametriin.

### SLM-mallien keskeiset edut

SLM-mallit tarjoavat useita perusetuja, jotka tekev√§t niist√§ ihanteellisia edge-laskentasovelluksiin:

**Toiminnallinen tehokkuus**: SLM-mallit tarjoavat nopeammat inferenssiajat, koska niiss√§ on v√§hemm√§n parametreja k√§sitelt√§v√§n√§. T√§m√§ tekee niist√§ ihanteellisia reaaliaikaisiin sovelluksiin. Ne vaativat v√§hemm√§n laskentaresursseja, mik√§ mahdollistaa k√§ytt√∂√∂noton resurssirajoitteisilla laitteilla samalla kun ne kuluttavat v√§hemm√§n energiaa ja pienent√§v√§t hiilijalanj√§lke√§.

**Joustava k√§ytt√∂√∂notto**: N√§m√§ mallit mahdollistavat teko√§lyn k√§yt√∂n laitteessa ilman internet-yhteytt√§, parantavat yksityisyytt√§ ja turvallisuutta paikallisen k√§sittelyn avulla, voidaan r√§√§t√§l√∂id√§ toimialakohtaisiin sovelluksiin ja soveltuvat erilaisiin edge-laskentaymp√§rist√∂ihin.

**Kustannustehokkuus**: SLM-mallit tarjoavat kustannustehokkaan koulutuksen ja k√§ytt√∂√∂noton verrattuna LLM-malleihin, pienemm√§t k√§ytt√∂kustannukset ja alhaisemmat kaistanleveysvaatimukset edge-sovelluksissa.

## Edistyneet mallien hankintastrategiat

### Hugging Face -ekosysteemi

Hugging Face toimii ensisijaisena keskuksena huippuluokan SLM-mallien l√∂yt√§miseen ja k√§ytt√∂√∂n:

**Mallien l√∂yt√§misen ominaisuudet**: Alusta tarjoaa kehittyneet suodatusmahdollisuudet parametrim√§√§r√§n, lisenssityypin ja suorituskykymittareiden perusteella. K√§ytt√§j√§t voivat k√§ytt√§√§ rinnakkaisia mallivertailuty√∂kaluja, reaaliaikaisia suorituskykytestej√§ ja arviointituloksia sek√§ WebGPU-demoja v√§litt√∂m√§√§n testaukseen.

**Kuratoidut SLM-kokoelmat**: Suosittuja malleja ovat Phi-4-mini-3.8B kehittyneisiin p√§√§ttelyteht√§viin, Qwen3-sarja (0.6B/1.7B/4B) monikielisiin sovelluksiin, Google Gemma3 tehokkaisiin yleisk√§ytt√∂isiin teht√§viin ja kokeelliset mallit kuten BitNET eritt√§in matalan tarkkuuden k√§ytt√∂√∂nottoon. Alusta sis√§lt√§√§ my√∂s yhteis√∂n luomia kokoelmia, joissa on erikoistuneita malleja tiettyihin toimialoihin sek√§ esikoulutettuja ja ohjeistettuja versioita, jotka on optimoitu eri k√§ytt√∂tarkoituksiin.

### Azure AI Foundry -mallikatalogi

Azure AI Foundry -mallikatalogi tarjoaa yritystason p√§√§syn SLM-malleihin parannetuilla integrointimahdollisuuksilla:

**Yritysintegraatio**: Katalogi sis√§lt√§√§ malleja, joita Azure myy suoraan yritystason tuella ja SLA-sopimuksilla, kuten Phi-4-mini-3.8B kehittyneisiin p√§√§ttelyominaisuuksiin ja Llama 3-8B tuotantok√§ytt√∂√∂n. Se sis√§lt√§√§ my√∂s malleja, kuten Qwen3 8B, luotettavilta kolmannen osapuolen avoimen l√§hdekoodin toimittajilta.

**Yritysedut**: Sis√§√§nrakennetut ty√∂kalut hienos√§√§t√∂√∂n, havainnointiin ja vastuulliseen teko√§lyyn, jotka on integroitu Provisioned Throughput -ominaisuuteen malliperheiden v√§lill√§. Microsoftin suora tuki yritystason SLA-sopimuksilla, integroitu turvallisuus ja vaatimustenmukaisuusominaisuudet sek√§ kattavat k√§ytt√∂√∂noton ty√∂nkulut parantavat yrityskokemusta.

## Edistyneet kvantisointi- ja optimointitekniikat

### Llama.cpp-optimointikehys

Llama.cpp tarjoaa huippuluokan kvantisointitekniikoita maksimaalisen tehokkuuden saavuttamiseksi edge-k√§yt√∂ss√§:

**Kvantisointimenetelm√§t**: Kehys tukee erilaisia kvantisointitasoja, kuten Q4_0 (4-bittinen kvantisointi erinomaisella koon pienennyksell√§ - ihanteellinen Qwen3-0.6B mobiilik√§ytt√∂√∂n), Q5_1 (5-bittinen kvantisointi tasapainottaen laadun ja pakkaamisen - sopiva Phi-4-mini-3.8B edge-inferenssiin) ja Q8_0 (8-bittinen kvantisointi l√§hes alkuper√§isen laadun s√§ilytt√§miseksi - suositeltu Google Gemma3 tuotantok√§ytt√∂√∂n). BitNET edustaa huippua 1-bittisell√§ kvantisoinnilla √§√§rimm√§isiin pakkausskenaarioihin.

**Toteutuksen edut**: CPU-optimoitu inferenssi SIMD-kiihdytyksell√§ tarjoaa muistitehokkaan mallien latauksen ja suorittamisen. Ristiin yhteensopivuus x86-, ARM- ja Apple Silicon -arkkitehtuurien v√§lill√§ mahdollistaa laitteistoriippumattoman k√§ytt√∂√∂noton.

**K√§yt√§nn√∂n toteutusesimerkki**:

```bash
# Clone and build llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release

# Convert Phi-4-mini model from Hugging Face to GGUF format
# First, download the model from Hugging Face
cd ..
python convert.py --outtype f16 --outfile phi-4-mini.gguf /path/to/downloaded/phi-4-mini/model

# Quantize the model to 4-bit precision (Q4_0)
./build/bin/quantize phi-4-mini.gguf phi-4-mini-q4_0.gguf q4_0

# Benchmark the model to check performance
./build/bin/llama-bench -m phi-4-mini-q4_0.gguf -p "Write a function to calculate the Fibonacci sequence"

# Run inference with the quantized model
./build/bin/main -m phi-4-mini-q4_0.gguf -n 512 -p "Explain quantum computing in simple terms"
```

**Muistijalanj√§ljen vertailu**:

```python
# Python script to analyze model size differences
import os
import matplotlib.pyplot as plt
import numpy as np

# Model sizes (in GB)
models = ['Phi-4-mini', 'Qwen3-0.6B', 'Gemma3']
original_sizes = [7.6, 1.2, 4.8]  # F16 format
q4_0_sizes = [2.0, 0.35, 1.3]     # Q4_0 format
q8_0_sizes = [3.9, 0.68, 2.5]     # Q8_0 format

# Calculate reduction percentages
q4_reduction = [(orig - q4) / orig * 100 for orig, q4 in zip(original_sizes, q4_0_sizes)]
q8_reduction = [(orig - q8) / orig * 100 for orig, q8 in zip(original_sizes, q8_0_sizes)]

print("Model Size Reduction:")
for i, model in enumerate(models):
    print(f"{model}: Q4_0 reduces size by {q4_reduction[i]:.1f}%, Q8_0 reduces size by {q8_reduction[i]:.1f}%")

# Memory usage during inference will be approximately:
# - Original F16: ~2x model size
# - Q4_0: ~1.2x model size
# - Q8_0: ~1.5x model size
```

### Microsoft Olive -optimointipaketti

Microsoft Olive tarjoaa kattavat mallien optimointity√∂nkulut, jotka on suunniteltu tuotantoymp√§rist√∂ihin:

**Optimointitekniikat**: Paketti sis√§lt√§√§ dynaamisen kvantisoinnin automaattiseen tarkkuuden valintaan (erityisen tehokas Qwen3-sarjan malleille), graafioptimoinnin ja operaattorifuusion (optimoitu Google Gemma3-arkkitehtuurille), laitteistokohtaiset optimoinnit CPU-, GPU- ja NPU-laitteille (erityistuki Phi-4-mini-3.8B ARM-laitteilla) sek√§ monivaiheiset optimointiputket. BitNET-mallit vaativat erikoistuneita 1-bittisi√§ kvantisointity√∂nkulkuja Olive-kehyksess√§.

**Ty√∂nkulkujen automatisointi**: Automaattinen vertailu optimointivaihtoehtojen v√§lill√§ varmistaa laatumetriikoiden s√§ilymisen optimoinnin aikana. Integrointi suosittuihin ML-kehyksiin, kuten PyTorch ja ONNX, tarjoaa pilvi- ja edge-k√§ytt√∂√∂noton optimointimahdollisuuksia.

**K√§yt√§nn√∂n toteutusesimerkki**:

```python
# Microsoft Olive optimization workflow for SLM
from olive.model import PyTorchModel, ONNXModel
from olive.workflows import run_workflow
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the workflow configuration
def create_olive_config(model_id="microsoft/phi-4-mini-instruct"):
    # Load model and create sample inputs
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
    
    # Create sample inputs for tracing
    sample_text = "Explain the concept of edge computing"
    inputs = tokenizer(sample_text, return_tensors="pt")
    
    # Export to ONNX first
    model_path = f"{model_id.split('/')[-1]}.onnx"
    torch.onnx.export(
        model,
        (inputs["input_ids"],),
        model_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "logits": {0: "batch", 1: "sequence"}
        },
        opset_version=15
    )
    
    # Create Olive optimization config
    config = {
        "input_model": ONNXModel(model_path),
        "systems": {
            "local_system": {
                "type": "LocalSystem"
            }
        },
        "passes": {
            # Graph optimization pass
            "graph_optimization": {
                "type": "OrtTransformersOptimization",
                "config": {
                    "optimization_options": {
                        "enable_gelu": True,
                        "enable_layer_norm": True,
                        "enable_attention": True,
                        "use_multi_head_attention": True
                    }
                }
            },
            # Quantization pass for INT8
            "quantization": {
                "type": "OrtQuantization",
                "config": {
                    "quant_mode": "static",
                    "activation_type": "int8",
                    "weight_type": "int8",
                    "op_types_to_quantize": ["MatMul", "Add", "Conv"]
                },
                "disable_search": True
            }
        },
        "engine": {
            "log_severity_level": 0,
            "cache_dir": "./cache"
        }
    }
    
    return config

# Run the optimization workflow
config = create_olive_config()
result = run_workflow(config)

# Save the optimized model
optimized_model = result.optimized_model
optimized_model.save("./optimized_phi4_mini")

# Benchmark performance comparison
print(f"Original model size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB")
print(f"Optimized model size: {os.path.getsize('./optimized_phi4_mini/model.onnx') / (1024 * 1024):.2f} MB")
```

### Apple MLX -kehys

Apple MLX tarjoaa natiivin optimoinnin, joka on erityisesti suunniteltu Apple Silicon -laitteille:

**Apple Silicon -optimointi**: Kehys hy√∂dynt√§√§ yhten√§ist√§ muistirakennetta Metal Performance Shaders -integraatiolla, automaattista sekatarkkuusinferenssi√§ (erityisen tehokas Google Gemma3-mallille) ja optimoitua muistikaistanleveyden k√§ytt√∂√§. Phi-4-mini-3.8B tarjoaa poikkeuksellista suorituskyky√§ M-sarjan siruilla, kun taas Qwen3-1.7B tarjoaa optimaalisen tasapainon MacBook Air -k√§ytt√∂√∂n.

**Kehitysominaisuudet**: Python- ja Swift-API-tuki NumPy-yhteensopivilla taulukko-operaatioilla, automaattiset differentiaatiokyvyt ja saumaton integrointi Applen kehitysty√∂kaluihin tarjoavat kattavan kehitysymp√§rist√∂n.

**K√§yt√§nn√∂n toteutusesimerkki**:

```python
# Apple MLX optimization for Phi-4-mini model
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from mlx_lm import load, generate

# Install the required packages
# pip install mlx transformers mlx-lm

# Load the Phi-4-mini model with MLX optimization
model_path = "microsoft/phi-4-mini-instruct"
model, tokenizer = load(model_path)

# Convert to float16 for better performance on Apple Silicon
model.convert_to_float16()

# Sample inference
prompt = "Write a function to find prime numbers in Python"
results = generate(
    model, 
    tokenizer,
    prompt=prompt,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
)

print(results[0]["generation"])

# Benchmark the model
import time

def benchmark_inference(model, tokenizer, prompt, runs=10):
    # Warmup
    generate(model, tokenizer, prompt=prompt, max_tokens=128)
    
    # Benchmark
    start_time = time.time()
    for _ in range(runs):
        generate(model, tokenizer, prompt=prompt, max_tokens=128)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / runs
    return avg_time

avg_inference_time = benchmark_inference(model, tokenizer, "Explain quantum computing")
print(f"Average inference time: {avg_inference_time:.4f} seconds")

# Save the optimized model for later use
model.save_weights("phi4_mini_optimized_mlx.npz")
```

## Tuotantok√§ytt√∂ ja inferenssistrategiat

### Ollama: Yksinkertaistettu paikallinen k√§ytt√∂√∂notto

Ollama yksinkertaistaa SLM-mallien k√§ytt√∂√∂nottoa yritysvalmiilla ominaisuuksilla paikallisissa ja edge-ymp√§rist√∂iss√§:

**K√§ytt√∂√∂notto-ominaisuudet**: Yhden komennon malliasennus ja suoritus automaattisella mallien latauksella ja v√§limuistilla. Tuki Phi-4-mini-3.8B:lle, koko Qwen3-sarjalle (0.6B/1.7B/4B) ja Google Gemma3:lle REST-API-sovellusintegraatiolla sek√§ monimallien hallinta- ja vaihtomahdollisuuksilla. BitNET-mallit vaativat kokeellisia rakennuskonfiguraatioita 1-bittisen kvantisoinnin tueksi.

**Edistyneet ominaisuudet**: R√§√§t√§l√∂ityjen mallien hienos√§√§t√∂, Dockerfile-luonti konttipohjaiseen k√§ytt√∂√∂nottoon, GPU-kiihdytys automaattisella tunnistuksella sek√§ mallien kvantisointi- ja optimointivaihtoehdot tarjoavat kattavan k√§ytt√∂√∂noton joustavuuden.

### VLLM: Korkean suorituskyvyn inferenssi

VLLM tarjoaa tuotantotason inferenssioptimointia korkean l√§pimenon skenaarioihin:

**Suorituskykyoptimoinnit**: PagedAttention muistitehokkaaseen huomion laskentaan (erityisen hy√∂dyllinen Phi-4-mini-3.8B:n transformer-arkkitehtuurille), dynaaminen er√§prosessi l√§pimenon optimointiin (optimoitu Qwen3-sarjan rinnakkaisk√§sittelyyn), tensoriparallellisuus monen GPU:n skaalaamiseen (Google Gemma3-tuki) ja spekulatiivinen dekoodaus viiveen v√§hent√§miseksi. BitNET-mallit vaativat erikoistuneita inferenssiytimi√§ 1-bittisiin operaatioihin.

**Yritysintegraatio**: OpenAI-yhteensopivat API-p√§√§tepisteet, Kubernetes-k√§ytt√∂√∂noton tuki, monitorointi- ja havainnointiintegraatio sek√§ automaattiset skaalausominaisuudet tarjoavat yritystason k√§ytt√∂√∂noton ratkaisuja.

### Foundry Local: Microsoftin edge-ratkaisu

Foundry Local tarjoaa kattavat edge-k√§ytt√∂√∂noton ominaisuudet yritysymp√§rist√∂ihin:

**Edge-laskennan ominaisuudet**: Offline-ensimm√§inen arkkitehtuurisuunnittelu resurssirajoitteiden optimoinnilla, paikallinen mallirekisterin hallinta ja edge-pilvi-synkronointiominaisuudet varmistavat luotettavan edge-k√§ytt√∂√∂noton.

**Turvallisuus ja vaatimustenmukaisuus**: Paikallinen datank√§sittely yksityisyyden suojaamiseksi, yritystason turvallisuusvalvonta, auditointilokit ja vaatimustenmukaisuusraportointi sek√§ roolipohjainen p√§√§synhallinta tarjoavat kattavan turvallisuuden edge-k√§ytt√∂√∂notossa.

## Parhaat k√§yt√§nn√∂t SLM-mallien toteutuksessa

### Mallin valintaperiaatteet

Kun valitset SLM-malleja edge-k√§ytt√∂√∂n, ota huomioon seuraavat tekij√§t:

**Parametrim√§√§r√§n huomioiminen**: Valitse mikro-SLM:t, kuten Qwen3-0.6B, eritt√§in kevyisiin mobiilisovelluksiin, pienet SLM:t, kuten Qwen3-1.7B tai Google Gemma3, tasapainoisiin suorituskykyskenaarioihin, ja keskikokoiset SLM:t, kuten Phi-4-mini-3.8B tai Qwen3-4B, kun l√§hestyt√§√§n LLM-ominaisuuksia s√§ilytt√§en tehokkuuden. BitNET-mallit tarjoavat kokeellista ultra-pakkausta erityisiin tutkimussovelluksiin.

**K√§ytt√∂tapauksen mukauttaminen**: Sovita mallin ominaisuudet tiettyihin sovellusvaatimuksiin, huomioiden tekij√§t kuten vastausten laatu, inferenssinopeus, muistirajoitukset ja offline-toiminnan tarpeet.

### Optimointistrategian valinta

**Kvantisointimenetelm√§**: Valitse sopivat kvantisointitasot laadunvaatimusten ja laitteistorajoitusten perusteella. Harkitse Q4_0 maksimaaliseen pakkaukseen (ihanteellinen Qwen3-0.6B mobiilik√§ytt√∂√∂n), Q5_1 tasapainoiseen laatu-pakkausvaihtokauppaan (sopiva Phi-4-mini-3.8B ja Google Gemma3) ja Q8_0 l√§hes alkuper√§isen laadun s√§ilytt√§miseen (suositeltu Qwen3-4B tuotantoymp√§rist√∂ihin). BitNET:n 1-bittinen kvantisointi edustaa √§√§rimm√§ist√§ pakkausrajaa erikoistuneisiin sovelluksiin.

**Kehyksen valinta**: Valitse optimointikehykset kohdelaitteiston ja k√§ytt√∂√∂noton vaatimusten perusteella. K√§yt√§ Llama.cpp:t√§ CPU-optimoituun k√§ytt√∂√∂nottoon, Microsoft Olivea kattaviin optimointity√∂nkulkuihin ja Apple MLX:√§√§ Apple Silicon -laitteille.

## K√§yt√§nn√∂n malliesimerkit ja k√§ytt√∂tapaukset

### Todelliset k√§ytt√∂skenaariot

**Mobiilisovellukset**: Qwen3-0.6B loistaa √§lypuhelinten chatbot-sovelluksissa, joissa on minimaalinen muistijalanj√§lki, kun taas Google Gemma3 tarjoaa tasapainoista suorituskyky√§ tablettipohjaisiin opetusv√§lineisiin. Phi-4-mini-3.8B tarjoaa erinomaisia p√§√§ttelyominaisuuksia mobiilituottavuussovelluksiin.

**P√∂yt√§koneet ja edge-laskenta**: Qwen3-1.7B tarjoaa optimaalista suorituskyky√§ ty√∂p√∂yt√§avustajasovelluksiin, Phi-4-mini-3.8B tarjoaa kehittyneit√§ koodinluontiominaisuuksia kehitt√§j√§ty√∂kaluihin, ja Qwen3-4B mahdollistaa monimutkaisen asiakirja-analyysin ty√∂asemaymp√§rist√∂iss√§.

**Tutkimus ja kokeilu**: BitNET-mallit mahdollistavat ultra-matalan tarkkuuden inferenssin tutkimusk√§ytt√∂√∂n ja konseptitodistussovelluksiin, joissa on √§√§rimm√§iset resurssirajoitukset.

### Suorituskykyvertailut ja -arviot

**Inferenssinopeus**: Qwen3-0.6B saavuttaa nopeimmat inferenssiajat mobiili-CPU:illa, Google Gemma3 tarjoaa tasapainoisen nopeus-laatusuhteen yleisiin sovelluksiin

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.