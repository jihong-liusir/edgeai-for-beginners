<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "eb6ccbc99954b9db058c3fabdbf39cc5",
  "translation_date": "2025-09-22T20:22:41+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fi"
}
-->
# Istunto 3: Avoimen lähdekoodin mallit Foundry Localin kanssa

## Yleiskatsaus

Tässä istunnossa tutkitaan, miten avoimen lähdekoodin malleja tuodaan Foundry Localiin: yhteisön mallien valinta, Hugging Face -sisällön integrointi ja "tuo oma mallisi" (BYOM) -strategioiden käyttöönotto. Lisäksi tutustut Model Mondays -sarjaan jatkuvaa oppimista ja mallien löytämistä varten.

Viitteet:
- Foundry Local -dokumentaatio: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
- Hugging Face -mallien kääntäminen: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Model Mondays: https://aka.ms/model-mondays
- Foundry Local GitHub: https://github.com/microsoft/Foundry-Local

## Oppimistavoitteet
- Löydä ja arvioi avoimen lähdekoodin malleja paikallista käyttöä varten
- Käännä ja suorita valittuja Hugging Face -malleja Foundry Localissa
- Sovella mallin valintastrategioita tarkkuuden, viiveen ja resurssitarpeiden perusteella
- Hallitse malleja paikallisesti välimuistin ja versioinnin avulla

## Osa 1: Mallien löytäminen ja valinta (askel askeleelta)

Vaihe 1) Listaa saatavilla olevat mallit paikallisessa katalogissa  
```cmd
foundry model list
```
  
Vaihe 2) Kokeile nopeasti kahta ehdokasta (lataa automaattisesti ensimmäisellä käynnistyskerralla)  
```cmd
foundry model run phi-4-mini
foundry model run qwen2.5-7b-instruct
```
  
Vaihe 3) Huomioi perusmittarit  
- Tarkkaile viivettä (subjektiivinen) ja laatua kiinteällä kehotteella  
- Seuraa muistin käyttöä Task Managerin avulla mallien suorittamisen aikana  

## Osa 2: Katalogimallien suorittaminen CLI:n kautta (askel askeleelta)

Vaihe 1) Käynnistä malli  
```cmd
foundry model run llama-3.2
```
  
Vaihe 2) Lähetä testikehote OpenAI-yhteensopivan päätepisteen kautta  
```cmd
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"llama-3.2\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}]}"

```
  

## Osa 3: BYOM – Hugging Face -mallien kääntäminen (askel askeleelta)

Noudata virallista ohjetta mallien kääntämiseen. Alla korkean tason kulku – tarkat komennot ja tuetut kokoonpanot löytyvät Microsoft Learn -artikkelista.

Vaihe 1) Valmistele työskentelyhakemisto  
```cmd
mkdir models
foundry cache cd models
foundry cache ls
```
  
Vaihe 2) Käännä tuettu HF-malli  
- Käytä Learn-dokumentin ohjeita ONNX-mallin muuntamiseen ja sijoittamiseen `models`-hakemistoon  
- Vahvista seuraavalla komennolla:  
```cmd
foundry cache ls
```
  
Näet käännetyn mallin nimen (esimerkiksi `llama-3.2`).  

Vaihe 3) Suorita käännetty malli  
```cmd
foundry model run llama-3.2 --verbose
```
  
Huomioita:  
- Varmista riittävä levytila ja RAM kääntämistä ja suorittamista varten  
- Aloita pienemmistä malleista prosessin validointia varten, ja laajenna sitten suurempiin  

## Osa 4: Käytännön mallien kuratointi (askel askeleelta)

Vaihe 1) Luo `models.json`-rekisteri  
```json
[
    {"name":"phi-4-mini","task":"chat","min_ram_gb":8,"notes":"fast, great for general chat"},
  {"name":"qwen2.5-7b-instruct","task":"chat","min_ram_gb":16,"notes":"larger context, good reasoning"},
  {"name":"deepseek-r1-distill-qwen-7b","task":"code","min_ram_gb":16,"notes":"coding-oriented"}
]
```
  
Vaihe 2) Pieni valintaskripti  
```python
# select_model.py
import json, psutil

with open('models.json','r',encoding='utf-8') as f:
    candidates = json.load(f)

ram_gb = psutil.virtual_memory().total / (1024**3)

def select(task: str):
    viable = [m for m in candidates if m['task']==task and ram_gb>=m['min_ram_gb']]
    return viable[0]['name'] if viable else None

print('Selected for chat:', select('chat'))
print('Selected for code:', select('code'))
```
  

## Osa 5: Käytännön vertailut (askel askeleelta)

Vaihe 1) Yksinkertainen viivevertailu  
```python
# bench_latency.py
import time, requests

BASE = "http://localhost:8000"

def bench(model, prompt="Explain edge AI in one sentence."):
    start = time.time()
    r = requests.post(f"{BASE}/v1/completions", json={
        "model": model,
        "prompt": prompt,
        "max_tokens": 64
    }, timeout=60)
    elapsed = time.time() - start
    r.raise_for_status()
    return {"model": model, "latency_sec": round(elapsed,3)}

for m in ["phi-4-mini","qwen2.5-7b-instruct","deepseek-r1-distill-qwen-7b"]:
    print(bench(m))
```
  
Vaihe 2) Laadun tarkistus  
- Käytä kiinteää kehotesarjaa, tallenna tulokset CSV/JSON-muotoon  
- Arvioi manuaalisesti sujuvuus, relevanssi ja oikeellisuus (1–5)  

## Osa 6: Seuraavat askeleet
- Tilaa Model Mondays saadaksesi uusia malleja ja vinkkejä: https://aka.ms/model-mondays  
- Jaa löydökset tiimisi `models.json`-rekisteriin  
- Valmistaudu istuntoon 4: LLM- ja SLM-mallien vertailu, paikallinen vs pilvipohjainen käyttö ja käytännön demot  

---

