<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b1b09b5c867abbccfdbc826d857ae0c2",
  "translation_date": "2025-09-24T23:29:40+00:00",
  "source_file": "Module08/03.OpenSourceModels.md",
  "language_code": "fi"
}
-->
# Istunto 3: Avoimen lähdekoodin mallien löytäminen ja hallinta

## Yleiskatsaus

Tässä istunnossa keskitytään käytännön mallien löytämiseen ja hallintaan Foundry Localin avulla. Opit listaamaan saatavilla olevat mallit, testaamaan eri vaihtoehtoja ja ymmärtämään perussuorituskykyominaisuuksia. Lähestymistapa painottaa käytännön tutkimista Foundry CLI:n avulla, jotta voit valita oikeat mallit käyttötarkoituksiisi.

## Oppimistavoitteet

- Hallitse Foundry CLI -komennot mallien löytämiseen ja hallintaan
- Ymmärrä mallien välimuisti ja paikallisen tallennuksen toimintamallit
- Opettele testaamaan ja vertailemaan malleja nopeasti
- Luo käytännön työnkulkuja mallien valintaan ja vertailuun
- Tutustu Foundry Localin kasvavaan malliekosysteemiin

## Esivaatimukset

- Suoritettu Istunto 1: Foundry Localin aloitus
- Foundry Local CLI asennettuna ja käytettävissä
- Riittävästi tallennustilaa mallien lataamiseen (mallien koko voi vaihdella 1GB:sta yli 20GB:iin)
- Perustiedot mallityypeistä ja käyttötarkoituksista

## Osa 6: Käytännön harjoitus

### Harjoitus: Mallien löytäminen ja vertailu

Luo oma mallien arviointiskripti Sample 03:n pohjalta:

```cmd
REM create_model_test.cmd
@echo off
echo Model Discovery and Testing Script
echo =====================================

echo.
echo Step 1: List available models
foundry model list

echo.
echo Step 2: Check what's cached
foundry cache list

echo.
echo Step 3: Start phi-4-mini for testing
foundry model run phi-4-mini --verbose

echo.
echo Step 4: Test with a simple prompt
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello, please introduce yourself.\"}],\"max_tokens\":100}"

echo.
echo Model test complete!
```

### Tehtäväsi

1. **Aja Sample 03 -skripti**: `samples\03\list_and_bench.cmd`
2. **Kokeile eri malleja**: Testaa vähintään 3 eri mallia
3. **Vertaa suorituskykyä**: Huomioi erot nopeudessa ja vastausten laadussa
4. **Dokumentoi havainnot**: Luo yksinkertainen vertailutaulukko

### Esimerkki vertailumuodosta

```
Model Comparison Results:
========================
phi-4-mini:        Fast (~2s), good for general chat
qwen2.5-7b:       Slower (~5s), better reasoning  
deepseek-r1:      Medium (~3s), excellent for code

Recommendation: Start with phi-4-mini for development, 
switch to qwen2.5-7b for production reasoning tasks.
```

## Osa 7: Vianetsintä ja parhaat käytännöt

### Yleiset ongelmat ja ratkaisut

**Malli ei käynnisty:**
```cmd
REM Check service status
foundry service status

REM Restart service if needed
foundry service stop
foundry service start

REM Try with verbose output
foundry model run phi-4-mini --verbose
```

**Riittämätön muisti:**
- Aloita pienemmillä malleilla (`phi-4-mini`)
- Sulje muut sovellukset
- Päivitä RAM-muistia, jos törmäät usein rajoituksiin

**Hidas suorituskyky:**
- Varmista, että malli on täysin ladattu (tarkista yksityiskohtainen lähtö)
- Sulje tarpeettomat taustasovellukset
- Harkitse nopeampaa tallennusta (SSD)

### Parhaat käytännöt

1. **Aloita pienestä**: Käytä `phi-4-mini` varmistaaksesi asetusten toimivuuden
2. **Yksi malli kerrallaan**: Lopeta edellinen malli ennen uuden käynnistämistä
3. **Seuraa resursseja**: Tarkkaile muistin käyttöä
4. **Testaa johdonmukaisesti**: Käytä samoja kehotteita reiluun vertailuun
5. **Dokumentoi tulokset**: Pidä kirjaa mallien suorituskyvystä käyttötarkoituksiasi varten

## Osa 8: Seuraavat askeleet ja viitteet

### Valmistautuminen Istuntoon 4

- **Istunto 4:n aihe**: Optimointityökalut ja -tekniikat
- **Esivaatimukset**: Mukavuus mallien vaihtamisessa ja perussuorituskyvyn testaamisessa
- **Suositeltavaa**: Valitse 2-3 suosikkimallia tästä istunnosta

### Lisäresurssit

- **[Foundry Local Dokumentaatio](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: Virallinen dokumentaatio
- **[CLI Viite](https://learn.microsoft.com/azure/ai-foundry/foundry-local/reference/reference-cli)**: Täydellinen komentoviite
- **[Model Mondays](https://aka.ms/model-mondays)**: Viikoittaiset malliesittelyt
- **[Foundry Local GitHub](https://github.com/microsoft/Foundry-Local)**: Yhteisö ja ongelmat
- **[Sample 03: Model Discovery](samples/03/README.md)**: Käytännön esimerkkiskripti

### Keskeiset opit

✅ **Mallien löytäminen**: Käytä `foundry model list` tutkiaksesi saatavilla olevia malleja  
✅ **Nopea testaus**: `list_and_bench.cmd` -malli nopeaan arviointiin  
✅ **Suorituskyvyn seuranta**: Perusresurssien käyttö ja vasteajan mittaus  
✅ **Mallien valinta**: Käytännön ohjeet mallien valintaan käyttötarkoituksen mukaan  
✅ **Välimuistin hallinta**: Tallennuksen ja siivousmenetelmien ymmärtäminen  

Nyt sinulla on käytännön taidot löytää, testata ja valita sopivia malleja AI-sovelluksiisi Foundry Localin yksinkertaisen CLI-lähestymistavan avulla.

## Oppimistavoitteet
- Löydä ja arvioi avoimen lähdekoodin malleja paikallista käyttöä varten
- Käännä ja aja valittuja Hugging Face -malleja Foundry Localissa
- Sovella mallien valintastrategioita tarkkuuden, viiveen ja resurssitarpeiden mukaan
- Hallitse malleja paikallisesti välimuistin ja versioinnin avulla

## Osa 1: Mallien löytäminen Foundry CLI:llä

### Peruskomennot mallien hallintaan

Foundry CLI tarjoaa yksinkertaiset komennot mallien löytämiseen ja hallintaan:

```cmd
REM List all available models in the catalog
foundry model list

REM List cached (downloaded) models
foundry cache list

REM Check cache directory location
foundry cache ls
```

### Ensimmäisten mallien käynnistäminen

Aloita suosituilla, hyvin testatuilla malleilla ymmärtääksesi suorituskykyominaisuuksia:

```cmd
REM Run Phi-4-Mini (lightweight, fast)
foundry model run phi-4-mini --verbose

REM Run Qwen 2.5 7B (larger, more capable)
foundry model run qwen2.5-7b-instruct --verbose

REM Run DeepSeek (specialized for coding)
foundry model run deepseek-r1-distill-qwen-7b --verbose
```

**Huom:** `--verbose` -lippu tarjoaa yksityiskohtaista käynnistystietoa, mukaan lukien:
- Mallin latauksen eteneminen (ensimmäisellä käynnistyskerralla)
- Muistin allokointitiedot
- Palvelun sidontatiedot
- Suorituskyvyn alustamittarit

### Mallikategoriat

**Pienet kielimallit (SLM):**
- `phi-4-mini`: Nopea, tehokas, erinomainen yleiseen keskusteluun
- `phi-4`: Kyvykkäämpi versio paremmalla päättelyllä

**Keskikokoiset mallit:**
- `qwen2.5-7b-instruct`: Erinomainen päättely ja pidempi konteksti
- `deepseek-r1-distill-qwen-7b`: Optimoitu koodin generointiin

**Suuret mallit:**
- `llama-3.2`: Metan uusin avoimen lähdekoodin malli
- `qwen2.5-14b-instruct`: Yritystason päättely

## Osa 2: Nopea mallien testaus ja vertailu

### Sample 03 -lähestymistapa: Yksinkertainen lista ja vertailu

Sample 03 -mallin pohjalta tässä on minimaalinen työnkulku:

```cmd
@echo off
REM Sample 03 - List and bench pattern
echo Listing available models...
foundry model list

echo.
echo Checking cached models...
foundry cache list

echo.
echo Starting phi-4-mini with verbose output...
foundry model run phi-4-mini --verbose
```

### Mallien suorituskyvyn testaus

Kun malli on käynnissä, testaa sitä johdonmukaisilla kehotteilla:

```cmd
REM Test via curl (Windows Command Prompt)
curl -X POST http://localhost:8000/v1/chat/completions ^
  -H "Content-Type: application/json" ^
  -d "{\"model\":\"phi-4-mini\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain edge AI in one sentence.\"}],\"max_tokens\":50}"
```

### PowerShell-testauksen vaihtoehto

```powershell
# PowerShell approach for testing
$body = @{
    model = "phi-4-mini"
    messages = @(
        @{
            role = "user"
            content = "Explain edge AI in one sentence."
        }
    )
    max_tokens = 50
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "http://localhost:8000/v1/chat/completions" -Method Post -Body $body -ContentType "application/json"
```

## Osa 3: Mallien välimuisti ja tallennuksen hallinta

### Mallivälimuistin ymmärtäminen

Foundry Local hallinnoi automaattisesti mallien latauksia ja välimuistia:

```cmd
REM Check cache directory and contents
foundry cache ls

REM View cache location
foundry cache cd

REM Clean up unused models (if needed)
foundry cache clean
```

### Mallien tallennuksen huomioiminen

**Tyypilliset mallikoot:**
- `phi-4-mini`: ~2.5 GB
- `qwen2.5-7b-instruct`: ~4.1 GB  
- `deepseek-r1-distill-qwen-7b`: ~4.3 GB
- `llama-3.2`: ~4.9 GB
- `qwen2.5-14b-instruct`: ~8.2 GB

**Tallennuksen parhaat käytännöt:**
- Pidä 2-3 mallia välimuistissa nopeaa vaihtoa varten
- Poista käyttämättömät mallit vapauttaaksesi tilaa: `foundry cache clean`
- Seuraa levyn käyttöä, erityisesti pienemmillä SSD-levyillä
- Harkitse mallin koon ja kyvykkyyden kompromisseja

### Mallien suorituskyvyn seuranta

Kun mallit ovat käynnissä, seuraa järjestelmän resursseja:

**Windowsin Tehtävienhallinta:**
- Tarkkaile muistin käyttöä (mallit pysyvät ladattuina RAM-muistissa)
- Seuraa CPU:n käyttöä inferenssin aikana
- Tarkista levyn I/O mallin alkuperäisen latauksen aikana

**Komentorivin seuranta:**
```cmd
REM Check memory usage (PowerShell)
Get-Process | Where-Object {$_.ProcessName -like "*foundry*"} | Select-Object ProcessName, WorkingSet64

REM Monitor running models
foundry service ps
```

## Osa 4: Käytännön ohjeet mallien valintaan

### Mallien valinta käyttötarkoituksen mukaan

**Yleiseen keskusteluun ja kysymys-vastaus -tilanteisiin:**
- Aloita: `phi-4-mini` (nopea, tehokas)
- Päivitä: `phi-4` (parempi päättely)
- Edistynyt: `qwen2.5-7b-instruct` (pidempi konteksti)

**Koodin generointiin:**
- Suositeltu: `deepseek-r1-distill-qwen-7b`
- Vaihtoehto: `qwen2.5-7b-instruct` (hyvä myös koodiin)

**Monimutkaiseen päättelyyn:**
- Paras: `qwen2.5-7b-instruct` tai `qwen2.5-14b-instruct`
- Edullinen vaihtoehto: `phi-4`

### Laitteistovaatimusten opas

**Minimivaatimukset:**
```
phi-4-mini:     8GB RAM,  entry-level CPU
phi-4:         12GB RAM,  mid-range CPU
qwen2.5-7b:    16GB RAM,  mid-range CPU
deepseek-r1:   16GB RAM,  mid-range CPU
qwen2.5-14b:   24GB RAM,  high-end CPU
```

**Suositeltu paras suorituskyky:**
- 32GB+ RAM mukavaan monimallien vaihtoon
- SSD-tallennus nopeampaan mallien lataukseen
- Moderni CPU hyvällä yksisäikeisellä suorituskyvyllä
- NPU-tuki (Windows 11 Copilot+ PC:t) kiihdytykseen

### Mallien vaihtotyönkulku

```cmd
REM Stop current model (if needed)
foundry service stop

REM Start different model
foundry model run qwen2.5-7b-instruct

REM Verify model is running
foundry service status
```

## Osa 5: Yksinkertainen mallien vertailu

### Perussuorituskyvyn testaus

Tässä on yksinkertainen lähestymistapa mallien suorituskyvyn vertailuun:

```python
# simple_bench.py - Based on Sample 03 patterns
import time
import requests
import json

def test_model_response(model_name, prompt="Explain edge AI in one sentence."):
    """Test a single model with a prompt and measure response time."""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 64
            },
            timeout=30
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            return {
                "model": model_name,
                "latency_sec": round(elapsed, 3),
                "response": result["choices"][0]["message"]["content"],
                "status": "success"
            }
        else:
            return {
                "model": model_name,
                "status": "error",
                "error": f"HTTP {response.status_code}"
            }
            
    except Exception as e:
        return {
            "model": model_name,
            "status": "error", 
            "error": str(e)
        }

# Test the currently running model
if __name__ == "__main__":
    # Test with different models (start each model first)
    test_models = ["phi-4-mini", "qwen2.5-7b-instruct", "deepseek-r1-distill-qwen-7b"]
    
    print("Model Performance Test")
    print("=" * 50)
    
    for model in test_models:
        print(f"\nTesting {model}...")
        print("Note: Make sure this model is running first with 'foundry model run {model}'")
        
        result = test_model_response(model)
        
        if result["status"] == "success":
            print(f"✅ {model}: {result['latency_sec']}s")
            print(f"   Response: {result['response'][:100]}...")
        else:
            print(f"❌ {model}: {result['error']}")
```

### Manuaalinen laadun arviointi

Testaa kutakin mallia johdonmukaisilla kehotteilla ja arvioi manuaalisesti:

**Testikehotteet:**
1. "Selitä kvanttitietokone yksinkertaisesti."
2. "Kirjoita Python-funktio listan lajitteluun."
3. "Mitkä ovat etätyön hyvät ja huonot puolet?"
4. "Tiivistä reunalaskennan edut."

**Arviointikriteerit:**
- **Tarkkuus**: Onko tieto oikein?
- **Selkeys**: Onko selitys helppo ymmärtää?
- **Täydellisyys**: Käsitteleekö se koko kysymyksen?
- **Nopeus**: Kuinka nopeasti se vastaa?

### Resurssien käytön seuranta

```cmd
REM Monitor while testing different models
REM Start model
foundry model run phi-4-mini

REM In another terminal, monitor resources
foundry service status
foundry service ps

REM Check system resources (PowerShell)
Get-Process | Where-Object ProcessName -Like "*foundry*" | Format-Table ProcessName, WorkingSet64, CPU
```

## Osa 6: Seuraavat askeleet
- Tilaa Model Mondays saadaksesi uusia malleja ja vinkkejä: https://aka.ms/model-mondays
- Jaa havainnot tiimisi `models.json`-tiedostoon
- Valmistaudu Istuntoon 4: LLM- ja SLM-mallien vertailu, paikallinen vs. pilvilaskenta ja käytännön demot

---

