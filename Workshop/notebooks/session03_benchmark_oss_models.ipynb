{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91644746",
   "metadata": {},
   "source": [
    "# Session 3 – Benchmark Open-Source Models\n",
    "\n",
    "Benchmark latency & approximate tokens/sec for multiple model aliases via Foundry Local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe2c2d",
   "metadata": {},
   "source": [
    "### Explanation: Dependency Installation\n",
    "Installs minimal packages for benchmarking:\n",
    "- `foundry-local-sdk` for managing/attaching to local models.\n",
    "- `openai` for a simple chat completion client.\n",
    "- `numpy` (future extension or vector ops if needed).\n",
    "Idempotent; safe to re-run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5751c",
   "metadata": {},
   "source": [
    "# Scenario\n",
    "This benchmark notebook measures latency and approximate throughput (tokens/sec) for one or more locally hosted open‑source model aliases via Foundry Local. It:\n",
    "- Discovers available model IDs (or respects BENCH_MODELS env override).\n",
    "- Warms each model once to mitigate first-token cold start.\n",
    "- Executes multiple chat completion rounds per model and aggregates latency + token usage.\n",
    "- Outputs JSON plus a Markdown-friendly summary table.\n",
    "\n",
    "Use this to compare small language model trade-offs (speed vs. capability) before integrating routing or cost heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q foundry-local-sdk openai numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38238d54",
   "metadata": {},
   "source": [
    "### Explanation: Preview Model Discovery\n",
    "Performs an early lightweight model listing to give immediate feedback on which models are currently loaded before we refine configuration. Non-fatal if service not yet started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, statistics, json\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "\n",
    "# Early model discovery preview (optional)\n",
    "# This gives users immediate feedback on what Foundry Local currently has loaded\n",
    "# before the configuration/discovery cell refines the model list.\n",
    "try:\n",
    "    _preview_base = os.getenv('BASE_URL','http://127.0.0.1:57127/v1')\n",
    "    _preview_key = os.getenv('API_KEY','not-needed')\n",
    "    _preview_client = OpenAI(base_url=_preview_base, api_key=_preview_key)\n",
    "    _preview_models = [m.id for m in _preview_client.models.list().data]\n",
    "    if _preview_models:\n",
    "        print(f\"Foundry Local models discovered (preview): {_preview_models}\")\n",
    "    else:\n",
    "        print(f\"No models discovered yet at {_preview_base}. Load models before benchmarking (e.g., 'foundry model run phi-4-mini').\")\n",
    "except Exception as _e:\n",
    "    print(f\"Model preview unavailable: {_e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7df483",
   "metadata": {},
   "source": [
    "### Explanation: Benchmark Configuration & Model Filtering\n",
    "Sets environment-driven benchmarking parameters (rounds, prompt, generation settings). Discovers models, optionally filters to requested list, and prints the final set to benchmark with basic validation and helpful warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark configuration & model discovery (override via environment variables)\n",
    "BASE_URL = os.getenv('BASE_URL',' http://127.0.0.1:57127/v1')\n",
    "API_KEY = os.getenv('API_KEY','not-needed')\n",
    "\n",
    "_raw_models = os.getenv('BENCH_MODELS','').strip()\n",
    "requested_models = [m.strip() for m in _raw_models.split(',') if m.strip()] if _raw_models else []\n",
    "\n",
    "ROUNDS = int(os.getenv('BENCH_ROUNDS','3'))\n",
    "if ROUNDS < 1:\n",
    "    raise ValueError('BENCH_ROUNDS must be >= 1')\n",
    "PROMPT = os.getenv('BENCH_PROMPT','Explain retrieval augmented generation briefly.')\n",
    "MAX_TOKENS = int(os.getenv('BENCH_MAX_TOKENS','120'))\n",
    "TEMPERATURE = float(os.getenv('BENCH_TEMPERATURE','0.2'))\n",
    "\n",
    "def _discover_models():\n",
    "    try:\n",
    "        c = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "        data = c.models.list().data\n",
    "        return [m.id for m in data]\n",
    "    except Exception as e:\n",
    "        print(f\"Model discovery failed: {e}\")\n",
    "        return []\n",
    "\n",
    "_discovered = _discover_models()\n",
    "if not _discovered:\n",
    "    print(\"Warning: No models discovered at BASE_URL. Ensure Foundry Local is running and models are loaded.\")\n",
    "\n",
    "if not requested_models or requested_models == ['auto'] or 'ALL' in requested_models:\n",
    "    MODELS = _discovered\n",
    "else:\n",
    "    # Filter requested models to those actually discovered\n",
    "    MODELS = [m for m in requested_models if m in _discovered] or requested_models  # fallback to requested even if not discovered\n",
    "    missing = [m for m in requested_models if m not in _discovered]\n",
    "    if missing:\n",
    "        print(f\"Notice: The following requested models were not discovered and may fail during benchmarking: {missing}\")\n",
    "\n",
    "MODELS = [m for m in MODELS if m]\n",
    "if not MODELS:\n",
    "    raise ValueError(\"No models available to benchmark. Start a model (e.g., 'foundry model run phi-4-mini') or set BENCH_MODELS.\")\n",
    "\n",
    "print(f\"Benchmarking models: {MODELS}\\nRounds: {ROUNDS}  Max Tokens: {MAX_TOKENS}  Temp: {TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92895bf4",
   "metadata": {},
   "source": [
    "### Explanation: Model Access Helper\n",
    "`ensure_loaded(alias)` attaches to Foundry Local via manager, resolves concrete model id, and returns both manager + OpenAI client. Raises a helpful error if the model isn't available so the benchmark loop can skip gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_loaded(alias):\n",
    "    \"\"\"Return (manager, client, model_id) ensuring the alias is accessible.\n",
    "    Raises RuntimeError with guidance if the model cannot be accessed.\"\"\"\n",
    "    try:\n",
    "        m = FoundryLocalManager(alias)  # triggers bootstrap / attaches to existing\n",
    "        info = m.get_model_info(alias)\n",
    "        model_id = getattr(info, 'id', alias)\n",
    "        c = OpenAI(base_url=m.endpoint, api_key=m.api_key or 'not-needed')\n",
    "        return m, c, model_id\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to access model '{alias}'. Ensure it is loaded in Foundry Local, or check BENCH_MODELS. Original error: {e}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2944316",
   "metadata": {},
   "source": [
    "### Explanation: Single Round Execution\n",
    "`run_round` performs one chat completion and returns latency + token usage fields if the backend provides them. This atomic unit powers warmup and statistical aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_round(client, model_id):\n",
    "    start = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{'role':'user','content':PROMPT}],\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    end = time.time()\n",
    "    usage = getattr(resp, 'usage', None)\n",
    "    total_tokens = getattr(usage, 'total_tokens', None) if usage else None\n",
    "    prompt_tokens = getattr(usage, 'prompt_tokens', None) if usage else None\n",
    "    completion_tokens = getattr(usage, 'completion_tokens', None) if usage else None\n",
    "    return end-start, total_tokens, prompt_tokens, completion_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d60c14",
   "metadata": {},
   "source": [
    "### Explanation: Benchmark Loop & Aggregation\n",
    "Iterates each model:\n",
    "- Warmup (excluded from stats) to mitigate cold start.\n",
    "- Multiple measured rounds capturing latency + tokens.\n",
    "- Aggregates mean, p95, and tokens/sec.\n",
    "Stores per-model summary dicts for later rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31f53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round failed for gpt-oss-20b: Connection error.\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for alias in MODELS:\n",
    "    try:\n",
    "        m, client, model_id = ensure_loaded(alias.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    # Warmup (not recorded)\n",
    "    try:\n",
    "        run_round(client, model_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Warmup failed for {alias}: {e}\")\n",
    "        continue\n",
    "\n",
    "    latencies, tps = [], []\n",
    "    prompt_tokens_total = 0\n",
    "    completion_tokens_total = 0\n",
    "    token_rounds = 0\n",
    "\n",
    "    for _ in range(ROUNDS):\n",
    "        try:\n",
    "            latency, total_tokens, p_tokens, c_tokens = run_round(client, model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Round failed for {alias}: {e}\")\n",
    "            continue\n",
    "        latencies.append(latency)\n",
    "        if total_tokens:\n",
    "            tps.append(total_tokens/latency)\n",
    "        if p_tokens is not None:\n",
    "            prompt_tokens_total += p_tokens\n",
    "        if c_tokens is not None:\n",
    "            completion_tokens_total += c_tokens\n",
    "            token_rounds += 1\n",
    "\n",
    "    if not latencies:\n",
    "        print(f\"Skipping {alias}: no successful rounds.\")\n",
    "        continue\n",
    "\n",
    "    latency_avg = statistics.mean(latencies)\n",
    "    latency_p95 = statistics.quantiles(latencies, n=20)[-1] if len(latencies) > 1 else latencies[0]\n",
    "    tokens_per_sec_avg = statistics.mean(tps) if tps else None\n",
    "\n",
    "    summary.append({\n",
    "        'alias': alias,\n",
    "        'latency_avg_s': latency_avg,\n",
    "        'latency_p95_s': latency_p95,\n",
    "        'tokens_per_sec_avg': tokens_per_sec_avg,\n",
    "        'prompt_tokens_total': prompt_tokens_total if token_rounds else None,\n",
    "        'completion_tokens_total': completion_tokens_total if token_rounds else None,\n",
    "        'rounds_ok': len(latencies),\n",
    "        'configured_rounds': ROUNDS,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e635d",
   "metadata": {},
   "source": [
    "### Explanation: Results Rendering\n",
    "Outputs a JSON summary (machine-friendly) and a Markdown table (human-friendly) with aligned columns. Table includes p95 latency for tail insights and tokens/sec if usage data was available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93452b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON Summary:\n",
      "[\n",
      "  {\n",
      "    \"alias\": \"gpt-oss-20b\",\n",
      "    \"latency_avg_s\": 229.65615725517273,\n",
      "    \"latency_p95_s\": 229.65615725517273,\n",
      "    \"tokens_per_sec_avg\": null,\n",
      "    \"prompt_tokens_total\": null,\n",
      "    \"completion_tokens_total\": null,\n",
      "    \"rounds_ok\": 1,\n",
      "    \"configured_rounds\": 3\n",
      "  },\n",
      "  {\n",
      "    \"alias\": \"Phi-4-mini\",\n",
      "    \"latency_avg_s\": 22.810930013656616,\n",
      "    \"latency_p95_s\": 28.72445616722107,\n",
      "    \"tokens_per_sec_avg\": null,\n",
      "    \"prompt_tokens_total\": null,\n",
      "    \"completion_tokens_total\": null,\n",
      "    \"rounds_ok\": 3,\n",
      "    \"configured_rounds\": 3\n",
      "  }\n",
      "]\n",
      "\n",
      "Markdown Table:\n",
      "\n",
      "alias       | lat_avg(s) | lat_p95(s) | tok/s(avg) | prompt_tokens | completion_tokens | rounds_ok\n",
      "----------- | ---------- | ---------- | ---------- | ------------- | ----------------- | ---------\n",
      "gpt-oss-20b | 229.656    | 229.656    | -          | -             | -                 | 1/3      \n",
      "Phi-4-mini  | 22.811     | 28.724     | -          | -             | -                 | 3/3      \n"
     ]
    }
   ],
   "source": [
    "# Render results as JSON and markdown table\n",
    "import math\n",
    "print(\"\\nJSON Summary:\\n\" + json.dumps(summary, indent=2))\n",
    "\n",
    "if summary:\n",
    "    # Build table\n",
    "    headers = [\"alias\",\"lat_avg(s)\",\"lat_p95(s)\",\"tok/s(avg)\",\"prompt_tokens\",\"completion_tokens\",\"rounds_ok\"]\n",
    "    rows = []\n",
    "    for r in summary:\n",
    "        rows.append([\n",
    "            r['alias'],\n",
    "            f\"{r['latency_avg_s']:.3f}\",\n",
    "            f\"{r['latency_p95_s']:.3f}\",\n",
    "            f\"{r['tokens_per_sec_avg']:.1f}\" if r['tokens_per_sec_avg'] else '-',\n",
    "            r.get('prompt_tokens_total') or '-',\n",
    "            r.get('completion_tokens_total') or '-',\n",
    "            f\"{r['rounds_ok']}/{r['configured_rounds']}\"\n",
    "        ])\n",
    "    col_widths = [max(len(str(cell)) for cell in col) for col in zip(headers, *rows)]\n",
    "    def fmt_row(row):\n",
    "        return \" | \".join(str(c).ljust(w) for c, w in zip(row, col_widths))\n",
    "    print(\"\\nMarkdown Table:\\n\")\n",
    "    print(fmt_row(headers))\n",
    "    print(\" | \".join('-'*w for w in col_widths))\n",
    "    for row in rows:\n",
    "        print(fmt_row(row))\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
