# Section 03 - Model Context Protocol (MCP) Integration

## Introduction to MCP (Model Context Protocol)

The Model Context Protocol (MCP) is a revolutionary framework that enables language models to interact with external tools and systems in a standardized way. Unlike traditional approaches where models are isolated, MCP creates a bridge between AI models and the real world through a well-defined protocol.

### What is MCP?

MCP serves as a communication protocol that allows language models to:
- Connect to external data sources
- Execute tools and functions
- Interact with APIs and services
- Access real-time information
- Perform complex multi-step operations

This protocol transforms static language models into dynamic agents capable of performing practical tasks beyond text generation.

## Small Language Models (SLMs) in MCP

Small Language Models represent an efficient approach to AI deployment, offering several advantages:

### Benefits of SLMs
- **Resource Efficiency**: Lower computational requirements
- **Faster Response Times**: Reduced latency for real-time applications  
- **Cost Effectiveness**: Minimal infrastructure needs
- **Privacy**: Can run locally without data transmission
- **Customization**: Easier to fine-tune for specific domains

### Why SLMs Work Well with MCP

SLMs paired with MCP create a powerful combination where the model's reasoning capabilities are augmented by external tools, compensating for their smaller parameter count through enhanced functionality.

## Python MCP SDK Overview

The Python MCP SDK provides the foundation for building MCP-enabled applications. The SDK includes:

- **Client Libraries**: For connecting to MCP servers
- **Server Framework**: For creating custom MCP servers
- **Protocol Handlers**: For managing communication
- **Tool Integration**: For executing external functions

## Practical Implementation: Phi-4 MCP Client

Let's explore a real-world implementation using Microsoft's Phi-4 mini model integrated with MCP capabilities.

### System Architecture

The implementation follows a layered architecture:

```
┌─────────────────────────────────────┐
│        Application Layer           │
│  ├── Interactive Loop              │
│  ├── CLI Interface                 │
│  └── Configuration Management      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│         LLM Client Layer           │
│  ├── OllamaClient                  │
│  ├── VLLMClient                    │
│  └── LLMClient (Abstract)          │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│        MCP Client Layer            │
│  ├── Phi4MiniMCPClient (STDIO)     │
│  ├── Phi4MiniSSEMCPClient (SSE)    │
│  └── BaseMCPClient (Abstract)      │
└─────────────────────────────────────┘
┌─────────────────────────────────────┐
│      Tool Processing Layer         │
│  ├── ToolCallHandler               │
│  ├── Function Format Transformer   │
│  └── Tool Schema Management        │
└─────────────────────────────────────┘
```

### Core Components

#### 1. MCP Client Classes

**BaseMCPClient**: Abstract foundation providing common functionality
- Async context manager protocol
- Standard interface definition
- Resource management

**Phi4MiniMCPClient**: STDIO-based implementation
- Local process communication
- Standard input/output handling
- Subprocess management

**Phi4MiniSSEMCPClient**: Server-Sent Events implementation
- HTTP streaming communication
- Real-time event handling
- Web-based server connectivity

#### 2. LLM Integration

**OllamaClient**: Local model hosting
```python
class OllamaClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:11434/api/chat"
        self.model_id = "phi4-mini:3.8b-fp16"
```

**VLLMClient**: High-performance serving
```python
class VLLMClient(LLMClient):
    def __init__(self):
        self.url = "http://localhost:8000/v1"
        self.model_id = "microsoft/Phi-4-mini-instruct"
```

#### 3. Tool Processing Pipeline

The tool processing pipeline transforms MCP tools into formats compatible with language models:

```python
def transform_functions_format(input_data):
    """Convert MCP tool schemas to LLM-compatible formats"""
    # Maps OpenAPI schemas to function calling schemas
    # Handles parameter type conversion
    # Maintains required field information
```

## Getting Started: Step-by-Step Guide

### Step 1: Environment Setup

Install required dependencies:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Step 2: Basic Configuration

Set up your environment variables:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Step 3: Running Your First MCP Client

**Basic Ollama Setup:**
```bash
python ghmodel_mcp_demo.py
```

**Using vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events Connection:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Custom MCP Server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Step 4: Programmatic Usage

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Advanced Features

### Multi-Backend Support

The implementation supports both Ollama and vLLM backends, allowing you to choose based on your requirements:

- **Ollama**: Better for local development and testing
- **vLLM**: Optimized for production and high-throughput scenarios

### Flexible Connection Protocols

Two connection modes are supported:

**STDIO Mode**: Direct process communication
- Lower latency
- Suitable for local tools
- Simple setup

**SSE Mode**: HTTP-based streaming
- Network-capable
- Better for distributed systems
- Real-time updates

### Tool Integration Capabilities

The system can integrate with various tools:
- Web automation (Playwright)
- File operations
- API interactions
- System commands
- Custom functions

## Error Handling and Best Practices

### Comprehensive Error Management

The implementation includes robust error handling for:

**Connection Errors:**
- MCP server failures
- Network timeouts
- Connectivity issues

**Tool Execution Errors:**
- Missing tools
- Parameter validation
- Execution failures

**Response Processing Errors:**
- JSON parsing issues
- Format inconsistencies
- LLM response anomalies

### Best Practices

1. **Resource Management**: Use async context managers
2. **Error Handling**: Implement comprehensive try-catch blocks
3. **Logging**: Enable appropriate logging levels
4. **Security**: Validate inputs and sanitize outputs
5. **Performance**: Use connection pooling and caching

## Real-World Applications

### Web Automation
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Data Processing
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API Integration
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Performance Optimization

### Memory Management
- Efficient message history handling
- Proper resource cleanup
- Connection pooling

### Network Optimization
- Async HTTP operations
- Configurable timeouts
- Graceful error recovery

### Concurrent Processing
- Non-blocking I/O
- Parallel tool execution
- Efficient async patterns

## Security Considerations

### Data Protection
- Secure API key management
- Input validation
- Output sanitization

### Network Security
- HTTPS support
- Local endpoint defaults
- Secure token handling

### Execution Safety
- Tool filtering
- Sandboxed environments
- Audit logging

## Conclusion

SLMs integrated with MCP represent a paradigm shift in AI application development. By combining the efficiency of small models with the power of external tools, developers can create intelligent systems that are both resource-efficient and highly capable.

The Phi-4 MCP client implementation demonstrates how this integration can be achieved in practice, providing a solid foundation for building sophisticated AI-powered applications.

Key takeaways:
- MCP bridges the gap between language models and external systems
- SLMs offer efficiency without sacrificing capability when augmented with tools
- The modular architecture enables easy extension and customization
- Proper error handling and security measures are essential for production use

This tutorial provides the foundation for building your own SLM-powered MCP applications, opening up possibilities for automation, data processing, and intelligent system integration.