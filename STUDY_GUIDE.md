# EdgeAI for Beginners: C### Concentrated Learning Path (2 weeks)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: AI Agents | 2 hours |
| Day 13-14 | Module 7: Development Tools | 3 hours |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |udy Guide

## Introduction

Welcome to the EdgeAI for Beginners study guide! This document is designed to help you navigate the course materials effectively and maximize your learning experience. It provides structured learning paths, suggested study schedules, key concept summaries, and supplementary resources to deepen your understanding of Edge AI technologies.

This is a concise 20-hour course that delivers essential knowledge about EdgeAI in a time-efficient format, making it perfect for busy professionals and students who want to quickly gain practical skills in this emerging field.

## Course Overview

This course is organized into seven comprehensive modules:

1. **EdgeAI Fundamentals and Transformation** - Understanding the core concepts and technology shift
2. **Small Language Model Foundations** - Exploring various model families and their architectures
3. **Small Language Model Deployment** - Implementing practical deployment strategies
4. **Model Format Conversion and Quantization** - Advanced optimization techniques for edge deployment
5. **SLMOps and Production Deployment** - Operational aspects of SLM lifecycle management
6. **AI Agents and Function Calling** - Building intelligent edge applications
7. **EdgeAI Development Tools and Samples** - Practical implementation across platforms

## How to Use This Study Guide

- **Progressive Learning**: Follow the modules in order for the most coherent learning experience
- **Knowledge Checkpoints**: Use the self-assessment questions after each section
- **Hands-on Practice**: Complete the suggested exercises to reinforce theoretical concepts
- **Supplementary Resources**: Explore additional materials for topics that interest you most

## Study Schedule Recommendations

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|-----------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 6 hours |
| Day 3-4 | Module 2: SLM Foundations | 8 hours |
| Day 5-6 | Module 3: SLM Deployment | 6 hours |

### Part-time Study (3 weeks)

| Week | Focus | Estimated Hours |
|------|-------|-----------------|
| Week 1 | Module 1: EdgeAI Fundamentals | 6-7 hours |
| Week 2 | Module 2: SLM Foundations | 7-8 hours |
| Week 3 | Module 3: SLM Deployment | 5-6 hours |

## Module 1: EdgeAI Fundamentals and Transformation

### Key Learning Objectives

- Understand the differences between cloud-based and edge-based AI
- Master core optimization techniques for resource-constrained environments
- Analyze real-world applications of EdgeAI technologies
- Set up a development environment for EdgeAI projects

### Study Focus Areas

#### Section 1: EdgeAI Fundamentals
- **Priority Concepts**: 
  - Edge vs. Cloud computing paradigms
  - Model quantization techniques
  - Hardware acceleration options (NPUs, GPUs, CPUs)
  - Privacy and security advantages

- **Supplementary Materials**:
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)

#### Section 2: Real-World Case Studies
- **Priority Concepts**: 
  - Microsoft Phi & Mu model ecosystem
  - Practical implementations across industries
  - Deployment considerations

#### Section 3: Practical Implementation Guide
- **Priority Concepts**: 
  - Development environment setup
  - Quantization and optimization tools
  - Assessment methods for EdgeAI implementations

#### Section 4: Edge Deployment Hardware
- **Priority Concepts**: 
  - Hardware platform comparisons
  - Optimization strategies for specific hardware
  - Deployment considerations

### Self-Assessment Questions

1. Compare and contrast cloud-based AI with edge-based AI implementations.
2. Explain three key techniques for optimizing models for edge deployment.
3. What are the primary advantages of running AI models at the edge?
4. Describe the process of quantizing a model and how it affects performance.
5. Explain how different hardware accelerators (NPUs, GPUs, CPUs) influence EdgeAI deployment.

### Hands-on Exercises

1. **Quick Environment Setup**: Configure a minimal development environment with the essential packages (30 minutes)
2. **Model Exploration**: Download and examine a pre-trained small language model (1 hour)
3. **Basic Quantization**: Try simple quantization on a small model (1 hour)

## Module 2: Small Language Model Foundations

### Key Learning Objectives

- Understand the architectural principles of different SLM families
- Compare model capabilities across different parameter scales
- Evaluate models based on efficiency, capability, and deployment requirements
- Recognize appropriate use cases for different model families

### Study Focus Areas

#### Section 1: Microsoft Phi Model Family
- **Priority Concepts**: 
  - Design philosophy evolution
  - Efficiency-first architecture
  - Specialized capabilities

#### Section 2: Qwen Family
- **Priority Concepts**: 
  - Open source contributions
  - Scalable deployment options
  - Advanced reasoning architecture

#### Section 3: Gemma Family
- **Priority Concepts**: 
  - Research-driven innovation
  - Multimodal capabilities
  - Mobile optimization

#### Section 4: BitNET Family
- **Priority Concepts**: 
  - 1-bit quantization technology
  - Inference optimization framework
  - Sustainability considerations

#### Section 5: Microsoft Mu Model
- **Priority Concepts**: 
  - Device-first architecture
  - System integration with Windows
  - Privacy-preserving operation

#### Section 6: Phi-Silica
- **Priority Concepts**: 
  - NPU-optimized architecture
  - Performance metrics
  - Developer integration

### Self-Assessment Questions

1. Compare the architectural approaches of the Phi and Qwen model families.
2. Explain how BitNET's quantization technology differs from traditional quantization.
3. What are the unique advantages of the Mu model for Windows integration?
4. Describe how Phi-Silica leverages NPU hardware for performance optimization.
5. For a mobile application with limited connectivity, which model family would be most appropriate and why?

### Hands-on Exercises

1. **Model Comparison**: Quick benchmark of two different SLM models (1 hour)
2. **Simple Text Generation**: Basic implementation of text generation with a small model (1 hour)
3. **Fast Optimization**: Apply one optimization technique to improve inference speed (1 hour)

## Module 3: Small Language Model Deployment

### Key Learning Objectives

- Select appropriate models based on deployment constraints
- Master optimization techniques for various deployment scenarios
- Implement SLMs in both local and cloud environments
- Design production-ready configurations for EdgeAI applications

### Study Focus Areas

#### Section 1: SLM Advanced Learning
- **Priority Concepts**: 
  - Parameter classification framework
  - Advanced optimization techniques
  - Model acquisition strategies

#### Section 2: Local Environment Deployment
- **Priority Concepts**: 
  - Ollama platform deployment
  - Microsoft Foundry local solutions
  - Framework comparative analysis

#### Section 3: Containerized Cloud Deployment
- **Priority Concepts**: 
  - vLLM high-performance inference
  - Container orchestration
  - ONNX Runtime implementation

### Self-Assessment Questions

1. What factors should be considered when selecting between local deployment and cloud deployment?
2. Compare Ollama and Microsoft Foundry Local as deployment options.
3. Explain the benefits of containerization for SLM deployment.
4. What are the key performance metrics to monitor for an edge-deployed SLM?
5. Describe a complete deployment workflow from model selection to production implementation.

### Hands-on Exercises

1. **Basic Local Deployment**: Deploy a simple SLM using Ollama (1 hour)
2. **Performance Check**: Run a quick benchmark on your deployed model (30 minutes)
3. **Simple Integration**: Create a minimal application that uses your deployed model (1 hour)

## Module 4: Model Format Conversion and Quantization

### Key Learning Objectives

- Master advanced quantization techniques from 1-bit to 8-bit precision
- Understand format conversion strategies (GGUF, ONNX)
- Implement optimization across multiple frameworks
- Deploy optimized models for production edge environments

### Study Focus Areas

#### Section 1: Quantization Foundations
- **Priority Concepts**: 
  - Precision classification framework
  - Performance vs. accuracy trade-offs
  - Memory footprint optimization

#### Section 2: Llama.cpp Implementation
- **Priority Concepts**: 
  - Cross-platform deployment
  - GGUF format optimization
  - Hardware acceleration techniques

#### Section 3: Microsoft Olive Suite
- **Priority Concepts**: 
  - Hardware-aware optimization
  - Enterprise-grade deployment
  - Automated optimization workflows

#### Section 4: OpenVINO Toolkit
- **Priority Concepts**: 
  - Intel hardware optimization
  - Neural Network Compression Framework (NNCF)
  - Cross-platform inference deployment

#### Section 5: Apple MLX Framework
- **Priority Concepts**: 
  - Apple Silicon optimization
  - Unified memory architecture
  - LoRA fine-tuning capabilities

### Self-Assessment Questions

1. Compare quantization strategies across different precision levels (1-bit to 8-bit).
2. Explain the advantages of GGUF format for edge deployment.
3. How does hardware-aware optimization in Microsoft Olive improve deployment efficiency?
4. What are the key benefits of OpenVINO's NNCF for model compression?
5. Describe how Apple MLX leverages unified memory architecture for optimization.

### Hands-on Exercises

1. **Model Quantization**: Apply different quantization levels to a model and compare results (1 hour)
2. **Framework Comparison**: Test the same model across two different optimization frameworks (1 hour)
3. **Performance Benchmarking**: Measure optimization impact on inference speed and memory usage (1 hour)

## Module 5: SLMOps and Production Deployment

### Key Learning Objectives

- Understand SLMOps lifecycle management
- Master distillation and fine-tuning techniques
- Implement production deployment strategies
- Monitor and maintain edge AI systems

### Study Focus Areas

#### Section 1: Introduction to SLMOps
- **Priority Concepts**: 
  - MLOps principles for small language models
  - Lifecycle management strategies
  - DevOps integration patterns

#### Section 2: Model Distillation
- **Priority Concepts**: 
  - Knowledge transfer techniques
  - Teacher-student model architectures
  - Performance preservation strategies

#### Section 3: Fine-tuning Strategies
- **Priority Concepts**: 
  - Parameter-efficient fine-tuning
  - Domain adaptation techniques
  - Resource-constrained training

#### Section 4: Production Deployment
- **Priority Concepts**: 
  - Scalable deployment architectures
  - Monitoring and observability
  - Performance optimization in production

### Self-Assessment Questions

1. How does SLMOps differ from traditional MLOps?
2. Explain the benefits of model distillation for edge deployment.
3. What are the key considerations for fine-tuning SLMs in resource-constrained environments?
4. Describe a complete production deployment pipeline for edge AI applications.

### Hands-on Exercises

1. **Basic Distillation**: Create a smaller model from a larger teacher model (1 hour)
2. **Fine-tuning Experiment**: Fine-tune a model for a specific domain (1 hour)
3. **Deployment Pipeline**: Set up a basic CI/CD pipeline for model deployment (1 hour)

## Module 6: AI Agents and Function Calling

### Key Learning Objectives

- Build intelligent AI agents for edge environments
- Implement function calling capabilities
- Understand Model Context Protocol (MCP)
- Create agent-based applications

### Study Focus Areas

#### Section 1: Introduction to AI Agents
- **Priority Concepts**: 
  - Agent architecture patterns
  - Edge-specific agent design
  - Resource optimization for agents

#### Section 2: Function Calling
- **Priority Concepts**: 
  - Function definition and schema
  - Execution patterns
  - Error handling and validation

#### Section 3: Model Context Protocol
- **Priority Concepts**: 
  - MCP architecture
  - Integration strategies
  - Best practices for implementation

### Self-Assessment Questions

1. What are the key architectural considerations for edge AI agents?
2. How does function calling enhance agent capabilities?
3. Explain the role of Model Context Protocol in agent communication.

### Hands-on Exercises

1. **Simple Agent**: Build a basic AI agent with function calling (1 hour)
2. **MCP Integration**: Implement MCP in an agent application (30 minutes)

## Module 7: EdgeAI Development Tools and Samples

### Key Learning Objectives

- Master AI Toolkit for Visual Studio Code
- Implement EdgeAI across multiple platforms
- Deploy solutions on various hardware architectures
- Build production-ready EdgeAI applications

### Study Focus Areas

#### Section 1: AI Toolkit for VS Code
- **Priority Concepts**: 
  - Model catalog and discovery
  - Local testing and optimization
  - Agent development workflows
  - Performance monitoring and evaluation

#### Section 2: Platform-Specific Implementations
- **Priority Concepts**: 
  - NVIDIA Jetson deployment
  - Mobile applications with .NET MAUI
  - Azure EdgeAI solutions
  - Windows ML optimization

#### Section 3: Windows Developer Tools
- **Priority Concepts**: 
  - Windows AI Foundry platform
  - Phi Silica API integration
  - NPU optimization strategies
  - Local development workflows

### Self-Assessment Questions

1. How does AI Toolkit streamline the EdgeAI development workflow?
2. Compare deployment strategies across different hardware platforms.
3. What are the advantages of Windows AI Foundry for edge development?
4. Explain the role of NPU optimization in modern edge AI applications.

### Hands-on Exercises

1. **AI Toolkit Setup**: Configure AI Toolkit and optimize a model (1 hour)
2. **Cross-Platform Deployment**: Deploy the same model on two different platforms (1 hour)
3. **Windows Integration**: Build a simple Windows AI application (1 hour)

## Time Allocation Guide

To help you make the most of the 20-hour course timeline, here's a suggested breakdown of how to allocate your time:

| Activity | Time Allocation | Description |
|----------|----------------|-------------|
| Reading Core Materials | 9 hours | Focusing on the essential concepts in each module |
| Hands-on Exercises | 6 hours | Practical implementation of key techniques |
| Self-Assessment | 2 hours | Testing your understanding through questions and reflection |
| Mini-Project | 3 hours | Applying knowledge to a small practical implementation |

### Key Focus Areas by Time Constraint

**If you only have 10 hours:**
- Complete Modules 1, 2, and 3 (core EdgeAI concepts)
- Do at least one hands-on exercise per module
- Focus on understanding the core concepts rather than implementation details

**If you can dedicate the full 20 hours:**
- Complete all seven modules
- Perform key hands-on exercises from each module
- Complete one mini-project from Module 7
- Explore at least 2-3 supplementary resources

**If you have more than 20 hours:**
- Complete all modules with detailed exercises
- Build multiple mini-projects
- Explore advanced optimization techniques in Module 4
- Implement production deployment from Module 5

## Essential Resources

These carefully selected resources provide the most value for your limited study time:

### Must-Read Documentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - The most efficient model optimization tool
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Fastest way to deploy SLMs locally
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Reference for a leading edge-optimized model
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intel's comprehensive optimization toolkit
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - Integrated EdgeAI development environment

### Time-Saving Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Quick model access and deployment
- [Gradio](https://www.gradio.app/docs/interface) - Rapid UI development for AI demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simplified model optimization
- [Llama.cpp](https://github.com/ggml-org/llama.cpp) - Efficient CPU inference
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Neural network compression framework

## Progress Tracking Template

Use this simplified template to track your learning progress through the 20-hour course:

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|---------------|
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization | | | |
| Module 5: SLMOps | | | |
| Module 6: AI Agents | | | |
| Module 7: Development Tools | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

## Mini Project Ideas

Consider completing one of these projects to practice EdgeAI concepts (each designed to take 2-4 hours):

### Beginner Projects (2-3 hours each)
1. **Edge Text Assistant**: Create a simple offline text completion tool using a small language model
2. **Model Comparison Dashboard**: Build a basic visualization of performance metrics across different SLMs
3. **Optimization Experiment**: Measure the impact of different quantization levels on the same base model

### Intermediate Projects (3-4 hours each)
4. **AI Toolkit Workflow**: Use VS Code AI Toolkit to optimize and deploy a model from start to finish
5. **Cross-Platform Deployment**: Deploy the same optimized model on Windows (OpenVINO) and mobile (.NET MAUI)
6. **Function Calling Agent**: Build an AI agent with function calling capabilities for edge scenarios

### Advanced Integration Projects (4-5 hours each)
7. **Windows AI Foundry Application**: Create a Windows app using Phi Silica API and NPU optimization
8. **SLMOps Pipeline**: Implement a complete model lifecycle from training to edge deployment
9. **Multi-Model Edge System**: Deploy multiple specialized models working together on edge hardware

## Learning Community

Join the discussion and connect with fellow learners:
- GitHub Discussions on the [EdgeAI for Beginners repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI represents the frontier of artificial intelligence implementation, bringing powerful capabilities directly to devices while addressing critical concerns about privacy, latency, and connectivity. This 20-hour course provides you with the essential knowledge and practical skills to begin working with EdgeAI technologies immediately.

The course is deliberately concise and focused on the most important concepts, allowing you to quickly gain valuable expertise without an overwhelming time commitment. Remember that hands-on practice, even with simple examples, is the key to reinforcing what you've learned.

Happy learning!
